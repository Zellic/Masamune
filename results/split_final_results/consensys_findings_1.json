[{"title": "4.1 Users can withdraw their funds immediately when they are over-leveraged ", "body": "  Description  Accounts.withdraw makes two checks before processing a withdrawal.  First, the method checks that the amount requested for withdrawal is not larger than the user s balance for the asset in question:  code/contracts/Accounts.sol:L197-L201  function withdraw(address _accountAddr, address _token, uint256 _amount) external onlyAuthorized returns(uint256) {  // Check if withdraw amount is less than user's balance  require(_amount <= getDepositBalanceCurrent(_token, _accountAddr), \"Insufficient balance.\");  uint256 borrowLTV = globalConfig.tokenInfoRegistry().getBorrowLTV(_token);  Second, the method checks that the withdrawal will not over-leverage the user. The amount to be withdrawn is subtracted from the user s current  borrow power  at the current price. If the user s total value borrowed exceeds this new borrow power, the method fails, as the user no longer has sufficient collateral to support their borrow positions. However, this require is only checked if a user is not already over-leveraged:  code/contracts/Accounts.sol:L203-L211  // This if condition is to deal with the withdraw of collateral token in liquidation.  // As the amount if borrowed asset is already large than the borrow power, we don't  // have to check the condition here.  if(getBorrowETH(_accountAddr) <= getBorrowPower(_accountAddr))  require(  getBorrowETH(_accountAddr) <= getBorrowPower(_accountAddr).sub(  _amount.mul(globalConfig.tokenInfoRegistry().priceFromAddress(_token))  .mul(borrowLTV).div(Utils.getDivisor(address(globalConfig), _token)).div(100)  ), \"Insufficient collateral when withdraw.\");  If the user has already borrowed more than their  borrow power  allows, they are allowed to withdraw regardless. This case may arise in several circumstances; the most common being price fluctuation.  Recommendation  Disallow withdrawals if the user is already over-leveraged.  From the comment included in the code sample above, this condition is included to support the liquidate method, but its inclusion creates an attack vector that may allow users to withdraw when they should not be able to do so. Consider adding an additional method to support liquidate, so that users may not exit without repaying debts.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.2 Users can borrow funds, deposit them, then borrow more   ", "body": "  Resolution  Comment from DeFiner team:  This is expected behaviour and our contracts are designed like that. Other lending protocols like Compound and AAVE allows this feature as well. So this is not a CRITICAL issue, as the user s funds are not at risk. The funds of the users are only at risk when their position is over-leveraged, which is expected behaviour.  Description  Users may deposit and borrow funds denominated in any asset supported by the TokenRegistry. Each time a user deposits or borrows a token, they earn FIN according to the difference in deposit / borrow rate indices maintained by Bank.  Borrowing funds  When users borrow funds, they may only borrow up to a certain amount: the user s  borrow power.  As long as the user is not requesting to borrow an amount that would cause their resulting borrowed asset value to exceed their available borrow power, the borrow is successful and the user receives the assets immediately. A user s borrow power is calculated in the following function:  code/contracts/Accounts.sol:L333-L353  /**  Calculate an account's borrow power based on token's LTV  /  function getBorrowPower(address _borrower) public view returns (uint256 power) {  for(uint8 i = 0; i < globalConfig.tokenInfoRegistry().getCoinLength(); i++) {  if (isUserHasDeposits(_borrower, i)) {  address token = globalConfig.tokenInfoRegistry().addressFromIndex(i);  uint divisor = INT_UNIT;  if(token != ETH_ADDR) {  divisor = 10**uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(token));  // globalConfig.bank().newRateIndexCheckpoint(token);  power = power.add(getDepositBalanceCurrent(token, _borrower)  .mul(globalConfig.tokenInfoRegistry().priceFromIndex(i))  .mul(globalConfig.tokenInfoRegistry().getBorrowLTV(token)).div(100)  .div(divisor)  );  return power;  For each asset, borrow power is calculated from the user s deposit size, multiplied by the current chainlink price, multiplied and that asset s  borrow LTV.   Depositing borrowed funds  After a user borrows tokens, they can then deposit those tokens, increasing their deposit balance for that asset. As a result, their borrow power increases, which allows the user to borrow again.  By continuing to borrow, deposit, and borrow again, the user can repeatedly borrow assets. Essentially, this creates positions for the user where the collateral for their massive borrow position is entirely made up of borrowed assets.  Conclusion  There are several potential side-effects of this behavior.  First, as described in issue 4.6, the system is comprised of many different tokens, each of which is subject to price fluctuation. By borrowing and depositing repeatedly, a user may establish positions across all supported tokens. At this point, if price fluctuations cause the user s account to cross the liquidation threshold, their positions can be liquidated.  Liquidation is a complicated function of the protocol, but in essence, the liquidator purchases a target s collateral at a discount, and the resulting sale balances the account somewhat. However, when a user repeatedly deposits borrowed tokens, their collateral is made up of borrowed tokens: the system s liquidity! As a result, this may allow an attacker to intentionally create a massively over-leveraged account on purpose, liquidate it, and exit with a chunk of the system liquidity.  Another potential problem with this behavior is FIN token mining. When users borrow and deposit, they earn FIN according to the size of the deposit / borrow, and the difference in deposit / borrow rate indices since the last deposit / borrow. By repeatedly depositing / borrowing, users are able to artificially deposit and borrow far more often than normal, which may allow them to generate FIN tokens at will. This additional strategy may make attacks like the one described above much more economically feasible.  Recommendation  Due to the limited time available during this engagement, these possibilities and potential mitigations were not fully explored. Definer is encouraged to investigate this behavior more carefully.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.3 Stale Oracle prices might affect the rates ", "body": "  Description  It s possible that due to network congestion or other reasons, the price that the ChainLink oracle returns is old and not up to date. This is more extreme in lesser known tokens that have fewer ChainLink Price feeds to update the price frequently. The codebase as is, relies on chainLink().getLatestAnswer() and does not check the timestamp of the price.  Examples  /contracts/registry/TokenRegistry.sol#L291-L296  function priceFromAddress(address tokenAddress) public view returns(uint256) {  if(Utils._isETH(address(globalConfig), tokenAddress)) {  return 1e18;  return uint256(globalConfig.chainLink().getLatestAnswer(tokenAddress));  Recommendation  Do a sanity check on the price returned from the oracle. If the price is older than a threshold, revert or handle in other means.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.4 Overcomplicated unit conversions ", "body": "  Description  There are many instances of unit conversion in the system that are implemented in a confusing way. This could result in mistakes in the conversion and possibly failure in correct accounting. It s been seen in the ecosystem that these type of complicated unit conversions could result in calculation mistake and loss of funds.  Examples  Here are a few examples:  /contracts/Bank.sol#L216-L224  function getBorrowRatePerBlock(address _token) public view returns(uint) {  if(!globalConfig.tokenInfoRegistry().isSupportedOnCompound(_token))  // If the token is NOT supported by the third party, borrowing rate = 3% + U * 15%.  return getCapitalUtilizationRatio(_token).mul(globalConfig.rateCurveSlope()).div(INT_UNIT).add(globalConfig.rateCurveConstant()).div(BLOCKS_PER_YEAR);  // if the token is suppored in third party, borrowing rate = Compound Supply Rate * 0.4 + Compound Borrow Rate * 0.6  return (compoundPool[_token].depositRatePerBlock).mul(globalConfig.compoundSupplyRateWeights()).  add((compoundPool[_token].borrowRatePerBlock).mul(globalConfig.compoundBorrowRateWeights())).div(10);  /contracts/Bank.sol#L350-L351  compoundPool[_token].depositRatePerBlock = cTokenExchangeRate.mul(UNIT).div(lastCTokenExchangeRate[cToken])  .sub(UNIT).div(blockNumber.sub(lastCheckpoint[_token]));  /contracts/Bank.sol#L384-L385  return lastDepositeRateIndex.mul(getBlockNumber().sub(lcp).mul(depositRatePerBlock).add(INT_UNIT)).div(INT_UNIT);  Recommendation  Simplify the unit conversions in the system. This can be done either by using a function wrapper for units to convert all values to the same unit before including them in any calculation or by better documenting every line of unit conversion  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.5 Commented out code in the codebase ", "body": "  Description  There are many instances of code lines (and functions) that are commented out in the code base. Having commented out code increases the cognitive load on an already complex system. Also, it hides the important parts of the system that should get the proper attention, but that attention gets to be diluted.  The main problem is that commented code adds confusion with no real benefit. Code should be code, and comments should be comments.  Examples  Here s a few examples of such lines of code, note that there are more.  /contracts/SavingAccount.sol#L211-L218  struct LiquidationVars {  // address token;  // uint256 tokenPrice;  // uint256 coinValue;  uint256 borrowerCollateralValue;  // uint256 tokenAmount;  // uint256 tokenDivisor;  uint256 msgTotalBorrow;  contracts/Accounts.sol#L341-L345  if(token != ETH_ADDR) {  divisor = 10**uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(token));  // globalConfig.bank().newRateIndexCheckpoint(token);  power = power.add(getDepositBalanceCurrent(token, _borrower)  Many usage of console.log() and also the commented import on most of the contracts  // import \"@nomiclabs/buidler/console.sol\";  ...  //console.log(\"tokenNum\", tokenNum);  /contracts/Accounts.sol#L426-L429  // require(  //     totalBorrow.mul(100) <= totalCollateral.mul(liquidationDiscountRatio),  //     \"Collateral is not sufficient to be liquidated.\"  // );  /contracts/registry/TokenRegistry.sol#L298-L306  // function _isETH(address _token) public view returns (bool) {  //     return globalConfig.constants().ETH_ADDR() == _token;  // }  // function getDivisor(address _token) public view returns (uint256) {  //     if(_isETH(_token)) return INT_UNIT;  //     return 10 ** uint256(getTokenDecimals(_token));  // }  /contracts/registry/TokenRegistry.sol#L118-L121  // require(_borrowLTV != 0, \"Borrow LTV is zero\");  require(_borrowLTV < SCALE, \"Borrow LTV must be less than Scale\");  // require(liquidationThreshold > _borrowLTV, \"Liquidation threshold must be greater than Borrow LTV\");  Recommendation  In many of the above examples, it s not clear if the commented code is for testing or obsolete code (e.g. in the last example, can _borrowLTV ==0?) . All these instances should be reviewed and the system should be fully tested for all edge cases after the code changes.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.6 Price volatility may compromise system integrity   ", "body": "  Resolution  Comment from DeFiner team:  The issue says that due to price volatility there could be an attack on DeFiner. However, price volatility is inherent in the Cryptocurrency ecosystem. All the other lending platforms like MakerDAO, Compound and AAVE also designed like that, in case of price volatility(downside) more liquidation happens on these platforms as well. Liquidations are in a sense good to keep the market stable. If there is no liquidation during those market crash, the system will be at risk. Due to this, it is always recommended to maintain the collateral and borrow ratio by the user. A user should keep checking his risk in the time when the market crashes.  Description  SavingAccount.borrow allows users to borrow funds from the bank. The funds borrowed may be denominated in any asset supported by the system-wide TokenRegistry. Borrowed funds come from the system s existing liquidity: other users  deposits.  Borrowing funds is an instant process. Assuming the user has sufficient collateral to service the borrow request (as well as any existing loans), funds are sent to the user immediately:  code/contracts/SavingAccount.sol:L130-L140  function borrow(address _token, uint256 _amount) external onlySupportedToken(_token) onlyEnabledToken(_token) whenNotPaused nonReentrant {  require(_amount != 0, \"Borrow zero amount of token is not allowed.\");  globalConfig.bank().borrow(msg.sender, _token, _amount);  // Transfer the token on Ethereum  SavingLib.send(globalConfig, _amount, _token);  emit Borrow(_token, msg.sender, _amount);  Users may borrow up to their  borrow power , which is the sum of their deposit balance for each token, multiplied by each token s borrowLTV, multiplied by the token price (queried from a chainlink oracle):  code/contracts/Accounts.sol:L344-L349  // globalConfig.bank().newRateIndexCheckpoint(token);  power = power.add(getDepositBalanceCurrent(token, _borrower)  .mul(globalConfig.tokenInfoRegistry().priceFromIndex(i))  .mul(globalConfig.tokenInfoRegistry().getBorrowLTV(token)).div(100)  .div(divisor)  );  If users borrow funds, their position may be liquidated via SavingAccount.liquidate. An account is considered liquidatable if the total value of borrowed funds exceeds the total value of collateral (multiplied by some liquidation threshold ratio). These values are calculated similarly to  borrow power:  the sum of the deposit balance for each token, multiplied by each token s borrowLTV, multiplied by the token price as determined by chainlink.  Conclusion  The instant-borrow approach, paired with the chainlink oracle represents a single point of failure for the Definer system. When the price of any single supported asset is sufficiently volatile, the entire liquidity held by the system is at risk as borrow power and collateral value become similarly volatile.  Some users may find their borrow power skyrocket and use this inflated value to drain large amounts of system liquidity they have no intention of repaying. Others may find their held collateral tank in value and be subject to sudden liquidations.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.7 Emergency withdrawal code present ", "body": "  Description  Code and functionality for emergency stop and withdrawal is present in this code base.  Examples  /contracts/lib/SavingLib.sol#L43-L48  // ============================================  // EMERGENCY WITHDRAWAL FUNCTIONS  // Needs to be removed when final version deployed  // ============================================  function emergencyWithdraw(GlobalConfig globalConfig, address _token) public {  address cToken = globalConfig.tokenInfoRegistry().getCToken(_token);  ...  /contracts/SavingAccount.sol#L307-L309  function emergencyWithdraw(address _token) external onlyEmergencyAddress {  SavingLib.emergencyWithdraw(globalConfig, _token);  /contracts/config/Constant.sol#L7-L8  ...  address payable public constant EMERGENCY_ADDR = 0xc04158f7dB6F9c9fFbD5593236a1a3D69F92167c;  ...  Recommendation  To remove the emergency code and fully test all the affected contracts.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.8 Accounts contains expensive looping ", "body": "  Description  Accounts.getBorrowETH performs multiple external calls to GlobalConfig and TokenRegistry within a for loop:  code/contracts/Accounts.sol:L381-L397  function getBorrowETH(  address _accountAddr  ) public view returns (uint256 borrowETH) {  uint tokenNum = globalConfig.tokenInfoRegistry().getCoinLength();  //console.log(\"tokenNum\", tokenNum);  for(uint i = 0; i < tokenNum; i++) {  if(isUserHasBorrows(_accountAddr, uint8(i))) {  address tokenAddress = globalConfig.tokenInfoRegistry().addressFromIndex(i);  uint divisor = INT_UNIT;  if(tokenAddress != ETH_ADDR) {  divisor = 10 ** uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(tokenAddress));  borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, _accountAddr).mul(globalConfig.tokenInfoRegistry().priceFromIndex(i)).div(divisor));  return borrowETH;  The loop also makes additional external calls and delegatecalls from:  TokenRegistry.priceFromIndex:  code/contracts/registry/TokenRegistry.sol:L281-L289  function priceFromIndex(uint index) public view returns(uint256) {  require(index < tokens.length, \"coinIndex must be smaller than the coins length.\");  address tokenAddress = tokens[index];  // Temp fix  if(Utils._isETH(address(globalConfig), tokenAddress)) {  return 1e18;  return uint256(globalConfig.chainLink().getLatestAnswer(tokenAddress));  Accounts.getBorrowBalanceCurrent:  code/contracts/Accounts.sol:L313-L331  function getBorrowBalanceCurrent(  address _token,  address _accountAddr  ) public view returns (uint256 borrowBalance) {  AccountTokenLib.TokenInfo storage tokenInfo = accounts[_accountAddr].tokenInfos[_token];  uint accruedRate;  if(tokenInfo.getBorrowPrincipal() == 0) {  return 0;  } else {  if(globalConfig.bank().borrowRateIndex(_token, tokenInfo.getLastBorrowBlock()) == 0) {  accruedRate = INT_UNIT;  } else {  accruedRate = globalConfig.bank().borrowRateIndexNow(_token)  .mul(INT_UNIT)  .div(globalConfig.bank().borrowRateIndex(_token, tokenInfo.getLastBorrowBlock()));  return tokenInfo.getBorrowBalance(accruedRate);  In a worst case scenario, each iteration may perform a maximum of 25+ calls/delegatecalls. Assuming a maximum tokenNum of 128 (TokenRegistry.MAX_TOKENS), the gas cost for this method may reach upwards of 2 million for external calls alone.  Given that this figure would only be a portion of the total transaction gas cost, getBorrowETH may represent a DoS risk within the Accounts contract.  Recommendation  Avoid for loops unless absolutely necessary  Where possible, consolidate multiple subsequent calls to the same contract to a single call, and store the results of calls in local variables for re-use. For example,  Instead of this:  uint tokenNum = globalConfig.tokenInfoRegistry().getCoinLength();  for(uint i = 0; i < tokenNum; i++) {  if(isUserHasBorrows(_accountAddr, uint8(i))) {  address tokenAddress = globalConfig.tokenInfoRegistry().addressFromIndex(i);  uint divisor = INT_UNIT;  if(tokenAddress != ETH_ADDR) {  divisor = 10 ** uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(tokenAddress));  borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, _accountAddr).mul(globalConfig.tokenInfoRegistry().priceFromIndex(i)).div(divisor));  Modify TokenRegistry to support a single call, and cache intermediate results like this:  TokenRegistry registry = globalConfig.tokenInfoRegistry();  uint tokenNum = registry.getCoinLength();  for(uint i = 0; i < tokenNum; i++) {  if(isUserHasBorrows(_accountAddr, uint8(i))) {  // here, getPriceFromIndex(i) performs all of the steps as the code above, but with only 1 ext call  borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, _accountAddr).mul(registry.getPriceFromIndex(i)).div(divisor));  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "4.9 Naming inconsistency ", "body": "  Description  There are some inconsistencies in the naming of some functions with what they do.  Examples  /contracts/registry/TokenRegistry.sol#L272-L274  function getCoinLength() public view returns (uint256 length) { //@audit-info coin vs token  return tokens.length;  Recommendation  Review the code for the naming inconsistencies.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/definer/"}, {"title": "5.1 The virtual price may not correspond to the actual price in the pool ", "body": "  Description  A Curve pool has a function that returns a  virtual price  of the LP token; this price is resistant to flash-loan attacks and any manipulations in the Curve pool. While this price formula works well in some cases, there may be a significant period when a trade cannot be executed with this price. So the deposit or withdrawal will also be done under another price and will have a different result than the one estimated under the  virtual price .  When depositing into Curve, Brahma is doing it in 2 steps. First, when depositing the user s ETH to the Vault, the user s share is calculated according to the  virtual price . And then, in a different transaction, the funds are deposited into the Curve pool. These funds only consist of ETH, and if the deposit price does not correspond (with 0.3% slippage) to the virtual price, it will revert.  So we have multiple problems here:  If the chosen slippage parameter is very low, the funds will not be deposited/withdrawn for a long time due to reverts.  If the slippage is large enough, the attacker can manipulate the price to steal the slippage. Additionally, because of the 2-steps deposit, the amount of Vault s share minted to the users may not correspond to the LP tokens minted during the second step.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.2 ConvexPositionHandler._claimRewards incorrectly calculates amount of LP tokens to unstake ", "body": "  Description  ConvexPositionHandler._claimRewards is an internal function that harvests Convex reward tokens and takes the generated yield in ETH out of the Curve pool by calculating the difference in LP token price. To do so, it receives the current share price of the curve LP tokens and compares it to the last one stored in the contract during the last rewards claim. The difference in share price is then multiplied by the LP token balance to get the ETH yield via the yieldEarned variable:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L293-L300  uint256 currentSharePrice = ethStEthPool.get_virtual_price();  if (currentSharePrice > prevSharePrice) {  // claim any gain on lp token yields  uint256 contractLpTokenBalance = lpToken.balanceOf(address(this));  uint256 totalLpBalance = contractLpTokenBalance +  baseRewardPool.balanceOf(address(this));  uint256 yieldEarned = (currentSharePrice - prevSharePrice) *  totalLpBalance;  However, to receive this ETH yield, LP tokens need to be unstaked from the Convex pool and then converted via the Curve pool. To do this, the contract introduces lpTokenEarned:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L302  uint256 lpTokenEarned = yieldEarned / NORMALIZATION_FACTOR; // 18 decimal from virtual price  This calculation is incorrect. It uses yieldEarned which is denominated in ETH and simply divides it by the normalization factor to get the correct number of decimals, which still returns back an amount denominated in ETH, whereas an amount denominated in LP tokens should be returned instead.  This could lead to significant accounting issues including losses in the  no-loss  parts of the vault s strategy as 1 LP token is almost always guaranteed to be worth more than 1 ETH. So, when the intention is to withdraw X ETH worth of an LP token, withdrawing X LP tokens will actually withdraw Y ETH worth of an LP token, where Y>X. As a result, less than expected ETH will remain in the Convex handler part of the vault, and the ETH yield will go to the Lyra options, which are much riskier. In the event Lyra options don t work out and there is more ETH withdrawn than expected, there is a possibility that this would result in a loss for the vault.  Recommendation  The fix is straightforward and that is to calculate lpTokenEarned using the currentSharePrice already received from the Curve pool. That way, it is the amount of LP tokens that will be sent to be unwrapped and unstaked from the Convex and Curve pools. This will also take care of the normalization factor.  uint256 lpTokenEarned = yieldEarned / currentSharePrice;  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.3 The WETH tokens are not taken into account in the ConvexTradeExecutor.totalFunds function ", "body": "  Description  The totalFunds function of every executor should include all the funds that belong to the contract:  code/contracts/ConvexTradeExecutor.sol:L21-L23  function totalFunds() public view override returns (uint256, uint256) {  return ConvexPositionHandler.positionInWantToken();  The ConvexTradeExecutor uses this function for calculations:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137  function positionInWantToken()  public  view  override  returns (uint256, uint256)  uint256 stakedLpBalanceInETH,  uint256 lpBalanceInETH,  uint256 ethBalance  ) = _getTotalBalancesInETH(true);  return (  stakedLpBalanceInETH + lpBalanceInETH + ethBalance,  block.number  );  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L337-L365  function _getTotalBalancesInETH(bool useVirtualPrice)  internal  view  returns (  uint256 stakedLpBalance,  uint256 lpTokenBalance,  uint256 ethBalance  uint256 stakedLpBalanceRaw = baseRewardPool.balanceOf(address(this));  uint256 lpTokenBalanceRaw = lpToken.balanceOf(address(this));  uint256 totalLpBalance = stakedLpBalanceRaw + lpTokenBalanceRaw;  // Here, in order to prevent price manipulation attacks via curve pools,  // When getting total position value -> its calculated based on virtual price  // During withdrawal -> calc_withdraw_one_coin() is used to get an actual estimate of ETH received if we were to remove liquidity  // The following checks account for this  uint256 totalLpBalanceInETH = useVirtualPrice  ? _lpTokenValueInETHFromVirtualPrice(totalLpBalance)  : _lpTokenValueInETH(totalLpBalance);  lpTokenBalance = useVirtualPrice  ? _lpTokenValueInETHFromVirtualPrice(lpTokenBalanceRaw)  : _lpTokenValueInETH(lpTokenBalanceRaw);  stakedLpBalance = totalLpBalanceInETH - lpTokenBalance;  ethBalance = address(this).balance;  This function includes ETH balance, LP balance, and staked balance. But WETH balance is not included here. WETH tokens are initially transferred to the contract, and before the withdrawal, the contract also stores WETH.  Recommendation  Include WETH balance into the totalFunds.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.4 LyraPositionHandlerL2 inaccurate modifier onlyAuthorized may lead to funds loss if keeper is compromised ", "body": "  Description  The LyraPositionHandlerL2 contract is operated either by the L2 keeper or by the L1 LyraPositionHandler via the L2CrossDomainMessenger. This is implemented through the onlyAuthorized modifier:  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L187-L195  modifier onlyAuthorized() {  require(  ((msg.sender == L2CrossDomainMessenger &&  OptimismL2Wrapper.messageSender() == positionHandlerL1) ||  msg.sender == keeper),  \"ONLY_AUTHORIZED\"  );  _;  This is set on:  withdraw()  openPosition()  closePosition()  setSlippage()  deposit()  sweep()  setSocketRegistry()  setKeeper()  Functions 1-3 have a corresponding implementation on the L1 LyraPositionHandler, so they could indeed be called by it with the right parameters. However, 4-8 do not have an implemented way to call them from L1, and this modifier creates an unnecessarily expanded list of authorised entities that can call them.  Additionally, even if their implementation is provided, it needs to be done carefully because msg.sender in their case is going to end up being the L2CrossDomainMessenger. For example, the sweep() function sends any specified token to msg.sender, with the intention likely being that the recipient is under the team s or the governance s control   yet, it will be L2CrossDomainMessenger and the tokens will likely be lost forever instead.  On the other hand, the setKeeper() function would need a way to be called by something other than the keeper because it is intended to change the keeper itself. In the event that the access to the L2 keeper is compromised, and the L1 LyraPositionHandler has no way to call setKeeper() on the LyraPositionHandlerL2, the whole contract and its funds will be compromised as well. So, there needs to be some way to at least call the setKeeper() by something other than the keeper to ensure security of the funds on L2.  Examples  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L153-L184  function closePosition(bool toSettle) public override onlyAuthorized {  LyraController._closePosition(toSettle);  UniswapV3Controller._estimateAndSwap(  false,  LyraController.sUSD.balanceOf(address(this))  );  /*///////////////////////////////////////////////////////////////  MAINTAINANCE FUNCTIONS  //////////////////////////////////////////////////////////////*/  /// @notice Sweep tokens  /// @param _token Address of the token to sweepr  function sweep(address _token) public override onlyAuthorized {  IERC20(_token).transfer(  msg.sender,  IERC20(_token).balanceOf(address(this))  );  /// @notice socket registry setter  /// @param _socketRegistry new address of socket registry  function setSocketRegistry(address _socketRegistry) public onlyAuthorized {  socketRegistry = _socketRegistry;  /// @notice keeper setter  /// @param _keeper new keeper address  function setKeeper(address _keeper) public onlyAuthorized {  keeper = _keeper;  Recommendation  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.5 Harvester.harvest swaps have no slippage parameters ", "body": "  Description  As part of the vault strategy, all reward tokens for staking in the Convex ETH-stETH pool are claimed and swapped into ETH. The swaps for these tokens are done with no slippage at the moment, i.e. the expected output amount for all of them is given as 0.  In particular, one reward token that is most susceptible to slippage is LDO, and its swap is implemented through the Uniswap router:  code/contracts/ConvexExecutor/Harvester.sol:L142-L155  function _swapLidoForWETH(uint256 amountToSwap) internal {  IUniswapSwapRouter.ExactInputSingleParams  memory params = IUniswapSwapRouter.ExactInputSingleParams({  tokenIn: address(ldo),  tokenOut: address(weth),  fee: UNISWAP_FEE,  recipient: address(this),  deadline: block.timestamp,  amountIn: amountToSwap,  amountOutMinimum: 0,  sqrtPriceLimitX96: 0  });  uniswapRouter.exactInputSingle(params);  The swap is called with amountOutMinimum: 0, meaning that there is no slippage protection in this swap. This could result in a significant loss of yield from this reward as MEV bots could  sandwich  this swap by manipulating the price before this transaction and immediately reversing their action after the transaction, profiting at the expense of our swap. Moreover, the Uniswap pools seem to have low liquidity for the LDO token as opposed to Balancer or Sushiswap, further magnifying slippage issues and susceptibility to frontrunning.  The other two tokens - CVX and CRV - are being swapped through their Curve pools, which have higher liquidity and are less susceptible to slippage. Nonetheless, MEV strategies have been getting more advanced and calling these swaps with 0 as expected output may place these transactions in danger of being frontrun and  sandwiched  as well.  code/contracts/ConvexExecutor/Harvester.sol:L120-L126  if (cvxBalance > 0) {  cvxeth.exchange(1, 0, cvxBalance, 0, false);  // swap CRV to WETH  if (crvBalance > 0) {  crveth.exchange(1, 0, crvBalance, 0, false);  In these calls .exchange , the last 0 is the min_dy argument in the Curve pools swap functions that represents the minimum expected amount of tokens received after the swap, which is 0 in our case.  Recommendation  Introduce some slippage parameters into the swaps.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.6 Harvester.rewardTokens doesn t account for LDO tokens ", "body": "  Description  As part of the vault s strategy, the reward tokens for participating in Curve s ETH-stETH pool and Convex staking are claimed and swapped for ETH. This is done by having the ConvexPositionHandler contract call the reward claims API from Convex via baseRewardPool.getReward(), which transfers the reward tokens to the handler s address. Then, the tokens are iterated through and sent to the harvester to be swapped from ConvexPositionHandler by getting their list from harvester.rewardTokens() and calling harvester.harvest()  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L274-L290  // get list of tokens to transfer to harvester  address[] memory rewardTokens = harvester.rewardTokens();  //transfer them  uint256 balance;  for (uint256 i = 0; i < rewardTokens.length; i++) {  balance = IERC20(rewardTokens[i]).balanceOf(address(this));  if (balance > 0) {  IERC20(rewardTokens[i]).safeTransfer(  address(harvester),  balance  );  // convert all rewards to WETH  harvester.harvest();  However, harvester.rewardTokens() doesn t have the LDO token s address in its list, so they will not be transferred to the harvester to be swapped.  code/contracts/ConvexExecutor/Harvester.sol:L77-L82  function rewardTokens() external pure override returns (address[] memory) {  address[] memory rewards = new address[](2);  rewards[0] = address(crv);  rewards[1] = address(cvx);  return rewards;  As a result, harvester.harvest() will not be able to execute its _swapLidoForWETH() function since its ldoBalance will be 0. This results in missed rewards and therefore yield for the vault as part of its normal flow.  There is a possible mitigation in the current state of the contract that would require governance to call sweep() on the LDO balance from the BaseTradeExecutor contract (that ConvexPositionHandler inherits) and then transferring those LDO tokens to the harvester contract to perform the swap at a later rewards claim. This, however, requires transactions separate from the intended flow of the system as well as governance intervention.  Recommendation  Add the LDO token address to the rewardTokens() function by adding the following line rewards[2] = address(ldo);  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.7 Keeper design complexity ", "body": "  Description  The current design of the protocol relies on the keeper being operated correctly in a complex manner. Since the offchain code for the keeper wasn t in scope of this audit, the following is a commentary on the complexity of the keeper operations in the context of the contracts. Keeper logic such as the order of operations and function argument parameters with log querying are some examples where if the keeper doesn t execute them correctly, there may be inconsistencies and issues with accounting of vault shares and vault funds resulting in unexpected behaviour. While it may represent little risk or issues to the current Brahma-fi team as the vault is recently live, the keeper logic and exact steps should be well documented so that public keepers (if and when they are enabled) can execute the logic securely and future iterations of the vault code can account for any intricacies of the keeper logic.  Examples  1. Order of operations: Convex rewards & new depositors profiting at the expense of old depositors  yielded reward tokens. As part of the vault s strategy, the depositors  ETH is provided to Curve and the LP tokens are staked in Convex, which yield rewards such as CRV, CVX, and LDO tokens. As new depositors provide their ETH, the vault shares minted for their deposits will be less compared to old deposits as they account for the increasing value of LP tokens staked in these pools. In other words, if the first depositor provides 1 ETH, then when a new depositor provides 1 ETH much later, the new depositor will get less shares back as the totalVaultFunds() will increase:  code/contracts/Vault.sol:L97-L99  shares = totalSupply() > 0  ? (totalSupply() * amountIn) / totalVaultFunds()  : amountIn;  code/contracts/Vault.sol:L127-L130  function totalVaultFunds() public view returns (uint256) {  return  IERC20(wantToken).balanceOf(address(this)) + totalExecutorFunds();  code/contracts/ConvexTradeExecutor.sol:L21-L23  function totalFunds() public view override returns (uint256, uint256) {  return ConvexPositionHandler.positionInWantToken();  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137  function positionInWantToken()  public  view  override  returns (uint256, uint256)  uint256 stakedLpBalanceInETH,  uint256 lpBalanceInETH,  uint256 ethBalance  ) = _getTotalBalancesInETH(true);  return (  stakedLpBalanceInETH + lpBalanceInETH + ethBalance,  block.number  );  However, this does not account for the reward tokens yielded throughout that time. From the smart contract logic alone, there is no requirement to first execute the reward token harvest. It is up to the keeper to execute ConvexTradeExecutor.claimRewards in order to claim and swap their rewards into ETH, which only then will be included into the yield in the above ConvexPositionHandler.positionInWantToken function. If this is not done prior to processing new deposits and minting new shares, new depositors would unfairly benefit from the reward tokens  yield that was generated before they deposited but accounted for in the vault funds only after they deposited.  2. Order of operations: closing Lyra options before processing new deposits.  The other part of the vault s strategy is utilising the yield from Convex to purchase options from Lyra on Optimism. While Lyra options are risky and can become worthless in the event of bad trades, only yield is used for them, therefore keeping user deposits  initial value safe. However, their value could also yield significant returns, increasing the overall funds of the vault. Just as with ConvexTradeExecutor, LyraTradeExecutor also has a totalFunds() function that feeds into the vault s totalVaultFunds() function. In Lyra s case, however, it is a manually set value by the keeper that is supposed to represent the value of Lyra L2 options:  code/contracts/LyraTradeExecutor.sol:L42-L53  function totalFunds()  public  view  override  returns (uint256 posValue, uint256 lastUpdatedBlock)  return (  positionInWantToken.posValue +  IERC20(vaultWantToken()).balanceOf(address(this)),  positionInWantToken.lastUpdatedBlock  );  code/contracts/LyraTradeExecutor.sol:L61-L63  function setPosValue(uint256 _posValue) public onlyKeeper {  LyraPositionHandler._setPosValue(_posValue);  code/contracts/LyraExecutor/LyraPositionHandler.sol:L218-L221  function _setPosValue(uint256 _posValue) internal {  positionInWantToken.posValue = _posValue;  positionInWantToken.lastUpdatedBlock = block.number;  Solely from the smart contract logic, there is a possibility that a user deposits when Lyra options are valued high, meaning the total vault funds are high as well, thus decreasing the amount of shares the user would have received if it weren t for the Lyra options  value. Consequently, if after the deposit the Lyra options become worthless, decreasing the total vault funds, the user s newly minted shares will now represent less than what they have deposited.  While this is not currently mitigated by smart contract logic, it may be worked around by the keeper first settling and closing all Lyra options and transferring all their yielded value in ETH, if any, to the Convex trade executor. Only then the keeper would process new deposits and mint new shares. This order of operations is critical to maintain the vault s intended safe strategy of maintaining the user s deposited value, and is dependent entirely on the keeper offchain logic.  Recommendation  Document the exact order of operations, steps, necessary logs and parameters that keepers need to keep track of in order for the vault strategy to succeed.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.8 Vault.deposit - Possible front running attack ", "body": "  Description  To determine the number of shares to mint to a depositor, (totalSupply() * amountIn) / totalVaultFunds() is used. Potential attackers can spot a call to Vault.deposit and front-run it with a transaction that sends tokens to the contract, causing the victim to receive fewer shares than what he expected.  In case totalVaultFunds() is greater than totalSupply() * amountIn, then the number of shares the depositor receives will be 0, although amountIn of tokens will be still pulled from the depositor s balance.  An attacker with access to enough liquidity and to the mem-pool data can spot a call to Vault.deposit(amountIn, receiver) and front-run it by sending at least totalSupplyBefore * (amountIn - 1) + 1 tokens to the contract . This way, the victim will get 0 shares, but amountIn will still be pulled from its account balance. Now the price for a share is inflated, and all shareholders can redeem this profit using Vault.withdraw.  Recommendation  The specific case that s mentioned in the last paragraph can be mitigated by adding a validation check to Vault.Deposit enforcing that shares > 0. However, it will not solve the general case since the victim can still lose value due to rounding errors. In order to fix that, Vault.Deposit should validate that shares >= amountMin where amountMin is an argument that should be determined by the depositor off-chain.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.9 Approving MAX_UINT amount of ERC20 tokens ", "body": "  Description  Approving the maximum value of uint256 is a known practice to save gas. However, this pattern was proven to increase the impact of an attack many times in the past, in case the approved contract gets hacked.  Examples  code/contracts/BaseTradeExecutor.sol:L19  IERC20(vaultWantToken()).approve(vault, MAX_INT);  code/contracts/Batcher/Batcher.sol:L48  IERC20(vaultInfo.tokenAddress).approve(vaultAddress, type(uint256).max);  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L106-L112  IERC20(LP_TOKEN).safeApprove(ETH_STETH_POOL, type(uint256).max);  // Approve max LP tokens to convex booster  IERC20(LP_TOKEN).safeApprove(  address(CONVEX_BOOSTER),  type(uint256).max  );  code/contracts/ConvexExecutor/Harvester.sol:L65-L69  crv.safeApprove(address(crveth), type(uint256).max);  // max approve CVX to CVX/ETH pool on curve  cvx.safeApprove(address(cvxeth), type(uint256).max);  // max approve LDO to uniswap swap router  ldo.safeApprove(address(uniswapRouter), type(uint256).max);  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L63-L71  IERC20(wantTokenL2).safeApprove(  address(UniswapV3Controller.uniswapRouter),  type(uint256).max  );  // approve max susd balance to uniV3 router  LyraController.sUSD.safeApprove(  address(UniswapV3Controller.uniswapRouter),  type(uint256).max  );  Recommendation  Consider approving the exact amount that s needed to be transferred, or alternatively, add an external function that allows the revocation of approvals.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.10 Batcher.depositFunds may allow for more deposits than vaultInfo.maxAmount ", "body": "  Description  As part of a gradual rollout strategy, the Brahma-fi system of contracts has a limit of how much can be deposited into the protocol. This is implemented through the Batcher contract that allows users to deposit into it and keep the amount they have deposited in the depositLedger[recipient] state variable. In order to cap how much is deposited, the user s input amountIn is evaluated within the following statement:  code/contracts/Batcher/Batcher.sol:L109-L116  require(  IERC20(vaultInfo.vaultAddress).totalSupply() +  pendingDeposit -  pendingWithdrawal +  amountIn <=  vaultInfo.maxAmount,  \"MAX_LIMIT_EXCEEDED\"  );  However, while pendingDeposit, amountIn, and vaultInfo.maxAmount are denominated in the vault asset token (WETH in our case), IERC20(vaultInfo.vaultAddress).totalSupply() and pendingWithdrawal represent vault shares tokens, creating potential mismatches in this evaluation.  Recommendation  Consider either documenting this potential discrepancy or keeping track of all deposits in a state variable and using that inside the require statement..  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.11 The Deposit and Withdraw event are always emitted with zero amount ", "body": "  Description  The events emitted during the deposit or withdraw are supposed to contain the relevant amounts of tokens involved in these actions. But in fact the current balance of the address is used in both cases. These balances will be equal to zero by that time:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L151-L155  IWETH9(address(wantToken)).withdraw(depositParams._amount);  _convertEthIntoLpToken(address(this).balance);  emit Deposit(address(this).balance);  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L207-L209  IWETH9(address(wantToken)).deposit{value: address(this).balance}();  emit Withdraw(address(this).balance);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.12 BaseTradeExecutor.confirmDeposit | confirmWithdraw - Violation of the  checks-effects-interactions  pattern ", "body": "  Description  Both confirmDeposit, confirmWithdraw might be re-entered by the keeper (in case it is a contract), in case the derived contract allows the execution of untrusted code.  Examples  code/contracts/BaseTradeExecutor.sol:L57-L61  function confirmDeposit() public override onlyKeeper {  require(depositStatus.inProcess, \"DEPOSIT_COMPLETED\");  _confirmDeposit();  depositStatus.inProcess = false;  code/contracts/BaseTradeExecutor.sol:L69-L73  function confirmWithdraw() public override onlyKeeper {  require(withdrawalStatus.inProcess, \"WIHDRW_COMPLETED\");  _confirmWithdraw();  withdrawalStatus.inProcess = false;  Recommendation  Although the impact is very limited, it is recommended to implement the  checks-effects-interactions  in both functions.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.13 Batcher doesn t work properly with arbitrary tokens ", "body": "  Description  The Batcher and the Vault contracts initially operate with ETH and WETH. But the contracts are supposed to be compatible with any other ERC-20 tokens.  For example, in the Batcher.deposit function, there is an option to transfer ETH instead of the token, which should only be happening if the token is WETH. Also, the token is named WETH, but if the intention is to use the Batcher contract with arbitrary tokens token, it should be named differently.  code/contracts/Batcher/Batcher.sol:L89-L100  if (ethSent > 0) {  amountIn = ethSent;  WETH.deposit{value: ethSent}();  /// If no wei sent, use amountIn and transfer WETH from txn sender  else {  IERC20(vaultInfo.tokenAddress).safeTransferFrom(  msg.sender,  address(this),  amountIn  );  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "6.1 EOPBCTemplate - permission documentation inconsistencies    ", "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by adding the undocumented and deviating permissions to the documentation.  Description  Undocumented  The template documentation provides an overview of the permissions set with the template. The following permissions are set by the template contract but are not documented in the accompanied fundraising/templates/externally_owned_presale_bonding_curve/README.md.  TokenManager  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L220-L221  _createPermissions(_acl, grantees, _fundraisingApps.bondedTokenManager, _fundraisingApps.bondedTokenManager.MINT_ROLE(), _owner);  _acl.createPermission(_fundraisingApps.marketMaker, _fundraisingApps.bondedTokenManager, _fundraisingApps.bondedTokenManager.BURN_ROLE(), _owner);  code/fundraising/templates/externally_owned_presale_bonding_curve/eopbc.yaml:L33-L44  Inconsistent  app: anj-token-manager  role: MINT_ROLE  grantee: market-maker  manager: owner  app: anj-token-manager  role: MINT_ROLE  grantee: presale  manager: owner  app: anj-token-manager  role: BURN_ROLE  grantee: market-maker  manager: owner  Inconsistent  The following permissions are set by the template but are inconsistent to the outline in the documentation:  Controller  owner has the following permissions even though they are documented as not being set https://github.com/ConsenSys/aragonone-presale-audit-2019-11/blob/9ddae8c7fde9dea3af3982b965a441239d81f370/code/fundraising/templates/externally_owned_presale_bonding_curve/README.md#controller.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L239-L240  _acl.createPermission(_owner, _fundraisingApps.controller, _fundraisingApps.controller.UPDATE_BENEFICIARY_ROLE(), _owner);  _acl.createPermission(_owner, _fundraisingApps.controller, _fundraisingApps.controller.UPDATE_FEES_ROLE(), _owner);  Recommendation  For transparency, all permissions set-up by the template must be documented.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.2 EOPBCTemplate - AppId of BalanceRedirectPresale should be different from AragonBlack/Presale namehash to avoid collisions    ", "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by generating a unique APMNameHash for  Description  The template references the new presale contract with apmNamehash 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5. However, this namehash is already used by the aragonBlack/Presale contract. To avoid confusion and collision a unique apmNamehash should be used for this variant of the contract.  Note that the contract that is referenced from an apmNamehash is controlled by the ENS resolver that is configured when deploying the template contract. Using the same namehash for both variants of the contract does not allow a single registry to simultaneously provide both variants of the contract and might lead to confusion as to which application is actually deployed. This also raises the issue that the ENS registry must be verified before actually using the contract as a malicious registry could force the template to deploy potentially malicious applications.  aragonOne/Fundraising:  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L32  bytes32   private constant PRESALE_ID                    = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  aragonBlack/Fundraising:  templates/multisig/contracts/FundraisingMultisigTemplate.sol:L35  bytes32   private constant PRESALE_ID             = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  bytes32 private constant PRESALE_ID = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  Recommendation  Create a new apmNamehash for BalanceRedirectPresale.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.3 BalanceRedirectPresale - Presale can be extended indefinitely   ", "body": "  Resolution  This issue was addressed with the following statement:  It is a very reasonable concern, but this is the intended behavior. That modification is permissioned and that OPEN_ROLE is going to be held by the Aragon Network Dao, so we expect a reasonable use of it. We may document it and make it clear that this is possible.  Description  The OPEN_ROLE can indefinitely extend the Presale even after users contributed funds to it by adjusting the presale period. The period might be further manipulated to avoid that token trading in the MarketMaker is opened.  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L136-L138  function setPeriod(uint64 _period) external auth(OPEN_ROLE) {  _setPeriod(_period);  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257  function _setPeriod(uint64 _period) internal {  require(_period > 0, ERROR_TIME_PERIOD_ZERO);  require(openDate == 0 || openDate + _period > getTimestamp64(), ERROR_INVALID_TIME_PERIOD);  period = _period;  Recommendation  Do not allow to extend the presale after funds have been contributed to it or only allow period adjustments in State.PENDING.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.4 Repository structure - Create a clean repository containing one Aragon Application unless changes are contributed upstream    Deferred", "body": "  Resolution  The issue has been deferred pending internal discussion.  Description  The repository is a fork of AragonBlack/fundraising. The main development repository for Aragon Fundraising is the origin repository at AragonBlock. This repository duplicates a state of the upstream repository that can quickly get out of sync and therefore hard to maintain.  It is unclear if both repositories will live side-by-side or if the BalanceRedirectPresale variant is contributed upstream.  Recommendation  In case changes are not planned to be contributed upstream it is recommended to create a clean Aragon Application from scratch removing any unused or duplicated files.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.5 BalanceRedirectPresale - Tokens vest during the Presale phase   ", "body": "  Resolution  The issue was addressed with the following statement:  This presale version is intended to be used along with the Externally Owned Presale and Bonding Curve Template, which doesn t have a Voting app, therefore contributors doesn t have any voting power. The use case is the deployment of Aragon Network Jurors Token (ANJ) for the Aragon Court, which is not going to be active before the presale starts, so we don t see any potential issue here.  Description  Tokens are directly minted and assigned to contributors during the Presale. While this might not be an issue if the minted token does not give any voting power of some sort in a DAO it can be a problem for scenarios where contributors get stake in return for contributions.  Recommendation  Vest tokens for contributors after the presale finishes. In case this is the expected we suggest to add a note to the documentation to make potential users aware of this behaviour that might have security implications if contributors get stake in return for their investments.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.6 BalanceRedirectPresale - setPeriod uint64 overflow in validation check    ", "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by performing the addition using  Description  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257  function _setPeriod(uint64 _period) internal {  require(_period > 0, ERROR_TIME_PERIOD_ZERO);  require(openDate == 0 || openDate + _period > getTimestamp64(), ERROR_INVALID_TIME_PERIOD);  period = _period;  Recommendation  Use SafeMath which is already imported to protect from overflow scenarios.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.7 EOPBCTemplate - misleading method names _cacheFundraisingApps and _cacheFundraisingParams    ", "body": "  Resolution   Fixed with   aragonone/fundraising@0ce7c72 by renaming the functions.  Description  The methods _cacheFundraisingApps and _cacheFundraisingParams suggest that parameters are cached as state variables in the contract similar to the multi-step deployment contract used for AragonBlack/Fundraising. However, the methods are just returning memory structs.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L254-L300  function _cacheFundraisingApps(  Agent          _reserve,  Presale        _presale,  MarketMaker    _marketMaker,  Tap            _tap,  Controller     _controller,  TokenManager   _tokenManager  internal  returns (FundraisingApps memory fundraisingApps)  fundraisingApps.reserve            = _reserve;  fundraisingApps.presale            = _presale;  fundraisingApps.marketMaker        = _marketMaker;  fundraisingApps.tap                = _tap;  fundraisingApps.controller         = _controller;  fundraisingApps.bondedTokenManager = _tokenManager;  function _cacheFundraisingParams(  address       _owner,  string        _id,  ERC20         _collateralToken,  MiniMeToken   _bondedToken,  uint64        _period,  uint256       _exchangeRate,  uint64        _openDate,  uint256       _reserveRatio,  uint256       _batchBlocks,  uint256       _slippage  internal  returns (FundraisingParams fundraisingParams)  fundraisingParams = FundraisingParams({  owner:           _owner,  id:              _id,  collateralToken: _collateralToken,  bondedToken:     _bondedToken,  period:          _period,  exchangeRate:    _exchangeRate,  openDate:        _openDate,  reserveRatio:    _reserveRatio,  batchBlocks:     _batchBlocks,  slippage:        _slippage  });  Recommendation  The functions are only called once throughout the deployment process. The structs can therefore be created directly in the main method. Otherwise rename the functions to properly reflect their purpose.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.8 EOPBCTemplate - Pool should be Agent or Reserve    ", "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by replacing  Description  The documentation refers to an non-existent Pool application.  code/fundraising/templates/externally_owned_presale_bonding_curve/README.md:L58-L68  | App  | Permission             | Grantee          | Manager          |  | ---- | ---------------------- | ---------------- | ---------------- |  | Pool | SAFE_EXECUTE           | Owner            | Owner            |  | Pool | ADD_PROTECTED_TOKEN    | Controller       | Owner            |  | Pool | REMOVE_PROTECTED_TOKEN | NULL             | NULL             |  | Pool | EXECUTE                | NULL             | NULL             |  | Pool | DESIGNATE_SIGNER       | NULL             | NULL             |  | Pool | ADD_PRESIGNED_HASH     | NULL             | NULL             |  | Pool | RUN_SCRIPT             | NULL             | NULL             |  | Pool | TRANSFER               | MarketMaker      | Owner            |  Recommendation  Pool should be Agent or Reserve.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.9 EOPBCTemplate - inconsistent storage location declaration    ", "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by adding the missing storage location declaration.  Description  _cacheFundraisingParams() does not explicitly declare the return value memory location.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L273-L286  function _cacheFundraisingParams(  address       _owner,  string        _id,  ERC20         _collateralToken,  MiniMeToken   _bondedToken,  uint64        _period,  uint256       _exchangeRate,  uint64        _openDate,  uint256       _reserveRatio,  uint256       _batchBlocks,  uint256       _slippage  internal  returns (FundraisingParams fundraisingParams)  _cacheFundraisingApps() explicitly declares to return a copy of the storage struct.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L254-L271  function _cacheFundraisingApps(  Agent          _reserve,  Presale        _presale,  MarketMaker    _marketMaker,  Tap            _tap,  Controller     _controller,  TokenManager   _tokenManager  internal  returns (FundraisingApps memory fundraisingApps)  fundraisingApps.reserve            = _reserve;  fundraisingApps.presale            = _presale;  fundraisingApps.marketMaker        = _marketMaker;  fundraisingApps.tap                = _tap;  fundraisingApps.controller         = _controller;  fundraisingApps.bondedTokenManager = _tokenManager;  Recommendation  Storage declarations should be consistent.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.10 EOPBCTemplate - Keep the template as closely aligned to the audited Company DAO-Template provided by Aragon    ", "body": "  Resolution   The issue was addressed with   aragonone/fundraising@bafe100 changing the main deployment method from  Description  The EOPBCTemplate is a simplified variant of the AragonBlack/FundraisingMultisigTemplate. The FundraisingMultisigTemplate is initially based on the Aragon/DAO-templates/company-board template.  Please note that the DAO-templates provided by Aragon have recently been audited.  The EOPBCTemplate is similar to the setup established with Aragon/DAO-templates/company. The scenario deploys in one step. However, interface names are different to the audited DAO-template variant (installFundraisingApps vs newInstance). We recommend the template and interface names to be kept as close as possible to the audited company template which established the entry point for deploying a one-step template as newInstance.  Recommendation  Take the Aragon/DAO-templates/company template as a starting point and add relevant parts for the presale variant.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.11 EOPBCTemplate - EtherTokenConstant is never used    ", "body": "  Resolution   Fixed with   aragonone/fundraising@bafe100 by removing the  Description  The constant value EtherTokenConstant.ETH is never used.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L3  import \"@aragon/os/contracts/common/EtherTokenConstant.sol\";  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L21  contract EOPBCTemplate is EtherTokenConstant, BaseTemplate {  Recommendation  Remove all references to EtherTokenConstant.  7 Tool-Based Analysis  Several tools were used to perform an automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open-source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium --version  Solium version 1.2.5  $ solium -d contracts  contracts/EOPBCTemplate.sol  118:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  208:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  221:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  224:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  231:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  232:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  233:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  234:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  235:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  236:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  237:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  265:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  266:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  267:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  268:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  269:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  289:13    warning    Name 'owner': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.           whitespace  290:13    warning    Name 'id': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.              whitespace  292:13    warning    Name 'bondedToken': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.     whitespace  293:13    warning    Name 'period': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.          whitespace  294:13    warning    Name 'exchangeRate': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.    whitespace  295:13    warning    Name 'openDate': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.        whitespace  296:13    warning    Name 'reserveRatio': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.    whitespace  297:13    warning    Name 'batchBlocks': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.     whitespace  298:13    warning    Name 'slippage': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.        whitespace  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "7.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers (please use horizontal scroll to view all columns):  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "4.1 ERC1400ERC20 whitelist circumvents partition restrictions    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#13.  Description  ERC1400/1410 enable  partially fungible tokens  in that not all tokens are equivalent. A specific use case is placing restrictions on some tokens, such as lock-up periods.  The whitelist in ERC1400ERC20 circumvents these restrictions. When a token holder uses the ERC20 transfer function, tokens are transferred from that user s  default partitions , which a user can choose themselves by calling ERC1410.setDefaultPartitions. This means they can transfer tokens from any partition, and the only restriction that s placed on the transfer is that the recipient must be whitelisted.  It should be noted that the comment and error message around the whitelisting feature suggests that it is meant to be applied to both the sender and recipient:  code/contracts/token/ERC20/ERC1400ERC20.sol:L24-L30  /**  @dev Modifier to verify if sender and recipient are whitelisted.  /  modifier isWhitelisted(address recipient) {  require(_whitelisted[recipient], \"A3: Transfer Blocked - Sender lockup period not ended\");  _;  Remediation  There are many possibilities, but here are concrete suggestions for addressing this:  Require whitelisting both the sender and recipient, and make sure that whitelisted accounts only own (and will only own) unrestricted tokens.  Make sure that the only whitelisted recipients are those that apply partition restrictions when receiving tokens. (I.e. they implement the modified ERC777 receiving hook, examine the source partition, and reject transfers that should not occur.)  Instead of implementing the ERC20 interface on top of the ERC1400 token, support transferring out of the ERC1400 token and into a standard ERC20 token. Partition restrictions can then be applied on the ERC1400 transfer, and once ERC20 tokens are obtained, they can be transferred without restriction.  Don t allow token holders to set their own default partitions. Rather, have the token specify a single, unrestricted partition that is used for all ERC20 transfers.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.2 Certificate controllers do not always constrain the last argument    ", "body": "  Resolution   The existing back end already does its own ABI encoding, which means it s not vulnerable to this issue. Documentation has been added in   https://gitlab.com/ConsenSys/client/fr/dauriel/smart-contracts/certificate-controller/merge_requests/9 to ensure future maintainers understand this potential issue.  Description  The certificate controllers (CertificateControllerNonce and CertificateControllerSalt) are used by passing a signature as a final argument in a function call. This signature is over the other arguments to the function. Specifically, the signature must match the call data that precedes the signature.  The way this is implemented assumes standard ABI encoding of parameters, but there s actually some room for manipulation by a malicious user. This manipulation can allow the user to change some of the call data without invalidating the signature.  The following code is from CertificateControllerNonce, but similar logic applies to CertificateControllerSalt:  code2/contracts/CertificateControllerNonce.sol:L127-L134  bytes memory payload;  assembly {  let payloadsize := sub(calldatasize, 160)  payload := mload(0x40) // allocate new memory  mstore(0x40, add(payload, and(add(add(payloadsize, 0x20), 0x1f), not(0x1f)))) // boolean trick for padding to 0x40  mstore(payload, payloadsize) // set length  calldatacopy(add(add(payload, 0x20), 4), 4, sub(payloadsize, 4))  Here the signature is over all call data except the final 160 bytes. 160 bytes makes sense because the byte array is length 97, and it s preceded by a 32-byte size. This is a total of 129 bytes, and typical ABI encoded pads this to the next multiple of 32, which is 160.  If an attacker does not pad their arguments, they can use just 129 bytes for the signature or even 128 bytes if the v value happens to be 0. This means that when checking the signature, not only will the signature be excluded, but also the 31 or 32 bytes that come before the signature. This means the attacker can call a function with a different final argument than the one that was signed.  That final argument is, in many cases, the number of tokens to transfer, redeem, or issue.  Mitigating factors  For this to be exploitable, the attacker has to be able to obtain a signature over shortened call data.  If the signer accepts raw arguments and does its own ABI encoding with standard padding, then there s likely no opportunity for an attacker to exploit this vulnerability. (They can shorten the call data length when they make the function call later, but the signature won t match.)  Remediation  We have two suggestions for how to address this:  Instead of signatures being checked directly against call data, compute a new hash based on the decoded values, e.g. keccak256(abi.encode(argument1, argument2, ...)).  Address this at the signing layer (off chain) by doing the ABI encoding there and denying an attacker the opportunity to construct their own call data.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.3 Salt-based certificate controller is subject to signature replay    ", "body": "  Resolution   This is fixed in   https://gitlab.com/ConsenSys/client/fr/dauriel/smart-contracts/certificate-controller/merge_requests/8.  Description  The salt-based certificate controller prevents signature replay by storing each full signature. Only a signature that is exactly identical to a previously-used signature will be rejected.  For ECDSA signatures, each signature has a second S value (and flipped V to match) that will recover the same address. An attacker can produce such a second signature trivially without knowing the signer s private key. This gives an attacker a way to produce a new unique signature based on a previously used one. This effectively means every signature can be used twice.  code2/contracts/CertificateControllerSalt.sol:L25-L32  modifier isValidCertificate(bytes memory data) {  require(  _certificateSigners[msg.sender] || _checkCertificate(data, 0, 0x00000000),  \"A3: Transfer Blocked - Sender lockup period not ended\"  );  _usedCertificate[data] = true; // Use certificate  References  See https://smartcontractsecurity.github.io/SWC-registry/docs/SWC-117.  Remediation  Instead of rejecting used signatures based on the full signature value, keep track of used salts (which are then better referred to as  nonces ).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.4 EIP-1400 is missing canTransfer* functions   ", "body": "  Description  The EIP-1400 states defines the interface to be implemented containing the 3 functions:  // Transfer Validity  function canTransfer(address _to, uint256 _value, bytes _data) external view returns (byte, bytes32);  function canTransferFrom(address _from, address _to, uint256 _value, bytes _data) external view returns (byte, bytes32);  function canTransferByPartition(address _from, address _to, bytes32 _partition, uint256 _value, bytes _data) external view returns (byte, bytes32, bytes32);  These functions were not implemented in ERC1400, thus making the implementation not completely compatible with EIP-1400.  In case the deployed contract needs to be added as a  lego block  part of a another application, there is a high chance that it will not correctly function. That external application could potentially call the EIP-1400 functions canTransfer, canTransferFrom or canTransferByPartition, in which case the transaction will likely fail.  This means that the current implementation will not be able to become part of external markets, exchanges or applications that need to interact with a generic EIP-1400 implementation.  Remediation  Even if the functions do not correctly reflect the transfer possibility, their omission can break other contracts interacting with the implementation.  A suggestion would be to add these functions and make them always return true. This way the contracts interacting with the current implementation do not break when they call these functions, while the actual transfer of the tokens is still limited by the current logic.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.5 ERC777 incompatibilities    ", "body": "  Resolution   This is fixed in   Description  As noted in the README, the ERC777 contract is not actually compatible with ERC 777.  Functions and events have been renamed, and the hooks ERC777TokensRecipient and ERC777TokensSender have been modified to add a partition parameter.  This means no tools that deal with standard ERC 777 contracts will work with this code s tokens.  Remediation  We suggest renaming these contracts to not use the term  ERC777 , as they lack compatibility. Most importantly, we recommend not using the interface names  ERC777TokensRecipient  and  ERC777TokensSender  when looking up the appropriate hook contracts via ERC 1820. Contracts that handle that interface will not be capable of handling the modified interface used here.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.6 Buffer over-read in ERC1410._getDestinationPartition    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#16.  Description  There s no check that data is at least 64 bytes long, so the following code can read past the end of data:  code/contracts/token/ERC1410/ERC1410.sol:L348-L361  function _getDestinationPartition(bytes32 fromPartition, bytes memory data) internal pure returns(bytes32 toPartition) {  bytes32 changePartitionFlag = 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff;  bytes32 flag;  assembly {  flag := mload(add(data, 32))  if(flag == changePartitionFlag) {  assembly {  toPartition := mload(add(data, 64))  } else {  toPartition = fromPartition;  The only caller is _transferByPartition, which only checks that data.length > 0:  code/contracts/token/ERC1410/ERC1410.sol:L263-L264  if(operatorData.length != 0 && data.length != 0) {  toPartition = _getDestinationPartition(fromPartition, data);  Depending on how the compiler chooses to lay out memory, the next data in memory is probably the operatorData buffer, so data may inadvertently be read from there.  Remediation  Check for sufficient length (at least 64 bytes) before attempting to read it.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.7 ERC20/ERC777 compatibility: ERC20 transfer functions should not revert if the recipient is a contract without a registered ERC777TokensRecipient implementation    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#17.  Description  This will block transfers to a contract that doesn t have an ERC777TokensRecipient implementation. This is in violation of ERC 777, which says:  If the recipient is a contract, which has not registered an ERC777TokensRecipient implementation; then the token contract:  MUST revert if the tokensReceived hook is called from a mint or send call.  SHOULD continue processing the transaction if the tokensReceived hook is called from an ERC20 transfer or transferFrom call.  Remediation  Make sure that ERC20-compatible transfer calls do not set preventLocking to true.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.8 ERC777 compatibility: authorizeOperator and revokeOperator should revert when the caller and operator are the same account    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#19.  Description  From ERC 777:  The autohrizeOperator implementation does not do that:  code/contracts/token/ERC777/ERC777.sol:L144-L147  function authorizeOperator(address operator) external {  _authorizedOperator[operator][msg.sender] = true;  emit AuthorizedOperator(operator, msg.sender);  The same holds for revokeOperator:  code/contracts/token/ERC777/ERC777.sol:L155-L158  function revokeOperator(address operator) external {  _authorizedOperator[operator][msg.sender] = false;  emit RevokedOperator(operator, msg.sender);  Remediation  Add require(operator != msg.sender) to those two functions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.9 Token receiver can mint gas tokens with sender s gas   ", "body": "  Description  When a transfer is executed, there are hooks activated on the sender s and on the receiver s side.  This is possible because the contract implements ERC1820Client which allows any address to define an implementation:  contracts/ERC1820Client.sol:L16-L19  function setInterfaceImplementation(string memory _interfaceLabel, address _implementation) internal {  bytes32 interfaceHash = keccak256(abi.encodePacked(_interfaceLabel));  ERC1820REGISTRY.setInterfaceImplementer(address(this), interfaceHash, _implementation);  Considering the receiver s side:  contracts/ERC1400.sol:L1016-L1020  recipientImplementation = interfaceAddr(to, ERC1400_TOKENS_RECIPIENT);  if (recipientImplementation != address(0)) {  IERC1400TokensRecipient(recipientImplementation).tokensReceived(msg.sig, partition, operator, from, to, value, data, operatorData);  The sender has to pay for the gas for the transaction to go through.  Because the receiver can define a contract to be called when receiving the tokens, and the sender has to pay for the gas, the receiver can mint gas tokens (or waste the gas).  Remediation  Because this is the way Ethereum works and the implementation allows calling external methods, there s no recommended remediation for this issue. It s just something the senders need to be aware of.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.10 Missing ERC Functions    ", "body": "  Resolution  This is fixed in https://github.com/ConsenSys/ERC1400/pull/18.  Description  There exist some functions, such as isOperator() ,that are part of the ERC1410 spec. Removing functions expected by ERC may break things like block explorers that expect to be able to query standard contracts for relevant metadata.  Remediation  It would be good to explicitly state any expected incompatibilities.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.11 Inaccurate error message in ERC777ERC20.approve    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#20.  Description  If the spender is address 0, the revert message says that the receiver is not eligible.  code/contracts/token/ERC20/ERC777ERC20.sol:L153  require(spender != address(0), \"A6: Transfer Blocked - Receiver not eligible\");  Remediation  Fix the revert message to match the actual issue.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.12 Non-standard treatment of a from address of 0    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#21.  Description  A number of functions throughout the system treat a from address of 0 as equivalent to msg.sender. In some cases, this seems to violate existing standards (e.g. in ERC20 transfers). In other cases, it is merely surprising.  ERC1400ERC20.transferFrom and ERC777ERC20.transferFrom both treat a from address as 0 as equivalent to msg.sender. This is unexpected behavior for an ERC20 token.  Examples  code/contracts/ERC1400.sol:L206-L214  function canOperatorTransferByPartition(bytes32 partition, address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  view  returns (byte, bytes32, bytes32)  if(!_checkCertificate(operatorData, 0, 0x8c0dee9c)) { // 4 first bytes of keccak256(operatorTransferByPartition(bytes32,address,address,uint256,bytes,bytes))  return(hex\"A3\", \"\", partition); // Transfer Blocked - Sender lockup period not ended  } else {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/ERC1400.sol:L417-L421  function redeemFrom(address from, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC20/ERC1400ERC20.sol:L180-L181  function transferFrom(address from, address to, uint256 value) external isWhitelisted(to) returns (bool) {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC20/ERC777ERC20.sol:L179-L180  function transferFrom(address from, address to, uint256 value) external isWhitelisted(to) returns (bool) {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC777/ERC777.sol:L194-L198  function transferFromWithData(address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC777/ERC777.sol:L226-L230  function redeemFrom(address from, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC1410/ERC1410.sol:L130-L142  function operatorTransferByPartition(  bytes32 partition,  address from,  address to,  uint256 value,  bytes calldata data,  bytes calldata operatorData  external  isValidCertificate(operatorData)  returns (bytes32)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC1410/ERC1410.sol:L430-L434  function transferFromWithData(address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  Remediation  Remove this fallback logic and always use the from address that was passed in. This avoids surprises where, for example, an uninitialized value leads to loss of funds.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.13 ERC1410 s redeem and redeemFrom should revert    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#22.  Description  ERC1410 contains two functions: redeem and redeemFrom that  erase  the underlying ERC777 versions of these functions because those functions don t handle partitions.  These functions silently succeed, while they should probably fail by reverting.  Examples  code/contracts/token/ERC1410/ERC1410.sol:L441-L453  /**  [NOT MANDATORY FOR ERC1410 STANDARD][OVERRIDES ERC777 METHOD]  @dev Empty function to erase ERC777 redeem() function since it doesn't handle partitions.  /  function redeem(uint256 /*value*/, bytes calldata /*data*/) external { // Comments to avoid compilation warnings for unused variables.  /**  [NOT MANDATORY FOR ERC1410 STANDARD][OVERRIDES ERC777 METHOD]  @dev Empty function to erase ERC777 redeemFrom() function since it doesn't handle partitions.  /  function redeemFrom(address /*from*/, uint256 /*value*/, bytes calldata /*data*/, bytes calldata /*operatorData*/) external { // Comments to avoid compilation warnings for unused variables.  Remediation  Add a revert() (possibly with a reason) so callers know that the call failed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.14 Unclear why operatorData.length is checked in _transferByPartition    ", "body": "  Resolution   This code is actually correct. When   Description  It s unclear why operatorData.length is being checked here:  code/contracts/token/ERC1410/ERC1410.sol:L263-L264  if(operatorData.length != 0 && data.length != 0) {  toPartition = _getDestinationPartition(fromPartition, data);  Remediation  Consider removing that check.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.15 Global partition enumeration can run into gas limits    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#25.  Description  In ERC1410, partitions are created on demand by issuing or transferring tokens, and these new partitions are added to the array _totalPartitions. When one of these partitions is later emptied, it s removed from that array with the following code in _removeTokenFromPartition:  code/contracts/token/ERC1410/ERC1410.sol:L303-L313  // If the total supply is zero, finds and deletes the partition.  if(_totalSupplyByPartition[partition] == 0) {  for (uint i = 0; i < _totalPartitions.length; i++) {  if(_totalPartitions[i] == partition) {  _totalPartitions[i] = _totalPartitions[_totalPartitions.length - 1];  delete _totalPartitions[_totalPartitions.length - 1];  _totalPartitions.length--;  break;  Finding the partition requires iterating over the entire array. This means that _removeTokenFromPartition can become very expensive and eventually bump up against the block gas limit if lots of partitions are created. This could be an attack vector for a malicious operator.  The same issue applies to a token holder s list of partitions, where transferring tokens in a large number of partitions to that token holder may block them from being able to transfer tokens out:  code/contracts/token/ERC1410/ERC1410.sol:L291-L301  // If the balance of the TokenHolder's partition is zero, finds and deletes the partition.  if(_balanceOfByPartition[from][partition] == 0) {  for (uint i = 0; i < _partitionsOf[from].length; i++) {  if(_partitionsOf[from][i] == partition) {  _partitionsOf[from][i] = _partitionsOf[from][_partitionsOf[from].length - 1];  delete _partitionsOf[from][_partitionsOf[from].length - 1];  _partitionsOf[from].length--;  break;  Remediation  Removing an item from a set can be accomplished in constant time if the set uses both an array (for storing the values) and a mapping of values to their index in that array. See https://programtheblockchain.com/posts/2018/06/03/storage-patterns-set/ for one example of doing this.  It also may be reasonable to cap the number of possible partitions or lock them down to a constant set of values on deployment, depending on the use case for the token.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.16 Optimization: redundant delete in ERC1400. _removeTokenFromPartition    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#23.  Description  Reducing the size of an array automatically deletes the removed elements, so the first of these two lines is redundant:  code/contracts/token/ERC1410/ERC1410.sol:L296-L297  delete _partitionsOf[from][_partitionsOf[from].length - 1];  _partitionsOf[from].length--;  The same applies here:  code/contracts/token/ERC1410/ERC1410.sol:L308-L309  delete _totalPartitions[_totalPartitions.length - 1];  _totalPartitions.length--;  Remediation  Remove the redundant deletions to save a little gas.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.17 Avoid hardcoding function selectors    ", "body": "  Resolution   This is fixed in   ConsenSys/ERC1400#24.  Description  In ERC1400, hardcoded function selectors can be replaced with this.transferByPartition.selector and this.operatorTransferByPartition.selector.  Examples  code/contracts/ERC1400.sol:L184  if(!_checkCertificate(data, 0, 0xf3d490db)) { // 4 first bytes of keccak256(transferByPartition(bytes32,address,uint256,bytes))  code/contracts/ERC1400.sol:L211  if(!_checkCertificate(operatorData, 0, 0x8c0dee9c)) { // 4 first bytes of keccak256(operatorTransferByPartition(bytes32,address,address,uint256,bytes,bytes))  Remediation  Replace the hardcoded function selectors with this.<method>.selector.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "3.1 Yearn: Re-entrancy attack during deposit ", "body": "  Description  During the deposit in the supplyTokenTo function, the token transfer is happening after the shares are minted and before tokens are deposited to the yearn vault:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128  function supplyTokenTo(uint256 _amount, address to) override external {  uint256 shares = _tokenToShares(_amount);  _mint(to, shares);  // NOTE: we have to deposit after calculating shares to mint  token.safeTransferFrom(msg.sender, address(this), _amount);  _depositInVault();  emit SuppliedTokenTo(msg.sender, shares, _amount, to);  If the token allows the re-entrancy (e.g., ERC-777), the attacker can do one more transaction during the token transfer and call the supplyTokenTo function again. This second call will be done with already modified shares from the first deposit but non-modified token balances. That will lead to an increased amount of shares minted during the supplyTokenTo. By using that technique, it s possible to steal funds from other users of the contract.  Recommendation  Have the re-entrancy guard on all the external functions.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.2 Yearn: Partial deposits are not processed properly ", "body": "  Description  The deposit is usually made with all the token balance of the contract:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L171-L172  // this will deposit full balance (for cases like not enough room in Vault)  return v.deposit();  The Yearn vault contract has a limit of how many tokens can be deposited there. If the deposit hits the limit, only part of the tokens is deposited (not to exceed the limit). That case is not handled properly, the shares are minted as if all the tokens are accepted, and the  change  is not transferred back to the caller:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128  function supplyTokenTo(uint256 _amount, address to) override external {  uint256 shares = _tokenToShares(_amount);  _mint(to, shares);  // NOTE: we have to deposit after calculating shares to mint  token.safeTransferFrom(msg.sender, address(this), _amount);  _depositInVault();  emit SuppliedTokenTo(msg.sender, shares, _amount, to);  Recommendation  Handle the edge cases properly.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.3 Sushi: redeemToken redeems less than it should ", "body": "  Description  The redeemToken function takes as argument the amount of SUSHI to redeem. Because the SushiBar s leave function   which has to be called to achieve this goal   takes an amount of xSUSHI that is to be burned in exchange for SUSHI, redeemToken has to compute the amount of xSUSHI that will result in a return of as many SUSHI tokens as were requested.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L62-L87  /// @notice Redeems tokens from the yield source from the msg.sender, it burn yield bearing tokens and return token to the sender.  /// @param amount The amount of `token()` to withdraw.  Denominated in `token()` as above.  /// @return The actual amount of tokens that were redeemed.  function redeemToken(uint256 amount) public override returns (uint256) {  ISushiBar bar = ISushiBar(sushiBar);  ISushi sushi = ISushi(sushiAddr);  uint256 totalShares = bar.totalSupply();  uint256 barSushiBalance = sushi.balanceOf(address(bar));  uint256 requiredShares = amount.mul(totalShares).div(barSushiBalance);  uint256 barBeforeBalance = bar.balanceOf(address(this));  uint256 sushiBeforeBalance = sushi.balanceOf(address(this));  bar.leave(requiredShares);  uint256 barAfterBalance = bar.balanceOf(address(this));  uint256 sushiAfterBalance = sushi.balanceOf(address(this));  uint256 barBalanceDiff = barBeforeBalance.sub(barAfterBalance);  uint256 sushiBalanceDiff = sushiAfterBalance.sub(sushiBeforeBalance);  balances[msg.sender] = balances[msg.sender].sub(barBalanceDiff);  sushi.transfer(msg.sender, sushiBalanceDiff);  return (sushiBalanceDiff);  Recommendation  Calculate requiredShares based on the formula above (x2). We also recommend dealing in a clean way with the special cases totalShares == 0 and barSushiBalance == 0.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.4 Sushi: balanceOfToken underestimates balance ", "body": "  Description  The balanceOfToken computation is too pessimistic, i.e., it can underestimate the current balance slightly.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L29-L45  /// @notice Returns the total balance (in asset tokens).  This includes the deposits and interest.  /// @return The underlying balance of asset tokens  function balanceOfToken(address addr) public override returns (uint256) {  if (balances[addr] == 0) return 0;  ISushiBar bar = ISushiBar(sushiBar);  uint256 shares = bar.balanceOf(address(this));  uint256 totalShares = bar.totalSupply();  uint256 sushiBalance =  shares.mul(ISushi(sushiAddr).balanceOf(address(sushiBar))).div(  totalShares  );  uint256 sourceShares = bar.balanceOf(address(this));  return (balances[addr].mul(sushiBalance).div(sourceShares));  Recommendation  The balanceOfToken function should use the formula above.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.5 Yearn: Redundant approve call ", "body": "  Description  The approval for token transfer is done in the following way:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L167-L170  if(token.allowance(address(this), address(v)) < token.balanceOf(address(this))) {  token.safeApprove(address(v), 0);  token.safeApprove(address(v), type(uint256).max);  Since the approval will be equal to the maximum value, there s no need to make zero-value approval first.  Recommendation  Change two safeApprove to one regular approve with the maximum value.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.6 Sushi: Some state variables should be immutable and have more specific types ", "body": "  Description  The state variables sushiBar and sushiAddr are initialized in the contract s constructor and never changed afterward.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L12-L21  contract SushiYieldSource is IYieldSource {  using SafeMath for uint256;  address public sushiBar;  address public sushiAddr;  mapping(address => uint256) public balances;  constructor(address _sushiBar, address _sushiAddr) public {  sushiBar = _sushiBar;  sushiAddr = _sushiAddr;  Recommendation  Make these two state variables immutable and change their types as indicated above. Remove the corresponding explicit type conversions in the rest of the contract, and add explicit conversions to type address where necessary.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.7 Sushi: Unnecessary balance queries ", "body": "  Description  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L73-L84  uint256 barBeforeBalance = bar.balanceOf(address(this));  uint256 sushiBeforeBalance = sushi.balanceOf(address(this));  bar.leave(requiredShares);  uint256 barAfterBalance = bar.balanceOf(address(this));  uint256 sushiAfterBalance = sushi.balanceOf(address(this));  uint256 barBalanceDiff = barBeforeBalance.sub(barAfterBalance);  uint256 sushiBalanceDiff = sushiAfterBalance.sub(sushiBeforeBalance);  balances[msg.sender] = balances[msg.sender].sub(barBalanceDiff);  Recommendation  Use requiredShares instead of barBalanceDiff, and remove the unnecessary queries and variables.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.8 Sushi: Unnecessary function declaration in interface ", "body": "  Description  The ISushiBar interface declares a transfer function.  code/sushi-pooltogether/contracts/ISushiBar.sol:L5-L17  interface ISushiBar {  function enter(uint256 _amount) external;  function leave(uint256 _share) external;  function totalSupply() external view returns (uint256);  function balanceOf(address account) external view returns (uint256);  function transfer(address recipient, uint256 amount)  external  returns (bool);  However, this function is never used, so it could be removed from the interface. Other functions that the SushiBar provides but are not used (approve, for example) aren t part of the interface either.  Recommendation  Remove the transfer declaration from the ISushiBar interface.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "4.1 Missing Input Validation for WalletAddress    ", "body": "  Resolution   The client acknowledged the issue and fixed it by implementing a regex validation in PR#25   here - Snap shasum  Description  The snap prompts users to input the wallet address to be monitored. Users can set wallet addreses that do not adhere to the common Ethereum address format. The user input is not sanitized. This could lead to various injection vulnerabilities such as markdown or control character injections that could break other components. In particular, the address is sent to the API as a URL query parameter. A malicious attacker could try using that to mount URL injection attacks.  packages/snap/src/index.ts:L50-L61  if (  request.method === RpcRequestMethods.UpdateAccount &&  'walletAddress' in request.params &&  typeof request.params.walletAddress === 'string'  ) {  const { walletAddress } = request.params;  if (!walletAddress) {  throw new Error('no wallet address provided');  updateWalletAddress(walletAddress);  Recommendation  Sanitize the address string input by the user and reject all addresses that do not adhere to the Ethereum address format.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.2 Server Should Not Rely on Clients  Randomness    ", "body": "  Resolution  Severity decreased: Major  > Medium: The client acknowledged the issue, and let us know that the ID is only used for analytics purposes, to be compatible with the existing API. A future release of the API will improve UID handling.  Description  The snap code sends a request to the Wallet Guard API with a random UUID crypto.randomUUID() generated by the client. We would like to underline that the API should never trust clients  randomness nor assume any property about it. Relying on client-generated randomness for the API could lead to many vulnerabilities, such as replay attacks or collision issues due to the inability to ensure uniqueness. The varying algorithms used by clients may be subpar or even compromised. As this id is not used anywhere else in the snap code, we assume that it might be used on the API side. Because the API is not in scope for this review, we don t have access to the code and cannot tell whether this pseudo-random UUID is used in a safe way.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  Recommendation  Don t rely on clients  randomness on the API. Instead, the server should assign a unique ID to every incoming request.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.3 Properties of the transaction Object Might Be Undefined ", "body": "  Description  The Metamask Snaps API does not guarantee that the properties from and method of the transaction object are defined. Depending on the transaction type, it could happen that these properties are not defined. This would result in a runtime error when undefined is casted to string.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  Recommendation  One should check whether properties from, and method are defined, before explicitly casting them to a string. This could be done by introducing a hasProperty utility function for instance.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.4 AssetChangeComponent Displays a Change With Value 0 if fiatValue < 0.005 ", "body": "  Description  The toFixed(2) method rounds the transaction value string to 2 decimals. For transactions with fiatValue < 0.005, the function returns 0, meaning the component will display a transaction with zero value to the user, even if the transaction has a small yet non-zero value. This is not a good idea as it might trick the user. In that case, it would be better to default to the smallest value that can represented (i.e. 0.01) instead of 0.  packages/snap/src/components/stateChanges/AssetChangeComponent.ts:L18  const fiatValue = Number(stateChange.fiatValue).toFixed(2);  Recommendation  If fiatValue < 0.005, consider displaying a value of 0.01 to the user, instead of 0.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.5 Incomplete NatSpec and General Documentation ", "body": "  Description  The code is missing NatSpec documentation in many places. NatSpec documentation plays an important role in improving code comprehension and maintenance. Adding NatSpec documentation to functions with significant logic that provides clear explanations of behavior, inputs, and outputs enhances code readability, transparency, and maintainability of the codebase.  Recommendation  We recommend adding NatSpec documentation to every function that contains significant logic. Especially all the Snaps handlers. This will improve the readability, transparency, and maintainability of the codebase. We also recommend adding a detailed high-level documentation about the Snaps features, components, and permissions in the README.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.6 formatFiatValue() Can Be Simplified ", "body": "  Description  The function formatFiatValue formats a number to a string that is displayed to the user. The function formats numbers with at most 2 decimal digits, removes the trailing zeros, and adds commas as thousands separators.  The function first converts the number to a string representing the number in fixed-point notation. Then, it uses regex to remove the trailing zeros if they exist. Finally, it adds the thousands separators.  packages/snap/src/utils/helpers.ts:L16-L26  export const formatFiatValue = (  fiatValue: string,  maxDecimals: number,  ): string => {  const fiatWithRoundedDecimals = Number(fiatValue)  .toFixed(maxDecimals) // round to maxDecimals  .replace(/\\.00$/u, ''); // removes 00 if it exists  const fiatWithCommas = numberWithCommas(fiatWithRoundedDecimals); // add commas  return `$${fiatWithCommas}`;  };  The design of the function is unnecessarily complex. The whole design could be simplified using the native toLocaleString() function with appropriate parameters.  Recommendation  Simplify the design by using the native toLocaleString function. For instance, the function could be used as follows toLocaleString('en-US',{minimumFractionDigits: 0, maximumFractionDigits: 2})  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.7 No Way to Disable Approvals Checking, and Transaction Analytics ", "body": "  Description  Currently, there is no easy way to disable wallet approval monitoring and/or transaction simulation apart from uninstalling the snap. Users might want to opt out of wallet monitoring or disable transaction simulation selectively e.g., for privacy concerns.  Recommendation  We would recommend implementing a mechanism that allows users to selectively disable the snap features.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.8 devDependencies Erroneously Listed as dependencies ", "body": "  Description  The following dependencies are only used for development purpose and should therefore be listed as  devDependencies  instead of  dependencies  in the package.json file. Indeed, the TypeScript code is compiled into a bundle, which is released. Meaning the snap  production  code should not contain any external dependency.  packages/snap/package.json:L28-L31  \"dependencies\": {  \"@metamask/snaps-types\": \"^0.32.2\",  \"@metamask/snaps-ui\": \"^0.32.2\"  },  Recommendation  List the dependencies as  devDependencies .  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.9 package.json - Missing Author ", "body": "  Description  The package.json file is missing the author name, the link to the project homepage, and to the bug tracker.  Recommendation  According to package publishing best practices, we recommend adding those elements to the package.json file.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.10 Extra  If  Statement", "body": "  Description  The onRpcRequest() handler returns early if walletAddress is not defined.  packages/snap/src/index.ts:L57-L59  if (!walletAddress) {  throw new Error('no wallet address provided');  Thus, the extra  if  check before calling snap.request() is superfluous and can be removed.  packages/snap/src/index.ts:L57-L64  if (!walletAddress) {  throw new Error('no wallet address provided');  updateWalletAddress(walletAddress);  if (walletAddress) {  await snap.request({  Recommendation  Remove the extra  if  check.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.11 Misleading Comment", "body": "  Description  The NatSpec comment indicates that onRpcRequest() returns  the result of snap_dialog  while the method either does not return anything, or returns the Ethereum address of the monitored wallet.  packages/snap/src/index.ts:L24-L34  /**  Handle incoming JSON-RPC requests, sent through `wallet_invokeSnap`.  @param args - The request handler args as object.  @param args.origin - The origin of the request, e.g., the website that  invoked the snap.  @param args.request - A validated JSON-RPC request object.  @returns The result of `snap_dialog`.  @throws If the request method is not valid for this snap.  /  Recommendation  Fix the comment.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.12 Wallet Monitoring Improvements", "body": "  Description  The snap allows the user to set an arbitrary wallet address to be monitored for dangerous approvals. This feature is only of limited use and could be improved by:  Allowing to specify multiple addresses to monitor (a wallet typically consists of many accounts that are managed under the wallet key)  Allowing users to fetch connected addresses via the ethereum API directly instead of requiring the user to input valid accounts  For privacy reasons, allowing users to opt out of transaction analytics on a per-account basis (Currently, every transaction and transaction origin is sent to the API, even if no monitored wallet address is set).  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.13 Consider Submitting Snap Version With Backend API Requests", "body": "  Description  Consider adding the snap package version to the API requests in order to get insights about what snap versions are used in the field. This could be useful for future debugging and forensics when multiple snap versions will coexist.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  packages/snap/src/types/simulateApi.ts:L25-L35  export type SimulateRequestParams = {  id: string;  chainID: string;  signer: string;  origin: string;  method: string;  transaction: {  [key: string]: Json;  };  source: 'SNAP';  };  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "6.1 FAIR can be stolen using ERC-777 hooks    ", "body": "  Resolution  fixed by completely removing ERC-777 support.  Description  The sell() function calls out to user-configured hooks when burning incoming FAIR tokens. The buy() function does the same if the DAT s currency is ERC-777 compliant.  Either of these hooks might invoke malicious code to re-enter the DAT, allowing them to sell and/or buy FAIR tokens at an unintentionally favourable price.  Such attacks may leave the DAT undercollateralized, resulting in other investors being unable to redeem their FAIR for currency.  Example  Here are some ordered extracts from the code invoked when DAT.buy() is called, when the DAT s currency is an ERC-777 compliant token.  code/contracts/DecentralizedAutonomousTrust.vy:L629  tokenValue: uint256 = self.estimateBuyValue(_currencyValue)  The code above does a calculation using FAIR.totalSupply as input. The higher FAIR.totalSupply is, the more expensive FAIR tokens become.  code/contracts/DecentralizedAutonomousTrust.vy:L502-L503  if(self.isCurrencyERC777):  self.currency.operatorSend(_from, self, _quantityToInvest, \"\", \"\")  Per the ERC-777 standard, the code above invokes an arbitrary tokensToSend() hook configured by the buyer.  code/contracts/DecentralizedAutonomousTrust.vy:L654  self.fair.mint(msg.sender, _to, tokenValue, \"\", \"\")  The code above increments FAIR.totalSupply, effectively increasing the price of FAIR tokens. This happens after the other two code extracts have completed.  An attacker can exploit re-entrancy during the tokensToSend() hook, to purchase further tokens at a (perhaps extremely) favourable price before FAIR.totalSupply is incremented.  If the price at the time of the initial buy() is very low (as it will be when totalSupply is small or zero), then they may be able to buy huge amounts of FAIR at that very low price.  Recommendation  Prevent reentrancy by adding a mutex (using Vyper s @nonreentrant() decorator) across all functions that result in ERC-777 token transfers (of FAIR or an ERC-777 currency).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.2 BigDiv does not prevent overflow in some cases where it should    ", "body": "  Resolution  Fixed in the Solidity implementation.  Description  BigDiv.vy has been created with the aim of allowing calculations like (a * b) / d to succeed where an intermediate step (e.g. a * b) might overflow but the end result is <= MAX_UINT256.  All of the functions sometimes fail in this aim if the numerators are large and of the same order of magnitude. (E.g. for bigDiv2x1, it fails if _numA / MAX_BEFORE_SQUARE = numB / MAX_BEFORE_SQUARE > 0)  The chances of this issue being hit accidentally or exploited deliberately in the current code will both greatly depend on the DAT s configuration and its state. (If the numbers are amenable, an attacker could conceivably front run transactions and adjust FAIR balances in a way that causes targeted transactions to fail.)  Having functions that unexpectedly fail is dangerous for future consumers of this code, and the (simplest possible) fix is small.  Examples  The following code overflows in the code as audited, but succeeds (returning MAX_INT) if MAX_BEFORE_SQUARE is altered as suggested in issue 6.4.  bigDiv2x1 also overflows for some simple cases where the result is far below MAX_UINT256. E.g.:  Recommendations  1. Fix overflows  The following code appears in each BigDiv function:  code/contracts/BigDiv.vy:L30-L31  if(factor == 0):  factor = 1  Replacing every instance of these two lines with simply factor += 1 will avoid overflows. It will also reduce the (currently undocumented) accuracy of the result in some cases, so see recommendations in issue 6.4.  2. Add automated regression tests for all BigDiv functions  We have already written some basic test code and can supply it on request.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.3 Square roots are not calculated accurately for inputs below ~10^30    ", "body": "  Resolution  This has been addressed.  Description  The rounding performed when calculating square roots results in an extreme loss of precision for numbers < ~10^30.  This may or may not be OK. Typically the numbers being square rooted will be significantly larger than 10^30, but when supply is low and the value of a buy / pay is also low, this rounding could have a dramatic effect.  In any case, the square rooting logic and its limitations could be be better documented and tested.  Examples  In both places where _toDecimalWithPlaces is used, it is surrounded by the same code, which combines with _toDecimalWithPlaces to calculate a square root of a uint256:  code/contracts/DecentralizedAutonomousTrust.vy:L792-L808  # Math: Truncates last 18 digits from tokenValue here  tokenValue /= DIGITS_UINT  # Math: Truncates another 8 digits from tokenValue (losing 26 digits in total)  # This will cause small values to round to 0 tokens for the payment (the payment is still accepted)  # Math: Max supported tokenValue is 1.7e+56. If supply is at the hard-cap tokenValue would be 1e38, leaving room  # for a _currencyValue up to 1.7e33 (or 1.7e15 after decimals)  decimalValue: decimal = self._toDecimalWithPlaces(tokenValue)  decimalValue = sqrt(decimalValue)  # Unshift results  # Math: decimalValue has a max value of 2^127 - 1 which after sqrt can always be multiplied  # here without overflow  decimalValue *= DIGITS_DECIMAL  tokenValue = convert(decimalValue, uint256)  This code casts the number to a decimal so that Vyper s sqrt can be used, after first doing some rounding to prevent overflow during the cast. After all of this is done, it casts back to a uint256.  The result of the rounding + casts is reasonably accurate square root for very large integers, but it loses a lot of accuracy for smaller integers.  E.g. an integer as  small  as 12345678901234567890123456 results in a  square root  value of 0.  Recommendations  1. Reduce code duplication and document assumption / limitations  By moving the common surrounding code inside the _toDecimalWithPlaces function, that function could be renamed (e.g. to integerSqrt) and the limitations of the whole square root calculation could be more easily documented.  2. Test the documented limitations of the integerSqrt operation  To verify the documented limitations, thereby reducing the chances of this code being misused by a different developer at a later stage of the same project.  3. If accuracy for smaller integers is important, improve it  Greater accuracy may be achievable by writing or importing a function that approximates square roots using integer arithmetic and Newton s Method, without ever casting to a decimal.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.4 BigDiv estimates some values that could be easily calculated    ", "body": "  Resolution  Fixed in the port to Solidity.  Description  The accuracy of BigDiv s functions is neither documented clearly nor directly tested. (The csv tests should exercise much of the BigDiv code, but it s hard to see exactly what calculations are being done.)  BigDiv returns estimates in some cases where it could easily calculate a precise answer. Having spoken offline about FAIR s requirements, we believe the lack of accuracy itself is probably not a problem right now, but it creates a small risk of BigDiv being accidentally misused in future scenarios where its level of accuracy is insufficient (perhaps by a different developer, during a new phase of the FAIR project).  In any case, BigDiv s behaviour could be better documented and tested.  Examples  For comparison, we define a simpler function:  @public  @constant  def simpleDiv(  _numA: uint256,  _numB: uint256,  _den: uint256  ) -> uint256:  return _numA * _numB / _den  In some cases where both bigDiv2x1 and simpleDiv both succeed, bigDiv2x1 is less accurate than simpleDiv:  a='1'  b='99993402823669209384634633746074317682114579999'  BigDiv.bigDiv2x1(a, b, '8', false) -- succeeds, approximate answer  simpleDiv(a, b, '8')               -- succeeds, exact answer  Also, the constants MAX_BEFORE_SQUARE and MAX_BEFORE_CUBE seem to have been miscalculated, resulting in estimation happening slightly more often than necessary.  Recommendations  1. Document expected accuracy / rounding  To prevent accidental misuse of these functions in the future.  2. Add automated regression tests for all BigDiv functions  We have written some basic unit test code as part of our audit, and can supply it on request.  3. If maximising accuracy is important, improve it  There is some low-hanging fruit here, such as:  increasing MAX_BEFORE_SQUARE and MAX_BEFORE_CUBE to 340282366920938463463374607431768211456 and 48740834812604276470692695, respectively.  Per code logic, these numbers are really the first numbers that cannot be squared and cubed, so you may also wish to rename the constants. Note that MAX_BEFORE_SQUARE is also defined in the DAT contract  Add a check for overflow before resorting to estimation. E.g. for bigDiv2x...:  if(MAX_UINT256 / _numA > _numB):  # No rounding required. Return exact result  return _numA * _numB / _den  This latter change may reduce gas consumption as well as improving accuracy.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.5 Unused code in BigDiv functions    ", "body": "  Resolution  Fixed.  Description  Some parameters and associated logic can be removed from BigDiv s functions. This would simplify the code, as well as the analysis and testing of the code.  Examples  The _roundUp parameter is always false in the following functions:  bigDiv2x1  bigDiv3x1  bigDiv3x3  Associated conditionals are numerous. E.g. bigDiv3x3 s code branches 7 times on the value of _roundUp, even though it is always false.  Recommendation  Remove unused code and associated logic.  Add tests for code that remains.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.6 FAIR - Calling transferFrom should not emit the Approval event   ", "body": "  Resolution  Closed as WontFix.  This behavior is a de facto standard based on it s usage in the OpenZeppelin implementation of ERC20.  Description  The method transferFrom() sends some already approved tokens to some address:  code/contracts/FAIR.vy:L427-L439  @public  def transferFrom(  _from: address,  _to: address,  _value: uint256  ) -> bool:  \"\"\"  @notice Transfers `_value` amount of tokens from address `_from` to address `_to` if authorized.  \"\"\"  self.allowances[_from][msg.sender] -= _value  self._send(msg.sender, _from, _to, _value, False, \"\", \"\")  log.Approval(_from, msg.sender, self.allowances[_from][msg.sender])  return True  But it also emits an Approval event.  code/contracts/FAIR.vy:L438  log.Approval(_from, msg.sender, self.allowances[_from][msg.sender])  The event does not seem to create problems, it basically updates the remaining approved tokens.  Examples  The EIP 20 documentation states that the event should be emitted when a successful call to approve happens. It does not say if it should (or should not) be used when successfully calling transferFrom().  https://eips.ethereum.org/EIPS/eip-20#approval  It does not seem to violate the EIP 20 or EIP 777 standard and it helps any off-chain service monitoring the contract, keep track of how many remaining approved tokens are left, without having any previous state.  However any re-entrancy issues will make the transaction emit multiple events, each event having different amounts approved, the last emitted event having the highest value, which will be the incorrect one.  Recommendation  We suggest removing the emitted log because it can create problems.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.7 On-chain logic cannot reliably prevent a malicious beneficiary from purchasing tokens at a discount    ", "body": "  Resolution  closed as WontFix. Fraudulent token purchases by the Beneficiary are prevented by the associated legal agreements, not by on-chain logic.  The extra code should actually be thought of as enabling a legitimate method for the Beneficiary purchase tokens at a fair price.  Description  The buy() function contains unique logic for identifying and processing an investment by the beneficiary:  code/contracts/DecentralizedAutonomousTrust.vy:L650-L658  elif(self.state == STATE_RUN):  if(_to != self.beneficiary):  self._distributeInvestment(_currencyValue)  self.fair.mint(msg.sender, _to, tokenValue, \"\", \"\")  if(self.state == STATE_RUN):  if(_to == self.beneficiary):  self._applyBurnThreshold() # must mint before this call  Because the beneficiary receives a portion amount invested, without this logic the beneficiary organization could purchase FAIRs for a fraction of the price compared to external investors.  However, this logic can be easily circumvented by a dishonest beneficiary using another address for investments.  The Fairmint team has explained that they are aware that this protection can be circumvented. The  legal layer  is necessary to enforce good behaviour, and the beneficiary would be committing fraud in case they purchased FAIRs using another address. Thus the extra code should actually be thought of as enabling the Beneficiary to legitimately purchase tokens.  Recommendation  This functionality introduces extra code. Consider reducing complexity by removing this functionality if it is not essential.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.8 FAIR is not ERC-777 compliant    ", "body": "  Resolution  fixed by removing ERC-777 support  Description  A comment at the top of FAIR.vy describes it as an  ERC-777 and ERC-20 compliant token .  But by the code s own acknowledgement, it is not fully ERC-777 compliant in its current state.  Examples  The contract is non-compliant with ERC-777 in at least the following ways:  Does not allow per-user revocation of the default operator (the DAT)  Does not call the ERC777 tokensToSend and tokensReceived hooks within transfer and transferFrom  It is (correctly, given the points above) not ERC820-registered as an ERC777Token  This list may not be exhaustive.  Recommendation  Implementing the standard fully may improve interop, so implementing all missing logic should be considered.  If not in full compliance:  avoid publishing any documentation that could be construed as claiming ERC-777 compliance, including code comments.  in accordance with other findings, consider removing ERC-777 compliance, and restricting the functionality to ERC-20.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.9 Not compliant with ERC1404    ", "body": "  Resolution  Fixed. Porting to solidity enabled compliance with ERC1404.  Description  The ERC1404 standard is an extension of the ERC20 standard. Here it has been implemented as a standalone contract, but does not contain all of the extra functions required by ERC1404.  As such, neither the FAIR contract nor the ERC1404 contract is ERC1404-compliant.  Recommendations  Rename the ERC1404 contract to be something more generic like Whitelist. This is more descriptive, and avoids confusion between Whitelist.approve() and the completely unrelated approve() function that an ERC1404-compliant contract should inherit from ERC20.  Fully implement ERC1404 in FAIR by adding messageForTransferRestriction(), if and only if the standard can be changed to accommodate Vyper s types. If it cannot, drop all claims or implications of ERC1404 support.  To further reduce confusion, consider renaming approve(), and perhaps splitting it into 2 separate functions. E.g. allow() and deny().  7 Code quality recommendations  This sections compiles suggestions which do not pose a direct threat to security, but would otherwise improve the quality of the code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "7.1 DecentralizedAutonomousTrust.sol", "body": "  The method _authorizeTransfer could be rewritten as a modifier, if desired.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "7.2 Whitelist.sol", "body": "  The argument _isSell is not used in the method authorizeTransfer().  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "7.3 Sqrt.sol", "body": "  SafeMath.sol is imported to Sqrt.sol, but is not used.  8 Gas efficiency optimization recommendations  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "8.1 DecentralizedAutonomousTrust.sol", "body": "  The BigDiv.sol and Sqrt.sol contracts are deployed separately and their methods are accessed as external calls. This is more expensive than accessing the functions as libraries. Ie. library BigDiv and using BigDiv for uint.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.1 RocketDaoNodeTrusted - DAO takeover during deployment/bootstrapping ", "body": "  Resolution  The node registration is enabled by default (node.registration.enabled) but the client intends to change this to disabling the registration until bootstrap mode finished.  We are intending to set node registrations to false during deployment, then open it up when we need to register our oDAO nodes  Description  The initial deployer of the RocketStorage contract is set as the Guardian/Bootstrapping role. This guardian can bootstrap the TrustedNode and Protocol DAO, add members, upgrade components, change settings.  Right after deploying the DAO contract the member count is zero. The Guardian can now begin calling any of the bootstrapping functions to add members, change settings, upgrade components, interact with the treasury, etc. The bootstrapping configuration by the Guardian is unlikely to all happen within one transaction which might allow other parties to interact with the system while it is being set up.  RocketDaoNodeTrusted also implements a recovery mode that allows any registered node to invite themselves directly into the DAO without requiring approval from the Guardian or potential other DAO members as long as the total member count is below daoMemberMinCount (3). The Guardian itself is not counted as a DAO member as it is a supervisory role.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L202-L215  /**** Recovery ***************/  // In an explicable black swan scenario where the DAO loses more than the min membership required (3), this method can be used by a regular node operator to join the DAO  // Must have their ID, email, current RPL bond amount available and must be called by their current registered node account  function memberJoinRequired(string memory _id, string memory _email) override public onlyLowMemberMode onlyRegisteredNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {  // Ok good to go, lets add them  (bool successPropose, bytes memory responsePropose) = getContractAddress('rocketDAONodeTrustedProposals').call(abi.encodeWithSignature(\"proposalInvite(string,string,address)\", _id, _email, msg.sender));  // Was there an error?  require(successPropose, getRevertMsg(responsePropose));  // Get the to automatically join as a member (by a regular proposal, they would have to manually accept, but this is no ordinary situation)  (bool successJoin, bytes memory responseJoin) = getContractAddress(\"rocketDAONodeTrustedActions\").call(abi.encodeWithSignature(\"actionJoinRequired(address)\", msg.sender));  // Was there an error?  require(successJoin, getRevertMsg(responseJoin));  This opens up a window during the bootstrapping phase where any Ethereum Address might be able to register as a node (RocketNodeManager.registerNode) if node registration is enabled (default=true) rushing into RocketDAONodeTrusted.memberJoinRequired adding themselves (up to 3 nodes) as trusted nodes to the DAO. The new DAO members can now take over the DAO by issuing proposals, waiting 2 blocks to vote/execute them (upgrade, change settings while Guardian is changing settings, etc.). The Guardian role can kick the new DAO members, however, they can invite themselves back into the DAO.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsNode.sol:L19-L19  setSettingBool(\"node.registration.enabled\", true);  Recommendation  Disable the DAO recovery mode during bootstrapping. Disable node registration by default and require the guardian to enable it. Ensure that bootstrapDisable (in both DAO contracts) performs sanity checks as to whether the DAO bootstrapping finished and permissions can effectively be revoked without putting the DAO at risk or in an irrecoverable state (enough members bootstrapped, vital configurations like registration and other settings are configured, \u2026).  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.2 RocketTokenRETH - sandwiching opportunity on price updates    ", "body": "  Resolution  This issue is being addressed in a currently pending pull request. By introducing a delay between an rETH deposit and a subsequent transfer or burn, sandwiching a price update transaction is not possible anymore. Specifically, a deposit delay of circa one day is introduced:  https://github.com/rocket-pool/rocketpool/pull/201/files#diff-0387338dc5dd7edd0a03766cfdaaee42d021d4e781239d5ebbff359c81497839R146-R150  // This is called by the base ERC20 contract before all transfer, mint, and burns  function _beforeTokenTransfer(address from, address, uint256) internal override {  // Don't run check if this is a mint transaction  if (from != address(0)) {  // Check which block the user's last deposit was  bytes32 key = keccak256(abi.encodePacked(\"user.deposit.block\", from));  uint256 lastDepositBlock = getUint(key);  if (lastDepositBlock > 0) {  // Ensure enough blocks have passed  RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));  uint256 blocksPassed = block.number.sub(lastDepositBlock);  require(blocksPassed > rocketDAOProtocolSettingsNetwork.getRethDepositDelay(), \"Not enough time has passed since deposit\");  // Clear the state as it's no longer necessary to check this until another deposit is made  deleteUint(key);  In the current version, it is correctly enforced that a deposit delay of zero is not possible.  Description  The rETH token price is not coupled to the amount of rETH tokens in circulation on the Ethereum chain. The price is reported by oracle nodes and committed to the system via a voting process. The price of rETH changes If 51% of nodes observe and submit the same price information. If nodes fail to find price consensus for a block, then the rETH price might be stale.  There is an opportunity for the user to front-run the price update right before it is committed. If the next price is higher than the previous (typical case), this gives an instant opportunity to perform a risk-free ETH -> rETH -> ETH exchange for profit. In the worst case, one could drain all the ETH held by the RocketTokenRETH contract + excess funds stored in the vault.  Note: there seems to be a \"network.submit.balances.frequency\" price and balance submission frequency of 24hrs. However, this frequency is not enforced, and it is questionable if it makes sense to pin the price for 24hrs.  Note: the total supply of the RocketTokenRETH contract may be completely disconnected from the reported total supply for RETH via oracle nodes.  Examples  The amount of ETH was only staked during this one process for the price update duration and unlikely to be useful to the system. This way, a whale (only limited by the max deposit amount set on deposit) can drain the RocketTokenRETH contract from all its ETH and excess eth funds.  mempool observed: submitPrice tx (an effective transaction that changes the price) wrapped with buying rETH and selling rETH for ETH:  RocketDepositPool.deposit() at old price => mints rETH at current rate  RocketNetworkPrices.submitPrices(newRate)  RocketTokenRETH.burn(balanceOf(msg.sender) => burns rETH for ETH at new rate  deposit (virtually no limit with 1000ETH being the limit right now)  rocketpool-2.5-Tokenomics-updates/contracts/contract/deposit/RocketDepositPool.sol:L63-L67  require(rocketDAOProtocolSettingsDeposit.getDepositEnabled(), \"Deposits into Rocket Pool are currently disabled\");  require(msg.value >= rocketDAOProtocolSettingsDeposit.getMinimumDeposit(), \"The deposited amount is less than the minimum deposit size\");  require(getBalance().add(msg.value) <= rocketDAOProtocolSettingsDeposit.getMaximumDepositPoolSize(), \"The deposit pool size after depositing exceeds the maximum size\");  // Mint rETH to user account  rocketTokenRETH.mint(msg.value, msg.sender);  trustedNodes submitPrice (changes params for getEthValue and getRethValue)  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updatePrices(_block, _rplPrice);  immediately burn at new rate (as params for getEthValue changed)  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRETH.sol:L107-L124  function burn(uint256 _rethAmount) override external {  // Check rETH amount  require(_rethAmount > 0, \"Invalid token burn amount\");  require(balanceOf(msg.sender) >= _rethAmount, \"Insufficient rETH balance\");  // Get ETH amount  uint256 ethAmount = getEthValue(_rethAmount);  // Get & check ETH balance  uint256 ethBalance = getTotalCollateral();  require(ethBalance >= ethAmount, \"Insufficient ETH balance for exchange\");  // Update balance & supply  _burn(msg.sender, _rethAmount);  // Withdraw ETH from deposit pool if required  withdrawDepositCollateral(ethAmount);  // Transfer ETH to sender  msg.sender.transfer(ethAmount);  // Emit tokens burned event  emit TokensBurned(msg.sender, _rethAmount, ethAmount, block.timestamp);  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.3 RocketDaoNodeTrustedActions - Incomplete implementation of member challenge process    ", "body": "  Resolution   As of the Smartnode s   Description  Nodes do not seem to monitor ActionChallengeMade events so that they could react to challenges  Nodes do not implement actionChallengeDecide and, therefore, cannot successfully stop a challenge  Funds/Tribute sent along with the challenge will be locked forever in the RocketDAONodeTrustedActions contract. There s no means to recover the funds.  It is questionable whether the incentives are aligned well enough for anyone to challenge stale nodes. The default of 1 eth compared to the risk of the  malicious  or  stale  node exiting themselves is quite high. The challenger is not incentivized to challenge someone other than for taking over the DAO. If the tribute is too low, this might incentivize users to grief trusted nodes and force them to close a challenge.  Requiring that the challenge initiator is a different registered node than the challenge finalized is a weak protection since the system is open to anyone to register as a node (even without depositing any funds.)  block time is subject to fluctuations. With the default of 43204 blocks, the challenge might expire at 5 days (10 seconds block time), 6.5 days (13 seconds Ethereum target median block time), 7 days (14 seconds), or more with historic block times going up to 20 seconds for shorter periods.  A minority of trusted nodes may use this functionality to boot other trusted node members off the DAO issuing challenges once a day until the DAO member number is low enough to allow them to reach quorum for their own proposals or until the member threshold allows them to add new nodes without having to go through the proposal process at all.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsMembers.sol:L22-L24  setSettingUint('members.challenge.cooldown', 6172);              // How long a member must wait before performing another challenge, approx. 1 day worth of blocks  setSettingUint('members.challenge.window', 43204);               // How long a member has to respond to a challenge. 7 days worth of blocks  setSettingUint('members.challenge.cost', 1 ether);               // How much it costs a non-member to challenge a members node. It's free for current members to challenge other members.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L204-L206  // In the event that the majority/all of members go offline permanently and no more proposals could be passed, a current member or a regular node can 'challenge' a DAO members node to respond  // If it does not respond in the given window, it can be removed as a member. The one who removes the member after the challenge isn't met, must be another node other than the proposer to provide some oversight  // This should only be used in an emergency situation to recover the DAO. Members that need removing when consensus is still viable, should be done via the 'kick' method.  Recommendation  Implement the challenge-response process before enabling users to challenge other nodes. Implement means to detect misuse of this feature for griefing e.g. when one trusted node member forces another trusted node to defeat challenges over and over again (technical controls, monitoring).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.4 RocketDAOProtocolSettings/RocketDAONodeTrustedSettings - anyone can set/overwrite settings until contract is declared  deployed    ", "body": "  Resolution  The client is aware of and acknowledges this potential issue. As with the current contracts the deployed flag is always set in the constructor and there will be no window for someone else to interact with the contract before this flag is set. The following statement was provided:  [\u2026] this method is purely to set the initial default vars. It shouldn t be run again due to the deployment flag being flagged incase that contract is upgraded and those default vars aren t removed.  Additionally, it was suggested to add safeguards to the access restricting modifier, to only allowing the guardian to change settings if a settings contract  forgets  to set the deployed flag in the constructor (Note: the deployed flag must be set with the deploing transaction or else there might be a window for someone to interact with the contract before it is fully configured).  Description  The onlyDAOProtocolProposal modifier guards all state-changing methods in this contract. However, analog to issue 6.5, the access control is disabled until the variable settingsNameSpace.deployed is set. If this contract is not deployed and configured in one transaction, anyone can update the contract while left unprotected on the blockchain.  See issue 6.5 for a similar issue.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L18-L23  modifier onlyDAOProtocolProposal() {  // If this contract has been initialised, only allow access from the proposals contract  if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress('rocketDAOProtocolProposals') == msg.sender, \"Only DAO Protocol Proposals contract can update a setting\");  _;  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L18-L22  modifier onlyDAONodeTrustedProposal() {  // If this contract has been initialised, only allow access from the proposals contract  if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress('rocketDAONodeTrustedProposals') == msg.sender, \"Only DAO Node Trusted Proposals contract can update a setting\");  _;  There are at least 9 more occurrences of this pattern.  Recommendation  Restrict access to the methods to a temporary trusted account (e.g. guardian) until the system bootstrapping phase ends by setting deployed to true.  ", "labels": ["Consensys", "Critical", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.5 RocketStorage - anyone can set/update values before the contract is initialized    ", "body": "  Resolution  Fixed by restricting access to the guardian while the contract is not yet initialized. The relevant changeset is rocket-pool/rocketpool@495a51f. The client provided the following statement:  tx.origin is only used in this deployment instance and should be safe since no external contracts are interacted with  The client is aware of the implication of using tx.origin and that the guardian should never be used to interact with third-party contracts as the contract may be able to impersonate the guardian changing settings in the storage contract during that transaction.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/rocketpool-rp3.0-updates/contracts/contract/RocketStorage.sol#L31-L32  Description  According to the deployment script, the contract is deployed, and settings are configured in multiple transactions. This also means that for a period of time, the contract is left unprotected on the blockchain. Anyone can delete/set any value in the centralized data store. An attacker might monitor the mempool for new deployments of the RocketStorage contract and front-run calls to contract.storage.initialised setting arbitrary values in the system.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L31  modifier onlyLatestRocketNetworkContract() {  // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled  if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {  // Make sure the access is permitted to only contracts in our Dapp  require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");  _;  Recommendation  Restrict access to the methods to a temporary trusted account (e.g. guardian) until the system bootstrapping phase ends by setting initialised to true.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.6 RocketDAOProposals - Unpredictable behavior due to short vote delay    Addressed", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by changing the default delay  Description  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L167-L170  require(_startBlock > block.number, \"Proposal start block must be in the future\");  require(_durationBlocks > 0, \"Proposal cannot have a duration of 0 blocks\");  require(_expiresBlocks > 0, \"Proposal cannot have a execution expiration of 0 blocks\");  require(_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");  The default vote delay configured in the system is 1 block.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsProposals.sol:L21-L21  setSettingUint('proposal.vote.delay.blocks', 1);                 // How long before a proposal can be voted on after it is created. Approx. Next Block  A vote is immediately passed when the required quorum is reached which allows it to be executed. This means that a group that is holding enough voting power can propose a change, wait for two blocks (block.number (of time of proposal creation) + configuredDelay (1) + 1 (for ACTIVE state), then vote and execute for the proposal to pass for it to take effect almost immediately after only 2 blocks (<30seconds).  Settings can be changed after 30 seconds which might be unpredictable for other DAO members and not give them enough time to oppose and leave the DAO.  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change after two blocks. The only guarantee is that users can be sure the settings don t change for the next block if no proposal is active.  We recommend giving the user advance notice of changes with a delay. For example, all upgrades should require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.7 RocketRewardPool - Unpredictable staking rewards as stake can be added just before claiming and rewards may be paid to to operators that do not provide a service to the system   Partially Addressed", "body": "  Resolution  Partially addressed in branch rp3.0-updates (rocket-pool/rocketpool@b424ca1) by changing the withdrawal requirements to 150% of the effective RPL.  The client provided the following statement:  Node operators can now only withdraw RPL above their 150% effective RPL stake.  Description  Nodes/TrustedNodes earn rewards based on the current share of the effective RPL stake provided backing the number of Minipools they run. The reward is paid out regardless of when the effective node stake was provided, as long as it is present just before the call to claim(). This means the reward does not take into account how long the stake was provided. The effective RPL stake is the nodes RPL stake capped at a maximum of halfDepositUserAmount * 150% * nr_of_minipools(node) / RPLPrice. If the node does not run any Minipools, the effective RPL stake is zero.  Since effective stake can be added just before calling the claim() method (effectively trying to get a reward for a period that passed without RPL being staked for the full duration), this might create an unpredictable outcome for other participants, as adding significant stake (requires creating Minipools and staking the max per pool; the stake is locked for at least the duration of a reward period rpl.rewards.claim.period.blocks) shifts the shares users get for the fixed total amount of rewards. This can be unfair if the first users claimed their reward, and then someone is artificially inflating the total amount of shares by adding more stake to get a bigger part of the remaining reward. However, this comes at the cost of the registered node having to create more Minipools to stake more, requiring an initial deposit (16ETH, or 0ETH under certain circumstances for trusted nodes) by the actor attempting to get a larger share of the rewards. The risk of losing funds for this actor, however, is rather low, as they can immediately dissolve() and close() the Minipool to refund their node deposit as NETH right after claiming the reward only losing the gas spent on the various transactions.  This can be extended to a node operator creating a Minipool and staking the maximum amount before calling claim to remove the Minipool right after, freeing up the ETH that was locked in the Minipool until the next reward period starts. The node operator is not providing any service to the network, loses some value in ETH for gas but may compensate that with the RPL staking rewards. If the node amassed a significant amount of RPL stake, they might even try to flash-loan enough ETH to spawn Minipools to inflate their effective stake and earn most of the rewards to return the loan RPL profit.  By staking just before claiming, the node effectively can earn rewards for 2 reward periods by only staking RPL for the duration of one period (claim the previous period, leave it in for 14 days, claim another period, withdraw).  The stake can be withdrawn at the earliest 14 days after staking. However, it can be added back at any time, and the stake addition takes effect immediately. This allows for optimizing the staking reward as follows (assuming we front-run other claimers to maximize profits and perform all transactions in one block):  Note that withdraw() can be called right at the time the new reward period starts:  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L165-L166  require(block.number.sub(getNodeRPLStakedBlock(msg.sender)) >= rocketDAOProtocolSettingsRewards.getRewardsClaimIntervalBlocks(), \"The withdrawal cooldown period has not passed\");  // Get & check node's current RPL stake  Examples  A node may choose to register and stake some RPL to collect rewards but never actually provide registered node duties, e.g., operating a Minipool.  Node shares for a passed reward epoch are unpredictable as nodes may change their stake (adding) after/before users claim their rewards.  A node can maximize its rewards by adding stake just before claiming it  A node can stake to claim rewards, wait 14 days, withdraw, lend on a platform and return the stake in time to claim the next period.  Recommendation  Review the incentive model for the RPL rewards. Consider adjusting it so that nodes that provide a service get a better share of the rewards. Consider accruing rewards for the duration the stake was provided instead of taking a snapshot whenever the node calls claim(). Require stake to be locked for > 14 days instead of >=14 days (withdraw()) or have users skip the first reward period after staking.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.8 RocketNodeStaking - Node operators can reduce slashing impact by withdrawing excess staked RPL    ", "body": "  Resolution  The RocketNodeStaking.withdrawRPL method now reverts if a node operator attempts to withdraw an RPL amount that results in the leftover RPL stake being smaller than the maximum required stake. This prevents operators from withdrawing excess RPL to avoid the impact of a slashing.  https://github.com/rocket-pool/rocketpool/blob/rp3.0-updates/contracts/contract/node/RocketNodeStaking.sol#L187  Description  Oracle nodes update the Minipools  balance and progress it to the withdrawable state when they observe the minipools stake to become withdrawable. If the observed stakingEndBalance is less than the user deposit for that pool, the node operator is punished for the difference.  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L89-L94  rocketMinipoolManager.setMinipoolWithdrawalBalances(_minipoolAddress, _stakingEndBalance, nodeAmount);  // Apply node penalties by liquidating RPL stake  if (_stakingEndBalance < userDepositBalance) {  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  rocketNodeStaking.slashRPL(minipool.getNodeAddress(), userDepositBalance - _stakingEndBalance);  The amount slashed is at max userDepositBalance - stakingEndBalance. The userDepositBalance is at least 16 ETH (minipool.half/.full) and at max 32 ETH (minipool.empty). The maximum amount to be slashed is therefore 32 ETH (endBalance = 0, minipool.empty).  https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/32; note that the RPL token is potentially affected by a similar issue as one can stake RPL, wait for the cooldown period & wait for the price to change, and withdraw stake at higher RPL price/ETH). The  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L188-L196  uint256 rplSlashAmount = calcBase.mul(_ethSlashAmount).div(rocketNetworkPrices.getRPLPrice());  // Cap slashed amount to node's RPL stake  uint256 rplStake = getNodeRPLStake(_nodeAddress);  if (rplSlashAmount > rplStake) { rplSlashAmount = rplStake; }  // Transfer slashed amount to auction contract  rocketVault.transferToken(\"rocketAuctionManager\", getContractAddress(\"rocketTokenRPL\"), rplSlashAmount);  // Update RPL stake amounts  decreaseTotalRPLStake(rplSlashAmount);  decreaseNodeRPLStake(_nodeAddress, rplSlashAmount);  If the node does not have a sufficient RPL stake to cover the losses, the slashing amount is capped at whatever amount of RPL the node has left staked.  The minimum amount of RPL a node needs to have staked if it operates minipools is calculated as follows:  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L115-L120  // Calculate minimum RPL stake  return rocketDAOProtocolSettingsMinipool.getHalfDepositUserAmount()  .mul(rocketDAOProtocolSettingsNode.getMinimumPerMinipoolStake())  .mul(rocketMinipoolManager.getNodeMinipoolCount(_nodeAddress))  .div(rocketNetworkPrices.getRPLPrice());  With the current configuration, this would resolve in a minimum stake of 16 ETH * 0.1 (10% collateralization) * 1 (nr_minipools) * RPL_Price for a node operating 1 minipool. This means a node operator basically only needs to have 10% of 16 ETH staked to operate one minipool.  An operator can withdraw their stake at any time, but they have to wait at least 14 days after the last time they staked (cooldown period). They can, at max, withdraw all but the minimum stake required to run the pools (nr_of_minipools * 16 ETH * 10%). This also means that after the cooldown period, they can reduce their stake to 10% of the half deposit amount (16ETH), then perform a voluntary exit on ETH2 so that the minipool becomes withdrawable. If they end up with less than the userDepositBalance in staking rewards, they would only get slashed the 1.6 ETH at max (10% of 16ETH half deposit amount for 1 minipool) even though they incurred a loss that may be up to 32 ETH (empty Minipool empty amount).  Furthermore, if a node operator runs multiple minipools, let s say 5, then they would have to provide at least 5*16ETH*0.1 = 8ETH as a security guarantee in the form of staked RPL. If the node operator incurs a loss with one of their minipools, their 8 ETH RPL stake will likely be slashed in full. Their other - still operating - minipools are not backed by any RPL anymore, and they effectively cannot be slashed anymore. This means that a malicious node operator can create multiple minipools, stake the minimum amount of RPL, get slashed for one minipool, and still operate the others without having the minimum RPL needed to run the minipools staked (getNodeMinipoolLimit).  The RPL stake is donated to the RocketAuctionManager, where they can attempt to buy back RPL potentially at a discount.  Note: Staking more RPL (e.g., to add another Minipool) resets the cooldown period for the total RPL staked (not only for the newly added)  Recommendation  It is recommended to redesign the withdrawal process to prevent users from withdrawing their stake while slashable actions can still occur. A potential solution may be to add a locking period in the process. A node operator may schedule the withdrawal of funds, and after a certain time has passed, may withdraw them. This prevents the immediate withdrawal of funds that may need to be reduced while slashable events can still occur. E.g.:  A node operator requests to withdraw all but the minimum required stake to run their pools.  The funds are scheduled for withdrawal and locked until a period of X days has passed.  (optional) In this period, a slashable event occurs. The funds for compensation are taken from the user s stake including the funds scheduled for withdrawal.  After the time has passed, the node operator may call a function to trigger the withdrawal and get paid out.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.9 RocketTokenRPL - inaccurate inflation rate and potential for manipulation lowering the real APY    Addressed", "body": "  Resolution   The main issue was addressed in branch   rocket-pool/rocketpool@b424ca1) by recording the timestamp up to when inflation was updated to instead of the current block timestamp (  Description  RocketTokenRPL allows users to swap their fixed-rate tokens to the inflationary RocketTokenRPL ERC20 token via a swapToken function. The DAO defines the inflation rate of this token and is initially set to be 5% APY. This APY is configured as a daily inflation rate (APD) with the corresponding 1 day in blocks inflation interval in the rocketDAOProtocolSettingsInflation contract. The DAO members control the inflation settings.  Anyone can call inflationMintTokens to inflate the token, which mints tokens to the contracts RocketVault. Tokens are minted for discreet intervals since the last time inflationMintTokens was called (recorded as inflationCalcBlock). The inflation is then calculated for the passed intervals without taking the current not yet completed interval. However, the inflationCalcBlock is set to the current block.number, effectively skipping some  time /blocks of the APY calculation.  The more often inflationMintTokens is called, the higher the APY likelihood dropping below the configured 5%. In the worst case, one could manipulate the APY down to 2.45% (assuming that the APD for a 5% APY was configured) by calling inflationMintTokens close to the end of every second interval. This would essentially restart the APY interval at block.number, skipping blocks of the current interval that have not been accounted for.  Note: updating the inflation rate will directly affect past inflation intervals that have not been minted! this might be undesirable, and it could be considered to force an inflation mint if the APY changes  Note: if the interval is small enough and there is a history of unaccounted intervals to be minted, and the Ethereum network is congested, gas fees may be high and block limits hit, the calculations in the for loop might be susceptible to DoS the inflation mechanism because of gas constraints.  Note: The inflation seems only to be triggered regularly on RocketRewardsPool.claim (or at any point by external actors). If the price establishes based on the total supply of tokens, then this may give attackers an opportunity to front-run other users trading large amounts of RPL that may previously have calculated their prices based on the un-inflated supply.  Note: that the discrete interval-based inflation (e.g., once a day) might create dynamics that put pressure on users to trade their RPL in windows instead of consecutively  Examples  the inflation intervals passed is the number of completed intervals. The current interval that is started is not included.  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L108-L119  function getInlfationIntervalsPassed() override public view returns(uint256) {  // The block that inflation was last calculated at  uint256 inflationLastCalculatedBlock = getInflationCalcBlock();  // Get the daily inflation in blocks  uint256 inflationInterval = getInflationIntervalBlocks();  // Calculate now if inflation has begun  if(inflationLastCalculatedBlock > 0) {  return (block.number).sub(inflationLastCalculatedBlock).div(inflationInterval);  }else{  return 0;  the inflation calculation calculates the to-be-minted tokens for the inflation rate at newTokens = supply * rateAPD^intervals - supply  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L126-L148  function inflationCalculate() override public view returns (uint256) {  // The inflation amount  uint256 inflationTokenAmount = 0;  // Optimisation  uint256 inflationRate = getInflationIntervalRate();  // Compute the number of inflation intervals elapsed since the last time we minted infation tokens  uint256 intervalsSinceLastMint = getInlfationIntervalsPassed();  // Only update  if last interval has passed and inflation rate is > 0  if(intervalsSinceLastMint > 0 && inflationRate > 0) {  // Our inflation rate  uint256 rate = inflationRate;  // Compute inflation for total inflation intervals elapsed  for (uint256 i = 1; i < intervalsSinceLastMint; i++) {  rate = rate.mul(inflationRate).div(10 ** 18);  // Get the total supply now  uint256 totalSupplyCurrent = totalSupply();  // Return inflation amount  inflationTokenAmount = totalSupplyCurrent.mul(rate).div(10 ** 18).sub(totalSupplyCurrent);  // Done  return inflationTokenAmount;  Recommendation  Properly track inflationCalcBlock as the end of the previous interval, as this is up to where the inflation was calculated, instead of the block at which the method was invoked.  Ensure APY/APD and interval configuration match up. Ensure the interval is not too small (potential gas DoS blocking inflation mint and RocketRewardsPool.claim).  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.10 Trusted node participation risk and potential client  optimizations     ", "body": "  Resolution  The development team considers this issue fixed as monitoring on the correct behaviour of node software is added to the system.  Description  The system might end up in a stale state with minipools never being setWithdrawable or network and prices being severely outdated because trusted nodes don t fulfill their duty of providing oracle values. Minipools not being able to advance to the Withdrawable state will severely harm the system as no rewards can be paid out. Outdated balances and prices may affect token economics around the tokens involved (specifically rETH price depends on oracle observations).  There is an incentive to be an oracle node as you get paid to provide oracle node duties when enrolled with the DAO. However, it is not enforced that nodes actually fulfill their duty of calling the respective onlyTrustedNode oracle functions to submit prices/balances/minipool rewards.  Therefore, a smart Rocket Pool trusted node operator might consider patching their client software to not or only sporadically fulfill their duties to save considerable amounts of gas, making more profit than other trusted nodes would.  There is no means to directly incentivize trusted nodes to call certain functions as they get their rewards anyway. The only risk they run is that other trusted nodes might detect their antisocial behavior and attempt to kick them out of the DAO. To detect this, monitoring tools and processes need to be established; it is questionable whether users would participate in high maintenance DAO operators.  Furthermore, trusted nodes might choose to gas optimize their submissions to avoid calling the actual action once quorum was established. They can, for example, attempt to submit prices as early as possible, avoiding that they re the first to hit the 51% threshold.  Recommendation  Create monitoring tools and processes to detect participants that do not fulfill their trusted DAO duties. Create direct incentives for trusted nodes to provide oracle services by, e.g., recording their participation rate and only payout rewards based on how active they are.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.11 RocketDAONodeTrustedUpgrade - upgrade does not prevent the use of the same address multiple times creating an inconsistency where getContractAddress returns outdated information    ", "body": "  Resolution   A check has been introduced to make sure that the new contract address is not already in use by checking against the corresponding   Description  When adding a new contract, it is checked whether the address is already in use. This check is missing when upgrading a named contract to a new implementation, potentially allowing someone to register one address to multiple names creating an inconsistent configuration.  The crux of this is, that, getContractAddress() will now return a contract address that is not registered anymore (while getContractName may throw). getContractAddress can therefore not relied upon when checking ACL.  add contract name=test, address=0xfefe  >  sets contract.exists.0xfefe=true  sets contract.name.0xfefe=test  sets contract.address.test=0xfefe  sets contract.abi.test=abi  add another contract name=badcontract, address=0xbadbad  > sets contract.exists.0xbadbad=true sets contract.name.0xbadbad=badcontract sets contract.address.badcontract=0xbadbad sets contract.abi.badcontract=abi  update contract name=test, address=0xbadbad reusing badcontradcts address, the address is now bound to 2 names (test, badcontract) overwrites contract.exists.0xbadbad=true` (even though its already true) updates contract.name.0xbadbad=test (overwrites the reference to badcontract; badcontracts config is now inconsistent) updates contract.address.test=0xbadbad (ok, expected) updates contract.abi.test=abi (ok, expected) removes contract.name.0xfefe (ok) removes contract.exists.0xfefe (ok)  update contract name=test, address=0xc0c0 sets contract.exists.0xc0c0=true sets contract.name.0xc0c0=test (ok, expected) updates contract.address.test=0xc0c0 (ok, expected) updates contract.abi.test=abi (ok, expected) removes contract.name.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now) removes contract.exists.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now)  After this, badcontract is partially cleared, getContractName(0xbadbad) throws while getContractAddress(badcontract) returns 0xbadbad which is already unregistered (contract.exists.0xbadbad=false)  Examples  check in `_addContract``  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L76-L76  require(_contractAddress != address(0x0), \"Invalid contract address\");  no checks in upgrade.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L53-L59  require(_contractAddress != address(0x0), \"Invalid contract address\");  require(_contractAddress != oldContractAddress, \"The contract address cannot be set to its current address\");  // Register new contract  setBool(keccak256(abi.encodePacked(\"contract.exists\", _contractAddress)), true);  setString(keccak256(abi.encodePacked(\"contract.name\", _contractAddress)), _name);  setAddress(keccak256(abi.encodePacked(\"contract.address\", _name)), _contractAddress);  setString(keccak256(abi.encodePacked(\"contract.abi\", _name)), _contractAbi);  Recommendation  Check that the address being upgraded to is not yet registered and properly clean up contract.address.<name|abi>.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.12 Rocketpool CLI - Lax data validation and output sanitation    Addressed", "body": "  Resolution  Addressed with v1.0.0-rc1 by sanitizing non-printables from strings stored in the smart contract.  This effectively mitigates terminal-based control character injection attacks. However, might still be used to inject context-sensitive information that may be consumed by different protocols/presentation layers (web, terminal by displaying falsified information next to fields).  E-mail and timezone format validation was introduced with https://github.com/rocket-pool/rocketpool-go/blob/c8738633ab973503b79c7dee5c2f78d7e44e48ae/dao/trustednode/proposals.go#L22 and rocket-pool/rocketpool-go@6e72501.  It is recommended to further tighten the checks on untrusted information enforcing an expected format of information and reject to interact with nodes/data that does not comply with the expected formats (e.g. email being in an email format, timezone information is a valid timezone, and does not contain extra information, \u2026).  Description  ValidateTimezoneLocation and ValidateDAOMemberEmail are only used to validate user input from the command line. Timezone location information and member email addresses are stored in the smart contract s string storage, e.g., using the setTimezoneLocation function of the RocketNodeManager contract. This function only validates that a minimum length of 4 has been given.  Through direct interaction with the contract, an attacker can submit arbitrary information, which is not validated on the CLI s side. With additional integrations of the Rocketpool smart contracts, the timezone location field may be used by an attacker to inject malicious code (e.g., for cross-site scripting attacks) or injecting false information (e.g. Balance: 1000 RPL or Status: Trusted), which is directly displayed on a user-facing application.  On the command line, control characters such as newline characters can be injected to alter how text is presented to the user, effectively exploiting user trust in the official application.  Examples  rocketpool-go-2.5-Tokenomics/node/node.go:L134-L153  wg.Go(func() error {  var err error  timezoneLocation, err = GetNodeTimezoneLocation(rp, nodeAddress, opts)  return err  })  // Wait for data  if err := wg.Wait(); err != nil {  return NodeDetails{}, err  // Return  return NodeDetails{  Address: nodeAddress,  Exists: exists,  WithdrawalAddress: withdrawalAddress,  TimezoneLocation: timezoneLocation,  }, nil  smartnode-2.5-Tokenomics/rocketpool-cli/odao/members.go:L34-L44  for _, member := range members.Members {  fmt.Printf(\"--------------------\\n\")  fmt.Printf(\"\\n\")  fmt.Printf(\"Member ID:            %s\\n\", member.ID)  fmt.Printf(\"Email address:        %s\\n\", member.Email)  fmt.Printf(\"Joined at block:      %d\\n\", member.JoinedBlock)  fmt.Printf(\"Last proposal block:  %d\\n\", member.LastProposalBlock)  fmt.Printf(\"RPL bond amount:      %.6f\\n\", math.RoundDown(eth.WeiToEth(member.RPLBondAmount), 6))  fmt.Printf(\"Unbonded minipools:   %d\\n\", member.UnbondedValidatorCount)  fmt.Printf(\"\\n\")  Recommendation  Validate user input before storing it on the blockchain. Validate and sanitize stored user tainted data before presenting it. Establish a register of data validation rules (e.g., email format, timezone format, etc.). Reject nodes operating with nodes that do not honor data validation rules.  Validate the correct format of variables (e.g., timezone location, email, name, \u2026) on the storage level (if applicable) and the lowest level of the go library to offer developers a strong foundation to build on and mitigate the risk in future integrations. Furthermore, on-chain validation might not be implemented (due to increased gas consumption) should be mentioned in the developer documentation security section as they need to be handled with special caution by consumer applications. Sanitize output before presenting it to avoid control character injections in terminal applications or other presentation technologies (e.g., SQL or HTML).  Review all usage of the fmt lib (especially Sprintf and string handling/concatenating functions). Ensure only sanitized data can reach this sink. Review the logging library and ensure it is hardened against control character injection by encoding non-printables and CR-LF.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.13 Rocketpool CLI - Various command injection vectors    Addressed", "body": "  Resolution   Initially, the client implemented the suggested fix using   https://github.com/rocket-pool/smartnode/compare/extra-escapes.  Description  Various commands in the Rocketpool CLI make use of the readOutput and printOutput functions. These do not perform sanitization of user-supplied inputs and allow an attacker to supply malicious values which can be used to execute arbitrary commands on the user s system.  Examples  All commands using the Client.readOutput, Client.printOutput and Client.compose functions are affected.  Furthermore, Client.callAPI is used for API-related calls throughout the Rocketpool service. However, it does not validate that the values passed into it are valid API commands. This can lead to arbitrary command execution, also inside the container using docker exec.  Recommendation  Perform strict validation on all user-supplied parameters. If parameter values need to be inserted into a command template string, the %q format string or other restrictive equivalents should be used.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.14 RocketStorage - Risk concentration by giving all registered contracts permissions to change any settings in RocketStorage   ", "body": "  Resolution  The client provided the following statement:  We ve looked at adding access control contracts using namespaces, but the increase in gas usage would be significant and could hinder upgrades.  Description  The ACL for changing settings in the centralized RocketStorage allows any registered contract (listed under contract.exists) to change settings that belong to other parts of the system.  The concern is that if someone finds a way to add their malicious contract to the registered contact list, they will override any setting in the system. The storage is authoritative when checking certain ACLs. Being able to set any value might allow an attacker to gain control of the complete system. Allowing any contract to overwrite other contracts  settings dramatically increases the attack surface.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L32  modifier onlyLatestRocketNetworkContract() {  // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled  if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {  // Make sure the access is permitted to only contracts in our Dapp  require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");  _;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L78-L85  function setAddress(bytes32 _key, address _value) onlyLatestRocketNetworkContract override external {  addressStorage[_key] = _value;  /// @param _key The key for the record  function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external {  uIntStorage[_key] = _value;  Recommendation  Allow contracts to only change settings related to their namespace.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.15 RocketDAOProposals - require a minimum participation quorum for DAO proposals    Addressed", "body": "  Resolution  Addressed by requiring the DAO minimum viable user count as the minium quorum with rocket-pool/rocketpool@11bc18c (in bootstrap mode). The check for the bootstrap mode has since been removed following our remark  [\u2026] the problem here was not so much the bootstrap mode but rather that the dao membership may fall below the recovery mode threshold. The question is, whether it should still be allowed to propose and execute votes if the memberCount at proposal time is below that treshold (e.g. malicious member boots off other members, sends new proposals (quorum required=1), dao membrers rejoin but cannot reject that proposal anymore). Question is if quorum should be at least the recovery treshold.  And the following feedback from the client:  [\u2026] had that as allowed to happen if bootstrap mode was enabled. I ve just disabled the check for bootstrap mode now so that any proposals can t be made if the min member count is below the amount required. This means new members can only be added in this case via the emergency join function before new proposals can be added  Description  If the DAO falls below the minimum viable membership threshold, voting for proposals still continues as DAO proposals do not require a minimum participation quorum. In the worst case, this would allow the last standing DAO member to create a proposal that would be passable with only one vote even if new members would be immediately ready to join via the recovery mode (which has its own risks) as the minimum votes requirement for proposals is set as >0.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L170-L170  require(_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L57-L69  function propose(string memory _proposalMessage, bytes memory _payload) override public onlyTrustedNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrustedProposals\", address(this)) returns (uint256) {  // Load contracts  RocketDAOProposalInterface daoProposal = RocketDAOProposalInterface(getContractAddress('rocketDAOProposal'));  RocketDAONodeTrustedInterface daoNodeTrusted = RocketDAONodeTrustedInterface(getContractAddress('rocketDAONodeTrusted'));  RocketDAONodeTrustedSettingsProposalsInterface rocketDAONodeTrustedSettingsProposals = RocketDAONodeTrustedSettingsProposalsInterface(getContractAddress(\"rocketDAONodeTrustedSettingsProposals\"));  // Check this user can make a proposal now  require(daoNodeTrusted.getMemberLastProposalBlock(msg.sender).add(rocketDAONodeTrustedSettingsProposals.getCooldown()) <= block.number, \"Member has not waited long enough to make another proposal\");  // Record the last time this user made a proposal  setUint(keccak256(abi.encodePacked(daoNameSpace, \"member.proposal.lastblock\", msg.sender)), block.number);  // Create the proposal  return daoProposal.add(msg.sender, 'rocketDAONodeTrustedProposals', _proposalMessage, block.number.add(rocketDAONodeTrustedSettingsProposals.getVoteDelayBlocks()), rocketDAONodeTrustedSettingsProposals.getVoteBlocks(), rocketDAONodeTrustedSettingsProposals.getExecuteBlocks(), daoNodeTrusted.getMemberQuorumVotesRequired(), _payload);  Sidenote: Since a proposals acceptance quorum is recorded on proposal creation, this may lead to another scenario where proposals acceptance quorum may never be reached if members leave the DAO. This would require a re-submission of the proposal.  Recommendation  Do not accept proposals if the member count falls below the minimum DAO membercount threshold.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.16 RocketDAONodeTrustedUpgrade - inconsistent upgrade blacklist    Addressed", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by updating the blacklist.  Description  upgradeContract defines a hardcoded list of contracts that cannot be upgraded because they manage their own settings (statevars) or they hold value in the system.  the list is hardcoded and cannot be extended when new contracts are added via addcontract. E.g. what if another contract holding value is added to the system? This would require an upgrade of the upgrade contract to update the whitelist (gas hungry, significant risk of losing access to the upgrade mechanisms if a bug is being introduced).  a contract named rocketPoolToken is blacklisted from being upgradeable but the system registers no contract called rocketPoolToken. This may be an oversight or artifact of a previous iteration of the code. However, it may allow a malicious group of nodes to add a contract that is not yet in the system which cannot be removed anymore as there is no removeContract functionality and upgradeContract to override the malicious contract will fail due to the blacklist.  Note that upgrading RocketTokenRPL requires an account balance migration as contracts in the system may hold value in RPL (e.g. a lot in AuctionManager) that may vanish after an upgrade. The contract is not exempt from upgrading. A migration may not be easy to perform as the system cannot be paused to e.g. snapshot balances.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L41-L49  function _upgradeContract(string memory _name, address _contractAddress, string memory _contractAbi) internal {  // Check contract being upgraded  bytes32 nameHash = keccak256(abi.encodePacked(_name));  require(nameHash != keccak256(abi.encodePacked(\"rocketVault\")),        \"Cannot upgrade the vault\");  require(nameHash != keccak256(abi.encodePacked(\"rocketPoolToken\")),    \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"rocketTokenRETH\")),     \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"rocketTokenNETH\")), \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"casperDeposit\")),      \"Cannot upgrade the casper deposit contract\");  // Get old contract address & check contract exists  Recommendation  Consider implementing a whitelist of contracts that are allowed to be upgraded instead of a more error-prone blacklist of contracts that cannot be upgraded.  Provide documentation that outlines what contracts are upgradeable and why.  Create a process to verify the blacklist before deploying/operating the system.  Plan for migration paths when upgrading contracts in the system  Any proposal that reaches the upgrade contract must be scrutinized for potential malicious activity (e.g. as any registered contract can directly modify storage or may contain subtle backdoors. Upgrading without performing a thorough security inspection may easily put the DAO at risk)  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.17 RocketDAONodeTrustedActions - member cannot be kicked if the vault does not hold enough RPL to cover the bond    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by returning the bond if enough RPL is in the treasury and else continue without returning the bond. This way the member kick action does not block and the member can be kicked regardless of the RPL balance.  Description  If a DAO member behaves badly other DAO members may propose the node be evicted from the DAO. If for some reason, RocketVault does not hold enough RPL to pay back the DAO member bond actionKick will throw. The node is not evicted.  Now this is a somewhat exotic scenario as the vault should always hold the bond for the members in the system. However, if the node was kicked for stealing RPL (e.g. passing an upgrade proposal to perform an attack) it might be impossible to execute the eviction.  Recommendation  Ensure that there is no way a node can influence a succeeded kick proposal to fail. Consider burning the bond (by keeping it) as there is a reason for evicting the node or allow them to redeem it in a separate step.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.18 RocketMinipoolStatus - DAO Membership changes can result in votes getting stuck    ", "body": "  Resolution   This issue has been fixed in PR   https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/204 by introducing a public method that allows anyone to manually trigger a DAO consensus threshold check and a subsequent balance update in case the issue s example scenario occurs.  Description  Changes in the DAO s trusted node members are reflected in the RocketDAONodeTrusted.getMemberCount() function. When compared with the vote on consensus threshold, a DAO-driven decision is made, e.g., when updating token price feeds and changing Minipool states.  Especially in the early phase of the DAO, the functions below can get stuck as execution is restricted to DAO members who have not voted yet. Consider the following scenario:  The DAO consists of five members  Two members vote to make a Minipool withdrawable  The other three members are inactive, the community votes, and they get kicked from the DAO  The two remaining members have no way to change the Minipool state now. All method calls to trigger the state update fails because the members have already voted before.  Note: votes of members that are kicked/leave are still count towards the quorum!  Examples  Setting a Minipool into the withdrawable state:  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L62-L65  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  setMinipoolWithdrawable(_minipoolAddress, _stakingStartBalance, _stakingEndBalance);  Submitting a block s network balances:  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L94-L97  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updateBalances(_block, _totalEth, _stakingEth, _rethSupply);  Submitting a block s RPL price information:  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updatePrices(_block, _rplPrice);  Recommendation  The conditional check and update of price feed information, Minipool state transition, etc., should be externalized into a separate public function. This function is also called internally in the existing code. In case the DAO gets into the scenario above, anyone can call the function to trigger a reevaluation of the condition with updated membership numbers and thus get the process unstuck.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.19 Trusted/Oracle-Nodes can vote multiple times for different outcomes ", "body": "  Description  Trusted/oracle nodes submit various ETH2 observations to the RocketPool contracts. When 51% of nodes submitted the same observation, the result is stored in the contract. However, while it is recorded that a node already voted for a specific minipool (being withdrawable & balance) or block (price/balance), a re-submission with different parameters for the same minipool/block is not rejected.  Since the oracle values should be distinct, clear, and there can only be one valid value, it should not be allowed for trusted nodes to change their mind voting for multiple different outcomes within one block or one minipool  Examples  RocketMinipoolStatus - a trusted node can submit multiple different results for one minipool  Note that setBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress)), true);  is recorded but never checked. (as for the other two instances)  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L48-L57  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress, _stakingStartBalance, _stakingEndBalance));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.count\", _minipoolAddress, _stakingStartBalance, _stakingEndBalance));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  RocketNetworkBalances - a trusted node can submit multiple different results for the balances at a specific block  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L80-L92  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, _block, _totalEth, _stakingEth, _rethSupply));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.balances.submitted.count\", _block, _totalEth, _stakingEth, _rethSupply));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, _block)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  // Emit balances submitted event  emit BalancesSubmitted(msg.sender, _block, _totalEth, _stakingEth, _rethSupply, block.timestamp);  // Check submission count & update network balances  RocketNetworkPrices - a trusted node can submit multiple different results for the price at a specific block  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L55-L67  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, _block, _rplPrice));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.prices.submitted.count\", _block, _rplPrice));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, _block)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  // Emit prices submitted event  emit PricesSubmitted(msg.sender, _block, _rplPrice, block.timestamp);  // Check submission count & update network prices  Recommendation  Only allow one vote per minipool/block. Don t give nodes the possibility to vote multiple times for different outcomes.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.20 RocketTokenNETH - Pot. discrepancy between minted tokens and deposited collateral    ", "body": "  Resolution  This issue is obsoleted by the fact that the nETH contract was removed completely. The client provided the following statement:  nETH has been removed completely.  Description  The nETH token is paid to node operators when minipool becomes withdrawable. nETH is supposed to be backed by ETH 1:1. However, in most cases, this will not be the case.  The nETH minting and deposition of collateral happens in two different stages of a minipool. nETH is minted in the minipool state transition from Staking to Withdrawable when the trusted/oracle nodes find consensus on the fact that the minipool became withdrawable (submitWinipoolWithdrawable).  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L63-L65  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  setMinipoolWithdrawable(_minipoolAddress, _stakingStartBalance, _stakingEndBalance);  When consensus is found on the state of the minipool, nETH tokens are minted to the minipool address according to the withdrawal amount observed by the trusted/oracle nodes. At this stage, ETH backing the newly minted nETH was not yet provided.  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L80-L87  uint256 nodeAmount = getMinipoolNodeRewardAmount(  minipool.getNodeFee(),  userDepositBalance,  minipool.getStakingStartBalance(),  minipool.getStakingEndBalance()  );  // Mint nETH to minipool contract  if (nodeAmount > 0) { rocketTokenNETH.mint(nodeAmount, _minipoolAddress); }  The minipool.receive() function receives the ETH  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L109-L112  receive() external payable {  (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature(\"receiveValidatorBalance()\"));  if (!success) { revert(getRevertMessage(data)); }  and forwards it to minipooldelegate.receiveValidatorBalance  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L227-L231  require(msg.sender == rocketDAOProtocolSettingsNetworkInterface.getSystemWithdrawalContractAddress(), \"The minipool's validator balance can only be sent by the eth1 system withdrawal contract\");  // Set validator balance withdrawn status  validatorBalanceWithdrawn = true;  // Process validator withdrawal for minipool  rocketNetworkWithdrawal.processWithdrawal{value: msg.value}();  Which calculates the nodeAmount based on the ETH received and submits it as collateral to back the previously minted nodeAmount of nETH.  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkWithdrawal.sol:L46-L60  uint256 totalShare = rocketMinipoolManager.getMinipoolWithdrawalTotalBalance(msg.sender);  uint256 nodeShare = rocketMinipoolManager.getMinipoolWithdrawalNodeBalance(msg.sender);  uint256 userShare = totalShare.sub(nodeShare);  // Get withdrawal amounts based on shares  uint256 nodeAmount = 0;  uint256 userAmount = 0;  if (totalShare > 0) {  nodeAmount = msg.value.mul(nodeShare).div(totalShare);  userAmount = msg.value.mul(userShare).div(totalShare);  // Set withdrawal processed status  rocketMinipoolManager.setMinipoolWithdrawalProcessed(msg.sender);  // Transfer node balance to nETH contract  if (nodeAmount > 0) { rocketTokenNETH.depositRewards{value: nodeAmount}(); }  // Transfer user balance to rETH contract or deposit pool  Looking at how the nodeAmount of nETH that was minted was calculated and comparing it to how nodeAmount of ETH is calculated, we can observe the following:  the nodeAmount of nETH minted is an absolute number of tokens based on the rewards observed by the trusted/oracle nodes. the nodeAmount is stored in the storage and later used to calculate the collateral deposit in a later step.  the nodeAmount calculated when depositing the collateral is first assumed to be a nodeShare (line 47), while it is actually an absolute number. the nodeShare is then turned into a nodeAmount relative to the ETH supplied to the contract.  Due to rounding errors, this might not always exactly match the nETH minted (see https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/26).  The collateral calculation is based on the ETH value provided to the contract. If this value does not exactly match what was reported by the oracle/trusted nodes when minting nETH, less/more collateral will be provided.  Note: excess collateral will be locked in the nETH contract as it is unaccounted for in the nETH token contract and therefore cannot be redeemed. Note: providing less collateral will go unnoticed and mess up the 1:1 nETH:ETH peg. In the worst case, there will be less nETH than ETH. Not everybody will be able to redeem their ETH.  Note: keep in mind that the receive() function might be subject to gas restrictions depending on the implementation of the withdrawal contract (.call() vs. .transfer())  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L201-L210  uint256 nethBalance = rocketTokenNETH.balanceOf(address(this));  if (nethBalance > 0) {  // Get node withdrawal address  RocketNodeManagerInterface rocketNodeManager = RocketNodeManagerInterface(getContractAddress(\"rocketNodeManager\"));  address nodeWithdrawalAddress = rocketNodeManager.getNodeWithdrawalAddress(nodeAddress);  // Transfer  require(rocketTokenNETH.transfer(nodeWithdrawalAddress, nethBalance), \"nETH balance was not successfully transferred to node operator\");  // Emit nETH withdrawn event  emit NethWithdrawn(nodeWithdrawalAddress, nethBalance, block.timestamp);  For reference, depositRewards (providing collateral) and mint are not connected at all, hence the risk of nETH being an undercollateralized token.  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenNETH.sol:L28-L42  function depositRewards() override external payable onlyLatestContract(\"rocketNetworkWithdrawal\", msg.sender) {  // Emit ether deposited event  emit EtherDeposited(msg.sender, msg.value, block.timestamp);  // Mint nETH  // Only accepts calls from the RocketMinipoolStatus contract  function mint(uint256 _amount, address _to) override external onlyLatestContract(\"rocketMinipoolStatus\", msg.sender) {  // Check amount  require(_amount > 0, \"Invalid token mint amount\");  // Update balance & supply  _mint(_to, _amount);  // Emit tokens minted event  emit TokensMinted(_to, _amount, block.timestamp);  Recommendation  It looks like nETH might not be needed at all, and it should be discussed if the added complexity of having a potentially out-of-sync nETH token contract is necessary and otherwise remove it from the contract system as the nodeAmount of ETH can directly be paid out to the withdrawalAddress in the receiveValidatorBalance or withdraw transitions.  If nETH cannot be removed, consider minting nodeAmount of nETH directly to withdrawalAddress on withdraw instead of first minting uncollateralized tokens. This will also reduce the gas footprint of the Minipool.  Ensure that the initial nodeAmount calculation matches the minted nETH and deposited to the contract as collateral (absolute amount vs. fraction).  Enforce that nETH requires collateral to be provided when minting tokens.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.21 RocketMiniPoolDelegate - on destroy() leftover ETH is sent to RocketVault where it cannot be recovered    ", "body": "  Resolution  Leftover ETH is now sent to the node operator address as expected.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/rocketpool-rp3.0-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol#L294  Description  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L314-L321  // Destroy the minipool  function destroy() private {  // Destroy minipool  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  rocketMinipoolManager.destroyMinipool();  // Self destruct & send any remaining ETH to vault  selfdestruct(payable(getContractAddress(\"rocketVault\")));  Recommendation  Implement means to recover and reuse ETH that was forcefully sent to the contract by MiniPool instances.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.22 RocketDAO - personally identifiable member information (PII) stored on-chain   ", "body": "  Resolution  Acknowledged with the following statement:  This is by design, need them to be publicly accountable. We ll advise their node should not be running on the same machine as their email software though.  Description  Like a DAO user s e-mail address, PII is stored on-chain and can, therefore, be accessed by anyone. This may allow de-pseudonymize users (and correlate Ethereum addresses to user email addresses) and be used for spamming or targeted phishing campaigns putting the DAO users at risk.  Examples  rocketpool-go-2.5-Tokenomics/dao/trustednode/dao.go:L173-L183  // Return  return MemberDetails{  Address: memberAddress,  Exists: exists,  ID: id,  Email: email,  JoinedBlock: joinedBlock,  LastProposalBlock: lastProposalBlock,  RPLBondAmount: rplBondAmount,  UnbondedValidatorCount: unbondedValidatorCount,  }, nil  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L110-L112  function getMemberEmail(address _nodeAddress) override public view returns (string memory) {  return getString(keccak256(abi.encodePacked(daoNameSpace, \"member.email\", _nodeAddress)));  Recommendation  Avoid storing PII on-chain where it is readily available for anyone.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.23 Rocketpool CLI - Insecure SSH HostKeyCallback    ", "body": "  Resolution  A proper host key callback function to validate the remote party s authenticity is now defined.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/smartnode-1.0.0-rc1/shared/services/rocketpool/client.go#L114-L117  Description  The SSH client factory returns instances that have an insecure HostKeyCallback set. This means that SSH servers  public key will not be validated and thus initialize a potentially insecure connection. The function should not be used for production code.  Examples  smartnode-2.5-Tokenomics/shared/services/rocketpool/client.go:L87  HostKeyCallback: ssh.InsecureIgnoreHostKey(),  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.24 Deployment - Docker containers running as root ", "body": "  Description  By default, Docker containers run commands as the root user. This means that there is little to no resistance for an attacker who has managed to break into the container and execute commands. This effectively negates file permissions already set into the system, such as storing wallet-related information with 0600 as an attacker will most likely drop into the container as root already.  Examples  Missing USER instructions affect both SmartNode Dockerfiles:  smartnode-2.5-Tokenomics/docker/rocketpool-dockerfile:L25-L36  # Start from ubuntu image  FROM ubuntu:20.10  # Install OS dependencies  RUN apt-get update && apt-get install -y ca-certificates  # Copy binary  COPY --from=builder /go/bin/rocketpool /go/bin/rocketpool  # Container entry point  ENTRYPOINT [\"/go/bin/rocketpool\"]  smartnode-2.5-Tokenomics/docker/rocketpool-pow-proxy-dockerfile:L24-L35  # Start from ubuntu image  FROM ubuntu:20.10  # Install OS dependencies  RUN apt-get update && apt-get install -y ca-certificates  # Copy binary  COPY --from=builder /go/bin/rocketpool-pow-proxy /go/bin/rocketpool-pow-proxy  # Container entry point  ENTRYPOINT [\"/go/bin/rocketpool-pow-proxy\"]  Recommendation  In the Dockerfiles, create an unprivileged user and use the USER instruction to switch. Only then, the entrypoint launching the SmartNode or the POW Proxy should be defined.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.25 RocketPoolMinipool - should check for address(0x0)    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by changing requiring that the contract address is not  Description  The two implementations for getContractAddress() in Minipool/Delegate are not checking whether the requested contract s address was ever set before. If it were never set, the method would return address(0x0), which would silently make all delegatecalls succeed without executing any code. In contrast, RocketBase.getContractAddress() fails if the requested contract is not known.  It should be noted that this can happen if rocketMinipoolDelegate is not set in global storage, or it was cleared afterward, or if _rocketStorageAddress points to a contract that implements a non-throwing fallback function (may not even be storage at all).  Examples  Missing checks  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L170-L172  function getContractAddress(string memory _contractName) private view returns (address) {  return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L91-L93  function getContractAddress(string memory _contractName) private view returns (address) {  return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  Checks implemented  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketBase.sol:L84-L92  function getContractAddress(string memory _contractName) internal view returns (address) {  // Get the current contract address  address contractAddress = getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  // Check it  require(contractAddress != address(0x0), \"Contract not found\");  // Return  return contractAddress;  Recommendation  Similar to RocketBase.getContractAddress() require that the contract is set.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.26 RocketDAONodeTrustedAction - ambiguous event emitted in actionChallengeDecide    ", "body": "  Resolution  Instead of emitting an event even though the challenge period has not passed yet, the function call will now revert if the challenge window has not passed yet.  Description  actionChallengeDecide succeeds and emits challengeSuccess=False in case the challenged node defeats the challenge. It also emits the same event if another node calls actionChallengeDecided before the refute window passed. This ambiguity may make a defeated challenge indistinguishable from a challenge that was attempted to be decided too early (unless the component listening for the event also checks the refute window).  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L244-L260  // Allow the challenged member to refute the challenge at anytime. If the window has passed and the challenge node does not run this method, any member can decide the challenge and eject the absent member  // Is it the node being challenged?  if(_nodeAddress == msg.sender) {  // Challenge is defeated, node has responded  deleteUint(keccak256(abi.encodePacked(daoNameSpace, \"member.challenged.block\", _nodeAddress)));  }else{  // The challenge refute window has passed, the member can be ejected now  if(getUint(keccak256(abi.encodePacked(daoNameSpace, \"member.challenged.block\", _nodeAddress))).add(rocketDAONodeTrustedSettingsMembers.getChallengeWindow()) < block.number) {  // Node has been challenged and failed to respond in the given window, remove them as a member and their bond is burned  _memberRemove(_nodeAddress);  // Challenge was successful  challengeSuccess = true;  // Log it  emit ActionChallengeDecided(_nodeAddress, msg.sender, challengeSuccess, block.timestamp);  Recommendation  Avoid ambiguities when emitting events. Consider throwing an exception in the else branch if the refute window has not passed yet (minimal gas savings; it s clear that the call failed; other components can rely on the event only being emitted if there was a decision.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.27 RocketDAOProtocolProposals, RocketDAONodeTrustedProposals - unused enum ProposalType    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the unused code.  Description  The enum ProposalType is defined but never used.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L29-L35  enum ProposalType {  Invite,             // Invite a registered node to join the trusted node DAO  Leave,              // Leave the DAO  Replace,            // Replace a current trusted node with a new registered node, they take over their bond  Kick,               // Kick a member from the DAO with optional penalty applied to their RPL deposit  Setting             // Change a DAO setting (Quorum threshold, RPL deposit size, voting periods etc)  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/RocketDAOProtocolProposals.sol:L28-L31  enum ProposalType {  Setting             // Change a DAO setting (Node operator min/max fees, inflation rate etc)  Recommendation  Remove unnecessary code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.28 RocketDaoNodeTrusted - Unused events    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the unused code.  Description  The MemberJoined MemberLeave events are not used within RocketDaoNodeTrusted.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L19-L23  // Events  event MemberJoined(address indexed _nodeAddress, uint256 _rplBondAmount, uint256 time);  event MemberLeave(address indexed _nodeAddress, uint256 _rplBondAmount, uint256 time);  Recommendation  Consider removing the events. Note: RocketDAONodeTrustedAction is emitting ActionJoin and ActionLeave event.s  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.29 RocketDAOProposal - expired, and defeated proposals can be canceled    ", "body": "  Resolution  Proposals can now only be cancelled if they are pending or active.  Description  The method emits an event that might trigger other components to perform actions.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L155-L159  } else {  // Check the votes, was it defeated?  // if (votesFor <= votesAgainst || votesFor < getVotesRequired(_proposalID))  return ProposalState.Defeated;  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L239-L250  function cancel(address _member, uint256 _proposalID) override public onlyDAOContract(getDAO(_proposalID)) {  // Firstly make sure this proposal that hasn't already been executed  require(getState(_proposalID) != ProposalState.Executed, \"Proposal has already been executed\");  // Make sure this proposal hasn't already been successful  require(getState(_proposalID) != ProposalState.Succeeded, \"Proposal has already succeeded\");  // Only allow the proposer to cancel  require(getProposer(_proposalID) == _member, \"Proposal can only be cancelled by the proposer\");  // Set as cancelled now  setBool(keccak256(abi.encodePacked(daoProposalNameSpace, \"cancelled\", _proposalID)), true);  // Log it  emit ProposalCancelled(_proposalID, _member, block.timestamp);  Recommendation  Preserve the true outcome. Do not allow to cancel proposals that are already in an end-state like canceled, expired, defeated.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.30 RocketDAOProposal - preserve the proposals correct state after expiration    ", "body": "  Resolution   Proposals that have been defeated now will show up as such even when expired. The new default value is   Description  The state of proposals is resolved to give a preference to a proposal being expired over the actual result which may be defeated. The preference for a proposal s status is checked in order: cancelled? -> executed? -> expired? -> succeeded? -> pending? -> active? -> defeated (default)  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L137-L159  if (getCancelled(_proposalID)) {  // Cancelled by the proposer?  return ProposalState.Cancelled;  // Has it been executed?  } else if (getExecuted(_proposalID)) {  return ProposalState.Executed;  // Has it expired?  } else if (block.number >= getExpires(_proposalID)) {  return ProposalState.Expired;  // Vote was successful, is now awaiting execution  } else if (votesFor >= getVotesRequired(_proposalID)) {  return ProposalState.Succeeded;  // Is the proposal pending? Eg. waiting to be voted on  } else if (block.number <= getStart(_proposalID)) {  return ProposalState.Pending;  // The proposal is active and can be voted on  } else if (block.number <= getEnd(_proposalID)) {  return ProposalState.Active;  } else {  // Check the votes, was it defeated?  // if (votesFor <= votesAgainst || votesFor < getVotesRequired(_proposalID))  return ProposalState.Defeated;  Recommendation  consider checking for voteAgainst explicitly and return defeated instead of expired if a proposal was defeated and is queried after expiration. Preserve the actual proposal result.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.31 RocketRewardsPool - registerClaimer should check if a node is already disabled before decrementing rewards.pool.claim.interval.claimers.total.next    ", "body": "  Resolution   In the case a submitted   Description  The other branch in registerClaimer does not check whether the provided _claimerAddress is already disabled (or invalid). This might lead to inconsistencies where rewards.pool.claim.interval.claimers.total.next is decremented because the caller provided an already deactivated address.  This issue is flagged as minor since we have not found an exploitable version of this issue in the current codebase. However, we recommend safeguarding the implementation instead of relying on the caller to provide sane parameters. Registered Nodes cannot unregister, and Trusted Nodes are unregistered when they leave.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/rewards/RocketRewardsPool.sol:L296-L316  function registerClaimer(address _claimerAddress, bool _enabled) override external onlyClaimContract {  // The name of the claiming contract  string memory contractName = getContractName(msg.sender);  // Record the block they are registering at  uint256 registeredBlock = 0;  // How many users are to be included in next interval  uint256 claimersIntervalTotalUpdate = getClaimingContractUserTotalNext(contractName);  // Ok register  if(_enabled) {  // Make sure they are not already registered  require(getClaimingContractUserRegisteredBlock(contractName, _claimerAddress) == 0, \"Claimer is already registered\");  // Update block number  registeredBlock = block.number;  // Update the total registered claimers for next interval  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.interval.claimers.total.next\", contractName)), claimersIntervalTotalUpdate.add(1));  }else{  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.interval.claimers.total.next\", contractName)), claimersIntervalTotalUpdate.sub(1));  // Save the registered block  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.contract.registered.block\", contractName, _claimerAddress)), registeredBlock);  Recommendation  Ensure that getClaimingContractUserRegisteredBlock(contractName, _claimerAddress) returns !=0 before decrementing the .total.next.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.32 RocketNetworkPrices - Price feed update lacks block number sanity check    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by only allowing price submissions for blocks in the range of  Description  Trusted nodes submit the RPL price feed. The function is called specifying a block number and the corresponding RPL price for that block. If a DAO vote goes through for that block-price combination, it is written to storage. In the unlikely scenario that a vote confirms a very high block number such as uint(-1), all future price updates will fail due to the require check below.  This issue becomes less likely the more active members the DAO has. Thus, it s considered a minor issue that mainly affects the initial bootstrapping process.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L53-L54  // Check block  require(_block > getPricesBlock(), \"Network prices for an equal or higher block are set\");  Recommendation  The function s _block parameter should be checked to prevent large block numbers from being submitted. This check could, e.g., specify that node operators are only allowed to submit price updates for a maximum of x blocks ahead of block.number.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.33 RocketDepositPool - Potential gasDoS in assignDeposits   ", "body": "  Resolution  The client acknowledges this issue.  Description  assignDeposits seems to be a gas heavy function, with many external calls in general, and few of them are inside the for loop itself. By default, rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments() returns 2, which is not a security concern. Through a DAO vote, the settings key deposit.assign.maximum can be set to a value that exhausts the block gas limit and effectively deactivates the deposit assignment process.  rocketpool-2.5-Tokenomics-updates/contracts/contract/deposit/RocketDepositPool.sol:L115-L116  for (uint256 i = 0; i < rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments(); ++i) {  // Get & check next available minipool capacity  Recommendation  The rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments() return value could be cached outside the loop. Additionally, a check should be added that prevents unreasonably high values.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.34 RocketNetworkWithdrawal - ETH dust lockup due to rounding errors    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by calculating  Description  There s a potential ETH dust lockup when processing a withdrawal due to rounding errors when performing a division.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkWithdrawal.sol:L46-L55  uint256 totalShare = rocketMinipoolManager.getMinipoolWithdrawalTotalBalance(msg.sender);  uint256 nodeShare = rocketMinipoolManager.getMinipoolWithdrawalNodeBalance(msg.sender);  uint256 userShare = totalShare.sub(nodeShare);  // Get withdrawal amounts based on shares  uint256 nodeAmount = 0;  uint256 userAmount = 0;  if (totalShare > 0) {  nodeAmount = msg.value.mul(nodeShare).div(totalShare);  userAmount = msg.value.mul(userShare).div(totalShare);  Recommendation  Calculate userAmount as msg.value - nodeAmount instead. This should also save some gas.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.35 RocketAuctionManager - calcBase should be declared constant    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by declaring  Description  Declaring the same constant value calcBase multiple times as local variables to some methods in RocketAuctionManager carries the risk that if that value is ever updated, one of the value assignments might be missed. It is therefore highly recommended to reduce duplicate code and declare the value as a public constant. This way, it is clear that the same calcBase is used throughout the contract, and there is a single point of change in case it ever needs to be changed.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L136-L139  function getLotPriceByTotalBids(uint256 _index) override public view returns (uint256) {  uint256 calcBase = 1 ether;  return calcBase.mul(getLotTotalBidAmount(_index)).div(getLotTotalRPLAmount(_index));  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L151-L154  function getLotClaimedRPLAmount(uint256 _index) override public view returns (uint256) {  uint256 calcBase = 1 ether;  return calcBase.mul(getLotTotalBidAmount(_index)).div(getLotCurrentPrice(_index));  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L173-L174  // Calculation base value  uint256 calcBase = 1 ether;  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L216-L217  uint256 bidAmount = msg.value;  uint256 calcBase = 1 ether;  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L247-L249  // Calculate RPL claim amount  uint256 calcBase = 1 ether;  uint256 rplAmount = calcBase.mul(bidAmount).div(currentPrice);  Recommendation  Consider declaring calcBase as a private const state var instead of re-declaring it with the same value in multiple, multiple functions. Constant, literal state vars are replaced in a preprocessing step and do not require significant additional gas when accessed than normal state vars.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.36 RocketDAO* - daoNamespace is missing a trailing dot; should be declared constant/immutable    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by adding a trailing dot to the  Description  string private daoNameSpace = 'dao.trustednodes' is missing a trailing dot, or else there s no separator when concatenating the namespace with the vars.  Examples  requests dao.trustednodesmember.index instead of dao.trustednodes.member.index  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L83-L86  function getMemberAt(uint256 _index) override public view returns (address) {  AddressSetStorageInterface addressSetStorage = AddressSetStorageInterface(getContractAddress(\"addressSetStorage\"));  return addressSetStorage.getItem(keccak256(abi.encodePacked(daoNameSpace, \"member.index\")), _index);  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L32-L33  // The namespace for any data stored in the trusted node DAO (do not change)  string private daoNameSpace = 'dao.trustednodes';  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L22-L26  // Calculate using this as the base  uint256 private calcBase = 1 ether;  // The namespace for any data stored in the trusted node DAO (do not change)  string private daoNameSpace = 'dao.trustednodes';  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/RocketDAOProtocol.sol:L12-L13  // The namespace for any data stored in the network DAO (do not change)  string private daoNameSpace = 'dao.protocol';  Recommendation  Remove the daoNameSpace and add the prefix to the respective variables directly.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.37 RocketVault - consider rejecting zero amount deposit/withdrawal requests    ", "body": "  Resolution  Addressed in branch rp3.0-updates (rocket-pool/rocketpool@b424ca1) by requiring that ETH and tokenAmounts are not zero.  Note that components that used to raise no exception when attempting to deposit/withdraw/transfer zero amount tokens/ETH may now throw which can be used to block certain functionalities (slashAmount==0).  The client provided the following statement:  We ll double check this. Currently the only way a slashAmount is 0 is if we allow node operators to not stake RPL (min 10% required currently). Though there isn t a check for 0 in the slash function atm, I ll add one now just as a safety check.  Description  Consider disallowing zero amount token transfers unless the system requires this to work. In most cases, zero amount token transfers will emit an event (that potentially triggers off-chain components). In some cases, they allow the caller without holding any balance to call back to themselves (pot. reentrancy) or the caller provided token address.  depositEther allows to deposit zero ETH  emits EtherDeposited  withdrawEther allows to withdraw zero ETH  calls back to withdrawer (msg.sender)! emits EtherWithdrawn  (depositToken checks for amount >0)  withdrawToken allows zero amount token withdrawals  calls into user provided (actually a network contract) tokenAddress) emits TokenWithdrawn  transferToken allows zero amount token transfers  emits TokenTransfer  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L50-L57  function depositEther() override external payable onlyLatestNetworkContract {  // Get contract key  bytes32 contractKey = keccak256(abi.encodePacked(getContractName(msg.sender)));  // Update contract balance  etherBalances[contractKey] = etherBalances[contractKey].add(msg.value);  // Emit ether deposited event  emit EtherDeposited(contractKey, msg.value, block.timestamp);  Recommendation  Zero amount transfers are no-operation calls in most cases and should be avoided. However, as all vault actions are authenticated (to registered system contracts), the risk of something going wrong is rather low. Nevertheless, it is recommended to deny zero amount transfers to avoid running code unnecessarily (gas consumption), emitting unnecessary events, or potentially call back to callers/token address for ineffective transfers.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.38 RocketVault - methods returning static return values and unchecked return parameters    ", "body": "  Resolution  The unused boolean return values have been removed and reverts have been introduced instead.  Description  The Token* methods in RocketVault either throw or return true, but they can never return false. If the method fails, it will always throw. Therefore, it is questionable if the static return value is needed at all. Furthermore, callees are in most cases not checking the return value of  Examples  static return value true  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L93-L96  // Emit token transfer  emit TokenDeposited(contractKey, _tokenAddress, _amount, block.timestamp);  // Done  return true;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L113-L115  emit TokenWithdrawn(contractKey, _tokenAddress, _amount, block.timestamp);  // Done  return true;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L134-L137  // Emit token withdrawn event  emit TokenTransfer(contractKeyFrom, contractKeyTo, _tokenAddress, _amount, block.timestamp);  // Done  return true;  return value not checked  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L149-L150  rocketVault.depositToken(\"rocketNodeStaking\", rplTokenAddress, _amount);  // Update RPL stake amounts & node RPL staked block  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L252-L252  rocketVault.withdrawToken(msg.sender, getContractAddress(\"rocketTokenRPL\"), rplAmount);  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L172-L172  rocketVault.withdrawToken(msg.sender, getContractAddress(\"rocketTokenRPL\"), _amount);  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L193-L193  rocketVault.transferToken(\"rocketAuctionManager\", getContractAddress(\"rocketTokenRPL\"), rplSlashAmount);  Recommendation  Define a clear interface for these functions. Remove the static return value in favor of having the method throw on failure (which is already the current behavior).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.39 Deployment - Overloaded Ubuntu base image ", "body": "  Description  The SmartNode and the corresponding proxy Dockerfiles base their builds on the ubuntu:20.10 image. This image introduces many unrelated tools that significantly increase the container s attack surface and the tools an attacker has at their disposal once they have gained access to the container. Some of these tools include:  apt  bash/sh  perl  Examples  smartnode-2.5-Tokenomics/docker/rocketpool-dockerfile:L26-L26  FROM ubuntu:20.10  smartnode-2.5-Tokenomics/docker/rocketpool-pow-proxy-dockerfile:L25-L25  FROM ubuntu:20.10  Recommendation  Consider using a smaller and more restrictive base image such as Alpine. Additionally, AppArmor or Seccomp policies should be used to prevent unexpected and potentially malicious activities during the container s lifecycle. As an illustrative example, a SmartNode container does not need to load/unload kernel modules or loading a BPF to capture network traffic.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.40 RocketMinipoolDelegate - enforce that the delegate contract cannot be called directly    ", "body": "  Resolution   Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the constructor and therefore the initialization code from the RocketMinipoolDelegate contract. The contract cannot be used directly anymore as all relevant methods are decorated  Description  This contract is not meant to be consumed directly and will only be delegate called from Minipool. Being able to call it directly might even create the problem that, in the worst case, someone might be able to selfdestruct the contract rendering all other contracts that link to it dysfunctional. This might even not be easily detectable because delegatecall to an EOA will act as a NOP.  The access control checks on the methods currently prevent methods from being called directly on the delegate. They require state variables to be set correctly, or the delegate is registered as a valid minipool in the system. Both conditions are improbable to be fulfilled, hence, mitigation any security risk. However, it looks like this is more of a side-effect than a design decision, and we would recommend not explicitly stating that the delegate contract cannot be used directly.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L65-L70  constructor(address _rocketStorageAddress) {  // Initialise RocketStorage  require(_rocketStorageAddress != address(0x0), \"Invalid storage address\");  rocketStorage = RocketStorageInterface(_rocketStorageAddress);  Recommendation  Remove the initialization from the constructor in the delegate contract. Consider adding a flag that indicates that the delegate contract is initialized and only set in the Minipool contract and not in the logic contract (delegate). On calls, check that the contract is initialized.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "5.1 TimeLock spam prevention can be bypassed    Addressed", "body": "  Resolution   This was addressed in   commit aa6fc49fbf3230d7f02956b33a3150c6885ee93f by parsing the input evm script and ensuring only a single external call is made. Additionally,  commit 453179e98159413d38196b6a5373cdd729483567 added  Description  The TimeLock app is a forwarder that requires users to lock some token before forwarding an EVM callscript. Its purpose is to introduce a  spam penalty  to hamper repeat actions within an Aragon org. In the context of a Dandelion org, this spam penalty is meant to stop users from repeatedly creating votes in DandelionVoting, as subsequent votes are buffered by a configurable number of blocks (DandelionVoting.bufferBlocks). Spam prevention is important, as the more votes are buffered, the longer it takes before  non-spam  votes are able to be executed.  By allowing arbitrary calls to be executed, the TimeLock app opens several potential vectors for bypassing spam prevention.  Examples  Using a callscript to transfer locked tokens to the sender  By constructing a callscript that executes a call to the lock token address, the sender execute calls to the lock token on behalf of TimeLock. Any function can be executed, making it possible to not only transfer  locked  tokens back to the sender, but also steal other users  locked tokens by way of transfer.  Using a batched callscript to call DandelionVoting.newVote repeatedly  Callscripts can be batched, meaning they can execute multiple calls before finishing. Within a Dandelion org, the spam prevention mechanism is used for the DandelionVoting.newVote function. A callscript that batches multiple calls to this function can execute newVote several times per call to TimeLock.forward. Although multiple new votes are created, only one spam penalty is incurred, making it trivial to extend the buffer imposed on  non-spam  votes.  Using a callscript to re-enter TimeLock and forward or withdrawAllTokens to itself  A callscript can be used to re-enter TimeLock.forward, as well as any other TimeLock functions. Although this may not be directly exploitable, it does seem unintentional that many of the TimeLock contract functions are accessible to itself in this manner.  Recommendation  Add the TimeLock contract s own address to the evmscript blacklist  Add the TimeLock lock token address to the evmscript blacklist  To fix spamming through batched callscripts, one option is to have users pass in a destination and calldata, and manually perform a call. Alternatively, CallsScript can be forked and altered to only execute a single external call to a single destination.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "5.2 Passing duplicate tokens to Redemptions and TokenRequest may have unintended consequences    Addressed", "body": "  Resolution  This was addressed in Redemptions commit 2b0034206a5b9cdf239da7a51900e89d9931554f by checking redeemableTokenAdded[token] == false for each subsequent token added during initialization. Note that ordering is not enforced.  Additionally, the issue in TokenRequest was addressed in commit eb4181961093439f142f2e74eb706b7f501eb5c0 by requiring that each subsequent token added during initialization has a value strictly greater than the previous token added.  Description  Both Redemptions and TokenRequest are initialized with a list of acceptable tokens to use with each app. For Redemptions, the list of tokens corresponds to an organization s treasury assets. For TokenRequest, the list of tokens corresponds to tokens accepted for payment to join an organization. Neither contract makes a uniqueness check on input tokens during initialization, which can lead to unintended behavior.  Examples  In Redemptions, each of an organization s assets are redeemed according to the sender s proportional ownership in the org. The redemption process iterates over the redeemableTokens list, paying out the sender their proportion of each token listed:  code/redemptions-app/contracts/Redemptions.sol:L112-L121  for (uint256 i = 0; i < redeemableTokens.length; i++) {  vaultTokenBalance = vault.balance(redeemableTokens[i]);  redemptionAmount = _burnableAmount.mul(vaultTokenBalance).div(burnableTokenTotalSupply);  totalRedemptionAmount = totalRedemptionAmount.add(redemptionAmount);  if (redemptionAmount > 0) {  vault.transfer(redeemableTokens[i], msg.sender, redemptionAmount);  If a token address is included more than once, the sender will be paid out more than once, potentially earning many times more than their proportional share of the token.  In TokenRequest, this behavior does not allow for any significant deviation from expected behavior. It was included because the initialization process is similar to that of Redemptions.  Recommendation  During initialization in both apps, check that input token addresses are unique. One simple method is to require that token addresses are submitted in ascending order, and that each subsequent address added is greater than the one before.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "5.3 The Delay app allows scripts to be paused even after execution time has elapsed    Addressed", "body": "  Resolution   This was addressed in   commit 46d8fa414cc3e68c68a5d9bc1174be5f32970611 by requiring that the current timestamp is before the delayed script s execution time.  Description  The Delay app is used to configure a delay between when an evm script is created and when it is executed. The entry point for this process is Delay.delayExecution, which stores the input script with a future execution date:  code/delay-app/contracts/Delay.sol:L153-L162  function _delayExecution(bytes _evmCallScript) internal returns (uint256) {  uint256 delayedScriptIndex = delayedScriptsNewIndex;  delayedScriptsNewIndex++;  delayedScripts[delayedScriptIndex] = DelayedScript(getTimestamp64().add(executionDelay), 0, _evmCallScript);  emit DelayedScriptStored(delayedScriptIndex);  return delayedScriptIndex;  An auxiliary capability of the Delay app is the ability to  pause  the delayed script, which sets the script s pausedAt value to the current block timestamp:  code/delay-app/contracts/Delay.sol:L80-L85  function pauseExecution(uint256 _delayedScriptId) external auth(PAUSE_EXECUTION_ROLE) {  require(!_isExecutionPaused(_delayedScriptId), ERROR_CAN_NOT_PAUSE);  delayedScripts[_delayedScriptId].pausedAt = getTimestamp64();  emit ExecutionPaused(_delayedScriptId);  A paused script cannot be executed until resumeExecution is called, which extends the script s executionTime by the amount of time paused. Essentially, the delay itself is paused:  code/delay-app/contracts/Delay.sol:L91-L100  function resumeExecution(uint256 _delayedScriptId) external auth(RESUME_EXECUTION_ROLE) {  require(_isExecutionPaused(_delayedScriptId), ERROR_CAN_NOT_RESUME);  DelayedScript storage delayedScript = delayedScripts[_delayedScriptId];  uint64 timePaused = getTimestamp64().sub(delayedScript.pausedAt);  delayedScript.executionTime = delayedScript.executionTime.add(timePaused);  delayedScript.pausedAt = 0;  emit ExecutionResumed(_delayedScriptId);  A delayed script whose execution time has passed and is not currently paused should be able to be executed via the execute function. However, the pauseExecution function still allows the aforementioned script to be paused, halting execution.  Recommendation  Add a check to pauseExecution to ensure that execution is not paused if the script s execution delay has already transpired.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "5.4 Misleading intentional misconfiguration possible through misuse of newToken and newBaseInstance    Addressed", "body": "  Resolution   This was addressed in   commit b68d89ab0deb22161987e19d1ff0bb9d7303f0a9 by making  Description  The instantiation process for a Dandelion organization requires two separate external calls to DandelionOrg. There are two primary functions: installDandelionApps, and newTokenAndBaseInstance.  installDandelionApps relies on cached results from prior calls to newTokenAndBaseInstance and completes the initialization step for a Dandelion org.  newTokenAndBaseInstance is a wrapper around two publicly accessible functions: newToken and newBaseInstance. Called together, the functions:  Deploy a new MiniMeToken used to represent shares in an organization, and cache the address of the created token:  code/dandelion-org/contracts/DandelionOrg.sol:L128-L137  /**  @dev Create a new MiniMe token and save it for the user  @param _name String with the name for the token used by share holders in the organization  @param _symbol String with the symbol for the token used by share holders in the organization  /  function newToken(string memory _name, string memory _symbol) public returns (MiniMeToken) {  MiniMeToken token = _createToken(_name, _symbol, TOKEN_DECIMALS);  _saveToken(token);  return token;  Create a new dao instance using Aragon s BaseTemplate contract:  code/dandelion-org/contracts/DandelionOrg.sol:L139-L160  /**  @dev Deploy a Dandelion Org DAO using a previously saved MiniMe token  @param _id String with the name for org, will assign `[id].aragonid.eth`  @param _holders Array of token holder addresses  @param _stakes Array of token stakes for holders (token has 18 decimals, multiply token amount `* 10^18`)  @param _useAgentAsVault Boolean to tell whether to use an Agent app as a more advanced form of Vault app  /  function newBaseInstance(  string memory _id,  address[] memory _holders,  uint256[] memory _stakes,  uint64 _financePeriod,  bool _useAgentAsVault  public  _validateId(_id);  _ensureBaseSettings(_holders, _stakes);  (Kernel dao, ACL acl) = _createDAO();  _setupBaseApps(dao, acl, _holders, _stakes, _financePeriod, _useAgentAsVault);  Set up prepackaged Aragon apps, like Vault, TokenManager, and Finance:  code/dandelion-org/contracts/DandelionOrg.sol:L162-L182  function _setupBaseApps(  Kernel _dao,  ACL _acl,  address[] memory _holders,  uint256[] memory _stakes,  uint64 _financePeriod,  bool _useAgentAsVault  internal  MiniMeToken token = _getToken();  Vault agentOrVault = _useAgentAsVault ? _installDefaultAgentApp(_dao) : _installVaultApp(_dao);  TokenManager tokenManager = _installTokenManagerApp(_dao, token, TOKEN_TRANSFERABLE, TOKEN_MAX_PER_ACCOUNT);  Finance finance = _installFinanceApp(_dao, agentOrVault, _financePeriod == 0 ? DEFAULT_FINANCE_PERIOD : _financePeriod);  _mintTokens(_acl, tokenManager, _holders, _stakes);  _saveBaseApps(_dao, finance, tokenManager, agentOrVault);  _saveAgentAsVault(_dao, _useAgentAsVault);  Note that newToken and newBaseInstance can be called separately. The token created in newToken is cached in _saveToken, which overwrites any previously-cached value:  code/dandelion-org/contracts/DandelionOrg.sol:L413-L417  function _saveToken(MiniMeToken _token) internal {  DeployedContracts storage senderDeployedContracts = deployedContracts[msg.sender];  senderDeployedContracts.token = address(_token);  Cached tokens are retrieved in _getToken:  code/dandelion-org/contracts/DandelionOrg.sol:L441-L447  function _getToken() internal returns (MiniMeToken) {  DeployedContracts storage senderDeployedContracts = deployedContracts[msg.sender];  require(senderDeployedContracts.token != address(0), ERROR_MISSING_TOKEN_CONTRACT);  MiniMeToken token = MiniMeToken(senderDeployedContracts.token);  return token;  By exploiting the overwriteable caching mechanism, it is possible to intentionally misconfigure Dandelion orgs.  Examples  installDandelionApps uses _getToken to associate a token with the DandelionVoting app. The value returned from _getToken depends on the sender s previous call to newToken, which overwrites any previously-cached value. The steps for intentional misconfiguration are as follows:  Sender calls newTokenAndBaseInstance, creating token m0 and DAO A.  The TokenManager app in A is automatically configured to be the controller of m0.  m0 is cached using _saveToken.  DAO A apps are cached for future use using _saveBaseApps and _saveAgentAsVault.  Sender calls newToken, creating token m1, and overwriting the cache of m0.  Future calls to _getToken will retrieve m1.  The DandelionOrg contract is the controller of m1.  Sender calls installDandelionApps, which installs Dandelion apps in DAO A  The DandelionVoting app is configured to use the current cached token, m1, rather than the token associated with A.TokenManager, m0  Many different misconfigurations are possible, and some may be underhandedly abusable.  Recommendation  Make newToken and newBaseInstance internal so they are only callable via newTokenAndBaseInstance.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "5.5 Delay.execute can re-enter and re-execute the same script twice    Addressed", "body": "  Resolution   This was addressed in   commit f049e978f93765e27783a3ecac4830498bb779ba by deleting the delayed script before it is run. 1Hive elected to keep an empty script blacklist in order to allow delayed actions to be taken on the  Description  Delay.execute does not follow the  checks-effects-interactions  pattern, and deletes a delayed script only after the script is run. Because the script being run executes arbitrary external calls, a script can be created that re-enters Delay and executes itself multiple times before being deleted:  code/delay-app/contracts/Delay.sol:L112-L123  /**  @notice Execute the script with ID `_delayedScriptId`  @param _delayedScriptId The ID of the script to execute  /  function execute(uint256 _delayedScriptId) external {  require(canExecute(_delayedScriptId), ERROR_CAN_NOT_EXECUTE);  runScript(delayedScripts[_delayedScriptId].evmCallScript, new bytes(0), new address[](0));  delete delayedScripts[_delayedScriptId];  emit ExecutedScript(_delayedScriptId);  Recommendation  Add the Delay contract address to the runScript blacklist, or delete the delayed script from storage before it is run.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "5.6 Delay.cancelExecution should revert on a non-existent script id    Addressed", "body": "  Resolution   This was addressed in   commit d99c94f5138a9af1fd5f0cd6990c140b46a55925 by adding the  Description  cancelExecution makes no existence check on the passed-in script ID, clearing its storage slot and emitting an event:  code/delay-app/contracts/Delay.sol:L102-L110  /**  @notice Cancel script execution with ID `_delayedScriptId`  @param _delayedScriptId The ID of the script execution to cancel  /  function cancelExecution(uint256 _delayedScriptId) external auth(CANCEL_EXECUTION_ROLE) {  delete delayedScripts[_delayedScriptId];  emit ExecutionCancelled(_delayedScriptId);  Recommendation  Add a check that the passed-in script exists.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "5.7 ID validation check missing for installDandelionApps    Addressed", "body": "  Resolution   This was addressed in   commit 8d1ecb1bc892d6ea1d34c7234e35de031db2bebd by removing the  Description  DandelionOrg allows users to kickstart an Aragon organization by using a dao template. There are two primary functions to instantiate an org: newTokenAndBaseInstance, and installDandelionApps. Both functions accept a parameter, string _id, meant to represent an ENS subdomain that will be assigned to the new org during the instantiation process. The two functions are called independently, but depend on each other.  In newTokenAndBaseInstance, a sanity check is performed on the _id parameter, which ensures the _id length is nonzero:  code/dandelion-org/contracts/DandelionOrg.sol:L155  _validateId(_id);  Note that the value of _id is otherwise unused in newTokenAndBaseInstance.  In installDandelionApps, this check is missing. The check is only important in this function, since it is in installDandelionApps that the ENS subdomain registration is actually performed.  Recommendation  Use _validateId in installDandelionApps rather than newTokenAndBaseInstance. Since the _id parameter is otherwise unused in newTokenAndBaseInstance, it can be removed.  Alternatively, the value of the submitted _id could be cached between calls and validated in newTokenAndBaseInstance, similarly to newToken.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "6.1 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium -V  Solium version 1.2.5  $ solium -d .  dandelion-org/contracts/DandelionOrg.sol  86:1     warning    Line contains trailing whitespace           no-trailing-whitespace  226:8    warning    Line exceeds the limit of 145 characters    max-len  dandelion-voting-app/contracts/DandelionVoting.sol  272:8    warning    Line exceeds the limit of 145 characters    max-len  token-request-app/contracts/TokenRequest.sol  62:4      warning    Line exceeds the limit of 145 characters                 max-len  104:1     warning    Line contains trailing whitespace                        no-trailing-whitespace  token-request-app/contracts/lib/UintArrayLib.sol  6:3    error    Only use indent of 4 spaces.    indentation  \u2716 1 error, 5 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "6.2 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AddressArrayLib  Library  deleteItem  Internal \ud83d\udd12  contains  Internal \ud83d\udd12  ArrayUtils  Library  deleteItem  Internal \ud83d\udd12  DandelionOrg  Implementation  BaseTemplate  <Constructor>  Public    BaseTemplate  newTokenAndBaseInstance  External    NO   installDandelionApps  External    NO   newToken  Public    NO   newBaseInstance  Public    NO   _setupBaseApps  Internal \ud83d\udd12  _installDandelionApps  Internal \ud83d\udd12  _installDandelionVotingApp  Internal \ud83d\udd12  _installDandelionVotingApp  Internal \ud83d\udd12  _createDandelionVotingPermissions  Internal \ud83d\udd12  _installRedemptionsApp  Internal \ud83d\udd12  _createRedemptionsPermissions  Internal \ud83d\udd12  _installTokenRequestApp  Internal \ud83d\udd12  _createTokenRequestPermissions  Internal \ud83d\udd12  _installTimeLockApp  Internal \ud83d\udd12  _installTimeLockApp  Internal \ud83d\udd12  _createTimeLockPermissions  Internal \ud83d\udd12  _installTokenBalanceOracle  Internal \ud83d\udd12  _createTokenBalanceOraclePermissions  Internal \ud83d\udd12  _setupBasePermissions  Internal \ud83d\udd12  _setupDandelionPermissions  Internal \ud83d\udd12  _saveToken  Internal \ud83d\udd12  _saveBaseApps  Internal \ud83d\udd12  _saveAgentAsVault  Internal \ud83d\udd12  _getDao  Internal \ud83d\udd12  _getToken  Internal \ud83d\udd12  _getBaseApps  Internal \ud83d\udd12  _getAgentAsVault  Internal \ud83d\udd12  _clearDeployedContracts  Internal \ud83d\udd12  _ensureBaseAppsDeployed  Internal \ud83d\udd12  _ensureBaseSettings  Private \ud83d\udd10  _ensureDandelionSettings  Private \ud83d\udd10  _registerApp  Private \ud83d\udd10  _setOracle  Private \ud83d\udd10  _paramsTo256  Private \ud83d\udd10  DandelionVoting  Implementation  IForwarder, IACLOracle, AragonApp  initialize  External    onlyInit  changeSupportRequiredPct  External    authP  changeMinAcceptQuorumPct  External    authP  changeBufferBlocks  External    auth  changeExecutionDelayBlocks  External    auth  newVote  External    auth  vote  External    voteExists  executeVote  External    NO   isForwarder  External    NO   forward  Public    NO   canForward  Public    NO   canPerform  External    NO   canExecute  Public    NO   canVote  Public    voteExists  getVote  Public    voteExists  getVoterState  Public    voteExists  _newVote  Internal \ud83d\udd12  _vote  Internal \ud83d\udd12  _canExecute  Internal \ud83d\udd12  voteExists  _votePassed  Internal \ud83d\udd12  _canVote  Internal \ud83d\udd12  _voterStake  Internal \ud83d\udd12  _isVoteOpen  Internal \ud83d\udd12  _isValuePct  Internal \ud83d\udd12  Delay  Implementation  AragonApp, IForwarder  initialize  External    onlyInit  setExecutionDelay  External    auth  delayExecution  External    auth  isForwarder  External    NO   pauseExecution  External    auth  resumeExecution  External    auth  cancelExecution  External    auth  execute  External    NO   canExecute  Public    NO   canForward  Public    NO   forward  Public    NO   _isExecutionPaused  Internal \ud83d\udd12  scriptExists  _delayExecution  Internal \ud83d\udd12  Redemptions  Implementation  AragonApp  initialize  External    onlyInit  addRedeemableToken  External    auth  removeRedeemableToken  External    auth  redeem  External    authP  getRedeemableTokens  External    NO   getToken  External    NO   getETHAddress  External    NO   TimeLock  Implementation  AragonApp, IForwarder, IForwarderFee  initialize  External    onlyInit  changeLockDuration  External    auth  changeLockAmount  External    auth  changeSpamPenaltyFactor  External    auth  withdrawAllTokens  External    NO   withdrawTokens  External    NO   forwardFee  External    NO   isForwarder  External    NO   canForward  Public    NO   forward  Public    NO   getWithdrawLocksCount  Public    NO   getSpamPenalty  Public    NO   _withdrawTokens  Internal \ud83d\udd12  TokenBalanceOracle  Implementation  AragonApp, IACLOracle  initialize  External    onlyInit  setToken  External    auth  setMinBalance  External    auth  canPerform  External    NO   TokenRequest  Implementation  AragonApp  initialize  External    onlyInit  setTokenManager  External    auth  setVault  External    auth  addToken  External    auth  removeToken  External    auth  createTokenRequest  External    NO   refundTokenRequest  External    nonReentrant tokenRequestExists  finaliseTokenRequest  External    nonReentrant tokenRequestExists auth  getAcceptedDepositTokens  Public    NO   getTokenRequest  Public    NO   getToken  Public    NO   UintArrayLib  Library  deleteItem  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}, {"title": "5.1 Tokens with no decimals can be locked in Niftyswap   ", "body": "  Resolution  This will be addressed by only listing tokens with at least 2 decimals. This should be well documented in the Niftyswap repository and code comments.  Description  Assume the Niftyswap exchange has:  wrapped DAI as the base currency, and  it s ERC1155 contract has a token called  Blue Dragons , which are a  low fungibility  token, with zero decimals, and a total supply of 100.  Consider the following scenario on the Niftyswap exchange:  10 people each add 1,000 DAI, and 1 BlueDragon. They get 1,000 pool tokens each.  Someone buys 1 BlueDragon, at a price of 1,117 base Tokens (per the constant product pricing model).  Niftyswap s balances are now 11,117 baseTokens, 9 Blue Dragons.  Someone removes liquidity by burning 1,000 pool tokens:  They would get 1111 base tokens (1000 * 11,117/ 10000). They would get 0 Blue Dragons due to the rounding on integer math.  Recommendation  Through conversation with the developers, we agreed the right approach is for tokens to have at least 2 decimals to minimize the negative effects of rounding down.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/02/horizon-games/"}, {"title": "5.2 Incorrect response from price feed if called during an onERC1155Received callback   ", "body": "  Resolution  The design will not be modified. Horizon Games should clearly document this risk for 3rd parties seeking to use Niftyswap as a price feed.  Description  The ERC 1155 standard requires that smart contracts must implement onERC1155Received and onERC1155BatchReceived to accept transfers.  This means that on any token received, code run on the receiving smart contract.  In NiftyswapExchange when adding / removing liquidity or buying tokens, the methods mentioned above are called when the tokens are sent. When this happens, the state of the contract is changed but not completed, the tokens are sent to the receiving smart contract but the state is not completely updated.  This happens in these cases  _baseToToken (when buying tokens)  code/niftyswap/contracts/exchange/NiftyswapExchange.sol:L163-L169  // // Refund Base Token if any  if (totalRefundBaseTokens > 0) {  baseToken.safeTransferFrom(address(this), _recipient, baseTokenID, totalRefundBaseTokens, \"\");  // Send Tokens all tokens purchased  token.safeBatchTransferFrom(address(this), _recipient, _tokenIds, _tokensBoughtAmounts, \"\");  _removeLiquidity  code/niftyswap/contracts/exchange/NiftyswapExchange.sol:L485-L487  // Transfer total Base Tokens and all Tokens ids  baseToken.safeTransferFrom(address(this), _provider, baseTokenID, totalBaseTokens, \"\");  token.safeBatchTransferFrom(address(this), _provider, _tokenIds, tokenAmounts, \"\");  _addLiquidity  code/niftyswap/contracts/exchange/NiftyswapExchange.sol:L403-L407  // Mint liquidity pool tokens  _batchMint(_provider, _tokenIds, liquiditiesToMint, \"\");  // Transfer all Base Tokens to this contract  baseToken.safeTransferFrom(_provider, address(this), baseTokenID, totalBaseTokens, abi.encode(DEPOSIT_SIG));  Each of these examples send some tokens to the smart contract, which triggers calling some code on the receiving smart contract.  While these methods have the nonReentrant modifier which protects them from re-netrancy, the result of the methods getPrice_baseToToken and getPrice_tokenToBase is affected. These 2 methods do not have the nonReentrant modifier.  The price reported by the getPrice_baseToToken and getPrice_tokenToBase methods is incorrect (until after the end of the transaction) because they rely on the number of tokens owned by the NiftyswapExchange; which between the calls is not finalized. Hence the price reported will be incorrect.  This gives the smart contract which receives the tokens, the opportunity to use other systems (if they exist) that rely on the result of getPrice_baseToToken and getPrice_tokenToBase to use the returned price to its advantage.  It s important to note that this is a bug only if other systems rely on the price reported by this NiftyswapExchange. Also the current contract is not affected, nor its balances or internal ledger, only other systems relying on its reported price will be fooled.  Recommendation  Because there is no way to enforce how other systems work, a restriction can be added on NiftyswapExchange to protect other systems (if any) that rely on NiftyswapExchange for price discovery.  Adding a nonReentrant modifier on the view methods getPrice_baseToToken and getPrice_tokenToBase will add a bit of protection for the ecosystem.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/02/horizon-games/"}, {"title": "6.1 Test code present in the code base    ", "body": "  Resolution   Fixed in   lukso-network/rICO-smart-contracts@edb880c.  Description  Test code are present in the code base. This is mainly a reminder to fix those before production.  Examples  rescuerAddress and freezerAddress are not even in the function arguments.  code/contracts/ReversibleICO.sol:L243-L247  whitelistingAddress = _whitelistingAddress;  projectAddress = _projectAddress;  freezerAddress = _projectAddress; // TODO change, here only for testing  rescuerAddress = _projectAddress; // TODO change, here only for testing  Recommendation  Make sure all the variable assignments are ready for production before deployment to production.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.2 FreezerAddress has more power than required   ", "body": "  Resolution   This issue is acknowledged by the client and the behaviour has been documented in   security measurements.  Description  FreezerAddress is designed to have the ability of freezing the contract in case of emergency. However, indirectly, there are other changes in the system that can result from the freeze.  Examples  FreezerAddress can extend the rICO time frame. Given that the frozenPeriod is deducted from the blockNumber in stage calculations, the buyPhaseEndBlock is technically equals to buyPhaseEndBlock + frozenPeriod  FreezerAddress can call disableEscapeHatch(), which disables the escape hatch and rendering RescuerAddress useless.  Recommendation  If these behaviors are intentional they should be well documented and specified. If not, they should be removed.  In the case they are, indeed, intentional the audit team believes that, for Example 1., there should be some event fired to serve as notification for the participants (possibly followed by off-chain infrastructure to warn them through email or other communication channel).  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.3 frozenPeriod is subtracted twice for calculating the current price    ", "body": "  Resolution   Found in parallel to the audit team and has been mitigated in   lukso-network/rICO-smart-contracts@ebc4bce . The issue was further simplified by adding  lukso-network/rICO-smart-contracts@e4c9ed5 to remove ambiguity when calculating current block number.  Description  If the contract had been frozen, the current stage price will calculate the price by subtracting the frozenPeriod twice and result in wrong calculation.  getCurrentBlockNumber() subtracts frozenPeriod once, and then getStageAtBlock() will also subtract the same number again.  Examples  code/contracts/ReversibleICO.sol:L617-L619  function getCurrentStage() public view returns (uint8) {  return getStageAtBlock(getCurrentBlockNumber());  code/contracts/ReversibleICO.sol:L711-L714  function getCurrentBlockNumber() public view returns (uint256) {  return uint256(block.number)  .sub(frozenPeriod); // make sure we deduct any frozenPeriod from calculations  code/contracts/ReversibleICO.sol:L654-L656  function getStageAtBlock(uint256 _blockNumber) public view returns (uint8) {  uint256 blockNumber = _blockNumber.sub(frozenPeriod); // adjust the block by the frozen period  Recommendation  Make sure frozenPeriod calculation is done correctly. It could be solved by renaming getCurrentBlockNumber() to reflect the calculation done inside the function.  e.g. :  getCurrentBlockNumber() : gets current block number  getCurrentEffectiveBlockNumber() : calculates the effective block number deducting frozenPeriod  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.4 Lockup condition in getStageAtBlock()    ", "body": "  Resolution  Even though the freeze pattern does indeed create a lot of additional complexity to the protocol, the particular require mentioned in the issue corpus by the audit team was found to never be triggered in a harmful way by rICO s development team.  In the light of this new discovery, we are greatly reducing the severity of the issue to  Minor . The reason why it is still kept as an issue is that the implementation of the freezing mechanism could still be greatly improved as we saw in the presented fixes here:  lukso-network/rICO-smart-contracts@e4c9ed5  The changes resulted in a much more resilient rICO implementation.  Description  Given that the contract has been frozen at least once, if the frozenPeriod is longer than the period before the freeze event (starting from commitPhaseStartBlock till the freezeStart), the following require in getStageAtBlock() will revert due to the fact that blockNumber < commitPhaseStartBlock:  uint256 blockNumber = _blockNumber.sub(frozenPeriod); // adjust the block by the frozen period  require(blockNumber >= commitPhaseStartBlock && blockNumber <= buyPhaseEndBlock, \"Block outside of rICO period.\");  Note that the issue here is also related to the way currentBlockNumber is calculated (See issue 6.3 and Separate currentBlock from currentEffectiveBlock.  getCurrentStage() is called for every accept or cancelation of contributions and this lockup can result in total system halt.  Recommendation  Given that in the init function, the following condition is checked:  require(_commitPhaseStartBlock > getCurrentBlockNumber(), \"Start block cannot be set in the past.\");  The check in the getStageAtBlock() can be removed. However this is assuming that the correct calculation of the currentEffectiveBlockNumber is used.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.5 emit events for significant state changes    ", "body": "  Resolution   This issue was discussed in the code walk through meeting and was fixed, by adding proper events to the code base in   lukso-network/rICO-smart-contracts@77517a4, before the end of the audit.  Description  Events are useful for UI changes and user notifications. The code base overall can use more use of events to update the UI and participants.  One of the most important aspects that must emit events, are when system state and functionality are changed. These functions require to emit events for better visibility to the participants:  freeze()  unfreeze()  disableEscapeHatch()  escapeHatch()  Recommendation  emit events when system state is changed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.1 An account that confirms a transaction via AssetProxyOwner can indefinitely block that transaction    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2297 by allowing transactions to be  over confirmed  without resetting the confirmation time. As long as there are enough honest signers, this prevents a malicious signer from blocking transactions.  Description  When a transaction reaches the required number of confirmations in confirmTransaction(), its confirmation time is recorded:  code/contracts/multisig/contracts/src/MultiSigWalletWithTimeLock.sol:L86-L100  /// @dev Allows an owner to confirm a transaction.  /// @param transactionId Transaction ID.  function confirmTransaction(uint256 transactionId)  public  ownerExists(msg.sender)  transactionExists(transactionId)  notConfirmed(transactionId, msg.sender)  notFullyConfirmed(transactionId)  confirmations[transactionId][msg.sender] = true;  emit Confirmation(msg.sender, transactionId);  if (isConfirmed(transactionId)) {  _setConfirmationTime(transactionId, block.timestamp);  Before the time lock has elapsed and the transaction is executed, any of the owners that originally confirmed the transaction can revoke their confirmation via revokeConfirmation():  code/contracts/multisig/contracts/src/MultiSigWallet.sol:L249-L259  /// @dev Allows an owner to revoke a confirmation for a transaction.  /// @param transactionId Transaction ID.  function revokeConfirmation(uint256 transactionId)  public  ownerExists(msg.sender)  confirmed(transactionId, msg.sender)  notExecuted(transactionId)  confirmations[transactionId][msg.sender] = false;  emit Revocation(msg.sender, transactionId);  Immediately after, that owner can call confirmTransaction() again, which will reset the confirmation time and thus the time lock.  This is especially troubling in the case of a single compromised key, but it s also an issue for disagreement among owners, where any m of the n owners should be able to execute transactions but could be blocked.  Mitigations  Only an owner can do this, and that owner has to be part of the group that originally confirmed the transaction. This means the malicious owner may have to front run the others to make sure they re in that initial confirmation set.  Even once a malicious owner is in position to execute this perpetual delay, they need to call revokeConfirmation() and confirmTransaction() again each time. Another owner can attempt to front the attacker and execute their own confirmTransaction() immediately after the revokeConfirmation() to regain control.  Recommendation  There are several ways to address this, but to best preserve the original MultiSigWallet semantics, once a transaction has reached the required number of confirmations, it should be impossible to revoke confirmations. In the original implementation, this is enforced by immediately executing the transaction when the final confirmation is received.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.2 Orders with signatures that require regular validation can have their validation bypassed if the order is partially filled    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2246. Signatures are now always validated each time, regardless of type.  Description  This re-validation step for Wallet, Validator, and EIP1271Wallet signatures is intended to facilitate their use with contracts whose validation depends on some state that may change over time. For example, a validating contract may call into a price feed and determine that some order is invalid if its price deviates from some expected range. In this case, the repeated validation allows 0x users to make orders with custom fill conditions which are evaluated at run-time.  We found that if the sender provides the contract with an invalid signature after the order in question has already been partially filled, the regular validation check required for Wallet, Validator, and EIP1271Wallet signatures can be bypassed entirely.  Examples  Signature validation takes place in MixinExchangeCore._assertFillableOrder. A signature is only validated if it passes the following criteria:  code/contracts/exchange/contracts/src/MixinExchangeCore.sol:L372-L381  // Validate either on the first fill or if the signature type requires  // regular validation.  address makerAddress = order.makerAddress;  if (orderInfo.orderTakerAssetFilledAmount == 0 ||  _doesSignatureRequireRegularValidation(  orderInfo.orderHash,  makerAddress,  signature  ) {  In effect, signature validation only occurs if:  orderInfo.orderTakerAssetFilledAmount == 0 OR  _doesSignatureRequireRegularValidation(orderHash, makerAddress, signature)  If an order is partially filled, the first condition will evaluate to false. Then, that order s signature will only be validated if _doesSignatureRequireRegularValidation evaluates to true:  code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L183-L206  function _doesSignatureRequireRegularValidation(  bytes32 hash,  address signerAddress,  bytes memory signature  internal  pure  returns (bool needsRegularValidation)  // Read the signatureType from the signature  SignatureType signatureType = _readSignatureType(  hash,  signerAddress,  signature  );  // Any signature type that makes an external call needs to be revalidated  // with every partial fill  needsRegularValidation =  signatureType == SignatureType.Wallet ||  signatureType == SignatureType.Validator ||  signatureType == SignatureType.EIP1271Wallet;  return needsRegularValidation;  The result is that an order whose signature requires regular validation can be forced to skip validation if it has been partially filled, by passing in an invalid signature.  Recommendation  There are a few options for remediation:  Have the Exchange validate the provided signature every time an order is filled.  Record the first seen signature type or signature hash for each order, and check that subsequent actions are submitted with a matching signature.  The first option requires the fewest changes, and does not require storing additional state. While this does mean some additional cost validating subsequent signatures, we feel the increase in flexibility is well worth it, as a maker could choose to create multiple valid signatures for use across different order books.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.3 Changing the owners or required confirmations in the AssetProxyOwner can unconfirm a previously confirmed transaction    ", "body": "  Resolution  This issue is somewhat inaccurate: isConfirmed() breaks out of the loop once it s found the correct number of confirmations. That means that lowering the number of required confirmations is not a problem.  Further, 0xProject/0x-monorepo#2297 allows signers to confirm transactions that have already been confirmed.  Increasing signing requirements or changing signers can still unconfirm previously confirmed transactions, but the development team is happy with that behavior.  Description  Once a transaction has been confirmed in the AssetProxyOwner, it cannot be executed until a lock period has passed. During that time, any change to the number of required confirmations will cause this transaction to no longer be executable.  If the number of required confirmations was decreased, then one or more owners will have to revoke their confirmation before the transaction can be executed.  If the number of required confirmations was increased, then additional owners will have to confirm the transaction, and when the new required number of confirmations is reached, a new confirmation time will be recorded, and thus the time lock will restart.  Similarly, if an owner that had previously confirmed the transaction is replaced, the number of confirmations will drop for existing transactions, and they will need to be confirmed again.  This is not disastrous, but it s almost certainly unintended behavior and may make it difficult to make changes to the multisig owners and parameters.  Examples  executeTransaction() requires that at the time of execution, the transaction is confirmed:  code/contracts/multisig/contracts/src/AssetProxyOwner.sol:L115-L118  function executeTransaction(uint256 transactionId)  public  notExecuted(transactionId)  fullyConfirmed(transactionId)  isConfirmed() checks for exact equality with the number of required confirmations. Having too many confirmations is just as bad as too few:  code/contracts/multisig/contracts/src/MultiSigWallet.sol:L318-L335  /// @dev Returns the confirmation status of a transaction.  /// @param transactionId Transaction ID.  /// @return Confirmation status.  function isConfirmed(uint256 transactionId)  public  view  returns (bool)  uint256 count = 0;  for (uint256 i = 0; i < owners.length; i++) {  if (confirmations[transactionId][owners[i]]) {  count += 1;  if (count == required) {  return true;  If additional confirmations are required to reconfirm a transaction, that resets the time lock:  code/contracts/multisig/contracts/src/MultiSigWalletWithTimeLock.sol:L86-L100  /// @dev Allows an owner to confirm a transaction.  /// @param transactionId Transaction ID.  function confirmTransaction(uint256 transactionId)  public  ownerExists(msg.sender)  transactionExists(transactionId)  notConfirmed(transactionId, msg.sender)  notFullyConfirmed(transactionId)  confirmations[transactionId][msg.sender] = true;  emit Confirmation(msg.sender, transactionId);  if (isConfirmed(transactionId)) {  _setConfirmationTime(transactionId, block.timestamp);  Recommendation  As in issue 6.1, the semantics of the original MultiSigWallet were that once a transaction is fully confirmed, it s immediately executed. The time lock means this is no longer possible, but it is possible to record that the transaction is confirmed and never allow this to change. In fact, the confirmation time already records this. Once the confirmation time is non-zero, a transaction should always be considered confirmed.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.4 Reentrancy in executeTransaction()   ", "body": "  Resolution  From the development team:  Reentrancy would be dangerous in executeTransaction if combined with updating the currentContextAddress. However, this is is prevented by checking currentContextAddress_ != address(0) when validating a transaction.  executeTransaction also inherits a lot of the safety from the reentrancy protection on other individual functions in the Exchange contract.  Setting transactionsExecuted before making the delegatecall also prevents the same transaction from being executed multiple times.  Description  In MixinTransactions, executeTransaction() and batchExecuteTransactions() do not have the nonReentrant modifier. Because of that, it is possible to execute nested transactions or call these functions during other reentrancy attacks on the exchange. The reason behind that decision is to be able to call functions with nonReentrant modifier as delegated transactions.  Nested transactions are partially prevented with a separate check that does not allow transaction execution if the exchange is currently in somebody else s context:  code/contracts/exchange/contracts/src/MixinTransactions.sol:L155-L162  // Prevent `executeTransaction` from being called when context is already set  address currentContextAddress_ = currentContextAddress;  if (currentContextAddress_ != address(0)) {  LibRichErrors.rrevert(LibExchangeRichErrors.TransactionInvalidContextError(  transactionHash,  currentContextAddress_  ));  This check still leaves some possibility of reentrancy. Allowing that behavior is dangerous and may create possible attack vectors in the future.  Recommendation  Add a new modifier to executeTransaction() and batchExecuteTransactions() which is similar to nonReentrant but uses different storage slot.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.5  Poison  order that consumes gas can block market trades   ", "body": "  Resolution  From the development team:  This can be prevented fairly easily by performing an eth_call off-chain before attempting to fill any orders (which is pretty standard practice). Hard coding gas limits reduces flexibility and may ultimately prevent some use cases from developing in the future.  (Note from the audit team: Hardcoding is not necessary. A parameter would do.)  Description  The market buy/sell functions gather a list of orders together for the same asset and try to fill them in order until a target amount has been traded.  These functions use MixinWrapperFunctions._fillOrderNoThrow() to attempt to fill each order but ignore failures. This way, if one order is unfillable for some reason, the overall market order can still succeed by filling other orders.  Orders can still force _fillOrderNoThrow() to revert by using an external contract for signature validation and having that contract consume all available gas.  This makes it possible to advertise a  poison  order for a low price that will block all market orders from succeeding. It s reasonable to assume that off-chain order books will automatically include the best prices when constructing market orders, so this attack would likely be quite effective. Note that such an attack costs the attacker nothing because all they need is an on-chain contract that consumers all available gas (maybe via an assert). This makes it a very appealing attack vector for, e.g., an order book that wants to temporarily disable a competitor.  Details  _fillOrderNoThrow() forwards all available gas when filling the order:  code/contracts/exchange/contracts/src/MixinWrapperFunctions.sol:L340-L348  // ABI encode calldata for `fillOrder`  bytes memory fillOrderCalldata = abi.encodeWithSelector(  IExchangeCore(address(0)).fillOrder.selector,  order,  takerAssetFillAmount,  signature  );  (bool didSucceed, bytes memory returnData) = address(this).delegatecall(fillOrderCalldata);  Similarly, when the Exchange attempts to fill an order that requires external signature validation (Wallet, Validator, or EIP1271Wallet signature types), it forwards all available gas:  code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L642  (bool didSucceed, bytes memory returnData) = verifyingContractAddress.staticcall(callData);  If the verifying contract consumes all available gas, it can force the overall transaction to revert.  Pedantic Note  Technically, it s impossible to consume all remaining gas when called by another contract because the EVM holds back a small amount, but even at the block gas limit, the amount held back would be insufficient to complete the transaction.  Recommendation  Constrain the gas that is forwarded during signature validation. This can be constrained either as a part of the signature or as a parameter provided by the taker.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.6 Front running in matchOrders()   ", "body": "  Resolution  From the development team:  Front-running is typically prevented with a combination of external contracts and various off-chain mechanics.  These functions are primarily intended to be used with  matching relayers . In this model, orders must set their takerAddress or senderAddress to the address of the matcher, who is the only party allowed to actually fill the orders. This prevents any other address from participating in a gas auction.  A commit-reveal scheme would be difficult to take advantage of in practice, since orders could be filled through a number of other functions on the Exchange contract. All of these functions would have to adhere to the commit-reveal scheme in order to be effective.  Description  Calls to matchOrders() are made to extract profit from the price difference between two opposite orders: left and right.  code/contracts/exchange/contracts/src/MixinMatchOrders.sol:L106-L111  function matchOrders(  LibOrder.Order memory leftOrder,  LibOrder.Order memory rightOrder,  bytes memory leftSignature,  bytes memory rightSignature  The caller only pays protocol and transaction fees, so it s almost always profitable to front run every call to matchOrders(). That would lead to gas auctions and would make matchOrders() difficult to use.  Recommendation  Consider adding a commit-reveal scheme to matchOrders() to stop front running altogether.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.7 The Exchange owner should not be able to call executeTransaction or batchExecuteTransaction   ", "body": "  Resolution  From the development team:  While this is a minor inconsistency in the logic of these functions, it is in no way dangerous. currentContextAddress is not used when calling any admin functions, so the address of the transaction signer will be completely disregarded.  Description  Examples  _executeTransaction sets the context address to the signer address, which is not msg.sender in this case: code/contracts/exchange/contracts/src/MixinTransactions.sol:L102-L104 // Set the current transaction signer address signerAddress = transaction.signerAddress; _setCurrentContextAddressIfRequired(signerAddress, signerAddress);  The resulting delegatecall could target an admin function like this one: code/contracts/exchange/contracts/src/MixinAssetProxyDispatcher.sol:L38-L61 /// @dev Registers an asset proxy to its asset proxy id. ///      Once an asset proxy is registered, it cannot be unregistered. /// @param assetProxy Address of new asset proxy to register. function registerAssetProxy(address assetProxy)     external     onlyOwner {     // Ensure that no asset proxy exists with current id.     bytes4 assetProxyId = IAssetProxy(assetProxy).getProxyId();     address currentAssetProxy = _assetProxies[assetProxyId];     if (currentAssetProxy != address(0)) {         LibRichErrors.rrevert(LibExchangeRichErrors.AssetProxyExistsError(             assetProxyId,             currentAssetProxy         ));     }      // Add asset proxy and log registration.     _assetProxies[assetProxyId] = assetProxy;     emit AssetProxyRegistered(         assetProxyId,         assetProxy     ); }  The onlyOwner modifier does not check the context address, but checks msg.sender: code/contracts/utils/contracts/src/Ownable.sol:L35-L45 function _assertSenderIsOwner()     internal     view {     if (msg.sender != owner) {         LibRichErrors.rrevert(LibOwnableRichErrors.OnlyOwnerError(             msg.sender,             owner         ));     } }  Recommendation  Add a check to _executeTransaction that prevents the owner from calling this function.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.8 Anyone can front run MixinExchangeCore.cancelOrder()   ", "body": "  Resolution  From the development team:  Front-running is typically prevented with a combination of external contracts and various off-chain mechanics.  It is not possible to cancel an order by providing less data to the cancelOrder function without drastically changing the logic of the fill functions. However, this type of behavior could possibly be enforced by using external contracts that are set to the senderAddress of the related orders.  Description  In order to cancel an order, an authorized address (maker or sender) calls cancelOrder(LibOrder.Order memory order). When calling that function, all data for the order becomes visible to everyone on the network, and anyone can fill that order before it s canceled.  Usually, a maker is canceling an order because it s no longer profitable for them, so an attacker is likely to profit from front running the cancelOrder() transaction.  Recommendation  Make it impossible to front run order cancelation by providing less data to the cancelOrder() function such that this data is insufficient to execute the order.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.9 By manipulating the gas limit, relayers can affect the outcome of ZeroExTransactions   ", "body": "  Resolution  From the development team:  While this is an annoyance when used in combination with marketBuyOrdersNoThrow and marketSellOrdersNoThrow, it does not seem worth it to add a gasLimit to 0x transactions for this reason alone. Instead, this quirk should be documented along with a recommendation to use the fillOrKill variants of each market fill function when used in combination with 0x transactions.  Description  ZeroExTransactions are meta transactions supported by the Exchange. They do not require that they are executed with a specific amount of gas, so the transaction relayer can choose how much gas to provide. By choosing a low gas limit, a relayer can affect the outcome of the transaction.  A ZeroExTransaction specifies a signer, an expiration, and call data for the transaction:  code/contracts/exchange-libs/contracts/src/LibZeroExTransaction.sol:L41-L47  struct ZeroExTransaction {  uint256 salt;                   // Arbitrary number to ensure uniqueness of transaction hash.  uint256 expirationTimeSeconds;  // Timestamp in seconds at which transaction expires.  uint256 gasPrice;               // gasPrice that transaction is required to be executed with.  address signerAddress;          // Address of transaction signer.  bytes data;                     // AbiV2 encoded calldata.  In MixinTransactions._executeTransaction(), all available gas is forwarded in the delegate call, and the transaction is marked as executed:  code/contracts/exchange/contracts/src/MixinTransactions.sol:L107-L108  transactionsExecuted[transactionHash] = true;  (bool didSucceed, bytes memory returnData) = address(this).delegatecall(transaction.data);  Examples  A likely attack vector for this is front running a ZeroExTransaction that ultimately invokes _fillNoThrow(). In this scenario, an attacker sees the call to executeTransaction() and makes their own call with a lower gas limit, causing the order being filled to run out of gas but allowing the transaction as a whole to succeed.  If such an attack is successful, the ZeroExTransaction cannot be replayed, so the signer must produce a new signature and try again, ad infinitum.  Recommendation  Add a gasLimit field to ZeroExTransaction and forward exactly that much gas via delegatecall. (Note that you must explicitly check that sufficient gas is available because the EVM allows you to supply a gas parameter that exceeds the actual remaining gas.)  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.10 Front running market orders   ", "body": "  Resolution  From the development team:  Front-running is typically prevented with a combination of external contracts and various off-chain mechanics.  Users should always understand the risk of using market orders in any market or exchange structure. Although they increase convenience and arguably have a better UX, they almost always carry more risk than other order types.  Users can always enforce a worst price by padding a market fill with an appropriate number of orders that do not exceed the worst acceptable price.  Description  MixinWrapperFunctions defines a number of functions for market buy/sell orders. These functions take a list of orders and a target asset amount to buy or sell. They fill each order in turn until the target has been reached.  These functions provide an appealing opportunity for front running because of the near-guaranteed profit to be had. This is most easily explained with an example:  Alice wishes to buy 10 FOO tokens. She creates a market buy order to purchase tokens first from Bob, who is selling 4 FOO tokens at $9 each, and then from Eve, who is selling 20 tokens at $10 each.  Eve front runs this market order with a transaction that buys all 4 FOO tokens from Bob for $9 each.  Alice s transaction goes through, but because Bob s inventory has been depleted, all 10 FOO tokens are purchased from Eve at a price of $10 each. By front running, Eve gained $4.  In a more traditional front running scheme, Alice would have just been trying to make a simple purchase of FOO tokens at $9 each, and Eve would be taking on non-trivial risk by buying them first and hoping Alice (or another buyer) would be willing to pay a higher price later.  With a market order, however, Eve s front running is nearly risk free because she knows the market order already commits Alice to buying at the higher price.  Recommendation  For the most part, traders will simply have to understand the risks of market orders and take care to only authorize trades they will be happy with.  That said, each order in a market order could specify a maximum quantity, e.g.  I want 10 FOO tokens, and I m willing to buy up to 10 from Bob but only up to 5 from Eve.  This would limit the trader s exposure to increased prices due to front running, but it would retain the convenience and efficiency of market orders.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.11 Modifier ordering plays a significant role in modifier efficacy    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2228 by introducing a new modifier that combines the two:  Description  The nonReentrant and refundFinalBalance modifiers always appear together across the 0x monorepo. When used, they invariably appear with nonReentrant listed first, followed by refundFinalBalance. This specific order appears inconsequential at first glance but is actually important. The order of execution is as follows:  The nonReentrant modifier runs (_lockMutexOrThrowIfAlreadyLocked).  If refundFinalBalance had a prefix, it would run now.  The function itself runs.  The refundFinalBalance modifier runs (_refundNonZeroBalanceIfEnabled).  The nonReentrant modifier runs (_unlockMutex).  The fact that the refundFinalBalance modifier runs before the mutex is unlocked is of particular importance because it potentially invokes an external call, which may reenter. If the order of the two modifiers were flipped, the mutex would unlock before the external call, defeating the purpose of the reentrancy guard.  Examples  code/contracts/exchange/contracts/src/MixinExchangeCore.sol:L64-L65  nonReentrant  refundFinalBalance  Recommendation  Although the order of the modifiers is correct as-is, this pattern introduces cognitive overhead when making or reviewing changes to the 0x codebase. Because the two modifiers always appear together, it may make sense to combine the two into a single modifier where the order of operations is explicit.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.12 Several overflows in LibBytes    Addressed", "body": "  Resolution   This is addressed in   0xProject/0x-monorepo#2265. Unused functions have been removed. The remaining functions are only used with safe parameters (ones guaranteed not to overflow).  Description  Several functions in LibBytes have integer overflows.  Examples  LibBytes.readBytesWithLength returns a pointer to a bytes array within an existing bytes array at some given index. The length of the nested array is added to the given index and checked against the parent array to ensure the data in the nested array is within the bounds of the parent. However, because the addition can overflow, the bounds check can be bypassed to return an array that points to data out of bounds of the parent array.  code/contracts/utils/contracts/src/LibBytes.sol:L546-L553  if (b.length < index + nestedBytesLength) {  LibRichErrors.rrevert(LibBytesRichErrors.InvalidByteOperationError(  LibBytesRichErrors  .InvalidByteOperationErrorCodes.LengthGreaterThanOrEqualsNestedBytesLengthRequired,  b.length,  index + nestedBytesLength  ));  The following functions have similar issues:  readAddress  writeAddress  readBytes32  writeBytes32  readBytes4  Recommendation  An overflow check should be added to the function. Alternatively, because readBytesWithLength does not appear to be used anywhere in the 0x project, the function should be removed from LibBytes. Additionally, the following functions in LibBytes are also not used and should be considered for removal:  popLast20Bytes  writeAddress  writeBytes32  writeUint256  writeBytesWithLength  deepCopyBytes  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "6.13 NSignatureTypes enum value bypasses Solidity safety checks   ", "body": "  Resolution  From the development team:  This has been left unchanged in order to provide more context with a revert when an invalid signature type is used.  Description  The ISignatureValidator contract defines an enum SignatureType to represent the different types of signatures recognized within the exchange. The final enum value, NSignatureTypes, is not a valid signature type. Instead, it is used by MixinSignatureValidator to check that the value read from the signature is a valid enum value. However, Solidity now includes its own check for enum casting, and casting a value over the maximum enum size to an enum is no longer possible.  Because of the added NSignatureTypes value, Solidity s check now recognizes 0x08 as a valid SignatureType value.  Examples  The check is made here:  code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L441-L449  // Ensure signature is supported  if (uint8(signatureType) >= uint8(SignatureType.NSignatureTypes)) {  LibRichErrors.rrevert(LibExchangeRichErrors.SignatureError(  LibExchangeRichErrors.SignatureErrorCodes.UNSUPPORTED,  hash,  signerAddress,  signature  ));  Recommendation  The check should be removed, as should the SignatureTypes.NSignatureTypes value.  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issues section.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The full set of MythX results for both the exchange and staking contracts are available in a separate report.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "7.2 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  S\u016brya s Description Report  Files Description Table  exchange/contracts/src/Exchange.sol  cb6733c32d3306348791b83a9ae76460b75555df  exchange/contracts/src/MixinAssetProxyDispatcher.sol  ee5492092ebea3397d53163cad5cfe8b8050f88e  exchange/contracts/src/MixinExchangeCore.sol  87f9d192c0d75569ee95705baa9c1cdfd129d7a5  exchange/contracts/src/MixinMatchOrders.sol  42868be4aea9327a636766682a8655686af3fc72  exchange/contracts/src/MixinProtocolFees.sol  4982d287aaa206897698039fb34f95f53deda0b5  exchange/contracts/src/MixinSignatureValidator.sol  a69bf0916642b2abaf7e2705d704c00bf2e79150  exchange/contracts/src/MixinTransactions.sol  c3108f751ef627e171ad35c445c8e38cbe0c4d2c  exchange/contracts/src/MixinTransferSimulator.sol  b3ceb9d2e4a8cc1c55648548b950ad1114d29961  exchange/contracts/src/MixinWrapperFunctions.sol  69ea7edd94fc6fd1ede6c6bad139e3e61472c3df  exchange/contracts/src/interfaces/IAssetProxy.sol  21860ce6d0fe6286966dab04b39784f6e2d23857  exchange/contracts/src/interfaces/IAssetProxyDispatcher.sol  f3022084eee2e1a87d4bc023d2aa58d44a3bc3c3  exchange/contracts/src/interfaces/IEIP1271Data.sol  3e98264aa000a238a3f954b17acb6c6606fb3104  exchange/contracts/src/interfaces/IEIP1271Wallet.sol  d99b3b52044cba515a1eebbee67cf5db4f0ae280  exchange/contracts/src/interfaces/IExchange.sol  82d342133ab823431dc07255853f99da8cd49b10  exchange/contracts/src/interfaces/IExchangeCore.sol  48b0562a46653734202a40cc2ce7fcf0e653327a  exchange/contracts/src/interfaces/IMatchOrders.sol  db34eec2bf4bc41c3b51ec35803e1c5aaae4a6fb  exchange/contracts/src/interfaces/IProtocolFees.sol  bcc0151ed53fa72a87102f18015b3bcbf604b4cc  exchange/contracts/src/interfaces/ISignatureValidator.sol  e2304c3b8612ec7b7899d163b82a1bb1145c191a  exchange/contracts/src/interfaces/ITransactions.sol  a2f67b8a9e047c0dc7c33efda4223e087a6e90b4  exchange/contracts/src/interfaces/ITransferSimulator.sol  02ea8f864e3277e1f7c30e0ea38aac177625177d  exchange/contracts/src/interfaces/IWallet.sol  81fbaee73e754cfbc57882e1cd81be5fbf70b9de  exchange/contracts/src/interfaces/IWrapperFunctions.sol  d1b20adfa9b2639aff21e8a0d8f864a5b9435fa4  exchange/contracts/src/libs/LibExchangeRichErrorDecoder.sol  02c13f0e1c57b12da14b0384bebb38d1039bc7c1  exchange-libs/contracts/src/IWallet.sol  d3c769706e00d8a68175a261d79c04a8750b6118  exchange-libs/contracts/src/LibEIP712ExchangeDomain.sol  823955e1f1b21a34ad3fda91c7e691dd68e9a62e  exchange-libs/contracts/src/LibExchangeRichErrors.sol  e58712de5e18edfe951ea694124859ca1a1c05f5  exchange-libs/contracts/src/LibFillResults.sol  49422e7a81067b52f6acc8fe5de1acf21134ee7a  exchange-libs/contracts/src/LibMath.sol  ca6e24ec1de03bdea83351ce5f96082f8f5a9976  exchange-libs/contracts/src/LibMathRichErrors.sol  7f3b0be62d7a8d6f3026018aad08dcc9cbb41825  exchange-libs/contracts/src/LibOrder.sol  114be366ad7a0a711a0c2e552500a2c9fb1bbddf  exchange-libs/contracts/src/LibZeroExTransaction.sol  95ea4427d1df12aef259e07ac6215f2e2d9bd6d9  multisig/contracts/src/AssetProxyOwner.sol  df9ed7cba84c1362fee9de80d7774592323a86df  multisig/contracts/src/MultiSigWallet.sol  33b84d070486847dcc86a140fd682a1d8c953164  multisig/contracts/src/MultiSigWalletWithTimeLock.sol  c54d8b6631eacb20fe6bfad6ee268ab81c112614  utils/contracts/src/Authorizable.sol  2ae731a21730cfdd30feb5d20da4d4d2fa194e1d  utils/contracts/src/LibAddress.sol  33eef1855488fbbbfd1eed92101f379343a8f0f7  utils/contracts/src/LibAddressArray.sol  b13d0359922c04fadb4b24abd3d5318462c62d8e  utils/contracts/src/LibAddressArrayRichErrors.sol  883bc123ba699ba1efc11a75f806e1150e8af1ba  utils/contracts/src/LibAuthorizableRichErrors.sol  abfba41b1c63ba91803721d4d0ec6a7a1678752b  utils/contracts/src/LibBytes.sol  7a0c37b1577f5a12378fbf529177ca62314a4e62  utils/contracts/src/LibBytesRichErrors.sol  611b4e660351ee4e24140074ee1df49756e496ec  utils/contracts/src/LibEIP1271.sol  2fe0c70163677ea228d9bcfecdbba2627a5be77f  utils/contracts/src/LibEIP712.sol  3b486180d6ee3e6d5e1f2fa57c1ca060a1bdca9b  utils/contracts/src/LibFractions.sol  552a637f32edb135942cd1ea25e88d6972b8cf79  utils/contracts/src/LibOwnableRichErrors.sol  dfda0c5639f5fc994712421dc92b284071fc9e56  utils/contracts/src/LibReentrancyGuardRichErrors.sol  8af2504839d0b9a4a7a4694886704bee31fb43ad  utils/contracts/src/LibRichErrors.sol  3be89d9503f6fb6aee08aa515119af83d63f7d29  utils/contracts/src/LibSafeMath.sol  f095f7330b0d2b0d85370b47bd5ac98360ed5b48  utils/contracts/src/LibSafeMathRichErrors.sol  7785c4a4076e3f0be3319ec4bc17aca0090c2ce0  utils/contracts/src/Ownable.sol  8ede7b82d2ee0ed63b2162709d8afa7250efc3cf  utils/contracts/src/ReentrancyGuard.sol  5364694b8a2bba36861bfdd8d5886ece26e301a4  utils/contracts/src/Refundable.sol  0fe9acae963bb683b6c3539de8377ed05240bae0  utils/contracts/src/SafeMath.sol  5b675f9c12bf862a72c7dc71d00839214d970d34  utils/contracts/src/interfaces/IAuthorizable.sol  3a438f74bdb79cf6bff4dbe52a31651928601022  utils/contracts/src/interfaces/IOwnable.sol  5fe3a74b7d5948bba5644db684459d87e84fb5c6  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  Exchange  Implementation  LibEIP712ExchangeDomain, MixinMatchOrders, MixinWrapperFunctions, MixinTransferSimulator  <Constructor>  Public    LibEIP712ExchangeDomain  MixinAssetProxyDispatcher  Implementation  Ownable, IAssetProxyDispatcher  registerAssetProxy  External    onlyOwner  getAssetProxy  External    NO   _dispatchTransferFrom  Internal \ud83d\udd12  MixinExchangeCore  Implementation  IExchangeCore, Refundable, LibEIP712ExchangeDomain, MixinAssetProxyDispatcher, MixinProtocolFees, MixinSignatureValidator  cancelOrdersUpTo  External    refundFinalBalanceNoReentry  fillOrder  Public    refundFinalBalanceNoReentry  cancelOrder  Public    refundFinalBalanceNoReentry  getOrderInfo  Public    NO   _fillOrder  Internal \ud83d\udd12  _cancelOrder  Internal \ud83d\udd12  _updateFilledState  Internal \ud83d\udd12  _updateCancelledState  Internal \ud83d\udd12  _assertFillableOrder  Internal \ud83d\udd12  _assertValidCancel  Internal \ud83d\udd12  _settleOrder  Internal \ud83d\udd12  _getOrderHashAndFilledAmount  Internal \ud83d\udd12  MixinMatchOrders  Implementation  MixinExchangeCore, IMatchOrders  batchMatchOrders  Public    refundFinalBalanceNoReentry  batchMatchOrdersWithMaximalFill  Public    refundFinalBalanceNoReentry  matchOrders  Public    refundFinalBalanceNoReentry  matchOrdersWithMaximalFill  Public    refundFinalBalanceNoReentry  _assertValidMatch  Internal \ud83d\udd12  _batchMatchOrders  Internal \ud83d\udd12  _matchOrders  Internal \ud83d\udd12  _settleMatchedOrders  Internal \ud83d\udd12  MixinProtocolFees  Implementation  IProtocolFees, Ownable  setProtocolFeeMultiplier  External    onlyOwner  setProtocolFeeCollectorAddress  External    onlyOwner  _paySingleProtocolFee  Internal \ud83d\udd12  _payTwoProtocolFees  Internal \ud83d\udd12  _payProtocolFeeToFeeCollector  Internal \ud83d\udd12  MixinSignatureValidator  Implementation  LibEIP712ExchangeDomain, LibEIP1271, ISignatureValidator, MixinTransactions  preSign  External    refundFinalBalanceNoReentry  setSignatureValidatorApproval  External    refundFinalBalanceNoReentry  isValidHashSignature  Public    NO   isValidOrderSignature  Public    NO   isValidTransactionSignature  Public    NO   _isValidOrderWithHashSignature  Internal \ud83d\udd12  _isValidTransactionWithHashSignature  Internal \ud83d\udd12  _validateHashSignatureTypes  Private \ud83d\udd10  _readSignatureType  Private \ud83d\udd10  _readValidSignatureType  Private \ud83d\udd10  _encodeEIP1271OrderWithHash  Private \ud83d\udd10  _encodeEIP1271TransactionWithHash  Private \ud83d\udd10  _validateHashWithWallet  Private \ud83d\udd10  _validateBytesWithWallet  Private \ud83d\udd10  _validateBytesWithValidator  Private \ud83d\udd10  _staticCallEIP1271WalletWithReducedSignatureLength  Private \ud83d\udd10  MixinTransactions  Implementation  Refundable, LibEIP712ExchangeDomain, ISignatureValidator, ITransactions  executeTransaction  Public    disableRefundUntilEnd  batchExecuteTransactions  Public    disableRefundUntilEnd  _executeTransaction  Internal \ud83d\udd12  _assertExecutableTransaction  Internal \ud83d\udd12  _setCurrentContextAddressIfRequired  Internal \ud83d\udd12  _getCurrentContextAddress  Internal \ud83d\udd12  MixinTransferSimulator  Implementation  MixinAssetProxyDispatcher  simulateDispatchTransferFromCalls  Public    NO   MixinWrapperFunctions  Implementation  IWrapperFunctions, MixinExchangeCore  fillOrKillOrder  Public    refundFinalBalanceNoReentry  batchFillOrders  Public    refundFinalBalanceNoReentry  batchFillOrKillOrders  Public    refundFinalBalanceNoReentry  batchFillOrdersNoThrow  Public    disableRefundUntilEnd  marketSellOrdersNoThrow  Public    disableRefundUntilEnd  marketBuyOrdersNoThrow  Public    disableRefundUntilEnd  marketSellOrdersFillOrKill  Public    NO   marketBuyOrdersFillOrKill  Public    NO   batchCancelOrders  Public    refundFinalBalanceNoReentry  _fillOrKillOrder  Internal \ud83d\udd12  _fillOrderNoThrow  Internal \ud83d\udd12  IAssetProxy  Implementation  transferFrom  External    NO   getProxyId  External    NO   IAssetProxyDispatcher  Implementation  registerAssetProxy  External    NO   getAssetProxy  External    NO   IEIP1271Data  Implementation  OrderWithHash  External    NO   ZeroExTransactionWithHash  External    NO   IEIP1271Wallet  Implementation  LibEIP1271  isValidSignature  External    NO   IExchange  Implementation  IProtocolFees, IExchangeCore, IMatchOrders, ISignatureValidator, ITransactions, IAssetProxyDispatcher, ITransferSimulator, IWrapperFunctions  IExchangeCore  Implementation  cancelOrdersUpTo  External    NO   fillOrder  Public    NO   cancelOrder  Public    NO   getOrderInfo  Public    NO   IMatchOrders  Implementation  batchMatchOrders  Public    NO   batchMatchOrdersWithMaximalFill  Public    NO   matchOrders  Public    NO   matchOrdersWithMaximalFill  Public    NO   IProtocolFees  Implementation  setProtocolFeeMultiplier  External    NO   setProtocolFeeCollectorAddress  External    NO   protocolFeeMultiplier  External    NO   protocolFeeCollector  External    NO   ISignatureValidator  Implementation  preSign  External    NO   setSignatureValidatorApproval  External    NO   isValidHashSignature  Public    NO   isValidOrderSignature  Public    NO   isValidTransactionSignature  Public    NO   _isValidOrderWithHashSignature  Internal \ud83d\udd12  _isValidTransactionWithHashSignature  Internal \ud83d\udd12  ITransactions  Implementation  executeTransaction  Public    NO   batchExecuteTransactions  Public    NO   _getCurrentContextAddress  Internal \ud83d\udd12  ITransferSimulator  Implementation  simulateDispatchTransferFromCalls  Public    NO   IWallet  Implementation  isValidSignature  External    NO   IWrapperFunctions  Implementation  fillOrKillOrder  Public    NO   batchFillOrders  Public    NO   batchFillOrKillOrders  Public    NO   batchFillOrdersNoThrow  Public    NO   marketSellOrdersNoThrow  Public    NO   marketBuyOrdersNoThrow  Public    NO   marketSellOrdersFillOrKill  Public    NO   marketBuyOrdersFillOrKill  Public    NO   batchCancelOrders  Public    NO   LibExchangeRichErrorDecoder  Implementation  decodeSignatureError  Public    NO   decodeEIP1271SignatureError  Public    NO   decodeSignatureValidatorNotApprovedError  Public    NO   decodeSignatureWalletError  Public    NO   decodeOrderStatusError  Public    NO   decodeExchangeInvalidContextError  Public    NO   decodeFillError  Public    NO   decodeOrderEpochError  Public    NO   decodeAssetProxyExistsError  Public    NO   decodeAssetProxyDispatchError  Public    NO   decodeAssetProxyTransferError  Public    NO   decodeNegativeSpreadError  Public    NO   decodeTransactionError  Public    NO   decodeTransactionExecutionError  Public    NO   decodeIncompleteFillError  Public    NO   _assertSelectorBytes  Private \ud83d\udd10  IWallet  Implementation  isValidSignature  External    NO   LibEIP712ExchangeDomain  Implementation  <Constructor>  Public    LibExchangeRichErrors  Library  SignatureErrorSelector  Internal \ud83d\udd12  SignatureValidatorNotApprovedErrorSelector  Internal \ud83d\udd12  EIP1271SignatureErrorSelector  Internal \ud83d\udd12  SignatureWalletErrorSelector  Internal \ud83d\udd12  OrderStatusErrorSelector  Internal \ud83d\udd12  ExchangeInvalidContextErrorSelector  Internal \ud83d\udd12  FillErrorSelector  Internal \ud83d\udd12  OrderEpochErrorSelector  Internal \ud83d\udd12  AssetProxyExistsErrorSelector  Internal \ud83d\udd12  AssetProxyDispatchErrorSelector  Internal \ud83d\udd12  AssetProxyTransferErrorSelector  Internal \ud83d\udd12  NegativeSpreadErrorSelector  Internal \ud83d\udd12  TransactionErrorSelector  Internal \ud83d\udd12  TransactionExecutionErrorSelector  Internal \ud83d\udd12  IncompleteFillErrorSelector  Internal \ud83d\udd12  BatchMatchOrdersErrorSelector  Internal \ud83d\udd12  TransactionGasPriceErrorSelector  Internal \ud83d\udd12  TransactionInvalidContextErrorSelector  Internal \ud83d\udd12  PayProtocolFeeErrorSelector  Internal \ud83d\udd12  BatchMatchOrdersError  Internal \ud83d\udd12  SignatureError  Internal \ud83d\udd12  SignatureValidatorNotApprovedError  Internal \ud83d\udd12  EIP1271SignatureError  Internal \ud83d\udd12  SignatureWalletError  Internal \ud83d\udd12  OrderStatusError  Internal \ud83d\udd12  ExchangeInvalidContextError  Internal \ud83d\udd12  FillError  Internal \ud83d\udd12  OrderEpochError  Internal \ud83d\udd12  AssetProxyExistsError  Internal \ud83d\udd12  AssetProxyDispatchError  Internal \ud83d\udd12  AssetProxyTransferError  Internal \ud83d\udd12  NegativeSpreadError  Internal \ud83d\udd12  TransactionError  Internal \ud83d\udd12  TransactionExecutionError  Internal \ud83d\udd12  TransactionGasPriceError  Internal \ud83d\udd12  TransactionInvalidContextError  Internal \ud83d\udd12  IncompleteFillError  Internal \ud83d\udd12  PayProtocolFeeError  Internal \ud83d\udd12  LibFillResults  Library  calculateFillResults  Internal \ud83d\udd12  calculateMatchedFillResults  Internal \ud83d\udd12  addFillResults  Internal \ud83d\udd12  _calculateMatchedFillResults  Private \ud83d\udd10  _calculateMatchedFillResultsWithMaximalFill  Private \ud83d\udd10  _calculateCompleteFillBoth  Private \ud83d\udd10  _calculateCompleteRightFill  Private \ud83d\udd10  LibMath  Library  safeGetPartialAmountFloor  Internal \ud83d\udd12  safeGetPartialAmountCeil  Internal \ud83d\udd12  getPartialAmountFloor  Internal \ud83d\udd12  getPartialAmountCeil  Internal \ud83d\udd12  isRoundingErrorFloor  Internal \ud83d\udd12  isRoundingErrorCeil  Internal \ud83d\udd12  LibMathRichErrors  Library  DivisionByZeroError  Internal \ud83d\udd12  RoundingError  Internal \ud83d\udd12  LibOrder  Library  getTypedDataHash  Internal \ud83d\udd12  getStructHash  Internal \ud83d\udd12  LibZeroExTransaction  Library  getTypedDataHash  Internal \ud83d\udd12  getStructHash  Internal \ud83d\udd12  TestLibEIP712ExchangeDomain  Implementation  LibEIP712ExchangeDomain  <Constructor>  Public    LibEIP712ExchangeDomain  TestLibFillResults  Implementation  calculateFillResults  Public    NO   calculateMatchedFillResults  Public    NO   addFillResults  Public    NO   TestLibMath  Implementation  safeGetPartialAmountFloor  Public    NO   safeGetPartialAmountCeil  Public    NO   getPartialAmountFloor  Public    NO   getPartialAmountCeil  Public    NO   isRoundingErrorFloor  Public    NO   isRoundingErrorCeil  Public    NO   TestLibOrder  Implementation  getTypedDataHash  Public    NO   getStructHash  Public    NO   TestLibZeroExTransaction  Implementation  getTypedDataHash  Public    NO   getStructHash  Public    NO   AssetProxyOwner  Implementation  MultiSigWalletWithTimeLock  <Constructor>  Public    MultiSigWalletWithTimeLock  registerFunctionCall  External    onlyWallet  executeTransaction  Public    notExecuted fullyConfirmed  _registerFunctionCall  Internal \ud83d\udd12  _assertValidFunctionCall  Internal \ud83d\udd12  MultiSigWallet  Implementation  <Fallback>  External    NO   <Constructor>  Public    validRequirement  addOwner  Public    onlyWallet ownerDoesNotExist notNull validRequirement  removeOwner  Public    onlyWallet ownerExists  replaceOwner  Public    onlyWallet ownerExists ownerDoesNotExist  changeRequirement  Public    onlyWallet validRequirement  submitTransaction  Public    NO   confirmTransaction  Public    ownerExists transactionExists notConfirmed  revokeConfirmation  Public    ownerExists confirmed notExecuted  executeTransaction  Public    ownerExists confirmed notExecuted  _externalCall  Internal \ud83d\udd12  isConfirmed  Public    NO   _addTransaction  Internal \ud83d\udd12  notNull  getConfirmationCount  Public    NO   getTransactionCount  Public    NO   getOwners  Public    NO   getConfirmations  Public    NO   getTransactionIds  Public    NO   MultiSigWalletWithTimeLock  Implementation  MultiSigWallet  <Constructor>  Public    MultiSigWallet  changeTimeLock  Public    onlyWallet  confirmTransaction  Public    ownerExists transactionExists notConfirmed notFullyConfirmed  executeTransaction  Public    notExecuted fullyConfirmed pastTimeLock  _setConfirmationTime  Internal \ud83d\udd12  Authorizable  Implementation  Ownable, IAuthorizable  <Constructor>  Public    Ownable  addAuthorizedAddress  External    onlyOwner  removeAuthorizedAddress  External    onlyOwner  removeAuthorizedAddressAtIndex  External    onlyOwner  getAuthorizedAddresses  External    NO   _assertSenderIsAuthorized  Internal \ud83d\udd12  _addAuthorizedAddress  Internal \ud83d\udd12  _removeAuthorizedAddressAtIndex  Internal \ud83d\udd12  LibAddress  Library  isContract  Internal \ud83d\udd12  LibAddressArray  Library  append  Internal \ud83d\udd12  contains  Internal \ud83d\udd12  indexOf  Internal \ud83d\udd12  LibAddressArrayRichErrors  Library  MismanagedMemoryError  Internal \ud83d\udd12  LibAuthorizableRichErrors  Library  AuthorizedAddressMismatchError  Internal \ud83d\udd12  IndexOutOfBoundsError  Internal \ud83d\udd12  SenderNotAuthorizedError  Internal \ud83d\udd12  TargetAlreadyAuthorizedError  Internal \ud83d\udd12  TargetNotAuthorizedError  Internal \ud83d\udd12  ZeroCantBeAuthorizedError  Internal \ud83d\udd12  LibBytes  Library  rawAddress  Internal \ud83d\udd12  contentAddress  Internal \ud83d\udd12  memCopy  Internal \ud83d\udd12  slice  Internal \ud83d\udd12  sliceDestructive  Internal \ud83d\udd12  popLastByte  Internal \ud83d\udd12  equals  Internal \ud83d\udd12  readAddress  Internal \ud83d\udd12  writeAddress  Internal \ud83d\udd12  readBytes32  Internal \ud83d\udd12  writeBytes32  Internal \ud83d\udd12  readUint256  Internal \ud83d\udd12  writeUint256  Internal \ud83d\udd12  readBytes4  Internal \ud83d\udd12  writeLength  Internal \ud83d\udd12  LibBytesRichErrors  Library  InvalidByteOperationError  Internal \ud83d\udd12  LibEIP1271  Implementation  LibEIP712  Library  hashEIP712Domain  Internal \ud83d\udd12  hashEIP712Message  Internal \ud83d\udd12  LibFractions  Library  add  Internal \ud83d\udd12  normalize  Internal \ud83d\udd12  normalize  Internal \ud83d\udd12  scaleDifference  Internal \ud83d\udd12  LibOwnableRichErrors  Library  OnlyOwnerError  Internal \ud83d\udd12  TransferOwnerToZeroError  Internal \ud83d\udd12  LibReentrancyGuardRichErrors  Library  IllegalReentrancyError  Internal \ud83d\udd12  LibRichErrors  Library  StandardError  Internal \ud83d\udd12  rrevert  Internal \ud83d\udd12  LibSafeMath  Library  safeMul  Internal \ud83d\udd12  safeDiv  Internal \ud83d\udd12  safeSub  Internal \ud83d\udd12  safeAdd  Internal \ud83d\udd12  max256  Internal \ud83d\udd12  min256  Internal \ud83d\udd12  LibSafeMathRichErrors  Library  Uint256BinOpError  Internal \ud83d\udd12  Uint256DowncastError  Internal \ud83d\udd12  Ownable  Implementation  IOwnable  <Constructor>  Public    transferOwnership  Public    onlyOwner  _assertSenderIsOwner  Internal \ud83d\udd12  ReentrancyGuard  Implementation  _lockMutexOrThrowIfAlreadyLocked  Internal \ud83d\udd12  _unlockMutex  Internal \ud83d\udd12  Refundable  Implementation  ReentrancyGuard  _refundNonZeroBalanceIfEnabled  Internal \ud83d\udd12  _refundNonZeroBalance  Internal \ud83d\udd12  _disableRefund  Internal \ud83d\udd12  _enableAndRefundNonZeroBalance  Internal \ud83d\udd12  _areRefundsDisabled  Internal \ud83d\udd12  SafeMath  Implementation  _safeMul  Internal \ud83d\udd12  _safeDiv  Internal \ud83d\udd12  _safeSub  Internal \ud83d\udd12  _safeAdd  Internal \ud83d\udd12  _max256  Internal \ud83d\udd12  _min256  Internal \ud83d\udd12  IAuthorizable  Implementation  IOwnable  addAuthorizedAddress  External    NO   removeAuthorizedAddress  External    NO   removeAuthorizedAddressAtIndex  External    NO   getAuthorizedAddresses  External    NO   IOwnable  Implementation  transferOwnership  Public    NO   Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/0x-v3-exchange/"}, {"title": "5.1 Superfluous Permission endowment:ethereum-provider    ", "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by removing the ethereum provider permission from the manifest.  Description  The snap requests permission endowment:ethereum-provider but window.ethereum is never accessed from within the snap s context.  snap/snap.manifest.json:L39  \"endowment:ethereum-provider\": {}  Recommendation  Remove superfluous permissions.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.2 A Trusted Website Can Add Any Address to the Snaps Address Storage; No Control Over Added Addresses; Confirmation Is a Notification    ", "body": "  Resolution  partially addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by only allowing trusted origins to interact with the snap.  Update: user confirmation for address management (add/remove current account) added with ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Description  Trusted websites can add addresses to the list of addresses the user wants to receive notifications for. However, the user has no control over the addresses, and even though the code suggests that the snap user must confirm new address addition, this confirmation is merely a notification that the address has been added.  The lack of address management may lead to a self-DoS when too many addresses are added to the extension.  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: \"local:http://localhost:8080\",  request: { method: 'hello', params: { address: \"\\nhi\\nho\" } },  }})  push-snap-site/components/buttons/ConfirmButton.tsx:L6-L35  export default function ConfirmButton() {  const { address, isConnecting, isDisconnected } = useAccount();  const defaultSnapOrigin = `local:http://localhost:8080`;  const sendHello = async (address: string) => {  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: defaultSnapOrigin,  request: { method: 'hello', params: { address: address } },  },  });  };  const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({  message:  `Confirm your Address ${address}, \\n this will be added to MetaMask for sending notifications`,  });  function sleep(ms: number) {  return new Promise((resolve) => setTimeout(resolve, ms));  const confirmAddition=async()=>{  signMessage();  if(isSuccess){  await sleep(5000);  await sendHello(String(address));  The same is true for configuration settings. Any connected dap may set togglepopup. This may be problematic in multi-dapp scenarios where multiple dapps request to set togglepopup.  Recommendation  Note that dapps are not necessarily completely trusted. They can be modified, or malicious behavior may be added later by the dapp deployer (unless used locally or via IPFS). Therefore, the snap should always notify the wallet owner of important state changes and allow them to reject them or, in this case, manage addresses that ve been added previously.  Consider checking the origin in onRPC if this dapp is only meant to be called from a specific dapp address. Otherwise, any connected dapp may change configuration settings.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.3 Lax Input Validation, Control Char, URI, and Markdown Injection    ", "body": "  Resolution  addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 validating the address with ethers.utils.isAddress.  Update 1:  Markdown rendering of newlines fixed with: ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Major: Markdown Injection in Confirmation Dialogue re-introduced with ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Update 2:  Markdown Injection in Confirmation Dialogue fixed with ethereum-push-notification-service/push-protocol-snaps@b40e141243c77bfd7ec109408b326607b19314c8  Description  There is no input validation on the address to be added. The input may be an ethereum address but can be anything, potentially breaking security assumptions in the code and leading to unwanted side effects.  request.params may be null, and  request.params.address may not be an ethereum address.  snap/src/index.ts:L18  await addAddress(request.params.address || \"0x0\");  Example  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: \"local:http://localhost:8080\",  request: { method: 'hello', params: { address: \"Hi \ud83d\ude4c\\n\\n \ud83d\udd38 **boom**\" } },  }})  URI injection if address contains ?#/  snap/src/utils/fetchnotifs.ts:L3-L13  export const getNotifications=async(address:string)=>{  const url = `https://backend-prod.epns.io/apis/v1/users/eip155:5:${address}/feeds`;  const response = await fetch(url, {  method: 'get',  headers: {  'Content-Type': 'application/json',  },  });  const data = await response.json();  return data;  Injection in notifications  snap/src/utils/popupHelper.ts:L3-L12  export const popupHelper = (notifs: String[]) => {  let msg = [];  if (notifs.length > 0) {  notifs.forEach((notif) => {  let str = `\\n\ud83d\udd14` + notif + \"\\n\";  msg.push(str);  });  return msg;  };  Markdown injection  snap/src/utils/fetchAddress.ts:L45-L52  const data = persistedData.addresses;  const popup = persistedData.popuptoggle;  let msg='';  for(let i = 0; i < data!.length; i++){  msg = msg + '\ud83d\udd39' + data![i] + '\\n';  return snap.request({  method: 'snap_dialog',  Also, note that the currently rendered markdown that lists addresses appears wrong, as markdown newlines require \\n\\n instead of \\n.  Recommendation  Strictly validate inputs from external origins. Ensure that the provided address is a valid ethereum address. Optionally check the addresses checksum to detect typos. Ensure that inputs may not lead to renderable markdown. Fix the rendered list of addresses to properly display as a newline d list. Ensure untrusted inputs cannot inject context-sensitive information into fetch urls.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.4 persistedData Race Where snap_manageState.get Returnsnull    ", "body": "  Resolution   addressed with   ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829 by introducing a wrapper function that ensures that snapstate returns sane defaults. This function is not used everywhere, but in places where it is not, custom checks are employed.  Description  Metamask Error:  snap.request(, {method: 'snap_manageState', params: {operation: 'get'}}) may return null. Snap state is only initialized on rpc request method hello via addAddress().  This is the only method that checks if the retrieved state is null:  snap/src/utils/fetchAddress.ts:L5-L20  export const addAddress = async (address:string) => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  if(persistedData == null){  const data = {  addresses: [address],  popuptoggle: 0,  };  await snap.request({  method: 'snap_manageState',  params: { operation: 'update', newState:data },  });  snap/src/index.ts:L12-L21  export const onRpcRequest: OnRpcRequestHandler = async ({  origin,  request,  }) => {  switch (request.method) {  case \"hello\": {  await addAddress(request.params.address || \"0x0\");  await confirmAddress();  break;  If the state was never initialized or there was a race where rpc-hello() was not called first, then the snap may run into a null deref exception (here rpc-togglepopup):  snap/src/utils/toggleHelper.ts:L2-L12  let persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  let popuptoggle = notifcount;  const data = {  addresses: persistedData.addresses,  popuptoggle: popuptoggle,  };  Recommendation  Wrap snap_manageState with a function that always falls back to safe defaults if the snap state was never set. This also obsoleted the future need to check if persistedData is null as the new method ensures safe non-null defaults.  This should also silence some of the type errors reported by tslint that warn that attributes of persistentdata are read while it might be null (see issue 5.6 ).  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.5 User Flow - Request to Sign Message Does Not Provide Security Guarantee    ", "body": "  Resolution   obsolete, removed with   ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829.  Description  A connected dapp can add any address to the snap via the RPC method hello. There is no added security by requesting the user to sign with their address as the backend API gives access to any address notification (they are not private) and the dapps request is a front-end-only solution. A user may add any other address by creating their dapp which allows custom addresses.  In light of this, the front-end (dapp) security check requiring the user to prove that they are in possession of the private key appears not to add any security guarantees to the snap. Instead, the snap may want to enumerate wallet account addresses internally instead and remove the hello API altogether, or, allow any address to be added without requiring a proof of ownership of an address.  Examples  push-snap-site/components/buttons/ConfirmButton.tsx:L20-L23  const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({  message:  `Confirm your Address ${address}, \\n this will be added to MetaMask for sending notifications`,  });  Recommendation  Remove the signature check, and add linked accounts from within the snaps context. Be transparent that notification texts are not private, and anyone can subscribe to the back-end API. If notifications are private to the recipient, we suggest encrypting them for the target account and adding logic in the snap to allow the recipient to decrypt them within the context of the snap.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.6 TypeScript Errors    ", "body": "  Resolution  partially addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761:  toggleHelper not addressed.  popupHelper addressed as per recommendation.  fetchAllAddrNotifs fixed by forcing fetchAddress() to return empty array instead.  persistedData partially addressed. might still null-deref at persistedData.addresses in index.ts  Update: toggleHelper and persistedData addressed with the snap data check wrapper function in ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Description  toggleHelper  persistedData should be checked for null and default to a sane initial config. notifcount:Number should be notifcount:number.  Type '{ addresses: Json; popuptoggle: Number; }' is not assignable to type 'Record<string, Json>'.  Property 'popuptoggle' is incompatible with index signature.  Type 'Number' is not assignable to type 'Json'.  Type 'Number' is not assignable to type '{ [prop: string]: Json; }'.  Index signature for type 'string' is missing in type 'Number'.ts(2322)  snap/src/utils/toggleHelper.ts:L7-L16  let popuptoggle = notifcount;  const data = {  addresses: persistedData.addresses,  popuptoggle: popuptoggle,  };  await snap.request({  method: 'snap_manageState',  params: { operation: 'update', newState:data },  });  popupHelper  let msg = [] should be let msg = [] as String[];  Variable 'msg' implicitly has an 'any[]' type.ts(7005)  addresses can be null  snap/src/utils/fetchnotifs.ts:L34-L37  export const fetchAllAddrNotifs = async () => {  const addresses = await fetchAddress();  let notifs:String[] = [];  for(let i = 0; i < addresses.length; i++){  persistedData can be null  snap/src/index.ts:L63-L68  let persistedData = await snap.request({  method: \"snap_manageState\",  params: { operation: \"get\" },  });  let popuptoggle = Number(persistedData.popuptoggle) + msgs.length;  Recommendation  Fix the typescript configuration (see issue 5.13 ). Fix all reported ts-lint errors. Avoid using any types and use safe types instead.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.7 Avoid Hardcoding the Local Snap ID ", "body": "  Description  The local snap-id is hardcoded in various places. Local snap IDs should not be used in production. Hence, we recommend defining and importing the snap id from a single source file within the project, setting it to local:http://localhost:8080 and npm:push-v1 depending on whether the build is set to be production or development (e.g., using an environment variable).  push-snap-site/components/buttons/ConfirmButton.tsx:L6-L8  export default function ConfirmButton() {  const { address, isConnecting, isDisconnected } = useAccount();  const defaultSnapOrigin = `local:http://localhost:8080`;  push-snap-site/components/buttons/ReconnectButton.tsx:L4-L6  export default function ReconnectButton() {  const defaultSnapOrigin = `local:http://localhost:8080`;  push-snap-site/components/buttons/SendMessageButton.tsx:L1-L3  export default function ReconnectButton() {  const defaultSnapOrigin = `local:http://localhost:8080`;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.8 package.json - Invalid License    ", "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by changing the license to GPLv2.  Description  The license field in package.json is invalid.  snap/package.json:L9  \"license\": \"(MIT-0 OR Apache-2.0)\",  Recommendation  Update the license field.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.9 fetchAddress  - Inaccurate Function Name ", "body": "  Description  Function fetchAddress returns an array of addresses and should, therefore, be named fetchAddresses  snap/src/utils/fetchAddress.ts:L66-L73  export const fetchAddress = async () => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  const addresses = persistedData!.addresses;  return addresses;  };  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.10 currentepoch - Unnecesary Conversion From/to String    ", "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by not converting  Description  It is unclear why currentepoch is declared as String while calculations require it to be numerical.  Examples  snap/src/utils/fetchnotifs.ts:L15-L31  export const filterNotifications=async(address:string)=>{  let fetchedNotifications = await getNotifications(address);  fetchedNotifications = fetchedNotifications?.feeds;  let notiffeeds:String[] = [];  const currentepoch:string = Math.floor(Date.now() / 1000).toString();  if(fetchedNotifications.length > 0){  for(let i = 0; i < fetchedNotifications.length; i++){  let feedepoch = fetchedNotifications[i].payload.data.epoch;  feedepoch = Number(feedepoch).toFixed(0);  if(feedepoch > parseInt(currentepoch)-60) {  let msg = fetchedNotifications[i].payload.data.app+' : '+fetchedNotifications[i].payload.data.amsg;  notiffeeds.push(msg);  notiffeeds = notiffeeds.reverse();  return notiffeeds;  Recommendation  currentepoch should be numerical.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.11 Dead Code popup    ", "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by removing the used code.  Description  const popup is retrieved from the snap state but never used within the context of confirmAddress(). This might be an indicator of an incomplete implementation of the togglePopup setting or dead code.  snap/src/utils/fetchAddress.ts:L40-L47  export const confirmAddress = async () => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  const data = persistedData.addresses;  const popup = persistedData.popuptoggle;  let msg='';  Recommendation  Double check if this setting is meant to be read (unlikely) or else clean up and remove unused code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.12 Unused Import ethers, @metamask/snaps-ui    ", "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by using  Description  ethers  ethers is listed as a dependency and imported by fetchAddress.ts but is never used.  snap/src/utils/fetchAddress.ts:L3  const {ethers} = require('ethers');  @metamask/snaps-ui  @metamask/snaps-ui is imported in popupHelper but the imported components are never used.  snap/src/utils/popupHelper.ts:L1  import { heading, panel, text } from \"@metamask/snaps-ui\";  Recommendation  Remove the unused import/dependency.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.13 Non-Existent Base Config (Eslint, Tsconfig) ", "body": "  Description  .eslintrc.js points to a base configuration outside of this repository.  snap/.eslintrc.js:L2  extends: ['../../.eslintrc.js'],  .eslintrc.js  tsconfig.json  snap/tsconfig.json:L2  \"extends\": \"../../tsconfig.json\",  Recommendation  Provide the eslint base configuration with the repository to allow for reproducible lint runs. Run the linter as part of github commit checks.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.14 Performance - await in for Loop   ", "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by using  Description  Performing an await as part of each operation is an indication that the program is not taking full advantage of the parallelization benefits of async/await:  snap/src/utils/fetchnotifs.ts:L38  let temp = await filterNotifications(addresses[i]);  Recommendation  Using Promise.all() fully utilizes parallelism and improves performance  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.15 API Design - Consider Using Consistent RPC Method Names   ", "body": "  Resolution   fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by changing the rpc method names as per recommendation.  Description  Consider using descriptive RPC method names with a distinct prefix, e.g. pushproto_initialize, pushproto_addaddress, pushprotoc_togglepopup:  snap/src/index.ts:L17  case \"hello\": {  snap/src/index.ts:L22  case \"init\": {  snap/src/index.ts:L36  case \"togglepopup\": {  Note that init can be called multiple times and is not initializing anything.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "7.1 readOnlyMode is ineffective and may result in a false sense of security    Addressed", "body": "  Resolution   This was addressed in   PegaSysEng/permissioning-smart-contracts@ed2d4a2 by adding comments to clarify that  Description  AccountRules and NodeRules can both enter and exit a mode of operation called readOnlyMode.  The only effect of readOnlyMode is to prevent admins (who are the only users able to change rules) from changing rules.  Those same admins can disable readOnlyMode, so this mode will not prevent a determined actor from doing something they want to do.  Recommendation  Either readOnlyMode should be removed to prevent it from providing a false sense of security, or the authorization required to toggle readOnlyMode should be separated from the authorization required to change rules.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.2 Ingress.setContractAddress() can cause duplicate entries in contractKeys    ", "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@faff726.  Description  code/contracts/Ingress.sol:L39-L62  function setContractAddress(bytes32 name, address addr) public returns (bool) {  require(name > 0x0000000000000000000000000000000000000000000000000000000000000000, \"Contract name must not be empty.\");  require(isAuthorized(msg.sender), \"Not authorized to update contract registry.\");  ContractDetails memory info = registry[name];  // create info if it doesn't exist in the registry  if (info.contractAddress == address(0)) {  info = ContractDetails({  owner: msg.sender,  contractAddress: addr  });  // Update registry indexing  contractKeys.push(name);  } else {  info.contractAddress = addr;  // update record in the registry  registry[name] = info;  emit RegistryUpdated(addr,name);  return true;  If, however, a contract is actually added with the address 0, which is currently allowed in the code, then the contract does already exists, and adding the name to contractKeys again will result in a duplicate.  Mitigation  An admin can call removeContract repeatedly with the same name to remove multiple duplicate entries.  Recommendation  Either disallow a contract address of 0 or check for existence via the owner field instead (which can never be 0).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.3 Use specific contract types instead of address where possible    ", "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@05d33ae and  PegaSysEng/permissioning-smart-contracts@2728bac.  Description  For clarity and to get more out of the Solidity type checker, it s generally preferred to use a specific contract type for variables rather than the generic address.  Examples  AccountRules.ingressContractAddress could instead be AccountRules.ingressContract and use the type IngressContract:  code/contracts/AccountRules.sol:L16  address private ingressContractAddress;  code/contracts/AccountRules.sol:L24  AccountIngress ingressContract = AccountIngress(ingressContractAddress);  code/contracts/AccountRules.sol:L32  constructor (address ingressAddress) public {  This same pattern is found in NodeRules:  code/contracts/NodeRules.sol:L32  address private nodeIngressContractAddress;  Recommendation  Where possible, use a specific contract type rather than address.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.4 Ingress should use a set    ", "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@2978bd0 and  PegaSysEng/permissioning-smart-contracts@f973035.  Description  The AdminList, AccountRulesList, and NodeRulesList contracts have been recently rewritten to use a set. Ingress has the semantics of a set but has not been written the same way.  This leads to some inefficiencies. In particular, Ingress.removeContract is an O(n) operation:  code/contracts/Ingress.sol:L68-L74  for (uint i = 0; i < contractKeys.length; i++) {  // Delete the key from the array + mapping if it is present  if (contractKeys[i] == name) {  delete registry[contractKeys[i]];  contractKeys[i] = contractKeys[contractKeys.length - 1];  delete contractKeys[contractKeys.length - 1];  contractKeys.length--;  Recommendation  Use the same set implementation for Ingress: an array of ContractDetails and a mapping of names to indexes in that array.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.5 Use a specific Solidity compiler version    ", "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@acf5a22 by pinning to Solidity 0.5.9 everywhere except the  Description  A number of files use a  floating  pragma as follows:  pragma solidity >=0.4.22 <0.6.0;  It s better to use a specific Solidity compiler version (preferably a current version). This removes any confusion about which compiler was used when the contract is deployed, and it makes sure the code is never subjected to older compiler bugs.  It s still a good idea to upgrade the compiler version in the future as compiler bugs are fixed, but this way you must explicitly choose the new compiler version in your code when you do so.  Recommendation  Based on the Truffle configuration, the code is currently compiled with Solidity 0.5.9. Consider changing the existing pragmas to the following:  pragma solidity 0.5.9;  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.6 ContractDetails.owner is never read    ", "body": "  Resolution   This is fixed in   PegaSysEng/permissioning-smart-contracts@d3f505e.  Description  The ContractDetails struct used by Ingress contracts has an owner field that is written to, but it is never read.  code/contracts/Ingress.sol:L14-L19  struct ContractDetails {  address owner;  address contractAddress;  mapping(bytes32 => ContractDetails) registry;  Recommendation  If owner is not (yet) needed, the ContractDetails struct should be removed altogether and the type of Ingress.registry should change to mapping(bytes32 => address)  8 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Below is the raw output of the MythX vulnerability scan:  Summary  40 problems (0 errors, 40 warnings)  Warnings  SWC-108  XXXXX  SWC-131  27  XXXXXXXXXXXXXXXXXXXX  SWC-110  XXX  SWC-128  XXX  SWC-123  XX  Details  AccountRules.sol - 7 problems (0 errors, 7 warnings)  Warning  12:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"readOnlyMode\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  14:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"version\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  61:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  62:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  63:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  64:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  65:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  AccountRulesList.sol - 2 problems (0 errors, 2 warnings)  Warning  15:4  A reachable exception has been detected. It is possible to trigger an exception (opcode 0xfe). Exceptions can be caused by type errors, division by zero, out-of-bounds array access, or assert violations. Note that explicit assert() should only be used to check invariants. Use require() for regular input checking.  SWC-110  Warning  36:8  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  AccountRulesProxy.sol - 12 problems (0 errors, 12 warnings)  Warning  5:8  Unused local variable \"sender\" The local variable \"sender\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  5:8  Unused local variable \"sender\" The local variable \"sender\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"target\" The local variable \"target\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"target\" The local variable \"target\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"value\" The local variable \"value\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"value\" The local variable \"value\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"gasPrice\" The local variable \"gasPrice\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"gasPrice\" The local variable \"gasPrice\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"gasLimit\" The local variable \"gasLimit\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"gasLimit\" The local variable \"gasLimit\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"payload\" The local variable \"payload\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"payload\" The local variable \"payload\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  AdminList.sol - 3 problems (0 errors, 3 warnings)  Warning  17:4  A reachable exception has been detected. It is possible to trigger an exception (opcode 0xfe). Exceptions can be caused by type errors, division by zero, out-of-bounds array access, or assert violations. Note that explicit assert() should only be used to check invariants. Use require() for regular input checking.  SWC-110  Warning  38:8  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  Warning  42:23  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  AdminProxy.sol - 3 problems (0 errors, 3 warnings)  Warning  4:10  precondition violation A precondition was violated. Make sure valid inputs are provided to both callees (e.g, via passed arguments) and callers (e.g., via return values).  SWC-123  Warning  4:26  Unused local variable \"source\" The local variable \"source\" is created within the contract \"Admin\" but does not seem to be used anywhere.  SWC-131  Warning  4:26  Unused local variable \"source\" The local variable \"source\" is created within the contract \"AdminProxy\" but does not seem to be used anywhere.  SWC-131  Ingress.sol - 3 problems (0 errors, 3 warnings)  Warning  12:14  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"contractKeys\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  19:40  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"registry\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  35:19  precondition violation A precondition was violated. Make sure valid inputs are provided to both callees (e.g, via passed arguments) and callers (e.g., via return values).  SWC-123  NodeRulesList.sol - 1 problem (0 errors, 1 warning)  Warning  15:4  assertion violation An assertion was violated. Make sure your program logic is correct (e.g., no division by zero) and that you add appropriate validation for inputs from both callers (e.g, passed arguments) and callees (e.g., return values).  SWC-110  NodeIngress.sol - 1 problem (0 errors, 1 warning)  Warning  9:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"version\" is internal. Other possible visibility values are public and private.  SWC-108  NodeRulesProxy.sol - 8 problems (0 errors, 8 warnings)  Warning  5:8  Unused local variable \"sourceEnodeHigh\" The local variable \"sourceEnodeHigh\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"sourceEnodeLow\" The local variable \"sourceEnodeLow\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"sourceEnodeIp\" The local variable \"sourceEnodeIp\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"sourceEnodePort\" The local variable \"sourceEnodePort\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"destinationEnodeHigh\" The local variable \"destinationEnodeHigh\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"destinationEnodeLow\" The local variable \"destinationEnodeLow\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  11:8  Unused local variable \"destinationEnodeIp\" The local variable \"destinationEnodeIp\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  12:8  Unused local variable \"destinationEnodePort\" The local variable \"destinationEnodePort\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  AccountIngress.sol - 0 problems  Admin.sol - 0 problems  ExposedAccountRulesList.sol - 0 problems  ExposedAdminList.sol - 0 problems  ExposedNodeRulesList.sol - 0 problems  Generated on Thu Aug 29 2019 15:16:37 GMT-0700 (Pacific Daylight Time)  MythX Logs:  AccountRules.sol  UUID: 6db36465-5d19-43b8-8318-20d038616ffb  info: skipped automated fuzz testing due to incompatible bytecode input  AccountRulesList.sol  UUID: 17faa2da-60ed-4e9c-8f76-c9d87ebfa025  AccountRulesProxy.sol  UUID: 0579eb33-82ef-4ac7-99e1-948ba46955df  Admin.sol  UUID: da4012ea-98e3-4116-9ee9-896da7904e7c  AdminList.sol  UUID: 6a5da947-d87d-4f3a-b3e6-94d76712aa73  AdminProxy.sol  UUID: ef18baac-d986-4bcd-aefc-1d0801e214d2  ExposedAccountRulesList.sol  UUID: 706738e2-35a4-4def-b7c4-f680920db1a1  ExposedAdminList.sol  UUID: ea78f05f-c0f2-46e0-84eb-0168d35fccc4  ExposedNodeRulesList.sol  UUID: 33d4d1a4-2f85-4d6d-99c7-dd76c738d305  Ingress.sol  UUID: 162745bf-308e-4cc8-a07b-b5e1564f7764  NodeIngress.sol  UUID: a40eebe4-d51e-40fd-8b0b-913491e63411  NodeRulesList.sol  UUID: c5d7bd11-591d-4bd6-8364-883d8db35bb9  NodeRulesProxy.sol  UUID: 851c3349-b9f2-459c-a3ec-5bbd7cb6d616  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Ethlint didn t find any issues.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  AccountIngress.sol  57207a6878535bc2f3d40216d96f07eef9bbdfd9  AccountRulesList.sol  73ffd92be5b6c3b1e18d1b860344dac578c9aa31  Admin.sol  e13931323093f1555f4dfcc74fad6a2c457c1082  AdminProxy.sol  eecd073b4e05a4445fb00888074b48c443c5bbf4  Ingress.sol  b0fcff06fa7d55136cfe483331280e4e9bb9def4  NodeIngress.sol  3f46f78e4c1b9a546287135a13ffa303f62a826b  NodeRulesList.sol  fa9382c4cf3f4d800aa3d0e89bb9a712d5aa5f0c  AccountRules.sol  c730212300e070ed22b1490f6e67347d1f36c051  AccountRulesProxy.sol  1024d00149ee0258f5ee4c0671a09ada723c3645  AdminList.sol  0304e06bfc4c87abc4d2f4c0361633590c5ef830  NodeRules.sol  8f0dc9efd5bc09a8c6346495e23a398c907baf21  NodeRulesProxy.sol  01967d8481a3f1497ecdfcfcd5e7dd2ea9f9c17e  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AccountIngress  Implementation  Ingress  getContractVersion  Public    NO   emitRulesChangeEvent  Public    NO   transactionAllowed  Public    NO   AccountRulesList  Implementation  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  addAll  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  Admin  Implementation  AdminProxy, AdminList  <Constructor>  Public    isAuthorized  Public    NO   addAdmin  Public    onlyAdmin  removeAdmin  Public    onlyAdmin notSelf  getAdmins  Public    NO   addAdmins  Public    onlyAdmin  AdminProxy  Interface  isAuthorized  External    NO   Ingress  Implementation  getContractAddress  Public    NO   isAuthorized  Public    NO   setContractAddress  Public    NO   removeContract  Public    NO   getAllContractKeys  Public    NO   NodeIngress  Implementation  Ingress  getContractVersion  Public    NO   emitRulesChangeEvent  Public    NO   connectionAllowed  Public    NO   NodeRulesList  Implementation  calculateKey  Internal \ud83d\udd12  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  AccountRules  Implementation  AccountRulesProxy, AccountRulesList  <Constructor>  Public    getContractVersion  Public    NO   isReadOnly  Public    NO   enterReadOnly  Public    onlyAdmin  exitReadOnly  Public    onlyAdmin  transactionAllowed  Public    NO   accountInWhitelist  Public    NO   addAccount  Public    onlyAdmin onlyOnEditMode  removeAccount  Public    onlyAdmin onlyOnEditMode  getSize  Public    NO   getByIndex  Public    NO   getAccounts  Public    NO   addAccounts  Public    onlyAdmin  AccountRulesProxy  Interface  transactionAllowed  External    NO   AdminList  Implementation  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  addAll  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  NodeRules  Implementation  NodeRulesProxy, NodeRulesList  <Constructor>  Public    getContractVersion  Public    NO   isReadOnly  Public    NO   enterReadOnly  Public    onlyAdmin  exitReadOnly  Public    onlyAdmin  connectionAllowed  Public    NO   enodeInWhitelist  Public    NO   addEnode  Public    onlyAdmin onlyOnEditMode  removeEnode  Public    onlyAdmin onlyOnEditMode  getSize  Public    NO   getByIndex  Public    NO   triggerRulesChangeEvent  Public    NO   NodeRulesProxy  Interface  connectionAllowed  External    NO   Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.4 Slither", "body": "  Slither is a Solidity static analysis framework written in Python 3. It runs a suite of vulnerability detectors.  Below is the raw output of the Slither scan:  INFO:Detectors:  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedAdminList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRules.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Ingress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRules.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AdminProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRulesProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedNodeRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AdminList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedAccountRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Admin.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Migrations.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeIngress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRulesProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountIngress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRulesList.sol#1)  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#incorrect-versions-of-solidity  INFO:Detectors:  Function 'ExposedAdminList._size()' (ExposedAdminList.sol#9-11) is not in mixedCase  Function 'ExposedAdminList._exists(address)' (ExposedAdminList.sol#13-15) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#13) is not in mixedCase  Function 'ExposedAdminList._add(address)' (ExposedAdminList.sol#17-19) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#17) is not in mixedCase  Function 'ExposedAdminList._remove(address)' (ExposedAdminList.sol#21-23) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#21) is not in mixedCase  Function 'ExposedAdminList._addBatch(address[])' (ExposedAdminList.sol#25-27) is not in mixedCase  Parameter '_addresses' of _addresses (ExposedAdminList.sol#25) is not in mixedCase  Parameter '_account' of _account (AccountRules.sol#77) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#22) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#26) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#45) is not in mixedCase  Variable 'Ingress.RULES_CONTRACT' (Ingress.sol#8) is not in mixedCase  Variable 'Ingress.ADMIN_CONTRACT' (Ingress.sol#9) is not in mixedCase  Function 'ExposedNodeRulesList._calculateKey(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#8-10) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#8) is not in mixedCase  Function 'ExposedNodeRulesList._size()' (ExposedNodeRulesList.sol#12-14) is not in mixedCase  Function 'ExposedNodeRulesList._exists(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#16-18) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#16) is not in mixedCase  Function 'ExposedNodeRulesList._add(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#20-22) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#20) is not in mixedCase  Function 'ExposedNodeRulesList._remove(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#24-26) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#24) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#28) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#56) is not in mixedCase  Function 'ExposedAccountRulesList._size()' (ExposedAccountRulesList.sol#8-10) is not in mixedCase  Function 'ExposedAccountRulesList._exists(address)' (ExposedAccountRulesList.sol#12-14) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#12) is not in mixedCase  Function 'ExposedAccountRulesList._add(address)' (ExposedAccountRulesList.sol#16-18) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#16) is not in mixedCase  Function 'ExposedAccountRulesList._addAll(address[])' (ExposedAccountRulesList.sol#20-22) is not in mixedCase  Function 'ExposedAccountRulesList._remove(address)' (ExposedAccountRulesList.sol#24-26) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#24) is not in mixedCase  Parameter '_address' of _address (Admin.sol#22) is not in mixedCase  Parameter '_address' of _address (Admin.sol#26) is not in mixedCase  Parameter '_address' of _address (Admin.sol#38) is not in mixedCase  Parameter 'new_address' of new_address (Migrations.sol#20) is not in mixedCase  Variable 'Migrations.last_completed_migration' (Migrations.sol#6) is not in mixedCase  Struct 'NodeRulesList.enode' (NodeRulesList.sol#8-13) is not in CapWords  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#18) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#18) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#18) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#18) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#26) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#26) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#26) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#26) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#30) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#30) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#30) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#30) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#39) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#39) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#39) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#39) is not in mixedCase  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#conformance-to-solidity-naming-conventions  INFO:Detectors:  AccountRules.slitherConstructorVariables (AccountRules.sol#9-113) uses literals with too many digits:  version = 1000000  NodeRules.slitherConstructorVariables (NodeRules.sol#9-170) uses literals with too many digits:  version = 1000000  NodeIngress.getContractAddress (Ingress.sol#26-29) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.setContractAddress (Ingress.sol#39-62) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.removeContract (Ingress.sol#64-81) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  RULES_CONTRACT = 0x72756c6573000000000000000000000000000000000000000000000000000000  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  ADMIN_CONTRACT = 0x61646d696e697374726174696f6e000000000000000000000000000000000000  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  version = 1000000  AccountIngress.getContractAddress (Ingress.sol#26-29) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.setContractAddress (Ingress.sol#39-62) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.removeContract (Ingress.sol#64-81) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  RULES_CONTRACT = 0x72756c6573000000000000000000000000000000000000000000000000000000  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  ADMIN_CONTRACT = 0x61646d696e697374726174696f6e000000000000000000000000000000000000  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  version = 1000000  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#too-many-digits  INFO:Detectors:  AccountIngress.version should be constant (AccountIngress.sol#9)  AccountRules.version should be constant (AccountRules.sol#14)  Ingress.ADMIN_CONTRACT should be constant (Ingress.sol#9)  Ingress.RULES_CONTRACT should be constant (Ingress.sol#8)  NodeIngress.version should be constant (NodeIngress.sol#9)  NodeRules.version should be constant (NodeRules.sol#30)  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#state-variables-that-could-be-declared-constant  INFO:Detectors:  ExposedAdminList._size() (ExposedAdminList.sol#9-11) should be declared external  ExposedAdminList._exists(address) (ExposedAdminList.sol#13-15) should be declared external  ExposedAdminList._add(address) (ExposedAdminList.sol#17-19) should be declared external  ExposedAdminList._remove(address) (ExposedAdminList.sol#21-23) should be declared external  ExposedAdminList._addBatch(address[]) (ExposedAdminList.sol#25-27) should be declared external  AccountRules.getContractVersion() (AccountRules.sol#38-40) should be declared external  AccountRules.isReadOnly() (AccountRules.sol#43-45) should be declared external  AccountRules.enterReadOnly() (AccountRules.sol#47-51) should be declared external  AccountRules.exitReadOnly() (AccountRules.sol#53-57) should be declared external  AccountRules.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountRules.sol#59-74) should be declared external  AccountRulesProxy.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountRulesProxy.sol#4-11) should be declared external  AccountRules.addAccount(address) (AccountRules.sol#82-88) should be declared external  AccountRules.removeAccount(address) (AccountRules.sol#90-96) should be declared external  AccountRules.getSize() (AccountRules.sol#98-100) should be declared external  AccountRules.getByIndex(uint256) (AccountRules.sol#102-104) should be declared external  AccountRules.getAccounts() (AccountRules.sol#106-108) should be declared external  AccountRules.addAccounts(address[]) (AccountRules.sol#110-112) should be declared external  Ingress.setContractAddress(bytes32,address) (Ingress.sol#39-62) should be declared external  Ingress.removeContract(bytes32) (Ingress.sol#64-81) should be declared external  Ingress.getAllContractKeys() (Ingress.sol#83-85) should be declared external  NodeRules.getContractVersion() (NodeRules.sol#53-55) should be declared external  NodeRules.isReadOnly() (NodeRules.sol#58-60) should be declared external  NodeRules.enterReadOnly() (NodeRules.sol#62-66) should be declared external  NodeRules.exitReadOnly() (NodeRules.sol#68-72) should be declared external  NodeRulesProxy.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeRulesProxy.sol#4-13) should be declared external  NodeRules.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeRules.sol#74-101) should be declared external  NodeRules.addEnode(bytes32,bytes32,bytes16,uint16) (NodeRules.sol#112-132) should be declared external  NodeRules.removeEnode(bytes32,bytes32,bytes16,uint16) (NodeRules.sol#134-154) should be declared external  NodeRules.getSize() (NodeRules.sol#156-158) should be declared external  NodeRules.getByIndex(uint256) (NodeRules.sol#160-165) should be declared external  ExposedNodeRulesList._calculateKey(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#8-10) should be declared external  ExposedNodeRulesList._size() (ExposedNodeRulesList.sol#12-14) should be declared external  ExposedNodeRulesList._exists(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#16-18) should be declared external  ExposedNodeRulesList._add(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#20-22) should be declared external  ExposedNodeRulesList._remove(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#24-26) should be declared external  ExposedAccountRulesList._size() (ExposedAccountRulesList.sol#8-10) should be declared external  ExposedAccountRulesList._exists(address) (ExposedAccountRulesList.sol#12-14) should be declared external  ExposedAccountRulesList._add(address) (ExposedAccountRulesList.sol#16-18) should be declared external  ExposedAccountRulesList._addAll(address[]) (ExposedAccountRulesList.sol#20-22) should be declared external  ExposedAccountRulesList._remove(address) (ExposedAccountRulesList.sol#24-26) should be declared external  Admin.addAdmin(address) (Admin.sol#26-36) should be declared external  Admin.removeAdmin(address) (Admin.sol#38-42) should be declared external  Admin.getAdmins() (Admin.sol#44-46) should be declared external  Admin.addAdmins(address[]) (Admin.sol#48-50) should be declared external  Migrations.setCompleted(uint256) (Migrations.sol#16-18) should be declared external  Migrations.upgrade(address) (Migrations.sol#20-23) should be declared external  NodeIngress.getContractVersion() (NodeIngress.sol#15-17) should be declared external  NodeIngress.emitRulesChangeEvent(bool) (NodeIngress.sol#19-22) should be declared external  NodeIngress.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeIngress.sol#24-48) should be declared external  AccountIngress.getContractVersion() (AccountIngress.sol#15-17) should be declared external  AccountIngress.emitRulesChangeEvent(bool) (AccountIngress.sol#19-22) should be declared external  AccountIngress.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountIngress.sol#24-39) should be declared external  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#public-function-that-could-be-declared-as-external  INFO:Slither:. analyzed (16 contracts), 157 result(s) found  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "5.1 Oracle s _sanityCheck for prices will not work with slashing ", "body": "  Description  The _sanityCheck is verifying that the new price didn t change significantly:  code/contracts/Portal/utils/OracleUtilsLib.sol:L405-L417  uint256 maxPrice = curPrice +  ((curPrice *  self.PERIOD_PRICE_INCREASE_LIMIT *  _periodsSinceUpdate) / PERCENTAGE_DENOMINATOR);  uint256 minPrice = curPrice -  ((curPrice *  self.PERIOD_PRICE_DECREASE_LIMIT *  _periodsSinceUpdate) / PERCENTAGE_DENOMINATOR);  require(  _newPrice >= minPrice && _newPrice <= maxPrice,  \"OracleUtils: price is insane\"  While the rewards of staking can be reasonably predicted, the balances may also be changed due to slashing. So any slashing event should reduce the price, and if enough ETH is slashed, the price will drop heavily. The oracle will not be updated because of a sanity check. After that, there will be an arbitrage opportunity, and everyone will be incentivized to withdraw as soon as possible. That process will inevitably devaluate gETH to zero. The severity of this issue is also amplified by the fact that operators have no skin in the game and won t lose anything from slashing.  Recommendation  Make sure that slashing can be adequately processed when updating the price.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.2 Multiple calculation mistakes in the _findPricesClearBuffer function ", "body": "  Description  The _findPricesClearBuffer function is designed to calculate the gETH/ETH prices. The first one (oracle price) is the price at the reference point, for ease of calculation let s assume it is midnight. The second price is the price at the time the reportOracle is called.  code/contracts/Portal/utils/OracleUtilsLib.sol:L388  return (unbufferedEther / unbufferedSupply, totalEther / supply);  To calculate the oracle price at midnight, the current ETH balance is reduced by all the minted gETH (converted to ETH with the old price) and increased by all the burnt gETH (converted to ETH with the old price) starting from midnight to the time transaction is being executed:  code/contracts/Portal/utils/OracleUtilsLib.sol:L368-L374  uint256 unbufferedEther = totalEther -  (DATASTORE.readUintForId(_poolId, _dailyBufferMintKey) * price) /  self.gETH.totalSupply(_poolId);  unbufferedEther +=  (DATASTORE.readUintForId(_poolId, _dailyBufferBurnKey) * price) /  self.gETH.denominator();  But in the first calculation, the self.gETH.totalSupply(_poolId) is mistakenly used instead of self.gETH.denominator(). This can lead to the unbufferedEther being much larger, and the eventual oracle price will be much larger too.  There is another serious calculation mistake. In the end, the function returns the following line:  code/contracts/Portal/utils/OracleUtilsLib.sol:L388  return (unbufferedEther / unbufferedSupply, totalEther / supply);  But none of these values are multiplied by self.gETH.denominator(); so they are in the same range. Both values will usually be around 1. While the actual price value should be multiplied by self.gETH.denominator();.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.3 New interfaces can add malicious code without any delay or check ", "body": "  Description  Geode Finance uses an interesting system of contracts for each individual staked ETH derivative. At the base of it all is an ERC1155 gETH contract where planet id acts as a token id. To make it more compatible with the rest of DeFi the Geode team pairs it up with an ERC20 contract that users would normally interact with and where all the allowances are stored. Naturally, since the balances are stored in the gETH contract, ERC20 interfaces need to ask gETH contract to update the balance. It is done in a way where the gETH contract will perform any transfer requested by the interface since the interface is expected to do all the checks and accountings. The issue comes with the fact that planet maintainers can whitelist new interfaces and that process does not require any approval. Planet maintainers could whitelist an interface that will send all the available tokens to the maintainer s wallet for example. This essentially allows Planet maintainers to steal all derivative tokens in circulation in one transaction.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L165-L173  function setInterface(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  address _interface  ) external {  DATASTORE.authenticate(id, true, [false, true, true]);  _setInterface(self, DATASTORE, id, _interface);  Recommendation  gETH.sol contract has a concept of avoiders. One of the ways to fix this issue is to have the avoidance be set on a per-interface basis and avoiding new interfaces by default. This way users will need to allow the new tokens to access the balances.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.4 MiniGovernance - fetchUpgradeProposal will always revert ", "body": "  Description  In the function fetchUpgradeProposal(), newProposal() is called with a hard coded duration of 4 weeks. This means the function will always revert since newProposal() checks that the proposal duration is not more than the constant MAX_PROPOSAL_DURATION of 2 weeks. Effectively, this leaves MiniGovernance non-upgradeable.  Examples  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L183  GEM.newProposal(proposal.CONTROLLER, 2, proposal.NAME, 4 weeks);  code/contracts/Portal/utils/GeodeUtilsLib.sol:L328-L331  require(  duration <= MAX_PROPOSAL_DURATION,  \"GeodeUtils: duration exceeds MAX_PROPOSAL_DURATION\"  );  Recommendation  Switch the hard coded proposal duration to 2 weeks.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.5 reportOracle can be sandwiched for profit. ", "body": "  Description  The fact that price update happens in an on-chain transaction gives the searches the ability to see the future price and then act accordingly.  Examples  MEV searcher can find the reportOracle transaction in the mem-pool and if the price is about to increase he could proceed to mint as much gETH as he can with a flash loan. They would then bundle the reportOracle transaction. Finally, they would redeem all the gETH for ETH at a higher price per share value as the last transaction in the bundle.  This paired with the fact that oracle might be updated less frequently than once per day, could lead to the fact that profits from this attack will outweigh the fees for performing it.  Fortunately, due to the nature of the protocol, the price fluctuations from day to day will most likely be smaller than the fees encountered during this arbitrage, but this is still something to be aware of when updating the values for DWP donations and fees. But it also makes it crucial to update the oracle every day not to increase the profit margins for this attack.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.6 Updating interfaces of derivatives is done in a dangerous and unpredictable manner. ", "body": "  Description  Geode Finance codebase provides planet maintainers with the ability to enable or disable different contracts to act as the main token contract. In fact, multiple separate contracts can be used at the same time if decided so by the planet maintainer. Those contracts will have shared balances but will not share the allowances as you can see below:  code/contracts/Portal/helpers/ERC1155SupplyMinterPauser.sol:L47  mapping(uint256 => mapping(address => uint256)) private _balances;  code/contracts/Portal/gETHInterfaces/ERC20InterfaceUpgradable.sol:L60  mapping(address => mapping(address => uint256)) private _allowances;  Unfortunately, this approach comes with some implications that are very hard to predict as they involve interactions with other systems, but is possible to say that the consequences of those implications will most always be negative. We will not be able to outline all the implications of this issue, but we can try and outline the pattern that they all would follow.  Examples  There are really two ways to update an interface: set the new one and immediately unset the old one, or have them both run in parallel for some time. Let s look at them one by one.  in the first case, the old interface is disabled immediately. Given that interfaces share balances that will lead to some very serious consequences. Imagine the following sequence:  Alice deposits her derivatives into the DWP contract for liquidity mining.  Planet maintainer updates the interface and immediately disables the old one.  DWP contract now has the old tokens and the new ones. But only the new ones are accounted for in the storage and thus can be withdrawn. Unfortunately, the old tokens are disabled meaning that now both old and new tokens are lost.  This can happen in pretty much any contract and not just the DWP token. Unless the holders had enough time to withdraw the derivatives back to their wallets all the funds deposited into contracts could be lost.  This leads us to the second case where the two interfaces are active in parallel. This would solve the issue above by allowing Alice to withdraw the old tokens from the DWP and make the new tokens follow. Unfortunately, there is an issue in that case as well.  Some DeFi contracts allow their owners to withdraw any tokens that are not accounted for by the internal accounting. DWP allows the withdrawal of admin fees if the contract has more tokens than balances[] store. Some contracts even allow to withdraw funds that were accidentally sent to the contract by people. Either to recover them or just as a part of dust collection. Let s call such contracts  dangerous contracts  for our purposes.  Alice deposits her derivatives into the dangerous contract.  Planet maintainer sets a new interface.  Owner of the dangerous contract sees that some odd and unaccounted tokens landed in the contract. He learns those are real and are part of Geode ecosystem. So he takes them.  Old tokens will follow the new tokens. That means Alice now has no claim to them and the contract that they just left has broken accounting since numbers there are not backed by tokens anymore.  One other issue we would like to highlight here is that despite the contracts being expected to have separate allowances, if the old contract has the allowance set, the initial 0 value of the new one will be ignored. Here is an example:  Alice approves Bob for 100 derivatives.  Planet maintainer sets a new interface. The new interface has no allowance from Alice to Bob.  Bob still can transfer new tokens from Alice to himself by transferring the old tokens for which he still has the allowance. New token balances will be updated accordingly.  Alice could also give Bob an allowance of 100 tokens in the new contract since that was her original intent, but this would mean that Bob now has 200 token allowance.  This is extremely convoluted and will most likely result in errors made by the planet maintainers when updating the interfaces.  Recommendation  The safest option is to only allow a list of whitelisted interfaces to be used that are well-documented and audited. Planet maintainers could then choose the once that they see fit.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.7 A sandwich attack on fetchUnstake ", "body": "  Description  Operators are incentivized to withdraw the stake when there is a debt in the system. Withdrawn ETH will be sold in the DWP, and a portion of the arbitrage profit will be sent to the operator. But the operators cannot unstake and earn the arbitrage boost instantly. Node operator will need to start the withdrawal process, signal unstake, and only then, after some time, potentially days, Oracle will trigger fetchUnstake and will take the arbitrage opportunity if it is still there.  code/contracts/Portal/utils/StakeUtilsLib.sol:L1276-L1288  function fetchUnstake(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  uint256 poolId,  uint256 operatorId,  bytes[] calldata pubkeys,  uint256[] calldata balances,  bool[] calldata isExit  ) external {  require(  msg.sender == self.TELESCOPE.ORACLE_POSITION,  \"StakeUtils: sender NOT ORACLE\"  );  In reality, the DWP contract s swap function is external and can be used by anyone, so anyone could try and take the arbitrage.  code/contracts/Portal/withdrawalPool/Swap.sol:L341-L358  function swap(  uint8 tokenIndexFrom,  uint8 tokenIndexTo,  uint256 dx,  uint256 minDy,  uint256 deadline  external  payable  virtual  override  nonReentrant  whenNotPaused  deadlineCheck(deadline)  returns (uint256)  return swapStorage.swap(tokenIndexFrom, tokenIndexTo, dx, minDy);  In fact, one could take this arbitrage with no risk or personal funds. This is due to the fact that fetchUnstake() could get sandwiched. Consider the following case:  There is a debt in the DWP and the node operator decides to withdraw the stake to take the arbitrage opportunity.  After some time the Oracle will actually finalize the withdrawal by calling fecthUnstake.  If debt is still there MEV searcher will see that transaction in the mem-pool and will take an ETH loan to buy cheap gETH.  fetchUnstake() will execute and since the debt was repaid in the previous step all of the withdrawn ETH will go into surplus.  Searcher will redeem gETH that they bought for the oracle price from surplus and will get all of the profit.  At the end of the day, the goal of regaining the peg will be accomplished, but node operators will not be interested in withdrawing early later. This will potentially create unhealthy situations when withdrawals are required in case of a serious de-peg.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.8 Only the GOVERNANCE can initialize the Portal ", "body": "  Description  In the Portal s initialize function, the _GOVERNANCE is passed as a parameter:  code/contracts/Portal/Portal.sol:L156-L196  function initialize(  address _GOVERNANCE,  address _gETH,  address _ORACLE_POSITION,  address _DEFAULT_gETH_INTERFACE,  address _DEFAULT_DWP,  address _DEFAULT_LP_TOKEN,  address _MINI_GOVERNANCE_POSITION,  uint256 _GOVERNANCE_TAX,  uint256 _COMET_TAX,  uint256 _MAX_MAINTAINER_FEE,  uint256 _BOOSTRAP_PERIOD  ) public virtual override initializer {  __ReentrancyGuard_init();  __Pausable_init();  __ERC1155Holder_init();  __UUPSUpgradeable_init();  GEODE.SENATE = _GOVERNANCE;  GEODE.GOVERNANCE = _GOVERNANCE;  GEODE.GOVERNANCE_TAX = _GOVERNANCE_TAX;  GEODE.MAX_GOVERNANCE_TAX = _GOVERNANCE_TAX;  GEODE.SENATE_EXPIRY = type(uint256).max;  STAKEPOOL.GOVERNANCE = _GOVERNANCE;  STAKEPOOL.gETH = IgETH(_gETH);  STAKEPOOL.TELESCOPE.gETH = IgETH(_gETH);  STAKEPOOL.TELESCOPE.ORACLE_POSITION = _ORACLE_POSITION;  STAKEPOOL.TELESCOPE.MONOPOLY_THRESHOLD = 20000;  updateStakingParams(  _DEFAULT_gETH_INTERFACE,  _DEFAULT_DWP,  _DEFAULT_LP_TOKEN,  _MAX_MAINTAINER_FEE,  _BOOSTRAP_PERIOD,  type(uint256).max,  type(uint256).max,  _COMET_TAX,  3 days  );  But then it calls the updateStakingParams function, which requires the msg.sender to be the governance:  code/contracts/Portal/Portal.sol:L651-L665  function updateStakingParams(  address _DEFAULT_gETH_INTERFACE,  address _DEFAULT_DWP,  address _DEFAULT_LP_TOKEN,  uint256 _MAX_MAINTAINER_FEE,  uint256 _BOOSTRAP_PERIOD,  uint256 _PERIOD_PRICE_INCREASE_LIMIT,  uint256 _PERIOD_PRICE_DECREASE_LIMIT,  uint256 _COMET_TAX,  uint256 _BOOST_SWITCH_LATENCY  ) public virtual override {  require(  msg.sender == GEODE.GOVERNANCE,  \"Portal: sender not GOVERNANCE\"  );  So only the future governance can initialize the Portal. In the case of the Geode protocol, the governance will be represented by a token contract, making it hard to initialize promptly. Initialization should be done by an actor that is more flexible than governance.  Recommendation  Split the updateStakingParams function into public and private ones and use them accordingly.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.9 The maintainer of the MiniGovernance can block the changeMaintainer function ", "body": "  Description  Every entity with an ID has a controller and a maintainer. The controller tends to have more control, and the maintainer is mostly used for operational purposes. So the controller should be able to change the maintainer if that is required. Indeed we see that it is possible in the MiniGovernance too:  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L224-L246  function changeMaintainer(  bytes calldata password,  bytes32 newPasswordHash,  address newMaintainer  external  virtual  override  onlyPortal  whenNotPaused  returns (bool success)  require(  SELF.PASSWORD_HASH == bytes32(0) ||  SELF.PASSWORD_HASH ==  keccak256(abi.encodePacked(SELF.ID, password))  );  SELF.PASSWORD_HASH = newPasswordHash;  _refreshSenate(newMaintainer);  success = true;  Here the changeMaintainer function can only be called by the Portal, and only the controller can initiate that call. But the maintainer can pause the MiniGovernance, which will make this call revert because the _refreshSenate function has the whenNotPaused modifier. Thus maintainer could intentionally prevent the controller from replacing it by another maintainer.  Recommendation  Make sure that the controller can always change the malicious maintainer.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.10 Entities are not required to be initiated ", "body": "  Description  Every entity (Planet, Comet, Operator) has a 3-step creation process:  Creation of the proposal.  Approval of the proposal.  Initiation of the entity.  The last step is crucial, but it is never explicitly checked that the entity is initialized. The initiation always includes the initiator modifier that works with the \"initiated\" slot on DATASTORE:  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L46-L72  modifier initiator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 _TYPE,  uint256 _id,  address _maintainer  ) {  require(  msg.sender == DATASTORE.readAddressForId(_id, \"CONTROLLER\"),  \"MaintainerUtils: sender NOT CONTROLLER\"  );  require(  DATASTORE.readUintForId(_id, \"TYPE\") == _TYPE,  \"MaintainerUtils: id NOT correct TYPE\"  );  require(  DATASTORE.readUintForId(_id, \"initiated\") == 0,  \"MaintainerUtils: already initiated\"  );  DATASTORE.writeAddressForId(_id, \"maintainer\", _maintainer);  _;  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  emit IdInitiated(_id, _TYPE);  But this slot is never actually checked when the entities are used. While we did not find any profitable attack vector using uninitiated entities, the code will be upgraded, which may allow for possible attack vectors related to this issue.  Recommendation  Make sure the entities are initiated before they are used.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.11 Node operators are not risking anything when abandoning their activity or performing malicious actions ", "body": "  Description  During the staking process, the node operators need to provide 1 ETH as a deposit for every validator that they would like to initiate. After that is done, Oracle needs to ensure that validator creation has been done correctly and then deposit the remaining 31 ETH on chain as well as reimburse 1 ETH back to the node operator. The node operator can then proceed to withdraw the funds that were used as initial deposits. As the result, node operators operate nodes that have 32 ETH each and none of which originally belonged to the operator. They essentially have no skin in the game to continue managing the validators besides a potential share in staking rewards. Instead, node operators could stop operation, or try to get slashed on purpose to create turmoil around derivatives on the market and try to capitalize while shorting the assets elsewhere.  Recommendation  Senate will need to be extra careful when approving operator onboarding proposals or potentially only reimburse the node operators the initial deposit after the funds were withdrawn from the MiniGovernance.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.12 Planets should not act as operators ", "body": "  Description  The system stores every entity (e.g., planet, comet, and operator) separately in DATASTORE under different IDs. But there is one exception, every planet can also act as an operator by default. This exception bypasses the general rule and goes against some expectations readers might have about the code:  Every entity with ID has fees; they are stored in DATASTORE for each entity DATASTORE.readUintForId(id, \"fee\"). The fees for a planet and an operator should be able to be different. But if a planet acts like an operator, both fees are stored under the same variable.  The same problem arises with the maintainer address. Since there will probably be different scripts for maintaining a planet and an operator, having separate addresses for the maintainers would make sense.  Every operator should be initialized before usage, but it is impossible to initialize a planet as an operator. There are two reasons behind it. First, only the original  Operator type  can call initiateOperator, while the planet will have a  Planet type . Second, an entity cannot be initialized twice; even different initialization functions use the same  initiated  storage slot.  Recommendation  Do not allow planets to be operators in the code. If every planet should be able to act as an operator simultaneously, it is better to create separate operator entities for every planet.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.13 The blameOperator can be called for an alienated validator ", "body": "  Description  The blameOperator  function is designed to be called by anyone. If some operator did not signal to exit in time, anyone can blame and imprison this operator.  code/contracts/Portal/utils/StakeUtilsLib.sol:L1205-L1224  /**  @notice allows improsening an Operator if the validator have not been exited until expectedExit  @dev anyone can call this function  @dev if operator has given enough allowence, they can rotate the validators to avoid being prisoned  /  function blameOperator(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  bytes calldata pk  ) external {  if (  block.timestamp > self.TELESCOPE._validators[pk].expectedExit &&  self.TELESCOPE._validators[pk].state != 3  ) {  OracleUtils.imprison(  DATASTORE,  self.TELESCOPE._validators[pk].operatorId  );  The problem is that it can be called for any state that is not 3 (self.TELESCOPE._validators[pk].state != 3). But it should only be called for active validators whose state equals 2. So the blameOperator can be called an infinite amount of time for alienated or not approved validators. These types of validators cannot switch to state 3.  The severity of the issue is mitigated by the fact that this function is currently unavailable for users to call. But it is intended to be external once the withdrawal process is in place.  Recommendation  Make sure that you can only blame the operator of an active validator.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.14 Latency timelocks on certain functions can be bypassed ", "body": "  Description  The functions switchMaintainerFee() and switchWithdrawalBoost() add a latency of typically three days to the current timestamp at which the new value is meant to be valid. However, they don t limit the number of times this value can be changed within the latency period. This allows a malicious maintainer to set their desired value twice and effectively make the change immediately. Let s take the first function as an example. The first call to it sets a value as the newFee, moving the old value to priorFee, which is effectively the fee in use until the time lock is up. A follow-up call to the function with the same value as a parameter would mean the  new  value overwrites the old priorFee while remaining in the queue for the switch.  Examples  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L311-L333  function switchMaintainerFee(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 newFee  ) external {  DATASTORE.writeUintForId(  id,  \"priorFee\",  DATASTORE.readUintForId(id, \"fee\")  );  DATASTORE.writeUintForId(  id,  \"feeSwitch\",  block.timestamp + FEE_SWITCH_LATENCY  );  DATASTORE.writeUintForId(id, \"fee\", newFee);  emit MaintainerFeeSwitched(  id,  newFee,  block.timestamp + FEE_SWITCH_LATENCY  );  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L296-L304  function getMaintainerFee(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id  ) internal view returns (uint256 fee) {  if (DATASTORE.readUintForId(id, \"feeSwitch\") > block.timestamp) {  return DATASTORE.readUintForId(id, \"priorFee\");  return DATASTORE.readUintForId(id, \"fee\");  Recommendation  Add a check to make sure only one value can be set between time lock periods.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.15 MiniGovernance s senate has almost unlimited validity ", "body": "  Description  A new senate for the MiniGovernance contract is set in the following line:  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L201  GEM._setSenate(newSenate, block.timestamp + SENATE_VALIDITY);  The validity period argument should not include block.timestamp, because it is going to be added a bit later in the code:  code/contracts/Portal/utils/GeodeUtilsLib.sol:L496  self.SENATE_EXPIRY = block.timestamp + _senatePeriod;  So currently, every senate of MiniGovernance will have much longer validity than it is supposed to.  Recommendation  Pass onlySENATE_VALIDITY in the _refreshSenate function.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.16 Proposed validators not accounted for in the monopoly check. ", "body": "  Description  The Geode team introduced a check that makes sure that node operators do not initiate more validators than a threshold called MONOPOLY_THRESHOLD allows. It is used on call to proposeStake(...) which the operator would call in order to propose new validators. It is worth mentioning that onboarding new validator nodes requires 2 steps: a proposal from the node operator and approval from the planet maintainer. After the first step validators get a status of proposed. After the second step validators get the status of active and all eth accounting is done. The issue we found is that the proposed validators step performs the monopoly check but does not account for previously proposed but not active validators.  Examples  Assume that MONOPOLY_THRESHOLD is set to 5. The node operator could propose 4 new validators and pass the monopoly check and label those validators as proposed. The node operator could then suggest 4 more validators in a separate transaction and since the monopoly check does not check for the proposed validators, that would pass as well. Then in beaconStake or the step of maintainer approval, there is no monopoly check at all, so 8 validators could be activated at once.  code/contracts/Portal/utils/StakeUtilsLib.sol:L978-L982  require(  (DATASTORE.readUintForId(operatorId, \"totalActiveValidators\") +  pubkeys.length) <= self.TELESCOPE.MONOPOLY_THRESHOLD,  \"StakeUtils: IceBear does NOT like monopolies\"  );  Recommendation  Include the (DATASTORE.readUintForId(poolId,DataStoreUtils.getKey(operatorId, \"proposedValidators\")) into the require statement, just like in the check for the node operator allowance check.  code/contracts/Portal/utils/StakeUtilsLib.sol:L983-L995  require(  (DATASTORE.readUintForId(  poolId,  DataStoreUtils.getKey(operatorId, \"proposedValidators\")  ) +  DATASTORE.readUintForId(  poolId,  DataStoreUtils.getKey(operatorId, \"activeValidators\")  ) +  pubkeys.length) <=  operatorAllowance(DATASTORE, poolId, operatorId),  \"StakeUtils: NOT enough allowance\"  );  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.17 Comparison operator used instead of assignment operator ", "body": "  Description  A common typo is present twice in the OracleUtilsLib.sol where == is used instead of = resulting in incorrect storage updates.  Examples  code/contracts/Portal/utils/OracleUtilsLib.sol:L250  self._validators[_pk].state == 2;  code/contracts/Portal/utils/OracleUtilsLib.sol:L269  self._validators[_pk].state == 3;  Recommendation  Replace == with =.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.18 initiator modifier will not work in the context of one transaction ", "body": "  Description  Each planet, comet or operator must be initialized after the onboarding proposal is approved. In order to make sure that these entities are not initialized more than once initiateOperator, initiateComet and initiatePlanet have the initiator modifier.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L135-L147  function initiatePlanet(  DataStoreUtils.DataStore storage DATASTORE,  uint256[3] memory uintSpecs,  address[5] memory addressSpecs,  string[2] calldata interfaceSpecs  external  initiator(DATASTORE, 5, uintSpecs[0], addressSpecs[1])  returns (  address miniGovernance,  address gInterface,  address withdrawalPool  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L184-L189  function initiateComet(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 fee,  address maintainer  ) external initiator(DATASTORE, 6, id, maintainer) {  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L119-L124  function initiateOperator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 fee,  address maintainer  ) external initiator(DATASTORE, 4, id, maintainer) {  Inside that modifier, we check that the initiated flag is 0 and if so we proceed to initialization. We later update it to the current timestamp.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L46-L72  modifier initiator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 _TYPE,  uint256 _id,  address _maintainer  ) {  require(  msg.sender == DATASTORE.readAddressForId(_id, \"CONTROLLER\"),  \"MaintainerUtils: sender NOT CONTROLLER\"  );  require(  DATASTORE.readUintForId(_id, \"TYPE\") == _TYPE,  \"MaintainerUtils: id NOT correct TYPE\"  );  require(  DATASTORE.readUintForId(_id, \"initiated\") == 0,  \"MaintainerUtils: already initiated\"  );  DATASTORE.writeAddressForId(_id, \"maintainer\", _maintainer);  _;  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  emit IdInitiated(_id, _TYPE);  Unfortunately, this does not follow the checks-effects-interractions pattern. If one for example would call initiatePlanet again from the body of the modifier, this check will still pass making it susceptible to a reentrancy attack. While we could not find a way to exploit this in the current engagement, given that system is designed to be upgradable this could become a risk in the future. For example, if during the initialization of the planet the maintainer will be allowed to pass a custom interface that could potentially allow reentering.  Recommendation  Bring the line that updated the initiated flag to the current timestamp before the _;.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L69  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.19 Incorrect accounting for the burned gEth ", "body": "  Description  Geode Portal records the amount of minted and burned gETH on any given day during the active period of the oracle. One case where some gETH is burned is when the users redeem gETH for ETH. In the burn function we burn the spentGeth - gEthDonation but in the accounting code we do not account for gEthDonation so the code records more assets burned than was really burned.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L823-L832  DATASTORE.subUintForId(poolId, \"surplus\", spentSurplus);  self.gETH.burn(address(this), poolId, spentGeth - gEthDonation);  if (self.TELESCOPE._isOracleActive()) {  bytes32 dailyBufferKey = DataStoreUtils.getKey(  block.timestamp - (block.timestamp % OracleUtils.ORACLE_PERIOD),  \"burnBuffer\"  );  DATASTORE.addUintForId(poolId, dailyBufferKey, spentGeth);  Recommendation  Record the spentGeth - gEthDonation instead of just spentGeth in the burn buffer.  code/contracts/Portal/utils/StakeUtilsLib.sol:L831  DATASTORE.addUintForId(poolId, dailyBufferKey, spentGeth);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.20 Boost calculation on fetchUnstake should not be using the cumBalance when it is larger than debt. ", "body": "  Description  The Geode team implemented the 2-step withdrawal mechanism for the staked ETH. First, node operators signal their intent to withdraw the stake, and then the oracle will trigger all of the accounting of rewards, balances, and buybacks if necessary. Buybacks are what we are interested in at this time. Buybacks are performed by checking if the derivative asset is off peg in the Dynamic Withdrawal Pool contract. Once the debt is larger than some ignorable threshold an arbitrage buyback will be executed. A portion of the arbitrage profit will go to the node operator. The issue here is that when simulating the arbitrage swap in the calculateSwap call we use the cumulative un-stake balance rather than ETH debt preset in the DWP. In the case where the withdrawal cumulative balance is higher than the debt node operator will receive a higher reward than intended.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L1353-L1354  uint256 arb = withdrawalPoolById(DATASTORE, poolId)  .calculateSwap(0, 1, cumBal);  Recommendation  Use the debt amount of ETH in the boost reward calculation when the cumulative balance is larger than the debt.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.21 DataStore struct not having the _gap for upgrades. ", "body": "  Description  Geode Finance codebase follows a structure where most of the storage variables are stored in the structs. You can see an example of that in the Portal.sol.  code/contracts/Portal/Portal.sol:L152-L154  DataStoreUtils.DataStore private DATASTORE;  GeodeUtils.Universe private GEODE;  StakeUtils.StakePool private STAKEPOOL;  It is worth mentioning that Geode contracts are meant to support the upgradability pattern. Given that information, one should be careful not to overwrite the storage variables by reordering the old ones or adding the new once not at the end of the list of variables when upgrading. The issue comes with the fact that structs seem to give a false sense of security making it feel like they are an isolated set of storage variables that will not override anything else. In reality, struts are just tuples that are expanded in storage sequentially just like all the other storage variables. For that reason, if you have two struct storage variables listed back to back like in the code above, you either need to make sure not to change the order or the number of variables in the structs other than the last one between upgrades or you need to add a uint256[N] _gap array of fixed size to reserve some storage slots for the future at the end of each struct. The Geode Finance team is missing the gap in the DataStrore struct making it non-upgradable.  code/contracts/Portal/utils/DataStoreUtilsLib.sol:L34-L39  struct DataStore {  mapping(uint256 => uint256[]) allIdsByType;  mapping(bytes32 => uint256) uintData;  mapping(bytes32 => bytes) bytesData;  mapping(bytes32 => address) addressData;  Recommendation  We suggest that gap is used in DataStore as well. Since it was used for all the other structs we consider it just a typo.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "4.1 Gold order size should be limited    Addressed", "body": "  Resolution   Addressed in   horizon-games/SkyWeaver-contracts#9 by adding a limit for cold cards amount in one order.  Description  When a user submits an order to buy gold cards, it s possible to buy a huge amount of cards. _commit function uses less gas than mineGolds, which means that the user can successfully commit to buying this amount of cards and when it s time to collect them, mineGolds function may run out of gas because it iterates over all card IDs and mints them:  code/contracts/shop/GoldCardsFactory.sol:L375-L376  // Mint gold cards  skyweaverAssets.batchMint(_order.cardRecipient, _ids, amounts, \"\");  Recommendation  Limit a maximum gold card amount in one order.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.2 Price and refund changes may cause failures    Addressed", "body": "  Resolution  Addressed in horizon-games/SkyWeaver-contracts#3.  Fix involves burning the weave when the commit occurs instead of when the minting of the gold cards occur.  Description  Price and refund for gold cards are used in 3 different places: commit, mint, refund.  Weave tokens spent during the commit phase  code/contracts/shop/GoldCardsFactory.sol:L274-L279  function _commit(uint256 _weaveAmount, GoldOrder memory _order)  internal  // Check if weave sent is sufficient for order  uint256 total_cost = _order.cardAmount.mul(goldPrice).add(_order.feeAmount);  uint256 refund_amount = _weaveAmount.sub(total_cost); // Will throw if insufficient amount received  but they are burned rngDelay blocks after  code/contracts/shop/GoldCardsFactory.sol:L371-L373  // Burn the non-refundable weave  uint256 weave_to_burn = (_order.cardAmount.mul(goldPrice)).sub(_order.cardAmount.mul(goldRefund));  weaveContract.burn(weaveID, weave_to_burn);  If the price is increased between these transactions, mining cards may fail because it should burn more weave tokens than there are tokens in the smart contract. Even if there are enough tokens during this particular transaction, someone may fail to melt a gold card later.  If the price is decreased, some weave tokens will be stuck in the contract forever without being burned.  Recommendation  Store goldPrice and goldRefund in GoldOrder.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.3 Re-entrancy attack allows to buy EternalHeroes cheaper    Addressed", "body": "  Resolution  Addressed in horizon-games/SkyWeaver-contracts#4.  Minting tokens before sending refunds. Subsequent PR will also add re-entrancy guard for all shops.  And re-entrancy guard added here: horizon-games/SkyWeaver-contracts#10  Description  When buying eternal heroes in _buy  function of EternalHeroesFactory contract, a buyer can do re-entracy before items are minted.  code/contracts/shop/EternalHeroesFactory.sol:L278-L284  uint256 refundAmount = _arcAmount.sub(total_cost);  if (refundAmount > 0) {  arcadeumCoin.safeTransferFrom(address(this), _recipient, arcadeumCoinID, refundAmount, \"\");  // Mint tokens to recipient  factoryManager.batchMint(_recipient, _ids, amounts_to_mint, \"\");  Since price should increase after every N items are minted, it s possible to buy more items with the old price.  Recommendation  Add re-entrancy protection or mint items before sending the refund.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.4 Supply limitation misbehaviors    Addressed", "body": "  Resolution   Logic remains unchanged as it s the desired behaviour. But the issue is mitigated in   horizon-games/SkyWeaver-contracts#5 by renaming the term  currentSupply  to  currentIssuance  and  maxSupply  to  maxIssuance  for maximum clarity.  Description  In SWSupplyManager contract, the owner can limit supply for any token ID by setting maxSupply:  code/contracts/shop/SWSupplyManager.sol:L149-L165  function setMaxSupplies(uint256[] calldata _ids, uint256[] calldata _newMaxSupplies) external onlyOwner() {  require(_ids.length == _newMaxSupplies.length, \"SWSupplyManager#setMaxSupply: INVALID_ARRAYS_LENGTH\");  // Can only *decrease* a max supply  // Can't set max supply back to 0  for (uint256 i = 0; i < _ids.length; i++ ) {  if (maxSupply[_ids[i]] > 0) {  require(  0 < _newMaxSupplies[i] && _newMaxSupplies[i] < maxSupply[_ids[i]],  \"SWSupplyManager#setMaxSupply: INVALID_NEW_MAX_SUPPLY\"  );  maxSupply[_ids[i]] = _newMaxSupplies[i];  emit MaxSuppliesChanged(_ids, _newMaxSupplies);  The problem is that you can set maxSupply that is lower than currentSupply, which would be an unexpected state to have.  Also, if some tokens are burned, their currentSupply is not decreasing:  code/contracts/shop/SWSupplyManager.sol:L339-L345  function burn(  uint256 _id,  uint256 _amount)  external  _burn(msg.sender, _id, _amount);  This unexpected behaviour may lead to burning all of the tokens without being able to mint more.  Recommendation  Properly track currentSupply by modifying it in burn function. Consider having a following restriction require(_newMaxSupplies[i] > currentSupply[_ids[i]]) in setMaxSupplies function.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.5 Owner can modify gold cards distribution after someone committed to buy   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  This issue will be addressed by having the owner be a delayed multisig, such that users will have time to witness a change in the distribution that is about to occur.  Description  When a user commits to buying a gold card (and sends weave), there is an expected distribution of possible outcomes. But the problem is that owner can change distribution by calling registerIDs and deregisterIDs  functions.  Additionally, owner can buy any specific gold card avoiding RNG mechanism. It can be done by deleting all the unwanted cards, mining the card and then returning them back. And if owner removes every card from the list, nothing is going to be minted.  Recommendation  There are a few possible recommendations:  Fix a distribution for every order after commit(costly solution).  Make it an explicit part of the trust model (increases trust to the admins).  Cancel pending orders if gold cards IDs are changed.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.6 A buyer of a gold card can manipulate randomness   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  We hereby assume that Horizon will always be willing to mine gold cards even at a loss considering the amount of gold cards that can be created per week is limited. If in practice this becomes a problem, we can upgrade this factory.  Description  When a user is buying a gold card, _commit function is called. After rngDelay number of blocks, someone should call mineGolds function to actually mint the card. If this function is not called during 255 blocks (around 1 hour), a user should call recommit to try to mint a gold card again with a new random seed. So if the user doesn t like a card that s going to be minted (randomly), user can try again until a card is good. The issue is medium because anyone can call mineGolds function in order to prevent this behaviour. But it costs money and there s no incentive for anyone to do so.  Recommendation  Create a mechanism to avoid this kind of manipulation. For example, make sure there is an incentive for someone to call mineGolds function  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.7 A refund is sent to recipient   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  It s unlikely users will send inexact amount since price is fixed. If this becomes a problem in practice we can re-deploy the factory with this added functionality.  Description  When a refund is sent, it s sent to recipient. In case if a user wants to keep game items and money separate, it makes sense to send a refund back to from address.  Recommendation  Since there may be different use cases, consider adding refundAddress to order structure.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.8 Randomness can be manipulated by miners   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  For miners to be able to profit, they would have to forfeit multiple blocks and the desired gold cards would have to be very expensive in the first place (e.g. in the $10k) for it to be worth it for them. In practice, there are also other oppotunities for miners that offer better returns, but if it ever turned out to be a problem, we would see it coming and we can then use a more secure and expensive source of RNG as the gold cards would be very expensive and the additional cost would be worth it.  Description  Random number generator uses future blockhash as a seed. So it s possible for miners to manipulate that value in order to get a better gold card. The issue is minor because it only makes sense if the cost of the card is high enough to do the extra work on the miner side.  Recommendation  Use better RNG algorithms if the price of gold cards is high enough for the miners to start manipulation.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "5.1 TokenFaucet refill can have an unexpected outcome ", "body": "  Description  The TokenFaucet contract can only disburse tokens to the users if it has enough balance. When the contract is running out of tokens, it stops dripping.  code/pool-contracts/contracts/token-faucet/TokenFaucet.sol:L119-L138  uint256 assetTotalSupply = asset.balanceOf(address(this));  uint256 availableTotalSupply = assetTotalSupply.sub(totalUnclaimed);  uint256 newSeconds = currentTimestamp.sub(lastDripTimestamp);  uint256 nextExchangeRateMantissa = exchangeRateMantissa;  uint256 newTokens;  uint256 measureTotalSupply = measure.totalSupply();  if (measureTotalSupply > 0 && availableTotalSupply > 0 && newSeconds > 0) {  newTokens = newSeconds.mul(dripRatePerSecond);  if (newTokens > availableTotalSupply) {  newTokens = availableTotalSupply;  uint256 indexDeltaMantissa = measureTotalSupply > 0 ? FixedPoint.calculateMantissa(newTokens, measureTotalSupply) : 0;  nextExchangeRateMantissa = nextExchangeRateMantissa.add(indexDeltaMantissa);  emit Dripped(  newTokens  );  The owners of the faucet can decide to refill the contract so it can disburse tokens again. If there s been a lot of time since the faucet was drained, the lastDripTimestamp value can be far behind the currentTimestamp. In that case, the users can instantly withdraw some amount (up to all the balance) right after the refill.  Recommendation  To avoid uncertainty, it s essential to call the drip function before the refill. If this call is made in a separate transaction, the owner should make sure that this transaction was successfully mined before sending tokens for the refill.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.2 Gas Optimization on transfers ", "body": "  Description  In TokenFaucet, on every transfer _captureNewTokensForUser is called twice. This function does a few calculations and writes the latest UserState to the storage. However, if lastExchangeRateMantissa == exchangeRateMantissa, or in other words, two transfers happen in the same block, there are no changes in the newToken amounts, so there is an extra storage store with the same values.  Examples  deltaExchangeRateMantissa will be 0 in case two transfers ( no matter from or to) are in the same block for a user.  /pool-contracts/contracts/token-faucet/TokenFaucet.sol  uint256 deltaExchangeRateMantissa = uint256(exchangeRateMantissa).sub(userState.lastExchangeRateMantissa);  uint128 newTokens = FixedPoint.multiplyUintByMantissa(userMeasureBalance, deltaExchangeRateMantissa).toUint128();  userStates[user] = UserState({  lastExchangeRateMantissa: exchangeRateMantissa,  balance: uint256(userState.balance).add(newTokens).toUint128()  });  Recommendation  Return without storage update if lastExchangeRateMantissa == exchangeRateMantissa, or by another method if deltaExchangeRateMantissa == 0. This reduces the gas cost for active users (high number of transfers that might be in the same block)  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.3 Handle transfer tokens where from == to ", "body": "  Description  In TokenFaucet, when calling beforeTokenTransfer it should also be optimized when to == from. This is to prevent any possible issues with internal accounting and token drip calculations.  /pool-contracts/contracts/token-faucet/TokenFaucet.sol  ...  if (token == address(measure) && from != address(0)) {  //add && from != to  drip();  ...  Recommendation  As ERC20 standard, from == to can be allowed but check in beforeTokenTransfer that if to == from, then do not call _captureNewTokensForUser(from); again.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.4 Redundant/Duplicate checks ", "body": "  Description  There are a few checks (require) in TokenFaucet that are redundant and/or checked twice.  Examples  _dripRatePerSecond > 0 checked twice, no need to check it in initialize pool-contracts/contracts/token-faucet/TokenFaucet.sol  require(_dripRatePerSecond > 0, \"TokenFaucet/dripRate-gt-zero\");  asset = _asset;  measure = _measure;  setDripRatePerSecond(_dripRatePerSecond);  function setDripRatePerSecond(uint256 _dripRatePerSecond) public onlyOwner {  require(_dripRatePerSecond > 0, \"TokenFaucet/dripRate-gt-zero\");  lastDripTimestamp == uint32(currentTimestamp) and newSeconds > 0 are basically the same check.  measureTotalSupply can never be < 0, as in the if statement enforces that /pool-contracts/contracts/token-faucet/TokenFaucet.sol#L111-L117  function drip() public returns (uint256) {  uint256 currentTimestamp = _currentTime();  // this should only run once per block.  if (lastDripTimestamp == uint32(currentTimestamp)) {  return 0;  ...  uint256 newSeconds = currentTimestamp.sub(lastDripTimestamp);  ...  if (measureTotalSupply > 0 && availableTotalSupply > 0 && newSeconds > 0) {  ...  uint256 indexDeltaMantissa = measureTotalSupply > 0 ? FixedPoint.calculateMantissa(newTokens, measureTotalSupply) : 0;  Recommendation  Remove the redundant checks to reduce the code size and complexity.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.5 Unnecessary use of upgradability   ", "body": "  Resolution   These contracts are part of   OpenZeppelin Contracts Upgradeable.  Description  Libraries such as SafeMath and SafeCast should not be upgradable as they should be used as pure functions.  Upgradable libraries used in TokenFaucet contract:  SafeMathUpgradeable  SafeCastUpgradeable  IERC20Upgradeable  Recommendation  Remove the upgradability functionality from any part of the system that is unnecessary, as they add complexity and centralization power to the admins.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "3.1 Memory corruption in Buffer    ", "body": "  Resolution   Issue has been closed in   ensdomains/buffer#3  Description  Although out of scope for this audit, the audit team noticed a memory corruption issue in the Buffer library. The init function is as follows:  contracts/Buffer.sol:L22-L41  /**  @dev Initializes a buffer with an initial capacity.  @param buf The buffer to initialize.  @param capacity The number of bytes of space to allocate the buffer.  @return The buffer, for chaining.  /  function init(buffer memory buf, uint capacity) internal pure returns(buffer memory) {  if (capacity % 32 != 0) {  capacity += 32 - (capacity % 32);  // Allocate space for the buffer data  buf.capacity = capacity;  assembly {  let ptr := mload(0x40)  mstore(buf, ptr)  mstore(ptr, 0)  mstore(0x40, add(32, add(ptr, capacity)))  return buf;  Note that memory is reserved only for capacity bytes, but the bytes actually requires capacity + 32 bytes to account for the prefixed array length. Other functions in Buffer assume correct allocation and therefore corrupt nearby memory.  Although we didn t immediately spot an ENS exploit for this vulnerability, we consider any memory corruption issue to be important to address.  Example  A simple test shows the memory corruption issue:  contract Test {  using Buffer for Buffer.buffer;  function test() external pure {  Buffer.buffer memory buffer;  buffer.init(1);  // foo immediately follows buffer.buf in memory  bytes memory foo = new bytes(0);  assert(foo.length == 0);  buffer.append(\"A\");  // \"A\" == 65, gets written to the high order byte of foo.length  assert(foo.length == 65 * 256**31);  Remediation  Allocate an additional 32 bytes as follows, to account for storing the uint256 size of the bytes array:  mstore(0x40, add(ptr, add(capacity, 32)))  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.2 SimplePriceOracle.price is susceptible to integer overflow    ", "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#17 by using  Description  SimplePriceOracle.price is as follows:  ethregistrar/contracts/SimplePriceOracle.sol:L26-L28  function price(string calldata /*name*/, uint /*expires*/, uint duration) external view returns(uint) {  return duration * rentPrice;  This is susceptible to a simple overflow attack, e.g. setting the duration to 2**256/rentPrice to give yourself a price of 0.  Severity note: It s unclear whether the SimplePriceOracle is expected to be used in practice, but the severity is set here under the assumption that the code may be used somewhere.  Remediation  Use SafeMath or explicitly check for the overflow.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.3 ETHRegistrarController.register is vulnerable to front running    ", "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#18  Description  commit() and then register() appears to serve the purpose of preventing front running. However, because the commitment is not tied to a specific owner, it serves equally well as a commitment for a front-running attacker.  Example  Alice calls commit(makeCommitment(\"mydomain\", <secret>)).  10 minutes later, Alice submits a transaction to register(\"mydomain\", Alice, ..., <secret>).  Eve observes this transaction in the transaction pool.  Eve submits register(\"mydomain\", Eve, ..., <secret>) with a higher gas price and wins the race.  Remediation  Commitments should commit to owners in addition to names. This way an attacker can t repurpose a previous commitment. (They would have to buy on behalf of the original committer.)  As an alternative, if it s undesirable to pin down owner, the commitment could include msg.sender instead (only allowing the original committer to call register).  E.g. the following (and corresponding changes to callers):  function makeCommitment(  string memory name,  address owner, /* or perhaps committer/sender */  bytes32 secret  pure  public  returns(bytes32)  bytes32 label = keccak256(bytes(name));  return keccak256(abi.encodePacked(label, owner, secret));  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.4 SOA record check on the wrong domain    ", "body": "  Resolution   During the audit, this issue was discovered by the client development team and already fixed in   ensdomains/root#25.  Description  The SOA record check in Root.getAddress is meant to happen on the root TLD, but in the version of the code audited, it is performed instead on _ens.nic.<tld>.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.5 Work towards a trustless model for ENS   ", "body": "  Resolution  Acknowledged by client team. As stated, this is a long-term issue for which there is no immediate fix, but work is already in progress.  Description  The ENS registry itself is owned by a multisig wallet owned by a number of reputable Ethereum community members. That multisig wallet can do just about anything, up to and including directly taking over any existing or future registered names.  It s important to note that even if we as a community trust the current owners of the multisig wallet, we also need to consider the possibility of their Ethereum private keys being compromised by malicious actors.  Remediation  This centralized control is by design, and the multisig owners have been chosen carefully. However, we do recommend\u2014as is already the plan\u2014that the multisig wallet s power be reduced in future updates to the system. Changes made by that wallet are already quite transparent to the community, but future enhancements might include requiring a waiting period for any changes or disallowing certain types of changes altogether.  In the meantime, wherever possible, the trust model should be made clear so that users understand what guarantees they do and do not have when interacting with ENS.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.6 Consider replacing the Buffer implementation    ", "body": "  Resolution   There will be no immediate fix for this, but the client team is working on collaborating to get a better audited   Description  The audit team uncovered two bugs in the Buffer library, one each in the only two functions that were looked at. (The library was in general not in scope for this audit.) One bug was a critical memory corruption bug. This calls into question how safe this library is to use in general.  Remediation  Consider using a different library, ideally one that has been fully tested and audited and that minimizes the use of inline assembly, particularly around memory allocation.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.7 Overzealous resizing in Buffer    ", "body": "  Resolution   Issue has been closed in   ensdomains/buffer#4  Description  In the following code, the buffer is resized even when sufficient capacity is available to perform the write. The buf.buf.length term is unnecessary and leads to unnecessary resizing:  contracts/Buffer.sol:L91-L95  function write(buffer memory buf, uint off, bytes memory data, uint len) internal pure returns(buffer memory) {  require(len <= data.length);  if (off + len > buf.capacity) {  resize(buf, max(buf.capacity, len + off) * 2);  Contrast with the calculation in a similar function:  contracts/Buffer.sol:L206-L209  function write(buffer memory buf, uint off, bytes32 data, uint len) private pure returns(buffer memory) {  if (len + off > buf.capacity) {  resize(buf, (len + off) * 2);  Remediation  Check just the condition if (off + len > buf.capacity) when deciding whether to resize the buffer. This will be a significant gas savings in the common case of reserving exactly the right capacity and then performing two append operations.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.8 Pending auctions in the legacy registrar don t result in proper ownership in ENS    ", "body": "  Resolution   Addressed in   ensdomains/ethregistrar#23 by reducing the waiting period to 28 days.  Description  If an auction has yet to be finalized in the legacy HashRegistrar at the time that the new, permanent .eth registrar is put in place, the auction winner doesn t get actual ownership of the ENS entry.  The sequence of events would look like:  Auction is started in the HashRegistrar for the name something.eth  The new BaseRegistrarImplementation becomes the owner of the .eth root node in ENS.  The auction is won.  The auction winner calls finalizeAuction, which calls trySetSubnodeOwner, which fails to actually set subnode ownership (as the HashRegistrar no longer has ownership of the .eth root node).  At this point, there s an owner of the deed for the name something.eth in the HashRegistrar, but the ENS subnode is unowned. It can t be transferred to the new registrar for 183 days, and the name can t be registered in the new registrar.  The owner can get themselves out of this situation by calling releaseDeed in the HashRegistrar. If they want to avoid potentially losing their domain in the process, they can transfer the deed to a smart contract which can then release the deed and rent the same name in the new registrar atomically.  Remediation  Here are a few ideas of improvements to help in this situation:  Discourage (or prevent, if possible) new auctions very close to the launch of the new registrar.  Allow domains to be transferred before the 183-day waiting period but require rent payment in those cases. (Perhaps just use the existing grace period to have people renew?)  Document the process for rescuing names that get stuck in this state, or better yet provide a tool for doing so.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.9 BaseRegistrarImplementation.acceptRegistrarTransfer should probably use the live modifier    ", "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#19.  Description  Most external functions in BaseRegistrarImplementation have the live modifier, which ensures that they can only be called on the current ENS owner of the registrar s base address. The acceptRegistrarTransfer function does not have this modifier, which means names can be transferred to the new registrar even if it s not the proper registry owner.  It s hard to think of a real-world example of why this is problematic, especially because the interim registrar appears to protect against this by only transferring to the ens.owner, but it seems safer to include the live modifier unless there s a specific reason not to.  Remediation  Add the live modifier to acceptRegistrarTransfer.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.10 Reconsider use of inline assembly in BytesUtils.sol    ", "body": "  Resolution   Issue has been closed in   ensdomains/dnssec-oracle#55  Description  Root.sol imports and uses @ensdomains/dnssec-oracle/contracts/BytesUtils.sol for byte operations.  BytesUtils.sol is mainly written in assembly. In general, inline assembly is concerning from a security perspective because it bypasses compiler checks and inhibits human code reasoning.  e.g.readUint8():  function readUint8(bytes memory self, uint idx) internal pure returns (uint8 ret) {  require(idx + 1 <= self.length);  assembly {  ret := and(mload(add(add(self, 1), idx)), 0xFF)  Remediation  Some of the functions in BytesUtil.sol can be written in Solidity without affecting the gas costs.  readUint8() can be written as following Solidity code which functions the same:  function readUint8(bytes memory self, uint idx) internal pure returns (uint8 ret) {  return uint8(self[idx]);  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.11 BaseRegistrarImplementation.acceptRegistrarTransfer does not check for invalid names    ", "body": "  Resolution  Short names will be manually canceled in the old registrar during the migration period. Note that this is still feasible with the reduced 28-day lock-up period.  Description  BaseRegistrarImplementation.acceptRegistrarTransfer does not explicitly check for invalid names.  In the old registrar it is possible to register domain names with length less than 7 characters. However anyone can call HashRegistrar.invalidateName() to invalidate the registration and get half of the deed amount as an incentive.  Assume that an invalid domain is registered in the old registrar and no one invalidates the registration (within the 183 days between the registrationDate and the transfer ETHRegistrarController.acceptRegistrarTransfer), it is possible to transfer the invalid domain to the new ENS registrar.  Remediation  Given that it is easy to check for invalid domains using a rainbow table for all possible <7 character domains, anyone can invalidate them before the new registrar goes live. Note that for the auctions starting right before the new registrar goes live, there will be a 183 days window in which anyone can call HashRegistrar.invalidateName() to invalidate the domain names.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.12 Sanity check around transferPeriodEnds    ", "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#23  Description  BaseRegistrarImplementation.acceptRegistrarTransfer has a hardcoded limit such that only domains registered 183 days ago can be transferred in.  This imposes an implicit constraint on the transferPeriodEnds state variable. If the transfer period ends too soon after the new registrar is put in place, names that were just registered won t be transferrable during the transfer period (and will thus become available to be rented by another user).  Remediation  A sanity check in the constructor would help here, e.g.:  require(_transferPeriodEnds > now + 183 days);  Note that the true requirement is something more like  The time between when this registrar becomes the ENS node owner of the .eth domain and the time of transferPeriodEnds must be at least 183 days plus a sufficient time window for late registrants to have a chance to perform the transfer.  But it s hard to see a way to encode this precisely. A broad sanity check will at least avoid simple timing mistakes.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.13 StablePriceOracle.price has an unimportant integer underflow    ", "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#20  Description  ethregistrar/contracts/StablePriceOracle.sol:L57-L63  function price(string calldata name, uint /*expires*/, uint duration) view external returns(uint) {  uint len = name.strlen();  require(len > 0);  if(len > rentPrices.length) {  len = rentPrices.length;  uint priceUSD = rentPrices[len - 1].mul(duration);  If the length of the rentPrices array is 0, then the last line above attempts to access rentPrices[2**256-1]. This will assert, but it might be more friendly (from a gas perspective) to revert in this case.  Remediation  A simple fix would be to move the require(len > 0) down until just before the array access.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.14 ETHRegistrarController.register should revert rather than silently fail    ", "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#22  Description  When called with an invalid commitment or unavailable domain, ETHRegistrarController.register refunds the sent ether and silently fails rather than reverting:  ethregistrar/contracts/ETHRegistrarController.sol:L56-L64  function register(string calldata name, address owner, uint duration, bytes32 secret) external payable {  // Require a valid commitment  bytes32 commitment = makeCommitment(name, secret);  require(commitments[commitment] + MIN_COMMITMENT_AGE <= now);  // If the commitment is too old, or the name is registered, stop  if(commitments[commitment] + MAX_COMMITMENT_AGE < now || !available(name))  {  msg.sender.transfer(msg.value);  return;  register also has no return value, so it s difficult for a caller to know whether the register action succeeded or failed.  Remediation  It s probably better to use require(...) to handle these invalid cases. This is roughly equivalent because no state changes have been made before this early return, but it seems less error prone and clearer to callers about what happened.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.15 StringUtils.strlen could be rewritten without assembly    ", "body": "  Resolution   Issue has been closed in   ensdomains/ethregistrar#21  Description  StringUtils.strlen uses inline assembly to walk through a UTF-8 string and count its character length. In general, inline assembly is concerning from a security perspective because it bypasses compiler checks and inhibits human code reasoning.  Remediation  Consider rewriting in Solidity, something similar to the following:  function strlen(string memory s) internal pure returns (uint256) {  uint256 i = 0;  uint256 len;  for (len = 0; i < bytes(s).length; len++) {  byte b = bytes(s)[i];  if (b < 0x80) {  i += 1;  } else if (b < 0xE0) {  i += 2;  ...  return len;  4 Threat Model  The creation of a threat model is beneficial when building smart contract systems as it helps to understand the potential security threats, assess risk, and identify appropriate mitigation strategies. This is especially useful during the design and development of a contract system as it allows to create a more resilient design which is more difficult to change post-development.  A threat model was created during the audit process in order to analyze the attack surface of the contract system and to focus review and testing efforts on key areas that a malicious actor would likely also attack. It consists of two parts: a high-level analysis that help to understand the attack surface and a list of threats that exist for the contract system.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "4.1 Overview", "body": "  The following assets are managed by contracts and likely targets for an attacker:  Registered domain names (e.g. foo.eth)  Ether, in the form of rent paid to the ETHRegistrarController  The following actors have access to the system to perform an attack:  System owners (ENS itself, registrars, controllers, price oracles)  DNS domain/subdomain owners, who can update DNSSEC records  Users who are registering, renewing, and transferring domains  The following describes the surface area available to attackers:  DNSSEC records  Registrars and controllers  Root contract  Ethereum private keys  Because they were out of scope for this audit, we did not consider some interesting targets such as the DNSSEC oracle, DNSSEC-based registrar, the interim .eth registrar, or the multisig wallet used for ENS ownership.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "4.2 Threat Analysis", "body": "  The following table contains a list of identified threats, along with their mitigations:  user may try to register/renew a domain for less than the expected price  overflow on the rent price / manipulate the price oracle  SafeMath mitigates some potential math errors  user may try to mount denial-of-service attacks on other users (e.g. censor their purchases/renewals)  network DoS  long purchase windows and grace periods  user may try to snipe a domain  front-running  a commit/reveal scheme attempts to prevent this but is ineffective (see section 3), a generous grace period prevents race conditions on expiration  user may try to register .eth TLD  update DNSSEC records  Root disallows changes to that node  Root owner may steal domains, manipulate prices, etc.  ENS root swaps the controller/registrar with malicious code  such manipulation would be transparent today, and future updates may limit the root owners  powers  domain owners may take over already-owned subdomains  change DNSSEC to replace registrar for a domain  this is allowed by design  5 Tool-based analysis  The issues from the tool based analysis have been reviewed and the relevant issues have been listed in chapter 3 - Issues.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Where possible, we ran the full MythX analysis. MythX is still in beta, and where analysis failed, we fell back to running Mythril Classic, a large subset of the functionality of MythX.  Below is the raw output of MythX and Mythril Classic vulnerability scans:  In order to run MythX, Root.sol contract was flattened. flat_root.sol line numbers reflect on the output of truffle-flattener contracts/Root.sol.  Title: Floating Pragma  Head: A floating pragma is set.  Description: It is recommended to make a conscious choice on what version of Solidity is used for compilation. Currently any version equal or greater than \"0.4.24\" is allowed.  Source code:  flat_root.sol 1:0  --------------------------------------------------  pragma solidity ^0.4.24;  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: State variable shadows another state variable.  Description: The state variable \"CLASS_INET\" in contract \"DNSClaimChecker\" shadows another state variable with the same name \"CLASS_INET\" in contract \"Root\".  Source code:  flat_root.sol 869:4  --------------------------------------------------  uint16 constant CLASS_INET = 1  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: State variable shadows another state variable.  Description: The state variable \"TYPE_TXT\" in contract \"DNSClaimChecker\" shadows another state variable with the same name \"TYPE_TXT\" in contract \"Root\".  Source code:  flat_root.sol 870:4  --------------------------------------------------  uint16 constant TYPE_TXT = 16  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"owner\" in contract \"ENS\" shadows the state variable with the same name \"owner\" in contract \"Ownable\".  Source code:  flat_root.sol 20:58  --------------------------------------------------  address owner  --------------------------------------------------  flat_root.sol 22:36  --------------------------------------------------  address owner  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"owner\" in contract \"Root\" shadows the state variable with the same name \"owner\" in contract \"Ownable\".  Source code:  flat_root.sol 1012:44  --------------------------------------------------  address owner  --------------------------------------------------  flat_root.sol 1037:36  --------------------------------------------------  address owner  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"oracle\" in contract \"DNSClaimChecker\" shadows the state variable with the same name \"oracle\" in contract \"Root\".  Source code:  flat_root.sol 881:29  --------------------------------------------------  DNSSEC oracle  --------------------------------------------------  ==================================================  BaseRegistrarImplementation  Mythril Classic results for BaseRegistrarImplementation are as follows. flat_BaseRegistrarImplementation.sol line numbers reflect on the output of truffle-flattener contracts/BaseRegistrarImplementation.sol  ETHRegistrarController  Mythril Classic results for ETHRegistrarController are as follows. flat_ETHRegistrarController.sol line numbers reflect on the output of truffle-flattener contracts/ETHRegistrarController.sol.  ==== Multiple Calls in a Single Transaction ====  SWC ID: 113  Severity: Medium  Contract: ETHRegistrarController  Function name: rentPrice(string,uint256)  PC address: 996  Estimated Gas Usage: 5179 - 79151  Multiple sends are executed in one transaction.  Consecutive calls are executed at the following bytecode offsets:  Offset: 2947  Offset: 3202  Try to isolate each external call into its own transaction, as external calls can fail accidentally or deliberately.  --------------------  In file: flat_ETHRegistrarController.sol:1467  function rentPrice(string memory name, uint duration) view public returns(uint) {  bytes32 hash = keccak256(bytes(name));  return prices.price(name, base.nameExpires(uint256(hash)), duration);  --------------------  ==== Dependence on predictable environment variable ====  SWC ID: 116  Severity: Low  Contract: ETHRegistrarController  Function name: register(string,address,uint256,bytes32)  PC address: 3552  Estimated Gas Usage: 2056 - 6247  Sending of Ether depends on a predictable variable.  The contract sends Ether depending on the values of the following variables:  block.timestamp  block.timestamp  block.timestamp  Note that the values of variables like coinbase, gaslimit, block number and timestamp are predictable and/or can be manipulated by a malicious miner. Don't use them for random number generation or to make critical decisions.  --------------------  In file: flat_ETHRegistrarController.sol:1498  msg.sender.transfer(msg.value)  --------------------  ==== Integer Overflow ====  SWC ID: 101  Severity: High  Contract: ETHRegistrarController  Function name: register(string,address,uint256,bytes32)  PC address: 5332  Estimated Gas Usage: 2333 - 9254  The binary addition can overflow.  The operands of the addition operation are not sufficiently constrained. The addition could therefore result in an integer overflow. Prevent the overflow by checking inputs or ensure sure that the overflow is caught by an assertion.  --------------------  In file: flat_ETHRegistrarController.sol:1411  add(mload(s), ptr)  --------------------  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  ethregistrar  contracts/BaseRegistrarImplementation.sol  36:36     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  60:42     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  65:15     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  73:16     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  73:48     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  75:23     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  83:39     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  85:15     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  89:47     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  114:37    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  118:35    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/ETHRegistrarController.sol  53:63    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  54:34    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  61:64    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  64:58    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/StringUtils.sol  15:8     error    Avoid using Inline Assembly.    security/no-inline-assembly  22:12    error    Avoid using Inline Assembly.    security/no-inline-assembly  \u2716 2 errors, 15 warnings found.  root  No issues found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  root/contracts/Migrations.sol  eac3bb098bace681296263c037b30123fd46e01a  root/contracts/Ownable.sol  b596da7ad9b5c92a119268e05a5f2190659de8d3  root/contracts/Root.sol  94c5fd45635c6d78cae15903bdf565e2c587bdfa  ethregistrar/contracts/BaseRegistrar.sol  dfadfc8a35024069ff66cbc4a82b67dc48129eab  ethregistrar/contracts/BaseRegistrarImplementation.sol  a1e04ce66a9588063155591b59cd695d2d35cabe  ethregistrar/contracts/DummyOracle.sol  e1dab33211d55e02874ae2510e5e773e13056939  ethregistrar/contracts/ETHRegistrarController.sol  7cb180a1d5102efd2acc04b0b518848b6127846e  ethregistrar/contracts/Migrations.sol  b6732a145e4cb6841945488f591b1cf383a6441e  ethregistrar/contracts/PriceOracle.sol  3257acda730f294f19984163f9fe4a19eabdef4d  ethregistrar/contracts/SafeMath.sol  5effc6db2209b2bf2d49abe4ad1ac247e106f8d9  ethregistrar/contracts/SimplePriceOracle.sol  fc11bff8c93e8471b8d8478f1a14b7f43fff2eef  ethregistrar/contracts/StablePriceOracle.sol  892333542a757ba6089c5c3d19d00b337cb0da78  ethregistrar/contracts/StringUtils.sol  4d784bb26b409cfd8ed841f43c4e0ffbfddc450b  ethregistrar/contracts/_TestDeps.sol  2077d541fedbd889d2f814c5c51aa046078f566d  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  Migrations  Implementation  <Constructor>  Public    setCompleted  Public    restricted  upgrade  Public    restricted  Ownable  Implementation  <Constructor>  Public    transferOwnership  Public    onlyOwner  isOwner  Public    NO   Root  Implementation  Ownable  <Constructor>  Public    proveAndRegisterTLD  External    NO   setSubnodeOwner  External    onlyOwner  setRegistrar  External    onlyOwner  registerTLD  Public    NO   setResolver  Public    onlyOwner  setOwner  Public    onlyOwner  setTTL  Public    onlyOwner  getLabel  Internal \ud83d\udd12  getAddress  Internal \ud83d\udd12  getSOAHash  Internal \ud83d\udd12  BaseRegistrar  Implementation  ERC721, Ownable  addController  External    NO   removeController  External    NO   nameExpires  External    NO   available  Public    NO   register  External    NO   renew  External    NO   reclaim  External    NO   acceptRegistrarTransfer  External    NO   BaseRegistrarImplementation  Implementation  BaseRegistrar  <Constructor>  Public    ownerOf  Public    NO   addController  External    onlyOwner  removeController  External    onlyOwner  nameExpires  External    NO   available  Public    NO   register  External    live onlyController  renew  External    live onlyController  reclaim  External    live  acceptRegistrarTransfer  External    NO   DummyOracle  Implementation  <Constructor>  Public    set  Public    NO   read  External    NO   ETHRegistrarController  Implementation  Ownable  <Constructor>  Public    rentPrice  Public    NO   valid  Public    NO   available  Public    NO   makeCommitment  Public    NO   commit  Public    NO   register  External    NO   renew  External    NO   setPriceOracle  Public    onlyOwner  withdraw  Public    onlyOwner  Migrations  Implementation  <Constructor>  Public    setCompleted  Public    restricted  upgrade  Public    restricted  PriceOracle  Interface  price  External    NO   SafeMath  Library  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  add  Internal \ud83d\udd12  mod  Internal \ud83d\udd12  SimplePriceOracle  Implementation  Ownable, PriceOracle  <Constructor>  Public    setPrice  Public    onlyOwner  price  External    NO   DSValue  Interface  read  External    NO   StablePriceOracle  Implementation  Ownable, PriceOracle  <Constructor>  Public    setOracle  Public    onlyOwner  setPrices  Public    onlyOwner  price  External    NO   StringUtils  Library  strlen  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  Root Control Flow  6 Test Coverage Measurement  Testing is implemented using Truffle. 12 tests are included for the Root contract, and they all pass. 30 tests are included for the .eth permanent registrar, and they all pass.  We were unable to obtain code coverage numbers for the tests, but the audit team s overall impression is that testing covers a high percentage of code branches. That said, the testing is weak, in particular regarding negative test cases and edge cases. As a specific example, changing the following in ETHRegistrarController.renew causes no test failures, which shows a serious lack of coverage:  // OLD: require(msg.value >= cost);  // NEW:  require(msg.value > 0);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.1 didTransferShares function has no access control modifier    ", "body": "  Resolution   The concerned function has now been restricted to be only called by   146 with final commit hash as  Description  The staked tokens (shares) in Forta are meant to be transferable. Similarly, the rewards allocation for these shares for delegated staking is meant to be transferable as well. This allocation for the shares  owner is tracked in the StakeAllocator. To enable this, the Forta staking contract FortaStaking implements a _beforeTokenTransfer() function that calls _allocator.didTransferShares() when it is appropriate to transfer the underlying allocation.  code/contracts/components/staking/FortaStaking.sol:L572-L585  function _beforeTokenTransfer(  address operator,  address from,  address to,  uint256[] memory ids,  uint256[] memory amounts,  bytes memory data  ) internal virtual override {  for (uint256 i = 0; i < ids.length; i++) {  if (FortaStakingUtils.isActive(ids[i])) {  uint8 subjectType = FortaStakingUtils.subjectTypeOfShares(ids[i]);  if (subjectType == DELEGATOR_NODE_RUNNER_SUBJECT && to != address(0) && from != address(0)) {  _allocator.didTransferShares(ids[i], subjectType, from, to, amounts[i]);  Due to this, the StakeAllocator.didTransferShares() has an external visibility so it can be called from the FortaStaking contract to perform transfers. However, there is no access control modifier to allow only the staking contract to call this. Therefore, anyone can call this function with whatever parameters they want.  code/contracts/components/staking/allocation/StakeAllocator.sol:L341-L349  function didTransferShares(  uint256 sharesId,  uint8 subjectType,  address from,  address to,  uint256 sharesAmount  ) external {  _rewardsDistributor.didTransferShares(sharesId, subjectType, from, to, sharesAmount);  Recommendation  Apply access control modifiers as appropriate for this contract, for example onlyRole().  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.2 Incorrect reward epoch start date calculation    ", "body": "  Resolution   The suggested recommendations have been implemented in a pull request   144 with a final hash as  Description  The Forta rewards system is based on epochs. A privileged address with the role REWARDER_ROLE calls the reward() function with a parameter for a specific epochNumber that consequently distributes the rewards for that epoch. Additionally, as users stake and delegate their stake, accounts in the Forta system accrue weight that is based on the active stake to distribute these rewards. Since accounts can modify their stake as well as delegate or un-delegate it, the rewards weight for each account can be modified, as seen, for example, in the didAllocate() function. In turn, this modifies the DelegatedAccRewards storage struct that stores the accumulated rewards for each share id. To keep track of changes done to the accumulated rewards, epochs with checkpoints are used to manage the accumulated rate of rewards, their value at the checkpoint, and the timestamp of the checkpoint.  For example, in the didAllocate() function the addRate() function is being called to modify the accumulated rewards.  code/contracts/components/staking/rewards/RewardsDistributor.sol:L89-L101  function didAllocate(  uint8 subjectType,  uint256 subject,  uint256 stakeAmount,  uint256 sharesAmount,  address staker  ) external onlyRole(ALLOCATOR_CONTRACT_ROLE) {  bool delegated = getSubjectTypeAgency(subjectType) == SubjectStakeAgency.DELEGATED;  if (delegated) {  uint8 delegatorType = getDelegatorSubjectType(subjectType);  uint256 shareId = FortaStakingUtils.subjectToActive(delegatorType, subject);  DelegatedAccRewards storage s = _rewardsAccumulators[shareId];  s.delegated.addRate(stakeAmount);  Then the function flow goes into setRate() that checks the existing accumulated rewards storage and modifies it based on the current timestamp.  code/contracts/components/staking/rewards/Accumulators.sol:L34-L36  function addRate(Accumulator storage acc, uint256 rate) internal {  setRate(acc, latest(acc).rate + rate);  code/contracts/components/staking/rewards/Accumulators.sol:L42-L50  function setRate(Accumulator storage acc, uint256 rate) internal {  EpochCheckpoint memory ckpt = EpochCheckpoint({ timestamp: SafeCast.toUint32(block.timestamp), rate: SafeCast.toUint224(rate), value: getValue(acc) });  uint256 length = acc.checkpoints.length;  if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {  acc.checkpoints[length - 1] = ckpt;  } else {  acc.checkpoints.push(ckpt);  Namely, it pushes epoch checkpoints to the list of account checkpoints based on its timestamp. If the last checkpoint s timestamp is during the current epoch, then the last checkpoint is replaced with the new one altogether. If the last checkpoint s timestamp is different from the current epoch, a new checkpoint is added to the list. However, the isCurrentEpoch() function calls a function getCurrentEpochTimestamp() that incorrectly determines the start date of the current epoch. In particular, it doesn t take the offset into account when calculating how many epochs have already passed.  code/contracts/components/staking/rewards/Accumulators.sol:L103-L110  function getCurrentEpochTimestamp() internal view returns (uint256) {  return ((block.timestamp / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET;  function isCurrentEpoch(uint256 timestamp) internal view returns (bool) {  uint256 currentEpochStart = getCurrentEpochTimestamp();  return timestamp > currentEpochStart;  Instead of ((block.timestamp / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET, it should be (((block.timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET. In fact, it should simply call the getEpochNumber() function that correctly provides the epoch number for any timestamp.  code/contracts/components/staking/rewards/Accumulators.sol:L95-L97  function getEpochNumber(uint256 timestamp) internal pure returns (uint32) {  return SafeCast.toUint32((timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH);  In other words, the resulting function would look something like the following:  code/contracts/components/staking/rewards/Accumulators.sol:L45-L48  if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {  acc.checkpoints[length - 1] = ckpt;  } else {  acc.checkpoints.push(ckpt);  This causes several checkpoints to be stored for the same epoch, which would cause issues in functions such as getAtEpoch(), that feeds into getValueAtEpoch() function that provides data for the rewards  share calculation. In the end, this would cause issues in the accounting for the rewards calculation resulting in incorrect distributions.  During the discussion with the Forta Foundation team, it was additionally discovered that there are edge cases around the limits of epochs. Specifically, epoch s end time and the subsequent epoch s start time are exactly the same, although it should be that it is only the start of the next epoch. Similarly, that start time isn t recognized as part of the epoch due to > sign instead of >=. In particular, the following changes need to be made:  Recommendation  A refactor of the epoch timestamp calculation functions is recommended to account for:  The correct epoch number to calculate the start and end timestamps of epochs.  The boundaries of epochs coinciding.  Clarity in functions  intent. For example, adding a function just to calculate any epoch s start time and renaming getCurrentEpochTimestamp() to getCurrentEpochStartTimestamp().  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.3 A single unfreeze dismisses all other slashing proposal freezes    ", "body": "  Resolution   As per the recommendation, the Forta team modified the logic in favor of open proposals. Now, every   149 with a final hash  Description  In order to retaliate against malicious actors, the Forta staking system allows users to submit slashing proposals that are guarded by submitting along a deposit with a slashing reason. These proposals immediately freeze the proposal s subject s stake, blocking them from withdrawing that stake.  At the same time, there can be multiple proposals submitted against the same subject, which works out with freezing   the subject remains frozen with each proposal submitted. However, once any one of the active proposals against the subject gets to the end of its lifecycle, be it REJECTED, DISMISSED, EXECUTED, or REVERTED, the subject gets unfrozen altogether. The other proposals might still be active, but the stake is no longer frozen, allowing the subject to withdraw it if they would like.  In terms of impact, this allows bad actors to avoid punishment intended by the slashes and freezes. A malicious actor could, for example, submit a faulty proposal against themselves in the hopes that it will get quickly rejected or dismissed while the existing, legitimate proposals against them are still being considered. This would allow them to get unfrozen quickly and withdraw their stake. Similarly, in the event a bad staker has several proposals against them, they could withdraw right after a single slashing proposal goes through.  Examples  code/contracts/components/staking/slashing/SlashingController.sol:L174-L179  function dismissSlashProposal(uint256 _proposalId, string[] calldata _evidence) external onlyRole(SLASHING_ARBITER_ROLE) {  _transition(_proposalId, DISMISSED);  _submitEvidence(_proposalId, DISMISSED, _evidence);  _returnDeposit(_proposalId);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L187-L192  function rejectSlashProposal(uint256 _proposalId, string[] calldata _evidence) external onlyRole(SLASHING_ARBITER_ROLE) {  _transition(_proposalId, REJECTED);  _submitEvidence(_proposalId, REJECTED, _evidence);  _slashDeposit(_proposalId);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L215-L229  function reviewSlashProposalParameters(  uint256 _proposalId,  uint8 _subjectType,  uint256 _subjectId,  bytes32 _penaltyId,  string[] calldata _evidence  ) external onlyRole(SLASHING_ARBITER_ROLE) onlyInState(_proposalId, IN_REVIEW) onlyValidSlashPenaltyId(_penaltyId) onlyValidSubjectType(_subjectType) notAgencyType(_subjectType, SubjectStakeAgency.DELEGATOR) {  // No need to check for proposal existence, onlyInState will revert if _proposalId is in undefined state  if (!subjectGateway.isRegistered(_subjectType, _subjectId)) revert NonRegisteredSubject(_subjectType, _subjectId);  _submitEvidence(_proposalId, IN_REVIEW, _evidence);  if (_subjectType != proposals[_proposalId].subjectType || _subjectId != proposals[_proposalId].subjectId) {  _unfreeze(_proposalId);  _freeze(_subjectType, _subjectId);  code/contracts/components/staking/slashing/SlashingController.sol:L254-L259  function revertSlashProposal(uint256 _proposalId, string[] calldata _evidence) external {  _authorizeRevertSlashProposal(_proposalId);  _transition(_proposalId, REVERTED);  _submitEvidence(_proposalId, REVERTED, _evidence);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L267-L272  function executeSlashProposal(uint256 _proposalId) external onlyRole(SLASHER_ROLE) {  _transition(_proposalId, EXECUTED);  Proposal memory proposal = proposals[_proposalId];  slashingExecutor.slash(proposal.subjectType, proposal.subjectId, getSlashedStakeValue(_proposalId), proposal.proposer, slashPercentToProposer);  slashingExecutor.freeze(proposal.subjectType, proposal.subjectId, false);  code/contracts/components/staking/slashing/SlashingController.sol:L337-L339  function _unfreeze(uint256 _proposalId) private {  slashingExecutor.freeze(proposals[_proposalId].subjectType, proposals[_proposalId].subjectId, false);  Recommendation  Introduce a check in the unfreezing mechanics to first ensure there are no other active proposals for that subject.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.4 Storage gap variables slightly off from the intended size    ", "body": "  Resolution  The Forta Team worked on the storage layout to maintain a consistent storage buffer in the inheritance tree. The changes were made through multiple pull requests, also an easy-to-understand layout description has been added through a pull request 157. However, we still found some inconsistencies and recommend doing a thorough review of the buffer space again.  For instance, in FortaStaking (considering the latest commit)  the above-mentioned storage variables will be taking a single slot, however, separate slots are considered for the buffer space(referring to the storage layout description to determine __gap buffer).  Description  The Forta staking system is using upgradeable proxies for its deployment strategy. To avoid storage collisions between contract versions during upgrades, uint256[] private __gap array variables are introduced that create a storage buffer. Together with contract state variables, the storage slots should sum up to 50. For example, the __gap variable is present in the BaseComponentUpgradeable component, which is the base of most Forta contracts, and there is a helpful comment in AgentRegistryCore that describes how its relevant __gap variable size was calculated:  code/contracts/components/BaseComponentUpgradeable.sol:L62  uint256[50] private __gap;  code/contracts/components/agents/AgentRegistryCore.sol:L196  uint256[41] private __gap; // 50 - 1 (frontRunningDelay) - 3 (_stakeThreshold) - 5 StakeSubjectUpgradeable  However, there are a few places where the __gap size was not computed correctly to get the storage slots up to 50. Some of these are:  code/contracts/components/scanners/ScannerRegistry.sol:L234  uint256[49] private __gap;  code/contracts/components/dispatch/Dispatch.sol:L333  uint256[47] private __gap;  code/contracts/components/node_runners/NodeRunnerRegistryCore.sol:L452  uint256[44] private __gap;  While these still provide large storage buffers, it is best if the __gap variables are calculated to hold the same buffer within contracts of similar types as per the initial intentions to avoid confusion.  During conversations with the Forta Foundation team, it appears that some contracts like ScannerRegistry and AgentRegistry should instead add up to 45 with their __gap variable due to the StakeSubject contracts they inherit from adding 5 from themselves. This is something to note and be careful with as well for future upgrades.  Recommendation  Provide appropriate sizes for the __gap variables to have a consistent storage layout approach that would help avoid storage issues with future versions of the system.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.5 AgentRegistryCore - Agent Creation DoS    ", "body": "  Resolution  The Forta team as per the recommendations modified the minting logic to allow users to mint an agentId only for their own address in a pull request 155 with final hash as 7426891222e2bcdf2bbbec669905d5041f9fb58e. Also, the team claims that the Agent Ids are generated through the Forta Bot SDK to minimize the collision risk. However, this has not been verified by the auditing team.  We still recommend notifying users to check whether an ID is already registered prior to making any commitment if a front-running delay is enabled, to avoid unintended DoS.  Description  AgentRegistryCore allows anyone to mint an agentID for the desired owner address. However, in some cases, it may fall prey to DoS, either deliberately or unintentionally.  For instance, let s assume the Front Running Protection is disabled or the frontRunningDelay is 0. It means anyone can directly create an agent without any prior commitment. Thus, anyone can observe pending transactions and try to front run them to mint an agentID prior to the victim s restricting it to mint a desired agentID.  Also, it may be possible that a malicious actor succeeds in frontrunning a transaction with manipulated data/chainIDs but with the same owner address and agentID. There is a good chance that victim still accepts the attacker s transaction as valid, even though its own transaction reverted, due to the fact that the victim is still seeing itself as the owner of that ID.  Taking an instance where let s assume the frontrunning protection is enabled. Still, there is a good chance that two users vouch for the same agentIDs and commits in the same block, thus getting the same frontrunning delay. Then, it will be a game of luck, whoever creates that agent first will get the ID minted to its address, and the other user s transaction will be reverted wasting the time they have spent on the delay.  As the agentIDs can be picked by users, the chances of collisions with an already minted ID will increase over time causing unnecessary reverts for others.  Adding to the fact that there is no restriction for owner address, anyone can spam mint any agentID to any address for any profitable reason.  Examples  code/contracts/components/agents/AgentRegistryCore.sol:L68-L77  function createAgent(uint256 agentId, address owner, string calldata metadata, uint256[] calldata chainIds)  public  onlySorted(chainIds)  frontrunProtected(keccak256(abi.encodePacked(agentId, owner, metadata, chainIds)), frontRunningDelay)  _mint(owner, agentId);  _beforeAgentUpdate(agentId, metadata, chainIds);  _agentUpdate(agentId, metadata, chainIds);  _afterAgentUpdate(agentId, metadata, chainIds);  Recommendation  Modify function prepareAgent to not commit an already registered agentID.  A better approach could be to allow sequential minting of agentIDs using some counters.  Only allow users to mint an agentID, either for themselves or for someone they are approved to.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.6 Lack of checks for rewarding an epoch that has already been rewarded    ", "body": "  Resolution   The suggested recommendations have been implemented in a pull request   150 with final hash  Description  To give rewards to the participating stakers, the Forta system utilizes reward epochs for each shareId, i.e. a delegated staking share. Each epoch gets their own reward distribution, and then StakeAllocator and RewardsDistributor contracts along with the Forta staking shares determine how much the users get.  Although totalRewardsDistributed is essentially isolated to the sweep() function to allow transferring out the reward tokens without taking away those tokens reserved for the reward distribution, this still creates an inconsistency, albeit a minor one in the context of the current system.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L155-L167  function reward(  uint8 subjectType,  uint256 subjectId,  uint256 amount,  uint256 epochNumber  ) external onlyRole(REWARDER_ROLE) {  if (subjectType != NODE_RUNNER_SUBJECT) revert InvalidSubjectType(subjectType);  if (!_subjectGateway.isRegistered(subjectType, subjectId)) revert RewardingNonRegisteredSubject(subjectType, subjectId);  uint256 shareId = FortaStakingUtils.subjectToActive(getDelegatorSubjectType(subjectType), subjectId);  _rewardsPerEpoch[shareId][epochNumber] = amount;  totalRewardsDistributed += amount;  emit Rewarded(subjectType, subjectId, amount, epochNumber);  Recommendation  Implement checks as appropriate to the reward() function to ensure correct behavior of totalRewardsDistributed tracking. Also, implement necessary changes to the tracking of pending rewards, if necessary.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.7 Reentrancy in FortaStaking during ERC1155 mints    ", "body": "  Resolution   The Forta team implemented a Reentrancy Guard in a pull request   151 with a final hash  Description  In the Forta staking system, the staking shares (both  active  and  inactive ) are represented as tokens implemented according to the ERC1155 standard. The specific implementation that is being used utilizes a smart contract acceptance check _doSafeTransferAcceptanceCheck() upon mints to the recipient.  code/contracts/components/staking/FortaStaking.sol:L54  contract FortaStaking is BaseComponentUpgradeable, ERC1155SupplyUpgradeable, SubjectTypeValidator, ISlashingExecutor, IStakeMigrator {  The specific implementation for ERC1155SupplyUpgradeable contracts can be found here, and the smart contract check can be found here.  This opens up reentrancy into the system s flow. In fact, the reentrancy occurs on all mints that happen in the below functions, and it happens before a call to another Forta contract for allocation is made via either _allocator.depositAllocation or _allocator.withdrawAllocation:  code/contracts/components/staking/FortaStaking.sol:L273-L295  function deposit(  uint8 subjectType,  uint256 subject,  uint256 stakeValue  ) external onlyValidSubjectType(subjectType) notAgencyType(subjectType, SubjectStakeAgency.MANAGED) returns (uint256) {  if (address(subjectGateway) == address(0)) revert ZeroAddress(\"subjectGateway\");  if (!subjectGateway.isStakeActivatedFor(subjectType, subject)) revert StakeInactiveOrSubjectNotFound();  address staker = _msgSender();  uint256 activeSharesId = FortaStakingUtils.subjectToActive(subjectType, subject);  bool reachedMax;  (stakeValue, reachedMax) = _getInboundStake(subjectType, subject, stakeValue);  if (reachedMax) {  emit MaxStakeReached(subjectType, subject);  uint256 sharesValue = stakeToActiveShares(activeSharesId, stakeValue);  SafeERC20.safeTransferFrom(stakedToken, staker, address(this), stakeValue);  _activeStake.mint(activeSharesId, stakeValue);  _mint(staker, activeSharesId, sharesValue, new bytes(0));  emit StakeDeposited(subjectType, subject, staker, stakeValue);  _allocator.depositAllocation(activeSharesId, subjectType, subject, staker, stakeValue, sharesValue);  return sharesValue;  code/contracts/components/staking/FortaStaking.sol:L303-L326  function migrate(  uint8 oldSubjectType,  uint256 oldSubject,  uint8 newSubjectType,  uint256 newSubject,  address staker  ) external onlyRole(SCANNER_2_NODE_RUNNER_MIGRATOR_ROLE) {  if (oldSubjectType != SCANNER_SUBJECT) revert InvalidSubjectType(oldSubjectType);  if (newSubjectType != NODE_RUNNER_SUBJECT) revert InvalidSubjectType(newSubjectType);  if (isFrozen(oldSubjectType, oldSubject)) revert FrozenSubject();  uint256 oldSharesId = FortaStakingUtils.subjectToActive(oldSubjectType, oldSubject);  uint256 oldShares = balanceOf(staker, oldSharesId);  uint256 stake = activeSharesToStake(oldSharesId, oldShares);  uint256 newSharesId = FortaStakingUtils.subjectToActive(newSubjectType, newSubject);  uint256 newShares = stakeToActiveShares(newSharesId, stake);  _activeStake.burn(oldSharesId, stake);  _activeStake.mint(newSharesId, stake);  _burn(staker, oldSharesId, oldShares);  _mint(staker, newSharesId, newShares, new bytes(0));  emit StakeDeposited(newSubjectType, newSubject, staker, stake);  _allocator.depositAllocation(newSharesId, newSubjectType, newSubject, staker, stake, newShares);  code/contracts/components/staking/FortaStaking.sol:L365-L387  function initiateWithdrawal(  uint8 subjectType,  uint256 subject,  uint256 sharesValue  ) external onlyValidSubjectType(subjectType) returns (uint64) {  address staker = _msgSender();  uint256 activeSharesId = FortaStakingUtils.subjectToActive(subjectType, subject);  if (balanceOf(staker, activeSharesId) == 0) revert NoActiveShares();  uint64 deadline = SafeCast.toUint64(block.timestamp) + _withdrawalDelay;  _lockingDelay[activeSharesId][staker].setDeadline(deadline);  uint256 activeShares = Math.min(sharesValue, balanceOf(staker, activeSharesId));  uint256 stakeValue = activeSharesToStake(activeSharesId, activeShares);  uint256 inactiveShares = stakeToInactiveShares(FortaStakingUtils.activeToInactive(activeSharesId), stakeValue);  SubjectStakeAgency agency = getSubjectTypeAgency(subjectType);  _activeStake.burn(activeSharesId, stakeValue);  _inactiveStake.mint(FortaStakingUtils.activeToInactive(activeSharesId), stakeValue);  _burn(staker, activeSharesId, activeShares);  _mint(staker, FortaStakingUtils.activeToInactive(activeSharesId), inactiveShares, new bytes(0));  if (agency == SubjectStakeAgency.DELEGATED || agency == SubjectStakeAgency.DELEGATOR) {  _allocator.withdrawAllocation(activeSharesId, subjectType, subject, staker, stakeValue, activeShares);  Although this doesn t seem to be an issue in the current Forta system of contracts since the allocator s logic doesn t seem to be manipulable, this could still be dangerous as it opens up an external execution flow.  Recommendation  Consider introducing a reentrancy check or emphasize this behavior in the documentation, so that both other projects using this system later and future upgrades along with maintenance work on the Forta staking system itself are implemented safely.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.8 Unnecessary code blocks that check the same condition    ", "body": "  Resolution   The code block has been refactored under a single conditional block as per the suggested recommendation in a pull request   152 with a final hash as  Description  In the RewardsDistributor there is a function that allows to set delegation fees for a NodeRunner. It adjusts the fees[] array for that node as appropriate. However, during its checks, it performs the same check twice in a row.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L259-L264  if (fees[1].sinceEpoch != 0) {  if (Accumulators.getCurrentEpochNumber() < fees[1].sinceEpoch + delegationParamsEpochDelay) revert SetDelegationFeeNotReady();  if (fees[1].sinceEpoch != 0) {  fees[0] = fees[1];  Recommendation  Consider refactoring this under a single code block.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.9 Event spam in RewardsDistributor.claimRewards    ", "body": "  Resolution  Forta team has implemented the recommended check in a pull request 153, as: if (epochRewards == 0) revert ZeroAmount(\"epochRewards\");  The implemented check will now be reverting the transaction if there exists no reward for an epoch number. However, it may not be a gas-efficient approach for the user claiming rewards and accidentally passing an incorrect epoch number. A better approach could be to transfer any reward and emit any event only for a non-zero epochReward.  Description  The RewardsDistributor contract allows users to claim their rewards through the claimRewards() function. It does check to see whether or not the user has already claimed the rewards for a specific epoch that they are claiming for, but it does not check to see if the user has any associated rewards at all. This could lead to event ClaimedRewards being spammed by malicious users, especially on low gas chains.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L224-L229  for (uint256 i = 0; i < epochNumbers.length; i++) {  if (_claimedRewardsPerEpoch[shareId][epochNumbers[i]][_msgSender()]) revert AlreadyClaimed();  _claimedRewardsPerEpoch[shareId][epochNumbers[i]][_msgSender()] = true;  uint256 epochRewards = _availableReward(shareId, isDelegator, epochNumbers[i], _msgSender());  SafeERC20.safeTransfer(rewardsToken, _msgSender(), epochRewards);  emit ClaimedRewards(subjectType, subjectId, _msgSender(), epochNumbers[i], epochRewards);  Recommendation  Add a check for rewards amounts being greater than 0.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.10 SubjectTypes.sol files unused    ", "body": "  Resolution   The unused file has now been removed in commit   2548e0a4f7b38926362a759f4fa0611394348d6e  Description  There is a rogue file SubjectTypes.sol that is not being utilized. It appears that its intended functionality is being done by the SubjectTypeValidator.sol file as it even has a contract with the same name implemented there.  Examples  code/contracts/components/staking/SubjectTypes.sol:L4-L10  pragma solidity ^0.8.9;  uint8 constant SCANNER_SUBJECT = 0;  uint8 constant AGENT_SUBJECT = 1;  uint8 constant NODE_RUNNER_SUBJECT = 3;  contract SubjectTypeValidator {  Recommendation  Remove the SubjectTypes.sol file.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.11 Lack of a check for the subject s stake for reviewSlashProposalParameters    ", "body": "  Resolution   The recommended check has now been added in a pull request   154 with final hash as  Description  While it may be assumed that the review function will be called by a privileged and knowledgeable actor, this additional check may avoid accidental mistakes.  Examples  code/contracts/components/staking/slashing/SlashingController.sol:L153  if (subjectGateway.totalStakeFor(_subjectType, _subjectId) == 0) revert ZeroAmount(\"subject stake\");  code/contracts/components/staking/slashing/SlashingController.sol:L226-L229  if (_subjectType != proposals[_proposalId].subjectType || _subjectId != proposals[_proposalId].subjectId) {  _unfreeze(_proposalId);  _freeze(_subjectType, _subjectId);  Recommendation  Add a check for the new subject having stake to slash.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.12 Comment and code inconsistencies    ", "body": "  Resolution   The comments have now been found fixed as per the implemented logic, primarily in the pull request   156 and in another commit with hash  f4ee799ee192084965643b09b69f3cbeababd5ae  Description  During the audit a few inconsistencies were found between what the comments say and what the implemented code actually did.  Examples  Subject Type Agency for Scanner Subjects  In the SubjectTypeValidator, the comment says that the SCANNER_SUBJECT is of type DIRECT agency type, i.e. it can be directly staked on by multiple different stakers. However, we found a difference in the implementation, where the concerned subject is defined as type MANAGED agency type, which says that it cannot be staked on directly; instead it s a delegated type and the allocation is supposed to be managed by its manager.  code/contracts/components/staking/SubjectTypeValidator.sol:L21  code/contracts/components/staking/SubjectTypeValidator.sol:L66-L67  - SCANNER_SUBJECT --> DIRECT  code/contracts/components/staking/SubjectTypeValidator.sol:L66-L67  } else if (subjectType == SCANNER_SUBJECT) {  return SubjectStakeAgency.MANAGED;  Dispatch refers to ERC721 tokens as ERC1155  One of the comments describing the functionality to link and unlink agents and scanners refers to them as ERC1155 tokens, when in reality they are ERC721.  code/contracts/components/dispatch/Dispatch.sol:L179-L185  /**  @notice Assigns the job of running an agent to a scanner.  @dev currently only allowed for DISPATCHER_ROLE (Assigner software).  @dev emits Link(agentId, scannerId, true) event.  @param agentId ERC1155 token id of the agent.  @param scannerId ERC1155 token id of the scanner.  /  NodeRunnerRegistryCore comment that implies the reverse of what happens  A comment describing a helper function that returns address for a given scanner ID describes the opposite behavior. It is the same comment for the function just above that actually does what the comment says.  code/contracts/components/node_runners/NodeRunnerRegistryCore.sol:L259-L262  /// Converts scanner address to uint256 for FortaStaking Token Id.  function scannerIdToAddress(uint256 scannerId) public pure returns (address) {  return address(uint160(scannerId));  ScannerToNodeRunnerMigration comment that says that no NodeRunner tokens must be owned  For the migration from Scanners to NodeRunners, a comment in the beginning of the file implies that for the system to work correctly, there must be no NodeRunner tokens owned prior to migration. After a conversation with the Forta Foundation team, it appears that this was an early design choice that is no longer relevant.  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L69  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L91  @param nodeRunnerId If set as 0, a new NodeRunnerRegistry ERC721 will be minted to nodeRunner (but it must not own any prior),  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L91  Recommendation  @param nodeRunnerId If set as 0, a new NodeRunnerRegistry ERC721 will be minted to nodeRunner (but it must not own any prior),  Recommendation  Verify the operational logic and fix either the concerned comments or defined logic as per the need.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.1 zNS - Domain bid might be approved by non owner account    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by storing the domain request data on-chain.  Description  The spec allows anyone to place a bid for a domain, while only parent domain owners are allowed to approve a bid. Bid placement is actually enforced and purely informational. In practice, approveDomainBid allows any parent domain owner to approve bids (signatures) for any other domain even if they do not own it. Once approved, anyone can call fulfillDomainBid to create a domain.  Examples  zNS/contracts/StakingController.sol:L95-L103  function approveDomainBid(  uint256 parentId,  string memory bidIPFSHash,  bytes memory signature  ) external authorizedOwner(parentId) {  bytes32 hashOfSig = keccak256(abi.encode(signature));  approvedBids[hashOfSig] = true;  emit DomainBidApproved(bidIPFSHash);  Recommendation  Consider adding a validation check that allows only the parent domain owner to approve bids on one of its domains. Reconsider the design of the system introducing more on-chain guarantees for bids.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.2 zAuction, zNS - Bids cannot be cancelled, never expire, and the auction lifecycle is unclear    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by refactoring the StakingController to control the lifecycle of bids instead of handling this off-chain.  Addressed with zer0-os/zAuction@135b2aa for zAuction by adding a bid/saleOffer expiration for bids. The client also provided the following statement:  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.6 added expireblock and startblock to zauction, expireblock to zsale", "body": " Decided not to add a cancel function. Paying gas to cancel isn t ideal, and it can be used as a griefing function. though that s still possible to do by moving weth but differently  The stateless nature of auctions may make it hard to enforce bid/sale expirations and it is not possible to cancel a bid/offer that should not be valid anymore. The expiration reduces the risk of old offers being used as they now automatically invalidate after time, however, it is still likely that multiple valid offers may be present at the same time. As outlined in the recommendation, one option would be to allow someone who signed a commitment to explicitly cancel it in the contract. Another option would be to create a stateful auction where the entity that puts up something for  starts  an auction, creating an auction id, requiring bidders to bid on that auction id. Once a bid is accepted the auction id is invalidated which invalidates all bids that might be floating around.  zer0-os/zAuction@2f92aa1 for  Description  The lifecycle of a bid both for zAuction and zNS is not clear, and has many flaws.  zAuction - Consider the case where a bid is placed, then the underlying asset in being transferred to a new owner. The new owner can now force to sell the asset even though it s might not be relevant anymore.  zAuction - Once a bid was accepted and the asset was transferred, all other bids need to be invalidated automatically, otherwise and old bid might be accepted even after the formal auction is over.  zAuction, zNS - There is no way for the bidder to cancel an old bid. That might be useful in the event of a significant change in market trend, where the old pricing is no longer relevant. Currently, in order to cancel a bid, the bidder can either withdraw his ether balance from the zAuctionAccountant, or disapprove WETH which requires an extra transaction that might be front-runned by the seller.  Examples  zAuction/contracts/zAuction.sol:L35-L45  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  Recommendation  Consider adding an expiration field to the message signed by the bidder both for zAuction and zNS. Consider adding auction control, creating an auctionId, and have users bid on specific auctions. By adding this id to the signed message, all other bids are invalidated automatically and users would have to place new bids for a new auction. Optionally allow users to cancel bids explicitly.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.3 zNS - Insufficient protection against replay attacks    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by avoiding the use of digital signatures and storing the domain request data on-chain.  Description  There is no dedicated data structure to prevent replay attacks on StakingController. approvedBids mapping offers only partial mitigation, due to the fact that after a domain bid is fulfilled, the only mechanism in place to prevent a replay attack is the Registrar contract that might be replaced in the case where StakingController is being re-deployed with a different Registrar instance. Additionally, the digital signature used for domain bids does not identify the buyer request uniquely enough. The bidder s signature could be replayed in future similar contracts that are deployed with a different registrar or in a different network.  Examples  zNS/contracts/StakingController.sol:L176-L183  function createBid(  uint256 parentId,  uint256 bidAmount,  string memory bidIPFSHash,  string memory name  ) public pure returns(bytes32) {  return keccak256(abi.encode(parentId, bidAmount, bidIPFSHash, name));  Recommendation  Consider adding a dedicated mapping to store the a unique identifier of a bid, as well as adding address(this), block.chainId, registrar and nonce to the message that is being signed by the bidder.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.4 zNS - domain name collisions    ", "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a by disallowing empty names for domain registrations. The name validation in off-chain components (e.g. subgraph components) has not been verified.  Description  Domain registration accepts an empty (zero-length) name. This may allow a malicious entity to register two different NFT s for the same visually indinstinguishable text representation of a domain. Similar to this the domain name is mapped to an NFT via a subgraph that connects parent names to the new subdomain using a domain separation character (dot/slash/\u2026). Someone might be able to register a.b to cats.cool which might resolve to the same domain as if someone registers cats.cool.a and then cats.cool.a.b.  Examples  0/cats/ = 0xfe  0/cats/<empty-string/ = 0xfe.keccak(\"\")  zNS/contracts/Registrar.sol:L76-L96  function registerDomain(  uint256 parentId,  string memory name,  address domainOwner,  address minter  ) external override onlyController returns (uint256) {  // Create the child domain under the parent domain  uint256 labelHash = uint256(keccak256(bytes(name)));  address controller = msg.sender;  // Domain parents must exist  require(_exists(parentId), \"Zer0 Registrar: No parent\");  // Calculate the new domain's id and create it  uint256 domainId =  uint256(keccak256(abi.encodePacked(parentId, labelHash)));  _createDomain(domainId, domainOwner, minter, controller);  emit DomainCreated(domainId, name, labelHash, parentId, minter, controller);  return domainId;  Recommendation  Disallow empty subdomain names. Disallow domain separators in names (in the offchain component or smart contract).  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.5 zAuction, zNS - gas griefing by spamming offchain fake bids   ", "body": "  Resolution  Addressed and acknowledged with changes from zer0-os/zAuction@135b2aa. The client provided the following remark:  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.19 I have attempted to order the requires sensibly, putting the least expensive first. Please advise if the ordering is optimal. gas griefing will be mitigated in the dapp with off-client checks", "body": "  Description  The execution status of both zAuction.acceptBid and StakingController.fulfillDomainBid transactions depend on the bidder, as his approval is needed, his signature is being validated, etc. However, these transactions can be submitted by accounts that are different from the bidder account, or for accounts that do not have the required funds/deposits available, luring the account that has to perform the on-chain call into spending gas on a transaction that is deemed to fail (gas griefing). E.g. posting high-value fake bids for zAuction without having funds deposited or WETH approved.  Examples  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  zAuction/contracts/zAuction.sol:L35-L44  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  Recommendation  Revert early for checks that depend on the bidder before performing gas-intensive computations.  Consider adding a dry-run validation for off-chain components before transaction submission.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.6 zNS - anyone can front-run fulfillDomainBid to lock the domain setting or set different metadata    ", "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a by restricting the method to only be callable by the requester.  Description  Anyone observing a call to fulfillDomainBid can front-run this call for the original bidder, provide different metadata/royalty amount, or lock the metadata, as these parameters are not part of the bidder s signature. The impact is limited as both metadata, royalty amount, and lock state can be changed by the domain owner after creation.  Examples  zNS/contracts/StakingController.sol:L120-L143  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  Recommendation  Consider adding metadata, royaltyAmount, and lockOnCreation to the message signed by the bidder if the parent should have some control over metadata and lockstatus and restrict access to this function to msg.sender==recoveredbidder.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.7 zNS- Using a digital signature as a hash preimage    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by avoiding the use of digital signatures.  Description  Using the encoded signature (r,s,v) or the hash of the signature to prevent replay or track if signatures have been seen/used is not recommended in general, as it may introduce signature malleability issues, as two different signature params (r,s,v) may be producable that validly sign the same data.  The impact for this codebase, however, is limited, due to the fact that openzeppelins ECDSA wrapper library is used which checks for malleable ECDSA signatures (high s value). We still decided to keep this as a medium issue to raise awareness, that it is bad practice to rely on the hash of signatures instead of the hash of the actual signed data for checks.  In another instance in zAuction, a global random nonce is used to prevent replay attacks. This is suboptimal and instead, the hash of the signed data (including a nonce) should be used.  Examples  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  zAuction/contracts/zAuction.sol:L35-L39  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  Recommendation  Consider creating the bid identifier by hashing the concatenation of all bid parameters instead. Ensure to add replay protection https://github.com/ConsenSys/zer0-zns-audit-2021-05/issues/19. Always check for the hash of the signed data instead of the hash of the encoded signature to track whether a signature has been seen before.  Consider implementing Ethereum typed structured data hashing and signing according to EIP-712.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.8 zNS - Registrar skips __ERC721Pausable_init()    ", "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a  Description  The initialization function of registrar skips the chained initializer __ERC721Pausable_init to initialize __ERC721_init(\"Zer0 Name Service\", \"ZNS\"). This basically skips the following initialization calls:  abstract contract ERC721PausableUpgradeable is Initializable, ERC721Upgradeable, PausableUpgradeable {  function __ERC721Pausable_init() internal initializer {  __Context_init_unchained();  __ERC165_init_unchained();  __Pausable_init_unchained();  __ERC721Pausable_init_unchained();  Examples  zNS/contracts/Registrar.sol:L39-L45  function initialize() public initializer {  __Ownable_init();  __ERC721_init(\"Zer0 Name Service\", \"ZNS\");  // create the root domain  _createDomain(0, msg.sender, msg.sender, address(0));  Recommendation  consider calling the missing initializers to register the interface for erc165 if needed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.9 zNS - Registrar is ERC721PausableUpgradeable but there is no way to actually pause it    ", "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a by exposing pausable functionality to the contract owner.  Description  The registrar is ownable and pausable but the functionality to pause the contract is not implemented.  zNS/contracts/Registrar.sol:L8-L12  contract Registrar is  IRegistrar,  OwnableUpgradeable,  ERC721PausableUpgradeable  Recommendation  Simplification is key. Remove the pausable functionality if the contract is not meant to be paused or consider implementing an external pause() function decorated onlyOwner.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.10 zNS - Avoid no-ops    ", "body": "  Resolution   Addressed with   zer0-os/ZNS@ab7d62a.  Description  Code paths that are causing transactions to be ended with an ineffective outcome or no-operation (no actual state changes) are not advisable, as they consume more gas, hide misconfiguration or error cases (e.g. adding the same controller multiple times), and may impact other processes that rely upon transaction s logs.  Examples  Reject adding an already existing controller, and removing non existing controller.  zNS/contracts/Registrar.sol:L51-L67  /**  @notice Authorizes a controller to control the registrar  @param controller The address of the controller  /  function addController(address controller) external override onlyOwner {  controllers[controller] = true;  emit ControllerAdded(controller);  /**  @notice Unauthorizes a controller to control the registrar  @param controller The address of the controller  /  function removeController(address controller) external override onlyOwner {  controllers[controller] = false;  emit ControllerRemoved(controller);  Recommendation  Consider reverting code paths that end up in ineffective outcomes (i.e. no-operation) as early as possible.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.1 RocketNodeDistributorDelegate - Reentrancy in distribute() allows node owner to drain distributor funds    ", "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by implementing a custom reentrancy guard via a new state variable lock that is appended to the end of the storage layout. The reentrancy guard is functionally equivalent to the OpenZeppelin implementation. The method was not refactored to give user funds priority over the node share. Additionally, the client provided the following statement:  We acknowledge this as a critical issue and have solved with a reentrancy guard.  We followed OpenZeppelin s design for a reentrancy guard. We were unable to use it directly as it is hardcoded to use storage slot 0 and because we already have deployment of this delegate in the wild already using storage slot 0 for another purpose, we had to append it to the end of the existing storage layout.  Description  The distribute() function distributes the contract s balance between the node operator and the user. The node operator is returned their initial collateral, including a fee. The rest is returned to the RETH token contract as user collateral.  After determining the node owner s share, the contract transfers ETH to the node withdrawal address, which can be the configured withdrawal address or the node address. Both addresses may potentially be a malicious contract that recursively calls back into the distribute() function to retrieve the node share multiple times until all funds are drained from the contract. The distribute() function is not protected against reentrancy:  code/contracts/contract/node/RocketNodeDistributorDelegate.sol:L59-L73  /// @notice Distributes the balance of this contract to its owners  function distribute() override external {  // Calculate node share  uint256 nodeShare = getNodeShare();  // Transfer node share  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  (bool success,) = withdrawalAddress.call{value : nodeShare}(\"\");  require(success);  // Transfer user share  uint256 userShare = address(this).balance;  address rocketTokenRETH = rocketStorage.getAddress(rocketTokenRETHKey);  payable(rocketTokenRETH).transfer(userShare);  // Emit event  emit FeesDistributed(nodeAddress, userShare, nodeShare, block.timestamp);  We also noticed that any address could set a withdrawal address as there is no check for the caller to be a registered node. In fact, the caller can be the withdrawal address or node operator.  code/contracts/contract/RocketStorage.sol:L118-L133  // Set a node's withdrawal address  function setWithdrawalAddress(address _nodeAddress, address _newWithdrawalAddress, bool _confirm) external override {  // Check new withdrawal address  require(_newWithdrawalAddress != address(0x0), \"Invalid withdrawal address\");  // Confirm the transaction is from the node's current withdrawal address  address withdrawalAddress = getNodeWithdrawalAddress(_nodeAddress);  require(withdrawalAddress == msg.sender, \"Only a tx from a node's withdrawal address can update it\");  // Update immediately if confirmed  if (_confirm) {  updateWithdrawalAddress(_nodeAddress, _newWithdrawalAddress);  // Set pending withdrawal address if not confirmed  else {  pendingWithdrawalAddresses[_nodeAddress] = _newWithdrawalAddress;  Recommendation  Add a reentrancy guard to functions that interact with untrusted contracts. Adhere to the checks-effects pattern and send user funds to the  trusted  RETH contract first. Only then send funds to the node s withdrawal address.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.2 RocketMinipoolDelegateOld - Node operator may reenter finalise() to manipulate accounting    ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this issue (it was reported via our Immunefi bug bounty). It is live but that code path is inaccessible. It requires the oDAO to mark a minipool as Withdrawable which we don t do and have removed from the withdrawal process moving forward.  In a later revision, the development team fixed the issue in the following commit: 73d5792a671db5d2f4dcbd35737e729f9e01aa11  Description  node.minipools.finalised.count<NodeAddress>: NodeAddress finalised count increased twice instead  minipools.finalised.count: global finalised count increased twice  eth.matched.node.amount<NodeAddress> - NodeAddress eth matched amount potentially reduced too many times; has an impact on getNodeETHCollateralisationRatio -> GetNodeShare, getNodeETHProvided -> getNodeEffectiveRPLStake and getNodeETHProvided->getNodeMaximumRPLStake->withdrawRPL and is the limiting factor when withdrawing RPL to ensure the pools stay collateralized.  Note: RocketMinipoolDelegateOld is assumed to be the currently deployed MiniPool implementation. Users may upgrade from this delegate to the new version and can roll back at any time and re-upgrade, even within the same transaction (see issue 5.3 ).  The following is an annotated call stack from a node operator calling minipool.finalise() reentering finalise() once more on their Minipool:  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L182-L191  // Called by node operator to finalise the pool and unlock their RPL stake  function finalise() external override onlyInitialised onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) {  // Can only call if withdrawable and can only be called once  require(status == MinipoolStatus.Withdrawable, \"Minipool must be withdrawable\");  // Node operator cannot finalise the pool unless distributeBalance has been called  require(withdrawalBlock > 0, \"Minipool balance must have been distributed at least once\");  // Finalise the pool  _finalise();  _refund() handing over control flow to nodeWithdrawalAddress  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L311-L341  // Perform any slashings, refunds, and unlock NO's stake  function _finalise() private {  // Get contracts  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  // Can only finalise the pool once  require(!finalised, \"Minipool has already been finalised\");  // If slash is required then perform it  if (nodeSlashBalance > 0) {  _slash();  // Refund node operator if required  if (nodeRefundBalance > 0) {  _refund();  // Send any left over ETH to rETH contract  if (address(this).balance > 0) {  // Send user amount to rETH contract  payable(rocketTokenRETH).transfer(address(this).balance);  // Trigger a deposit of excess collateral from rETH contract to deposit pool  RocketTokenRETHInterface(rocketTokenRETH).depositExcessCollateral();  // Unlock node operator's RPL  rocketMinipoolManager.incrementNodeFinalisedMinipoolCount(nodeAddress);  // Update unbonded validator count if minipool is unbonded  if (depositType == MinipoolDeposit.Empty) {  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  rocketDAONodeTrusted.decrementMemberUnbondedValidatorCount(nodeAddress);  // Set finalised flag  finalised = true;  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L517-L528  function _refund() private {  // Update refund balance  uint256 refundAmount = nodeRefundBalance;  nodeRefundBalance = 0;  // Get node withdrawal address  address nodeWithdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  // Transfer refund amount  (bool success,) = nodeWithdrawalAddress.call{value : refundAmount}(\"\");  require(success, \"ETH refund amount was not successfully transferred to node operator\");  // Emit ether withdrawn event  emit EtherWithdrawn(nodeWithdrawalAddress, refundAmount, block.timestamp);  Methods adjusting system settings called twice:  code/contracts/contract/old/minipool/RocketMinipoolManagerOld.sol:L265-L272  // Increments _nodeAddress' number of minipools that have been finalised  function incrementNodeFinalisedMinipoolCount(address _nodeAddress) override external onlyLatestContract(\"rocketMinipoolManager\", address(this)) onlyRegisteredMinipool(msg.sender) {  // Update the node specific count  addUint(keccak256(abi.encodePacked(\"node.minipools.finalised.count\", _nodeAddress)), 1);  // Update the total count  addUint(keccak256(bytes(\"minipools.finalised.count\")), 1);  code/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L139-L142  function decrementMemberUnbondedValidatorCount(address _nodeAddress) override external onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) onlyRegisteredMinipool(msg.sender) {  subUint(keccak256(abi.encodePacked(daoNameSpace, \"member.validator.unbonded.count\", _nodeAddress)), 1);  Recommendation  We recommend setting the finalised = true flag immediately after checking for it. Additionally, the function flow should adhere to the checks-effects-interactions pattern whenever possible. We recommend adding generic reentrancy protection whenever the control flow is handed to an untrusted entity.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.3 RocketMinipoolDelegate - Sandwiching of Minipool calls can have unintended side effects ", "body": "  Resolution  The client provided the following statement:  The slashed value is purely for NO informational purposes and not used in any logic in the contracts so this example is benign as you say. We have fixed this particular issue by moving the slashed boolean out of the delegate and into RocketMinipooLManager. It is now set on any call to rocketNodeStaking.slashRPL which covers both old delegate and new.  We appreciate that the finding was more a classification of potential issues with upgrades and rollbacks. At this stage, we cannot change this functionality as it is already deployed in a non-upgradable way to over 12,000 contracts.  As this is more of a guidance and there is no immediate threat, we don t believe this should be considered a  major  finding.  With https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 the slashed flag was moved to RocketNodeStaking.slashRPL() (minipool.rpl.slashed|<msg.sender> = true).  The audit team acknowledges that this issue does not provide a concrete exploit that puts funds at risk. However, due to the sensitive nature and potential for issues regarding future updates, we stand by the initial severity rating as it stands for security vulnerabilities that may not be directly exploitable or require certain conditions to be exploited.  Description  The RocketMinipoolBase contract exposes the functions delegateUpgrade and delegateRollback, allowing the minipool owner to switch between delegate implementations. While giving the minipool owner a chance to roll back potentially malfunctioning upgrades, the fact that upgrades and rollback are instantaneous also gives them a chance to alternate between executing old and new code (e.g. by utilizing callbacks) and sandwich user calls to the minipool.  Examples  Assuming the latest minipool delegate implementation, any user can call RocketMinipoolDelegate.slash, which slashes the node operator s RPL balance if a slashing has been recorded on their validator. To mark the minipool as having been slashed, the slashed contract variable is set to true. A minipool owner can avoid this flag from being set By sandwiching the user calls:  Minipool owner rolls back to the old implementation from RocketMinipoolDelegateOld.sol  User calls slash on the now old delegate implementation (where slashed is not set)  Minipool owner upgrades to the latest delegate implementation again  In detail, the new slash implementation:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L687-L696  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  // Record slashing  slashed = true;  Compared to the old slash implementation:  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L531-L539  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  While the bypass of slashed being set is a benign example, the effects of this issue, in general, could result in a significant disruption of minipool operations and potentially affect the system s funds. The impact highly depends on the changes introduced by future minipool upgrades.  Recommendation  We recommend limiting upgrades and rollbacks to prevent minipool owners from switching implementations with an immediate effect. A time lock can fulfill this purpose when a minipool owner announces an upgrade to be done at a specific block. A warning can precede user-made calls that an upgrade is pending, and their interaction can have unintended side effects.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.4 RocketDAONodeTrustedActions - No way to access ETH provided by non-member votes   ", "body": "  Resolution  According to the client, this is the intended behavior. The client provided the following statement:  This is by design.  Description  DAO members can challenge nodes to prove liveliness for free. Non-DAO members must provide members.challenge.cost = 1 eth to start a challenge. However, the provided challenge cost is locked within the contract instead of being returned or recycled as system collateral.  Examples  code/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L181-L192  // In the event that the majority/all of members go offline permanently and no more proposals could be passed, a current member or a regular node can 'challenge' a DAO members node to respond  // If it does not respond in the given window, it can be removed as a member. The one who removes the member after the challenge isn't met, must be another node other than the proposer to provide some oversight  // This should only be used in an emergency situation to recover the DAO. Members that need removing when consensus is still viable, should be done via the 'kick' method.  function actionChallengeMake(address _nodeAddress) override external onlyTrustedNode(_nodeAddress) onlyRegisteredNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrustedActions\", address(this)) payable {  // Load contracts  RocketDAONodeTrustedInterface rocketDAONode = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  RocketDAONodeTrustedSettingsMembersInterface rocketDAONodeTrustedSettingsMembers = RocketDAONodeTrustedSettingsMembersInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMembers\"));  // Members can challenge other members for free, but for a regular bonded node to challenge a DAO member, requires non-refundable payment to prevent spamming  if(rocketDAONode.getMemberIsValid(msg.sender) != true) require(msg.value == rocketDAONodeTrustedSettingsMembers.getChallengeCost(), \"Non DAO members must pay ETH to challenge a members node\");  // Can't challenge yourself duh  require(msg.sender != _nodeAddress, \"You cannot challenge yourself\");  // Is this member already being challenged?  Recommendation  We recommend locking the ETH inside the contract during the challenge process. If a challenge is refuted, we recommend feeding the locked value back into the system as protocol collateral. If the challenge succeeds and the node is kicked, it is assumed that the challenger will be repaid the amount they had to lock up to prove non-liveliness.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.5 Multiple checks-effects violations ", "body": "  Resolution  The client provided the following statement:  In many of the cited examples, the  external call  is a call to another network contract that has the same privileges as the caller. Preventing reentrancy against our own internal contracts provides no additional security. If a malicious contract is introduced via a malicious oDAO they already have full keys to the kingdom.  None of the examples provide an attack surface and so we don t believe this to be a  major  finding and should be downgraded.  This finding highlights our concerns about a dangerous pattern used throughout the codebase that may eventually lead to exploitable scenarios if continued to be followed, especially on codebases that do not employ protective measures against reentrant calls. This report also flagged one such exploitable instance, leading to a critical exploitable issue in one of the components.  This repeated occurrence led us to flag this as a major issue to highlight a general error and attack surface present in several places.  From our experience, there are predominantly positive side-effects of adhering to safe coding patterns, even for trusted contract interactions, as developers indirectly follow or pick up the coding style from existing code, reducing the likelihood of following a pattern that may be prone to be taken advantage of.  For example, to a developer, it might not always be directly evident that control flow is passed to potentially untrusted components/addresses from the code itself, especially when calling multiple  trusted  components in the system. Furthermore, individual components down the call stack may be updated at later times, introducing an untrusted external call (i.e., because funds are refunded) and exposing the initially calling contract to a reentrancy-type issue. Therefore, we highly recommend adhering to a safe checks-effects pattern even though the contracts mainly interact with other trusted components and build secure code based on defense-in-depth principles to contain potential damage in favor of assuming worst-case scenarios.  Description  Throughout the system, there are various violations of the checks-effects-interactions pattern where the contract state is updated after an external call. Since large parts of the Rocket Pool system s smart contracts are not guarded against reentrancy, the external call s recipient may reenter and potentially perform malicious actions that can impact the overall accounting and, thus, system funds.  Examples  distributeToOwner() sends the contract s balance to the node or the withdrawal address before clearing the internal accounting:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L564-L581  /// @notice Withdraw node balances from the minipool and close it. Only accepts calls from the owner  function close() override external onlyMinipoolOwner(msg.sender) onlyInitialised {  // Check current status  require(status == MinipoolStatus.Dissolved, \"The minipool can only be closed while dissolved\");  // Distribute funds to owner  distributeToOwner();  // Destroy minipool  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  require(rocketMinipoolManager.getMinipoolExists(address(this)), \"Minipool already closed\");  rocketMinipoolManager.destroyMinipool();  // Clear state  nodeDepositBalance = 0;  nodeRefundBalance = 0;  userDepositBalance = 0;  userDepositBalanceLegacy = 0;  userDepositAssignedTime = 0;  The withdrawal block should be set before any other contracts are called:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L498-L499  // Save block to prevent multiple withdrawals within a few blocks  withdrawalBlock = block.number;  The slashed state should be set before any external calls are made:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L686-L696  /// @dev Slash node operator's RPL balance based on nodeSlashBalance  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  // Record slashing  slashed = true;  In the bond reducer, the accounting values should be cleared before any external calls are made:  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L120-L134  // Get desired to amount  uint256 newBondAmount = getUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));  require(rocketNodeDeposit.isValidDepositAmount(newBondAmount), \"Invalid bond amount\");  // Calculate difference  uint256 existingBondAmount = minipool.getNodeDepositBalance();  uint256 delta = existingBondAmount.sub(newBondAmount);  // Get node address  address nodeAddress = minipool.getNodeAddress();  // Increase ETH matched or revert if exceeds limit based on current RPL stake  rocketNodeDeposit.increaseEthMatched(nodeAddress, delta);  // Increase node operator's deposit credit  rocketNodeDeposit.increaseDepositCreditBalance(nodeAddress, delta);  // Clean up state  deleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.time\", msg.sender)));  deleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));  The counter for reward snapshot execution should be incremented before RPL gets minted:  code/contracts/contract/rewards/RocketRewardsPool.sol:L210-L213  // Execute inflation if required  rplContract.inflationMintTokens();  // Increment the reward index and update the claim interval timestamp  incrementRewardIndex();  Recommendation  We recommend following the checks-effects-interactions pattern and adjusting any contract state variables before making external calls. With the upgradeable nature of the system, we also recommend strictly adhering to this practice when all external calls are being made to trusted network contracts.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.6 Minipool state machine design and pseudo-states   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement.  We agree that the state machine is complicated. This is a symptom of technical debt and backwards compatibility.  There is no actionable response to this finding as we cannot make changes to the existing 12,000 contracts already deployed.  We want to emphasize that this finding strongly suggests that there are design deficits in the minipool state machine that, sooner or later, may impact the overall system s security. We suggest refactoring a clean design with clear transitions and states for the current iteration removing technical debt from future versions. This may mean that it may be warranted to release a new major Rocketpool version as a standalone system with a clean migration path avoiding potential problems otherwise introduced by dealing with the current technical debt.  Description  Recommendation  We strongly discourage the use of pseudo-states in state machines as they make the state machine less intuitive and present challenges in mapping state transitions to the code base. Real states and transitions should be used where possible.  Generally, we recommend the following when designing state machines:  Using clear and descriptive transition names,  Avoiding having multiple transitions with the same trigger,  Modeling decisions in the form of state transitions rather than states themselves.  In any case, every Minipool should terminate in a clear end state.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.7 RocketMinipoolDelegate - Redundant refund() call on forced finalization    ", "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by refactoring refund() to avoid a double invocation of _refund() in the _finalise() codepath.  Fixed per the recommendation. Thanks.  Description  The RocketMinipoolDelegate.refund function will force finalization if a user previously distributed the pool. However, _finalise already calls _refund() if there is a node refund balance to transfer, making the additional call to _refund() in refund() obsolete.  Examples  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L200-L209  function refund() override external onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) onlyInitialised {  // Check refund balance  require(nodeRefundBalance > 0, \"No amount of the node deposit is available for refund\");  // If this minipool was distributed by a user, force finalisation on the node operator  if (!finalised && userDistributed) {  _finalise();  // Refund node  _refund();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L445-L459  function _finalise() private {  // Get contracts  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  // Can only finalise the pool once  require(!finalised, \"Minipool has already been finalised\");  // Set finalised flag  finalised = true;  // If slash is required then perform it  if (nodeSlashBalance > 0) {  _slash();  // Refund node operator if required  if (nodeRefundBalance > 0) {  _refund();  Recommendation  We recommend refactoring the if condition to contain _refund() in the else branch.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.8 Sparse documentation and accounting complexity   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  Acknowledged and agree.  Description  Throughout the project, inline documentation is either sparse or missing altogether. Furthermore, few technical documents about the system s design rationale are available. The recent releases  increased complexity makes it significantly harder to trace the flow of funds through the system as components change semantics, are split into separate contracts, etc.  It is essential that documentation not only outlines what is being done but also why and what a function s role in the system s  bigger picture  is. Many comments in the code base fail to fulfill this requirement and are thus redundant, e.g.  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L292-L293  // Sanity check that refund balance is zero  require(nodeRefundBalance == 0, \"Refund balance not zero\");  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L333-L334  // Remove from vacant set  rocketMinipoolManager.removeVacantMinipool();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L381-L383  if (ownerCalling) {  // Finalise the minipool if the owner is calling  _finalise();  The increased complexity and lack of documentation can increase the likelihood of developer error. Furthermore, the time spent maintaining the code and introducing new developers to the code base will drastically increase. This effect can be especially problematic in the system s accounting of funds as the various stages of a Minipool imply different flows of funds and interactions with external dependencies. Documentation should explain the rationale behind specific hardcoded values, such as the magic 8 ether boundary for withdrawal detection. An example of a lack of documentation and distribution across components is the calculation and influence of ethMatched as it plays a role in:  the minipool bond reducer,  the node deposit contract,  the node manager, and  the node staking contract.  Recommendation  As the Rocketpool system grows in complexity, we highly recommend significantly increasing the number of inline comments and general technical documentation and exploring ways to centralize the system s accounting further to provide a clear picture of which funds move where and at what point in time. Where the flow of funds is obscured because multiple components or multi-step processes are involved, we recommend adding extensive inline documentation to give context.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.9 RocketNodeDistributor - Missing extcodesize check in dynamic proxy   ", "body": "  Resolution  The client decided not to address the finding with the upcoming update. As per their assessment, the scenario outlined would require a series of misconfigurations/failures and hence is unlikely to happen. Following a defense-in-depth approach we, nevertheless, urge to implement safeguards on multiple layers as a condition like this can easily go undetected. However, after reviewing the feedback provided by the client we share the assessment that the finding should be downgraded from Major to Medium as funds are not at immediate risk and they can recover from this problem by fixing the delegate. For transparency, the client provided the following statement:  Agree that an extcodesize check here would add safety against a future mistake. But it does require a failure at many points for it to actually lead to an issue. Beacuse this contract is not getting upgraded in Atlas, we will leave it as is. We will make note to add a safety check on it in a future update of this contract.  We don t believe this consitutes a  major  finding given that it requires a future significant failure. If such a failure were to happen, the impact is also minimal as any calls to distribute() would simply do nothing. A contract upgrade would fix the problem and no funds would be at risk.  Description  Examples  code/contracts/contract/node/RocketNodeDistributor.sol:L23-L31  fallback() external payable {  address _target = rocketStorage.getAddress(distributorStorageKey);  assembly {  calldatacopy(0x0, 0x0, calldatasize())  let result := delegatecall(gas(), _target, 0x0, calldatasize(), 0x0, 0)  returndatacopy(0x0, 0x0, returndatasize())  switch result case 0 {revert(0, returndatasize())} default {return (0, returndatasize())}  code/contracts/contract/RocketStorage.sol:L153-L155  function getAddress(bytes32 _key) override external view returns (address r) {  return addressStorage[_key];  Recommendation  Before delegate-calling into the target contract, check if it exists.  assembly {  codeSize := extcodesize(_target)  require(codeSize > 0);  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.10 Kicked oDAO members  votes taken into account   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but the additional changes required to implement a fix outweigh the concern in our opinion.  Description  oDAO members can vote on proposals or submit external data to the system, acting as an oracle. Data submission is based on a vote by itself, and multiple oDAO members must submit the same data until a configurable threshold (51% by default) is reached for the data to be confirmed.  When a member gets kicked or leaves the oDAO after voting, their vote is still accounted for while the total number of oDAO members decreases.  A (group of) malicious oDAO actors may exploit this fact to artificially lower the consensus threshold by voting for a proposal and then leaving the oDAO. This will leave excess votes with the proposal while the total member count decreases.  For example, let s assume there are 17 oDAO members. 9 members must vote for the proposal for it to pass (52.9%). Let s assume 8 members voted for, and the rest abstained and is against the proposal (47%, threshold not met). The proposal is unlikely to pass unless two malicious oDAO members leave the DAO, lowering the member count to 15 in an attempt to manipulate the vote, suddenly inflating vote power from 8/17 (47%; rejected) to 8/15 (53.3%; passed).  The crux is that the votes of ex-oDAO members still count, while the quorum is based on the current oDAO member number.  Here are some examples, however, this is a general pattern used for oDAO votes in the system.  Example: RocketNetworkPrices  Members submit votes via submitPrices(). If the threshold is reached, the proposal is executed. Quorum is based on the current oDAO member count, votes of ex-oDAO members are still accounted for. If a proposal is a near miss, malicious actors can force execute it by leaving the oDAO, lowering the threshold, and then calling executeUpdatePrices() to execute it.  code/contracts/contract/network/RocketNetworkPrices.sol:L75-L79  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  // Update the price  updatePrices(_block, _rplPrice);  code/contracts/contract/network/RocketNetworkPrices.sol:L85-L86  function executeUpdatePrices(uint256 _block, uint256 _rplPrice) override external onlyLatestContract(\"rocketNetworkPrices\", address(this)) {  // Check settings  RocketMinipoolBondReducer  The RocketMinipoolBondReducer contract s voteCancelReduction function takes old votes of previously kicked oDAO members into account. This results in the vote being significantly higher and increases the potential for malicious actors, even after their removal, to sway the vote. Note that a canceled bond reduction cannot be undone.  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L94-L98  RocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));  uint256 quorum = rocketDAONode.getMemberCount().mul(rocketDAONodeTrustedSettingsMinipool.getCancelBondReductionQuorum()).div(calcBase);  bytes32 totalCancelVotesKey = keccak256(abi.encodePacked(\"minipool.bond.reduction.vote.count\", _minipoolAddress));  uint256 totalCancelVotes = getUint(totalCancelVotesKey).add(1);  if (totalCancelVotes > quorum) {  RocketNetworkPenalties  code/contracts/contract/network/RocketNetworkPenalties.sol:L47-L51  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodePenaltyThreshold()) {  setBool(executedKey, true);  incrementMinipoolPenaltyCount(_minipoolAddress);  code/contracts/contract/network/RocketNetworkPenalties.sol:L54-L58  // Executes incrementMinipoolPenaltyCount if consensus threshold is reached  function executeUpdatePenalty(address _minipoolAddress, uint256 _block) override external onlyLatestContract(\"rocketNetworkPenalties\", address(this)) {  // Get contracts  RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));  // Get submission keys  Recommendation  Track oDAO members  votes and remove them from the tally when the removal from the oDAO is executed.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.11 RocketDAOProtocolSettingsRewards - settings key collission   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but making this change now with an existing deployment outweighs the concern in our opinion.  Description  A malicious user may craft a DAO protocol proposal to set a rewards claimer for a specific contract, thus overwriting another contract s settings. This issue arises due to lax requirements when choosing safe settings keys.  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49  function setSettingRewardsClaimer(string memory _contractName, uint256 _perc) override public onlyDAOProtocolProposal {  // Get the total perc set, can't be more than 100  uint256 percTotal = getRewardsClaimersPercTotal();  // If this group already exists, it will update the perc  uint256 percTotalUpdate = percTotal.add(_perc).sub(getRewardsClaimerPerc(_contractName));  // Can't be more than a total claim amount of 100%  require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");  // Update the total  setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);  // Update/Add the claimer amount  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", _contractName)), _perc);  // Set the time it was updated at  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", _contractName)), block.timestamp);  The method updates the rewards claimer for a specific contract by writing to the following two setting keys:  settingNameSpace.rewards.claimsgroup.amount<_contractName>  settingNameSpace.rewards.claimsgroup.amount.updated.time<_contractName>  Due to the way the settings hierarchy was chosen in this case, a malicious proposal might define a <_contractName> = .updated.time<targetContract> that overwrites the settings of a different contract with an invalid value.  Note that the issue of delimiter consistency is also discussed in issue 5.12.  The severity rating is based on the fact that this should be detectable by DAO members. However, following a defense-in-depth approach means that such collisions should be avoided wherever possible.  Recommendation  We recommend enforcing a unique prefix and delimiter when concatenating user-provided input to setting keys. In this specific case, the settings could be renamed as follows:  settingNameSpace.rewards.claimsgroup.amount.value<_contractName>  settingNameSpace.rewards.claimsgroup.amount.updated.time<_contractName>  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.12 RocketDAOProtocolSettingsRewards - missing setting delimiters   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but making this change now with an existing deployment outweighs the concern in our opinion.  Description  Settings in the Rocket Pool system are hierarchical, and namespaces are prefixed using dot delimiters.  Calling abi.encodePacked(<string>, <string>) on strings performs a simple concatenation. According to the settings  naming scheme, it is suggested that the following example writes to a key named: <settingNameSpace>.rewards.claims.group.amount.<_contractName>. However, due to missing delimiters, the actual key written to is: <settingNameSpace>.rewards.claimsgroup.amount<_contractName>.  Note that there is no delimiter between claims|group and amount|<_contractName>.  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49  function setSettingRewardsClaimer(string memory _contractName, uint256 _perc) override public onlyDAOProtocolProposal {  // Get the total perc set, can't be more than 100  uint256 percTotal = getRewardsClaimersPercTotal();  // If this group already exists, it will update the perc  uint256 percTotalUpdate = percTotal.add(_perc).sub(getRewardsClaimerPerc(_contractName));  // Can't be more than a total claim amount of 100%  require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");  // Update the total  setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);  // Update/Add the claimer amount  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", _contractName)), _perc);  // Set the time it was updated at  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", _contractName)), block.timestamp);  Recommendation  We recommend adding the missing intermediate delimiters. The system should enforce delimiters after the last setting key before user input is concatenated to reduce the risk of accidental namespace collisions.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.13 Use of address instead of specific contract types   ", "body": "  Resolution  The client acknowledges the finding, removed the unnecessary casts from canReduceBondAmount and voteCancelReduction with https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6, and provided the following statement:  Acknowledged. We will migrate to this pattern as we upgrade contracts.  Description  Rather than using a low-level address type and then casting to the safer contract type, it s better to use the best type available by default so the compiler can eventually check for type safety and contract existence and only downcast to less secure low-level types (address) when necessary.  Examples  RocketStorageInterface _rocketStorage should be declared in the arguments, removing the need to cast the address explicitly.  code/contracts/contract/minipool/RocketMinipoolBase.sol:L39-L47  /// @notice Sets up starting delegate contract and then delegates initialisation to it  function initialise(address _rocketStorage, address _nodeAddress) external override notSelf {  // Check input  require(_nodeAddress != address(0), \"Invalid node address\");  require(storageState == StorageState.Undefined, \"Already initialised\");  // Set storage state to uninitialised  storageState = StorageState.Uninitialised;  // Set rocketStorage  rocketStorage = RocketStorageInterface(_rocketStorage);  RocketMinipoolInterface _minipoolAddress should be declared in the arguments, removing the need to cast the address explicitly. Downcast to low-level address if needed. The event can be redeclared with the contract type.  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L33-L34  function beginReduceBondAmount(address _minipoolAddress, uint256 _newBondAmount) override external onlyLatestContract(\"rocketMinipoolBondReducer\", address(this)) {  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L69-L76  /// @notice Returns whether owner of given minipool can reduce bond amount given the waiting period constraint  /// @param _minipoolAddress Address of the minipool  function canReduceBondAmount(address _minipoolAddress) override public view returns (bool) {  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  RocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));  uint256 reduceBondTime = getUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.time\", _minipoolAddress)));  return rocketDAONodeTrustedSettingsMinipool.isWithinBondReductionWindow(block.timestamp.sub(reduceBondTime));  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L80-L84  function voteCancelReduction(address _minipoolAddress) override external onlyTrustedNode(msg.sender) onlyLatestContract(\"rocketMinipoolBondReducer\", address(this)) {  // Prevent calling if consensus has already been reached  require(!getReduceBondCancelled(_minipoolAddress), \"Already cancelled\");  // Get contracts  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  Note that abi.encode*(contractType) assumes address for contract types by default. An explicit downcast is not required.  More examples of address _minipool declarations:  code/contracts/contract/minipool/RocketMinipoolManager.sol:L449-L455  /// @dev Internal logic to set a minipool's pubkey  /// @param _pubkey The pubkey to set for the calling minipool  function _setMinipoolPubkey(address _minipool, bytes calldata _pubkey) private {  // Load contracts  AddressSetStorageInterface addressSetStorage = AddressSetStorageInterface(getContractAddress(\"addressSetStorage\"));  // Initialize minipool & get properties  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipool);  code/contracts/contract/minipool/RocketMinipoolManager.sol:L474-L478  function getMinipoolDetails(address _minipoolAddress) override external view returns (MinipoolDetails memory) {  // Get contracts  RocketMinipoolInterface minipoolInterface = RocketMinipoolInterface(_minipoolAddress);  RocketMinipoolBase minipool = RocketMinipoolBase(payable(_minipoolAddress));  RocketNetworkPenaltiesInterface rocketNetworkPenalties = RocketNetworkPenaltiesInterface(getContractAddress(\"rocketNetworkPenalties\"));  More examples of RocketStorageInterface _rocketStorage casts:  code/contracts/contract/node/RocketNodeDistributor.sol:L8-L13  contract RocketNodeDistributor is RocketNodeDistributorStorageLayout {  bytes32 immutable distributorStorageKey;  constructor(address _nodeAddress, address _rocketStorage) {  rocketStorage = RocketStorageInterface(_rocketStorage);  nodeAddress = _nodeAddress;  Recommendation  We recommend using more specific types instead of address where possible. Downcast if necessary. This goes for parameter types as well as state variable types.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.14 Redundant double casts   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  Acknowledged. These contracts are non-upgradable.  Description  _rocketStorageAddress  is already of contract type RocketStorageInterface.  code/contracts/contract/RocketBase.sol:L78-L82  /// @dev Set the main Rocket Storage address  constructor(RocketStorageInterface _rocketStorageAddress) {  // Update the contract address  rocketStorage = RocketStorageInterface(_rocketStorageAddress);  _tokenAddress  is already of contract type ERC20Burnable.  code/contracts/contract/RocketVault.sol:L132-L138  function burnToken(ERC20Burnable _tokenAddress, uint256 _amount) override external onlyLatestNetworkContract {  // Get contract key  bytes32 contractKey = keccak256(abi.encodePacked(getContractName(msg.sender), _tokenAddress));  // Update balances  tokenBalances[contractKey] = tokenBalances[contractKey].sub(_amount);  // Get the token ERC20 instance  ERC20Burnable tokenContract = ERC20Burnable(_tokenAddress);  _rocketTokenRPLFixedSupplyAddress is already of contract type IERC20.  code/contracts/contract/token/RocketTokenRPL.sol:L47-L51  constructor(RocketStorageInterface _rocketStorageAddress, IERC20 _rocketTokenRPLFixedSupplyAddress) RocketBase(_rocketStorageAddress) ERC20(\"Rocket Pool Protocol\", \"RPL\") {  // Version  version = 1;  // Set the mainnet RPL fixed supply token address  rplFixedSupplyContract = IERC20(_rocketTokenRPLFixedSupplyAddress);  Recommendation  We recommend removing the unnecessary double casts and copies of local variables.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.15 RocketMinipoolDelegate - Missing event in prepareVacancy    ", "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by emitting a new event MinipoolVacancyPrepared.  Agreed. Added event per recommendation. Thanks.  Description  The function prepareVacancy updates multiple contract state variables and should therefore emit an event.  Examples  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L286-L309  /// @dev Sets the bond value and vacancy flag on this minipool  /// @param _bondAmount The bond amount selected by the node operator  /// @param _currentBalance The current balance of the validator on the beaconchain (will be checked by oDAO and scrubbed if not correct)  function prepareVacancy(uint256 _bondAmount, uint256 _currentBalance) override external onlyLatestContract(\"rocketMinipoolManager\", msg.sender) onlyInitialised {  // Check status  require(status == MinipoolStatus.Initialised, \"Must be in initialised status\");  // Sanity check that refund balance is zero  require(nodeRefundBalance == 0, \"Refund balance not zero\");  // Check balance  RocketDAOProtocolSettingsMinipoolInterface rocketDAOProtocolSettingsMinipool = RocketDAOProtocolSettingsMinipoolInterface(getContractAddress(\"rocketDAOProtocolSettingsMinipool\"));  uint256 launchAmount = rocketDAOProtocolSettingsMinipool.getLaunchBalance();  require(_currentBalance >= launchAmount, \"Balance is too low\");  // Store bond amount  nodeDepositBalance = _bondAmount;  // Calculate user amount from launch amount  userDepositBalance = launchAmount.sub(nodeDepositBalance);  // Flag as vacant  vacant = true;  preMigrationBalance = _currentBalance;  // Refund the node whatever rewards they have accrued prior to becoming a RP validator  nodeRefundBalance = _currentBalance.sub(launchAmount);  // Set status to preLaunch  setStatus(MinipoolStatus.Prelaunch);  Recommendation  Emit the missing event.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.16 Compiler error due to missing RocketMinipoolBaseInterface    ", "body": "  Resolution   Fixed in   https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by adding the missing interface file.  Description  The interface RocketMinipoolBaseInterface is missing from the code repository. Manually generating the interface and adding it to the repository fixes the error.  Recommendation  Add the missing source unit to the repository.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.17 Unused Imports   Partially Addressed", "body": "  Resolution  Addressed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by removing all but the following two mentioned unused imports:  RocketRewardsPoolInterface  RocketSmoothingPoolInterface  Description  The following source units are imported but not referenced in the importing source unit:  code/contracts/contract/rewards/RocketMerkleDistributorMainnet.sol:L11  import \"../../interface/rewards/RocketSmoothingPoolInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L12-L18  import \"../../interface/minipool/RocketMinipoolManagerInterface.sol\";  import \"../../interface/minipool/RocketMinipoolQueueInterface.sol\";  import \"../../interface/node/RocketNodeStakingInterface.sol\";  import \"../../interface/util/AddressSetStorageInterface.sol\";  import \"../../interface/node/RocketNodeManagerInterface.sol\";  import \"../../interface/network/RocketNetworkPricesInterface.sol\";  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsMinipoolInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L8-L10  import \"../../types/MinipoolStatus.sol\";  import \"../../types/MinipoolDeposit.sol\";  import \"../../interface/dao/node/RocketDAONodeTrustedInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolBase.sol:L7-L8  import \"../../types/MinipoolDeposit.sol\";  import \"../../types/MinipoolStatus.sol\";  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L13-L14  import \"../../interface/network/RocketNetworkPricesInterface.sol\";  import \"../../interface/node/RocketNodeManagerInterface.sol\";  code/contracts/contract/node/RocketNodeManager.sol:L13  import \"../../interface/rewards/claims/RocketClaimNodeInterface.sol\";  code/contracts/contract/rewards/RocketClaimDAO.sol:L7  import \"../../interface/rewards/RocketRewardsPoolInterface.sol\";  Duplicate Import:  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L19-L20  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsNodeInterface.sol\";  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsNodeInterface.sol\";  The above list is exemplary, and there are likely more occurrences across the code base.  Recommendation  We recommend checking all imports and removing unused/unreferenced and unnecessary imports.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.18 RocketMinipool - Inconsistent access control modifier declaration onlyMinipoolOwner   ", "body": "  Resolution  Acknowledged by the client. Not addressed within rocket-pool/rocketpool@77d7cca  Agreed. This would change a lot of contracts just for a minor improvement in readbility.  Description  The access control modifier onlyMinipoolOwner should be renamed to onlyMinipoolOwnerOrWithdrawalAddress to be consistent with the actual check permitting the owner or the withdrawal address to interact with the function. This would also be consistent with other declarations in the codebase.  Example  The onlyMinipoolOwner modifier in RocketMinipoolBase is the same as onlyMinipoolOwnerOrWithdrawalAddress in other modules.  code/contracts/contract/minipool/RocketMinipoolBase.sol:L31-L37  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  code/contracts/contract/old/minipool/RocketMinipoolOld.sol:L21-L27  // Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  Other declarations:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L97-L107  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner(address _nodeAddress) {  require(_nodeAddress == nodeAddress, \"Invalid minipool owner\");  _;  /// @dev Only allow access from the owning node address or their withdrawal address  modifier onlyMinipoolOwnerOrWithdrawalAddress(address _nodeAddress) {  require(_nodeAddress == nodeAddress || _nodeAddress == rocketStorage.getNodeWithdrawalAddress(nodeAddress), \"Invalid minipool owner\");  _;  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L82-L92  // Only allow access from the owning node address  modifier onlyMinipoolOwner(address _nodeAddress) {  require(_nodeAddress == nodeAddress, \"Invalid minipool owner\");  _;  // Only allow access from the owning node address or their withdrawal address  modifier onlyMinipoolOwnerOrWithdrawalAddress(address _nodeAddress) {  require(_nodeAddress == nodeAddress || _nodeAddress == rocketStorage.getNodeWithdrawalAddress(nodeAddress), \"Invalid minipool owner\");  _;  Recommendation  We recommend renaming RocketMinipoolBase.onlyMinipoolOwner to RocketMinipoolBase.onlyMinipoolOwnerOrWithdrawalAddress.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.19 RocketDAO*Settings - settingNameSpace should be immutable   ", "body": "  Resolution  Acknowledged by the client. Not addressed within rocket-pool/rocketpool@77d7cca  Acknowledged. We can fix this as we upgrade the related contracts.  Description  The settingNameSpace in the abstract contract RocketDAONodeTrustedSettings is only set on contract deployment. Hence, the fields should be declared immutable to make clear that the settings namespace cannot change after construction.  Examples  RocketDAONodeTrustedSettings  code/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L13-L16  // The namespace for a particular group of settings  bytes32 settingNameSpace;  code/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L25-L30  // Construct  constructor(RocketStorageInterface _rocketStorageAddress, string memory _settingNameSpace) RocketBase(_rocketStorageAddress) {  // Apply the setting namespace  settingNameSpace = keccak256(abi.encodePacked(\"dao.trustednodes.setting.\", _settingNameSpace));  RocketDAOProtocolSettings  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L13-L14  // The namespace for a particular group of settings  bytes32 settingNameSpace;  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L25-L29  // Construct  constructor(RocketStorageInterface _rocketStorageAddress, string memory _settingNameSpace) RocketBase(_rocketStorageAddress) {  // Apply the setting namespace  settingNameSpace = keccak256(abi.encodePacked(\"dao.protocol.setting.\", _settingNameSpace));  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsAuction.sol:L13-L15  constructor(RocketStorageInterface _rocketStorageAddress) RocketDAOProtocolSettings(_rocketStorageAddress, \"auction\") {  // Set version  version = 1;  Recommendation  We recommend using the immutable annotation in Solidity (see Immutable).  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.20 Inefficiencies with the onlyMinipoolOwner modifier  ", "body": "  Resolution  Acknowledged by the client. No further actions.  Correct. This change would change every single contract we have and so the benefit does not outweigh the change.  Description  If a withdrawal address has not been set (or has been zeroed out), rocketStorage.getNodeWithdrawalAddress(nodeAddress) returns nodeAddress. This outcome leads to the modifier checking the same address twice (msg.sender == nodeAddress || msg.sender == nodeAddress):  code/contracts/contract/minipool/RocketMinipoolBase.sol:L31-L37  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  code/contracts/contract/RocketStorage.sol:L103-L111  // Get a node's withdrawal address  function getNodeWithdrawalAddress(address _nodeAddress) public override view returns (address) {  // If no withdrawal address has been set, return the nodes address  address withdrawalAddress = withdrawalAddresses[_nodeAddress];  if (withdrawalAddress == address(0)) {  return _nodeAddress;  return withdrawalAddress;  ", "labels": ["Consensys", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.21 RocketNodeDeposit - Duplicate check to avoid revert   ", "body": "  Resolution  Fixed with rocket-pool/rocketpool@3ab7af1 by introducing a new method maybeAssignDeposits() that does not revert by default but returns a boolean instead. This way, RocketNodeDeposit directly call the maybeAssignDeposits() function, avoiding the duplicate check.  This finding does not present a security-related problem in the code base, which is why we downgrade its severity to informational. However, we opted to keep this recommendation present in the report since it underlines a form of technical debt where old functionality is wrapped by new functionality using a workaround.  Description  When receiving and subsequently assigning deposits, the RocketNodeDeposit contract s assignDeposits function calls RocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled and skips the assignment of funds. This is done because the RocketDepositPool.assignDeposits function reverts if the setting is disabled:  code/contracts/contract/deposit/RocketDepositPool.sol:L207-L212  function assignDeposits() override external onlyThisLatestContract {  // Load contracts  RocketDAOProtocolSettingsDepositInterface rocketDAOProtocolSettingsDeposit = RocketDAOProtocolSettingsDepositInterface(getContractAddress(\"rocketDAOProtocolSettingsDeposit\"));  // Revert if assigning is disabled  require(_assignDeposits(rocketDAOProtocolSettingsDeposit), \"Deposit assignments are currently disabled\");  However, the underlying _assignDeposits function already performs a check for the setting and returns prematurely to avoid assignment.  code/contracts/contract/deposit/RocketDepositPool.sol:L217-L219  if (!_rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled()) {  return false;  The rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled() setting is checked twice. The first occurrence is in RocketNodeDeposit.assignDeposits and the second one in the same flow is contained in RocketDepositPool._assignDeposits. The second check is performed in a reverting fashion, thus requiring the top-level check in the RocketNodeDeposit contract to preemptively fetch and check the setting before continuing.  Recommendation  Since Rocketpool v1.2 already aims to perform an upgrade on the RocketDepositPool contract, we do recommend adding a separate, non-reverting version of the RocketDepositPool.assignDeposits function to the code base and removing the redundant preemptive check in RocketNodeDeposit.assignDeposits. This will improve readability and maintainability of future versions of the code, and save gas cost on deposit assignment operations.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.22 Inconsistent Coding Style  ", "body": "  Resolution  The client provided the following statement:  Acknolwedge your recommendation but we are dealing with an existing deployed codebase and if we change codestyle on only the contracts we update we will end up with a codebase with different code styles which is worse than one that is internally consistent but not consistent with best practice.  Description  Deviations from the Solidity Style Guide were identified throughout the codebase. Considering how much value a consistent coding style adds to the project s readability, enforcing a standard coding style with the help of linter tools is recommended.  Inconsistent Function naming scheme for external and internal interfaces  Throughout the codebase, private/internal functions are generally prefixed with an underscore (_<name>). This allows for an easy way to see if an external party can interact with a function without having to scan the declaration line for the corresponding visibility keywords. However, this naming scheme is not enforced consistently. Many internal function names are indistinguishable from external function names. It is therefore highly recommended to implement a consistent naming scheme and prefix internal functions with an underscore (_<name>).  code/contracts/contract/node/RocketNodeDeposit.sol:L268-L283  /// @dev Reverts if vacant minipools are not enabled  function checkVacantMinipoolsEnabled() private view {  // Get contracts  RocketDAOProtocolSettingsNodeInterface rocketDAOProtocolSettingsNode = RocketDAOProtocolSettingsNodeInterface(getContractAddress(\"rocketDAOProtocolSettingsNode\"));  // Check node settings  require(rocketDAOProtocolSettingsNode.getVacantMinipoolsEnabled(), \"Vacant minipools are currently disabled\");  /// @dev Executes an assignDeposits call on the deposit pool  function assignDeposits() private {  RocketDAOProtocolSettingsDepositInterface rocketDAOProtocolSettingsDeposit = RocketDAOProtocolSettingsDepositInterface(getContractAddress(\"rocketDAOProtocolSettingsDeposit\"));  if (rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled()) {  RocketDepositPoolInterface rocketDepositPool = RocketDepositPoolInterface(getContractAddress(\"rocketDepositPool\"));  rocketDepositPool.assignDeposits();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L339-L345  /// @dev Stakes the balance of this minipool into the deposit contract to set withdrawal credentials to this contract  /// @param _validatorSignature A signature over the deposit message object  /// @param _depositDataRoot The hash tree root of the deposit data object  function preStake(bytes calldata _validatorPubkey, bytes calldata _validatorSignature, bytes32 _depositDataRoot) internal {  // Load contracts  DepositInterface casperDeposit = DepositInterface(getContractAddress(\"casperDeposit\"));  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L651-L654  /// @dev Distributes the current contract balance based on capital ratio and node fee  function distributeSkimmedRewards() internal {  uint256 rewards = address(this).balance.sub(nodeRefundBalance);  uint256 nodeShare = calculateNodeRewards(nodeDepositBalance, getUserDepositBalance(), rewards);  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L661-L663  /// @dev Set the minipool's current status  /// @param _status The new status  function setStatus(MinipoolStatus _status) private {  code/contracts/contract/node/RocketNodeDeposit.sol:L202-L206  /// @dev Adds a minipool to the queue  function enqueueMinipool(address _minipoolAddress) private {  // Add minipool to queue  RocketMinipoolQueueInterface(getContractAddress(\"rocketMinipoolQueue\")).enqueueMinipool(_minipoolAddress);  code/contracts/contract/node/RocketNodeDeposit.sol:L208-L213  /// @dev Reverts if node operator has not initialised their fee distributor  function checkDistributorInitialised() private view {  // Check node has initialised their fee distributor  RocketNodeManagerInterface rocketNodeManager = RocketNodeManagerInterface(getContractAddress(\"rocketNodeManager\"));  require(rocketNodeManager.getFeeDistributorInitialised(msg.sender), \"Fee distributor not initialised\");  code/contracts/contract/node/RocketNodeDeposit.sol:L215-L218  /// @dev Creates a minipool and returns an instance of it  /// @param _salt The salt used to determine the minipools address  /// @param _expectedMinipoolAddress The expected minipool address. Reverts if not correct  function createMinipool(uint256 _salt, address _expectedMinipoolAddress) private returns (RocketMinipoolInterface) {  code/contracts/contract/auction/RocketAuctionManager.sol:L58-L60  function setLotCount(uint256 _amount) private {  setUint(keccak256(\"auction.lots.count\"), _amount);  ", "labels": ["Consensys", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.1 uint overflow may lead to stealing funds    Addressed", "body": "  Resolution  safeMath was added in SKALE-215. At the time of the writing this comment, the review has not been comprehensive to all arithmetic calculations in the scope.  Note that in some cases usage of safeMath due to reverts can result in unexpected halting of the system, that too should be reviewed again.  Description  It s possible to create a delegation with a very huge amount which may result in a lot of critically bad malicious usages:  code/contracts/delegation/DelegationRequestManager.sol:L74-L76  uint holderBalance = SkaleToken(contractManager.getContract(\"SkaleToken\")).balanceOf(holder);  uint lockedToDelegate = tokenState.getLockedCount(holder) - tokenState.getPurchasedAmount(holder);  require(holderBalance >= amount + lockedToDelegate, \"Delegator hasn't enough tokens to delegate\");  amount is passed by a user as a parameter, so if it s close to uint max value, amount + lockedToDelegate would overflow and this requirement would pass.  Having delegation with an almost infinite amount of tokens can lead to many various attacks on the system up to stealing funds and breaking everything.  Recommendation  Using SafeMath everywhere should prevent this and other similar issues. There should be more critical attacks caused by overflows/underflows, so SafeMath should be used everywhere in the codebase.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.2 Holders can burn locked funds    Addressed", "body": "  Resolution   Fixed in   SKALE-2144 by adding proper checks in  Description  Skale token is a modified ERC-777 that allows locking some part of the balance. Locking is checked during every transfer:  code/contracts/ERC777/LockableERC777.sol:L433-L441  // Property of the company SKALE Labs inc.---------------------------------  uint locked = _getLockedOf(from);  if (locked > 0) {  require(_balances[from] >= locked + amount, \"Token should be unlocked for transferring\");  //-------------------------------------------------------------------------  _balances[from] = _balances[from].sub(amount);  _balances[to] = _balances[to].add(amount);  But it s not checked during burn function and it s possible to  burn  locked tokens. Tokens will be burned, but locked amount will remain the same. That will result in having more locked tokens than the balance which may have very unpredictable behaviour.  Recommendation  Allow burning only unlocked tokens.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.3 Node can unlink validator    Addressed", "body": "  Resolution   Fixed in   SKALE-2145-unlink-node by adding a check in  Description  Validators can link a node address to them by calling linkNodeAddress function:  code/contracts/delegation/ValidatorService.sol:L109-L119  function linkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {  uint validatorId = getValidatorId(validatorAddress);  require(_validatorAddressToId[nodeAddress] == 0, \"Validator cannot override node address\");  _validatorAddressToId[nodeAddress] = validatorId;  function unlinkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {  uint validatorId = getValidatorId(validatorAddress);  require(_validatorAddressToId[nodeAddress] == validatorId, \"Validator hasn't permissions to unlink node\");  _validatorAddressToId[nodeAddress] = 0;  After that, the node has the same rights and is almost indistinguishable from the validator. So the node can even remove validator s address from _validatorAddressToId list and take over full control over validator. Additionally, the node can even remove itself by calling unlinkNodeAddress, leaving validator with no control at all forever.  Also, even without nodes, a validator can initially call unlinkNodeAddress to remove itself.  Recommendation  Linked nodes (and validator) should not be able to unlink validator s address from the _validatorAddressToId mapping.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.4 Unlocking funds after slashing    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  The initial funds can be unlocked if 51+% of them are delegated. However if any portion of the funds are slashed, the rest of the funds will not be unlocked at the end of the delegation period.  code/contracts/delegation/TokenState.sol:L258-L263  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Consider slashed tokens as delegated, or include them in the calculation for process to unlock in endingDelegatedToUnlocked  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.5 Bounties and fees should only be locked for the first 3 months    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  Bounties are currently locked for the first 3 months after delegation:  code/contracts/delegation/DelegationService.sol:L315  skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));  Instead, they should be locked for the first 3 months after the token launch.  Recommendation  It s better just to forbid any withdrawals for the first 3 months, no need to track it separately for every delegation. This recommendation is mainly to simplify the process.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.6 getLockedCount is iterating over all history of delegations    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  getLockedCount is iterating over all delegations of a specific holder and may even change the state of these delegations by calling getState.  code/contracts/delegation/TokenState.sol:L60-L71  function getLockedCount(address holder) external returns (uint amount) {  amount = 0;  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  uint[] memory delegationIds = delegationController.getDelegationsByHolder(holder);  for (uint i = 0; i < delegationIds.length; ++i) {  uint id = delegationIds[i];  if (isLocked(getState(id))) {  amount += delegationController.getDelegation(id).amount;  return amount + getPurchasedAmount(holder) + this.getSlashedAmount(holder);  This problem is major because delegations number is growing over time and may even potentially grow more than the gas limit and lock all tokens forever. getLockedCount is called during every transfer which makes any token transfer much more expensive than it should be.  Recommendation  Remove iterations over a potentially unlimited amount of tokens. All the necessary data can be precalculated before and getLockedCount function can have O(1) complexity.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.7 Tokens are unlocked only when delegation ends    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  After the first 3 months since at least 50% of tokens are delegated, all tokens should be unlocked. In practice, they are only unlocked if at least 50% of tokens, that were bought on the initial launch, are undelegated.  code/contracts/delegation/TokenState.sol:L258-L264  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Implement lock mechanism according to the legal requirement.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.8 Tokens after delegation should not be unlocked automatically    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  When some amount of tokens are delegated to a validator when the delegation period ends, these tokens are unlocked. However these tokens should be added to _purchased as they were in that state before their delegation.  code/contracts/delegation/TokenState.sol:L258-L264  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Tokens should only be unlocked if the main legal requirement (_totalDelegated[holder] >= _purchased[holder]) is satisfied, which in the above case this has not happened.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.9 Some unlocked tokens can become locked after delegation is rejected    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  When some amount of tokens are requested to be delegated to a validator, the validator can reject the request. The previous status of these tokens should be intact and not changed (locked or unlocked).  Here the initial status of tokens gets stored and it s either completely locked or unlocked:  code/contracts/delegation/TokenState.sol:L205-L214  if (_purchased[delegation.holder] > 0) {  _isPurchased[delegationId] = true;  if (_purchased[delegation.holder] > delegation.amount) {  _purchased[delegation.holder] -= delegation.amount;  } else {  _purchased[delegation.holder] = 0;  } else {  _isPurchased[delegationId] = false;  The problem is that if some amount of these tokens are locked at the time of the request and the rest tokens are unlocked, they will all be considered as locked after the delegation was rejected.  code/contracts/delegation/TokenState.sol:L272-L278  function _cancel(uint delegationId, DelegationController.Delegation memory delegation) internal returns (State state) {  if (_isPurchased[delegationId]) {  state = purchasedProposedToPurchased(delegationId, delegation);  } else {  state = proposedToUnlocked(delegationId);  Recommendation  Don t change the status of the rejected tokens.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.10 Gas limit for bounty and slashing distribution    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  After every bounty payment (should be once per month) to a validator, the bounty is distributed to all delegators. In order to do that, there is a for loop that iterates over all active delegators and sends their bounty to SkaleBalances contract:  code/contracts/delegation/DelegationService.sol:L310-L316  for (uint i = 0; i < shares.length; ++i) {  skaleToken.send(address(skaleBalances), shares[i].amount, abi.encode(shares[i].holder));  uint created = delegationController.getDelegation(shares[i].delegationId).created;  uint delegationStarted = timeHelpers.getNextMonthStartFromDate(created);  skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));  There are also few more loops over all the active delegators. This leads to a huge gas cost of distribution mechanism. A number of active delegators that can be processed before hitting the gas limit is limited and not big enough.  The same issue is with slashing:  code/contracts/delegation/DelegationService.sol:L95-L106  function slash(uint validatorId, uint amount) external allow(\"SkaleDKG\") {  ValidatorService validatorService = ValidatorService(contractManager.getContract(\"ValidatorService\"));  require(validatorService.validatorExists(validatorId), \"Validator does not exist\");  Distributor distributor = Distributor(contractManager.getContract(\"Distributor\"));  TokenState tokenState = TokenState(contractManager.getContract(\"TokenState\"));  Distributor.Share[] memory shares = distributor.distributePenalties(validatorId, amount);  for (uint i = 0; i < shares.length; ++i) {  tokenState.slash(shares[i].delegationId, shares[i].amount);  Recommendation  The best solution would require major changes to the codebase, but would eventually make it simpler and safer. Instead of distributing and centrally calculating bounty for each delegator during one call it s better to just store all the necessary values, so delegator would be able to calculate the bounty on withdrawal. Amongst the necessary values, there should be history of total delegated amounts per validator during each bounty payment and history of all delegations with durations of their active state.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.11 ERC-777 callback issue    Partially fixed", "body": "  Resolution  SKALE-2153 to  skalenetwork/skale-manager#128  This report raises this as an unfixed minor issue. This issue will be fixed if the upgrade capability for _getAndUpdateLockedAmount() is revoked by SKALE network governance in the future.  Description  ERC-777 token comes with callback functions to the receiver and the sender on every token transfer. This gives re-entrancy opportunities for everyone who s using this token. There is a chance that other systems might not handle ERC-777 correctly.  Examples  Uniswap reentrancy critical bug: https://medium.com/consensys-diligence/uniswap-audit-b90335ac007  Recommendation  Use ERC-20 standard or remove callback function calls.  Remove callback function usage from the system and replace them with a standard ERC-20 flow:  code/contracts/delegation/SkaleBalances.sol:L55-L68  function tokensReceived(  address operator,  address from,  address to,  uint256 amount,  bytes calldata userData,  bytes calldata operatorData  external  allow(\"SkaleToken\")  address recipient = abi.decode(userData, (address));  stashBalance(recipient, amount);  code/contracts/delegation/DelegationService.sol:L275-L289  function tokensReceived(  address operator,  address from,  address to,  uint256 amount,  bytes calldata userData,  bytes calldata operatorData  external  allow(\"SkaleToken\")  require(userData.length == 32, \"Data length is incorrect\");  uint validatorId = abi.decode(userData, (uint));  distributeBounty(amount, validatorId);  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.12 Rename functions    Addressed", "body": "  Resolution   Fixed in   SKALE-2154-naming by renaming the functions. The functions that are not solely getters and update the state of the smart contract are renamed to have  Description  The naming of the functions should reflect their nature, such as functions starting with  get  should be only getters and do not change state. This will result in confusion developments and the implicit state changes might not be noticed.  Other than getters, some other function or variable names are misleading.  Examples  The following functions are a few examples that are named as getters but they change the state.  getState -> updateState  getDelegationsTotal getDelegationsForValidator getDelegationsByHolder  Some other naming that does not reflect the nature of the functionality:  getPurchasedAmount -> getPurchasedUnlocked  tokenState.Sold -> lock  Recommendation  For functions that get and update variables use getAndUpdate naming. Similarly use variable names that reflect the nature of the values they store.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.13 Delegations might stuck in non-active validator   Pending", "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions.  Description  If a validator does not get enough funds to run a node (MSR - Minimum staking requirement), all token holders that delegated tokens to the validator cannot switch to a different validator, and might result in funds getting stuck with the nonfunctioning validator for up to 12 months.  Example  code/contracts/delegation/ValidatorService.sol:L166  require((validatorNodes.length + 1) * msr <= delegationsTotal, \"Validator has to meet Minimum Staking Requirement\");  Recommendation  Allow token holders to withdraw delegation earlier if the validator didn t get enough funds for running nodes.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.14 Disabled Validators still have delegated funds   Pending", "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions.  Description  The owner of ValidatorService contract can enable and disable validators. The issue is that when a validator is disabled, it still has its delegations, and delegated funds will be locked until the end of their delegation period (up to 12 months).  code/contracts/delegation/ValidatorService.sol:L84-L90  function enableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {  trustedValidators[validatorId] = true;  function disableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {  trustedValidators[validatorId] = false;  Recommendation  It might make sense to release all delegations and stop validator s nodes if it s not trusted anymore. However, the rationale behind disabling the validators might be different that what we think, in any case there should be a way to handle this scenario, where the validator is disabled but there are funds delegated to it.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.15 Fees can be > 100%    Addressed", "body": "  Resolution   Added a check to prevent fee rates equal or higher than 100% in   SKALE-2157-fee-check.  Description  A validator can be created with feeRate > 1000 which would mean that the fee rate would be higher than 100%. Severity is not high because that validator will most likely be not whitelisted.  Also, 100%+ fees would still somehow work and not revert because of the absence of SafeMath.  Recommendation  Add sanity check for the input values in registerValidator, and do not allow adding a validator with a fee rate higher than 100%.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.16 getState changes state implicitly    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  getState function is checking and changing the state of a delegation struct. This function is called in many places in the codebase. Every delegation has a lot of different possible states and all of them are changed implicitly during other transactions, which makes it hard to track the logic in the code and make future changes in the code close to impossible without breaking some functionalities.  Recommendation  The general suggestion would be to minimize the number of implicit storage changes. Many states can be either changed explicitly or be calculated without additional storage changes.  As an option, it s possible to get rid of state storage slot at all. startDate and endDate fields may set the current state:  initProposed can be called during the creation of the proposal.  no need to explicitly change states between ACCEPTED and DELEGATED, you can set the start date on acceptance and no further changes are required.  no need to switch states between DELEGATED and ENDING_DELEGATED, when delegation is set to end, it s fine to just have end_date storage slot and make assign the date there when undelegate function is called.  unlocking funds from delegation (or not accepted request) can be explicit.  Also see issue 5.19 for other suggestions regarding getState usage in the code  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.17 _endingDelegations list is redundant    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  _endingDelegations is a list of delegations that is created for optimisation purposes. But the only place it s used is in getPurchasedAmount function, so only a subset of all delegations is going to be updated.  code/contracts/delegation/TokenState.sol:L159-L164  function getPurchasedAmount(address holder) public returns (uint amount) {  // check if any delegation was ended  for (uint i = 0; i < _endingDelegations[holder].length; ++i) {  getState(_endingDelegations[holder][i]);  return _purchased[holder];  But getPurchasedAmount function is mostly used after iterating over all delegations of the holder.  Recommendation  Remove _endingDelegations and switch to a mechanism that does not require looping through delegations list of potentially unlimited size.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.18 Some functions are defined but not implemented    Addressed", "body": "  Resolution   Fixed by removing the empty functions and implementing some others in   SKALE-2160. At the time of the writing this comment, the review has not been comprehensive to all functions in the scope.  Description  There are many functions that are defined but not implemented. They have a revert with a message as not implemented.  This results in complex code and reduces readability. Here is a some of these functions within the scope of this audit:  DelegationService.setMinimumStakingRequirement()  DelegationService.getAllDelegationRequests()  DelegationService.getDelegationRequestsForValidator()  DelegationService.listDelegationRequests()  DelegationService.getDelegationRequestsForValidator() Many more functions in DelegationService.sol  Examples  code/contracts/delegation/DelegationService.sol:L152-L158  function getAllDelegationRequests() external returns(uint[] memory) {  revert(\"Not implemented\");  function getDelegationRequestsForValidator(uint validatorId) external returns (uint[] memory) {  revert(\"Not implemented\");  Recommendation  If these functions are needed for this release, they must be implemented. If they are for future plan, it s better to remove the extra code in the smart contracts.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.19 tokenState.setState redundant checks    Addressed", "body": "  Resolution   Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  tokenState.setState is used to change the state of the token from:  PROPOSED to ACCEPTED (in accept())  DELEGATED to ENDING_DELEGATED (in requestUndelegation()  The if/else statement in setState is too complicated and can be simplified, both to optimize gas usage and to increase readability.  Examples  code/contracts/delegation/TokenState.sol:L173-L197  function setState(uint delegationId, State newState) internal {  TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  require(newState != State.PROPOSED, \"Can't set state to proposed\");  if (newState == State.ACCEPTED) {  State currentState = getState(delegationId);  require(currentState == State.PROPOSED, \"Can't set state to accepted\");  _state[delegationId] = State.ACCEPTED;  _timelimit[delegationId] = timeHelpers.getNextMonthStart();  } else if (newState == State.DELEGATED) {  revert(\"Can't set state to delegated\");  } else if (newState == State.ENDING_DELEGATED) {  require(getState(delegationId) == State.DELEGATED, \"Can't set state to ending delegated\");  DelegationController.Delegation memory delegation = delegationController.getDelegation(delegationId);  _state[delegationId] = State.ENDING_DELEGATED;  _timelimit[delegationId] = timeHelpers.calculateDelegationEndTime(delegation.created, delegation.delegationPeriod, 3);  _endingDelegations[delegation.holder].push(delegationId);  } else {  revert(\"Unknown state\");  Recommendation  Some of the changes that do not change the functionality of the setState function:  Remove reverts() and add the valid states to the require() at the beginning of the function  Remove multiple calls to getState()  Remove final else/revert as this is an internal function and States passed should be valid More optimization can be done which requires further understanding of the system and the state machine.  function setState(uint delegationId, State newState) internal {  TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  require(newState != State.PROPOSED || newState != State.DELEGATED, \"Invalid state change\");  State currentState = getState(delegationId);  if (newState == State.ACCEPTED) {  require(currentState == State.PROPOSED, \"Can't set state to accepted\");  _state[delegationId] = State.ACCEPTED;  _timelimit[delegationId] = timeHelpers.getNextMonthStart();  } else if (newState == State.ENDING_DELEGATED) {  require(currentState == State.DELEGATED, \"Can't set state to ending delegated\");  DelegationController.Delegation memory delegation = delegationController.getDelegation(delegationId);  _state[delegationId] = State.ENDING_DELEGATED;  _timelimit[delegationId] = timeHelpers.calculateDelegationEndTime(delegation.created, delegation.delegationPeriod, 3);  _endingDelegations[delegation.holder].push(delegationId);  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.20 Validator should be able to remove delegator    Addressed", "body": "  Resolution   Code added in   SKALE-2162, If the delegation is not in  Description  In order to delegate tokens to a validator, the validator should accept the delegation request, however it s not possible to remove the delegator for the next period.  Recommendation  For consistency, either allow a validator to undelegate delegators for the next period or remove acceptance mechanism if it s not needed.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.21 Lack of logs and events on state changes   Pending", "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions, but given the minor level and need to begin remediation, it s left out of scope from the re-mediation tag.  Description  Events in Solidity are used to log major state changes in the system, as for tracebility and also trigger UI changes or user notifications. It is a good practice to use events for every value storage change to be able to trace back the system.  Recommendation  emit events whenever a state change happens. As an example slashing does not emit any events and cannot notify a user unless a service is polling the system state regularly.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.22 DelegationService redundancy    Addressed", "body": "  Resolution  pull/114 and the functionality is distributed in  Description  DelegationService acts as a gateway for every external call. The problem is that it adds extra complexity to the code, which makes it harder to read and add a new code. Also, it costs more gas because of extra calls between contracts.  Recommendation  The same functionality of DelegationService can be added through UI to allow direct calls to each contract. However, as the whole system is modular and upgradable, it is understandable why using one main contract as the point of interaction might make sense as well.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.23 Add timelock for some onlyOwner functions   Pending", "body": "  Resolution  Skale team acknowledged this issue and gave us the following response:  The SKALE Network Upgrade key will soon transition to an on-chain voting mechanism therefore making the ownership a function of community governance. It will be centrally managed through a multi-sig process for the initial 3 months to prioritize agility for resolving critical issues prior to becoming a community owned on-chain function. Successful Ethereum projects such as Maker have given clear data points on successful voting mechanism and community control which the SKALE Network will employ as soon as possible.  Description  The system is trusted in a way that there are some owners have the power to do major changes in the system. The most powerful is owner of ContractManager which can update any contract in any way. Even though the system is trusted and this is intended behaviour, it s possible to mitigate this trust a bit.  Recommendation  Add timelock to major admin functions, so people would know about it beforehand (2 weeks before) and would be able to react somehow.  Severity is minor because if owners of SKALE would want to attack the system in that way, tokens would lose the value anyway, and security of SKALE chains would be unreliable. So it s unclear what can be done even having that knowledge beforehand.  6 Mitigation issues  This section lists the issues found in the mitigation phase. The audit team, reviewed the code fixes after the initial report was delivered,  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.1 Users can burn delegated tokens using re-entrancy attack    Addressed", "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#128  Description  When a user burns tokens, the following code is called:  new_code/contracts/ERC777/LockableERC777.sol:L413-L426  uint locked = _getAndUpdateLockedAmount(from);  if (locked > 0) {  require(_balances[from] >= locked.add(amount), \"Token should be unlocked for burning\");  //-------------------------------------------------------------------------  _callTokensToSend(  operator, from, address(0), amount, data, operatorData  );  // Update state variables  _totalSupply = _totalSupply.sub(amount);  _balances[from] = _balances[from].sub(amount);  There is a callback function right after the check that there are enough unlocked tokens to burn. In this callback, the user can delegate all the tokens right before burning them without breaking the code flow.  Recommendation  _callTokensToSend  should be called before checking for the unlocked amount of tokens, which is better defined as Checks-Effects-Interactions Pattern.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.2 Rounding errors after slashing    Addressed", "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#130.  Description  When slashing happens _delegatedToValidator and _effectiveDelegatedToValidator values are reduced.  new_code/contracts/delegation/DelegationController.sol:L349-L355  function confiscate(uint validatorId, uint amount) external {  uint currentMonth = getCurrentMonth();  Fraction memory coefficient = reduce(_delegatedToValidator[validatorId], amount, currentMonth);  reduce(_effectiveDelegatedToValidator[validatorId], coefficient, currentMonth);  putToSlashingLog(_slashesOfValidator[validatorId], coefficient, currentMonth);  _slashes.push(SlashingEvent({reducingCoefficient: coefficient, validatorId: validatorId, month: currentMonth}));  When holders process slashings, they reduce _delegatedByHolderToValidator, _delegatedByHolder, _effectiveDelegatedByHolderToValidator values.  new_code/contracts/delegation/DelegationController.sol:L892-L904  if (oldValue > 0) {  reduce(  _delegatedByHolderToValidator[holder][validatorId],  _delegatedByHolder[holder],  _slashes[index].reducingCoefficient,  month);  reduce(  _effectiveDelegatedByHolderToValidator[holder][validatorId],  _slashes[index].reducingCoefficient,  month);  slashingSignals[index.sub(begin)].holder = holder;  slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId, month));  Also when holders are undelegating, they are calculating how many tokens from delegations[delegationId].amount were slashed.  new_code/contracts/delegation/DelegationController.sol:L316  uint amountAfterSlashing = calculateDelegationAmountAfterSlashing(delegationId);  If rounding error reduces amount not that much as other values, we can have uint underflow. This is especially dangerous because all calculations are delayed and we will know about underflow and SafeMath revert in the next month or later. Developers already made sure that rounding errors are aligned in a correct way, and that the reduced value should always be larger than the subtracted, so there should not be underflow. This solution is very unstable because it s hard to verify it and keep in mind even during a small code change.  If rounding errors make amount smaller then it should be, when other values should be zero (for example, when all the delegations are undelegated), these values will become some very small values. The problem here is that it would be impossible to compare values to zero.  Recommendation  Consider not calling revert on these subtractions and make result value be equals to zero if underflow happens.  Consider comparing to some small epsilon value instead of zero. Or similar to the previous point, on every subtraction check if the value is smaller then epsilon, and make it zero if it is.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.3 Slashes do not affect bounty distribution    Addressed", "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#118  Description  When slashes are processed by a holder, only _delegatedByHolderToValidator and _delegatedByHolder values are reduced. But _effectiveDelegatedByHolderToValidator value remains the same. This value is used to distribute bounties amongst delegators. So slashing will not affect that distribution.  contracts/delegation/DelegationController.sol:L863-L873  uint oldValue = getAndUpdateDelegatedByHolderToValidator(holder, validatorId);  if (oldValue > 0) {  uint month = _slashes[index].month;  reduce(  _delegatedByHolderToValidator[holder][validatorId],  _delegatedByHolder[holder],  _slashes[index].reducingCoefficient,  month);  slashingSignals[index.sub(begin)].holder = holder;  slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId));  Recommendation  Reduce _effectiveDelegatedByHolderToValidator and _effectiveDelegatedToValidator when slashes are processed.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.4 Iterations over slashes    Addressed", "body": "  Resolution   Partially mitigated in   skalenetwork/skale-manager#163 .  Description  Every user should iterate over each slash (but only once) and process them in order to determine whether this slash impacted his delegations or not.  However, the check is done during almost every action that the user does because it updates the current state of the user s balance. The downside of this method is that if there are a lot of slashes in the system, every user would be forced to iterate over all of them even if the user is only trading tokens and only calls transfer function.  If the number of slashes is huge, checking them all in one function would impossible due to the block gas limit. It s possible to call the checking function separately and process slashes in batches. So this attack should not result in system halt and can be mitigated with manual intervention.  Also, there are two separate pipelines for iterating over slashes. One pipeline is for iterating over months to determine amount of slashed tokens in separate delegations. This one can potentially hit gas limit in many-many years. The other one is for modifying aggregated delegation values.  Recommendation  Try to avoid all the unnecessary iterations over a potentially unlimited number of items. Additionally, it s possible to optimize some calculations:  When slashing signals are processed, all of them always have the same holder. There s no reason for having an array of signals with the same holder (always with predefined length and values will most likely be zero). It seems possible to remove signals functionality and just aggregate the changes for the Punisher.  Try merge two pipelines into one.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.5 Storage operations optimization    Addressed", "body": "  Resolution   Mitigated in   skalenetwork/skale-manager#179  Description  There are a lot of operations that write some value to the storage (uses SSTORE opcode) without actually changing it.  Examples  In getAndUpdateValue  function of DelegationController and TokenLaunchLocker:  new_code/contracts/delegation/DelegationController.sol:L711-L715  for (uint i = sequence.firstUnprocessedMonth; i <= month; ++i) {  sequence.value = sequence.value.add(sequence.addDiff[i]).sub(sequence.subtractDiff[i]);  delete sequence.addDiff[i];  delete sequence.subtractDiff[i];  In handleSlash function of Punisher contract amount will be zero in most cases:  new_code/contracts/delegation/Punisher.sol:L66-L68  function handleSlash(address holder, uint amount) external allow(\"DelegationController\") {  _locked[holder] = _locked[holder].add(amount);  Recommendation  Check if the value is the same and don t write it to the storage in that case.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.6 Duplicate function implementation addMonths()    Addressed", "body": "  Resolution   Fixed in   skalenetwork/skale-manager#127  Description  TimeHelpers.addMonths() implementation is redundant as it can directly use BokkyPooBahsDateTimeLibrary.addMonths() function.  Recommendation  Simply use return BokkyPooBahsDateTimeLibrary.addMonths() on the same function to prevent further code changes, it s still a good idea to call addMonth through TimeHelpers contract.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.7 Function overloading    Addressed", "body": "  Resolution   Fixed in   skalenetwork/skale-manager#181  Description  Some functions in the codebase are overloaded. That makes code less readable and increases the probability of missing bugs.  For example, there are a lot of reduce function implementations in DelegationController:  new_code/contracts/delegation/DelegationController.sol:L722-L820  function reduce(PartialDifferencesValue storage sequence, uint amount, uint month) internal returns (Fraction memory) {  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  if (sequence.firstUnprocessedMonth == 0) {  return createFraction(0);  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return createFraction(0);  uint _amount = amount;  if (value < amount) {  _amount = value;  Fraction memory reducingCoefficient = createFraction(value.sub(_amount), value);  reduce(sequence, reducingCoefficient, month);  return reducingCoefficient;  function reduce(PartialDifferencesValue storage sequence, Fraction memory reducingCoefficient, uint month) internal {  reduce(  sequence,  sequence,  reducingCoefficient,  month,  false);  function reduce(  PartialDifferencesValue storage sequence,  PartialDifferencesValue storage sumSequence,  Fraction memory reducingCoefficient,  uint month) internal  reduce(  sequence,  sumSequence,  reducingCoefficient,  month,  true);  function reduce(  PartialDifferencesValue storage sequence,  PartialDifferencesValue storage sumSequence,  Fraction memory reducingCoefficient,  uint month,  bool hasSumSequence) internal  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  if (hasSumSequence) {  require(month.add(1) >= sumSequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  require(reducingCoefficient.numerator <= reducingCoefficient.denominator, \"Increasing of values is not implemented\");  if (sequence.firstUnprocessedMonth == 0) {  return;  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return;  uint newValue = sequence.value.mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  if (hasSumSequence) {  subtract(sumSequence, sequence.value.sub(newValue), month);  sequence.value = newValue;  for (uint i = month.add(1); i <= sequence.lastChangedMonth; ++i) {  uint newDiff = sequence.subtractDiff[i].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  if (hasSumSequence) {  sumSequence.subtractDiff[i] = sumSequence.subtractDiff[i].sub(sequence.subtractDiff[i].sub(newDiff));  sequence.subtractDiff[i] = newDiff;  function reduce(  PartialDifferences storage sequence,  Fraction memory reducingCoefficient,  uint month) internal  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  require(reducingCoefficient.numerator <= reducingCoefficient.denominator, \"Increasing of values is not implemented\");  if (sequence.firstUnprocessedMonth == 0) {  return;  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return;  sequence.value[month] = sequence.value[month].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  for (uint i = month.add(1); i <= sequence.lastChangedMonth; ++i) {  sequence.subtractDiff[i] = sequence.subtractDiff[i].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  Recommendation  Avoid function overloading as a general guideline.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.1 Exchange - CancelOrder has no effect   Pending", "body": "  Resolution  This issue has been addressed with mai-protocol-v2/2fcbf4b44f4595e5879ff5efea4e42c529ef0ce1 by verifying that an order has not been cancelled in method validateOrderParam.  cancelOrder still does not verify the order signature.  Description  The exchange provides means for the trader or broker to cancel the order. The cancelOrder method, however, only stores the hash of the canceled order in mapping but the mapping is never checked. It is therefore effectively impossible for a trader to cancel an order.  Examples  code/contracts/exchange/Exchange.sol:L179-L187  function cancelOrder(LibOrder.Order memory order) public {  require(msg.sender == order.trader || msg.sender == order.broker, \"invalid caller\");  bytes32 orderHash = order.getOrderHash();  cancelled[orderHash] = true;  emit Cancel(orderHash);  Recommendation  matchOrders* or validateOrderParam should check if cancelled[orderHash] == true and abort fulfilling the order.  Verify the order params (Signature) before accepting it as canceled.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.2 AMM - funding can be called in emergency mode   Pending", "body": "  Resolution   This issue was addressed by silently skipping   Description  specification for  Recommendation  According to the specification, forceFunding should not be allowed in EMERGENCY mode. However, it is assumed that this method should only be callable in NORMAL mode.  The assessment team would like to note that the specification appears to be inconsistent and dated (method names, variable names, \u2026).  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.3 Perpetual - withdraw should only be available in NORMAL state   Pending", "body": "  Resolution   This issue was resolved by requiring   Description  According to the specification withdraw can only be called in NORMAL state. However, the implementation allows it to be called in NORMAL and SETTLED mode.  Examples  Withdraw only checks for !SETTLING state which resolves to NORMAL and SETTLED.  code/contracts/perpetual/Perpetual.sol:L175-L178  function withdraw(uint256 amount) public {  withdrawFromAccount(msg.sender, amount);  code/contracts/perpetual/Perpetual.sol:L156-L169  function withdrawFromAccount(address payable guy, uint256 amount) private {  require(guy != address(0), \"invalid guy\");  require(status != LibTypes.Status.SETTLING, \"wrong perpetual status\");  uint256 currentMarkPrice = markPrice();  require(isSafeWithPrice(guy, currentMarkPrice), \"unsafe before withdraw\");  remargin(guy, currentMarkPrice);  address broker = currentBroker(guy);  bool forced = broker == address(amm.perpetualProxy()) || broker == address(0);  withdraw(guy, amount, forced);  require(isSafeWithPrice(guy, currentMarkPrice), \"unsafe after withdraw\");  require(availableMarginWithPrice(guy, currentMarkPrice) >= 0, \"withdraw margin\");  In contrast, withdrawFor requires the state to be NORMAL:  code/contracts/perpetual/Perpetual.sol:L171-L174  function withdrawFor(address payable guy, uint256 amount) public onlyWhitelisted {  require(status == LibTypes.Status.NORMAL, \"wrong perpetual status\");  withdrawFromAccount(guy, amount);  Recommendation  withdraw should only be available in the NORMAL operation mode.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.4 Perpetual - withdrawFromInsuranceFund should check wadAmount instead of rawAmount   Pending", "body": "  Resolution   This issue was addressed by checking   Description  withdrawFromInsurance checks that enough funds are in the insurance fund before allowing withdrawal by an admin by checking the provided rawAmount <= insuranceFundBalance.toUint256(). rawAmount is the ETH (18 digit precision) or collateral token amount (can be less than 18 digit precision) to be withdrawn while insuranceFundBalance is a WAD-denominated value (18 digit precision).  The check does not hold if the configured collateral has different precision and may have unwanted consequences, e.g. the withdrawal of more funds than expected.  Note: there is another check for insuranceFundBalance staying positive after the potential external call to collateral.  Examples  code/contracts/perpetual/Perpetual.sol:L204-L216  function withdrawFromInsuranceFund(uint256 rawAmount) public onlyWhitelistAdmin {  require(rawAmount > 0, \"invalid amount\");  require(insuranceFundBalance > 0, \"insufficient funds\");  require(rawAmount <= insuranceFundBalance.toUint256(), \"insufficient funds\");  int256 wadAmount = toWad(rawAmount);  insuranceFundBalance = insuranceFundBalance.sub(wadAmount);  withdrawFromProtocol(msg.sender, rawAmount);  require(insuranceFundBalance >= 0, \"negtive insurance fund\");  emit UpdateInsuranceFund(insuranceFundBalance);  When looking at the test-cases there seems to be a misconception about what unit of amount withdrawFromInsuranceFund is taking. For example, the insurance fund withdrawal and deposit are not tested for collateral that specifies a precision that is not 18. The test-cases falsely assume that the input to withdrawFromInsuranceFund is a WAD value, while it is taking the collateral s rawAmount which is then converted to a WAD number.  code/test/test_perpetual.js:L471-L473  await perpetual.withdrawFromInsuranceFund(toWad(10.111));  fund = await perpetual.insuranceFundBalance();  assert.equal(fund.toString(), 0);  Recommendation  Check that require(wadAmount <= insuranceFundBalance.toUint256(), \"insufficient funds\");, add a test-suite testing the insurance fund with collaterals with different precision and update existing tests that properly provide the expected input to withdraFromInsurance.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.5 Perpetual - liquidateFrom should not have public visibility   Pending", "body": "  Resolution   This issue has been resolved by removing the   Description  Perpetual.liquidate is used to liquidate an account that is  unsafe,  determined by the relative sizes of marginBalanceWithPrice and maintenanceMarginWithPrice:  code/contracts/perpetual/Perpetual.sol:L248-L253  // safe for liquidation  function isSafeWithPrice(address guy, uint256 currentMarkPrice) public returns (bool) {  return  marginBalanceWithPrice(guy, currentMarkPrice) >=  maintenanceMarginWithPrice(guy, currentMarkPrice).toInt256();  Perpetual.liquidate allows the caller to assume the liquidated account s position, as well as a small amount of  penalty collateral.  The steps to liquidate are, roughly:  Close the liquidated account s position  Perform a trade on the liquidated assets with the liquidator acting as counter-party  Grant the liquidator a portion of the liquidated assets as a reward. An additional portion is added to the insurance fund.  Handle any losses  We found several issues in Perpetual.liquidate:  Examples  liquidateFrom has public visibility:  code/contracts/perpetual/Perpetual.sol:L270  function liquidateFrom(address from, address guy, uint256 maxAmount) public returns (uint256, uint256) {  Given that liquidate only calls liquidateFrom after checking the current contract s status, this oversight allows anyone to call liquidateFrom during the SETTLED stage:  code/contracts/perpetual/Perpetual.sol:L291-L294  function liquidate(address guy, uint256 maxAmount) public returns (uint256, uint256) {  require(status != LibTypes.Status.SETTLED, \"wrong perpetual status\");  return liquidateFrom(msg.sender, guy, maxAmount);  Additionally, directly calling liquidateFrom allows anyone to liquidate on behalf of other users, forcing other accounts to assume liquidated positions.  Finally, neither liquidate nor liquidateFrom check that the liquidated account and liquidator are the same. Though the liquidation accounting process is hard to follow, we believe this is unintended and could lead to large errors in internal contract accounting.  Recommendation  Make liquidateFrom an internal function  In liquidate or liquidateFrom, check that msg.sender != guy  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.6 Unpredictable behavior due to front running or general bad timing   Pending", "body": "  Resolution  This issue was addressed by the client providing the following statement:  Not fixed in the perpetual. But later a voting system will take over the administration key. We intent to add a waiting period before voted changes applying.  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.  Some instances of this are more important than others, but in general users of the system should have assurances about the behavior of the action they re about to take.  Examples  Updating governance and global configuration parameters are not protected by a time-lock and take effect immediately. This, therefore, creates an opportunity for administrators to front-run users on the exchange by changing parameters for orders. It may also allow an administrator to temporarily lift restrictions for themselves (e.g. withdrawalLockBlockCount).  GlobalConfig  withdrawalLockBlockCount is queried when applying for withdrawal. This value can be set zero enabling allowing immediate withdrawal. brokerLockBlockCount is queried when setting a new broker. This value can e set to zero effectively enabling immediate broker changes.  code/contracts/global/GlobalConfig.sol:L18-L27  function setGlobalParameter(bytes32 key, uint256 value) public onlyWhitelistAdmin {  if (key == \"withdrawalLockBlockCount\") {  withdrawalLockBlockCount = value;  } else if (key == \"brokerLockBlockCount\") {  brokerLockBlockCount = value;  } else {  revert(\"key not exists\");  emit UpdateGlobalParameter(key, value);  PerpetualGovernance  e.g. Admin can front-run specific matchOrder calls and set arbitrary dev fees or curve parameters\u2026  code/contracts/perpetual/PerpetualGovernance.sol:L39-L80  function setGovernanceParameter(bytes32 key, int256 value) public onlyWhitelistAdmin {  if (key == \"initialMarginRate\") {  governance.initialMarginRate = value.toUint256();  require(governance.initialMarginRate > 0, \"require im > 0\");  require(governance.initialMarginRate < 10**18, \"require im < 1\");  require(governance.maintenanceMarginRate < governance.initialMarginRate, \"require mm < im\");  } else if (key == \"maintenanceMarginRate\") {  governance.maintenanceMarginRate = value.toUint256();  require(governance.maintenanceMarginRate > 0, \"require mm > 0\");  require(governance.maintenanceMarginRate < governance.initialMarginRate, \"require mm < im\");  require(governance.liquidationPenaltyRate < governance.maintenanceMarginRate, \"require lpr < mm\");  require(governance.penaltyFundRate < governance.maintenanceMarginRate, \"require pfr < mm\");  } else if (key == \"liquidationPenaltyRate\") {  governance.liquidationPenaltyRate = value.toUint256();  require(governance.liquidationPenaltyRate < governance.maintenanceMarginRate, \"require lpr < mm\");  } else if (key == \"penaltyFundRate\") {  governance.penaltyFundRate = value.toUint256();  require(governance.penaltyFundRate < governance.maintenanceMarginRate, \"require pfr < mm\");  } else if (key == \"takerDevFeeRate\") {  governance.takerDevFeeRate = value;  } else if (key == \"makerDevFeeRate\") {  governance.makerDevFeeRate = value;  } else if (key == \"lotSize\") {  require(  governance.tradingLotSize == 0 || governance.tradingLotSize.mod(value.toUint256()) == 0,  \"require tls % ls == 0\"  );  governance.lotSize = value.toUint256();  } else if (key == \"tradingLotSize\") {  require(governance.lotSize == 0 || value.toUint256().mod(governance.lotSize) == 0, \"require tls % ls == 0\");  governance.tradingLotSize = value.toUint256();  } else if (key == \"longSocialLossPerContracts\") {  require(status == LibTypes.Status.SETTLING, \"wrong perpetual status\");  socialLossPerContracts[uint256(LibTypes.Side.LONG)] = value;  } else if (key == \"shortSocialLossPerContracts\") {  require(status == LibTypes.Status.SETTLING, \"wrong perpetual status\");  socialLossPerContracts[uint256(LibTypes.Side.SHORT)] = value;  } else {  revert(\"key not exists\");  emit UpdateGovernanceParameter(key, value);  Admin can set devAddress or even update to a new amm and globalConfig  code/contracts/perpetual/PerpetualGovernance.sol:L82-L94  function setGovernanceAddress(bytes32 key, address value) public onlyWhitelistAdmin {  require(value != address(0x0), \"invalid address\");  if (key == \"dev\") {  devAddress = value;  } else if (key == \"amm\") {  amm = IAMM(value);  } else if (key == \"globalConfig\") {  globalConfig = IGlobalConfig(value);  } else {  revert(\"key not exists\");  emit UpdateGovernanceAddress(key, value);  AMMGovernance  code/contracts/liquidity/AMMGovernance.sol:L22-L43  function setGovernanceParameter(bytes32 key, int256 value) public onlyWhitelistAdmin {  if (key == \"poolFeeRate\") {  governance.poolFeeRate = value.toUint256();  } else if (key == \"poolDevFeeRate\") {  governance.poolDevFeeRate = value.toUint256();  } else if (key == \"emaAlpha\") {  require(value > 0, \"alpha should be > 0\");  governance.emaAlpha = value;  emaAlpha2 = 10**18 - governance.emaAlpha;  emaAlpha2Ln = emaAlpha2.wln();  } else if (key == \"updatePremiumPrize\") {  governance.updatePremiumPrize = value.toUint256();  } else if (key == \"markPremiumLimit\") {  governance.markPremiumLimit = value;  } else if (key == \"fundingDampener\") {  governance.fundingDampener = value;  } else {  revert(\"key not exists\");  emit UpdateGovernanceParameter(key, value);  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all updates to system parameters or upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  Additionally, users should verify the whitelist setup before using the contract system and monitor it for new additions to the whitelist. Documentation should clearly outline what roles are owned by whom to support suitability. Sane parameter bounds should be enforced (e.g. min. disallow block delay of zero )  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.7 AMM - Governance is able to set an invalid alpha value   Pending", "body": "  Resolution   This issue was addressed by checking that the provided   Description  According to https://en.wikipedia.org/wiki/Moving_average  The coefficient \u03b1 represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. A higher \u03b1 discounts older observations faster.  However, the code does not check upper bounds. An admin may, therefore, set an invalid alpha that puts emaAlpha2 out of bounds or negative.  Examples  code/contracts/liquidity/AMMGovernance.sol:L27-L31  } else if (key == \"emaAlpha\") {  require(value > 0, \"alpha should be > 0\");  governance.emaAlpha = value;  emaAlpha2 = 10**18 - governance.emaAlpha;  emaAlpha2Ln = emaAlpha2.wln();  Recommendation  Ensure that the system configuration is always within safe bounds. Document expected system variable types and their safe operating ranges. Enforce that bounds are checked every time a value is set. Enforce safe defaults when deploying contracts.  Ensure emaAlpha is 0 < value < 1 WAD  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.8 AMM - Amount of collateral spent or shares received may be unpredictable for liquidity provider   ", "body": "  Resolution  The client acknowledges this issue without providing further information or implementing the recommended fixes.  Description  When providing liquidity with addLiquidity(), the amount of collateral required is based on the current price and the amount of shares received depends on the total amount of shares in circulation. This price can fluctuate at a moment s notice, making the behavior of the function unpredictable for the user.  The same is true when removing liquidity via removeLiquidity().  Recommendation  Unpredictability can be introduced by someone front-running the transaction, or simply by poor timing. For example, adjustments to global variable configuration by the system admin will directly impact subsequent actions by the user. In order to ensure users know what to expect:  Allow the caller to specify a price limit or maximum amount of collateral to be spent  Allow the caller to specify the minimum amount of shares expected to be received  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.9 Exchange - insufficient input validation in matchOrders   Pending", "body": "  Resolution   This issue was addressed by following the recommendation to verify that   Description  Additionally, the method allows the sender to provide no makerOrderParams at all, resulting in no state changes.  matchOrders also does not reject trades with an amount set to zero. Such orders should be rejected because they do not comply with the minimum tradingLotSize configured for the system. As a side-effect, events may be emitted for zero-amount trades and unexpected state changes may occur.  Examples  code/contracts/exchange/Exchange.sol:L34-L39  function matchOrders(  LibOrder.OrderParam memory takerOrderParam,  LibOrder.OrderParam[] memory makerOrderParams,  address _perpetual,  uint256[] memory amounts  ) public {  code/contracts/exchange/Exchange.sol:L113-L113  function matchOrderWithAMM(LibOrder.OrderParam memory takerOrderParam, address _perpetual, uint256 amount) public {  Recommendation  Require makerOrderParams.length > 0 && amounts.length == makerOrderParams.length  Require that amount or any of the amounts[i] provided to matchOrders is >=tradingLotSize.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.10 AMM - Liquidity provider may lose up to lotSize when removing liquidity   ", "body": "  Resolution  The client acknowledges this issue without providing further information.  Description  When removing liquidity, the amount of collateral received is calculated from the shareAmount (ShareToken) of the liquidity provider. The liquidity removal process registers a trade on the amount, with the liquidity provider and AMM taking opposite sides. Because trading only accepts multiple of the lotSize, the leftover is discarded. The amount discarded may be up to lotSize - 1.  The expectation is that this value should not be too high, but as lotSize can be set to arbitrary values by an admin, it is possible that this step discards significant value. Additionally, see issue 6.6 for how this can be exploited by an admin.  Note that similar behavior is present in Perpetual.liquidateFrom, where the liquidatableAmount calculated undergoes a similar modulo operation:  code/contracts/perpetual/Perpetual.sol:L277-L278  uint256 liquidatableAmount = totalPositionSize.sub(totalPositionSize.mod(governance.lotSize));  liquidationAmount = liquidationAmount.ceil(governance.lotSize).min(maxAmount).min(liquidatableAmount);  Examples  lotSize can arbitrarily be set up to pos_int256_max as long as tradingLotSize % lotSize == 0  code/contracts/perpetual/PerpetualGovernance.sol:L61-L69  } else if (key == \"lotSize\") {  require(  governance.tradingLotSize == 0 || governance.tradingLotSize.mod(value.toUint256()) == 0,  \"require tls % ls == 0\"  );  governance.lotSize = value.toUint256();  } else if (key == \"tradingLotSize\") {  require(governance.lotSize == 0 || value.toUint256().mod(governance.lotSize) == 0, \"require tls % ls == 0\");  governance.tradingLotSize = value.toUint256();  amount is derived from shareAmount rounded down to the next multiple of the lotSize. The leftover is discarded.  code/contracts/liquidity/AMM.sol:L289-L294  uint256 amount = shareAmount.wmul(oldPoolPositionSize).wdiv(shareToken.totalSupply());  amount = amount.sub(amount.mod(perpetualProxy.lotSize()));  perpetualProxy.transferBalanceOut(trader, price.wmul(amount).mul(2));  burnShareTokenFrom(trader, shareAmount);  uint256 opened = perpetualProxy.trade(trader, LibTypes.Side.LONG, price, amount);  Recommendation  Ensure that documentation makes users aware of the fact that they may lose up to lotsize-1 in value.  Alternatively, track accrued value and permit trades on values that exceed lotSize. Note that this may add significant complexity.  Ensure that similar system behavior, like the liquidatableAmount calculated in Perpetual.liquidateFrom, is also documented and communicated clearly to users.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.11 Oracle - Unchecked oracle response timestamp and integer over/underflow    ", "body": "  Resolution  This issue was resolved by following the recommendations,  using LibMath for arithmetic operations to guard against over/underflows,  checking that newPrice != 0  verifying that the timestamp is within a configurable range,  duplicating the code and combining the reverse oracle into one contract  The assessment team would like to note that the acceptable time-frame for answers can vary, the price may be outdated, and it is totally up to the deployer to configure the acceptable timeout. The timeout can be changed by the account deploying the oracle feed without a delay allowing the price-feed owner to arbitrarily make calls to AMM.indexPrice fail (front-running). A timeout may be set to an arbitrarily high value to bypass the check. User s of the system are advised to validate that they trust the account operating the feeder and that the timeout is set correctly.  Description  The external Chainlink oracle, which provides index price information to the system, introduces risk inherent to any dependency on third-party data sources. For example, the oracle could fall behind or otherwise fail to be maintained, resulting in outdated data being fed to the index price calculations of the AMM. Oracle reliance has historically resulted in crippled on-chain systems, and complications that lead to these outcomes can arise from things as simple as network congestion.  Ensuring that unexpected oracle return values are properly handled will reduce reliance on off-chain components and increase the resiliency of the smart contract system that depends on them.  Examples  The ChainlinkAdapter and InversedChainlinkAdapter take the oracle s (int256) latestAnswer and convert the result using chainlinkDecimalsAdapter. This arithmetic operation can underflow/overflow if the Oracle provides a large enough answer:  code/contracts/oracle/ChainlinkAdapter.sol:L10-L19  int256 public constant chainlinkDecimalsAdapter = 10**10;  constructor(address _feeder) public {  feeder = IChainlinkFeeder(_feeder);  function price() public view returns (uint256 newPrice, uint256 timestamp) {  newPrice = (feeder.latestAnswer() * chainlinkDecimalsAdapter).toUint256();  timestamp = feeder.latestTimestamp();  code/contracts/oracle/InversedChainlinkAdapter.sol:L11-L20  int256 public constant chainlinkDecimalsAdapter = 10**10;  constructor(address _feeder) public {  feeder = IChainlinkFeeder(_feeder);  function price() public view returns (uint256 newPrice, uint256 timestamp) {  newPrice = ONE.wdiv(feeder.latestAnswer() * chainlinkDecimalsAdapter).toUint256();  timestamp = feeder.latestTimestamp();  The oracle provides a timestamp for the latestAnswer that is not validated and may lead to old oracle timestamps being accepted (e.g. caused by congestion on the blockchain or a directed censorship attack).  code/contracts/oracle/InversedChainlinkAdapter.sol:L19-L20  timestamp = feeder.latestTimestamp();  Recommendation  Use SafeMath for mathematical computations  Verify latestAnswer is within valid bounds (!=0)  Verify latestTimestamp is within accepted bounds (not in the future, was updated within a reasonable amount of time)  Deduplicate code by combining both Adapters into one as the only difference is that the InversedChainlinkAdapter returns ONE.wdiv(price).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.12 AMM - Liquidity pools can be initialized with zero collateral   Pending", "body": "  Resolution  This issue was addressed by checking that amount > 0. The assessment team would like to note that;  The client chose to verify that amount is non-zero when calling createPool instead of requiring a minimum of a lotSize.  The client did not address the issues about removeLiquidity and addLiquidity allowing to remove and add zero liquidity.  Description  createPool can be initialized with amount == 0. Because a subsequent call to initFunding can only happen once, the contract is now initialized with a zero size pool that does not allow any liquidity to be added.  Trying to recover by calling createPool again fails as the funding state is already initialized. The specification also states the following about createPool:  Open asset pool by deposit to AMM. Only available when pool is empty.  This is inaccurate, as createPool can only be called once due to a check in initFunding, but this call may leave the pool empty.  Furthermore, the contract s liquidity management functionality (addLiquidity and removeLiquidity) allows adding zero liquidity (amount == 0) and removing zero shares (shareAmount == 0). As these actions do not change the liquidity of the pool, they should be rejected.  Recommendation  Require a minimum amount lotSize to be provided when creating a Pool and adding liquidity via addLiquidity  Require a minimum amount of shares to be provided when removing liquidity via removeLiquidity  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.13 Perpetual - Administrators can put the system into emergency mode indefinitely   Pending", "body": "  Resolution  The client provided the following statement addressing the issue:  It should be solved by voting. Moreover, we add two roles who is able to disable withdrawing /pause the system.  The duration of the emergency phase is still unrestricted.  Description  There is no limitation on how long an administrator can put the Perpetual contract into emergency mode. Users cannot trade or withdraw funds in emergency mode and are effectively locked out until the admin chooses to put the contract in SETTLED mode.  Examples  code/contracts/perpetual/PerpetualGovernance.sol:L96-L101  function beginGlobalSettlement(uint256 price) public onlyWhitelistAdmin {  require(status != LibTypes.Status.SETTLED, \"already settled\");  settlementPrice = price;  status = LibTypes.Status.SETTLING;  emit BeginGlobalSettlement(price);  code/contracts/perpetual/Perpetual.sol:L146-L154  function endGlobalSettlement() public onlyWhitelistAdmin {  require(status == LibTypes.Status.SETTLING, \"wrong perpetual status\");  address guy = address(amm.perpetualProxy());  settleFor(guy);  status = LibTypes.Status.SETTLED;  emit EndGlobalSettlement();  Recommendation  Set a time-lock when entering emergency mode that allows anyone to set the system to SETTLED after a fixed amount of time.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.14 Signed data may be usable cross-chain    ", "body": "  Resolution   This issue was addressed by adding the   order data and verifying it as part of  Description  Signed order data may be re-usable cross-chain as the chain-id is not explicitly part of the signed data.  Examples  The signed order data currently includes the EIP712 Domain Name Mai Protocol and the following information:  code/contracts/lib/LibOrder.sol:L23-L48  struct Order {  address trader;  address broker;  address perpetual;  uint256 amount;  uint256 price;  /**  Data contains the following values packed into 32 bytes  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557  \u2551                    \u2502 length(bytes)   desc                                      \u2551  \u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562  \u2551 version            \u2502 1               order version                             \u2551  \u2551 side               \u2502 1               0: buy (long), 1: sell (short)            \u2551  \u2551 isMarketOrder      \u2502 1               0: limitOrder, 1: marketOrder             \u2551  \u2551 expiredAt          \u2502 5               order expiration time in seconds          \u2551  \u2551 asMakerFeeRate     \u2502 2               maker fee rate (base 100,000)             \u2551  \u2551 asTakerFeeRate     \u2502 2               taker fee rate (base 100,000)             \u2551  \u2551 (d) makerRebateRate\u2502 2               rebate rate for maker (base 100)          \u2551  \u2551 salt               \u2502 8               salt                                      \u2551  \u2551 isMakerOnly        \u2502 1               is maker only                             \u2551  \u2551 isInversed         \u2502 1               is inversed contract                      \u2551  \u2551                    \u2502 8               reserved                                  \u2551  \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d  /  bytes32 data;  Signature verification:  code/contracts/lib/LibSignature.sol:L24-L47  function isValidSignature(OrderSignature memory signature, bytes32 hash, address signerAddress)  internal  pure  returns (bool)  uint8 method = uint8(signature.config[1]);  address recovered;  uint8 v = uint8(signature.config[0]);  if (method == uint8(SignatureMethod.ETH_SIGN)) {  recovered = ecrecover(  keccak256(abi.encodePacked(\"\\x19Ethereum Signed Message:\\n32\", hash)),  v,  signature.r,  signature.s  );  } else if (method == uint8(SignatureMethod.EIP712)) {  recovered = ecrecover(hash, v, signature.r, signature.s);  } else {  revert(\"invalid sign method\");  return signerAddress == recovered;  Recommendation  Include the chain-id in the signature to avoid cross-chain validity of signatures  verify s is within valid bounds to avoid signature malleability  if (uint256(s) > 0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0) {  revert(\"ECDSA: invalid signature 's' value\");  verify v is within valid bounds  if (v != 27 && v != 28) {  revert(\"ECDSA: invalid signature 'v' value\");  return invalid if the result of ecrecover() is 0x0  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.15 Exchange - validateOrderParam does not check against SUPPORTED_ORDER_VERSION    ", "body": "  Resolution   This issue was resolved by checking against   Description  validateOrderParam verifies the signature and version of a provided order. Instead of checking against the contract constant SUPPORTED_ORDER_VERSION it, however, checks against a hardcoded version 2 in the method itself.  This might be a problem if SUPPORTED_ORDER_VERSION is seen as the configuration parameter for the allowed version. Changing it would not change the allowed order version for validateOrderParam as this constant literal is never used.  At the time of this audit, however, the SUPPORTED_ORDER_VERSION value equals the hardcoded value in the validateOrderParam method.  Examples  code/contracts/exchange/Exchange.sol:L155-L170  function validateOrderParam(IPerpetual perpetual, LibOrder.OrderParam memory orderParam)  internal  view  returns (bytes32)  address broker = perpetual.currentBroker(orderParam.trader);  require(broker == msg.sender, \"invalid broker\");  require(orderParam.getOrderVersion() == 2, \"unsupported version\");  require(orderParam.getExpiredAt() >= block.timestamp, \"order expired\");  bytes32 orderHash = orderParam.getOrderHash(address(perpetual), broker);  require(orderParam.signature.isValidSignature(orderHash, orderParam.trader), \"invalid signature\");  require(filled[orderHash] < orderParam.amount, \"fullfilled order\");  return orderHash;  Recommendation  Check against SUPPORTED_ORDER_VERSION instead of the hardcoded value 2.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.16 LibMathSigned - wpowi returns an invalid result for a negative exponent   Pending", "body": "  Resolution   This issue was addressed by requiring that   here). The method is still lacking proper natspec documentation outlining expected argument types and valid ranges. The client chose not to implement a check to detect the case where a user accidentally provides  Description  LibMathSigned.wpowi(x,n) calculates Wad value x (base) to the power of n (exponent). The exponent is declared as a signed int, however, the method returns wrong results when calculating x ^(-n).  The comment for the wpowi method suggests that n is a normal integer instead of a Wad-denominated value. This, however, is not being enforced.  Examples  LibMathSigned.wpowi(8000000000000000000, 2) = 64000000000000000000  (wrong) LibMathSigned.wpowi(8000000000000000000, -2) = 64000000000000000000  code/contracts/lib/LibMath.sol:L103-L116  // x ^ n  // NOTE: n is a normal integer, do not shift 18 decimals  // solium-disable-next-line security/no-assign-params  function wpowi(int256 x, int256 n) internal pure returns (int256 z) {  z = n % 2 != 0 ? x : _WAD;  for (n /= 2; n != 0; n /= 2) {  x = wmul(x, x);  if (n % 2 != 0) {  z = wmul(z, x);  Recommendation  Make wpowi support negative exponents or use the proper type for n (uint) and reject negative values.  Enforce that the exponent bounds are within sane ranges and less than a Wad to detect potential misuse where someone accidentally provides a Wad value as n.  Add positive and negative unit-tests to fully cover this functionality.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.17 Outdated solidity version and floating pragma   Pending", "body": "  Resolution   This issue was addressed by removing the floating pragma and fixing the compiler version to v0.5.15. The assessment team would like to note, that the latest   ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "0.5.17 with 0.5.16 addressing an ABIEncoder issue.", "body": "  Description  Using an outdated compiler version can be problematic especially if there are publicly disclosed bugs and issues (see also https://github.com/ethereum/solidity/releases) that affect the current compiler version.  The codebase specifies a floating version of ^0.5.2 and makes use of the experimental feature ABIEncoderV2.  It should be noted, that ABIEncoderV2 was subject to multiple bug-fixes up until the latest 0.6.xversion and contracts compiled with earlier versions are - for example - susceptible to the following issues:  ImplicitConstructorCallvalueCheck  TupleAssignmentMultiStackSlotComponents  MemoryArrayCreationOverflow  privateCanBeOverridden  YulOptimizerRedundantAssignmentBreakContinue0.5  ABIEncoderV2CalldataStructsWithStaticallySizedAndDynamicallyEncodedMembers  SignedArrayStorageCopy  ABIEncoderV2StorageArrayWithMultiSlotElement  DynamicConstructorArgumentsClippedABIV2  Examples  Codebase declares compiler version ^0.5.2:  code/contracts/liquidity/AMM.sol:L1-L2  pragma solidity ^0.5.2;  pragma experimental ABIEncoderV2; // to enable structure-type parameters  According to etherscan.io, the currently deployed main-net AMM contract is compiled with solidity version 0.5.8:  https://etherscan.io/address/0xb95B9fb0539Ec84DeD2855Ed1C9C686Af9A4e8b3#code  Recommendation  It is recommended to settle on the latest stable 0.6.x or 0.5.x version of the Solidity compiler and lock the pragma version to a specifically tested compiler release.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.18 AMM - ONE_WAD_U is never used    ", "body": "  Resolution   This issue is resolved by removing   Description  The const ONE_WAD_U is declared but never used. Avoid re-declaring the same constants in multiple source-units (and unit-test cases) as this will be hard to maintain.  Examples  code/contracts/liquidity/AMM.sol:L17-L17  uint256 private constant ONE_WAD_U = 10**18;  Recommendation  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.19 Perpetual - Variable shadowing in constructor    ", "body": "  Resolution  This issue was addressed by following the recommendation.  Description  Perpetual inherits from PerpetualGovernance and Collateral, which declare state variables that are shadowed in the Perpetual constructor.  Examples  Local constructor argument shadows PerpetualGovernance.globalConfig, PerpetualGovernance.devAddress, Collateral.collateral  Note: Confusing name: Collateral is an inherited contract and a state variable.  code/contracts/perpetual/Perpetual.sol:L34-L41  constructor(address globalConfig, address devAddress, address collateral, uint256 collateralDecimals)  public  Position(collateral, collateralDecimals)  setGovernanceAddress(\"globalConfig\", globalConfig);  setGovernanceAddress(\"dev\", devAddress);  emit CreatePerpetual();  Recommendation  Rename the parameter or state variable.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.20 Perpetual - The specified decimals for the collateral may not reflect the token s actual decimals   ", "body": "  Resolution  The client acknowledges this issue without providing further information.  Description  When initializing the Perpetual contract, the deployer can decide to use either ETH, or an ERC20-compliant collateral. In the latter case, the deployer must provide a nonzero address for the token, as well as the number of decimals used by the token:  code/contracts/perpetual/Collateral.sol:L28-L34  constructor(address _collateral, uint256 decimals) public {  require(decimals <= MAX_DECIMALS, \"decimals out of range\");  require(_collateral != address(0x0) || (_collateral == address(0x0) && decimals == 18), \"invalid decimals\");  collateral = _collateral;  scaler = (decimals == MAX_DECIMALS ? 1 : 10**(MAX_DECIMALS - decimals)).toInt256();  The provided decimals value is not checked for validity and can differ from the actual token s decimals.  Recommendation  Ensure to establish documentation that makes users aware of the fact that the decimals configured are not enforced to match the actual tokens decimals. This is to allow users to audit the system configuration and decide whether they want to participate in it.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.21 AMM - Unchecked return value in ShareToken.mint   Pending", "body": "  Resolution   This issue was addressed by adding checks to   Description  ShareToken is an extension of the Openzeppelin ERC20Mintable pattern which exposes a method called mint() that allows accounts owning the minter role to mint new tokens. The return value of ShareToken.mint() is not checked.  Since the ERC20 standard does not define whether this method should return a value or revert it may be problematic to assume that all tokens revert. If, for example, an implementation is used that does not revert on error but returns a boolean error indicator instead the caller might falsely continue without the token minted.  We would like to note that the functionality is intended to be used with the provided ShareToken and therefore the contract is safe to use assuming ERC20Mintable.mint reverts on error. The issue arises if the system is used with a different ShareToken implementation that is not implemented in the same way.  Examples  Openzeppelin implementation  function mint(address account, uint256 amount) public onlyMinter returns (bool) {  _mint(account, amount);  return true;  Call with unchecked return value  code/contracts/liquidity/AMM.sol:L499-L502  function mintShareTokenTo(address guy, uint256 amount) internal {  shareToken.mint(guy, amount);  Recommendation  Consider wrapping the mint statement in a require clause, however, this way only tokens that are returning a boolean error indicator are supported. Document the specification requirements for the ShareToken and clearly state if the token is expected to revert or return an error indicator.  It should also be documented that the Token exposes a burn method that does not adhere to the Openzeppelin ERC20Burnable implementation. The ERC20Burnable import is unused as noted in issue 6.23.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.22 Perpetual - beginGlobalSettlement can be called multiple times   ", "body": "  Resolution  The client addressed this issue with the following statement:  Acknowledged. It sill can be call multiple times to correct the settlement price. Voting and pausemay improve the situation.When pause, no liquidation which may leading to losing position happens event in theemergency mode.  beginGlobalSettlement can still be called multiple times.  Description  The system can be put into emergency mode by an admin calling beginGlobalSettlement and providing a fixed settlementPrice. The method can be invoked even when the contract is already in SETTLING (emergency) mode, allowing an admin to selectively adjust the settlement price again. This does not seem to be the intended behavior as calling the method again re-sets the status to SETTLING. Furthermore, it may affect users  behavior during the SETTLING phase.  Examples  code/contracts/perpetual/PerpetualGovernance.sol:L96-L101  function beginGlobalSettlement(uint256 price) public onlyWhitelistAdmin {  require(status != LibTypes.Status.SETTLED, \"already settled\");  settlementPrice = price;  status = LibTypes.Status.SETTLING;  emit BeginGlobalSettlement(price);  Recommendation  Emergency mode should only be allowed to be set once  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.23 Unused Imports    ", "body": "  Resolution  This issue was addressed by removing the listed imports.  Description  The following source units are imported but not referenced in the contract:  Examples  code/contracts/perpetual/Perpetual.sol:L4-L5  import \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";  import \"@openzeppelin/contracts/token/ERC20/SafeERC20.sol\";  code/contracts/perpetual/Perpetual.sol:L14-L15  import \"../interface/IPriceFeeder.sol\";  import \"../interface/IGlobalConfig.sol\";  code/contracts/token/ShareToken.sol:L5-L5  import \"@openzeppelin/contracts/token/ERC20/ERC20Burnable.sol\";  code/contracts/token/ShareToken.sol:L3-L3  import \"@openzeppelin/contracts/token/ERC20/ERC20.sol\";  Recommendation  Check all imports and remove all unused/unreferenced and unnecessary imports.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.24 Exchange - OrderStatus is never used    ", "body": "  Resolution  This issue was resolved by removing the unused code.  Description  The enum OrderStatus is declared but never used.  Examples  code/contracts/exchange/Exchange.sol:L20-L20  enum OrderStatus {EXPIRED, CANCELLED, FILLABLE, FULLY_FILLED}  Recommendation  Remove unused code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.25 LibMath - Inaccurate declaration of _UINT256_MAX    ", "body": "  Resolution   This issue was addressed by renaming   Description  LibMathUnsigned declares _UINT256_MAX as 2^255-1 while this value actually represents _INT256_MAX. This appears to just be a naming issue.  Examples  (UINT256_MAX/2-1 => pos INT256_MAX; 2**256/2-1==2**255-1)  code/contracts/lib/LibMath.sol:L228-L230  library LibMathUnsigned {  uint256 private constant _WAD = 10**18;  uint256 private constant _UINT256_MAX = 2**255 - 1;  Recommendation  Rename _UINT256_MAX to _INT256MAX or _SIGNED_INT256MAX.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.26 LibMath - inconsistent assertion text and improve representation of literals with many digits   ", "body": "  Resolution  The client acknowledges this issue without providing further information.  Description  The assertion below states that logE only accepts v <= 1e22 * 1e18 while the argument name is x. In addition to that we suggest representing large literals in scientific notation.  Examples  code/contracts/lib/LibMath.sol:L153-L157  function wln(int256 x) internal pure returns (int256) {  require(x > 0, \"logE of negative number\");  require(x <= 10000000000000000000000000000000000000000, \"logE only accepts v <= 1e22 * 1e18\"); // in order to prevent using safe-math  int256 r = 0;  uint8 extra_digits = longer_digits - fixed_digits;  Recommendation  Update the inconsistent assertion text v -> x and represent large literals in scientific notation as they are otherwise difficult to read and review.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.27 LibMath - roundHalfUp returns unfinished result    ", "body": "  Resolution  This issue was addressed by adding the following comment to the function signature. Please note that the code documentation does not adhere to the natspec format.  // ROUND_HALF_UP rule helper. You have to call roundHalfUp(x, y) / y to finish the rounding operation  There is still the residual risk that someone might miss the comment and wrongly assume that the method finishes rounding. This is, however, accepted by the client.  Description  It is assumed that the final rounding step is not executed for performance reasons. However, this might easily introduce errors when the caller assumes the result is rounded for base while it is not.  Examples  roundHalfUp(-4700, 1000) = -4700 instead of 5000  roundHalfUp(4700, 1000) = 4700 instead of 5000  code/contracts/lib/LibMath.sol:L126-L133  // ROUND_HALF_UP rule helper. 0.5 \u2248 1, 0.4 \u2248 0, -0.5 \u2248 -1, -0.4 \u2248 0  function roundHalfUp(int256 x, int256 y) internal pure returns (int256) {  require(y > 0, \"roundHalfUp only supports y > 0\");  if (x >= 0) {  return add(x, y / 2);  return sub(x, y / 2);  Recommendation  We have verified the current code-base and the callers for roundHalfUp are correctly finishing the rounding step. However, it is recommended to finish the rounding within the method or document this behavior to prevent errors caused by code that falsely assumes that the returned value finished rounding.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.28 LibMath/LibOrder - unused named return value    ", "body": "  Resolution  This issue was resolved by either returning a named value or using the return statement.  Description  The following methods declare a named return value but explicitly return a value instead. The named return value is not used.  LibMathSigned.min()  LibMathSigned.max()  LibMathUnsigned.min()  LibMathUnsigned.max()  LibOrder.getOrderHash()  LibOrder.hashOrder()  Examples  code/contracts/lib/LibMath.sol:L90-L96  function min(int256 x, int256 y) internal pure returns (int256 z) {  return x <= y ? x : y;  function max(int256 x, int256 y) internal pure returns (int256 z) {  return x >= y ? x : y;  code/contracts/lib/LibMath.sol:L285-L292  function min(uint256 x, uint256 y) internal pure returns (uint256 z) {  return x <= y ? x : y;  function max(uint256 x, uint256 y) internal pure returns (uint256 z) {  return x >= y ? x : y;  code/contracts/lib/LibOrder.sol:L68-L71  function getOrderHash(Order memory order) internal pure returns (bytes32 orderHash) {  orderHash = LibEIP712.hashEIP712Message(hashOrder(order));  return orderHash;  code/contracts/lib/LibOrder.sol:L86-L97  function hashOrder(Order memory order) internal pure returns (bytes32 result) {  bytes32 orderType = EIP712_ORDER_TYPE;  // solium-disable-next-line security/no-inline-assembly  assembly {  let start := sub(order, 32)  let tmp := mload(start)  mstore(start, orderType)  result := keccak256(start, 224)  mstore(start, tmp)  return result;  Recommendation  Remove the named return value and explicitly return the value.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "6.29 Where possible, a specific contract type should be used rather than address   Pending", "body": "  Resolution   This issue was partially addressed by changing some state variable declarations from   Description  Rather than storing addresses and then casting to the known contract type, it s better to use the best type available so the compiler can check for type safety.  Examples  Collateral. collateral is of type address, but it could be type IERC20 instead. Not only would this give a little more type safety when deploying new modules, but it would avoid repeated casts throughout the codebase of the form IERC20(collateral), IPerpetual(_perpetual) and others. The following is an incomplete list of examples:  declare collateral as IERC20  code/contracts/perpetual/Collateral.sol:L19-L19  address public collateral;  code/contracts/perpetual/Collateral.sol:L51-L51  IERC20(collateral).safeTransferFrom(guy, address(this), rawAmount);  declare argument perpetual as IPerpetual  code/contracts/exchange/Exchange.sol:L34-L42  function matchOrders(  LibOrder.OrderParam memory takerOrderParam,  LibOrder.OrderParam[] memory makerOrderParams,  address _perpetual,  uint256[] memory amounts  ) public {  require(!takerOrderParam.isMakerOnly(), \"taker order is maker only\");  IPerpetual perpetual = IPerpetual(_perpetual);  declare argument feeder as IChainlinkFeeder  code/contracts/oracle/ChainlinkAdapter.sol:L12-L14  constructor(address _feeder) public {  feeder = IChainlinkFeeder(_feeder);  Remediation  Where possible, use more specific types instead of address. This goes for parameter types as well as state variable types.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/05/mcdex-mai-protocol-v2/"}, {"title": "5.1 Oracle updates can be manipulated to perform atomic front-running attack    Addressed", "body": "  Resolution  The issue was mitigated by updating the Oracle price only once per block and consistently only using the old value throughout the block instead of querying the Oracle when adding or removing liquidity. Arbitrageurs can now no longer do the profitable trade within a single transaction which also precludes the possibility of using flash loans to amplify the attack.  Description  It is possible to atomically arbitrage rate changes in a risk-free way by  sandwiching  the Oracle update between two transactions. The attacker would send the following 2 transactions at the moment the Oracle update appears in the mempool:  The first transaction, which is sent with a higher gas price than the Oracle update transaction, converts a very small amount. This  locks in  the conversion weights for the block since handleExternalRateChange() only updates weights once per block. By doing this, the arbitrageur ensures that the stale Oracle price is initially used when doing the first conversion in the following transaction.  The second transaction, which is sent at a slightly lower gas price than the transaction that updates the Oracle, does the following:  Perform a large conversion at the old weight;  Add a small amount of Liquidity to trigger rebalancing;  Convert back at the new rate.  The attacker can also leverage the incentive generated by the formula by converting such that primary reserve balance == primary reserve staked balance.  The attacker can obtain liquidity for step 2 using a flash loan. The attack will deplete the reserves of the pool. An example is shown in section 5.4.  Recommendation  Do not allow users to trade at a stale Oracle rate and trigger an Oracle price update in the same transaction.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.2 Slippage and fees can be manipulated by a trader    Addressed", "body": "  Resolution  The issue was addressed by introducing an exit fee mechanism. When a liquidity provider wants to withdraw some liquidity, the smart contract returns fewer tokens if the primary reserve is not in the balanced state. So in most cases, the manipulations described in the issue should potentially be non-profitable anymore. Although, in some cases, the traders still may have some incentive to add liquidity before making the trade and remove it after to get a part of the fees (i.e., if the pool is going to be in a balanced state after the trade).  Description  Users are making trades against the liquidity pool (converter) with slippage and fees defined in the converter contract and Bancor formula. The following steps can be done to optimize trading costs:  Instead of just making a trade, a user can add a lot of liquidity (of both tokens, or only one of them) to the pool after taking a flash loan, for example.  Make the trade.  Remove the added liquidity.  Because the liquidity is increased on the first step, slippage is getting smaller for this trade. Additionally, the trader receives a part of the fees for this trade by providing liquidity.  One of the reasons why this is possible is described in another issue issue 5.3.  This technique of reducing slippage could be used by the trader to get more profit from any frontrunning/arbitrage opportunity and can help to deplete the reserves.  Example  Consider the initial state with an amplification factor of 20 and zero fees:  Here a user can make a trade with the following rate:  > Convert 9000000 TKN into 8612440 BNT.  But if the user adds 100% of the liquidity in both tokens before the trade, the slippage will be lower:  > Convert 9000000 TKN into 8801955 BNT.  Recommendation  Fixing this issue requires some modification of the algorithm.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.3 Loss of the liquidity pool is not equally distributed    Addressed", "body": "  Resolution  The issue was addressed by adding a new fee mechanism called  adjusted fees . This mechanism aims to decrease the deficit of the reserves over time. If there is a deficit of reserves, it is usually present on the secondary token side, because there is a strong incentive to bring the primary token to the balanced state. Roughly speaking, the idea is that if the secondary token has a deficit in reserves, there are additional fees for trading that token. These fees are not distributed across the liquidity providers like the regular fees. Instead, they are just populating the reserve, decreasing the existing deficit.  Loss is still not distributed across the liquidity providers, and there is a possibility that there are not enough funds for everyone to withdraw them. In the case of a run on reserves, LPs will be able to withdraw funds on a first-come-first-serve basis.  Description  All stakeholders in the liquidity pool should be able to withdraw the same amount as they staked plus a share of fees that the converter earned during their staking period.  code/contracts/converter/LiquidityPoolV2Converter.sol:L491-L505  IPoolTokensContainer(anchor).burn(_poolToken, msg.sender, _amount);  // calculate how much liquidity to remove  // if the entire supply is liquidated, the entire staked amount should be sent, otherwise  // the price is based on the ratio between the pool token supply and the staked balance  uint256 reserveAmount = 0;  if (_amount == initialPoolSupply)  reserveAmount = balance;  else  reserveAmount = _amount.mul(balance).div(initialPoolSupply);  // sync the reserve balance / staked balance  reserves[reserveToken].balance = reserves[reserveToken].balance.sub(reserveAmount);  uint256 newStakedBalance = stakedBalances[reserveToken].sub(reserveAmount);  stakedBalances[reserveToken] = newStakedBalance;  The problem is that sometimes there might not be enough funds in reserve (for example, due to this issue https://github.com/ConsenSys/bancor-audit-2020-06/issues/4). So the first ones who withdraw their stakes receive all the tokens they own. But the last stakeholders might not be able to get their funds back because the pool is empty already.  So under some circumstances, there is a chance that users can lose all of their staked funds.  This issue also has the opposite side: if the liquidity pool makes an extra profit, the stakers do not owe this profit and cannot withdraw it.  Recommendation  Distribute losses evenly across the liquidity providers.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.4 Oracle front-running could deplete reserves over time    Addressed", "body": "  Resolution  To mitigate this issue, the Bancor team has added a mechanism that adjusts the effective weights once per block based on its internal price feed. The conversion rate re-anchors to the external oracle price once the next oracle update comes in. This mechanism should help to cause the weight rebalancing caused by the external Oracle update to be less pronounced, thereby limiting the profitability of Oracle frontrunning. It should be noted that it also adds another layer of complexity to the system. It is difficult to predict the actual effectiveness and impact of this mitigation measure without simulating the system under real-world conditions.  Description  Bancor s weight rebalancing mechanism uses Chainlink price oracles to dynamically update the weights of the assets in the pool to track the market price. Due to Oracle price updates being visible in the mempool before they are included in a block, it is always possible to know about Oracle updates in advance and attempt to make a favourable conversion which takes the future rebalancing into account, followed by the reverse conversion after the rebalancing has occurred. This can be done with high liquidity and medium risk since transaction ordering on the Ethereum blockchain is largely predictable.  Over time, this could deplete the secondary reserve as the formula compensates by rebalancing the weights such that the secondary token is sold slightly below its market rate (this is done to create an incentive to bring the primary reserve back to the amount staked by liquidity providers).  Example  Consider the initial state with an amplification factor of 20 and zero fees:  The frontrunner sees a Chainlink transaction in the mempool that changes Oracle B rate to 10,500. He sends a transaction with a slightly higher gas price than the Oracle update.  Convert 1,000,000 TKN into 999,500 BNT.  The intermediate state:  In the following block, the frontrunner sends another transaction with a high gas price (the goal is to be first to convert at the new rate set by the Oracle update):  Convert 999,500 BNT back into TKN.  The state is:  The frontrunner can now leverage the incentive created by the formula to bring back TKN reserve balance to staked TKN balance by converting TKN back to BNT:  Convert 4,994 TKN to BNT  The final state is:  The pool is now balanced and the frontrunner has gained 4,969 BNT.  Recommendation  This appears to be a fundamental problem caused by the fact that rebalancing is predictable. It is difficult to assess the actual impact of this issue without also reviewing components external to the scope of this audit (Chainlink) and extensively testing the system under real-world conditions.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.5 Use of external calls with a fixed amount of gas   ", "body": "  Resolution  It was decided to accept this minor risk as the usage of .call() might introduce other unexpected behavior.  Description  The converter smart contract uses the Solidity transfer() function to transfer Ether.  .transfer() and .send() forward exactly 2,300 gas to the recipient. The goal of this hardcoded gas stipend was to prevent reentrancy vulnerabilities, but this only makes sense under the assumption that gas costs are constant. Recently EIP 1884 was included in the Istanbul hard fork. One of the changes included in EIP 1884 is an increase to the gas cost of the SLOAD operation, causing a contract s fallback function to cost more than 2300 gas.  Examples  code/contracts/converter/ConverterBase.sol:L228  _to.transfer(address(this).balance);  code/contracts/converter/LiquidityPoolV2Converter.sol:L370  if (_targetToken == ETH_RESERVE_ADDRESS)  code/contracts/converter/LiquidityPoolV2Converter.sol:L509  msg.sender.transfer(reserveAmount);  Recommendation  It s recommended to stop using .transfer() and .send() and instead use .call(). Note that .call() does nothing to mitigate reentrancy attacks, so other precautions must be taken. To prevent reentrancy attacks, it is recommended that you use the checks-effects-interactions pattern.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.6 Use of assert statement for input validation    Addressed", "body": "  Resolution  Assertions are no longer used in the final version reviewed.  Description  Solidity assertion should only be used to assert invariants, i.e. statements that are expected to always hold if the code behaves correctly. Note that all available gas is consumed when an assert-style exception occurs.  Examples  It appears that assert() is used in one location within the test scope to catch invalid user inputs:  code/contracts/converter/LiquidityPoolV2Converter.sol:L354  assert(amount < targetReserveBalance);  Recommendation  Using require() instead of assert().  6 Bytecode Verification  Bytecode-level checking helps to ensure that the code behaves correctly for all input values. In this audit we used Mythx deep analysis to verify a small number of basic properties on the weight rebalancing and conversion functions and to detect conditions that would cause runtime exceptions. MythX uses symbolic execution and input fuzzing to explore a large amount of possible inputs and program states.  Note that the Bancor formula is compiled with solc-0.4.25 / 20,000 optimization passes.  We checked whether the following properties hold for all inputs:  [P1] Function balancedWeights: Sum of weights returned by must equal MAX_WEIGHT  [P2a] Function crossReserveTargetAmount: Output amount must not be greater than target reserve balance  [P2b] Function crossReserveTargetAmount: If reserve balances are equal and source weight < target weight, target amount must be lower than input amount  Note that balancedWeights is known to revert when (t * p) / (r * q) * log( s / t) is not in the range [-1/e, 1/e], where:  t is the primary reserve staked balance  s is the primary reserve current balance  r is the secondary reserve current balance  q is the primary reserve rate  p is the secondary reserve rate  The following preconditions were set on the input to reflect realistic input ranges. For balancedWeights:  For crossReserveTargetAmount:  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "6.1 Results", "body": "  No violations of the properties tested were found. Our tools also did not identify any cases that would cause the function to revert for the given input ranges.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.1 TokenStaking.recoverStake allows instant stake undelegation    Addressed", "body": "  Resolution   Addressed with   keep-network/keep-core#1521 by adding a non-zero check for the undelegation block.  Description  TokenStaking.recoverStake is used to recover stake that has been designated to be undelegated. It contains a single check to ensure that the undelegation period has passed:  keep-core/contracts/solidity/contracts/TokenStaking.sol:L182-L187  function recoverStake(address _operator) public {  uint256 operatorParams = operators[_operator].packedParams;  require(  block.number > operatorParams.getUndelegationBlock().add(undelegationPeriod),  \"Can not recover stake before undelegation period is over.\"  );  However, if an undelegation period is never set, this will always return true, allowing any operator to instantly undelegate stake at any time.  Recommendation  Require that the undelegation period is nonzero before allowing an operator to recover stake.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.2 Improper length validation in BLS signature library allows RNG manipulation    Addressed", "body": "  Resolution   Addressed with   keep-network/keep-core#1523 by adding input length checks to  Description  KeepRandomBeaconOperator.relayEntry(bytes memory _signature) is used to submit random beacon results:  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L418-L433  function relayEntry(bytes memory _groupSignature) public nonReentrant {  require(isEntryInProgress(), \"Entry was submitted\");  require(!hasEntryTimedOut(), \"Entry timed out\");  bytes memory groupPubKey = groups.getGroupPublicKey(signingRequest.groupIndex);  require(  BLS.verify(  groupPubKey,  signingRequest.previousEntry,  _groupSignature  ),  \"Invalid signature\"  );  emit RelayEntrySubmitted();  The function calls BLS.verify, which validates that the submitted signature correctly signs the previous recorded random beacon entry. BLS.verify calls AltBn128.g1Unmarshal(signature):  keep-core/contracts/solidity/contracts/cryptography/BLS.sol:L31-L37  function verify(  bytes memory publicKey,  bytes memory message,  bytes memory signature  ) public view returns (bool) {  AltBn128.G1Point memory _signature = AltBn128.g1Unmarshal(signature);  AltBn128.g1Unmarshal(signature) reads directly from memory without making any length checks:  keep-core/contracts/solidity/contracts/cryptography/AltBn128.sol:L214-L228  /**  @dev Unmarshals a point on G1 from bytes in an uncompressed form.  /  function g1Unmarshal(bytes memory m) internal pure returns(G1Point memory) {  bytes32 x;  bytes32 y;  /* solium-disable-next-line */  assembly {  x := mload(add(m, 0x20))  y := mload(add(m, 0x40))  return G1Point(uint256(x), uint256(y));  There are two potential issues with this:  g1Unmarshal may be reading out-of-bounds of the signature from dirty memory.  g1Unmarshal may not be reading all of the signature. If more than 64 bytes are supplied, they are ignored for the purposes of signature validation.  These issues are important because the hash of the signature is the  random number  supplied to user contracts:  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L435-L448  // Spend no more than groupSelectionGasEstimate + 40000 gas max  // This will prevent relayEntry failure in case the service contract is compromised  signingRequest.serviceContract.call.gas(groupSelectionGasEstimate.add(40000))(  abi.encodeWithSignature(  \"entryCreated(uint256,bytes,address)\",  signingRequest.relayRequestId,  _groupSignature,  msg.sender  );  if (signingRequest.callbackFee > 0) {  executeCallback(signingRequest, uint256(keccak256(_groupSignature)));  An attacker can use this behavior to game random number generation by frontrunning a valid signature submission with additional byte padding.  Recommendation  Ensure each function in BLS.sol properly validates input lengths for all parameters; the same length validation issue exists in BLS.verifyBytes.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.3 tbtc - the tecdsa keep is never closed, signer bonds are not released    Addressed", "body": "  Resolution  Addressed with https://github.com/keep-network/tbtc/issues/473, https://github.com/keep-network/tbtc/issues/490, keep-network/tbtc#534, and keep-network/tbtc#520.  failed_setup:  notifySignerSetupFailure \u2705closed by seizing funds with issue 5.10 notifyFundingTimeout \u2705closed with keep-network/tbtc#534 provideFundingECDSAFraudProof, \u2705slashes stake, distributes signer bonds to funder (push payment -> should be pull or funder may block), closes keep. provideFraudBTCFundingProof \u2705 removed with keep-network/tbtc#534 notifyFraudFundingTimeout \u2705 removed with keep-network/tbtc#534  liquidated:  provideSPVFraudProof \u2705removed purchaseSignerBondsAtAuction \u2705 via startSignerAbortLiquidation, \u2705 via startSignerFraudLiquidation (implicitly via seizebonds)  redeemed:  provideRedemptionProof \u2705  Description  At the end of the TBTC deposit lifecycle happy path, the deposit is supposed to close the keep in order to release the signer bonds. However, there is no call to closeKeep in any of the code-bases under audit.  Recommendation  Close the keep releasing the signer bonds.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.4 tbtc - No access control in TBTCSystem.requestNewKeep    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#514. Each call to  Description  TBTCSystem.requestNewKeep is used by each new Deposit contract on creation. It calls BondedECDSAKeepFactory.openKeep, which sets the Deposit contract as the  owner,  a permissioned role within the created keep. openKeep also automatically allocates bonds from members registered to the application. The  application  from which member bonds are allocated is the tbtc system itself.  Because requestNewKeep has no access controls, anyone can request that a keep be opened with msg.sender as the  owner,  and arbitrary signing threshold values:  tbtc/implementation/contracts/system/TBTCSystem.sol:L231-L243  /// @notice Request a new keep opening.  /// @param _m Minimum number of honest keep members required to sign.  /// @param _n Number of members in the keep.  /// @return Address of a new keep.  function requestNewKeep(uint256 _m, uint256 _n, uint256 _bond)  external  payable  returns (address)  IBondedECDSAKeepVendor _keepVendor = IBondedECDSAKeepVendor(keepVendor);  IBondedECDSAKeepFactory _keepFactory = IBondedECDSAKeepFactory(_keepVendor.selectFactory());  return _keepFactory.openKeep.value(msg.value)(_n, _m, msg.sender, _bond);  Given that the owner of a keep is able to seize signer bonds, close the keep, and more, having control of this role could be detrimental to group members.  Recommendation  Add access control to requestNewKeep, so that it can only be called as a part of the Deposit creation and initialization process.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.5 Unpredictable behavior due to front running or general bad timing    Addressed", "body": "  Resolution  This issue has been addressed with https://github.com/keep-network/tbtc/issues/493 and the following set of PRs:  https://github.com/keep-network/tbtc/issues/493  https://github.com/keep-network/keep-tecdsa/issues/296 - note: initializeImplementation should be done in completeUpgrade otherwise this could be used as a backdoor.  fixed by keep-network/keep-ecdsa#327 - fixed: initialization moved to complete upgrade step  https://github.com/keep-network/keep-core/issues/1423 - note: initializeImplementationshould be done incompleteUpgrade` otherwise this could be used as a backdoor.  fixed by keep-network/keep-core#1517 - fixed: initialization moved to complete upgrade step  The client also provided the following statements:  In general, our current stance on frontrunning proofs that lead to rewards is that as long as it doesn t significantly compromise an incentive on the primary actors of the system, we re comfortable with having it present. In particular, frontrunnable actions that include rewards in several cases have additional incentives\u2014for tBTC deposit owners, for example, claiming bonds in case of misbehavior; for signers, reclaiming bonds in case of deposit owner absence or other misbehavior. We consider signer reclamation of bonds to be a strong incentive, as bond value is expected to be large enough that there is ongoing expected value to having the bond value liquid rather than bonded.  Some of the frontrunning cases (e.g. around beacon signing) did not have this additional incentive, and in those cases we ve taken up the recommendations in the audit.  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.  Some instances of this are more important than others, but in general users of the system should have assurances about the behavior of the action they re about to take.  Examples  System Parameters  The owner of the TBTCSystem contract can change system parameters at any time with changes taking effect immediately.  setSignerFeeDivisor - stored in the deposit contract when creating a new deposit. emits an event.  setLotSizes - stored in the deposit contract when creating a new deposit. emits an event.  setCollateralizationThresholds - stored in the deposit contract when creating a new deposit. emits an event.  This also opens up an opportunity for malicious owner to:  interfere with other participants deposit creation attempts (front-running transactions)  craft a series of transactions that allow the owner to set parameters that are more beneficial to them, then create a deposit and reset the parameters to the systems  initial settings.  tbtc/implementation/contracts/system/TBTCSystem.sol:L113-L121  /// @notice Set the system signer fee divisor.  /// @param _signerFeeDivisor The signer fee divisor.  function setSignerFeeDivisor(uint256 _signerFeeDivisor)  external onlyOwner  require(_signerFeeDivisor > 9, \"Signer fee divisor must be greater than 9, for a signer fee that is <= 10%.\");  signerFeeDivisor = _signerFeeDivisor;  emit SignerFeeDivisorUpdated(_signerFeeDivisor);  Upgradables  The proxy pattern used in many places throughout the system allows the operator to set a new implementation which takes effect immediately.  keep-core/contracts/solidity/contracts/KeepRandomBeaconService.sol:L67-L80  /**  @dev Upgrade current implementation.  @param _implementation Address of the new implementation contract.  /  function upgradeTo(address _implementation)  public  onlyOwner  address currentImplementation = implementation();  require(_implementation != address(0), \"Implementation address can't be zero.\");  require(_implementation != currentImplementation, \"Implementation address must be different from the current one.\");  setImplementation(_implementation);  emit Upgraded(_implementation);  keep-tecdsa/solidity/contracts/BondedECDSAKeepVendor.sol:L57-L71  /// @notice Upgrades the current vendor implementation.  /// @param _implementation Address of the new vendor implementation contract.  function upgradeTo(address _implementation) public onlyOwner {  address currentImplementation = implementation();  require(  _implementation != address(0),  \"Implementation address can't be zero.\"  );  require(  _implementation != currentImplementation,  \"Implementation address must be different from the current one.\"  );  setImplementation(_implementation);  emit Upgraded(_implementation);  Registry  keep-tecdsa/solidity/contracts/BondedECDSAKeepVendorImplV1.sol:L43-L50  function registerFactory(address payable _factory) external onlyOperatorContractUpgrader {  require(_factory != address(0), \"Incorrect factory address\");  require(  registry.isApprovedOperatorContract(_factory),  \"Factory contract is not approved\"  );  keepFactory = _factory;  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.6 keep-core - reportRelayEntryTimeout creates an incentive for nodes to race for rewards potentially wasting gas and it creates an opportunity for front-running    Addressed", "body": "  Resolution   Following the discussion at   https://github.com/keep-network/keep-core/issues/1404 it was verified that the method throws as early as possible in an attempt to safe gas in case many nodes call out the timeout in the same block. The client is currently comfortable with this tradeoff. We would like to note that this issue cannot easily be addressed (e.g. allowing nodes to disable calling out timeouts impacts the security of the system; a commit/reveal proxy adds overhead and is unlikely to make the situation better as nodes are programmed to call out timeouts) and we therefore recommend to monitor the network for this scenario.  Description  The incentive on reportRelayEntryTimeout for being rewarded with 5% of the seized amount creates an incentive to call the method but might also kick off a race for front-running this call. This method is being called from the keep node which is unlikely to adjust the gasPrice and might always lose the race against a front-running bot collecting rewards for all timeouts and fraud proofs (issue 5.7)  Examples  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L600-L626  /**  @dev Function used to inform about the fact the currently ongoing  new relay entry generation operation timed out. As a result, the group  which was supposed to produce a new relay entry is immediately  terminated and a new group is selected to produce a new relay entry.  All members of the group are punished by seizing minimum stake of  their tokens. The submitter of the transaction is rewarded with a  tattletale reward which is limited to min(1, 20 / group_size) of the  maximum tattletale reward.  /  function reportRelayEntryTimeout() public {  require(hasEntryTimedOut(), \"Entry did not time out\");  groups.reportRelayEntryTimeout(signingRequest.groupIndex, groupSize, minimumStake);  // We could terminate the last active group. If that's the case,  // do not try to execute signing again because there is no group  // which can handle it.  if (numberOfGroups() > 0) {  signRelayEntry(  signingRequest.relayRequestId,  signingRequest.previousEntry,  signingRequest.serviceContract,  signingRequest.entryVerificationAndProfitFee,  signingRequest.callbackFee  );  Recommendation  Make sure that reportRelayEntryTimeout throws as early as possible if the group was previously terminated (isGroupTerminated) to avoid that keep-nodes spend gas on a call that will fail. Depending on the reward for calling out the timeout this might create a front-running opportunity that cannot be resolved.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.7 keep-core - reportUnauthorizedSigning fraud proof is not bound to reporter and can be front-run    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-core/issues/1405 by binding the proof to  Description  An attacker can monitor reportUnauthorizedSigning() for fraud reports and attempt to front-run the original call in an effort to be the first one reporting the fraud and be rewarded 5% of the total seized amount.  Examples  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L742-L755  /**  @dev Reports unauthorized signing for the provided group. Must provide  a valid signature of the group address as a message. Successful signature  verification means the private key has been leaked and all group members  should be punished by seizing their tokens. The submitter of this proof is  rewarded with 5% of the total seized amount scaled by the reward adjustment  parameter and the rest 95% is burned.  /  function reportUnauthorizedSigning(  uint256 groupIndex,  bytes memory signedGroupPubKey  ) public {  groups.reportUnauthorizedSigning(groupIndex, signedGroupPubKey, minimumStake);  Recommendation  Require the reporter to include msg.sender in the signature proving the fraud or implement a two-step commit/reveal scheme to counter front-running opportunities by forcing a reporter to secretly commit the fraud parameters in one block and reveal them in another.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.8 keep-core - operator contracts disabled via panic button can be re-enabled by RegistryKeeper    Addressed", "body": "  Resolution  Addressed by https://github.com/keep-network/keep-core/issues/1406 with changes from keep-network/keep-core#1463:  the contract is now using enums instead of int literals  only new operator contracts can be approved  only approved contracts can be disabled  disabled contracts cannot be re-enabled  disabling an operator contract does not yield an event  changes take effect immediately  Description  The keep specification states the following:  Panic Button The Panic Button can disable malicious or malfunctioning contracts that have been previously approved by the Registry Keeper. When a contract is disabled by the Panic Button, its status on the registry changes to reflect this, and it becomes ineligible to penalize operators. Contracts disabled by the Panic Button can not be reactivated. The Panic Button can be rekeyed by Governance.  With the current implementation of the Registry the registryKeeper account can re-enable an operator contract that has previously been disabled by the panicButton account.  We would also like to note the following:  The contract should use enums instead of integer literals when working with contract states.  Changes to the contract take effect immediately, allowing an administrative account to selectively front-run calls to the Registry ACL and interfere with user activity.  The operator contract state can be set to the current value without raising an error.  The panic button can be called for operator contracts that are not yet active.  Examples  keep-core/contracts/solidity/contracts/Registry.sol:L67-L75  function approveOperatorContract(address operatorContract) public onlyRegistryKeeper {  operatorContracts[operatorContract] = 1;  function disableOperatorContract(address operatorContract) public onlyPanicButton {  operatorContracts[operatorContract] = 2;  Recommendation  The keep specification states:  The Panic Button can be used to set the status of an APPROVED contract to DISABLED. Operator Contracts disabled with the Panic Button cannot be re-enabled, and disabled contracts may not punish operators nor be selected by service contracts to perform work.  All three accounts are typically trusted. We recommend requiring the Governance or paniceButton accounts to reset the contract operator state before registryKeeper can change the state or disallow re-enabling of disabled operator contracts as stated in the specification.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.9 tbtc - State transitions are not always enforced    Addressed", "body": "  Resolution  This issue was addressed with https://github.com/keep-network/tbtc/issues/494 and accepted by the client with the following statement. Deposits that are timed out can still be pushed to an active state.  For 5.7 around state transitions, our stance (specifically for the upcoming release) is that a skipped state is acceptable as long as it does not result in data loss or incentive skew. Taken in turn, the listed examples:   A TDT holder can choose not to call out notifySignerSetupFailure hoping that the signing group still forms after the signer setup timeout passes.  -> we consider this fine. If the TDT holder wishes to hold out hope, it is their choice. Signers should be incentivized to call notifySignerSetupFailure in case of actual failure to release their bond.   The deposit can be pushed to active state even after notifySignerSetupFailure, notifyFundingTimeout have passed but nobody called it out.  -> again, we consider this fine. A deposit that is funded and proven past its timeout is still a valid deposit, since the two players in question (the depositor and the signing group) were willing to wait longer to complete the flow. The timeouts in question are largely a matter of allowing signers to release their bond in case there is an issue setting up the deposit.   Members of the signing group might decide to call notifyFraudFundingTimeout in a race to avoid late submissions for provideFraudBTCFundingProof to succeed in order to contain funds lost due to fraud.  -> We are intending to change the mechanic here so that signers lose their whole bond in either case.   A malicious signing group observes BTC funding on the bitcoin chain in an attempt to commit fraud at the time the provideBTCFundingProof transition becomes available to front-run provideFundingECDSAFraudProof forcing the deposit into active state.  -> this one is tough, and we re working on changing the liquidation initiator reward so it is no longer a useful attack. In particular, we re looking at the suggestion in 2.4 for this.   If oracle price slippage occurs for one block (flash-crash type of event) someone could call an undercollateralization transition.  -> We are still investigating this possibility.   A deposit term expiration courtesy call can be exit in the rare case where _d.fundedAt + TBTCConstants.getDepositTerm() == block.timestamp  -> Deposit term expiration courtsey calls should no longer apply; see keep-network/tbtc@6344892 . Courtesy call after deposit term is identical to courtsey call pre-term.  Description  State transitions from one deposit state to another require someone calling the corresponding transition method on the deposit and actually spend gas on it. The incentive to call a transition varies and is analyzed in more detail in the security-specification section of this report.  This issue assumes that participants are not always pushing forward through the state machine as soon as a new state becomes available, opening up the possibility of having multiple state transitions being a valid option for a deposit (e.g. pushing a deposit to active state even though a timeout should have been called on it).  Examples  A TDT holder can choose not to call out notifySignerSetupFailure hoping that the signing group still forms after the signer setup timeout passes.  there is no incentive for the TDT holder to terminate its own deposit after a timeout.  the deposit might end up never being in a final error state.  there is no incentive for the signing group to terminate the deposit.  This affects all states that can time out.  The deposit can be pushed to active state even after notifySignerSetupFailure, notifyFundingTimeout have passed but nobody called it out.  There is no timeout check in retrieveSignerPubkey, provideBTCFundingProof.  tbtc/implementation/contracts/deposit/DepositFunding.sol:L108-L117  /// @notice             we poll the Keep contract to retrieve our pubkey  /// @dev                We store the pubkey as 2 bytestrings, X and Y.  /// @param  _d          deposit storage pointer  /// @return             True if successful, otherwise revert  function retrieveSignerPubkey(DepositUtils.Deposit storage _d) public {  require(_d.inAwaitingSignerSetup(), \"Not currently awaiting signer setup\");  bytes memory _publicKey = IBondedECDSAKeep(_d.keepAddress).getPublicKey();  require(_publicKey.length == 64, \"public key not set or not 64-bytes long\");  tbtc/implementation/contracts/deposit/DepositFunding.sol:L263-L278  function provideBTCFundingProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public returns (bool) {  require(_d.inAwaitingBTCFundingProof(), \"Not awaiting funding\");  bytes8 _valueBytes;  bytes memory  _utxoOutpoint;  Members of the signing group might decide to call notifyFraudFundingTimeout in a race to avoid late submissions for provideFraudBTCFundingProof to succeed in order to contain funds lost due to fraud.  It should be noted that even after the fraud funding timeout passed the TDT holder could provideFraudBTCFundingProof as it does not check for the timeout.  A malicious signing group observes BTC funding on the bitcoin chain in an attempt to commit fraud at the time the provideBTCFundingProof transition becomes available to front-run provideFundingECDSAFraudProof forcing the deposit into active state.  The malicious users of the signing group can then try to report fraud, set themselves as liquidationInitiator to be awarded part of the signer bond (in addition to taking control of the BTC collateral).  The TDT holders fraud-proof can be front-run, see issue 5.15  If oracle price slippage occurs for one block (flash-crash type of event) someone could call an undercollateralization transition.  For severe oracle errors deposits might be liquidated by calling notifyUndercollateralizedLiquidation. The TDT holder cannot exit liquidation in this case.  For non-severe under collateralization someone could call notifyCourtesyCall to impose extra effort on TDT holders to exitCourtesyCall deposits.  A deposit term expiration courtesy call can be exit in the rare case where _d.fundedAt + TBTCConstants.getDepositTerm() == block.timestamp  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L289-L298  /// @notice     Goes from courtesy call to active  /// @dev        Only callable if collateral is sufficient and the deposit is not expiring  /// @param  _d  deposit storage pointer  function exitCourtesyCall(DepositUtils.Deposit storage _d) public {  require(_d.inCourtesyCall(), \"Not currently in courtesy call\");  require(block.timestamp <= _d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit is expiring\");  require(getCollateralizationPercentage(_d) >= _d.undercollateralizedThresholdPercent, \"Deposit is still undercollateralized\");  _d.setActive();  _d.logExitedCourtesyCall();  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L318-L327  /// @notice     Notifies the contract that its term limit has been reached  /// @dev        This initiates a courtesy call  /// @param  _d  deposit storage pointer  function notifyDepositExpiryCourtesyCall(DepositUtils.Deposit storage _d) public {  require(_d.inActive(), \"Deposit is not active\");  require(block.timestamp >= _d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit term not elapsed\");  _d.setCourtesyCall();  _d.logCourtesyCalled();  _d.courtesyCallInitiated = block.timestamp;  Allow exiting the courtesy call only if the deposit is not expired: block.timestamp < _d.fundedAt + TBTCConstants.getDepositTerm()  Recommendation  Ensure that there are no competing interests between participants of the system to favor one transition over the other, causing race conditions, front-running opportunities or stale deposits that are not pushed to end-states.  Note: Please find an analysis of incentives to call state transitions in the security section of this document.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.10 tbtc - Funder loses payment to keep if signing group is not established in time   Pending", "body": "  Resolution  This issue was addressed with https://github.com/keep-network/tbtc/issues/495 by refunding the cost of creating a new keep. We recommend using the pull instead of a push payment pattern to avoid that the funder can block the call.  Additionally, the client provided the following statement:  The remaining push vs pull question is being tracked in https://github.com/keep-network/tbtc/issues/551, part of recommendation 2.7.  Description  The funder had to provide payment for the keep but the signing group failed to establish. Payment for the keep is not returned even though one could assume that the signing group tried to play unfairly. The signing group might intentionally try to cause this scenario to interfere with the system.  Examples  retrieveSignerPubkey fails if keep provided pubkey is empty or of an unexpected length  tbtc/implementation/contracts/deposit/DepositFunding.sol:L108-L127  /// @notice             we poll the Keep contract to retrieve our pubkey  /// @dev                We store the pubkey as 2 bytestrings, X and Y.  /// @param  _d          deposit storage pointer  /// @return             True if successful, otherwise revert  function retrieveSignerPubkey(DepositUtils.Deposit storage _d) public {  require(_d.inAwaitingSignerSetup(), \"Not currently awaiting signer setup\");  bytes memory _publicKey = IBondedECDSAKeep(_d.keepAddress).getPublicKey();  require(_publicKey.length == 64, \"public key not set or not 64-bytes long\");  _d.signingGroupPubkeyX = _publicKey.slice(0, 32).toBytes32();  _d.signingGroupPubkeyY = _publicKey.slice(32, 32).toBytes32();  require(_d.signingGroupPubkeyY != bytes32(0) && _d.signingGroupPubkeyX != bytes32(0), \"Keep returned bad pubkey\");  _d.fundingProofTimerStart = block.timestamp;  _d.setAwaitingBTCFundingProof();  _d.logRegisteredPubkey(  _d.signingGroupPubkeyX,  _d.signingGroupPubkeyY);  notifySignerSetupFailure can be called by anyone after a timeout of 3hrs  tbtc/implementation/contracts/deposit/DepositFunding.sol:L93-L106  /// @notice     Anyone may notify the contract that signing group setup has timed out  /// @dev        We rely on the keep system punishes the signers in this case  /// @param  _d  deposit storage pointer  function notifySignerSetupFailure(DepositUtils.Deposit storage _d) public {  require(_d.inAwaitingSignerSetup(), \"Not awaiting setup\");  require(  block.timestamp > _d.signingGroupRequestedAt + TBTCConstants.getSigningGroupFormationTimeout(),  \"Signing group formation timeout not yet elapsed\"  );  _d.setFailedSetup();  _d.logSetupFailed();  fundingTeardown(_d);  Recommendation  It should be ensured that a keep group always establishes or otherwise the funder is refunded the fee for the keep.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.11 tbtc - Ethereum block gas limit imposes a fundamental limitation on SPV proofs    Addressed", "body": "  Resolution   SPV fraud proofs were removed in   keep-network/tbtc#521. Remember to continue exploring this limitation of the EVM with benchmarking and gas estimates in the tBTC UI.  Description  Several components of the tBTC system rely on SPV proofs to prove the existence of transactions on Bitcoin. Because an SPV proof must provide the entire Bitcoin transaction to the proving smart contract, the Ethereum block gas limit imposes an upper bound on the size of the transaction in question. Although an exact upper bound is subject to several variables, reasonable estimates show that even a moderately-sized Bitcoin transaction may not be able to be successfully validated on Ethereum.  This limitation is significant for two reasons:  Depositors may deposit BTC to the signers by way of a legitimate Bitcoin transaction, only to find that this transaction is unable to be verified on Ethereum. Although the depositor in question was not acting maliciously, they may lose their deposit entirely.  In case signers collude to spend a depositor s BTC unprompted, the system allows depositors to prove a fraudulent spend occurred by way of SPV fraud proof. Given that signers can easily spend BTC with a transaction that is too large to validate by way of SPV proof, this method of fraud proof is unreliable at best. Deposit owners should instead prove fraud by using an ECDSA fraud proof, which operates on a hash of the signed message.  Recommendation  It s important that prospective depositors are able to guarantee that their deposit transaction will be verified successfully. To that end, efforts should be made to provide a deposit UI that checks whether or not a given transaction will be verified successfully before it is submitted. Several variables can affect transaction verification:  Current Ethereum block gas limits  Number of zero-bytes in the Bitcoin transaction in question  Size of the merkle proof needed to prove the transaction s existence  Given that not all of these can be calculated before the transaction is submitted to the Bitcoin blockchain, calculations should attempt to provide a margin of error for the process. Additionally, users should be well-educated about the process, including how to perform a deposit with relatively low risk.  Understanding the relative limitations of the EVM will help this process significantly. Consider benchmarking the gas cost of verifying Bitcoin transactions of various sizes.  Finally, because SPV fraud proofs can be gamed by colluding signers, they should be removed from the system entirely. Deposit owners should always be directed towards ECDSA fraud proofs, as these require relatively fewer assumptions and stronger guarantees.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.12 bitcoin-spv - SPV proofs do not support transactions with larger numbers of inputs and outputs   Pending", "body": "  Resolution  The client provided the following statement:  Benchmarks and takeaways are being tracked in issue https://github.com/keep-network/tbtc/issues/556.  Description  There is no explicit restriction on the number of inputs and outputs a Bitcoin transaction can have - as long as the transaction fits into a block. The number of inputs and outputs in a transaction is denoted by a leading  varint  - a variable length integer. In BTCUtils.validateVin and BTCUtils.validateVout, the value of this varint is restricted to under 0xFD, or 253:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L404-L415  /// @notice      Checks that the vin passed up is properly formatted  /// @dev         Consider a vin with a valid vout in its scriptsig  /// @param _vin  Raw bytes length-prefixed input vector  /// @return      True if it represents a validly formatted vin  function validateVin(bytes memory _vin) internal pure returns (bool) {  uint256 _offset = 1;  uint8 _nIns = uint8(_vin.slice(0, 1)[0]);  // Not valid if it says there are too many or no inputs  if (_nIns >= 0xfd || _nIns == 0) {  return false;  Transactions that include more than 252 inputs or outputs will not pass this validation, leading to some legitimate deposits being rejected by the tBTC system.  Examples  The 252-item limit exists in a few forms throughout the system, outside of the aforementioned BTCUtils.validateVin and BTCUtils.validateVout:  BTCUtils.determineOutputLength:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L294-L303  /// @notice          Determines the length of an output  /// @dev             5 types: WPKH, WSH, PKH, SH, and OP_RETURN  /// @param _output   The output  /// @return          The length indicated by the prefix, error if invalid length  function determineOutputLength(bytes memory _output) internal pure returns (uint256) {  uint8 _len = uint8(_output.slice(8, 1)[0]);  require(_len < 0xfd, \"Multi-byte VarInts not supported\");  return _len + 8 + 1; // 8 byte value, 1 byte for _len itself  DepositUtils.findAndParseFundingOutput:  tbtc/implementation/contracts/deposit/DepositUtils.sol:L150-L154  function findAndParseFundingOutput(  DepositUtils.Deposit storage _d,  bytes memory _txOutputVector,  uint8 _fundingOutputIndex  ) public view returns (bytes8) {  DepositUtils.validateAndParseFundingSPVProof:  tbtc/implementation/contracts/deposit/DepositUtils.sol:L181-L191  function validateAndParseFundingSPVProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public view returns (bytes8 _valueBytes, bytes memory _utxoOutpoint){  DepositFunding.provideFraudBTCFundingProof:  tbtc/implementation/contracts/deposit/DepositFunding.sol:L213-L223  function provideFraudBTCFundingProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public returns (bool) {  DepositFunding.provideBTCFundingProof:  tbtc/implementation/contracts/deposit/DepositFunding.sol:L263-L273  function provideBTCFundingProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  uint8 _fundingOutputIndex,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  bytes memory _bitcoinHeaders  ) public returns (bool) {  DepositLiquidation.provideSPVFraudProof:  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L150-L160  function provideSPVFraudProof(  DepositUtils.Deposit storage _d,  bytes4 _txVersion,  bytes memory _txInputVector,  bytes memory _txOutputVector,  bytes4 _txLocktime,  bytes memory _merkleProof,  uint256 _txIndexInBlock,  uint8 _targetInputIndex,  bytes memory _bitcoinHeaders  ) public {  Recommendation  Incorporate varint parsing in BTCUtils.validateVin and BTCUtils.validateVout. Ensure that other components of the system reflect the removal of the 252-item limit.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.13 bitcoin-spv - multiple integer under-/overflows    Addressed", "body": "  Resolution  This was partially addressed in summa-tx/bitcoin-spv#118, summa-tx/bitcoin-spv#119, and summa-tx/bitcoin-spv#122.  Summa opted not to fix the underflow in extractTarget.  In summa-tx/bitcoin-spv#118, the determineOutputLength overflow was addressed by casting _len to a uint256 before addition.  In summa-tx/bitcoin-spv#119, the extractHash underflow was addressed by returning an empty bytes array if the extracted length would cause underflow. Note that an explicit error and transaction revert is favorable in these cases, in order to avoid returning unusable data to the calling function.  Underflow and overflow in BytesLib was addressed in summa-tx/bitcoin-spv#122. Multiple requires were added to the mentioned functions, ensuring memory reads stayed in-bounds for each array. A later change in summa-tx/bitcoin-spv#128 added support for slice with a length of 0.  Description  The bitcoin-spv library allows for multiple integer under-/overflows while processing or converting potentially untrusted or user-provided data.  Examples  uint8 underflow uint256(uint8(_e - 3))  Note: _header[75] will throw consuming all gas if out of bounds while the majority of the library usually uses slice(start, 1) to handle this more gracefully.  bitcoin-spv/solidity/contracts/BTCUtils.sol:L483-L494  /// @dev             Target is a 256 bit number encoded as a 3-byte mantissa and 1 byte exponent  /// @param _header   The header  /// @return          The target threshold  function extractTarget(bytes memory _header) internal pure returns (uint256) {  bytes memory _m = _header.slice(72, 3);  uint8 _e = uint8(_header[75]);  uint256 _mantissa = bytesToUint(reverseEndianness(_m));  uint _exponent = _e - 3;  return _mantissa * (256 ** _exponent);  uint8 overflow uint256(uint8(_len + 8 + 1))  Note: might allow a specially crafted output to return an invalid determineOutputLength <= 9.  Note: while type VarInt is implemented for inputs, it is not for the output length.  bitcoin-spv/solidity/contracts/BTCUtils.sol:L295-L304  /// @dev             5 types: WPKH, WSH, PKH, SH, and OP_RETURN  /// @param _output   The output  /// @return          The length indicated by the prefix, error if invalid length  function determineOutputLength(bytes memory _output) internal pure returns (uint256) {  uint8 _len = uint8(_output.slice(8, 1)[0]);  require(_len < 0xfd, \"Multi-byte VarInts not supported\");  return _len + 8 + 1; // 8 byte value, 1 byte for _len itself  uint8 underflow uint256(uint8(extractOutputScriptLen(_output)[0]) - 2)  bitcoin-spv/solidity/contracts/BTCUtils.sol:L366-L378  /// @dev             Determines type by the length prefix and validates format  /// @param _output   The output  /// @return          The hash committed to by the pk_script, or null for errors  function extractHash(bytes memory _output) internal pure returns (bytes memory) {  if (uint8(_output.slice(9, 1)[0]) == 0) {  uint256 _len = uint8(extractOutputScriptLen(_output)[0]) - 2;  // Check for maliciously formatted witness outputs  if (uint8(_output.slice(10, 1)[0]) != uint8(_len)) {  return hex\"\";  return _output.slice(11, _len);  } else {  bytes32 _tag = _output.keccak256Slice(8, 3);  BytesLib input validation multiple start+length overflow  Note: multiple occurrences. should check start+length > start && bytes.length >= start+length  bitcoin-spv/solidity/contracts/BytesLib.sol:L246-L248  function slice(bytes memory _bytes, uint _start, uint _length) internal  pure returns (bytes memory res) {  require(_bytes.length >= (_start + _length), \"Slice out of bounds\");  BytesLib input validation multiple start overflow  bitcoin-spv/solidity/contracts/BytesLib.sol:L280-L281  function toUint(bytes memory _bytes, uint _start) internal  pure returns (uint256) {  require(_bytes.length >= (_start + 32), \"Uint conversion out of bounds.\");  bitcoin-spv/solidity/contracts/BytesLib.sol:L269-L270  function toAddress(bytes memory _bytes, uint _start) internal  pure returns (address) {  require(_bytes.length >= (_start + 20), \"Address conversion out of bounds.\");  bitcoin-spv/solidity/contracts/BytesLib.sol:L246-L248  function slice(bytes memory _bytes, uint _start, uint _length) internal  pure returns (bytes memory res) {  require(_bytes.length >= (_start + _length), \"Slice out of bounds\");  bitcoin-spv/solidity/contracts/BytesLib.sol:L410-L412  function keccak256Slice(bytes memory _bytes, uint _start, uint _length) pure internal returns (bytes32 result) {  require(_bytes.length >= (_start + _length), \"Slice out of bounds\");  Recommendation  We believe that a general-purpose parsing and verification library for bitcoin payments should be very strict when processing untrusted user input. With strict we mean, that it should rigorously validate provided input data and only proceed with the processing of the data if it is within a safe-to-use range for the method to return valid results. Relying on the caller to provide pre-validate data can be unsafe especially if the caller assumes that proper input validation is performed by the library.  Given the risk profile for this library, we recommend a conservative approach that balances security instead of gas efficiency without relying on certain calls or instructions to throw on invalid input.  For this issue specifically, we recommend proper input validation and explicit type expansion where necessary to prevent values from wrapping or processing data for arguments that are not within a safe-to-use range.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.14 tbtc - Unreachable state LIQUIDATION_IN_PROGRESS    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/497 with commits from  keep-network/tbtc#517 changing all non-fraud transitions to end up in  Description  According to the specification (overview, states, version 2020-02-06), a deposit can be in one of two liquidation_in_progress states.  LIQUIDATION_IN_PROGRESS  LIQUIDATION_IN_PROGRESS Liquidation due to undercollateralization or an abort has started Automatic (on-chain) liquidation was unsuccessful  FRAUD_LIQUIDATION_IN_PROGRESS  FRAUD_LIQUIDATION_IN_PROGRESS Liquidation due to fraud has started Automatic (on-chain) liquidation was unsuccessful  However, LIQUIDATION_IN_PROGRESS is unreachable and instead, FRAUD_LIQUIDATION_IN_PROGRESS is always called. This means that all non-fraud state transitions end up in the fraud liquidation path and will perform actions as if fraud was detected even though it might be caused by an undercollateralized notification or courtesy timeout.  Examples  startSignerAbortLiquidation transitions to FRAUD_LIQUIDATION_IN_PROGRESS on non-fraud events notifyUndercollateralizedLiquidation and notifyCourtesyTimeout  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L96-L108  /// @notice         Starts signer liquidation due to abort or undercollateralization  /// @dev            We first attempt to liquidate on chain, then by auction  /// @param  _d      deposit storage pointer  function startSignerAbortLiquidation(DepositUtils.Deposit storage _d) internal {  _d.logStartedLiquidation(false);  // Reclaim used state for gas savings  _d.redemptionTeardown();  _d.seizeSignerBonds();  _d.liquidationInitiated = block.timestamp;  // Store the timestamp for auction  _d.liquidationInitiator = msg.sender;  _d.setFraudLiquidationInProgress();  Recommendation  Verify state transitions and either remove LIQUIDATION_IN_PROGRESS if it is redundant or fix the state transitions for non-fraud liquidations.  Note that Deposit states can be simplified by removing redundant states by setting a flag (e.g. fraudLiquidation) in the deposit instead of adding a state to track the fraud liquidation path.  According to the specification, we assume the following state transitions are desired:  LIQUIDATION_IN_PROGRESS  In case of liquidation due to undercollateralization or abort, the remaining bond value is split 50-50 between the account which triggered the liquidation and the signers.  FRAUD_LIQUIDATION_IN_PROGRESS  In case of liquidation due to fraud, the remaining bond value in full goes to the account which triggered the liquidation by proving fraud.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.15 tbtc - various deposit state transitions can be front-run (e.g. fraud proofs, timeouts)   ", "body": "  Resolution  Addressed with the discussion at https://github.com/keep-network/tbtc/issues/498. It is accepted that a malicious entity may be able to front-run certain fraud proofs as long as fraud is being called out. It is also accepted that calls to certain timeouts may be front-run which could lead to a scenario where the client implementation is always front-run by a malicious actor.  Additionally, the client provided the following statement:  In general, we are comfortable with front-runnable interactions that ensure system integrity, as long as such front-running does not remove the original incentive of the submitter. We believe remaining front-runnable interactions have clear benefits to system actors, such that even if they are front-run, they have reason to submit the transaction.  Description  An entity that can provide proof for fraudulent ECDSA signatures or SPV proofs in the liquidation flow is rewarded with part of the deposit contract ETH value.  Specification: Liquidation Any signer bond left over after the deposit owner is compensated is distributed to the account responsible for reporting the misbehavior (for fraud) or between the signers and the account that triggered liquidation (for collateralization issues).  However, the methods under which proof is provided are not protected from front-running allowing anyone to observe transactions to provideECDSAFraudProof/ provideSPVFraudProof and submit the same proofs with providing a higher gas value.  Please note that a similar issue exists for timeout states providing rewards for calling them out (i.e. they set the liquidationInitiator address).  Examples  provideECDSAFraudProof verifies the fraudulent proof  r,s,v,signedDigest appear to be the fraudulent signature. _preimage is the correct value.  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L117-L137  /// @param _preimage        The sha256 preimage of the digest  function provideECDSAFraudProof(  DepositUtils.Deposit storage _d,  uint8 _v,  bytes32 _r,  bytes32 _s,  bytes32 _signedDigest,  bytes memory _preimage  ) public {  require(  !_d.inFunding() && !_d.inFundingFailure(),  \"Use provideFundingECDSAFraudProof instead\"  );  require(  !_d.inSignerLiquidation(),  \"Signer liquidation already in progress\"  );  require(!_d.inEndState(), \"Contract has halted\");  require(submitSignatureFraud(_d, _v, _r, _s, _signedDigest, _preimage), \"Signature is not fraud\");  startSignerFraudLiquidation(_d);  startSignerFraudLiquidation sets the address that provides the proof as the beneficiary  tbtc/implementation/contracts/deposit/DepositFunding.sol:L153-L179  function provideFundingECDSAFraudProof(  DepositUtils.Deposit storage _d,  uint8 _v,  bytes32 _r,  bytes32 _s,  bytes32 _signedDigest,  bytes memory _preimage  ) public {  require(  _d.inAwaitingBTCFundingProof(),  \"Signer fraud during funding flow only available while awaiting funding\"  );  bool _isFraud = _d.submitSignatureFraud(_v, _r, _s, _signedDigest, _preimage);  require(_isFraud, \"Signature is not fraudulent\");  _d.logFraudDuringSetup();  // If the funding timeout has elapsed, punish the funder too!  if (block.timestamp > _d.fundingProofTimerStart + TBTCConstants.getFundingTimeout()) {  address(0).transfer(address(this).balance);  // Burn it all down (fire emoji)  _d.setFailedSetup();  } else {  /* NB: This is reuse of the variable */  _d.fundingProofTimerStart = block.timestamp;  _d.setFraudAwaitingBTCFundingProof();  purchaseSignerBondsAtAuction pays out the funds  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L260-L276  uint256 contractEthBalance = address(this).balance;  address payable initiator = _d.liquidationInitiator;  if (initiator == address(0)){  initiator = address(0xdead);  if (contractEthBalance > 1) {  if (_wasFraud) {  initiator.transfer(contractEthBalance);  } else {  // There will always be a liquidation initiator.  uint256 split = contractEthBalance.div(2);  _d.pushFundsToKeepGroup(split);  initiator.transfer(split);  Recommendation  For fraud proofs, it should be required that the reporter uses a commit/reveal scheme to lock in a proof in one block, and reveal the details in another.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.16 tbtc - Anyone can emit log events due to missing access control    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/477,  keep-network/tbtc#467 and  keep-network/tbtc#537 by restricting log calls to known  Description  Examples  tbtc/implementation/contracts/DepositLog.sol:L95-L99  function approvedToLog(address _caller) public pure returns (bool) {  /* TODO: auth via system */  _caller;  return true;  Recommendation  Log events are typically initiated by the Deposit contract. Make sure only Deposit contracts deployed by an approved factory can emit logs on TBTCSystem.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.17 DKGResultVerification.verify unsafe packing in signed data    Addressed", "body": "  Resolution   Addressed with   keep-network/keep-core#1525 by adding additional checks for  Description  DKGResultVerification.verify allows the sender to arbitrarily move bytes between groupPubKey and misbehaved:  keep-core/contracts/solidity/contracts/libraries/operator/DKGResultVerification.sol:L80  bytes32 resultHash = keccak256(abi.encodePacked(groupPubKey, misbehaved));  Recommendation  Validate the expected length of both and add a salt between the two.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.18 keep-core - Service contract callbacks can be abused to call into other contracts    Addressed", "body": "  Resolution  Addressed with keep-network/keep-core#1532 by hardcoding the callback method signature and the following statement:  We still allow specifying an address of the callback contract. This could be beneficial in a situations where one contract pays for a random number for another contract.  A subsequent change in keep-network/keep-ecdsa#339 updated keep-tecdsa to use the new, hardcoded callback function: __beaconCallback(uint256).  Description  KeepRandomBeaconServiceImplV1 allows senders to specify an arbitrary method and contract that will receive a callback once the beacon generates a relay entry:  keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L228-L245  /**  @dev Creates a request to generate a new relay entry, which will include  a random number (by signing the previous entry's random number).  @param callbackContract Callback contract address. Callback is called once a new relay entry has been generated.  @param callbackMethod Callback contract method signature. String representation of your method with a single  uint256 input parameter i.e. \"relayEntryCallback(uint256)\".  @param callbackGas Gas required for the callback.  The customer needs to ensure they provide a sufficient callback gas  to cover the gas fee of executing the callback. Any surplus is returned  to the customer. If the callback gas amount turns to be not enough to  execute the callback, callback execution is skipped.  @return An uint256 representing uniquely generated relay request ID. It is also returned as part of the event.  /  function requestRelayEntry(  address callbackContract,  string memory callbackMethod,  uint256 callbackGas  ) public nonReentrant payable returns (uint256) {  Once an operator contract receives the relay entry, it calls executeCallback:  keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L314-L335  /**  @dev Executes customer specified callback for the relay entry request.  @param requestId Request id tracked internally by this contract.  @param entry The generated random number.  @return Address to receive callback surplus.  /  function executeCallback(uint256 requestId, uint256 entry) public returns (address payable surplusRecipient) {  require(  _operatorContracts.contains(msg.sender),  \"Only authorized operator contract can call execute callback.\"  );  require(  _callbacks[requestId].callbackContract != address(0),  \"Callback contract not found\"  );  _callbacks[requestId].callbackContract.call(abi.encodeWithSignature(_callbacks[requestId].callbackMethod, entry));  surplusRecipient = _callbacks[requestId].surplusRecipient;  delete _callbacks[requestId];  Arbitrary callbacks can be used to force the service contract to execute many functions within the keep contract system. Currently, the KeepRandomBeaconOperator includes an onlyServiceContract modifier:  keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L150-L159  /**  @dev Checks if sender is authorized.  /  modifier onlyServiceContract() {  require(  serviceContracts.contains(msg.sender),  \"Caller is not an authorized contract\"  );  _;  The functions it protects cannot be targeted by the aforementioned service contract callbacks due to Solidity s CALLDATASIZE checking. However, the presence of the modifier suggests that the service contract is expected to be a permissioned actor within some contracts.  Recommendation  Stick to a constant callback method signature, rather than allowing users to submit an arbitrary string. An example is __beaconCallback__(uint256).  Consider disallowing arbitrary callback destinations. Instead, rely on contracts making requests directly, and default the callback destination to msg.sender. Ensure the sender is not an EOA.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.19 tbtc - Disallow signatures with high-s values in DepositRedemption.provideRedemptionSignature    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#518  Description  DepositRedemption.provideRedemptionSignature is used by signers to publish a signature that can be used to redeem a deposit on Bitcoin. The function accepts a signature s value in the upper half of the secp256k1 curve:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L183-L202  function provideRedemptionSignature(  DepositUtils.Deposit storage _d,  uint8 _v,  bytes32 _r,  bytes32 _s  ) public {  require(_d.inAwaitingWithdrawalSignature(), \"Not currently awaiting a signature\");  // If we're outside of the signature window, we COULD punish signers here  // Instead, we consider this a no-harm-no-foul situation.  // The signers have not stolen funds. Most likely they've just inconvenienced someone  // The signature must be valid on the pubkey  require(  _d.signerPubkey().checkSig(  _d.lastRequestedDigest,  _v, _r, _s  ),  \"Invalid signature\"  );  Although ecrecover accepts signatures with these s values, they are no longer used in Bitcoin. As such, the signature will appear to be valid to the Ethereum smart contract, but will likely not be accepted on Bitcoin. If no users watching malleate the signature, the redemption process will likely enter a fee increase loop, incurring a cost on the deposit owner.  Recommendation  Ensure the passed-in s value is restricted to the lower half of the secp256k1 curve, as done in BondedECDSAKeep:  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L333-L340  // Validate `s` value for a malleability concern described in EIP-2.  // Only signatures with `s` value in the lower half of the secp256k1  // curve's order are considered valid.  require(  uint256(_s) <=  0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0,  \"Malleable signature - s should be in the low half of secp256k1 curve's order\"  );  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.20 Consistent use of SafeERC20 for external tokens    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-core/issues/1407 and  https://github.com/keep-network/keep-tecdsa/issues/272.  Description  Use SafeERC20 features to interact with potentially broken tokens used in the system. E.g. TokenGrant.receiveApproval() is using safeTransferFrom while other contracts aren t.  Examples  TokenGrant.receiveApproval using safeTransferFrom  keep-core/contracts/solidity/contracts/TokenGrant.sol:L200-L200  token.safeTransferFrom(_from, address(this), _amount);  TokenStaking.receiveApproval not using safeTransferFrom while safeTransfer is being used.  keep-core/contracts/solidity/contracts/TokenStaking.sol:L75-L75  token.transferFrom(_from, address(this), _value);  keep-core/contracts/solidity/contracts/TokenStaking.sol:L103-L103  token.safeTransfer(owner, amount);  keep-core/contracts/solidity/contracts/TokenStaking.sol:L193-L193  token.transfer(tattletale, tattletaleReward);  distributeERC20ToMembers not using safeTransferFrom  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L459-L463  token.transferFrom(  msg.sender,  tokenStaking.magpieOf(members[i]),  dividend  );  Recommendation  Consistently use SafeERC20 to support potentially broken tokens external to the system.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.21 Initialize implementations for proxy contracts and protect initialization methods    Addressed", "body": "  Resolution   This issue is addressed with the following changesets that ensure that the logic contracts cannot be used by other parties by initializing them in the constructor:   https://github.com/keep-network/keep-tecdsa/issues/297,  https://github.com/keep-network/keep-core/issues/1424, and  https://github.com/keep-network/tbtc/issues/500.  Description  It should be avoided that the implementation for proxy contracts can be initialized by third parties. This can be the case if the initialize function is unprotected. Since the implementation contract is not meant to be used directly without a proxy delegate-calling it is recommended to protect the initialization method of the implementation by initializing on deployment.  Changing the proxies implementation (upgradeTo()) to a version that does not protect the initialization method may allow someone to front-run and initialize the contract if it is not done within the same transaction.  Examples  KeepVendor delegates to KeepVendorImplV1. The implementations initialization method is unprotected.  keep-tecdsa/solidity/contracts/BondedECDSAKeepVendorImplV1.sol:L22-L32  /// @notice Initializes Keep Vendor contract implementation.  /// @param registryAddress Keep registry contract linked to this contract.  function initialize(  address registryAddress  public  require(!initialized(), \"Contract is already initialized.\");  _initialized[\"BondedECDSAKeepVendorImplV1\"] = true;  registry = Registry(registryAddress);  KeepRandomBeaconServiceImplV1 and KeepRandomBeaconServiceUpgradeExample  keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L118-L137  function initialize(  uint256 priceFeedEstimate,  uint256 fluctuationMargin,  uint256 dkgContributionMargin,  uint256 withdrawalDelay,  address registry  public  require(!initialized(), \"Contract is already initialized.\");  _initialized[\"KeepRandomBeaconServiceImplV1\"] = true;  _priceFeedEstimate = priceFeedEstimate;  _fluctuationMargin = fluctuationMargin;  _dkgContributionMargin = dkgContributionMargin;  _withdrawalDelay = withdrawalDelay;  _pendingWithdrawal = 0;  _previousEntry = _beaconSeed;  _registry = registry;  _baseCallbackGas = 18845;  Deposit is deployed via cloneFactory delegating to a masterDepositAddress in DepositFactory. The masterDepositAddress (Deposit) might be left uninitialized.  tbtc/implementation/contracts/system/DepositFactoryAuthority.sol:L3-L14  contract DepositFactoryAuthority {  bool internal _initialized = false;  address internal _depositFactory;  /// @notice Set the address of the System contract on contract initialization  function initialize(address _factory) public {  require(! _initialized, \"Factory can only be initialized once.\");  _depositFactory = _factory;  _initialized = true;  Recommendation  Initialize unprotected implementation contracts in the implementation s constructor. Protect initialization methods from being called by unauthorized parties or ensure that deployment of the proxy and initialization is performed in the same transaction.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.22 keep-tecdsa - If caller sends more than is contained in the signer subsidy pool, the value is burned    Addressed", "body": "  Resolution   Issue addressed in   keep-network/keep-ecdsa#306. The  Description  The signer subsidy pool in BondedECDSAKeepFactory tracks funds sent to the contract. Each time a keep is opened, the subsidy pool is intended to be distributed to the members of the new keep:  keep-tecdsa/solidity/contracts/BondedECDSAKeepFactory.sol:L312-L320  // If subsidy pool is non-empty, distribute the value to signers but  // never distribute more than the payment for opening a keep.  uint256 signerSubsidy = subsidyPool < msg.value  ? subsidyPool  : msg.value;  if (signerSubsidy > 0) {  subsidyPool -= signerSubsidy;  keep.distributeETHToMembers.value(signerSubsidy)();  The tracking around subsidy pool increases is inconsistent, and can lead to sent value being burned. In the case that subsidyPool contains less Ether than is sent in msg.value, msg.value is unused and remains in the contract. It may or may not be added to subsidyPool, depending on the return status of the random beacon:  keep-tecdsa/solidity/contracts/BondedECDSAKeepFactory.sol:L347-L357  (bool success, ) = address(randomBeacon).call.gas(400000).value(msg.value)(  abi.encodeWithSignature(  \"requestRelayEntry(address,string,uint256)\",  address(this),  \"setGroupSelectionSeed(uint256)\",  callbackGas  );  if (!success) {  subsidyPool += msg.value; // beacon is busy  Recommendation  Rather than tracking the subsidyPool individually, simply distribute this.balance to each new keep s members.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.23 keep-core - TokenGrant and TokenStaking allow staking zero amount of tokens and front-running    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-core/issues/1425 and  keep-network/keep-core#1461 by requiring a hardcoded minimum amount of tokens to be staked.  Description  Tokens are staked via the callback receiveApproval() which is normally invoked when calling approveAndCall(). The method is not restricting who can initiate the staking of tokens and relies on the fact that the token transfer to the TokenStaking contract is pre-approved by the owner, otherwise, the call would revert.  However, receiveApproval() allows the staking of a zero amount of tokens. The only check performed on the number of tokens transferred is, that the token holders balance covers the amount to be transferred. This check is both relatively weak - having enough balance does not imply that tokens are approved for transfer - and does not cover the fact that someone can call the method with a zero amount of tokens.  This way someone could create an arbitrary number of operators staking no tokens at all. This passes the token balance check, token.transferFrom() will succeed and an operator struct with a zero stake and arbitrary values for operator, from, magpie, authorizer can be set. Finally, an event is emitted for a zero stake.  An attacker could front-run calls to receiveApproval to block staking of a legitimate operator by creating a zero stake entry for the operator before she is able to. This vector might allow someone to permanently inconvenience an operator s address. To recover from this situation one could be forced to cancelStake terminating the zero stake struct in order to call the contract with the correct stake again.  The same issue exists for TokenGrant.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L54-L81  /**  @notice Receives approval of token transfer and stakes the approved amount.  @dev Makes sure provided token contract is the same one linked to this contract.  @param _from The owner of the tokens who approved them to transfer.  @param _value Approved amount for the transfer and stake.  @param _token Token contract address.  @param _extraData Data for stake delegation. This byte array must have the  following values concatenated: Magpie address (20 bytes) where the rewards for participation  are sent, operator's (20 bytes) address, authorizer (20 bytes) address.  /  function receiveApproval(address _from, uint256 _value, address _token, bytes memory _extraData) public {  require(ERC20Burnable(_token) == token, \"Token contract must be the same one linked to this contract.\");  require(_value <= token.balanceOf(_from), \"Sender must have enough tokens.\");  require(_extraData.length == 60, \"Stake delegation data must be provided.\");  address payable magpie = address(uint160(_extraData.toAddress(0)));  address operator = _extraData.toAddress(20);  require(operators[operator].owner == address(0), \"Operator address is already in use.\");  address authorizer = _extraData.toAddress(40);  // Transfer tokens to this contract.  token.transferFrom(_from, address(this), _value);  operators[operator] = Operator(_value, block.number, 0, _from, magpie, authorizer);  ownerOperators[_from].push(operator);  emit Staked(operator, _value);  Recommendation  Require tokens to be staked and explicitly disallow the zero amount of tokens case. The balance check can be removed.  Note: Consider checking the calls return value or calling the contract via SafeERC20 to support potentially broken tokens that do not revert in error cases (token.transferFrom).  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.24 tbtc - Inconsistency between increaseRedemptionFee and provideRedemptionProof may create un-provable redemptions    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#522  Description  DepositRedemption.increaseRedemptionFee is used by signers to approve a signable bitcoin transaction with a higher fee, in case the network is congested and miners are not approving the lower-fee transaction.  Fee increases can be performed every 4 hours:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L225  require(block.timestamp >= _d.withdrawalRequestTime + TBTCConstants.getIncreaseFeeTimer(), \"Fee increase not yet permitted\");  In addition, each increase must increment the fee by exactly the initial proposed fee:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L260-L263  // Check that we're incrementing the fee by exactly the redeemer's initial fee  uint256 _previousOutputValue = DepositUtils.bytes8LEToUint(_previousOutputValueBytes);  _newOutputValue = DepositUtils.bytes8LEToUint(_newOutputValueBytes);  require(_previousOutputValue.sub(_newOutputValue) == _d.initialRedemptionFee, \"Not an allowed fee step\");  Outside of these two restrictions, there is no limit to the number of times increaseRedemptionFee can be called. Over a 20-hour period, for example, increaseRedemptionFee could be called 5 times, increasing the fee to initialRedemptionFee * 5. Over a 24-hour period, increaseRedemptionFee could be called 6 times, increasing the fee to initialRedemptionFee * 6.  Eventually, it is expected that a transaction will be submitted and mined. At this point, anyone can call DepositRedemption.provideRedemptionProof, finalizing the redemption process and rewarding the signers. However, provideRedemptionProof will fail if the transaction fee is too high:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L308  require((_d.utxoSize().sub(_fundingOutputValue)) <= _d.initialRedemptionFee * 5, \"Fee unexpectedly very high\");  In the case that increaseRedemptionFee is called 6 times and the signers provide a signature for this transaction, the transaction can be submitted and mined but provideRedemptionProof for this will always fail. Eventually, a redemption proof timeout will trigger the deposit into liquidation and the signers will be punished.  Recommendation  Because it is difficult to say with certainty that a 5x fee increase will always ensure a transaction s redeemability, the upper bound on fee bumps should be removed from provideRedemptionProof.  This should be implemented in tandem with issue 5.37, so that signers cannot provide a proof that bypasses increaseRedemptionFee flow to spend the highest fee possible.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.25 keep-tecdsa - keep cannot be closed if a members bond was seized or fully reassigned    Addressed", "body": "  Description  A keep cannot be closed if the bonds have been completely reassigned or seized before, leaving at least one member with zero lockedBonds. In this case closeKeep() will throw in freeMembersBonds() because the requirement in keepBonding.freeBond is not satisfied anymore (lockedBonds[bondID] > 0). As a result of this, none of the potentially remaining bonds (reassign) are freed, the keep stays active even though it should be closed.  Examples  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L373-L396  /// @notice Closes keep when owner decides that they no longer need it.  /// Releases bonds to the keep members. Keep can be closed only when  /// there is no signing in progress or requested signing process has timed out.  /// @dev The function can be called by the owner of the keep and only is the  /// keep has not been closed already.  function closeKeep() external onlyOwner onlyWhenActive {  require(  !isSigningInProgress() || hasSigningTimedOut(),  \"Requested signing has not timed out yet\"  );  isActive = false;  freeMembersBonds();  emit KeepClosed();  /// @notice Returns bonds to the keep members.  function freeMembersBonds() internal {  for (uint256 i = 0; i < members.length; i++) {  keepBonding.freeBond(members[i], uint256(address(this)));  keep-tecdsa/solidity/contracts/KeepBonding.sol:L173-L190  /// @notice Releases the bond and moves the bond value to the operator's  /// unbounded value pool.  /// @dev Function requires that caller is the holder of the bond which is  /// being released.  /// @param operator Address of the bonded operator.  /// @param referenceID Reference ID of the bond.  function freeBond(address operator, uint256 referenceID) public {  address holder = msg.sender;  bytes32 bondID = keccak256(  abi.encodePacked(operator, holder, referenceID)  );  require(lockedBonds[bondID] > 0, \"Bond not found\");  uint256 amount = lockedBonds[bondID];  lockedBonds[bondID] = 0;  unbondedValue[operator] = amount;  Recommendation  Make sure the keep can be set to an end-state (closed/inactive) indicating its end-of-life even if the bond has been seized before. Avoid throwing an exception when freeing member bonds to avoid blocking the unlocking of bonds.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.26 tbtc - provideFundingECDSAFraudProof attempts to burn non-existent funds    Addressed", "body": "  Resolution   Addressed as   https://github.com/keep-network/tbtc/issues/502 and fixed with  keep-network/tbtc#523.  Description  The funding flow was recently changed from requiring the funder to provide a bond that stays in the Deposit contract to forwarding the funds to the keep, paying for the keep setup.  So at a high level, the funding bond was designed to ensure that funders had some minimum skin in the game, so that DoSing signers/the system was expensive. The upside was that we could refund it in happy paths. Now that we ve realized that opening the keep itself will cost enough to prevent DoS, the concept of refunding goes away entirely. We definitely missed cleaning up the funder handling in provideFundingECDSAFraudProof though.  Examples  tbtc/implementation/contracts/deposit/DepositFunding.sol:L170-L173  // If the funding timeout has elapsed, punish the funder too!  if (block.timestamp > _d.fundingProofTimerStart + TBTCConstants.getFundingTimeout()) {  address(0).transfer(address(this).balance);  // Burn it all down (fire emoji)  _d.setFailedSetup();  Recommendation  Remove the line that attempts to punish the funder by burning the Deposit contract balance which is zero due to recent changes in how the payment provided with createNewDepositis handled.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.27 bitcoin-spv - Bitcoin output script length is not checked in wpkhSpendSighash   ", "body": "  Resolution   Summa opted not to make this change. See   https://github.com/summa-tx/bitcoin-spv/issues/112 for details.  Description  CheckBitcoinSigs.wpkhSpendSighash calculates the sighash of a Bitcoin transaction. Among its parameters, it accepts bytes memory _outpoint, which is a 36-byte UTXO id consisting of a 32-byte transaction hash and a 4-byte output index.  The function in question should not accept an _outpoint that is not 36-bytes, but no length check is made:  bitcoin-spv/solidity/contracts/CheckBitcoinSigs.sol:L130-L159  function wpkhSpendSighash(  bytes memory _outpoint,  // 36 byte UTXO id  bytes20 _inputPKH,       // 20 byte hash160  bytes8 _inputValue,      // 8-byte LE  bytes8 _outputValue,     // 8-byte LE  bytes memory _outputScript    // lenght-prefixed output script  ) internal pure returns (bytes32) {  // Fixes elements to easily make a 1-in 1-out sighash digest  // Does not support timelocks  bytes memory _scriptCode = abi.encodePacked(  hex\"1976a914\",  // length, dup, hash160, pkh_length  _inputPKH,  hex\"88ac\");  // equal, checksig  bytes32 _hashOutputs = abi.encodePacked(  _outputValue,  // 8-byte LE  _outputScript).hash256();  bytes memory _sighashPreimage = abi.encodePacked(  hex\"01000000\",  // version  _outpoint.hash256(),  // hashPrevouts  hex\"8cb9012517c817fead650287d61bdd9c68803b6bf9c64133dcab3e65b5a50cb9\",  // hashSequence(00000000)  _outpoint,  // outpoint  _scriptCode,  // p2wpkh script code  _inputValue,  // value of the input in 8-byte LE  hex\"00000000\",  // input nSequence  _hashOutputs,  // hash of the single output  hex\"00000000\",  // nLockTime  hex\"01000000\"  // SIGHASH_ALL  );  return _sighashPreimage.hash256();  Recommendation  Check that _outpoint.length is 36.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.28 tbtc - liquidationInitiator can block purchaseSignerBondsAtAuction indefinitely    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/503 and commits from  keep-network/tbtc#524 switching from  Description  When reporting a fraudulent proof the deposits liquidationInitiator is set to the entity reporting and proofing the fraud. The deposit that is in a *_liquidation_in_progress state can be bought by anyone at an auction calling purchaseSignerBondsAtAuction.  Instead of receiving a share of the funds the liquidationInitiator can decide to intentionally reject the funds by raising an exception causing initiator.transfer(contractEthBalance) to throw, blocking the auction and forcing the liquidation to fail. The deposit will stay in one of the *_liquidation_in_progress states.  Examples  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L224-L276  /// @notice     Closes an auction and purchases the signer bonds. Payout to buyer, funder, then signers if not fraud  /// @dev        For interface, reading auctionValue will give a past value. the current is better  /// @param  _d  deposit storage pointer  function purchaseSignerBondsAtAuction(DepositUtils.Deposit storage _d) public {  bool _wasFraud = _d.inFraudLiquidationInProgress();  require(_d.inSignerLiquidation(), \"No active auction\");  _d.setLiquidated();  _d.logLiquidated();  // send the TBTC to the TDT holder. If the TDT holder is the Vending Machine, burn it to maintain the peg.  address tdtHolder = _d.depositOwner();  TBTCToken _tbtcToken = TBTCToken(_d.TBTCToken);  uint256 lotSizeTbtc = _d.lotSizeTbtc();  require(_tbtcToken.balanceOf(msg.sender) >= lotSizeTbtc, \"Not enough TBTC to cover outstanding debt\");  if(tdtHolder == _d.VendingMachine){  _tbtcToken.burnFrom(msg.sender, lotSizeTbtc);  // burn minimal amount to cover size  else{  _tbtcToken.transferFrom(msg.sender, tdtHolder, lotSizeTbtc);  // Distribute funds to auction buyer  uint256 _valueToDistribute = _d.auctionValue();  msg.sender.transfer(_valueToDistribute);  // Send any TBTC left to the Fee Rebate Token holder  _d.distributeFeeRebate();  // For fraud, pay remainder to the liquidation initiator.  // For non-fraud, split 50-50 between initiator and signers. if the transfer amount is 1,  // division will yield a 0 value which causes a revert; instead,  // we simply ignore such a tiny amount and leave some wei dust in escrow  uint256 contractEthBalance = address(this).balance;  address payable initiator = _d.liquidationInitiator;  if (initiator == address(0)){  initiator = address(0xdead);  if (contractEthBalance > 1) {  if (_wasFraud) {  initiator.transfer(contractEthBalance);  } else {  // There will always be a liquidation initiator.  uint256 split = contractEthBalance.div(2);  _d.pushFundsToKeepGroup(split);  initiator.transfer(split);  Recommendation  Use a pull vs push funds pattern or use address.send instead of address.transfer which might leave some funds locked in the contract if it fails.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.29 bitcoin-spv - verifyHash256Merkle allows existence proofs for the same leaf in multiple locations in the tree   ", "body": "  Resolution   Summa opted not to make this change, citing inconsistencies in Bitcoin s merkle implementation. See   https://github.com/summa-tx/bitcoin-spv/issues/108 for details.  Description  BTCUtils.verifyHash256Merkle is used by ValidateSPV.prove to validate a transaction s existence in a Bitcoin block. The function accepts as input a _proof and an _index. The _proof consists of, in order: the transaction hash, a list of intermediate nodes, and the merkle root.  The proof is performed iteratively, and uses the _index to determine whether the next proof element represents a  left branch  or a  right branch:   bitcoin-spv/solidity/contracts/BTCUtils.sol:L574-L586  uint _idx = _index;  bytes32 _root = _proof.slice(_proof.length - 32, 32).toBytes32();  bytes32 _current = _proof.slice(0, 32).toBytes32();  for (uint i = 1; i < (_proof.length.div(32)) - 1; i++) {  if (_idx % 2 == 1) {  _current = _hash256MerkleStep(_proof.slice(i * 32, 32), abi.encodePacked(_current));  } else {  _current = _hash256MerkleStep(abi.encodePacked(_current), _proof.slice(i * 32, 32));  _idx = _idx >> 1;  return _current == _root;  If _idx is even, the computed hash is placed before the next proof element. If _idx is odd, the computed hash is placed after the next proof element. After each iteration, _idx is decremented by _idx /= 2.  Because verifyHash256Merkle makes no requirements on the size of _proof relative to _index, it is possible to pass in invalid values for _index that prove a transaction s existence in multiple locations in the tree.  Examples  By modifying existing tests, we showed that any transaction can be proven to exist at least one alternate index. This alternate index is calculated as (2 ** treeHeight) + prevIndex - though other alternate indices are possible. The modified test is below:  Recommendation  Use the length of _proof to determine the maximum allowed _index. _index should satisfy the following criterion: _index < 2 ** (_proof.length.div(32) - 2).  Note that subtraction by 2 accounts for the transaction hash and merkle root, which are assumed to be encoded in the proof along with the intermediate nodes.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.30 keep-core - stake operator should not be eligible if undelegatedAt is set    Addressed", "body": "  Resolution  Addressed with https://github.com/keep-network/keep-core/issues/1433 by enforcing that stake must be canceled in initialization period.  undelegatedAt is intended to support undelegation in advance at any given time. Whether we do < or <= is not actually significant, as transaction reordering also means ability to include/not include transactions arbitrarily, but changing the check to operator.UndelegatedAt == 0 would ruin e.g. the use-case where Alice wants to delegate to Bob for 12 months. If we don t currently need that use-case, the check can be simplified to == 0.  Description  An operator s stake should not be eligible if they stake an amount and immediately call undelegate in an attempt to indicate that they are going to recover their stake soon.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L232-L236  bool notUndelegated = block.number <= operator.undelegatedAt || operator.undelegatedAt == 0;  if (isAuthorized && isActive && notUndelegated) {  balance = operator.amount;  Recommendation  A stake that is entering undelegation is indicated by operator.undelegatedAt being non-zero. Change the notUndelegated check block.number <= operator.undelegatedAt || operator.undelegatedAt == 0 to operator.undelegatedAT == 0 as any value being set indicates that undelegation is in progress.  Enforce that within the initialization period stake is canceled instead of being undelegated.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.31 keep-core - Specification inconsistency: TokenStaking amount to be slashed/seized    Addressed", "body": "  Resolution   Partially addressed with   https://github.com/keep-network/keep-core/issues/1428 by ensuring that at least some stack is slashed. As noted in the issue, the case where less than the minimum stake was slashed from an operator is left unhandled with this fix.  Description  The keep specification states that slash and seize affect at least the amount specified or the remaining stake of a member.  Slash each operator in the list misbehavers by the specified amount (or their remaining stake, whichever is lower).  Punish each operator in the list misbehavers by the specified amount or their remaining stake.  The implementation, however, bails if one of the accounts does not have enough stake to be slashed or seized because of the use of SafeMath.sub(). This behavior is inconsistent with the specification which states that min(amount, misbehaver.stake) stake should be affected. The call to slash/seize will revert and no stakes are affected. At max, the staked amount of the lowest staker can be slashed/seized from every staker.  Implementing this method as stated in the specification using min(amount, misbehaver.stake) will cover the fact that slashing/seizing was only partially successful. If misbehaver.stake is zero no error might be emitted even though no stake was slashed/seized.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L151-L195  /**  @dev Slash provided token amount from every member in the misbehaved  operators array and burn 100% of all the tokens.  @param amount Token amount to slash from every misbehaved operator.  @param misbehavedOperators Array of addresses to seize the tokens from.  /  function slash(uint256 amount, address[] memory misbehavedOperators)  public  onlyApprovedOperatorContract(msg.sender) {  for (uint i = 0; i < misbehavedOperators.length; i++) {  address operator = misbehavedOperators[i];  require(authorizations[msg.sender][operator], \"Not authorized\");  operators[operator].amount = operators[operator].amount.sub(amount);  token.burn(misbehavedOperators.length.mul(amount));  /**  @dev Seize provided token amount from every member in the misbehaved  operators array. The tattletale is rewarded with 5% of the total seized  amount scaled by the reward adjustment parameter and the rest 95% is burned.  @param amount Token amount to seize from every misbehaved operator.  @param rewardMultiplier Reward adjustment in percentage. Min 1% and 100% max.  @param tattletale Address to receive the 5% reward.  @param misbehavedOperators Array of addresses to seize the tokens from.  /  function seize(  uint256 amount,  uint256 rewardMultiplier,  address tattletale,  address[] memory misbehavedOperators  ) public onlyApprovedOperatorContract(msg.sender) {  for (uint i = 0; i < misbehavedOperators.length; i++) {  address operator = misbehavedOperators[i];  require(authorizations[msg.sender][operator], \"Not authorized\");  operators[operator].amount = operators[operator].amount.sub(amount);  uint256 total = misbehavedOperators.length.mul(amount);  uint256 tattletaleReward = (total.mul(5).div(100)).mul(rewardMultiplier).div(100);  token.transfer(tattletale, tattletaleReward);  token.burn(total.sub(tattletaleReward));  Recommendation  Require that minimumStake has been provided and can be seized/slashed. Update the documentation to reflect the fact that the solution always seizes/slashes minimumStake. Ensure that stakers cannot cancel their stake while they are actively participating in the network.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.32 keep-tecdsa - Change state-mutability of checkSignatureFraud to view    Addressed", "body": "  Resolution   Addressed as part of   https://github.com/keep-network/keep-tecdsa/issues/254 with commits from  keep-network/keep-tecdsa#283 splitting the method into two parts:  Description  BondedECDSAKeep.sol.submitSignatureFraud is not state-changing and should, therefore, be declared with the function state-mutability view.  Examples  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L265-L290  function submitSignatureFraud(  uint8 _v,  bytes32 _r,  bytes32 _s,  bytes32 _signedDigest,  bytes calldata _preimage  ) external returns (bool _isFraud) {  require(publicKey.length != 0, \"Public key was not set yet\");  bytes32 calculatedDigest = sha256(_preimage);  require(  _signedDigest == calculatedDigest,  \"Signed digest does not match double sha256 hash of the preimage\"  );  bool isSignatureValid = publicKeyToAddress(publicKey) ==  ecrecover(_signedDigest, _v, _r, _s);  // Check if the signature is valid but was not requested.  require(  isSignatureValid && !digests[_signedDigest],  \"Signature is not fraudulent\"  );  return true;  Recommendation  Declare method as view. Consider renaming submitSignatureFraud to e.g. checkSignatureFraud to emphasize that it is only checking the signature and not actually changing state.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.33 keep-core - Specification inconsistency: TokenStaking.slash() is never called    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-tecdsa/issues/254 and changesets from  keep-network/keep-tecdsa#283 by slashing the signer stakes when signature fraud is proven.  Description  According to the keep specification stake should be slashed if a staker violates the protocol:  Slashing If a staker violates the protocol of an operation in a way which can be proven on-chain, they will be penalized by having their stakes slashed.  While this functionality can only be called by the approved operator contract, it is not being used throughout the system. In contrast seize() is being called when reporting unauthorized signing or relay entry timeout.  Examples  keep-core/contracts/solidity/contracts/TokenStaking.sol:L151-L167  /**  @dev Slash provided token amount from every member in the misbehaved  operators array and burn 100% of all the tokens.  @param amount Token amount to slash from every misbehaved operator.  @param misbehavedOperators Array of addresses to seize the tokens from.  /  function slash(uint256 amount, address[] memory misbehavedOperators)  public  onlyApprovedOperatorContract(msg.sender) {  for (uint i = 0; i < misbehavedOperators.length; i++) {  address operator = misbehavedOperators[i];  require(authorizations[msg.sender][operator], \"Not authorized\");  operators[operator].amount = operators[operator].amount.sub(amount);  token.burn(misbehavedOperators.length.mul(amount));  Recommendation  Implement slashing according to the specification.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.34 tbtc - Remove notifyDepositExpiryCourtesyCall and allow exitCourtesyCall exiting the courtesy call at term    Addressed", "body": "  Resolution   Addressed with   keep-network/tbtc#476 following the recommendation.  Description  Following a deep dive into state transitions with the client it was agreed that notifyDepositExpiryCourtesyCall should be removed from the system as it is a left-over of a previous version of the deposit contract.  Additionally, exitCourtesyCall should be callable at any time.  Examples  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L289-L298  /// @notice     Goes from courtesy call to active  /// @dev        Only callable if collateral is sufficient and the deposit is not expiring  /// @param  _d  deposit storage pointer  function exitCourtesyCall(DepositUtils.Deposit storage _d) public {  require(_d.inCourtesyCall(), \"Not currently in courtesy call\");  require(block.timestamp <= _d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit is expiring\");  require(getCollateralizationPercentage(_d) >= _d.undercollateralizedThresholdPercent, \"Deposit is still undercollateralized\");  _d.setActive();  _d.logExitedCourtesyCall();  Recommendation  Remove the notifyDepositExpiryCourtesyCall state transition and remove the requirement on exitCourtesyCall being callable only before the deposit expires.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.35 keep-tecdsa - withdraw should check for zero value transfer    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/keep-tecdsa/issues/280 by denying zero value withdrawals.  Description  Requesting the withdrawal of zero ETH in KeepBonding.withdraw should fail as this would allow the method to succeed, calling the user-provided destination even though the sender has no unbonded value.  Examples  keep-tecdsa/solidity/contracts/KeepBonding.sol:L78-L88  function withdraw(uint256 amount, address payable destination) public {  require(  unbondedValue[msg.sender] >= amount,  \"Insufficient unbonded value\"  );  unbondedValue[msg.sender] -= amount;  (bool success, ) = destination.call.value(amount)(\"\");  require(success, \"Transfer failed\");  And a similar instance in BondedECDSAKeep:  keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L487-L498  /// @notice Withdraws amount of ether hold in the keep for the member.  /// The value is sent to the beneficiary of the specific member.  /// @param _member Keep member address.  function withdraw(address _member) external {  uint256 value = memberETHBalances[_member];  memberETHBalances[_member] = 0;  /* solium-disable-next-line security/no-call-value */  (bool success, ) = tokenStaking.magpieOf(_member).call.value(value)(\"\");  require(success, \"Transfer failed\");  Recommendation  Require that the amount to be withdrawn is greater than zero.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.36 keep-core - TokenStaking owner should be protected from slash() and seize() during initializationPeriod    Addressed", "body": "  Resolution   Addressed by   https://github.com/keep-network/keep-core/issues/1426 and fixed with  keep-network/keep-core#1453.  Description  From the specification:  Slashing If a staker violates the protocol of an operation in a way which can be proven on-chain, they will be penalized by having their stakes slashed.  The initialization period is a backoff time during which operator stakes are not active nor eligible to receive work. Since they cannot misbehave they should be protected from having their stake slashed or seized.  It should also be noted that slash() and seize() can be front-run during the initializationPeriod by having the operator owner cancel the deposit before it is being slashed or seized.  Recommendation  Require deposits to be in active state for being slashed or seized.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.37 tbtc - Signer collusion may bypass increaseRedemptionFee flow    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#522  Description  DepositRedemption.increaseRedemptionFee is used by signers to approve a signable bitcoin transaction with a higher fee, in case the network is congested and miners are not approving the lower-fee transaction.  Fee increases can be performed every 4 hours:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L225  require(block.timestamp >= _d.withdrawalRequestTime + TBTCConstants.getIncreaseFeeTimer(), \"Fee increase not yet permitted\");  In addition, each increase must increment the fee by exactly the initial proposed fee:  tbtc/implementation/contracts/deposit/DepositRedemption.sol:L260-L263  // Check that we're incrementing the fee by exactly the redeemer's initial fee  uint256 _previousOutputValue = DepositUtils.bytes8LEToUint(_previousOutputValueBytes);  _newOutputValue = DepositUtils.bytes8LEToUint(_newOutputValueBytes);  require(_previousOutputValue.sub(_newOutputValue) == _d.initialRedemptionFee, \"Not an allowed fee step\");  Outside of these two restrictions, there is no limit to the number of times increaseRedemptionFee can be called. Over a 20-hour period, for example, increaseRedemptionFee could be called 5 times, increasing the fee to initialRedemptionFee * 5.  Rather than calling increaseRedemptionFee 5 times over 20 hours, colluding signers may immediately create and sign a transaction with a fee of initialRedemptionFee * 5, wait for it to be mined, then submit it to provideRedemptionProof. Because provideRedemptionProof does not check that a transaction signature signs an approved digest, interested parties would need to monitor the bitcoin blockchain, notice the spend, and provide an ECDSA fraud proof before provideRedemptionProof is called.  Recommendation  Track the latest approved fee, and ensure the transaction in provideRedemptionProof does not include a higher fee.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.38 tbtc - liquidating a deposit does not send the complete remainder of the contract balance to recipients    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/504 and commits from  keep-network/tbtc#524, transferring the remaining balance of the contract to the initiator and switching from  Description  purchaseSignerBondsAtAuction might leave a wei in the contract if:  there is only one wei remaining in the contract  there is more than one wei remaining but the contract balance is odd.  Examples  contract balances must be > 1 wei otherwise no transfer is attempted  the division at line 271 floors the result if dividing an odd balance. The contract is sending floor(contract.balance / 2) to the keep group and liquidationInitiator leaving one 1 in the contract.  tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L266-L275  if (contractEthBalance > 1) {  if (_wasFraud) {  initiator.transfer(contractEthBalance);  } else {  // There will always be a liquidation initiator.  uint256 split = contractEthBalance.div(2);  _d.pushFundsToKeepGroup(split);  initiator.transfer(split);  Recommendation  Define a reasonable minimum amount when awarding the fraud reporter or liquidation initiator. Alternatively, always transfer the contract balance. When splitting the amount use the contract balance after the first transfer as the value being sent to the second recipient. Use the presence of locked funds in a contract as an error indicator unless funds were sent forcefully to the contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.39 tbtc - approveAndCall unused return parameter    Addressed", "body": "  Resolution   Addressed with   https://github.com/keep-network/tbtc/issues/505 by returning  Description  approveAndCall always returns false because the return value bool success is never set.  Examples  tbtc/implementation/contracts/system/TBTCDepositToken.sol:L42-L54  /// @notice           Set allowance for other address and notify.  ///                   Allows `_spender` to transfer the specified TDT  ///                   on your behalf and then ping the contract about it.  /// @dev              The `_spender` should implement the `tokenRecipient` interface below  ///                   to receive approval notifications.  /// @param _spender   Address of contract authorized to spend.  /// @param _tdtId     The TDT they can spend.  /// @param _extraData Extra information to send to the approved contract.  function approveAndCall(address _spender, uint256 _tdtId, bytes memory _extraData) public returns (bool success) {  tokenRecipient spender = tokenRecipient(_spender);  approve(_spender, _tdtId);  spender.receiveApproval(msg.sender, _tdtId, address(this), _extraData);  Recommendation  Return the correct success state.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.40 bitcoin-spv - Unnecessary memory allocation in BTCUtils   Pending", "body": "  Resolution  The client provided feedback that this issue is not scheduled to be addressed.  Description  BTCUtils makes liberal use of BytesLib.slice, which returns a freshly-allocated slice of an existing bytes array. In many cases, the desired behavior is simply to read a 32-byte slice of a byte array. As a result, the typical pattern used is: bytesVar.slice(start, start + 32).toBytes32().  This pattern introduces unnecessary complexity and memory allocation in a critically important library: cloning a portion of the array, storing that clone in memory, and then reading it from memory. A simpler alternative would be to implement BytesLib.readBytes32(bytes _b, uint _idx) and other  memory-read  functions.  Rather than moving the free memory pointer and redundantly reading, storing, then re-reading memory, readBytes32 and similar functions would perform a simple length check and mload directly from the desired index in the array.  Examples  extractInputTxIdLE:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L254-L260  /// @notice          Extracts the outpoint tx id from an input  /// @dev             32 byte tx id  /// @param _input    The input  /// @return          The tx id (little-endian bytes)  function extractInputTxIdLE(bytes memory _input) internal pure returns (bytes32) {  return _input.slice(0, 32).toBytes32();  verifyHash256Merkle:  bitcoin-spv/solidity/contracts/BTCUtils.sol:L574-L586  uint _idx = _index;  bytes32 _root = _proof.slice(_proof.length - 32, 32).toBytes32();  bytes32 _current = _proof.slice(0, 32).toBytes32();  for (uint i = 1; i < (_proof.length.div(32)) - 1; i++) {  if (_idx % 2 == 1) {  _current = _hash256MerkleStep(_proof.slice(i * 32, 32), abi.encodePacked(_current));  } else {  _current = _hash256MerkleStep(abi.encodePacked(_current), _proof.slice(i * 32, 32));  _idx = _idx >> 1;  return _current == _root;  Recommendation  Implement BytesLib.readBytes32 and favor its use over the bytesVar.slice(start, start + 32).toBytes32() pattern. Implement other memory-read functions where possible, and avoid the use of slice.  Note, too, that implementing this change in verifyHash256Merkle would allow _hash256MerkleStep to accept 2 bytes32 inputs (rather than bytes), removing additional unnecessary casting and memory allocation.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.41 bitcoin-spv - ValidateSPV.validateHeaderChain does not completely validate input   ", "body": "  Resolution   Summa opted not to make this change. See   https://github.com/summa-tx/bitcoin-spv/issues/111  Description  ValidateSPV.validateHeaderChain takes as input a sequence of Bitcoin headers and calculates the total accumulated difficulty across the entire sequence. The input headers are checked to ensure they are relatively well-formed:  bitcoin-spv/solidity/contracts/ValidateSPV.sol:L173-L174  // Check header chain length  if (_headers.length % 80 != 0) {return ERR_BAD_LENGTH;}  However, the function lacks a check for nonzero length of _headers. Although the total difficulty returned would be zero, an explicit check would make this more clear.  Recommendation  If headers.length is zero, return ERR_BAD_LENGTH  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.42 bitcoin-spv - unnecessary intermediate cast    Addressed", "body": "  Resolution   Issue addressed in   summa-tx/bitcoin-spv#123  Description  Examples  bitcoin-spv/solidity/contracts/CheckBitcoinSigs.sol:L15-L25  /// @notice          Derives an Ethereum Account address from a pubkey  /// @dev             The address is the last 20 bytes of the keccak256 of the address  /// @param _pubkey   The public key X & Y. Unprefixed, as a 64-byte array  /// @return          The account address  function accountFromPubkey(bytes memory _pubkey) internal pure returns (address) {  require(_pubkey.length == 64, \"Pubkey must be 64-byte raw, uncompressed key.\");  // keccak hash of uncompressed unprefixed pubkey  bytes32 _digest = keccak256(_pubkey);  return address(uint160(uint256(_digest)));  Recommendation  The intermediate cast from uint256 to uint160 can be omitted. Refactor to return address(uint256(_digest)) instead.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.43 bitcoin-spv - unnecessary logic in BytesLib.toBytes32()    Addressed", "body": "  Resolution   Issue addressed in   summa-tx/bitcoin-spv#125  Description  The heavily used library function BytesLib.toBytes32() unnecessarily casts _source to bytes (same type) and creates a copy of the dynamic byte array to check it s length, while this can be done directly on the user-provided bytes _source.  Examples  bitcoin-spv/solidity/contracts/BytesLib.sol:L399-L408  function toBytes32(bytes memory _source) pure internal returns (bytes32 result) {  bytes memory tempEmptyStringTest = bytes(_source);  if (tempEmptyStringTest.length == 0) {  return 0x0;  assembly {  result := mload(add(_source, 32))  Recommendation  function toBytes32(bytes memory _source) pure internal returns (bytes32 result) {  if (_source.length == 0) {  return 0x0;  assembly {  result := mload(add(_source, 32))  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.44 bitcoin-spv - redundant functionality   ", "body": "  Resolution   Summa opted not to make this change. See   https://github.com/summa-tx/bitcoin-spv/issues/116 for details.  Description  The library exposes redundant implementations of bitcoins double sha256.  Examples  solidity native implementation with an overzealous type correction issue 5.45  bitcoin-spv/solidity/contracts/BTCUtils.sol:L110-L116  /// @notice          Implements bitcoin's hash256 (double sha2)  /// @dev             abi.encodePacked changes the return to bytes instead of bytes32  /// @param _b        The pre-image  /// @return          The digest  function hash256(bytes memory _b) internal pure returns (bytes32) {  return abi.encodePacked(sha256(abi.encodePacked(sha256(_b)))).toBytes32();  assembly implementation  Note this implementation does not handle errors when staticcall ing the precompiled sha256 contract (private chains).  bitcoin-spv/solidity/contracts/BTCUtils.sol:L118-L129  /// @notice          Implements bitcoin's hash256 (double sha2)  /// @dev             sha2 is precompiled smart contract located at address(2)  /// @param _b        The pre-image  /// @return          The digest  function hash256View(bytes memory _b) internal view returns (bytes32 res) {  assembly {  let ptr := mload(0x40)  pop(staticcall(gas, 2, add(_b, 32), mload(_b), ptr, 32))  pop(staticcall(gas, 2, ptr, 32, ptr, 32))  res := mload(ptr)  Recommendation  We recommend providing only one implementation for calculating the double sha256 as maintaining two interfaces for the same functionality is not desirable. Furthermore, even though the assembly implementation is saving gas, we recommend keeping the language provided implementation.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.45 bitcoin-spv - unnecessary type correction    Addressed", "body": "  Resolution   Issue addressed in   summa-tx/bitcoin-spv#126  Description  The type correction encodePacked().toBytes32() is not needed as sha256 already returns bytes32.  Examples  bitcoin-spv/solidity/contracts/BTCUtils.sol:L114-L117  function hash256(bytes memory _b) internal pure returns (bytes32) {  return abi.encodePacked(sha256(abi.encodePacked(sha256(_b)))).toBytes32();  Recommendation  Refactor to return sha256(abi.encodePacked(sha256(_b))); to save gas.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.46 tbtc - Restrict access to fallback function in Deposit.sol    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#526  Description  Deposit.sol has an empty, payable fallback function. It is unused except when seizing signer bonds from BondedECDSAKeep.  Recommendation  So that Ether is not accidentally sent to a Deposit, have the fallback revert if the sender is not the BondedECDSAKeep.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.47 tbtc - Where possible, a specific contract type should be used rather than address    Addressed", "body": "  Resolution   This issue has been addressed with   https://github.com/keep-network/tbtc/issues/507 and  keep-network/tbtc#542.  Description  Rather than storing addresses and then casting to the known contract type, it s better to use the best type available so the compiler can check for type safety.  Examples  tbtc/implementation/contracts/deposit/DepositUtils.sol:L25-L37  struct Deposit {  // SET DURING CONSTRUCTION  address TBTCSystem;  address TBTCToken;  address TBTCDepositToken;  address FeeRebateToken;  address VendingMachine;  uint256 lotSizeSatoshis;  uint8 currentState;  uint256 signerFeeDivisor;  uint128 undercollateralizedThresholdPercent;  uint128 severelyUndercollateralizedThresholdPercent;  tbtc/implementation/contracts/proxy/DepositFactory.sol:L16-L28  contract DepositFactory is CloneFactory, TBTCSystemAuthority{  // Holds the address of the deposit contract  // which will be used as a master contract for cloning.  address public masterDepositAddress;  address public tbtcSystem;  address public tbtcToken;  address public tbtcDepositToken;  address public feeRebateToken;  address public vendingMachine;  uint256 public keepThreshold;  uint256 public keepSize;  Remediation  Where possible, use more specific types instead of address. This goes for parameter types as well as state variable types.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.48 tbtc - Variable shadowing in DepositFactory    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#512  Description  DepositFactory inherits from TBTCSystemAuthority. Both contracts declare a state variable with the same name, tbtcSystem.  tbtc/implementation/contracts/proxy/DepositFactory.sol:L21  address public tbtcSystem;  Recommendation  Remove the shadowed variable.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.49 tbtc - Values may contain dirty lower-order bits   Pending", "body": "  Resolution   This is being tracked as   https://github.com/keep-network/tbtc/issues/557.  Description  Examples  FundingScript.receiveApproval:  tbtc/implementation/contracts/scripts/FundingScript.sol:L38-L44  // Verify _extraData is a call to unqualifiedDepositToTbtc.  bytes4 functionSignature;  assembly { functionSignature := mload(add(_extraData, 0x20)) }  require(  functionSignature == vendingMachine.unqualifiedDepositToTbtc.selector,  \"Bad _extraData signature. Call must be to unqualifiedDepositToTbtc.\"  );  RedemptionScript.receiveApproval:  tbtc/implementation/contracts/scripts/RedemptionScript.sol:L39-L45  // Verify _extraData is a call to tbtcToBtc.  bytes4 functionSignature;  assembly { functionSignature := mload(add(_extraData, 0x20)) }  require(  functionSignature == vendingMachine.tbtcToBtc.selector,  \"Bad _extraData signature. Call must be to tbtcToBtc.\"  );  Recommendation  Solidity truncates these unneeded bytes in the subsequent comparison operations, so there is no action required. However, this is good to keep in mind if these values are ever used for anything outside of strict comparison.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.50 tbtc - Revert error string may be malformed   Pending", "body": "  Resolution   This issue is being tracked as   https://github.com/keep-network/tbtc/issues/509.  Description  FundingScript handles an error from a call to VendingMachine like so.  tbtc/implementation/contracts/scripts/FundingScript.sol:L46-L52  // Call the VendingMachine.  // We could explictly encode the call to vending machine, but this would  // involve manually parsing _extraData and allocating variables.  (bool success, bytes memory returnData) = address(vendingMachine).call(  _extraData  );  require(success, string(returnData));  On a high-level revert, returnData will already include the typical  error selector . As FundingScript propagates this error message, it will add another error selector, which may make it difficult to read the error message.  The same issue is present in RedemptionScript:  tbtc/implementation/contracts/scripts/RedemptionScript.sol:L47-L52  (bool success, bytes memory returnData) = address(vendingMachine).call(_extraData);  // By default, `address.call`  will catch any revert messages.  // Converting the `returnData` to a string will effectively forward any revert messages.  // https://ethereum.stackexchange.com/questions/69133/forward-revert-message-from-low-level-solidity-call  // TODO: there's some noisy couple bytes at the beginning of the converted string, maybe the ABI-coded length?  require(success, string(returnData));  Recommendation  Rather than adding an assembly-level revert to the affected contracts, ensure nested error selectors are handled in external libraries.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.51 tbtc - Where possible, use constant rather than state variables    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#513  Description  TBTCSystem uses a state variable for pausedDuration, but this value is never changed.  tbtc/implementation/contracts/system/TBTCSystem.sol:L34  uint256 pausedDuration = 10 days;  Recommendation  Consider using the constant keyword.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.52 tbtc - Variable shadowing in TBTCDepositToken constructor    Addressed", "body": "  Resolution   Issue addressed in   keep-network/tbtc#512  Description  TBTCDepositToken inherits from DepositFactoryAuthority, which has a single state variable, _depositFactory. This variable is shadowed in the TBTCDepositToken constructor.  tbtc/implementation/contracts/system/TBTCDepositToken.sol:L21-L26  constructor(address _depositFactory)  ERC721Metadata(\"tBTC Deopsit Token\", \"TDT\")  DepositFactoryAuthority(_depositFactory)  public {  // solium-disable-previous-line no-empty-blocks  Recommendation  Rename the parameter or state variable.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/02/thesis-tbtc-and-keep/"}, {"title": "5.1 Eliminate assembly code by using ABI decode    ", "body": "  Resolution   All assembly code was replaced with proper use of   Description  There are several locations where assembly code is used to access and decode byte arrays (including uses inside loops). Even though assembly code was used for gas optimization, it reduces the readability (and future updatability) of the code.  Examples  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L39-L44  assembly {  flag := mload(add(_data, 32))  if (flag == CHANGE_PARTITION_FLAG) {  assembly {  toPartition := mload(add(_data, 64))  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L43-L44  assembly {  toPartition := mload(add(_data, 64))  Same code as above is also present here: /flexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1403 flexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1407  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1463-L1470  for (uint256 i = 116; i <= _operatorData.length; i = i + 32) {  bytes32 temp;  assembly {  temp := mload(add(_operatorData, i))  proof[index] = temp;  index++;  Recommendation  As discussed in the mid-audit meeting, it is a good solution to use ABI decode since all uses of assembly simply access 32-byte chunks of data from user input. This should eliminate all assembly code and make the code significantly more clean. In addition, it might allow for more compact encoding in some cases (for instance, by eliminating or reducing the size of the flags).  This suggestion can be also applied to Merkle Root verifications/calculation code, which can reduce the for loops and complexity of these functions.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.2 Ignored return value for transferFrom call    ", "body": "  Resolution   Fixed by adding a   Description  When burning swap tokens the return value of the transferFrom call is ignored. Depending on the token s implementation this could allow an attacker to mint an arbitrary amount of Amp tokens.  Note that the severity of this issue could have been Critical if Flexa token was any arbitrarily tokens. We quickly verified that Flexa token implementation would revert if the amount exceeds the allowance, however it might not be the case for other token implementations.  code/amp-contracts/contracts/Amp.sol:L619-L620  swapToken.transferFrom(_from, swapTokenGraveyard, amount);  Recommendation  The code should be changed like this:  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.3 No integration tests for the two main components    ", "body": "  Resolution  amp-contracts added as a submodule to collateral-manager and full integration tests added  It is recommended to write test suites that achieve high code coverage to prevent missing obvious bugs that tests could cover.  Description  The existing tests cover each of the two main components and each set of tests mocks the other component. While this is good for unit testing some issues might be missed without proper system/integration tests that cover all components.  Recommendation  Consider adding system/integration tests for all components. As we ve seen in the recent issues in multi-contract smart contract systems, it s becoming more crucial to have a full test suits for future changes to the code base. Not having inter-component tests, could result in issues in the next development and deployment cycles.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.4 Potentially insufficient validation for operator transfers    ", "body": "  Resolution  removing operatorTransferByPartition and simplifying the interfaces to only tranferByPartition  This removes the existing tranferByPartition, converting operatorTransferByPartition to it. The reason for this is to make the client interface simpler, where there is one method to transfer by partition, and that method can be called by either a sender wanting to transfer from their own address, or an operator wanting to transfer from a different token holder address. We found that it was redundant to have multiple methods, and the client convenience wasn t worth the confusion.  Description  For operator transfers, the current validation does not require the sender to be an operator (as long as the transferred value does not exceed the allowance):  code/amp-contracts/contracts/Amp.sol:L755-L759  require(  _isOperatorForPartition(_partition, msg.sender, _from) ||  (_value <= _allowedByPartition[_partition][_from][msg.sender]),  EC_53_INSUFFICIENT_ALLOWANCE  );  It is unclear if this is the intention or whether the logical or should be a logical and.  Recommendation  Confirm that the code matches the intention. If so, consider documenting the behavior (for instance, by changing the name of function operatorTransferByPartition.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.5 Potentially missing nonce check   ", "body": "  Resolution  Nothing was done here, as Dave M writes:  The first two are working as intended, and the third does check that the value is monotonically increasing.  Description  When executing withdrawals in the collateral manager the per-address withdrawal nonce is simply updated without checking that the new nonce is one greater than the previous one (see Examples). It seems like without such a check it might be easy to make mistakes and causing issues with ordering of withdrawals.  Examples  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L663-L664  addressToWithdrawalNonce[_partition][supplier] = withdrawalRootNonce;  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L845-L846  addressToWithdrawalNonce[_partition][supplier] = maxWithdrawalRootNonce;  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1155-L1156  maxWithdrawalRootNonce = _nonce;  Recommendation  Consider adding more validation and sanity checks for nonces on per-address withdrawals.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.6 Unbounded loop when validating Merkle proofs    ", "body": "  Resolution   The loop was removed by switching to   Description  It seems like the loop for validating Merkle proofs is unbounded. If possible it would be good to have an upper bound to prevent DoS-like attacks. It seems like the depth of the tree, and thus, the length of the proof could be bounded.  This could also simplify the decoding and make it more robust. For instance, in _decodeWithdrawalOperatorData it is unclear what happens if the data length is not a multiple of 32. It seems like it might result in out-of-bound reads.  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1460-L1470  uint256 proofNb = (_operatorData.length - 84) / 32;  bytes32[] memory proof = new bytes32[](proofNb);  uint256 index = 0;  for (uint256 i = 116; i <= _operatorData.length; i = i + 32) {  bytes32 temp;  assembly {  temp := mload(add(_operatorData, i))  proof[index] = temp;  index++;  Recommendation  Consider enforcing a bound on the length of Merkle proofs.  Also note that if similar mitigation method as issue 5.1 is used, this method can be replaced by a simpler function using ABI Decode, which does not have any unbounded issues as the sizes of the hashes are fixed (or can be indicated in the passed objects)  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.7 Mitigation for possible reentrancy in token transfers    ", "body": "  Resolution  Fixed as recommended.  Description  ERC777 adds significant features to the token implementation, however there are some known risks associated with this token, such as possible reentrancy attack vector. Given that the Amp token uses hooks to communicate to Collateral manager, it seems that the environment is trusted and safe. However, a minor modification to the implementation can result in safer implementation of the token transfer.  Examples  In Amp.sol --> _transferByPartition()  code/amp-contracts/contracts/Amp.sol:L1152-L1177  require(  _balanceOfByPartition[_from][_fromPartition] >= _value,  EC_52_INSUFFICIENT_BALANCE  );  bytes32 toPartition = _fromPartition;  if (_data.length >= 64) {  toPartition = _getDestinationPartition(_fromPartition, _data);  _callPreTransferHooks(  _fromPartition,  _operator,  _from,  _to,  _value,  _data,  _operatorData  );  _removeTokenFromPartition(_from, _fromPartition, _value);  _transfer(_from, _to, _value);  _addTokenToPartition(_to, toPartition, _value);  _callPostTransferHooks(  toPartition,  Recommendation  It is suggested to move any condition check that is checking the balance to after the external call. However _callPostTransferHooks needs to be called after the state changes, so the suggested mitigation here is to move the require at line 1152 to after _callPreTransferHooks() function (e.g. line 1171).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.8 Potentially inconsistent input validation    ", "body": "  Resolution  transferWithData was removed as a resolution of another filed issue, the rest are documented properly.  The msg.sender cannot be authorized or revoked from being an operator for itself. This should also be clear from the natspec comments now.  Description  There are some functions that might require additional input validation (similar to other functions):  Examples  Amp.transferWithData: require(_isOperator(msg.sender, _from), EC_58_INVALID_OPERATOR); like in  code/amp-contracts/contracts/Amp.sol:L699  require(_isOperator(msg.sender, _from), EC_58_INVALID_OPERATOR);  Amp.authorizeOperatorByPartition: require(_operator != msg.sender); like in  code/amp-contracts/contracts/Amp.sol:L789  require(_operator != msg.sender);  Amp.revokeOperatorByPartition: require(_operator != msg.sender); like in  code/amp-contracts/contracts/Amp.sol:L800  require(_operator != msg.sender);  Recommendation  Consider adding additional input validation.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.9 ERC20 compatibility of Amp token using defaultPartition    ", "body": "  Resolution  This fix resulted in significant changes to the token allowance work flow. The new implementation of balanceOf represents the total balance of tokens at that address (across any partition), instead of only default partition.  The approve + allowance based operations were using a distinct global allowance mapping, while the rest of the ERC20 compat operations were using the partition state mappings with the default partition. This makes the allowance operations behave the same as the balance based operations.  Description  It is somewhat unclear how the Amp token ensures ERC20 compatibility. While the default partition is used in some places (for instance, in function balanceOf) there are also separate fields for (aggregated) balances/allowances. This seems to introduce some redundancy and raises certain questions about when which fields are relevant.  Examples  _allowed is used in function allowance instead of _allowedByPartition with the default partition  An Approval event should be emitted when approving the default partition  code/amp-contracts/contracts/Amp.sol:L1494  emit ApprovalByPartition(_partition, _tokenHolder, _spender, _amount);  increaseAllowance() vs. increaseAllowanceByPartition()  Recommendation  After the mid-audit discussion, it was clear that the general balanceOf method (with no partition) is not needed and can be replaced with a balanceOf function that returns balance of the default partition, similarly for allowance, the general increaseAllowance function can simply call increaseAllowanceByPartition using default partition (same for decreaseAllowance).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.10 Duplicate code better be moved to shared library    ", "body": "  Resolution   aforementioned functions were moved to a shared library   Description  There are some functionalities that the code is duplicated between different smart contracts.  Examples  _getDestinationPartition() is present in both PartitionBase.sol and FlexaCollateralManager.sol  Note that in PartitionBase the usage results in dead code in the contract.  code/amp-contracts/contracts/Amp.sol:L1158-L1160  if (_data.length >= 64) {  toPartition = _getDestinationPartition(_fromPartition, _data);  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L33-L36  toPartition = _fromPartition;  if (_data.length < 64) {  return toPartition;  _splitPartition() is present in FlexaCollateralManager.sol, PartitionBase.sol with slightly different implementations. One has an extra return value for subPartition which is not used in the code under audit  Recommendation  Use a shared library for these functions, possibly ParitionBased.sol can be used in Collateral Manager.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.11 Additional validation for canReceive    ", "body": "  Resolution   Added proper checks and merged   Description  For FlexaCollateralManager.tokensReceived there is validation to ensure that only the Amp calls the function. In contrast, there is no such validation for canReceive and it is unclear if this is the intention.  Examples  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L492-L493  require(msg.sender == amp, \"Invalid sender\");  Recommendation  Consider adding a conjunct msg.sender == amp in function _canReceive.  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L468-L470  function _canReceive(address _to, bytes32 _destinationPartition) internal view returns (bool) {  return _to == address(this) && partitions[_destinationPartition];  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.12 Update to Solidity 0.6.10    ", "body": "  Resolution   Updated to   Description  Due to an issue found in 0.6.9, it is recommended to update the compiler version to latest version 0.6.10.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.13 Discrepancy between code and comments    ", "body": "  Description  There are some discrepancies between (uncommented) code and the documentations comment:  Examples  code/amp-contracts/contracts/Amp.sol:L459-L462  // Indicate token verifies Amp, ERC777 and ERC20 interfaces  ERC1820Implementer._setInterface(AMP_INTERFACE_NAME);  ERC1820Implementer._setInterface(ERC20_INTERFACE_NAME);  // ERC1820Implementer._setInterface(ERC777_INTERFACE_NAME);  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L268-L279  /**  @notice Indicates a supply refund was executed  @param supplier Address whose refund authorization was executed  @param partition Partition from which the tokens were transferred  @param amount Amount of tokens transferred  /  event SupplyRefund(  address indexed supplier,  bytes32 indexed partition,  uint256 amount,  uint256 indexed nonce  );  Recommendation  Consider updating either the code or the comment.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.14 Several fields could potentially be private   ", "body": "  Resolution  : Comment from Flexa team:  We audited the suggested fields, and determined that we would like them to be public for transparency and/or functionality reasons.  Description  Several fields in Amp could possibly be private:  Examples  swapToken:  code/amp-contracts/contracts/Amp.sol:L261  ISwapToken public swapToken;  swapTokenGraveyard:  code/amp-contracts/contracts/Amp.sol:L268  address public constant swapTokenGraveyard = 0x000000000000000000000000000000000000dEaD;  collateralManagers:  code/amp-contracts/contracts/Amp.sol:L236  address[] public collateralManagers;  partitionStrategies:  code/amp-contracts/contracts/Amp.sol:L248  bytes4[] public partitionStrategies;  The same hold for several fields in FlexaCollateralManager. For instance:  partitions:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L78  mapping(bytes32 => bool) public partitions;  nonceToSupply:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L144  mapping(uint256 => Supply) public nonceToSupply;  withdrawalRootToNonce:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L163  mapping(bytes32 => uint256) public withdrawalRootToNonce;  Recommendation  Double-check that you really want to expose those fields.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.15 Several fields could be declared immutable   ", "body": "  Resolution  Comment from Flexa team:  We tried to add this, but found that it made validating the contract on Etherscan impossible. We have added comments to a reader of the contract indicating the fields are immutable after deployment, though.  Description  Several fields could be declared immutable to make clear that they never change after construction:  Examples  Amp._name:  code/amp-contracts/contracts/Amp.sol:L129  string internal _name;  Amp._symbol:  code/amp-contracts/contracts/Amp.sol:L134  string internal _symbol;  Amp.swapToken:  code/amp-contracts/contracts/Amp.sol:L261  ISwapToken public swapToken;  FlexaCollateralManager.amp:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L73  address public amp;  Recommendation  Use the immutable annotation in Solidity (see Immutable).  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.1 Attacker can abuse swapLiquidity function to drain users  funds    ", "body": "  Resolution   Solved by removing   Description  The swapLiquidity function allows liquidity providers to atomically swap their collateral. The function takes a receiverAddressargument that normally points to an ISwapAdapter implementation trusted by the user.  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L490-L517  vars.fromReserveAToken.burn(  msg.sender,  receiverAddress,  amountToSwap,  fromReserve.liquidityIndex  );  // Notifies the receiver to proceed, sending as param the underlying already transferred  ISwapAdapter(receiverAddress).executeOperation(  fromAsset,  toAsset,  amountToSwap,  address(this),  params  );  vars.amountToReceive = IERC20(toAsset).balanceOf(receiverAddress);  if (vars.amountToReceive != 0) {  IERC20(toAsset).transferFrom(  receiverAddress,  address(vars.toReserveAToken),  vars.amountToReceive  );  if (vars.toReserveAToken.balanceOf(msg.sender) == 0) {  _usersConfig[msg.sender].setUsingAsCollateral(toReserve.id, true);  vars.toReserveAToken.mint(msg.sender, vars.amountToReceive, toReserve.liquidityIndex);  However, since an attacker can pass any address as the receiverAddress, they can arbitrarily transfer funds from other contracts that have given allowances to the LendingPool contract (for example, another ISwapAdapter).  The amountToSwap is defined by the caller and can be very small. The attacker gets the difference between IERC20(toAsset).balanceOf(receiverAddress) value of toAsset and the amountToSwap of fromToken.  Remediation  Ensure that no funds can be stolen from contracts that have granted allowances to the LendingPool contract.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.2 Griefing attack by taking flash loan on behalf of user ", "body": "  Description  When taking a flash loan from the protocol, the arbitrary receiverAddress  address can be passed as the argument:  code/contracts/lendingpool/LendingPool.sol:L547-L554  function flashLoan(  address receiverAddress,  address asset,  uint256 amount,  uint256 mode,  bytes calldata params,  uint16 referralCode  ) external override {  That may allow anyone to execute a flash loan on behalf of other users. In order to make that attack, the receiverAddress should give the allowance to the LendingPool contract to make a transfer for the amount of currentAmountPlusPremium.  Example  If someone is giving the allowance to the LendingPool contract to make a deposit, the attacker can execute a flash loan on behalf of that user, forcing the user to pay fees from the flash loan. That will also prevent the victim from making a successful deposit transaction.  Remediation  Make sure that only the user can take a flash loan.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.3 Interest rates are updated incorrectly ", "body": "  Resolution  This issue was independently discovered by the Aave developers and had already been fixed by the end of the audit.  The function updateInterestRates() updates the borrow rates of a reserve. Since the rates depend on the available liquidity they must be recalculated each time liquidity changes. The function takes the amount of liquidity added or removed as the input and is called ahead of minting or burning ATokens. However, in LendingPoolCollateralManager an interest rate update is performed after aTokens have been burned, resulting in an incorrect interest rate.  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L377-L382  vars.collateralAtoken.burn(  user,  receiver,  vars.maxCollateralToLiquidate,  collateralReserve.liquidityIndex  );  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L427-L433  //updating collateral reserve  collateralReserve.updateInterestRates(  collateral,  address(vars.collateralAtoken),  0,  vars.maxCollateralToLiquidate  );  Recommendation  Update interest rates before calling collateralAtoken.burn().  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.4 Unhandled return values of transfer and transferFrom ", "body": "  Resolution  ERC20 implementations are not always consistent. Some implementations of transfer and transferFrom could return  false  on failure instead of reverting. It is safer to wrap such calls into require() statements to these failures. Unsafe transferFrom calls were found in the following locations:  code/contracts/lendingpool/LendingPool.sol:L578  IERC20(asset).transferFrom(receiverAddress, vars.aTokenAddress, vars.amountPlusPremium);  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L407  IERC20(principal).transferFrom(receiver, vars.principalAToken, vars.actualAmountToLiquidate);  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L507-L511  IERC20(toAsset).transferFrom(  receiverAddress,  address(vars.toReserveAToken),  vars.amountToReceive  );  Recommendation  Check the return value and revert on 0/false or use OpenZeppelin s SafeERC20 wrapper functions.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.5 Re-entrancy attacks with ERC-777 ", "body": "  Resolution   The issue was partially mitigated in   Description  Some tokens may allow users to perform re-entrancy while calling the transferFrom function. For example, it would be possible for an attacker to  borrow  a large amount of ERC-777 tokens from the lending pool by re-entering the deposit function from within transferFrom.  code/contracts/lendingpool/LendingPool.sol:L91-L118  function deposit(  address asset,  uint256 amount,  address onBehalfOf,  uint16 referralCode  ) external override {  _whenNotPaused();  ReserveLogic.ReserveData storage reserve = _reserves[asset];  ValidationLogic.validateDeposit(reserve, amount);  address aToken = reserve.aTokenAddress;  reserve.updateState();  reserve.updateInterestRates(asset, aToken, amount, 0);  bool isFirstDeposit = IAToken(aToken).balanceOf(onBehalfOf) == 0;  if (isFirstDeposit) {  _usersConfig[onBehalfOf].setUsingAsCollateral(reserve.id, true);  IAToken(aToken).mint(onBehalfOf, amount, reserve.liquidityIndex);  //transfer to the aToken contract  IERC20(asset).safeTransferFrom(msg.sender, aToken, amount);  emit Deposit(asset, msg.sender, onBehalfOf, amount, referralCode);  Because the safeTransferFrom call is happening at the end of the deposit function, the deposit will be fully processed before the tokens are actually transferred.  So at the beginning of the transfer, the attacker can re-enter the call to withdraw their deposit. The withdrawal will succeed even though the attacker s tokens have not yet been transferred to the lending pool. Essentially, the attacker is granted a flash-loan but without paying fees.  Additionally, after these calls, interest rates will be skewed because interest rate update relies on the actual current balance.  Remediation  Do not whitelist ERC-777 or other re-entrable tokens to prevent this kind of attack.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.6 Potential manipulation of stable interest rates using flash loans ", "body": "  Resolution  This type of manipulation is difficult to prevent completely especially when flash loans are available. In practice however, attacks are mitigated by the following factors:  Liquidity providers attempting to increase users  stable rates would have to pay a high flash loan premium. Users could also immediately swap to variable interest meaning that the attack could result in a net loss for the LP. In practice, it is likely that this makes the attack economically unfeasible.  Under normal conditions, users would only gain a relatively small advantage by lowering their stable rate due to the design of the stable rate curve. If a user attempted to manipulate their stable rate during a liquidity crisis, Aave could immediately rebalance them and bring the rate back to normal.  Flash loans allow users to borrow large amounts of liquidity from the protocol. It is possible to adjust the stable rate up or down by momentarily removing or adding large amounts of liquidity to reserves.  LPs increasing the interest rate of borrowers  The function rebalanceStableBorrowRate() increases the stable interest rate of a user if the current liquidity rate is higher than the user s stable rate. A liquidity provider could trigger an artificial  liquidity crisis  in a reserve and increase the stable interest rates of borrowers by atomically performing the following steps:  Take a flash loan to take a large number of tokens from a reserve  Re-balance the stable rate of the emptied reserves  borrowers  Repay the flash loan (plus premium)  Withdraw the collateral and repay the flash loan  Individual borrowers would then have to switch to the variable rate to return to a lower interest rate.  User borrowing at an artificially lowered interest rate  Users wanting to borrow funds could attempt to get a lower interest rate by temporarily adding liquidity to a reserve (which could e.g. be flash borrowed from a different protocol). While there s a check that prevents users from borrowing an asset while also adding a higher amount of the same asset as collateral, this can be bypassed rather easily by depositing the collateral from a different address (via smart contracts). Aave would then have to rebalance the user to restore an appropriate interest rate.  In practice, users would gain only a relatively small advantage here due to the design of the stable rate curve.  Recommendation  This type of manipulation is difficult to prevent especially when flash loans are available. The safest option to prevent the first variant would be to restrict access to rebalanceStableBorrowRate() to admins. In any case, Aave should monitor the protocol at all times to make sure that interest rates are being rebalanced to sane values.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.7 Code quality could be improved ", "body": "  Some minor code quality improvements are recommended to improve readability.  Explicitly set the visibility for of variables:  code/contracts/tokenization/StableDebtToken.sol:L23-L24  mapping(address => uint40) _timestamps;  uint40 _totalSupplyTimestamp;  code/contracts/configuration/LendingPoolAddressesProviderRegistry.sol:L17-L18  mapping(address => uint256) addressesProviders;  address[] addressesProvidersList;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.8 Attacker can front-run delegator when changing allowance ", "body": "  Users can grant allowances to borrow debt assets to other users using the delegateAllowance function. Similar to the classical ERC20 approve attack, it is possible for a malicious user to front-run the delegator when they attempt to change the allowance and borrow the sum of the old and new values.  Example scenario:  Bob creates an allowance of 100 DAI for Malice: delegateBorrowAllowance(DAI, Malice, 100)  Later, Bob attempts to lower the allowance to 90: delegateBorrowAllowance(DAI, Malice, 90)  Malice borrows a total of 190 DAI by first frontrunning Bob s second transaction borrowing 100 DAI and then borrowing another 90 DAI after Bob s transaction was mined.  Recommentation  A commonly used way of preventing this attack is using increaseAllowance() and decreaseAllowance() functions specifically for increasing and decreasing allowances.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.9 Description of flash loan function is inconsistent with code ", "body": "  The function flashLoan in LendingPool.sol takes an argument mode that specifies the interest rate mode. If the mode is ReserveLogic.InterestRateMode.NONE the function call is treated as a flash loan, if not a normal borrow is executed.  However, inline comments in the function describe the behaviour as  If the transfer didn t succeed, the receiver either didn t return the funds, or didn t approve the transfer . It is unclear how this relates to the actual code or why it is possible to specify a mode in the first place.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "3.1 Winning pods can be frontrun with large deposits ", "body": "  Description  Pod.depositTo() grants users shares of the pod pool in exchange for tokenAmount of token.  code/pods-v3-contracts/contracts/Pod.sol:L266-L288  function depositTo(address to, uint256 tokenAmount)  external  override  returns (uint256)  require(tokenAmount > 0, \"Pod:invalid-amount\");  // Allocate Shares from Deposit To Amount  uint256 shares = _deposit(to, tokenAmount);  // Transfer Token Transfer Message Sender  IERC20Upgradeable(token).transferFrom(  msg.sender,  address(this),  tokenAmount  );  // Emit Deposited  emit Deposited(to, tokenAmount, shares);  // Return Shares Minted  return shares;  The winner of a prize pool is typically determined by an off-chain random number generator, which requires a request to first be made on-chain. The result of this RNG request can be seen in the mempool and frontrun. In this case, an attacker could identify a winning Pod contract and make a large deposit, diluting existing user shares and claiming the entire prize.  Recommendation  The modifier pauseDepositsDuringAwarding is included in the Pod contract but is unused.  code/pods-v3-contracts/contracts/Pod.sol:L142-L148  modifier pauseDepositsDuringAwarding() {  require(  !IPrizeStrategyMinimal(_prizePool.prizeStrategy()).isRngRequested(),  \"Cannot deposit while prize is being awarded\"  );  _;  Add this modifier to the depositTo() function along with corresponding test cases.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.2 Token transfers may return false ", "body": "  Description  There are a lot of token transfers in the code, and most of them are just calling transfer or transferFrom without checking the return value. Ideally, due to the ERC-20 token standard, these functions should always return True or False (or revert). If a token returns False, the code will process the transfer as if it succeeds.  Recommendation  Use the safeTransfer and the safeTransferFrom versions of transfers from OZ.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.3 TokenDrop: Unprotected initialize() function ", "body": "  Description  The TokenDrop.initialize() function is unprotected and can be called multiple times.  code/pods-v3-contracts/contracts/TokenDrop.sol:L81-L87  function initialize(address _measure, address _asset) external {  measure = IERC20Upgradeable(_measure);  asset = IERC20Upgradeable(_asset);  // Set Factory Deployer  factory = msg.sender;  Recommendation  Add the initializer modifier to the initialize() function and include an explicit test that every initialization function in the system can be called once and only once.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.4 Pod: Re-entrancy during deposit or withdrawal can lead to stealing funds ", "body": "  Description  During the deposit, the token transfer is made after the Pod shares are minted:  code/pods-v3-contracts/contracts/Pod.sol:L274-L281  uint256 shares = _deposit(to, tokenAmount);  // Transfer Token Transfer Message Sender  IERC20Upgradeable(token).transferFrom(  msg.sender,  address(this),  tokenAmount  );  That means that if the token allows re-entrancy, the attacker can deposit one more time inside the token transfer. If that happens, the second call will mint more tokens than it is supposed to, because the first token transfer will still not be finished. By doing so with big amounts, it s possible to drain the pod.  Recommendation  Add re-entrancy guard to the external functions.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.5 TokenDrop: Re-entrancy in the claim function can cause to draining funds ", "body": "  Description  code/pods-v3-contracts/contracts/TokenDrop.sol:L139-L153  function claim(address user) external returns (uint256) {  drop();  _captureNewTokensForUser(user);  uint256 balance = userStates[user].balance;  userStates[user].balance = 0;  totalUnclaimed = uint256(totalUnclaimed).sub(balance).toUint112();  // Transfer asset/reward token to user  asset.transfer(user, balance);  // Emit Claimed  emit Claimed(user, balance);  return balance;  Because the totalUnclaimed is already changed, but the current balance is not, the drop function will consider the funds from the unfinished transfer as the new tokens. These tokens will be virtually redistributed to everyone.  After that, the transfer will still happen, and further calls of the drop() function will fail because the following line will revert:  uint256 newTokens = assetTotalSupply.sub(totalUnclaimed);  That also means that any transfers of the Pod token will fail because they all are calling the drop function. The TokenDrop will  unfreeze  only if someone transfers enough tokens to the TokenDrop contract.  The severity of this issue is hard to evaluate because, at the moment, there s not a lot of tokens that allow this kind of re-entrancy.  Recommendation  Simply adding re-entrancy guard to the drop and the claim function won t help because the drop function is called from the claim. For that, the transfer can be moved to a separate function, and this function can have the re-entrancy guard as well as the drop function.  Also, it s better to make sure that _beforeTokenTransfer will not revert to prevent the token from being frozen.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.6 Pod: Having multiple token drops is inconsistent ", "body": "  Description  code/pods-v3-contracts/contracts/Pod.sol:L455-L477  function setTokenDrop(address _token, address _tokenDrop)  external  returns (bool)  require(  msg.sender == factory || msg.sender == owner(),  \"Pod:unauthorized-set-token-drop\"  );  // Check if target<>tokenDrop mapping exists  require(  drops[_token] == TokenDrop(0),  \"Pod:target-tokendrop-mapping-exists\"  );  // Set TokenDrop Referance  drop = TokenDrop(_tokenDrop);  // Set target<>tokenDrop mapping  drops[_token] = drop;  return true;  Recommendation  The mapping seems to be unused, and only one TokenDrop will normally be in the system. If that code is not used, it should be deleted.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.7 Pod: Fees are not limited by a user during the withdrawal ", "body": "  Description  When withdrawing from the Pod, the shares are burned, and the deposit is removed from the Pod. If there are not enough deposit tokens in the contract, the remaining tokens are withdrawn from the pool contract:  code/pods-v3-contracts/contracts/Pod.sol:L523-L532  if (amount > currentBalance) {  // Calculate Withdrawl Amount  uint256 _withdraw = amount.sub(currentBalance);  // Withdraw from Prize Pool  uint256 exitFee = _withdrawFromPool(_withdraw);  // Add Exit Fee to Withdrawl Amount  amount = amount.sub(exitFee);  These tokens are withdrawn with a fee from the pool, which is not controlled or limited by the user.  Recommendation  Allow users to pass a maxFee parameter to control fees.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.8 ProxyFactory.deployMinimal() does not check for contract creation failure ", "body": "  Description  The function ProxyFactory.deployMinimal() is used by both the PodFactory and the TokenDropFactory to deploy minimal proxy contracts. This function uses inline assembly to inline a target address into the minimal proxy and deploys the resulting bytecode. It then emits an event containing the resulting address and optionally makes a low-level call to the resulting address with user-provided data.  The result of a create() operation in assembly will be the zero address in the event that a revert or an exceptional halting state is encountered during contract creation. If execution of the contract initialization code succeeds but returns no runtime bytecode, it is also possible for the create() operation to return a nonzero address that contains no code.  code/pods-v3-contracts/contracts/external/ProxyFactory.sol:L9-L35  function deployMinimal(address _logic, bytes memory _data)  public  returns (address proxy)  // Adapted from https://github.com/optionality/clone-factory/blob/32782f82dfc5a00d103a7e61a17a5dedbd1e8e9d/contracts/CloneFactory.sol  bytes20 targetBytes = bytes20(_logic);  assembly {  let clone := mload(0x40)  mstore(  clone,  0x3d602d80600a3d3981f3363d3d373d3d3d363d73000000000000000000000000  mstore(add(clone, 0x14), targetBytes)  mstore(  add(clone, 0x28),  0x5af43d82803e903d91602b57fd5bf30000000000000000000000000000000000  proxy := create(0, clone, 0x37)  emit ProxyCreated(address(proxy));  if (_data.length > 0) {  (bool success, ) = proxy.call(_data);  require(success, \"ProxyFactory/constructor-call-failed\");  Recommendation  At a minimum, add a check that the resulting proxy address is nonzero before emitting the ProxyCreated event and performing the low-level call. Consider also checking the extcodesize of the proxy address is greater than zero.  Also note that the bytecode in the deployed  Clone  contract was not reviewed due to time constraints.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.9 Pod.setManager() checks validity of wrong address ", "body": "  Description  The current check will always pass once the contract is initialized with a nonzero manager. But, the contract can currently be initialized with a manager of IPodManager(address(0)). In this case, the check would prevent the manager from ever being updated.  code/pods-v3-contracts/contracts/Pod.sol:L233-L240  function setManager(IPodManager newManager)  public  virtual  onlyOwner  returns (bool)  // Require Valid Address  require(address(manager) != address(0), \"Pod:invalid-manager-address\");  Recommendation  Change the check to:  require(address(newManager) != address(0), \"Pod:invalid-manager-address\");  More generally, attempt to define validity criteria for all input values that are as strict as possible. Consider preventing zero inputs or inputs that might conflict with other addresses in the smart contract system altogether, including in contract initialization functions.  4 Recommendations  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "4.1 Rename Withdrawl event to Withdrawal", "body": "  Description  The Pod contract contains an event Withdrawl(address, uint256, uint256):  code/pods-v3-contracts/contracts/Pod.sol:L76-L79  /**  @dev Emitted when user withdraws  /  event Withdrawl(address user, uint256 amount, uint256 shares);  This appears to be a misspelling of the word Withdrawal. This is of course not a problem given it s consistent use, but could cause confusion for users or issues in future contract updates.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "5.1 Anyone is able to mint NFTs by calling mintNFTsForLM    ", "body": "  Resolution  Fixed. Not an issue, as the contract is meant to be used as a mock.  Description  The contract LiquidityMiningNFT has the method mintNFTsForLM.  code/contracts/LiquidityMiningNFT.sol:L12-L29  function mintNFTsForLM(address _liquidiyMiningAddr) external {  uint256[] memory _ids = new uint256[](NFT_TYPES_COUNT);  uint256[] memory _amounts = new uint256[](NFT_TYPES_COUNT);  _ids[0] = 1;  _amounts[0] = 5;  _ids[1] = 2;  _amounts[1] = 1 * LEADERBOARD_SIZE;  _ids[2] = 3;  _amounts[2] = 3 * LEADERBOARD_SIZE;  _ids[3] = 4;  _amounts[3] = 6 * LEADERBOARD_SIZE;  _mintBatch(_liquidiyMiningAddr, _ids, _amounts, \"\");  However, this contract does not have any kind of special permissions to limit who is able to mint tokens.  An attacker could call LiquidityMiningNFT.mintNFTsForLM(0xhackerAddress) to mint tokens for their address and sell them on the marketplace. They are also allowed to mint as many tokens as they want by calling the method multiple times.  Recommendation  Add some permissions to limit only some actors to mint tokens.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.2 Liquidity providers can create deficit of DAI tokens    ", "body": "  Resolution  Fixed by keeping all the DAI inside the PolicyBook.  Description  The current staking system is built in a way that a liquidity provider can stake DAIx tokens to the staking contract. By doing so, DAI tokens are getting withdrawn from the PolicyBook and there may be not enough funds to fulfill claims.  Recommendation  This issue requires major changes in the logic of the system.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.3 Profit and loss distribution mechanism is not working    ", "body": "  Resolution   Fixed by updating the   Description  That error may also lead to the deficit of funds during withdrawals or claims.  Recommendation  Properly keep track of the totalLiquidity.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.4 A liquidity provider can withdraw all his funds anytime    ", "body": "  Resolution  The funds are now locked when the withdrawal is requested, so funds cannot be transferred after the request, and this bug cannot be exploited anymore.  Description  Since some users provide liquidity to sell the insurance policies, it is important that these providers cannot withdraw their funds when the security breach happens and the policyholders are submitting claims. The liquidity providers can only request their funds first and withdraw them later (in a week).  code/contracts/PolicyBook.sol:L358-L382  function requestWithdrawal(uint256 _tokensToWithdraw) external override {  WithdrawalStatus _status = getWithdrawalStatus(msg.sender);  require(_status == WithdrawalStatus.NONE || _status == WithdrawalStatus.EXPIRED,  \"PB: Can't request withdrawal\");  uint256 _daiTokensToWithdraw = _tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  uint256 _availableDaiBalance = balanceOf(msg.sender).mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  if (block.timestamp < liquidityMining.getEndLMTime().add(neededTimeAfterLM)) {  _availableDaiBalance = _availableDaiBalance.sub(liquidityFromLM[msg.sender]);  require(totalLiquidity >= totalCoverTokens.add(_daiTokensToWithdraw),  \"PB: Not enough liquidity\");  require(_availableDaiBalance >= _daiTokensToWithdraw, \"PB: Wrong announced amount\");  WithdrawalInfo memory _newWithdrawalInfo;  _newWithdrawalInfo.amount = _tokensToWithdraw;  _newWithdrawalInfo.readyToWithdrawDate = block.timestamp.add(withdrawalPeriod);  withdrawalsInfo[msg.sender] = _newWithdrawalInfo;  emit RequestWithdraw(msg.sender, _tokensToWithdraw, _newWithdrawalInfo.readyToWithdrawDate);  code/contracts/PolicyBook.sol:L384-L396  function withdrawLiquidity() external override {  require(getWithdrawalStatus(msg.sender) == WithdrawalStatus.READY,  \"PB: Withdrawal is not ready\");  uint256 _tokensToWithdraw = withdrawalsInfo[msg.sender].amount;  uint256 _daiTokensToWithdraw = _tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  if (withdrawalQueue.length != 0 || totalLiquidity.sub(_daiTokensToWithdraw) < totalCoverTokens) {  withdrawalQueue.push(msg.sender);  } else {  _withdrawLiquidity(msg.sender, _tokensToWithdraw);  There is a restriction in requestWithdrawal that requires the liquidity provider to have enough funds at the moment of request:  code/contracts/PolicyBook.sol:L371-L374  require(totalLiquidity >= totalCoverTokens.add(_daiTokensToWithdraw),  \"PB: Not enough liquidity\");  require(_availableDaiBalance >= _daiTokensToWithdraw, \"PB: Wrong announced amount\");  But after the request is created, these funds can then be transferred to another address. When the request is created, the provider should wait for 7 days, and then there will be 2 days to withdraw the requested amount:  code/contracts/PolicyBook.sol:L113-L114  withdrawalPeriod = 1 weeks;  withdrawalExpirePeriod = 2 days;  The attacker would have 4 addresses that will send the pool tokens to each other and request withdrawal of the full amount one by one every 2 days. So at least one of the addresses can withdraw all of the funds at any point in time. If the liquidity provider needs to withdraw funds immediately, he should transfer all funds to that address and execute the withdrawal.  Recommendation  One of the solutions would be to block the DAIx tokens from being transferred after the withdrawal request.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.5 Re-entrancy issue for ERC1155    ", "body": "  Resolution   Addressed by moving   Description  ERC1155 tokens have callback functions on some of the transfers, like safeTransferFrom, safeBatchTransferFrom. During these transfers, the IERC1155ReceiverUpgradeable(to).onERC1155Received function is called in the to address.  For example, safeTransferFrom is used in the LiquidityMining contract:  code/contracts/LiquidityMining.sol:L204-L224  function distributeAllNFT() external {  require(block.timestamp > getEndLMTime(),  \"2 weeks after liquidity mining time has not expired\");  require(!isNFTDistributed, \"NFT is already distributed\");  for (uint256 i = 0; i < leaderboard.length; i++) {  address[] memory _groupLeaders = groupsLeaders[leaderboard[i]];  for (uint256 j = 0; j < _groupLeaders.length; j++) {  _sendNFT(j, _groupLeaders[j]);  for (uint256 i = 0; i < topUsers.length; i++) {  address _currentAddress = topUsers[i];  LMNFT.safeTransferFrom(address(this), _currentAddress, 1, 1, \"\");  emit NFTSent(_currentAddress, 1);  isNFTDistributed = true;  During that transfer, the distributeAllNFT  function can be called again and again. So multiple transfers will be done for each user.  In addition to that, any receiver of the tokens can revert the transfer. If that happens, nobody will be able to receive their tokens.  Recommendation  Add a reentrancy guard.  Avoid transferring tokens for different receivers in a single transaction.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.6 The buyPolicyFor/addLiquidityFor should transfer funds from msg.sender    ", "body": "  Resolution   Addressed by removing the   Description  When calling the buyPolicyFor/addLiquidityFor functions, are called with the parameter _policyHolderAddr/_liquidityHolderAddr who is going to be the beneficiary in buying policy/adding liquidity:  code/contracts/PolicyBook.sol:L183-L189  function buyPolicyFor(  address _policyHolderAddr,  uint256 _epochsNumber,  uint256 _coverTokens  ) external override {  _buyPolicyFor(_policyHolderAddr, _epochsNumber, _coverTokens);  code/contracts/PolicyBook.sol:L264-L266  function addLiquidityFor(address _liquidityHolderAddr, uint256 _liquidityAmount) external override {  _addLiquidityFor(_liquidityHolderAddr, _liquidityAmount, false);  During the execution, the funds for the policy/liquidity are transferred from the _policyHolderAddr/_liquidityHolderAddr, while it s usually expected that they should be transferred from msg.sender. Because of that, anyone can call a function on behalf of a user that gave the allowance to the PolicyBook.  For example, a user(victim) wants to add some DAI to the liquidity pool and gives allowance to the PolicyBook. After that, the user should call addLiquidity, but the attacker can front-run this transaction and buy a policy on behalf of the victim instead.  Also, there is a curious edge case that makes this issue Critical: _policyHolderAddr/_liquidityHolderAddr parameters can be equal to the address of the PolicyBook contract. That may lead to multiple different dangerous attack vectors.  Recommendation  Make sure that nobody can transfer funds on behalf of the users if it s not intended.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.7 LiquidityMining can t accept single ERC1155 tokens    ", "body": "  Resolution   Fixed by properly implementing the   Description  The contract LiquidityMining is also defined as an ERC1155Receiver  code/contracts/LiquidityMining.sol:L19  contract LiquidityMining is ILiquidityMining, ERC1155Receiver, Ownable {  The finalized EIP-1155 standard states that a contract which acts as an EIP-1155 Receiver must implement all the functions in the ERC1155TokenReceiver interface to be able to accept transfers.  These are indeed implemented here:  code/contracts/LiquidityMining.sol:L502  function onERC1155Received(  code/contracts/LiquidityMining.sol:L517  function onERC1155BatchReceived(  The standard states that they will be called and they MUST return a specific byte4 value, otherwise the transfer will fail.  However one of the methods returns an incorrect value. This seems to an error generated by a copy/paste action.  code/contracts/LiquidityMining.sol:L502-L515  function onERC1155Received(  address operator,  address from,  uint256 id,  uint256 value,  bytes memory data  external  pure  override  returns(bytes4)  return bytes4(keccak256(\"onERC1155BatchReceived(address,address,uint256[],uint256[],bytes)\"));  The value returned is equal to  bytes4(keccak256(\"onERC1155BatchReceived(address,address,uint256[],uint256[],bytes)\"));  But it should be  bytes4(keccak256(\"onERC1155Received(address,address,uint256,uint256,bytes)\")).  On top of this, the contract MUST implement the ERC-165 standard to correctly respond to supportsInterface.  Recommendation  Change the return value of onERC1155Received to be equal to 0xf23a6e61 which represents bytes4(keccak256(\"onERC1155Received(address,address,uint256,uint256,bytes)\")).  Also, make sure to implement supportsInterface to signify support of ERC1155TokenReceiver to accept transfers.  Add tests to check the functionality is correct and make sure these kinds of bugs do not exist in the future.  Make sure to read the EIP-1155 and EIP-165 standards in detail and implement them correctly.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.8 DAI is assumed to have the same price as DAIx in the staking contract    ", "body": "  Resolution  Fixed by not transferring DAI anymore.  Description  When a liquidity provider stakes tokens to the BMIDAIStaking contract, the equal amount of DAI and DAIx are transferred from the pool contract.  code/contracts/BMIDAIStaking.sol:L113-L124  function _stakeDAIx(address _user, uint256 _amount, address _policyBookAddr) internal {  require (_amount > 0, \"BMIDAIStaking: Can't stake zero tokens\");  PolicyBook _policyBook = PolicyBook(_policyBookAddr);  // transfer DAI from PolicyBook to yield generator  daiToken.transferFrom(_policyBookAddr, address(defiYieldGenerator), _amount);  // transfer bmiDAIx from user to staking  _policyBook.transferFrom(_user, address(this), _amount);  _mintNFT(_user, _amount, _policyBook);  Recommendation  Only the corresponding amount of DAI should be transferred to the pool.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.9 _updateWithdrawalQueue can run out of gas    ", "body": "  Resolution   The   Description  When there s not enough collateral to withdraw liquidity from a policy book, the withdrawal request is added to a queue. The queue is supposed to be processed and cleared once there are enough funds for that. The only way to do so is the _updateWithdrawalQueue function that is caller when new liquidity is added:  code/contracts/PolicyBook.sol:L315-L338  function _updateWithdrawalQueue() internal {  uint256 _availableLiquidity = totalLiquidity.sub(totalCoverTokens);  uint256 _countToRemoveFromQueue;  for (uint256 i = 0; i < withdrawalQueue.length; i++) {  uint256 _tokensToWithdraw = withdrawalsInfo[withdrawalQueue[i]].amount;  uint256 _amountInDai = _tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE_100);  if (balanceOf(withdrawalQueue[i]) < _tokensToWithdraw) {  _countToRemoveFromQueue++;  continue;  if (_availableLiquidity >= _amountInDai) {  _withdrawLiquidity(withdrawalQueue[i], _tokensToWithdraw);  _availableLiquidity = _availableLiquidity.sub(_amountInDai);  _countToRemoveFromQueue++;  } else {  break;  _removeFromQueue(_countToRemoveFromQueue);  The problem is that this function can only process all queue until the pool run out of available funds or the whole queue is going to be processed. If the queue is big enough, this process can be stuck.  Recommendation  Pass the parameter to the _updateWithdrawalQueue that defines how many requests to process in the queue per one call.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.10 The PolicyBook should make DAI transfers inside the contract    ", "body": "  Resolution   The   Description  The PolicyBook contract gives full allowance over DAI tokens to the other contracts:  code/contracts/PolicyBook.sol:L120-L125  function approveAllDaiTokensForStakingAndVotingAndTransferOwnership() internal {  daiToken.approve(address(bmiDaiStaking), MAX_INT);  daiToken.approve(address(claimVoting), MAX_INT);  transferOwnership(address(bmiDaiStaking));  That behavior is dangerous because it s hard to keep track of and control the contract s DAI balance. And it s also hard to track in the code where the balance of the PolicyBook can be changed from.  Recommendation  It s better to perform all the transfers inside the PolicyBook contract. So if the bmiDaiStaking and the claimVoting contracts need DAI tokens from the PolicyBook, they should call some function of the PolicyBook to perform transfers.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.11 Premium is payed instantly to the liquidity providers    ", "body": "  Resolution  The premium is now distributed on a daily basis.  Description  When the policy is bought, the premium is transferred to the PolicyBook instantly. Currently, these funds are not going to the liquidity providers as a reward due to the issue 5.3. But when the issue is fixed, it seems like the premium is paid and distributed as a reward instantly when the policy is purchased.  The problem is that if someone buys the policy for a long period of time, every liquidity provider instantly gets the premium from the full period. If there s enough liquidity, any provider can withdraw the funds after that without taking a risk for this period.  Recommendation  Distribute the premium over time. For example, increase the reward after each epoch.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.12 The totalCoverTokens is only updated when the policy is bought    ", "body": "  Resolution   The   Description  The totalCoverTokens value represents the amount of collateral that needs to be locked in the policy book. It should be changed either by buying a new policy or when an old policy expires. The problem is that when the old policy expires, this value is not updated; it is only updated when someone buys a policy by calling the _updateEpochsInfo  function:  code/contracts/PolicyBook.sol:L240-L251  function _updateEpochsInfo() internal {  uint256 _totalEpochTime = block.timestamp.sub(epochStartTime);  uint256 _countOfPassedEpoch = _totalEpochTime.div(epochDuration);  uint256 _lastEpochUpdate = currentEpochNumber;  currentEpochNumber = _countOfPassedEpoch.add(1);  for (uint256 i = _lastEpochUpdate; i < currentEpochNumber; i++) {  totalCoverTokens = totalCoverTokens.sub(epochAmounts[i]);  delete epochAmounts[i];  Users waiting to withdraw liquidity should wait for someone to buy the policy to update the totalCoverTokens.  Recommendation  Make sure it s possible to call the _updateEpochsInfo function without buying a new policy.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.13 Unbounded loops in LiquidityMining    ", "body": "  Resolution  Fixed by adding the limits.  Description  There are some methods that have unbounded loops and will fail when enough items exist in the arrays.  code/contracts/LiquidityMining.sol:L83  for (uint256 i = 0; i < _teamsNumber; i++) {  code/contracts/LiquidityMining.sol:L97  for (uint256 i = 0; i < _membersNumber; i++) {  code/contracts/LiquidityMining.sol:L110  for (uint256 i = 0; i < _usersNumber; i++) {  These methods will fail when lots of items will be added to them.  Recommendation  Consider adding limits (from, to) when requesting the items.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.14 The _removeFromQueue is very gas greedy    ", "body": "  Resolution  The queue structure has changed significantly and became more optimized. On the other hand, the new structure has some overhead and can be simplified to optimize more gas.  Description  The _removeFromQueue function is supposed to remove _countToRemove elements from the queue:  code/contracts/PolicyBook.sol:L296-L313  function _removeFromQueue(uint256 _countToRemove) internal {  for (uint256 i = 0; i < _countToRemove; i++) {  delete withdrawalsInfo[withdrawalQueue[i]];  if (_countToRemove == withdrawalQueue.length) {  delete withdrawalQueue;  } else {  uint256 _remainingArrLength = withdrawalQueue.length.sub(_countToRemove);  address[] memory _remainingArr = new address[](_remainingArrLength);  for (uint256 i = 0; i < _remainingArrLength; i++) {  _remainingArr[i] = withdrawalQueue[i.add(_countToRemove)];  withdrawalQueue = _remainingArr;  This function uses too much gas, which makes it easier to make attacks on the system. Even if only one request is removed and executed, this function rewrites all the requests to the storage.  Recommendation  The data structure should be changed so this function shouldn t rewrite the requests that did not change. For example, it can be a mapping (unit => address) with 2 indexes (start, end) that are only increasing.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.15 Withdrawal with zero amount is possible    ", "body": "  Resolution   The   Description  When creating a withdrawal request, the amount of tokens to withdraw is passed as a parameter:  code/contracts/PolicyBook.sol:L358  function requestWithdrawal(uint256 _tokensToWithdraw) external override {  The problem is that this parameter can be zero, and the function will be successfully executed. Moreover, this request can then be added to the queue, and the actual withdrawal will also be executed with zero value. Addresses that never added any liquidity could spam the system with these requests.  Recommendation  Do not allow withdrawals of zero tokens.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.16 The withdrawal queue is only updated when the liquidity is added    ", "body": "  Resolution   The queue is now updated via the   Description  Sometimes when the amount of liquidity is not much higher than the number of tokens locked for the collateral, it s impossible to withdraw liquidity. For a user that wants to withdraw liquidity, a withdrawal request is created. If the request can t be executed, it s added to the withdrawal queue, and the user needs to wait until there s enough collateral for withdrawal. There are potentially 2 ways to achieve that: either someone adds more liquidity or some existing policies expire.  Currently, the queue can only be cleared when the internal _updateWithdrawalQueue  function is called. And it is only called in one place while adding liquidity:  code/contracts/PolicyBook.sol:L276-L290  function _addLiquidityFor(address _liquidityHolderAddr, uint256 _liquidityAmount, bool _isLM) internal {  daiToken.transferFrom(_liquidityHolderAddr, address(this), _liquidityAmount);  uint256 _amountToMint = _liquidityAmount.mul(PERCENTAGE_100).div(getDAIToDAIxRatio());  totalLiquidity = totalLiquidity.add(_liquidityAmount);  _mintERC20(_liquidityHolderAddr, _amountToMint);  if (_isLM) {  liquidityFromLM[_liquidityHolderAddr] = liquidityFromLM[_liquidityHolderAddr].add(_liquidityAmount);  _updateWithdrawalQueue();  emit AddLiquidity(_liquidityHolderAddr, _liquidityAmount, totalLiquidity);  Recommendation  It would be better if the queue could be processed when some policies expire without adding new liquidity. For example, there may be an external function that allows users to process the queue.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.17 Optimize gas usage when checking max length of arrays    ", "body": "  Description  There are a few cases where some arrays have to be limited to a number of items.  And the max size is enforced by removing the last item if the array reached max size + 1.  code/contracts/LiquidityMining.sol:L386-L388  if (leaderboard.length == MAX_LEADERBOARD_SIZE.add(1)) {  leaderboard.pop();  code/contracts/LiquidityMining.sol:L439-L441  if (topUsers.length == MAX_TOP_USERS_SIZE.add(1)) {  topUsers.pop();  code/contracts/LiquidityMining.sol:L495-L497  if (_addresses.length == MAX_GROUP_LEADERS_SIZE.add(1)) {  groupsLeaders[_referralLink].pop();  A simpler and cheaper way to check if an item should be removed is to change the condition to  if (limitedSizedArray.length > MAX_DEFINED_SIZE_FOR_ARRAY) {  limitedSizedArray.pop();  This check does not need or do a SafeMath call (which is more expensive), and because of the limited number of items, as well as a practical impossibility to add enough items to overflow the limit, makes it a preferred way to check the maximum limit.  Recommendation  Rewrite the checks and remove SafeMath operations, as well as the addition by 1 and change the check to a  greater than  verification.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.18 Methods return values that are never used    ", "body": "  Description  When a user calls investDAI these 3 methods are called internally:  code/contracts/LiquidityMining.sol:L196-L198  _updateTopUsers();  _updateLeaderboard(_userTeamInfo.teamAddr);  _updateGroupLeaders(_userTeamInfo.teamAddr);  Each method returns a boolean, but the value is never used. It is also unclear what the value should represent.  Recommendation  Remove the returned variable or use it in method investDAI.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.19 Save some gas when looping over state arrays    ", "body": "  Resolution  Fixed by caching array state length in a local variable.  Description  There are a few loops over state arrays in LiquidutyMining.  code/contracts/LiquidityMining.sol:L209  for (uint256 i = 0; i < leaderboard.length; i++) {  code/contracts/LiquidityMining.sol:L217  for (uint256 i = 0; i < topUsers.length; i++) {  Consider caching the length in a local variable to reduce gas costs.  Examples  Similar to  code/contracts/LiquidityMining.sol:L107  uint256 _usersNumber = allUsers.length;  code/contracts/LiquidityMining.sol:L110  for (uint256 i = 0; i < _usersNumber; i++) {  Recommendation  Reduce gas cost by caching array state length in a local variable.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.20 Optimize gas costs when handling liquidity start and end times ", "body": "  Description  When the LiquidityMining contract is deployed, startLiquidityMiningTime saves the current block timestamp.  code/contracts/LiquidityMining.sol:L46  startLiquidityMiningTime = block.timestamp;  This value is never changed.  There also exists an end limit calculated by getEndLMTime.  code/contracts/LiquidityMining.sol:L271-L273  function getEndLMTime() public view override returns (uint256) {  return startLiquidityMiningTime.add(2 weeks);  This value is also fixed, once the start was defined.  None of the values change after the contract was deployed. This is why you can use the immutable feature provided by Solidity.  It will reduce costs significantly.  Examples  contract A {  uint public immutable start;  uint public immutable end;  constructor() {  start = block.timestamp;  end = block.timestamp + 2 weeks;  This contract defines 2 variables: start and end and their value is fixed on deploy and cannot be changed.  It does not need to use SafeMath because there s no risk of overflowing.  Setting public on both variables creates getters, and calling A.start() and A.end() returns the respective values.  Having set as immutable does not request EVM storage and makes them very cheap to access.  Recommendation  Use Solidity s immutable feature to reduce gas costs and rename variables for consistency.  Use the example for inspiration.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "5.21 Computing the quote should be done for a positive amount of tokens    ", "body": "  Description  When a policy is bought, a quote is requested from the PolicyQuote contract.  code/contracts/PolicyBook.sol:L191-L195  function _buyPolicyFor(  address _policyHolderAddr,  uint256 _epochsNumber,  uint256 _coverTokens  ) internal {  code/contracts/PolicyBook.sol:L213  uint256 _totalPrice = policyQuote.getQuote(_totalSeconds, _coverTokens, address(this));  The getQuote call is then forwarded to an internal function  code/contracts/PolicyQuote.sol:L39-L43  function getQuote(uint256 _durationSeconds, uint256 _tokens, address _policyBookAddr)  external view override returns (uint256 _daiTokens)  _daiTokens = _getQuote(_durationSeconds, _tokens, _policyBookAddr);  code/contracts/PolicyQuote.sol:L45-L47  function _getQuote(uint256 _durationSeconds, uint256 _tokens, address _policyBookAddr)  internal view returns (uint256)  There are some basic checks that make sure the total covered tokens with the requested quote do not exceed the total liquidity. On top of that check, it makes sure the total liquidity is positive.  code/contracts/PolicyQuote.sol:L52-L53  require(_totalCoverTokens.add(_tokens) <= _totalLiquidity, \"PolicyBook: Requiring more than there exists\");  require(_totalLiquidity > 0, \"PolicyBook: The pool is empty\");  But there is no check for the number of quoted tokens. It should also be positive.  Recommendation  Add an additional check for the number of quoted tokens to be positive. The check could fail or return 0, depending on your use case.  If you add a check for the number of quoted tokens to be positive, the check for _totalLiquidity to be positive becomes obsolete and can be removed.  6 Re-audit issues  This section lists the issues found in the re-audit phase. The audit team, reviewed the code fixes after the initial report was delivered.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.1 Anyone can win all the funds from the LiquidityMining without investing any DAI ", "body": "  Description  When a user decides to investDAI in the LiquidityMining contract, the policy book address is passed as a parameter:  code_new/contracts/LiquidityMining.sol:L198  function investDAI(uint256 _tokensAmount, address _policyBookAddr) external override {  But this parameter is never checked and only used at the end of the function:  code_new/contracts/LiquidityMining.sol:L223  IPolicyBook(_policyBookAddr).addLiquidityFromLM(msg.sender, _tokensAmount);  The attacker can pass the address of a simple multisig that will process this transaction successfully without doing anything. And pretend to invest a lot of DAI without actually doing that to win all the rewards in the LiquidityMining contract.  Recommendation  Check that the pool address is valid.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.2 Liquidity withdrawal can be blocked ", "body": "  Description  The main problem in that issue is that the liquidity provider may face many potential issues when withdrawing the liquidity. Under some circumstances, a normal user will never be able to withdraw the liquidity. This issue consists of multiple factors that are interconnected and share the same solution.  There are no partial withdrawals when in the queue. When the withdrawal request is added to the queue, it can only be processed fully: code_new/contracts/PolicyBook.sol:L444-L451 address _currentAddr = withdrawalQueue.head(); uint256 _tokensToWithdraw = withdrawalsInfo[_currentAddr].withdrawalAmount;  uint256 _amountInDAI = convertDAIXtoDAI(_tokensToWithdraw);  if (_availableLiquidity < _amountInDAI) {   break; } But when the request is not in the queue, it can still be processed partially, and the rest of the locked tokens will wait in the queue. code_new/contracts/PolicyBook.sol:L581-L590 } else if (_availableLiquidity < convertDAIXtoDAI(_tokensToWithdraw)) {   uint256 _availableDAIxTokens = convertDAIToDAIx(_availableLiquidity);   uint256 _currentWithdrawalAmount = _tokensToWithdraw.sub(_availableDAIxTokens);   withdrawalsInfo[_msgSender()].withdrawalAmount = _currentWithdrawalAmount;    aggregatedQueueAmount = aggregatedQueueAmount.add(_currentWithdrawalAmount);   withdrawalQueue.push(_msgSender());    _withdrawLiquidity(_msgSender(), _availableDAIxTokens); } else { If there s a huge request in the queue, it can become a bottleneck that does not allow others to withdraw even if there is enough free liquidity.  Withdrawals can be blocked forever by the bots. The withdrawal can only be requested if there are enough free funds in the contract. But once these funds appear, the bots can instantly buy a policy, and for the normal users, it will be impossible to request the withdrawal. Even when a withdrawal is requested and then in the queue, the same problem appears at that stage.  The policy can be bought even if there are pending withdrawals in the queue.  Recommendation  One of the solutions would be to implement the following changes, but the team should thoroughly consider them:  Allow people to request the withdrawal even if there is not enough liquidity at the moment.  Do not allow people to buy policies if there are pending withdrawals in the queue and cannot be executed.  (Optional) Even when the queue is empty, do not allow people to buy policies if there is not enough liquidity for the pending requests (that are not yet in the queue).  (Optional if the points above are implemented) Allow partial executions of the withdrawals in the queue.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.3 The totalCoverTokens can be decreased before the claim is committed ", "body": "  Description  The totalCoverTokens is decreased right after the policy duration ends (_endEpochNumber). When that happens, the liquidity providers can withdraw their funds:  code_new/contracts/PolicyBook.sol:L262-L265  policyHolders[_msgSender()] = PolicyHolder(_coverTokens, currentEpochNumber,  _endEpochNumber, _totalPrice, _reinsurancePrice);  epochAmounts[_endEpochNumber] = epochAmounts[_endEpochNumber].add(_coverTokens);  code_new/contracts/PolicyBook.sol:L343-L351  uint256 _countOfPassedEpoch = block.timestamp.sub(epochStartTime).div(EPOCH_DURATION);  newTotalCoverTokens = totalCoverTokens;  lastEpochUpdate = currentEpochNumber;  newEpochNumber = _countOfPassedEpoch.add(1);  for (uint256 i = lastEpochUpdate; i < newEpochNumber; i++) {  newTotalCoverTokens = newTotalCoverTokens.sub(epochAmounts[i]);  On the other hand, the claim can be created while the policy is still  active . And is considered active until one week after the policy expired:  code_new/contracts/PolicyRegistry.sol:L50-L58  function isPolicyActive(address _userAddr, address _policyBookAddr) public override view returns (bool) {  PolicyInfo storage _currentInfo = policyInfos[_userAddr][_policyBookAddr];  if (_currentInfo.endTime == 0) {  return false;  return _currentInfo.endTime.add(STILL_CLAIMABLE_FOR) > block.timestamp;  By the time when the claim is created + voted, the liquidity provider can potentially withdraw all of their funds already, and the claim will fail.  Recommendation  Make sure that there will always be enough funds for the claim.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.4 The totalCoverTokens is not decreased after the claim happened ", "body": "  Description  When the claim happens and the policy is removed, the totalCoverTokens should be decreased instantly, that s why the scheduled reduction value is removed:  code_new/contracts/PolicyBook.sol:L228-L236  PolicyHolder storage holder = policyHolders[claimer];  epochAmounts[holder.endEpochNumber] = epochAmounts[holder.endEpochNumber].sub(holder.coverTokens);  totalLiquidity = totalLiquidity.sub(claimAmount);  daiToken.transfer(claimer, claimAmount);  delete policyHolders[claimer];  policyRegistry.removePolicy(claimer);  But the totalCoverTokens is not changed and will have the coverage from the removed policy forever.  Recommendation  Decrease the totalCoverTokens inside the commitClaim function.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.5 The Queue remove function does not remove the item completely ", "body": "  Description  When removing an item in a queue, the following function is used:  code_new/contracts/helpers/Queue.sol:L78-L98  function remove(UniqueAddressQueue storage baseQueue, address addrToRemove) internal returns (bool) {  if (!contains(baseQueue, addrToRemove)) {  return false;  if (baseQueue.HEAD == addrToRemove) {  return removeFirst(baseQueue);  if (baseQueue.TAIL == addrToRemove) {  return removeLast(baseQueue);  address prevAddr = baseQueue.queue[addrToRemove].prev;  address nextAddr = baseQueue.queue[addrToRemove].next;  baseQueue.queue[prevAddr].next = nextAddr;  baseQueue.queue[nextAddr].prev = prevAddr;  baseQueue.queueLength--;  return true;  As the result, the baseQueue.queue[addrToRemove] is not deleted, so the contains function will still return True after the removal.  Recommendation  Remove the element from the queue completely.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.6 Optimization issue ", "body": "  Description  The codebase is huge, and there are still a lot of places where these complications and gas efficiency can be improved.  Examples  _updateTopUsers, _updateGroupLeaders, _updateLeaderboard are having a similar mechanism of adding users to a sorted set which makes more storage operations than needed: code_new/contracts/LiquidityMining.sol:L473-L486 uint256 _tmpIndex = _currentIndex - 1; uint256 _currentUserAmount = usersTeamInfo[msg.sender].stakedAmount;  while (_currentUserAmount > usersTeamInfo[topUsers[_tmpIndex]].stakedAmount) {     address _tmpAddr = topUsers[_tmpIndex];     topUsers[_tmpIndex] = msg.sender;     topUsers[_tmpIndex + 1] = _tmpAddr;      if (_tmpIndex == 0) {         break;     }      _tmpIndex--; } Instead of doing 2 operations per item that is lower than the new_item, same can be done with one operation: while topUsers[_tmpIndex] is lower than the new itemtopUsers[_tmpIndex + 1] = topUsers[_tmpIndex].  creating the Queue library looks like overkill for the intended task. It is only used for the withdrawal queue in the PolicyBook. The structure stores and processes extra data, which is unnecessary and more expensive. A larger codebase also has a higher chance of introducing a bug (and it happened here https://github.com/ConsenSys/bridge-mutual-audit-2021-03/issues/25). It s usually better to have a simpler and optimized version like described here issue 5.14.  There are a few for loops that are using uint8 iterators. It s unnecessary and can be even more expensive because, under the hood, it s additionally converted to uint256 all the time. In general, shrinking data to uint8 makes sense to optimize storage slots, but that s not the case here.  The value that is calculated in a loop can be obtained simpler by just having a 1-line formula: code_new/contracts/LiquidityMining.sol:L351-L367 function _getAvailableMonthForReward(address _userAddr) internal view returns (uint256) {     uint256 _oneMonth = 30 days;     uint256 _startRewardTime = getEndLMTime();      uint256 _countOfRewardedMonth = countsOfRewardedMonth[usersTeamInfo[_userAddr].teamAddr][_userAddr];     uint256 _numberOfMonthForReward;      for (uint256 i = _countOfRewardedMonth; i < MAX_MONTH_TO_GET_REWARD; i++) {         if (block.timestamp > _startRewardTime.add(_oneMonth.mul(i))) {         _numberOfMonthForReward++;         } else {             break;         }     }      return _numberOfMonthForReward; }  The mapping is using 2 keys, but the first key is strictly defined by the second one, so there s no need for it: code_new/contracts/LiquidityMining.sol:L60-L61 // Referral link => Address => count of rewarded month mapping (address => mapping (address => uint256)) public countsOfRewardedMonth;  There are a lot of structures in the code with duplicated and unnecessary data, for example: code_new/contracts/LiquidityMining.sol:L42-L48 struct UserTeamInfo {     string teamName;     address teamAddr;      uint256 stakedAmount;     bool isNFTDistributed; } Here the structure is created for every team member, duplicating the team name for each member.  Recommendation  Optimize and simplify the code.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.7 Proper usage of the transfer and the transferFrom functions ", "body": "  Description  Many ERC-20 transfers in the code are just called without checking the return values:  code_new/contracts/PolicyBook.sol:L269-L270  daiToken.transferFrom(_msgSender(), reinsurancePoolAddress, _reinsurancePrice);  daiToken.transferFrom(_msgSender(), address(this), _price);  code_new/contracts/PolicyBook.sol:L556-L559  function _unlockTokens(uint256 _amountToUnlock) internal {  this.transfer(_msgSender(), _amountToUnlock);  delete withdrawalsInfo[_msgSender()];  code_new/contracts/LiquidityMining.sol:L278  bmiToken.transfer(msg.sender, _userReward);  Even though the tokens in these calls are not arbitrary (DAI, BMI, DAIx, stkBMIToken) and probably always return True or call revert, it s still better to comply with the ERC-20 standard and make sure that the transfer went well.  Recommendation  The best solution would be better to always use the safe version of the transfers from openzeppelin/contracts/token/ERC20/SafeERC20.sol.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.8 The price and the duration of a policy may be unpredictable ", "body": "  Description  When the user is buying a policy, the price is calculated based on the current liquidity/coverage ratio, and the duration is calculated based on the current timestamp. A malicious actor can front-run the buyer (e.g., buy short-term insurance with a huge coverage) and increase the policy s price. Or the transaction can be executed much later for some reason, and the number of the totalSeconds may be larger, the coverage period can be between _epochsNumber - 1 and _epochsNumber.  Recommendation  Given the unpredictability of the price, it s better to pass the hard limit for the insurance price as a parameter. Also, as an opinion, you can add a deadline for the transaction as a parameter.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.9 The aggregatedQueueAmount  value is used inconsistently ", "body": "  Description  The aggregatedQueueAmount variable represents the cumulative DAIx amount in the queue that is waiting for the withdrawal. When requesting the withdrawal, this value is used as the amount of DAI that needs to be withdrawn, which may be significantly different:  code_new/contracts/PolicyBook.sol:L539-L540  require(totalLiquidity >= totalCoverTokens.add(aggregatedQueueAmount).add(_daiTokensToWithdraw),  \"PB: Not enough available liquidity\");  That may lead to allowing the withdrawal request even if it shouldn t be allowed and the opposite.  Recommendation  Convert aggregatedQueueAmount to DAI in the _requestWithdrawal.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.10 The claim can only be done once ", "body": "  Description  When the claim happens, the policy is removed afterward:  code_new/contracts/PolicyBook.sol:L222-L237  function commitClaim(address claimer, uint256 claimAmount)  external  override  onlyClaimVoting  updateBMIDAIXStakingReward  PolicyHolder storage holder = policyHolders[claimer];  epochAmounts[holder.endEpochNumber] = epochAmounts[holder.endEpochNumber].sub(holder.coverTokens);  totalLiquidity = totalLiquidity.sub(claimAmount);  daiToken.transfer(claimer, claimAmount);  delete policyHolders[claimer];  policyRegistry.removePolicy(claimer);  If the claim amount is much lower than the coverage, the users are incentivized not to submit it and wait until the end of the coverage period to accumulate all the claims into one.  Recommendation  Allow the policyholders to submit multiple claims until the coverTokens is not reached.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "6.11 Users are incentivised to invest right before the getEndLMTime to join the winning team ", "body": "  Description  When investing, there are 3 types of rewards in the LiquidityMining contracts: for the top users, for the top teams, for the group leaders in the top teams. EVERY member from the top teams is getting a reward proportional to the provided stake. Only the final snapshot of the stakes is used to determine the leaderboard which is right after the getEndLMTime.  Everyone can join any team, and everyone s goal is to go to the winning teams. The best way to do so is to wait right until the end of the period and join the most beneficial team.  Recommendation  It s better to avoid extra incentives that create race conditions.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/bridge-mutual/"}, {"title": "4.1 iETH.exchangeRateStored may not be accurate when invoked from external contracts ", "body": "  Resolution   This issue was addressed in commit   9876e3a by using a modifier to track the current  Description  iETH.exchangeRateStored returns the exchange rate of the contract as a function of the current cash of the contract. In the case of iETH, current cash is calculated as the contract s ETH balance minus msg.value:  code/contracts/iETH.sol:L54-L59  /**  @dev Gets balance of this contract in terms of the underlying  /  function _getCurrentCash() internal view override returns (uint256) {  return address(this).balance.sub(msg.value);  msg.value is subtracted because the majority of iETH methods are payable, and msg.value is implicitly added to a contract s balance before execution begins. If msg.value were not subtracted, the value sent with a call could be used to inflate the contract s exchange rate artificially.  Examples  This problem occurs in multiple locations in the Controller:  beforeMint uses the exchange rate to ensure the supply capacity of the market is not reached. In this case, inflation would prevent the entire supply capacity of the market from being utilized:  code/contracts/Controller.sol:L670-L678  // Check the iToken's supply capacity, -1 means no limit  uint256 _totalSupplyUnderlying =  IERC20Upgradeable(_iToken).totalSupply().rmul(  IiToken(_iToken).exchangeRateStored()  );  require(  _totalSupplyUnderlying.add(_mintAmount) <= _market.supplyCapacity,  \"Token supply capacity reached\"  );  beforeLiquidateBorrow uses the exchange rate via calcAccountEquity to calculate the value of the borrower s collateral. In this case, inflation would increase the account s equity, which could prevent the liquidator from liquidating:  code/contracts/Controller.sol:L917-L919  (, uint256 _shortfall, , ) = calcAccountEquity(_borrower);  require(_shortfall > 0, \"Account does not have shortfall\");  Recommendation  Rather than having the Controller query the iETH.exchangeRateStored, the exchange rate could be passed-in to Controller methods as a parameter.  Ensure no other components in the system rely on iETH.exchangeRateStored after being called from iETH.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.2 Unbounded loop in Controller.calcAccountEquity allows DoS on liquidation ", "body": "  Description  Controller.calcAccountEquity calculates the relative value of a user s supplied collateral and their active borrow positions. Users may mark an arbitrary number of assets as collateral, and may borrow from an arbitrary number of assets. In order to calculate the value of both of these positions, this method performs two loops.  First, to calculate the sum of the value of a user s collateral:  code/contracts/Controller.sol:L1227-L1233  // Calculate value of all collaterals  // collateralValuePerToken = underlyingPrice * exchangeRate * collateralFactor  // collateralValue = balance * collateralValuePerToken  // sumCollateral += collateralValue  uint256 _len = _accountData.collaterals.length();  for (uint256 i = 0; i < _len; i++) {  IiToken _token = IiToken(_accountData.collaterals.at(i));  Second, to calculate the sum of the value of a user s borrow positions:  code/contracts/Controller.sol:L1263-L1268  // Calculate all borrowed value  // borrowValue = underlyingPrice * underlyingBorrowed / borrowFactor  // sumBorrowed += borrowValue  _len = _accountData.borrowed.length();  for (uint256 i = 0; i < _len; i++) {  IiToken _token = IiToken(_accountData.borrowed.at(i));  From dForce, we learned that 200 or more assets would be supported by the Controller. This means that a user with active collateral and borrow positions on all 200 supported assets could force any calcAccountEquity action to perform some 400 iterations of these loops, each with several expensive external calls.  Examples  By modifying dForce s unit test suite, we showed that an attacker could force the cost of calcAccountEquity above the block gas limit. This would prevent all of the following actions, as each relies on calcAccountEquity:  iToken.transfer and iToken.transferFrom  iToken.redeem and iToken.redeemUnderlying  iToken.borrow  iToken.liquidateBorrow and iToken.seize  The following actions would still be possible:  iToken.mint  iToken.repayBorrow and iToken.repayBorrowBehalf  As a result, an attacker may abuse the unbounded looping in calcAccountEquity to prevent the liquidation of underwater positions. We provided dForce with a PoC here: gist.  Recommendation  There are many possible ways to address this issue. Some ideas have been outlined below, and it may be that a combination of these ideas is the best approach:  In general, cap the number of markets and borrowed assets a user may have: The primary cause of the DoS is that the number of collateral and borrow positions held by a user is only restricted by the number of supported assets. The PoC provided above showed that somewhere around 150 collateral positions and 150 borrow positions, the gas costs of calcAccountEquity use most of the gas in a block. Given that gas prices often spike along with turbulent market conditions and that liquidations are far more likely in turbulent market conditions, a cap on active markets / borrows should be much lower than 150 each so as to keep the cost of liquidations as low as possible.  dForce should perform their own gas cost estimates to determine a cap, and choose a safe, low value. Estimates should be performed on the high-level liquidateBorrow method, so as to simulate an actual liquidation event. Additionally, estimates should factor in a changing block gas limit, and the possibility of opcode gas costs changing in future forks. It may be wise to make this cap configurable, so that the limits may be adjusted for future conditions.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.3 Fix utilization rate computation and respect reserves when lending ", "body": "  Resolution   The dForce team has informed us that the only two interest rate models that are still in use are   2a0e974 and  c11fa9b.  Description  The utilization rate UR of an asset forms the basis for interest calculations and is defined as borrows / ( borrows + cash - reserves).  code/contracts/InterestRateModel/InterestRateModel.sol:L72-L88  /**  @notice Calculate the utilization rate: `_borrows / (_cash + _borrows - _reserves)`  @param _cash Asset balance  @param _borrows Asset borrows  @param _reserves Asset reserves  @return Asset utilization [0, 1e18]  /  function utilizationRate(  uint256 _cash,  uint256 _borrows,  uint256 _reserves  ) internal pure returns (uint256) {  // Utilization rate is 0 when there are no borrows  if (_borrows == 0) return 0;  return _borrows.mul(BASE).div(_cash.add(_borrows).sub(_reserves));  issue 4.4.  Recommendation  If reserves > cash \u2014 or, in other words, available cash is negative \u2014 this means part of the reserves have been borrowed, which ideally shouldn t happen in the first place. However, the reserves grow automatically over time, so it might be difficult to avoid this entirely. We recommend (1) avoiding this situation whenever it is possible and (2) fixing the UR computation such that it deals more gracefully with this scenario. More specifically:  Loan amounts should not be checked to be smaller than or equal to cash but cash - reserves (which might be negative). Note that the current check against cash happens more or less implicitly because the transfer just fails for insufficient cash.  Make the utilization rate computation return 1 if reserves > cash (unless borrows == 0, in which case return 0 as is already the case).  Remark  Internally, the utilization rate and other fractional values are scaled by 1e18. The discussion above has a more conceptual than technical perspective, so we used unscaled numbers. When making changes to the code, care must be taken to apply the scaling.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.4 If Base._updateInterest fails, the entire system will halt ", "body": "  Resolution   dForce removed   27f9a28.  Description  Before executing most methods, the iETH and iToken contracts update interest accumulated on borrows via the method Base._updateInterest. This method uses the contract s interest rate model to calculate the borrow interest rate. If the calculated value is above maxBorrowRate (0.001e18), the method will revert:  code/contracts/TokenBase/Base.sol:L92-L107  function _updateInterest() internal virtual override {  InterestLocalVars memory _vars;  _vars.currentCash = _getCurrentCash();  _vars.totalBorrows = totalBorrows;  _vars.totalReserves = totalReserves;  // Gets the current borrow interest rate.  _vars.borrowRate = interestRateModel.getBorrowRate(  _vars.currentCash,  _vars.totalBorrows,  _vars.totalReserves  );  require(  _vars.borrowRate <= maxBorrowRate,  \"_updateInterest: Borrow rate is too high!\"  );  If this method reverts, the entire contract may halt and be unrecoverable. The only ways to change the values used to calculate this interest rate lie in methods that must first call Base._updateInterest. In this case, those methods would fail.  One other potential avenue for recovery exists: the Owner role may update the interest rate calculation contract via TokenAdmin._setInterestRateModel:  code/contracts/TokenBase/TokenAdmin.sol:L46-L63  /**  @dev Sets a new interest rate model.  @param _newInterestRateModel The new interest rate model.  /  function _setInterestRateModel(  IInterestRateModelInterface _newInterestRateModel  ) external virtual onlyOwner settleInterest {  // Gets current interest rate model.  IInterestRateModelInterface _oldInterestRateModel = interestRateModel;  // Ensures the input address is the interest model contract.  require(  _newInterestRateModel.isInterestRateModel(),  \"_setInterestRateModel: This is not the rate model contract!\"  );  // Set to the new interest rate model.  interestRateModel = _newInterestRateModel;  However, this method also calls Base._updateInterest before completing the upgrade, so it would fail as well.  Examples  We used interest rate parameters taken from dForce s unit tests to determine whether any of the interest rate models could return a borrow rate that would cause this failure. The default InterestRateModel is deployed using these values:  Plugging these values in to their borrow rate calculations, we determined that the utilization rate of the contract would need to be 2103e18 in order to reach the max borrow rate and trigger a failure. Plugging this in to the formula for utilization rate, we derived the following ratio:  reserves >= (2102/2103)*borrows + cash  With the given interest rate parameters, if token reserves, total borrows, and underlying cash meet the above ratio, the interest rate model would return a borrow rate above the maximum, leading to the failure conditions described above.  Recommendation  Note that the examples above depend on the specific interest rate parameters configured by dForce. In general, with reasonable interest rate parameters and a reasonable reserve ratio, it seems unlikely that the maximum borrow rate will be reached. Consider implementing the following changes as a precaution:  As utilization rate should be between 0 and 1 (scaled by 1e18), prevent utilization rate calculations from returning anything above 1e18. See issue 4.3 for a more thorough discussion of this topic.  Remove the settleInterest modifier from TokenAdmin._setInterestRateModel: In a worst case scenario, this will allow the Owner role to update the interest rate model without triggering the failure in Base._updateInterest.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.5 RewardDistributor requirement prevents transition of Owner role to smart contract ", "body": "  Resolution   This issue was addressed in commit   4f1e31b by invoking  Description  From dForce, we learned that the eventual plan for the system Owner role is to use a smart contract (a multisig or DAO). However, a requirement in RewardDistributor would prevent the onlyOwner method _setDistributionFactors from working in this case.  _setDistributionFactors calls updateDistributionSpeed, which requires that the caller is an EOA:  code/contracts/RewardDistributor.sol:L179-L189  /**  @notice Update each iToken's distribution speed according to current global speed  @dev Only EOA can call this function  /  function updateDistributionSpeed() public override {  require(msg.sender == tx.origin, \"only EOA can update speeds\");  require(!paused, \"Can not update speeds when paused\");  // Do the actual update  _updateDistributionSpeed();  In the event the Owner role is a smart contract, this statement would necessitate a complicated upgrade to restore full functionality.  Recommendation  Rather than invoking updateDistributionSpeed, have _setDistributionFactors directly call the internal helper _updateDistributionSpeed, which does not require the caller is an EOA.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.6 MSDController._withdrawReserves does not update interest before withdrawal ", "body": "  Resolution  This issue was addressed in commit 2b5946e by changing calcEquity to update the interest of each MSDMinter assigned to an MSD asset.  Note that this method iterates over each MSDMinter, which may cause out-of-gas issues if the number of MSDMinters grows. dForce has informed us that the MSDMinter role will only be held by two contracts per asset (iMSD and MSDS).  Description  MSDController._withdrawReserves allows the Owner to mint the difference between an MSD asset s accumulated debt and earnings:  code/contracts/msd/MSDController.sol:L182-L195  function _withdrawReserves(address _token, uint256 _amount)  external  onlyOwner  onlyMSD(_token)  (uint256 _equity, ) = calcEquity(_token);  require(_equity >= _amount, \"Token do not have enough reserve\");  // Increase the token debt  msdTokenData[_token].debt = msdTokenData[_token].debt.add(_amount);  // Directly mint the token to owner  MSD(_token).mint(owner, _amount);  Debt and earnings are updated each time the asset s iMSD and MSDS contracts are used for the first time in a given block. Because _withdrawReserves does not force an update to these values, it is possible for the withdrawal amount to be calculated using stale values.  Recommendation  Ensure _withdrawReserves invokes iMSD.updateInterest() and MSDS.updateInterest().  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.7 permit functions use deployment-time instead of execution-time chain ID ", "body": "  Resolution   This has been addressed in commits   a7b8fb0 and  d659f2b. The approach taken by the dForce team is to include the chain ID separately in the digest to be signed and keep the deployment/initialization-time chain ID in the  Description  EIP-2612-style  EIP-712 signatures. We focus this discussion on the  code/contracts/TokenBase/Base.sol:L23-L56  function _initialize(  string memory _name,  string memory _symbol,  uint8 _decimals,  IControllerInterface _controller,  IInterestRateModelInterface _interestRateModel  ) internal virtual {  controller = _controller;  interestRateModel = _interestRateModel;  accrualBlockNumber = block.number;  borrowIndex = BASE;  flashloanFeeRatio = 0.0008e18;  protocolFeeRatio = 0.25e18;  __Ownable_init();  __ERC20_init(_name, _symbol, _decimals);  __ReentrancyGuard_init();  uint256 chainId;  assembly {  chainId := chainid()  DOMAIN_SEPARATOR = keccak256(  abi.encode(  keccak256(  \"EIP712Domain(string name,string version,uint256 chainId,address verifyingContract)\"  ),  keccak256(bytes(_name)),  keccak256(bytes(\"1\")),  chainId,  address(this)  );  The DOMAIN_SEPARATOR is supposed to prevent replay attacks by providing context for the signature; it is hashed into the digest to be signed.  code/contracts/TokenBase/Base.sol:L589-L610  bytes32 _digest =  keccak256(  abi.encodePacked(  \"\\x19\\x01\",  DOMAIN_SEPARATOR,  keccak256(  abi.encode(  PERMIT_TYPEHASH,  _owner,  _spender,  _value,  _currentNonce,  _deadline  );  address _recoveredAddress = ecrecover(_digest, _v, _r, _s);  require(  _recoveredAddress != address(0) && _recoveredAddress == _owner,  \"permit: INVALID_SIGNATURE!\"  );  The chain ID is not necessarily constant, though. In the event of a chain split, only one of the resulting chains gets to keep the original chain ID and the other will have to use a new one. With the current pattern, a signature will be valid on both chains; if the DOMAIN_SEPARATOR is recomputed for every verification, a signature will only be valid on the chain that keeps the original ID \u2014 which is probably the intended behavior.  Remark  The reason why the not necessarily constant chain ID is part of the supposedly constant DOMAIN_SEPARATOR is that EIP-712 predates the introduction of the CHAINID opcode. Originally, it was not possible to query the chain ID via opcode, so it had to be supplied to the constructor of a contract by the deployment script.  Recommendation  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.8 iETH.receive() does not support contracts executing during their constructor ", "body": "  Description  iETH.receive() requires that the caller is a contract:  code/contracts/iETH.sol:L187-L195  /**  @notice receive ETH, used for flashloan repay.  /  receive() external payable {  require(  msg.sender.isContract(),  \"receive: Only can call from a contract!\"  );  This method uses the extcodesize of an account to check that the account belongs to a contract. However, contracts currently executing their constructor will have an extcodesize of 0, and will not be able to use this method.  This is unlikely to cause significant issues, but dForce may want to consider supporting this edge case.  Recommendation  Use msg.sender != tx.origin as a more reliable method to detect use by a contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.1 Intentional secret reuse can block borrower and lender from accepting liquidation payment    ", "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#65.  Description  For Dave (the liquidator) to claim the collateral he s purchasing, he must reveal secret D. Once that secret is revealed, Alice and Bob (the borrower and lender) can claim the payment.  Secrets must be provided via the Sales.provideSecret() function:  code/ethereum/contracts/Sales.sol:L193-L200  function provideSecret(bytes32 sale, bytes32 secret_) external {  require(sales[sale].set);  if      (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashA) { secretHashes[sale].secretA = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashB) { secretHashes[sale].secretB = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashC) { secretHashes[sale].secretC = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashD) { secretHashes[sale].secretD = secret_; }  else                                                                          { revert(); }  Note that if Dave chooses the same secret hash as either Alice, Bob, or Charlie (arbiter), there is no way to set secretHashes[sale].secretD because one of the earlier conditionals will execute.  For Alice and Bob to later receive payment, they must be able to provide Dave s secret:  code/ethereum/contracts/Sales.sol:L218-L222  function accept(bytes32 sale) external {  require(!accepted(sale));  require(!off(sale));  require(hasSecrets(sale));  require(sha256(abi.encodePacked(secretHashes[sale].secretD)) == secretHashes[sale].secretHashD);  Dave can exploit this to obtain the collateral for free:  Dave looks at Alice s secret hashes to see which will be used in the sale.  Dave begins the liquidation process, using the same secret hash.  Alice and Bob reveal their secrets A and B through the process of moving the collateral.  Dave now knows the preimage for the secret hash he provided. It was revealed by Alice already.  Dave uses that secret to obtain the collateral.  Alice and Bob now want to receive payment, but they re unable to provide Dave s secret to the Sales smart contract due to the order of conditionals in provideSecret().  After an expiration, Dave can claim a refund.  Mitigating factors  Alice and Bob could notice that Dave chose a duplicate secret hash and refuse to proceed with the sale. This is not something they are likely to do.  Recommendation  Either change the way provideSecret() works to allow for duplicate secret hashes or reject duplicate hashes in create().  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.2 There is no way to convert between custom and non-custom funds   ", "body": "  Resolution  Users who want to switch between custom and non-custom funds can create a new address to do so. This is not actually a big burden because lenders need to use agent software to manage their funds anyway. That workflow typically involves generating a new address because the private key needs to be given to the agent software.  Description  Each fund is created using either Funds.create() or Funds.createCustom(). Both enforce a limitation that there can only be one fund per account:  code/ethereum/contracts/Funds.sol:L348-L355  function create(  uint256  maxLoanDur_,  uint256  maxFundDur_,  address  arbiter_,  bool     compoundEnabled_,  uint256  amount_  ) external returns (bytes32 fund) {  require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address  code/ethereum/contracts/Funds.sol:L383-L397  function createCustom(  uint256  minLoanAmt_,  uint256  maxLoanAmt_,  uint256  minLoanDur_,  uint256  maxLoanDur_,  uint256  maxFundDur_,  uint256  liquidationRatio_,  uint256  interest_,  uint256  penalty_,  uint256  fee_,  address  arbiter_,  bool     compoundEnabled_,  uint256  amount_  ) external returns (bytes32 fund) {  require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address  These functions are the only place where bools[fund].custom is set, and there s no way to delete a fund once it exists. This means there s no way for a given account to switch between a custom and non-custom fund.  This could be a problem if, for example, the default parameters change in a way that a user finds unappealing. They may want to switch to using a custom fund but find themselves unable to do so without moving to a new Ethereum account.  Recommendation  Either allow funds to be deleted or allow funds to be switched between custom and non-custom.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.3 Funds.maxFundDur has no effect if maxLoanDur is set    ", "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#68.  Description  Funds.maxFundDur specifies the maximum amount of time a fund should be active. It s checked in request() to ensure the duration of the loan won t exceed that time, but the check is skipped if maxLoanDur is set:  code/ethereum/contracts/Funds.sol:L510-L514  if (maxLoanDur(fund) > 0) {  require(loanDur_       <= maxLoanDur(fund));  } else {  require(now + loanDur_ <= maxFundDur(fund));  Examples  If a user sets maxLoanDur (the maximum loan duration) to 1 week and sets the maxFundDur (timestamp when all loans should be complete) to December 1st, then there can actually be a loan that ends on December 7th.  Recommendation  Check against maxFundDur even when maxLoanDur is set.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.4 In Funds, maxFundDur is misnamed    ", "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#66.  Description  This is a timestamp, not a duration.  Recommendation  Rename to something with  timestamp  or perhaps  expiration  in the name.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.5 Funds.update() lets users update fields that may not have any effect    ", "body": "  Resolution   This is fixed in   AtomicLoans/atomicloans-eth-contracts#67.  Description  Funds.update() allows users to update the following fields which are only used if bools[fund].custom is set:  minLoanamt  maxLoanAmt  minLoanDur  interest  penalty  fee  liquidationRatio  If bools[fund].custom is not set, then these changes have no effect. This may be misleading to users.  Examples  code/ethereum/contracts/Funds.sol:L454-L478  function update(  bytes32  fund,  uint256  minLoanAmt_,  uint256  maxLoanAmt_,  uint256  minLoanDur_,  uint256  maxLoanDur_,  uint256  maxFundDur_,  uint256  interest_,  uint256  penalty_,  uint256  fee_,  uint256  liquidationRatio_,  address  arbiter_  ) external {  require(msg.sender == lender(fund));  funds[fund].minLoanAmt       = minLoanAmt_;  funds[fund].maxLoanAmt       = maxLoanAmt_;  funds[fund].minLoanDur       = minLoanDur_;  funds[fund].maxLoanDur       = maxLoanDur_;  funds[fund].maxFundDur       = maxFundDur_;  funds[fund].interest         = interest_;  funds[fund].penalty          = penalty_;  funds[fund].fee              = fee_;  funds[fund].liquidationRatio = liquidationRatio_;  funds[fund].arbiter          = arbiter_;  Recommendation  This could be addressed by creating two update functions: one for custom funds and one for non-custom funds. Only the update for custom funds would allow setting these values.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "5.1 Reward rate changes are not taken into account in LP staking ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented two new Emission logic for pToken & Other Reward Tokens in StakeLP which mandatorily distributes rewards only after updating the Reward Pool, thereby fixing this potential issue.  Description  When users update their reward (e.g., by calling the calculateRewards function), the reward amount is calculated according to all reward rate changes after the last update. So it does not matter when and how frequently you update the reward; in the end, you re going to have the same amount.  On the other hand, we can t say the same about the lp staking provided in the StakeLPCoreV8 contract. The amount of these rewards depends on when you call the calculateRewardsAndLiquidity function, and the reward amount can even decrease over time.  Two main factors lead to this:  Changes in the reward rate. If the reward rate is decreased at some point, it s getting partially propagated to all the rewards there were not distributed yet. So the reward of the users that didn t call the calculateRewardsAndLiquidity function may decrease. On the other hand, if the reward rate is supposed to increase, it s better to wait and not call calculateRewardsAndLiquidity for as long as possible.  Not every liquidity provider will stake their LP tokens. When users provide liquidity but do not stake the LP tokens, the reward for these Stokens is still going to the Holder contract. These rewards getting proportionally distributed to the users that are staking their LP tokens. Basically, these rewards are added to the current reward rate but change more frequently. The same logic applies to that rewards; if you expect the unstaked LP tokens to increase, it s in your interest not to withdraw your rewards. But if they are decreasing, it s better to gather the rewards as early as possible.  Recommendation  The most preferred staking solution is to have an algorithm that is not giving people an incentive to gather the rewards earlier or later.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.2 The withdrawUnstakedTokens may run out of gas ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented a batchingLimit variable which enforces a definite number of iterations during withdrawal of unstaked tokens, instead of indefinite iterations.  Description  The withdrawUnstakedTokens is iterating over all batches of unstaked tokens. One user, if unstaked many times, could get their tokens stuck in the contract.  code/contracts/LiquidStakingV2.sol:L369-L403  function withdrawUnstakedTokens(address staker)  public  virtual  override  whenNotPaused  require(staker == _msgSender(), \"LQ20\");  uint256 _withdrawBalance;  uint256 _unstakingExpirationLength = _unstakingExpiration[staker]  .length;  uint256 _counter = _withdrawCounters[staker];  for (  uint256 i = _counter;  i < _unstakingExpirationLength;  i = i.add(1)  ) {  //get getUnstakeTime and compare it with current timestamp to check if 21 days + epoch difference has passed  (uint256 _getUnstakeTime, , ) = getUnstakeTime(  _unstakingExpiration[staker][i]  );  if (block.timestamp >= _getUnstakeTime) {  //if 21 days + epoch difference has passed, then add the balance and then mint uTokens  _withdrawBalance = _withdrawBalance.add(  _unstakingAmount[staker][i]  );  _unstakingExpiration[staker][i] = 0;  _unstakingAmount[staker][i] = 0;  _withdrawCounters[staker] = _withdrawCounters[staker].add(1);  require(_withdrawBalance > 0, \"LQ21\");  emit WithdrawUnstakeTokens(staker, _withdrawBalance, block.timestamp);  _uTokens.mint(staker, _withdrawBalance);  Recommendation  Limit the number of processed unstaked batches, and possibly add pagination.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.3 The _calculatePendingRewards can run out of gas ", "body": "  Resolution  Comment from pSTAKE Finance team:  A solution of maintaining cumulative timeshare values in an array and implementing binary search drastically lowers the iterations, has been implemented for calculating rewards for Other Reward Tokens. Also, for moving reward rate, strategically it will only be set max once a month making number of iterations very limited.  Description  The reward rate in STokens can be changed, and the history of these changes are stored in the contract:  code/contracts/STokensV2.sol:L124-L139  function setRewardRate(uint256 rewardRate)  public  virtual  override  returns (bool success)  // range checks for rewardRate. Since rewardRate cannot be more than 100%, the max cap  // is _valueDivisor * 100, which then brings the fees to 100 (percentage)  require(rewardRate <= _valueDivisor.mul(100), \"ST17\");  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"ST2\");  _rewardRate.push(rewardRate);  _lastMovingRewardTimestamp.push(block.timestamp);  emit SetRewardRate(rewardRate);  return true;  When the reward is calculated for each user, all changes of the _rewardRate are considered. So there is a for loop that iterates over all changes since the last reward update. If the reward rate was changed many times, the _calculatePendingRewards function could run out of gas.  Recommendation  Provide an option to partially update the reward, so the full update can be split in multiple transactions.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.4 Increase test coverage ", "body": "  Resolution  Comment from pSTAKE Finance team:  Created deep-dive test for each unique scenarios locally and tested before the code was deployed.  Description  Test coverage is fairly limited. LPStaking tests only cover the happy path. StakeLPCoreV8 has no tests. Many test descriptions are inaccurate.  Examples  Test description inaccuracy examples:  This tests that a Staker can mint new tokens, but does not check to make sure that Stakers are the ONLY group that can mint. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/LiquidStakingTest.js#L82  This test only shows that an unauthorized address can t use the stake function to mint tokens. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/LiquidStakingTest.js#L99  This test actually tests for the inverse case. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/STokensTest.js#L82  Recommendation  Increase test coverage for entire codebase. Add tests for the inherited contracts from OpenZeppelin. Test for edge cases, and multiple expected cases. Ensure that the test description matches the functionality that is actually tested.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.5 The calculateRewards should not be callable by the whitelisted contract ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have created a require condition in Smart Contract code to disallow whitelisted contracts from calling the function  Description  The calculateRewards function should only be called for non-whitelisted addresses:  code/contracts/STokensV2.sol:L348-L359  function calculateRewards(address to)  public  virtual  override  whenNotPaused  returns (bool success)  require(to == _msgSender(), \"ST5\");  uint256 reward = _calculateRewards(to);  emit TriggeredCalculateRewards(to, reward, block.timestamp);  return true;  For all the whitelisted addresses, the calculateHolderRewards function is called. But if the calculateRewards function is called by the whitelisted address directly, the function will execute, and the rewards will be distributed to the caller instead of the intended recipients.  Recommendation  While this scenario is unlikely to happen, adding the additional check in the calculateRewards is a good option.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.6 Presence of testnet code ", "body": "  Resolution  Comment from pSTAKE Finance team:  The testnet code has been re-considered as out of scope for audit  Description  Based on the discussions with pStake team and in-line comments, there are a few instances of code and commented code in the code base under audit that are not finalized for mainnet deployment.  Examples  code/contracts/PSTAKE.sol:L25-L37  function initialize(address pauserAddress) public virtual initializer {  __ERC20_init(\"pSTAKE Token\", \"PSTAKE\");  __AccessControl_init();  __Pausable_init();  _setupRole(DEFAULT_ADMIN_ROLE, _msgSender());  _setupRole(PAUSER_ROLE, pauserAddress);  // PSTAKE IS A SIMPLE ERC20 TOKEN HENCE 18 DECIMAL PLACES  _setupDecimals(18);  // pre-allocate some tokens to an admin address which will air drop PSTAKE tokens  // to each of holder contracts. This is only for testnet purpose. in Mainnet, we  // will use a vesting contract to allocate tokens to admin in a certain schedule  _mint(_msgSender(), 5000000000000000000000000);  The initialize function currently mints all the tokens to msg.sender, however the goal for mainnet is to use a vesting contract which is not present in the current code.  Recommendation  It is recommended to fully test the final code before deployment to the mainnet.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.7 Re-entrancy from LP token transfers ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented the nonReentrant modifier from the ReentrancyGuardUpgradeable OpenZeppelin contract in addition to strictly keeping to Checks-Effects-Interactions pattern throughout relevant areas  Description  The StakeLPCoreV8 contract is designed to stake LP tokens. These LP tokens are not directly controlled or developed by the protocol, so it can t be easily verified that no re-entrancy can happen during token transfers.  Recommendation  During the review, we did not find any specific ways to build the attack using the re-entrancy of LP tokens, but it is still better to have the re-entrancy protection modifiers in functions that use LP tokens transfers.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.8 Sanity check on all important variables ", "body": "  Resolution  Comment from pSTAKE Finance team:  Post the implementation of new emission logic there have been a rearrangement of some variables, but the rest have been sanity tested and corrected  Description  Most of the functionalities have proper sanity checks when it comes to setting system-wide variables, such as whitelist addresses. However there are a few key setters that lack such sanity checks.  Examples  Sanity check (!= address(0)) on all token contracts.  code/contracts/StakeLPCoreV8.sol:L303-L333  function setUTokensContract(address uAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP9\");  _uTokens = IUTokens(uAddress);  emit SetUTokensContract(uAddress);  /**  @dev Set 'contract address', called from constructor  @param sAddress: stoken contract address  Emits a {SetSTokensContract} event with '_contract' set to the stoken contract address.  /  function setSTokensContract(address sAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP10\");  _sTokens = ISTokens(sAddress);  emit SetSTokensContract(sAddress);  /**  @dev Set 'contract address', called from constructor  @param pstakeAddress: pStake contract address  Emits a {SetPSTAKEContract} event with '_contract' set to the stoken contract address.  /  function setPSTAKEContract(address pstakeAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP11\");  _pstakeTokens = IPSTAKE(pstakeAddress);  emit SetPSTAKEContract(pstakeAddress);  Sanity check on unstakingLockTime to be in the acceptable range (21 hours to 21 days)  code/contracts/LiquidStakingV2.sol:L105-L121  /**  @dev Set 'unstake props', called from admin  @param unstakingLockTime: varies from 21 hours to 21 days  Emits a {SetUnstakeProps} event with 'fee' set to the stake and unstake.  /  function setUnstakingLockTime(uint256 unstakingLockTime)  public  virtual  returns (bool success)  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LQ3\");  _unstakingLockTime = unstakingLockTime;  emit SetUnstakingLockTime(unstakingLockTime);  return true;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.9 Remove unused/commented code", "body": "  Description  There are a few snippets of commented code in the code base. It is suggested to remove and clean any unused and commented code in the final code.  Examples  code/contracts/PSTAKE.sol:L50-L73  /* function mint(address to, uint256 tokens) public virtual override returns (bool success) {  require(_msgSender() == _stakeLPCoreContract, \"PS1\");  // minted by STokens contract  _mint(to, tokens);  return true;  } */  /*  @dev Burn utokens for the provided 'address' and 'amount'  @param from: account address, tokens: number of tokens  Emits a {BurnTokens} event with 'from' set to address and 'tokens' set to amount of tokens.  Requirements:  - `amount` cannot be less than zero.  /  /* function burn(address from, uint256 tokens) public virtual override returns (bool success) {  require((tx.origin == from && _msgSender()==_liquidStakingContract) ||  // staking operation  (tx.origin == from && _msgSender() == _wrapperContract), \"UT2\"); // unwrap operation  _burn(from, tokens);  return true;  } */  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "4.1 Initialization flaws    ", "body": "  Resolution   This has been fixed in   Description  OpenZeppelin Contracts Upgradeable is to have a  Examples  The __ERC20WrapperGluwacoin_init function is implemented as follows:  code/contracts/ERC20WrapperGluwacoin.sol:L36-L48  function __ERC20WrapperGluwacoin_init(  string memory name,  string memory symbol,  IERC20 token  ) internal initializer {  __Context_init_unchained();  __ERC20_init_unchained(name, symbol);  __ERC20ETHless_init_unchained();  __ERC20Reservable_init_unchained();  __AccessControlEnumerable_init_unchained();  __ERC20Wrapper_init_unchained(token);  __ERC20WrapperGluwacoin_init_unchained();  And the C3 linearization is:  The calls __ERC165_init_unchained(); and __AccessControl_init_unchained(); are missing, and __ERC20Wrapper_init_unchained(token); should move between __ERC20_init_unchained(name, symbol); and __ERC20ETHless_init_unchained();.  Recommendation  Review all *_init functions, add the missing *_init_unchained calls, and fix the order of these calls.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"}, {"title": "4.2 Flaw in _beforeTokenTransfer call chain and missing tests    ", "body": "  Resolution   This has been fixed in   Description  In OpenZeppelin s ERC-20 implementation, the virtual _beforeTokenTransfer function provides a hook that is called before tokens are transferred, minted, or burned. In the Gluwacoin codebase, it is used to check whether the unreserved balance (as opposed to the regular balance, which is checked by the ERC-20 implementation) of the sender is sufficient to allow this transfer or burning.  In ERC20WrapperGluwacoin, ERC20Reservable, and ERC20Wrapper, the _beforeTokenTransfer function is implemented in the following way:  code/contracts/ERC20WrapperGluwacoin.sol:L54-L61  function _beforeTokenTransfer(  address from,  address to,  uint256 amount  ) internal override(ERC20Upgradeable, ERC20Wrapper, ERC20Reservable) {  ERC20Wrapper._beforeTokenTransfer(from, to, amount);  ERC20Reservable._beforeTokenTransfer(from, to, amount);  code/contracts/abstracts/ERC20Reservable.sol:L156-L162  function _beforeTokenTransfer(address from, address to, uint256 amount) internal virtual override (ERC20Upgradeable) {  if (from != address(0)) {  require(_unreservedBalance(from) >= amount, \"ERC20Reservable: transfer amount exceeds unreserved balance\");  super._beforeTokenTransfer(from, to, amount);  code/contracts/abstracts/ERC20Wrapper.sol:L176-L178  function _beforeTokenTransfer(address from, address to, uint256 amount) internal virtual override (ERC20Upgradeable) {  super._beforeTokenTransfer(from, to, amount);  Finally, the C3-linearization of the contracts is:  Moreover, while reviewing the correctness and coverage of the tests is not in scope for this engagement, we happened to notice that there are no tests that check whether the unreserved balance is sufficient for transferring or burning tokens.  Recommendation  ERC20WrapperGluwacoin._beforeTokenTransfer should just call super._beforeTokenTransfer. Moreover, the _beforeTokenTransfer implementation can be removed from ERC20Wrapper.  We would like to stress the importance of careful and comprehensive testing in general and of this functionality in particular, as it is crucial for the system s integrity. We also encourage investigating whether there are more such omissions and an evaluation of the test quality and coverage in general.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"}, {"title": "4.3 Hard-coded decimals    ", "body": "  Resolution   In   Description  The Gluwacoin wrapper token should have the same number of decimals as the wrapped ERC-20. Currently, the number of decimals is hard-coded to 6. This limits flexibility or requires source code changes and recompilation if a token with a different number of decimals is to be wrapped.  code/contracts/ERC20WrapperGluwacoin.sol:L32-L34  function decimals() public pure override returns (uint8) {  return 6;  Recommendation  We recommend supplying the number of decimals as an initialization parameter and storing it in a state variable. That increases gas consumption of the decimals function, but we doubt this view function will be frequently called from a contract, and even if it was, we think the benefits far outweigh the costs. Moreover, we believe the decimals logic (i.e., function decimals and the new state variable) should be implemented in the ERC20Wrapper contract   which holds the basic ERC-20 functionality of the wrapper token   and not in ERC20WrapperGluwacoin, which is the base contract of the entire system.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"}, {"title": "5.1 Delegated transactions can be executed for multiple accounts    ", "body": "  Resolution  Comment from the client: The issue has been solved  Description  The Gateway contract allows users to create meta transactions triggered by the system s backend. To do so, one of the owners of the account should sign the message in the following format:  code/src/gateway/Gateway.sol:L125-L131  address sender = _hashPrimaryTypedData(  _hashTypedData(  nonce,  to,  data  ).recoverAddress(senderSignature);  The message includes a nonce, destination address, and call data. The problem is that this message does not include the account address. So if the sender is the owner of multiple accounts, this meta transaction can be called for multiple accounts.  Recommendation  Add the account field in the signed message or make sure that any address can be the owner of only one account.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.2 Removing an owner does not work in PersonalAccountRegistry    ", "body": "  Resolution  Comment from the client: The issue has been solved  Description  An owner of a personal account can be added/removed by other owners. When removing the owner, only removedAtBlockNumber value is updated. accounts[account].owners[owner].added remains true:  code/src/personal/PersonalAccountRegistry.sol:L116-L121  accounts[account].owners[owner].removedAtBlockNumber = block.number;  emit AccountOwnerRemoved(  account,  owner  );  But when the account is checked whether this account is the owner, only accounts[account].owners[owner].added is actually checked:  code/src/personal/PersonalAccountRegistry.sol:L255-L286  function _verifySender(  address account  private  returns (address)  address sender = _getContextSender();  if (!accounts[account].owners[sender].added) {  require(  accounts[account].salt == 0  );  bytes32 salt = keccak256(  abi.encodePacked(sender)  );  require(  account == _computeAccountAddress(salt)  );  accounts[account].salt = salt;  accounts[account].owners[sender].added = true;  emit AccountOwnerAdded(  account,  sender  );  return sender;  So the owner will never be removed, because accounts[account].owners[owner].added will always be `true.  Recommendation  Properly check if the account is still the owner in the _verifySender  function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.3 The withdrawal mechanism is overcomplicated    ", "body": "  Resolution  Comment from the client:  The withdrawal mechanism has been refactored. In current version user can withdraw funds from the deposit account in two ways:  with guardian signature - withdrawDeposit  using  deposit exit  process  Description  To withdraw the funds, anyone who has the account in PaymentRegistry should call the withdrawDeposit function and go through the withdrawal process. After the lockdown period (30 days), the user will withdraw all the funds from the account.  code/src/payment/PaymentRegistry.sol:L160-L210  function withdrawDeposit(  address token  external  address owner = _getContextAccount();  uint256 lockedUntil = deposits[owner].withdrawalLockedUntil[token];  /* solhint-disable not-rely-on-time */  if (lockedUntil != 0 && lockedUntil <= now) {  deposits[owner].withdrawalLockedUntil[token] = 0;  address depositAccount = deposits[owner].account;  uint256 depositValue;  if (token == address(0)) {  depositValue = depositAccount.balance;  } else {  depositValue = ERC20Token(token).balanceOf(depositAccount);  _transferFromDeposit(  depositAccount,  owner,  token,  depositValue  );  emit DepositWithdrawn(  depositAccount,  owner,  token,  depositValue  );  } else {  _deployDepositAccount(owner);  lockedUntil = now.add(depositWithdrawalLockPeriod);  deposits[owner].withdrawalLockedUntil[token] = lockedUntil;  emit DepositWithdrawalRequested(  deposits[owner].account,  owner,  token,  lockedUntil  );  /* solhint-enable not-rely-on-time */  During that period, everyone who has a channel with the user is forced to commit their channels or lose money from that channel. When doing so, every user will reset the initial lockdown period and the withdrawer should start the process again.  code/src/payment/PaymentRegistry.sol:L479-L480  if (deposits[sender].withdrawalLockedUntil[token] > 0) {  deposits[sender].withdrawalLockedUntil[token] = 0;  There is no way for the withdrawer to close the channel by himself. If the withdrawer has N channels, it s theoretically possible to wait for up to N*(30 days) period and make N+2 transactions.  Recommendation  There may be some minor recommendations on how to improve that without major changes:  When committing a payment channel, do not reset the lockdown period to zero. Two better option would be either not change it at all or extend to now + depositWithdrawalLockPeriod  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.4 A malicious guardian can steal funds   ", "body": "  Resolution  Comment from the client: The etherspot payment system is semi-trusted by design.  Description  A guardian is signing every message that should be submitted as a payment channel update. A guardian s two main things to verify are: blockNumber and the fact that the sender has enough funds.  There are two main attack vectors for the malicious guardian:  It s possible to conspire with the previous owner of the account and submit the old blockNumber. This allows them to drain the account.  A guardian can also conspire with the sender and send more funds to multiple channels than funds in the account.  Recommendation  Reduce the system s reliance on single points of failure like the guardians.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.5 Upgrade solidity version    ", "body": "  Resolution  Solidity version has been upgraded to 0.6.12  Description  The current minimal solidity version is 0.6.0. But some parts of the code use features from the later versions of solidity, like the high-level version of CREATE2 to create accounts.  Recommendation  Upgrade solidity version to the latest stable (0.6.12).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.6 The lockdown period shouldn t be extended when called multiple times    ", "body": "  Resolution  Comment from the client: The issue has been solved  Description  In order to withdraw a deposit from the PaymentRegistry, the account owner should call the withdrawDeposit function and wait for depositWithdrawalLockPeriod (30 days) before actually transferring all the tokens from the account.  The issue is that if the withdrawer accidentally calls it for the second time before these 30 days pass, the waiting period gets extended for 30 days again.  code/src/payment/PaymentRegistry.sol:L170-L199  if (lockedUntil != 0 && lockedUntil <= now) {  deposits[owner].withdrawalLockedUntil[token] = 0;  address depositAccount = deposits[owner].account;  uint256 depositValue;  if (token == address(0)) {  depositValue = depositAccount.balance;  } else {  depositValue = ERC20Token(token).balanceOf(depositAccount);  _transferFromDeposit(  depositAccount,  owner,  token,  depositValue  );  emit DepositWithdrawn(  depositAccount,  owner,  token,  depositValue  );  } else {  _deployDepositAccount(owner);  lockedUntil = now.add(depositWithdrawalLockPeriod);  Recommendation  Only extend the waiting period when a withdrawal is requested for the first time.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.7 Missing documentation    ", "body": "  Resolution  Comment from the client: Code has been documented - We will work on white paper, graphs later  Description  The code base as is, is missing proper documentations to understand the code work flow and logic. The most important pieces are high-level diagrams, user work flows, and updated white paper.  It is important for readability and maintainability of the codebase to add in-line documentations. The Pillar code base under the audit lacks any type of inline documentation and it makes the code reviewer s job much harder. We highly recommend to provide inline documentation using Solidity s natspec format, as this will be easier to maintain.  As an example PaymentRegistry.sol without the documentation is really hard to read and understand. There are many assumptions or off-chain dependencies and it s impossible to understand the flows simply by reading the solidity code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}]