[{"title": "6.1 EOPBCTemplate - permission documentation inconsistencies    ", "body": "  Resolution                           Fixed with   aragonone/fundraising@bafe100 by adding the undocumented and deviating permissions to the documentation.  Description  Undocumented  The template documentation provides an overview of the permissions set with the template. The following permissions are set by the template contract but are not documented in the accompanied fundraising/templates/externally_owned_presale_bonding_curve/README.md.  TokenManager  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L220-L221  _createPermissions(_acl, grantees, _fundraisingApps.bondedTokenManager, _fundraisingApps.bondedTokenManager.MINT_ROLE(), _owner);  _acl.createPermission(_fundraisingApps.marketMaker, _fundraisingApps.bondedTokenManager, _fundraisingApps.bondedTokenManager.BURN_ROLE(), _owner);  code/fundraising/templates/externally_owned_presale_bonding_curve/eopbc.yaml:L33-L44  Inconsistent  app: anj-token-manager  role: MINT_ROLE  grantee: market-maker  manager: owner  app: anj-token-manager  role: MINT_ROLE  grantee: presale  manager: owner  app: anj-token-manager  role: BURN_ROLE  grantee: market-maker  manager: owner  Inconsistent  The following permissions are set by the template but are inconsistent to the outline in the documentation:  Controller  owner has the following permissions even though they are documented as not being set https://github.com/ConsenSys/aragonone-presale-audit-2019-11/blob/9ddae8c7fde9dea3af3982b965a441239d81f370/code/fundraising/templates/externally_owned_presale_bonding_curve/README.md#controller.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L239-L240  _acl.createPermission(_owner, _fundraisingApps.controller, _fundraisingApps.controller.UPDATE_BENEFICIARY_ROLE(), _owner);  _acl.createPermission(_owner, _fundraisingApps.controller, _fundraisingApps.controller.UPDATE_FEES_ROLE(), _owner);  Recommendation  For transparency, all permissions set-up by the template must be documented.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.2 EOPBCTemplate - AppId of BalanceRedirectPresale should be different from AragonBlack/Presale namehash to avoid collisions    ", "body": "  Resolution                           Fixed with   aragonone/fundraising@bafe100 by generating a unique APMNameHash for  Description  The template references the new presale contract with apmNamehash 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5. However, this namehash is already used by the aragonBlack/Presale contract. To avoid confusion and collision a unique apmNamehash should be used for this variant of the contract.  Note that the contract that is referenced from an apmNamehash is controlled by the ENS resolver that is configured when deploying the template contract.  Using the same namehash for both variants of the contract does not allow a single registry to simultaneously provide both variants of the contract and might lead to confusion as to which application is actually deployed. This also raises the issue that the ENS registry must be verified before actually using the contract as a malicious registry could force the template to deploy potentially malicious applications.  aragonOne/Fundraising:  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L32  bytes32   private constant PRESALE_ID                    = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  aragonBlack/Fundraising:  templates/multisig/contracts/FundraisingMultisigTemplate.sol:L35  bytes32   private constant PRESALE_ID             = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  bytes32   private constant PRESALE_ID             = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;  Recommendation  Create a new apmNamehash for BalanceRedirectPresale.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.3 BalanceRedirectPresale - Presale can be extended indefinitely   ", "body": "  Resolution  This issue was addressed with the following statement:  It is a very reasonable concern, but this is the intended behavior. That modification is permissioned and that OPEN_ROLE is going to be held by the Aragon Network Dao, so we expect a reasonable use of it. We may document it and make it clear that this is possible.  Description  The OPEN_ROLE can indefinitely extend the Presale even after users contributed funds to it by adjusting the presale period. The period might be further manipulated to avoid that token trading in the MarketMaker is opened.  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L136-L138  function setPeriod(uint64 _period) external auth(OPEN_ROLE) {  _setPeriod(_period);  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257  function _setPeriod(uint64 _period) internal {  require(_period > 0, ERROR_TIME_PERIOD_ZERO);  require(openDate == 0 || openDate + _period > getTimestamp64(), ERROR_INVALID_TIME_PERIOD);  period = _period;  Recommendation  Do not allow to extend the presale after funds have been contributed to it or only allow period adjustments in State.PENDING.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.4 Repository structure - Create a clean repository containing one Aragon Application unless changes are contributed upstream    Deferred", "body": "  Resolution  The issue has been deferred pending internal discussion.  Description  The repository is a fork of AragonBlack/fundraising. The main development repository for Aragon Fundraising is the origin repository at AragonBlock. This repository duplicates a state of the upstream repository that can quickly get out of sync and therefore hard to maintain.  It is unclear if both repositories will live side-by-side or if the BalanceRedirectPresale variant is contributed upstream.  Recommendation  In case changes are not planned to be contributed upstream it is recommended to create a clean Aragon Application from scratch removing any unused or duplicated files.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.5 BalanceRedirectPresale - Tokens vest during the Presale phase   ", "body": "  Resolution  The issue was addressed with the following statement:  This presale version is intended to be used along with the Externally Owned Presale and Bonding Curve Template, which doesn t have a Voting app, therefore contributors doesn t have any voting power. The use case is the deployment of Aragon Network Jurors Token (ANJ) for the Aragon Court, which is not going to be active before the presale starts, so we don t see any potential issue here.  Description  Tokens are directly minted and assigned to contributors during the Presale. While this might not be an issue if the minted token does not give any voting power of some sort in a DAO it can be a problem for scenarios where contributors get stake in return for contributions.  Recommendation  Vest tokens for contributors after the presale finishes. In case this is the expected we suggest to add a note to the documentation to make potential users aware of this behaviour that might have security implications if contributors get stake in return for their investments.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.6 BalanceRedirectPresale - setPeriod uint64 overflow in validation check    ", "body": "  Resolution                           Fixed with   aragonone/fundraising@bafe100 by performing the addition using  Description  code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257  function _setPeriod(uint64 _period) internal {  require(_period > 0, ERROR_TIME_PERIOD_ZERO);  require(openDate == 0 || openDate + _period > getTimestamp64(), ERROR_INVALID_TIME_PERIOD);  period = _period;  Recommendation  Use SafeMath which is already imported to protect from overflow scenarios.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.7 EOPBCTemplate - misleading method names _cacheFundraisingApps and _cacheFundraisingParams    ", "body": "  Resolution                           Fixed with   aragonone/fundraising@0ce7c72 by renaming the functions.  Description  The methods _cacheFundraisingApps and _cacheFundraisingParams suggest that parameters are cached as state variables in the contract similar to the multi-step deployment contract used for AragonBlack/Fundraising. However, the methods are just returning memory structs.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L254-L300  function _cacheFundraisingApps(  Agent          _reserve,  Presale        _presale,  MarketMaker    _marketMaker,  Tap            _tap,  Controller     _controller,  TokenManager   _tokenManager  internal  returns (FundraisingApps memory fundraisingApps)  fundraisingApps.reserve            = _reserve;  fundraisingApps.presale            = _presale;  fundraisingApps.marketMaker        = _marketMaker;  fundraisingApps.tap                = _tap;  fundraisingApps.controller         = _controller;  fundraisingApps.bondedTokenManager = _tokenManager;  function _cacheFundraisingParams(  address       _owner,  string        _id,  ERC20         _collateralToken,  MiniMeToken   _bondedToken,  uint64        _period,  uint256       _exchangeRate,  uint64        _openDate,  uint256       _reserveRatio,  uint256       _batchBlocks,  uint256       _slippage  internal  returns (FundraisingParams fundraisingParams)  fundraisingParams = FundraisingParams({  owner:           _owner,  id:              _id,  collateralToken: _collateralToken,  bondedToken:     _bondedToken,  period:          _period,  exchangeRate:    _exchangeRate,  openDate:        _openDate,  reserveRatio:    _reserveRatio,  batchBlocks:     _batchBlocks,  slippage:        _slippage  });  Recommendation  The functions are only called once throughout the deployment process. The structs can therefore be created directly in the main method. Otherwise rename the functions to properly reflect their purpose.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.8 EOPBCTemplate - Pool should be Agent or Reserve    ", "body": "  Resolution                           Fixed with   aragonone/fundraising@bafe100 by replacing  Description  The documentation refers to an non-existent Pool application.  code/fundraising/templates/externally_owned_presale_bonding_curve/README.md:L58-L68  | App  | Permission             | Grantee          | Manager          |  | ---- | ---------------------- | ---------------- | ---------------- |  | Pool | SAFE_EXECUTE           | Owner            | Owner            |  | Pool | ADD_PROTECTED_TOKEN    | Controller       | Owner            |  | Pool | REMOVE_PROTECTED_TOKEN | NULL             | NULL             |  | Pool | EXECUTE                | NULL             | NULL             |  | Pool | DESIGNATE_SIGNER       | NULL             | NULL             |  | Pool | ADD_PRESIGNED_HASH     | NULL             | NULL             |  | Pool | RUN_SCRIPT             | NULL             | NULL             |  | Pool | TRANSFER               | MarketMaker      | Owner            |  Recommendation  Pool should be Agent or Reserve.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.9 EOPBCTemplate - inconsistent storage location declaration    ", "body": "  Resolution                           Fixed with   aragonone/fundraising@bafe100 by adding the missing storage location declaration.  Description  _cacheFundraisingParams() does not explicitly declare the return value memory location.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L273-L286  function _cacheFundraisingParams(  address       _owner,  string        _id,  ERC20         _collateralToken,  MiniMeToken   _bondedToken,  uint64        _period,  uint256       _exchangeRate,  uint64        _openDate,  uint256       _reserveRatio,  uint256       _batchBlocks,  uint256       _slippage  internal  returns (FundraisingParams fundraisingParams)  _cacheFundraisingApps() explicitly declares to return a copy of the storage struct.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L254-L271  function _cacheFundraisingApps(  Agent          _reserve,  Presale        _presale,  MarketMaker    _marketMaker,  Tap            _tap,  Controller     _controller,  TokenManager   _tokenManager  internal  returns (FundraisingApps memory fundraisingApps)  fundraisingApps.reserve            = _reserve;  fundraisingApps.presale            = _presale;  fundraisingApps.marketMaker        = _marketMaker;  fundraisingApps.tap                = _tap;  fundraisingApps.controller         = _controller;  fundraisingApps.bondedTokenManager = _tokenManager;  Recommendation  Storage declarations should be consistent.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.10 EOPBCTemplate - Keep the template as closely aligned to the audited Company DAO-Template provided by Aragon    ", "body": "  Resolution                           The issue was addressed with   aragonone/fundraising@bafe100 changing the main deployment method from  Description  The EOPBCTemplate is a simplified variant of the AragonBlack/FundraisingMultisigTemplate. The FundraisingMultisigTemplate is initially based on the Aragon/DAO-templates/company-board template.  Please note that the DAO-templates provided by Aragon have recently been audited.  The EOPBCTemplate is similar to the setup established with Aragon/DAO-templates/company. The scenario deploys in one step. However, interface names are different to the audited DAO-template variant (installFundraisingApps vs newInstance). We recommend the template and interface names to be kept as close as possible to the audited company template which established the entry point for deploying a one-step template as newInstance.  Recommendation  Take the Aragon/DAO-templates/company template as a starting point and add relevant parts for the presale variant.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "6.11 EOPBCTemplate - EtherTokenConstant is never used    ", "body": "  Resolution                           Fixed with   aragonone/fundraising@bafe100 by removing the  Description  The constant value EtherTokenConstant.ETH is never used.  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L3  import \"@aragon/os/contracts/common/EtherTokenConstant.sol\";  code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L21  contract EOPBCTemplate is EtherTokenConstant, BaseTemplate {  Recommendation  Remove all references to EtherTokenConstant.  7 Tool-Based Analysis  Several tools were used to perform an automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open-source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium --version  Solium version 1.2.5  $ solium -d contracts  contracts/EOPBCTemplate.sol  118:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  208:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  221:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  224:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  231:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  232:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  233:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  234:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  235:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  236:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  237:8     warning    Line exceeds the limit of 145 characters                                                                  max-len  265:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  266:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  267:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  268:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  269:8     warning    Assignment operator must have exactly single space on both sides of it.                                   operator-whitespace  289:13    warning    Name 'owner': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.           whitespace  290:13    warning    Name 'id': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.              whitespace  292:13    warning    Name 'bondedToken': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.     whitespace  293:13    warning    Name 'period': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.          whitespace  294:13    warning    Name 'exchangeRate': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.    whitespace  295:13    warning    Name 'openDate': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.        whitespace  296:13    warning    Name 'reserveRatio': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.    whitespace  297:13    warning    Name 'batchBlocks': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.     whitespace  298:13    warning    Name 'slippage': Only \"N: V\", \"N : V\" or \"N:V\" spacing style should be used in Name-Value Mapping.        whitespace  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "7.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers (please use horizontal scroll to view all columns):  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonone-aragon-network-presale/"}, {"title": "3.1 Winning pods can be frontrun with large deposits ", "body": "  Description  Pod.depositTo() grants users shares of the pod pool in exchange for tokenAmount of token.  code/pods-v3-contracts/contracts/Pod.sol:L266-L288  function depositTo(address to, uint256 tokenAmount)  external  override  returns (uint256)  require(tokenAmount > 0, \"Pod:invalid-amount\");  // Allocate Shares from Deposit To Amount  uint256 shares = _deposit(to, tokenAmount);  // Transfer Token Transfer Message Sender  IERC20Upgradeable(token).transferFrom(  msg.sender,  address(this),  tokenAmount  );  // Emit Deposited  emit Deposited(to, tokenAmount, shares);  // Return Shares Minted  return shares;  The winner of a prize pool is typically determined by an off-chain random number generator, which requires a request to first be made on-chain. The result of this RNG request can be seen in the mempool and frontrun. In this case, an attacker could identify a winning Pod contract and make a large deposit, diluting existing user shares and claiming the entire prize.  Recommendation  The modifier pauseDepositsDuringAwarding is included in the Pod contract but is unused.  code/pods-v3-contracts/contracts/Pod.sol:L142-L148  modifier pauseDepositsDuringAwarding() {  require(  !IPrizeStrategyMinimal(_prizePool.prizeStrategy()).isRngRequested(),  \"Cannot deposit while prize is being awarded\"  );  _;  Add this modifier to the depositTo() function along with corresponding test cases.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.2 Token transfers may return false ", "body": "  Description  There are a lot of token transfers in the code, and most of them are just calling transfer or transferFrom without checking the return value. Ideally, due to the ERC-20 token standard, these functions should always return True or False (or revert). If a token returns False,  the code will process the transfer as if it succeeds.  Recommendation  Use the safeTransfer  and the safeTransferFrom versions of transfers from OZ.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.3 TokenDrop: Unprotected initialize() function ", "body": "  Description  The TokenDrop.initialize() function is unprotected and can be called multiple times.  code/pods-v3-contracts/contracts/TokenDrop.sol:L81-L87  function initialize(address _measure, address _asset) external {  measure = IERC20Upgradeable(_measure);  asset = IERC20Upgradeable(_asset);  // Set Factory Deployer  factory = msg.sender;  Recommendation  Add the initializer modifier to the initialize() function and include an explicit test that every initialization function in the system can be called once and only once.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.4 Pod: Re-entrancy during deposit or withdrawal can lead to stealing funds ", "body": "  Description  During the deposit, the token transfer is made after the Pod shares are minted:  code/pods-v3-contracts/contracts/Pod.sol:L274-L281  uint256 shares = _deposit(to, tokenAmount);  // Transfer Token Transfer Message Sender  IERC20Upgradeable(token).transferFrom(  msg.sender,  address(this),  tokenAmount  );  That means that if the token allows re-entrancy, the attacker can deposit one more time inside the token transfer. If that happens, the second call will mint more tokens than it is supposed to, because the first token transfer will still not be finished. By doing so with big amounts, it s possible to drain the pod.  Recommendation  Add re-entrancy guard to the external functions.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.5 TokenDrop: Re-entrancy in the claim function can cause to draining funds ", "body": "  Description  code/pods-v3-contracts/contracts/TokenDrop.sol:L139-L153  function claim(address user) external returns (uint256) {  drop();  _captureNewTokensForUser(user);  uint256 balance = userStates[user].balance;  userStates[user].balance = 0;  totalUnclaimed = uint256(totalUnclaimed).sub(balance).toUint112();  // Transfer asset/reward token to user  asset.transfer(user, balance);  // Emit Claimed  emit Claimed(user, balance);  return balance;  Because the totalUnclaimed is already changed, but the current balance is not, the drop function will consider the funds from the unfinished transfer as the new tokens. These tokens will be virtually redistributed to everyone.  After that, the transfer will still happen, and further calls of the drop() function will fail because the following line will revert:  uint256 newTokens = assetTotalSupply.sub(totalUnclaimed);  That also means that any transfers of the Pod token will fail because they all are calling the drop function. The TokenDrop will  unfreeze  only if someone transfers enough tokens to the TokenDrop contract.  The severity of this issue is hard to evaluate because, at the moment, there s not a lot of tokens that allow this kind of re-entrancy.  Recommendation  Simply adding re-entrancy guard to the drop and the claim function won t help because the drop function is called from the claim. For that, the transfer can be moved to a separate function, and this function can have the re-entrancy guard as well as the drop function.  Also, it s better to make sure that _beforeTokenTransfer will not revert to prevent the token from being frozen.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.6 Pod: Having multiple token drops is inconsistent ", "body": "  Description  code/pods-v3-contracts/contracts/Pod.sol:L455-L477  function setTokenDrop(address _token, address _tokenDrop)  external  returns (bool)  require(  msg.sender == factory || msg.sender == owner(),  \"Pod:unauthorized-set-token-drop\"  );  // Check if target<>tokenDrop mapping exists  require(  drops[_token] == TokenDrop(0),  \"Pod:target-tokendrop-mapping-exists\"  );  // Set TokenDrop Referance  drop = TokenDrop(_tokenDrop);  // Set target<>tokenDrop mapping  drops[_token] = drop;  return true;  Recommendation  The mapping seems to be unused, and only one TokenDrop will normally be in the system. If that code is not used, it should be deleted.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.7 Pod: Fees are not limited by a user during the withdrawal ", "body": "  Description  When withdrawing from the Pod, the shares are burned, and the deposit is removed from the Pod. If there are not enough deposit tokens in the contract, the remaining tokens are withdrawn from the pool contract:  code/pods-v3-contracts/contracts/Pod.sol:L523-L532  if (amount > currentBalance) {  // Calculate Withdrawl Amount  uint256 _withdraw = amount.sub(currentBalance);  // Withdraw from Prize Pool  uint256 exitFee = _withdrawFromPool(_withdraw);  // Add Exit Fee to Withdrawl Amount  amount = amount.sub(exitFee);  These tokens are withdrawn with a fee from the pool, which is not controlled or limited by the user.  Recommendation  Allow users to pass a maxFee parameter to control fees.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.8 ProxyFactory.deployMinimal() does not check for contract creation failure ", "body": "  Description  The function ProxyFactory.deployMinimal() is used by both the PodFactory and the TokenDropFactory to deploy minimal proxy contracts. This function uses inline assembly to inline a target address into the minimal proxy and deploys the resulting bytecode. It then emits an event containing the resulting address and optionally makes a low-level call to the resulting address with user-provided data.  The result of a create() operation in assembly will be the zero address in the event that a revert or an exceptional halting state is encountered during contract creation. If execution of the contract initialization code succeeds but returns no runtime bytecode, it is also possible for the create() operation to return a nonzero address that contains no code.  code/pods-v3-contracts/contracts/external/ProxyFactory.sol:L9-L35  function deployMinimal(address _logic, bytes memory _data)  public  returns (address proxy)  // Adapted from https://github.com/optionality/clone-factory/blob/32782f82dfc5a00d103a7e61a17a5dedbd1e8e9d/contracts/CloneFactory.sol  bytes20 targetBytes = bytes20(_logic);  assembly {  let clone := mload(0x40)  mstore(  clone,  0x3d602d80600a3d3981f3363d3d373d3d3d363d73000000000000000000000000  mstore(add(clone, 0x14), targetBytes)  mstore(  add(clone, 0x28),  0x5af43d82803e903d91602b57fd5bf30000000000000000000000000000000000  proxy := create(0, clone, 0x37)  emit ProxyCreated(address(proxy));  if (_data.length > 0) {  (bool success, ) = proxy.call(_data);  require(success, \"ProxyFactory/constructor-call-failed\");  Recommendation  At a minimum, add a check that the resulting proxy address is nonzero before emitting the ProxyCreated event and performing the low-level call. Consider also checking the extcodesize of the proxy address is greater than zero.  Also note that the bytecode in the deployed  Clone  contract was not reviewed due to time constraints.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "3.9 Pod.setManager() checks validity of wrong address ", "body": "  Description  The current check will always pass once the contract is initialized with a nonzero manager. But, the contract can currently be initialized with a manager of IPodManager(address(0)). In this case, the check would prevent the manager from ever being updated.  code/pods-v3-contracts/contracts/Pod.sol:L233-L240  function setManager(IPodManager newManager)  public  virtual  onlyOwner  returns (bool)  // Require Valid Address  require(address(manager) != address(0), \"Pod:invalid-manager-address\");  Recommendation  Change the check to:  require(address(newManager) != address(0), \"Pod:invalid-manager-address\");  More generally, attempt to define validity criteria for all input values that are as strict as possible. Consider preventing zero inputs or inputs that might conflict with other addresses in the smart contract system altogether, including in contract initialization functions.  4 Recommendations  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "4.1 Rename Withdrawl event to Withdrawal", "body": "  Description  The Pod contract contains an event Withdrawl(address, uint256, uint256):  code/pods-v3-contracts/contracts/Pod.sol:L76-L79  /**  @dev Emitted when user withdraws  /  event Withdrawl(address user, uint256 amount, uint256 shares);  This appears to be a misspelling of the word Withdrawal. This is of course not a problem given it s consistent use, but could cause confusion for users or issues in future contract updates.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/pooltogether-pods/"}, {"title": "5.1 ERC20 tokens with no return value will fail to transfer    ", "body": "  Resolution  This issue was addressed using OpenZeppelin s SafeERC20.  Description  Although the ERC20 standard suggests that a transfer should return true on success, many tokens are non-compliant in this regard.  In that case, the .transfer() call here will revert even if the transfer is successful, because solidity will check that the RETURNDATASIZE matches the ERC20 interface.  code/contracts/ExchangeDeposit.sol:L229-L231  if (!instance.transfer(getSendAddress(), forwarderBalance)) {  revert('Could not gather ERC20');  Recommendation  Consider using OpenZeppelin s SafeERC20.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/bitbank/"}, {"title": "3.1 GenesisGroup.commit overwrites previously-committed values ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#16.  Description  commit allows anyone to commit purchased FGEN to a swap that will occur once the genesis group is launched. This commitment may be performed on behalf of other users, as long as the calling account has sufficient allowance:  code/contracts/genesis/GenesisGroup.sol:L87-L94  function commit(address from, address to, uint amount) external override onlyGenesisPeriod {  burnFrom(from, amount);  committedFGEN[to] = amount;  totalCommittedFGEN += amount;  emit Commit(from, to, amount);  The amount stored in the recipient s committedFGEN balance overwrites any previously-committed value. Additionally, this also allows anyone to commit an amount of  0  to any account, deleting their commitment entirely.  Recommendation  Ensure the committed amount is added to the existing commitment.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.2 Purchasing and committing still possible after launch ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#11.  Description  Even after GenesisGroup.launch has successfully been executed, it is still possible to invoke GenesisGroup.purchase and GenesisGroup.commit.  Recommendation  Consider adding validation in GenesisGroup.purchase and GenesisGroup.commit to make sure that these functions cannot be called after the launch.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.3 UniswapIncentive overflow on pre-transfer hooks ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#15.  Description  Before a token transfer is performed, Fei performs some combination of mint/burn operations via UniswapIncentive.incentivize:  code/contracts/token/UniswapIncentive.sol:L49-L65  function incentivize(  address sender,  address receiver,  address operator,  uint amountIn  ) external override onlyFei {  updateOracle();  if (isPair(sender)) {  incentivizeBuy(receiver, amountIn);  if (isPair(receiver)) {  require(isSellAllowlisted(sender) || isSellAllowlisted(operator), \"UniswapIncentive: Blocked Fei sender or operator\");  incentivizeSell(sender, amountIn);  Both incentivizeBuy and incentivizeSell calculate buy/sell incentives using overflow-prone math, then mint / burn from the target according to the results. This may have unintended consequences, like allowing a caller to mint tokens before transferring them, or burn tokens from their recipient.  Examples  incentivizeBuy calls getBuyIncentive to calculate the final minted value:  code/contracts/token/UniswapIncentive.sol:L173-L186  function incentivizeBuy(address target, uint amountIn) internal ifMinterSelf {  if (isExemptAddress(target)) {  return;  (uint incentive, uint32 weight,  Decimal.D256 memory initialDeviation,  Decimal.D256 memory finalDeviation) = getBuyIncentive(amountIn);  updateTimeWeight(initialDeviation, finalDeviation, weight);  if (incentive != 0) {  fei().mint(target, incentive);  getBuyIncentive calculates price deviations after casting amount to an int256, which may overflow:  code/contracts/token/UniswapIncentive.sol:L128-L134  function getBuyIncentive(uint amount) public view override returns(  uint incentive,  uint32 weight,  Decimal.D256 memory initialDeviation,  Decimal.D256 memory finalDeviation  ) {  (initialDeviation, finalDeviation) = getPriceDeviations(-1 * int256(amount));  Recommendation  Ensure casts in getBuyIncentive and getSellPenalty do not overflow.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.4 BondingCurve allows users to acquire FEI before launch ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#59  Description  BondingCurve.allocate allocates the protocol s held PCV, then calls _incentivize, which rewards the caller with FEI if a certain amount of time has passed:  code-update/contracts/bondingcurve/BondingCurve.sol:L180-L186  /// @notice if window has passed, reward caller and reset window  function _incentivize() internal virtual {  if (isTimeEnded()) {  _initTimed(); // reset window  fei().mint(msg.sender, incentiveAmount);  allocate can be called before genesis launch, as long as the contract holds some nonzero PCV. By force-sending the contract 1 wei, anyone can bypass the majority of checks and actions in allocate, and mint themselves FEI each time the timer expires.  Recommendation  Prevent allocate from being called before genesis launch.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.5 Timed.isTimeEnded returns true if the timer has not been initialized ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#62  Description  Timed initialization is a 2-step process:  Timed.duration is set in the constructor: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/utils/Timed.sol#L15-L20  Timed.startTime is set when the method _initTimed is called: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/utils/Timed.sol#L43-L46  Before this second method is called, isTimeEnded() calculates remaining time using a startTime of 0, resulting in the method returning true for most values, even though the timer has not technically been started.  Recommendation  If Timed has not been initialized, isTimeEnded() should return false, or revert  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.6 Overflow/underflow protection ", "body": "  Resolution                           This was partially addressed in   fei-protocol/fei-protocol-core#17 by using  Description  Having overflow/underflow vulnerabilities is very common for smart contracts. It is usually mitigated by using SafeMath or using solidity version ^0.8 (after solidity 0.8 arithmetical operations already have default overflow/underflow protection).  In this code, many arithmetical operations are used without the  safe  version. The reasoning behind it is that all the values are derived from the actual ETH values, so they can t overflow.  On the other hand, some operations can t be checked for overflow/underflow without going much deeper into the codebase that is out of scope:  code/contracts/genesis/GenesisGroup.sol:L131  uint totalGenesisTribe = tribeBalance() - totalCommittedTribe;  Recommendation  In our opinion, it is still safer to have these operations in a safe mode. So we recommend using SafeMath or solidity version ^0.8 compiler.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.7 Unchecked return value for IWETH.transfer call ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#12.  Description  In EthUniswapPCVController, there is a call to IWETH.transfer that does not check the return value:  code/contracts/pcv/EthUniswapPCVController.sol:L122  weth.transfer(address(pair), amount);  It is usually good to add a require-statement that checks the return value or to use something like safeTransfer; unless one is sure the given token reverts in case of a failure.  Recommendation  Consider adding a require-statement or using safeTransfer.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.8 GenesisGroup.emergencyExit remains functional after launch ", "body": "  Resolution                           This was partially addressed in   fei-protocol/fei-protocol-core#14 and  fei-protocol/fei-protocol-core#13 by addressing the last two recommendations.  Description  emergencyExit is intended as an escape mechanism for users in the event the genesis launch method fails or is frozen. emergencyExit becomes callable 3 days after launch is callable. These two methods are intended to be mutually-exclusive, but are not: either method remains callable after a successful call to the other.  This may result in accounting edge cases. In particular, emergencyExit fails to decrease totalCommittedFGEN by the exiting user s commitment:  code/contracts/genesis/GenesisGroup.sol:L185-L188  burnFrom(from, amountFGEN);  committedFGEN[from] = 0;  payable(to).transfer(total);  As a result, calling launch after a user performs an exit will incorrectly calculate the amount of FEI to swap:  code/contracts/genesis/GenesisGroup.sol:L165-L168  uint amountFei = feiBalance() * totalCommittedFGEN / (totalSupply() + totalCommittedFGEN);  if (amountFei != 0) {  totalCommittedTribe = ido.swapFei(amountFei);  Recommendation  Ensure launch cannot be called if emergencyExit has been called  Ensure emergencyExit cannot be called if launch has been called  In emergencyExit, reduce totalCommittedFGEN by the exiting user s committed amount  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.9 Unchecked return value for transferFrom calls ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#12.  Description  There are two transferFrom calls that do not check the return value (some tokens signal failure by returning false):  code/contracts/pool/Pool.sol:L121  stakedToken.transferFrom(from, address(this), amount);  code/contracts/genesis/IDO.sol:L58  fei().transferFrom(msg.sender, address(pair), amountFei);  It is usually good to add a require-statement that checks the return value or to use something like safeTransferFrom; unless one is sure the given token reverts in case of a failure.  Recommendation  Consider adding a require-statement or using safeTransferFrom.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.10 GovernorAlpha proposals may be canceled by the proposer, even after they have been accepted and queued ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#61  Description  GovernorAlpha allows proposals to be canceled via cancel. To cancel a proposal, two conditions must be met by the proposer:  The proposal should not already have been executed: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/dao/GovernorAlpha.sol#L206-L208  The proposer must have under proposalThreshold() TRIBE balance: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/dao/GovernorAlpha.sol#L210-L211  Recommendation  Prevent proposals from being canceled unless they are in the Pending or Active states.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.11 Pool: claiming to the pool itself causes accounting issues ", "body": "  Resolution                           This was addressed in   fei-protocol/fei-protocol-core#57  Description  In Pool.sol, claim(address from, address to) is used to claim staking rewards and send them to a destination address to:  code-update/contracts/pool/Pool.sol:L229-L238  function _claim(address from, address to) internal returns (uint256) {  (uint256 amountReward, uint256 amountPool) = redeemableReward(from);  require(amountPool != 0, \"Pool: User has no redeemable pool tokens\");  _burnFrom(from, amountPool);  _incrementClaimed(amountReward);  rewardToken.transfer(to, amountReward);  return amountReward;  If the destination address to is the pool itself, the pool will burn tokens and increment the amount of tokens claimed, then transfer the reward tokens to itself.  Recommendation  Prevent claims from specifying the pool as a destination.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.12 Assertions that can fail ", "body": "  Description  In UniswapSingleEthRouter there are two assert-statements that may fail:  code/contracts/router/UniswapSingleEthRouter.sol:L21  assert(msg.sender == address(WETH)); // only accept ETH via fallback from the WETH contract  code/contracts/router/UniswapSingleEthRouter.sol:L48  assert(IWETH(WETH).transfer(address(PAIR), amountIn));  Since they do some sort of input validation it might be good to replace them with require-statements. I would only use asserts for checks that should never fail and failure would constitute a bug in the code.  Recommendation  Consider replacing the assert-statements with require-statements. An additional benefit is that this will not result in consuming all the gas in case of a violation.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.13 Simplify API of GenesisGroup.purchase ", "body": "  Description  The API of GenesisGroup.purchase could be simplified by not including the value parameter that is required to be equivalent to msg.value:  code/contracts/genesis/GenesisGroup.sol:L79  require(msg.value == value, \"GenesisGroup: value mismatch\");  Using msg.value might make the API more explicit and avoid requiring msg.value == value. It can also save some gas due to fewer inputs and fewer checks.  Recommendation  Consider dropping the value parameter and changing the code to use msg.value instead.  4 Infrastructure Security Assessment  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.1 Clickjacking and Missing Content Security Policy    ", "body": "  Resolution  After multiple iterations, the following Content Security Policy has been put into effect:  The CSP is transmitted through the following headers:  Content-Security-Policy  X-Content-Security-Policy  X-WebKit-CSP  as well as through corresponding meta HTML tags. Additionally, the following frame-busting JavaScript code has been added to prevent Clickjacking attacks in the unlikely event that existing CSP measures fail or are bypassed:  Description  A content security policy (CSP) provides an added layer of protection against cross-site scripting (XSS), clickjacking, and other client-side attacks that rely on executing malicious content in the context of the website.  Specifically, the lack of a content security policy allows an adversary to perform a clickjacking attack by including the target URL (such as app.fei.money) in an iframe element on their site. The attacker then uses one or more transparent layers on top of the embedded site to trick a user into performing a click action on a different element.  This technique can be used to spawn malicious Metamask dialogues, tricking users into thinking that they are signing a legitimate transaction.  Affected Assets  All S3-hosted web sites.  Recommendation  It is recommended to add content security policy headers to the served responses to prevent browsers from embedding Fei-owned sites into malicious parent sites. Furthermore, CSP can be used to limit the permissions of JavaScript and CSS on the page, which can be used to further harden the deployment against a potential compromise of script dependencies.  It should be noted that security headers should not only be served from Cloudfront but any public-facing endpoint. Otherwise, it will be trivial for an attacker to circumvent the security headers added by Cloudfront, e.g. by embedding the index.html file directly from the public-facing S3 bucket URL.  Besides CSP headers, clickjacking can also be mitigated by directly including frame-busting JavaScript code into the served page.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.2 S3 Buckets Cleartext Communication    ", "body": "  Resolution                           Direct access to S3 buckets through   Description  The system s S3 buckets are configured to allow unencrypted traffic:  Affected Assets  arn:aws:s3:::ropsten-app.fei.money/*  arn:aws:s3:::www.fei.money/*  arn:aws:s3:::feiprotocol.com/*  arn:aws:s3:::www.app.fei.money/*  arn:aws:s3:::www.ropsten-app.fei.money/*  arn:aws:s3:::app.fei.money/*  arn:aws:s3:::fei.money/*  Recommendation  It is recommended to enforce encryption of data in transit using TLS certificates. To accomplish this, the aws:SecureTransport can be set in the S3 bucket s policies.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.3 Missing Log Aggregation    ", "body": "  Resolution  CloudFrond and CloudTrail have been enabled. These components send endpoint-related and organizational log messages into S3 buckets where they can be queried using AWS Athena. The security review process section of this report contains sample queries for Athena.  Description  There is no centralized system that gathers operational events of AWS stack components. This includes S3 server access logs, configuration changes, as well as Cloudfront-related logging.  Recommendation  It is recommended to enable CloudTrail for internal log aggregation as it integrates seamlessly with S3, Cloudfront, and IAM. Furthermore, regular reviews should be set up where system activity is checked to detect suspicious activity as soon as possible.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.4 Enforce Strict Transport Security    ", "body": "  Resolution  All domains in scope now ship with the following header:  Description  The HTTP Strict-Transport-Security response header (often abbreviated as HSTS) lets a web site tell browsers that it should only be accessed using HTTPS, instead of using HTTP. This prevents attackers from stripping TLS certificates from connections and removing encryption.  Recommendation  It is recommended to deliver all responses with the Strict-Transport-Security header. In an S3-Cloudfront setup, this can be achieved using Lambda@Edge lambda functions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.5 Server Information Leak ", "body": "  Description  Responses from the fei.money domain and related assets leak server information in their response headers. This information can be used by an adversary to prepare more sophisticated attacks tailored to the deployed infrastructure.  Note: At the time of reporting, this issue was deemed not possible to fix due to technical limitations on AWS-hosted static sites using S3 and CloudFront.  Examples  Recommendation  It is recommended to remove any headers that hint at server technologies and are not directly required by the frontend.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.6 Missing Route53 Domain Lock    ", "body": "  Resolution                           A transfer lock on both the   Description  Domain registrars often give customers the option to lock a domain. This prevents unauthorized parties from transferring it to another registrar, either through malicious interaction with the registrar itself, or compromised domain owner credentials. No domain currently has a lock enabled.  Affected Assets  fei.money  feiprotocol.com  Recommendation  It is recommended to set a lock for the affected domains, assuming that the registrar allows domain locks:  Sign in to the AWS Management Console and open the Route 53 console at https://console.aws.amazon.com/route53/.  In the navigation pane, choose Registered Domains.  Choose the name of the domain that you want to update.  Choose Enable (to lock the domain) or Disable (to unlock the domain).  Choose Save.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.7 Weak IAM Password Policy    ", "body": "  Resolution  This has been fixed by the client with the following notes:  Enforced 14 character password length  Enabled 90 day password expiration  Prevent password reuse  Require one uppercase, one lowercase, one number, one non-alphanumeric character  Require 2FA on all users via this doc and this post (Create new Force_MFA policy, attach it to the new Engineers group, and then assign all users (including Dominik) to this group  Also requiring 2FA on command line access. Using src/infra/aws-token.sh for generating the credentials and putting them in ~/.aws/config  Description  The password policy for IAM users currently does not enforce the use of strong passwords, multi-factor authentication, and regular password rotation.  Currently, only a minimum password length of 8 is enforced.  Recommendation  Require a minimum password length of 14  Set a password expiration policy of at most 90 days  Disallow the reuse of passwords  Enable mandatory multi-factor authentication with a virtual app  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.8 Review Access Key Expiration    ", "body": "  Resolution  This issue is considered resolved with the implementation of regular security review meetings.  Description  It is recommended to only create access keys when absolutely necessary. There should be no access keys given out to root users. Instead, temporary security credentials (IAM Roles) should be created.  Recommendation  It is recommended to read the Best practices for managing AWS access keys and incorporate the security practices where reasonable.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.9 Dependency Security    ", "body": "  Resolution  This issue has been resolved by implementing Snyk for continuous dependency security scanning. This allows the developers to review potential risks of included packages and receiving automated pull requests with fixes if necessary.  Furthermore, a manual review of select dependencies has been conducted by the penetration tester without significant, actionable results. The following dependencies have been checked:  bignumber  numeral  validator  web-vitals  Description  The Yarn audit feature currently finds two low-severity dependency issues:  Prototype pollution in ini - a dependency of react-scripts  Insecure Credential Storage in web3  Recommendations  It is recommended to apply the ini patch, which is already available. For web3, it is recommended to monitor the repository s Github issue https://github.com/ConsenSys/fei-protocol-audit-2021-01/issues/2739 and upgrade as soon as a fix is available.  For additional dependency security, it is recommended to integrate a security monitoring service. Snyk has a free plan which allows unlimited tests on public repositories, and 200 tests per month for private ones. A bot will automatically add a pull request to bump vulnerable dependency versions.  It should be noted that the quality and reliability of such automated contributions are highly dependent on the quality of the test suite. It is recommended to build strict tests around core functionality and expected dependency behaviour to detect breaking changes as soon as possible.  5 Security Review Process  In an additional effort to achieve security-in-depth, it is recommended to implement a schedule or recurring security review meetings. The goal of these meetings is to complete a checklist to enforce security best-practices, as well as find anomalies in the system as soon as possible to commence mitigation and investigations.  This section outlines recommendations for the contents of such a checklist. It should be noted that security requirements are likely to change, and thus, this list should be treated as a working document as the project s infrastructure and attack surface change.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.1 CloudTrail Anomalies", "body": "  Event filter query template:  SELECT useridentity.username,  sourceipaddress,  eventtime,  additionaleventdata  FROM cloudtrail_logs  WHERE {{ event filter }}  AND eventtime >= '<yyyy-mm-dd>'  AND eventtime < '<yyyy-mm-dd>';  eventname = 'ConsoleLogin'  Sign-in activity  eventname = 'AddUserToGroup'  User added to group  eventname = 'ChangePassword'  User password change  eventname LIKE '%AccessKey%'  Key management events  eventname LIKE '%MFADevice'  MFA deactivation/deletion/resync  eventname = 'StopLogging'  Logging stopped  eventname LIKE '%BucketPolicy%'  Bucket policy activity  eventname LIKE '%GroupPolicy%'  Group policy activity  eventname LIKE '%UserPolicy%'  User policy activity  eventname LIKE '%RolePolicy%'  Role policy activity  Aggregate statistics about failed authentication and user authorization attempts can be gathered with the following query:  SELECT count (*) AS totalEvents,  useridentity.arn,  eventsource,  eventname,  errorCode,  errorMessage  FROM cloudtrail_logs  WHERE (errorcode LIKE '%Denied%'  OR errorcode LIKE '%Unauthorized%')  AND eventtime >= '2021-02-17'  AND eventtime < '2021-02-17'  GROUP BY  eventsource, eventname, errorCode, errorMessage, useridentity.arn  ORDER BY  eventsource, eventname  For investigative purposes or the goal of covering new infrastructure components, it might be necessary to add more event names to the review process. AWS does not provide a comprehensive list of event names per stack component. An external list of CloudTrail event names is available on the GorillaStack blog.  Note: In case of issues with Athena or ingestion into the database, CloudTrail allows users to view the unfiltered event history for a user-specified time range as well. Particularly notable is the ability to filter by resource types, of which the following are relevant to the Fei AWS infrastructure:  AWS::S3::Bucket  AWS::CloudTrail::Trail  AWS::IAM::AccessKey  AWS::IAM::MfaDevice  AWS::IAM::Group  AWS::IAM::Policy  AWS::IAM::Role  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.2 Cloudfront Endpoint Anomalies", "body": "  Top 10 endpoints hit in a given time frame:  SELECT uri,  status,  count(*) AS ct  FROM cloudfront_logs_fei_landing  WHERE date >= DATE('2021-02-01')  AND date <= DATE('2021-02-28')  GROUP BY  uri, status  ORDER BY  ct DESC limit 10  This query can be filtered further by adding AND status = 500 or a similar condition to find suspicious response codes.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.3 Route53 CNAME Review", "body": "  Subdomain takeover vulnerabilities occur when a subdomain is pointing to a service, e.g. a previously deleted CloudFront endpoint or S3 bucket. This allows an attacker to set up a page on the service that was being used and point their page to that subdomain. Especially with wildcard certificates on the system, e.g. *.fei.money, this can lead to an exploitation of user trust and enables attacks that can result in reputational and financial loss.  It is recommended that DNS records in Route53 are reviewed regularly and removed as soon as the underlying resource is decommissioned.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.4 External Monitoring and Notifications", "body": "  Beyond manual checks, it is recommended that a service such as Assertible is used. This will allow the development team to detect unavailable endpoints and enforce regularly-checked assertions, such as proper return codes or page content. Furthermore, such a service should integrate other means of communication such as Slack notifications, SMS messages, or arbitrary webhook calls to notify an on-duty developer as quickly as possible.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.1 Reuse of CHAINID from contract deployment    ", "body": "  Resolution                           This is addressed in   ScopeLift/umbra-protocol@7cfdc81.  Description  The internal function _validateWithdrawSignature() is used to check whether a sponsored token withdrawal is approved by the owner of the stealth address that received the tokens. Among other data, the chain ID is signed over to prevent replay of signatures on other EVM-compatible chains.  contracts/contracts/Umbra.sol:L307-L329  function _validateWithdrawSignature(  address _stealthAddr,  address _acceptor,  address _tokenAddr,  address _sponsor,  uint256 _sponsorFee,  IUmbraHookReceiver _hook,  bytes memory _data,  uint8 _v,  bytes32 _r,  bytes32 _s  ) internal view {  bytes32 _digest =  keccak256(  abi.encodePacked(  \"\\x19Ethereum Signed Message:\\n32\",  keccak256(abi.encode(chainId, version, _acceptor, _tokenAddr, _sponsor, _sponsorFee, address(_hook), _data))  );  address _recoveredAddress = ecrecover(_digest, _v, _r, _s);  require(_recoveredAddress != address(0) && _recoveredAddress == _stealthAddr, \"Umbra: Invalid Signature\");  However, this chain ID is set as an immutable value in the contract constructor. In the case of a future contentious hard fork of the Ethereum network, the same Umbra contract would exist on both of the resulting chains. One of these two chains would be expected to change the network s chain ID, but the Umbra contracts would not be aware of this change. As a result,  signatures to the Umbra contract on either chain would be replayable on the other chain.  This is a common pattern in contracts that implement EIP-712 signatures. Presumably, the motivation in most cases for committing to the chain ID at deployment time is to avoid recomputing the EIP-712 domain separator for every signature verification. In this case, the chain ID is a direct input to the generation of the signed digest, so this should not be a concern.  Recommendation  Replace the use of the chainId immutable value with the CHAINID opcode in _validateWithdrawSignature(). Note that CHAINID is only available using Solidity s inline assembly, so this would need to be accessed in the same way as it is currently accessed in the contract s constructor:  contracts/contracts/Umbra.sol:L68-L72  uint256 _chainId;  assembly {  _chainId := chainid()  5 Recommendations  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.1 Use separate mappings for keys in StealthKeyResolver", "body": "  Description  The StealthKeyResolver currently stores keys in a mapping bytes32 => uint256 => uint256 that maps nodes => prefixes => keys. The prefixes are offset in the setStealthKeys() function to differentiate between viewing public keys and spending public keys, and these offsets are reversed in the stealthKeys() view function.  contracts/profiles/StealthKeyResolver.sol:L37-L56  function setStealthKeys(bytes32 node, uint256 spendingPubKeyPrefix, uint256 spendingPubKey, uint256 viewingPubKeyPrefix, uint256 viewingPubKey) external authorised(node) {  require(  (spendingPubKeyPrefix == 2 || spendingPubKeyPrefix == 3) &&  (viewingPubKeyPrefix == 2 || viewingPubKeyPrefix == 3),  \"StealthKeyResolver: Invalid Prefix\"  );  emit StealthKeyChanged(node, spendingPubKeyPrefix, spendingPubKey, viewingPubKeyPrefix, viewingPubKey);  // Shift the spending key prefix down by 2, making it the appropriate index of 0 or 1  spendingPubKeyPrefix -= 2;  // Ensure the opposite prefix indices are empty  delete _stealthKeys[node][1 - spendingPubKeyPrefix];  delete _stealthKeys[node][5 - viewingPubKeyPrefix];  // Set the appropriate indices to the new key values  _stealthKeys[node][spendingPubKeyPrefix] = spendingPubKey;  _stealthKeys[node][viewingPubKeyPrefix] = viewingPubKey;  This manual adjustment of prefixes adds complexity to an otherwise simple function. To avoid this, consider splitting this into two separate mappings   one for viewing keys and one for spending keys. For clarity, also specify the visibility of these mappings explicitly.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.2 Document potential edge cases for hook receiver contracts", "body": "  Description  The functions withdrawTokenAndCall() and withdrawTokenAndCallOnBehalf() make a call to a hook contract designated by the owner of the withdrawing stealth address.  contracts/contracts/Umbra.sol:L289-L291  if (address(_hook) != address(0)) {  _hook.tokensWithdrawn(_withdrawalAmount, _stealthAddr, _acceptor, _tokenAddr, _sponsor, _sponsorFee, _data);  There are very few constraints on the parameters to these calls in the Umbra contract itself. Anyone can force a call to a hook contract by transferring a small amount of tokens to an address that they control and withdrawing these tokens, passing the target address as the hook receiver. Developers of these UmbraHookReceiver contracts should be sure to validate both the caller of the tokensWithdrawn() function and the function parameters. There are a number of possible edge cases that should be handled when relevant. These include, but are not limited to, the following:  The _amount may not have been transferred to the hook receiver itself.  All four addresses passed to tokensWithdrawn() could be the same. Most of these address parameters could also be any arbitrary address. This includes the token contract address, the address of the hook receiver, or the address of the Umbra contract itself.  The token received may be valueless.  The token received may be malicious. The only requirements are that the token contract address contains code and accepts calls to the ERC20 methods transfer() and transferFrom().  While it is difficult to determine a feasible exploit without knowledge of what hook receiver contracts may do in the future, a slightly contrived example follows.  Suppose a user builds a hook receiver contract that accepts an arbitrary token, TOK, and immediately provides liquidity to the ETH-TOK Uniswap pair when tokensWithdrawn() is called by the Umbra contract. An attacker could create a malicious token that can not be transferred out of its own Uniswap Pair contract and force a call to the hook receiver contract from Umbra. The hook receiver would be able to provide liquidity to the pool but would be unable to remove it, losing any ETH that was provided.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.3 Document token behavior restrictions", "body": "  As with any protocol that interacts with arbitrary ERC20 tokens, it is important to clearly document which tokens are supported. Often this is best done by providing a specification for the behavior of the expected ERC20 tokens and only relaxing this specification after careful review of a particular class of tokens and their interactions with the protocol.  In the absence of this, the following is a necessarily incomplete list of some known deviations from  normal  ERC20 behavior that should be explicitly noted as NOT supported by the Umbra Protocol:  Deflationary or fee-on-transfer tokens: These are tokens in which the balance of the recipient of a transfer may not be increased by the amount of the transfer. There may also be some alternative mechanism by which balances are unexpectedly decreased. While these tokens can be successfully sent via the sendToken() function, the internal accounting of the Umbra contract will be out of sync with the balance as recorded in the token contract, resulting in loss of funds.  Inflationary tokens: The opposite of deflationary tokens. The Umbra contract provides no mechanism for claiming positive balance adjustments.  Rebasing tokens: A combination of the above cases, these are tokens in which an account s balance increases or decreases along with expansions or contractions in supply. The contract provides no mechanism to update its internal accounting in response to these unexpected balance adjustments, and funds may be lost as a result.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.4 Add an address parameter to withdrawal signatures   ", "body": "  Resolution                           This is addressed in   ScopeLift/umbra-protocol@d6e4235, which replaces the  Description  As discussed above, the _validateWithdrawSignature() function checks the signer of a digest consisting of the keccak-256 hash of the following preimage:  abi.encodePacked(  \"\\x19Ethereum Signed Message:\\n32\",  keccak256(abi.encode(chainId, version, _acceptor, _tokenAddr, _sponsor, _sponsorFee, address(_hook), _data))  Consider adding the address of the contract itself to this signed message. Currently, it is possible to deploy any number of contracts with the same version to the same chain, and signatures would be replayable across all of these contracts. While users are likely to only have balances for the same stealth address in one of these contracts, adding an address parameter provides some additional replay protection. Because the contract can not be self-destructed, a given address can only ever contain a single version of the Umbra contract.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "4.1 ESMS use of sanitized user_amount &  user_id values    ", "body": "  Resolution                           Fixed in   https://github.com/nopslip/gtc-request-signer/pull/4/ , by using the sanitized integer value in the code flow.  Description  In the Signer service, values are properly checked, however the checked values are not preserved and the user input is passed down in the function.  The values are sanitized here:  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L98-L108  try:  int(user_id)  except ValueError:  gtc_sig_app.logger.error('Invalid user_id received!')  return Response('{\"message\":\"ESMS error\"}', status=400, mimetype='application/json')  # make sure it's an int  try:  int(user_amount)  except ValueError:  gtc_sig_app.logger.error('Invalid user_amount received!')  return Response('{\"message\":\"ESMS error\"}', status=400, mimetype='application/json')  But the original user inputs are being used here:  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L110-L113  try:  leaf = proofs[str(user_id)]['leaf']  proof = proofs[str(user_id)]['proof']  leaf_bytes = Web3.toBytes(hexstr=leaf)  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L128-L131  # this is a bit of hack to avoid bug in old web3 on frontend  # this means that user_amount is not converted back to wei before tx is broadcast!  user_amount_in_eth = Web3.fromWei(user_amount, 'ether')  Examples  if a float amount is passed for user_amount, all checks will pass, however the final amount will be slightly different that what it is intended:  >>> print(str(Web3.fromWei(123456789012345, 'ether')))  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "0.000123456789012345", "body": "  >>> print(str(Web3.fromWei(123456789012345.123, 'ether')))  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "0.000123456789012345125", "body": "  Recommendation  After the sanity check, use the sanitized value for the rest of the code flow.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.2 Prefer using abi.encode in TokenDistributor    ", "body": "  Resolution                           Fixed in   gitcoinco/governance#7  Description  The method _hashLeaf is called when a user claims their airdrop.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L128-L129  // can we repoduce leaf hash included in the claim?  require(_hashLeaf(user_id, user_amount, leaf), 'TokenDistributor: Leaf Hash Mismatch.');  This method receives the user_id and the user_amount as arguments.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L253-L257  /**  @notice hash user_id + claim amount together & compare results to leaf hash  @return boolean true on match  /  function _hashLeaf(uint32 user_id, uint256 user_amount, bytes32 leaf) private returns (bool) {  These arguments are abi encoded and hashed together to produce a unique hash.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L258  bytes32 leaf_hash = keccak256(abi.encodePacked(keccak256(abi.encodePacked(user_id, user_amount))));  This hash is checked against the third argument for equality.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L259  return leaf == leaf_hash;  If the hash matches the third argument, it returns true and considers the provided user_id and user_amount are correct.  However, packing differently sized arguments may produce collisions.  The Solidity documentation states that packing dynamic types will produce collisions, but this is also the case if packing uint32 and uint256.  Examples  Below there s an example showing that packing uint32 and uint256 in both orders can produce collisions with carefully picked values.  library Encode {  function encode32Plus256(uint32 _a, uint256 _b) public pure returns (bytes memory) {  return abi.encodePacked(_a, _b);  function encode256Plus32(uint256 _a, uint32 _b) public pure returns (bytes memory) {  return abi.encodePacked(_a, _b);  contract Hash {  function checkEqual() public pure returns (bytes32, bytes32) {  // Pack 1  uint32  a1 = 0x12345678;  uint256 b1 = 0x99999999999999999999999999999999999999999999999999999999FFFFFFFF;  // Pack 2  uint256 a2 = 0x1234567899999999999999999999999999999999999999999999999999999999;  uint32  b2 = 0xFFFFFFFF;  // Encode these 2 different values  bytes memory packed1 = Encode.encode32Plus256(a1, b1);  bytes memory packed2 = Encode.encode256Plus32(a2, b2);  // Check if the packed encodings match  require(keccak256(packed1) == keccak256(packed2), \"Hash of representation should match\");  // The hashes are the same  // 0x9e46e582607c5c6e05587dacf66d311c4ced0819378a41d4b4c5adf99d72408e  return (  keccak256(packed1),  keccak256(packed2)  );  Changing abi.encodePacked to abi.encode in the library will make the transaction fail with error message Hash of representation should match.  Recommendation  Unless there s a specific use case to use abi.encodePacked, you should always use abi.encode. You might need a few more bytes in the transaction data, but it prevents collisions. Similar fix can be achieved by using unit256 for both values to be packed to prevent any possible collisions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.3 Simplify claim tokens for a gas discount and less code    ", "body": "  Resolution  Fixed in gitcoinco/governance#4  Structure Claim can still be removed for further optimization.  Description  The method claimTokens in TokenDistributor needs to do a few checks before it can distribute the tokens.  A few of these checks can be simplified and optimized.  The method hashMatch can be removed because it s only used once and the contents can be moved directly into the parent method.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L125-L126  // can we reproduce the same hash from the raw claim metadata?  require(hashMatch(user_id, user_address, user_amount, delegate_address, leaf, eth_signed_message_hash_hex), 'TokenDistributor: Hash Mismatch.');  Because this method also uses a few other internal calls, they also need to be moved into the parent method.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L211  return getDigest(claim) == eth_signed_message_hash_hex;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L184  hashClaim(claim)  Moving the code directly in the parent method and removing them will improve gas costs for users.  The structure Claim can also be removed because it s not used anywhere else in the code.  Recommendation  Consider simplifying claimTokens and remove unused methods.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.4 ESMS use of environment variable for chain info [Optimization]    ", "body": "  Resolution                           Fixed in   nopslip/gtc-request-signer#5 by moving the variables to the environment variable.  Description  Variables to create domain separator are hardcoded in the code, and it requires the modify code on different deployments (e.g. testnet, mainnet, etc).  Examples  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L203-L208  domain = make_domain(  name='GTA',  version='1.0.0',  chainId=4,  verifyingContract='0xBD2525B5F0B2a663439a78A99A06605549D25cE5')  Recommendation  Use environment variable for these values. This way there is no need to change the source code on different deployments and it can be scripted to prevent any possible errors on the code base.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.5 Rename method _hashLeaf to something that represents the validity of the leaf    ", "body": "  Resolution                           Closed because the method was removed in   gitcoinco/governance#4  Description  The method _hashLeaf accepts 3 arguments.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L257  function _hashLeaf(uint32 user_id, uint256 user_amount, bytes32 leaf) private returns (bool) {  The arguments user_id and user_amount are used to create a keccak256 hash.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L258  bytes32 leaf_hash = keccak256(abi.encodePacked(keccak256(abi.encodePacked(user_id, user_amount))));  This hash is then checked if it matches the third argument.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L259  return leaf == leaf_hash;  The result of the equality is returned by the method.  The name of the method is confusing because it should say that it returns true if the leaf is considered valid.  Recommendation  Consider renaming the method to something like isValidLeafHash.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.6 Method returns bool but result is never used in TokenDistributor.claimTokens    ", "body": "  Resolution                           Removed in   gitcoinco/governance#4  Description  The method _delegateTokens is called when a user claims their tokens to automatically delegate the claimed tokens to their own address or to a different one.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L135  _delegateTokens(user_address, delegate_address);  The method accepts the addresses of the delegator and the delegate and returns a boolean.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L262-L270  /**  @notice execute call on token contract to delegate tokens  @return boolean true on success  /  function _delegateTokens(address delegator, address delegatee) private returns (bool) {  GTCErc20  GTCToken = GTCErc20(token);  GTCToken.delegateOnDist(delegator, delegatee);  return true;  But this boolean is never used.  Recommendation  Remove the returned boolean because it s always returned as true anyway and the transaction will be a bit cheaper.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.7 Use a unified compiler version for all contracts    ", "body": "  Resolution                           Compiler versions updated to   gitcoinco/governance#2  Description  Currently the smart contracts for the Gitcoin token and governance use different versions of Solidity compiler (^0.5.16, 0.6.12 , 0.5.17).  Recommendation  It is suggested to use a unified compiler version for all contracts (e.g. 0.6.12).  Note that it is recommended to use the latest version of Solidity compiler with security patches (currently 0.8.3), although given that these contracts are forks of the battle tested Uniswap governance contracts, the Gitcoin team prefer to keep the modifications to the code at minimum.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.8 Improve efficiency by using immutable in TreasuryVester    ", "body": "  Resolution                           Fixed in   gitcoinco/governance#5  Description  The TreasuryVester contract when deployed has a few fixed storage variables.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L30  gtc = gtc_;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L33-L36  vestingAmount = vestingAmount_;  vestingBegin = vestingBegin_;  vestingCliff = vestingCliff_;  vestingEnd = vestingEnd_;  These storage variables are defined in the contract.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L8  address public gtc;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L11-L14  uint public vestingAmount;  uint public vestingBegin;  uint public vestingCliff;  uint public vestingEnd;  But they are never changed.  Recommendation  Consider setting storage variables as immutable type for a considerable gas improvement.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.1 Limiting the Price in the buy and onTokenTransfer Functions    ", "body": "  Resolution                           Fixed   here for the  mitigated in the  Description  When an investor tries to buy the tokens in the Crowdinvesting contract, the buy function does not allow to limit the amount of tokens that can be spent during this particular transaction:  contracts/Crowdinvesting.sol:L277-L279  function buy(uint256 _amount, address _tokenReceiver) public whenNotPaused nonReentrant {  // rounding up to the next whole number. Investor is charged up to one currency bit more in case of a fractional currency bit.  uint256 currencyAmount = Math.ceilDiv(_amount * getPrice(), 10 ** token.decimals());  The owner of the price oracle can front-run the transaction and twist the price.  Of course, the buyer can try to regulate that limit with the token allowance, but there may be some exceptions. Sometimes, users want to give more allowance and buy in multiple transactions over time. Or even give an infinite allowance (not recommended) out of convenience.  The same issue can be found in the onTokenTransfer function. This function works differently because the amount of currency is fixed, and the amount of tokens minted is undefined. Because of that, limiting the allowance won t help, so the user doesn t know how many tokens can be bought.  Recommendation  It s recommended to explicitly limit the amount of tokens that can be transferred from the buyer for the buy function. And allow users to define a minimal amount of tokens bought in the onTokenTransfer function.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.2 Potential Re-Entrancy Attack in the Crowdinvesting Contract    ", "body": "  Resolution  Fixed by storing the currency early in the function to the memory and reusing that value.  Description  The attack requires a set of pre-requisites:  The currency token should have a re-entrancy opportunity inside the token transfer.  The re-entrancy can be done on a token transfer from the _msgSender() to the feeCollector, so there are not a lot of attackers who can potentially execute it.  The owner should be involved in the attack, so it s most likely an attack by the owner.  contracts/Crowdinvesting.sol:L277-L290  function buy(uint256 _amount, address _tokenReceiver) public whenNotPaused nonReentrant {  // rounding up to the next whole number. Investor is charged up to one currency bit more in case of a fractional currency bit.  uint256 currencyAmount = Math.ceilDiv(_amount * getPrice(), 10 ** token.decimals());  (uint256 fee, address feeCollector) = _getFeeAndFeeReceiver(currencyAmount);  if (fee != 0) {  currency.safeTransferFrom(_msgSender(), feeCollector, fee);  currency.safeTransferFrom(_msgSender(), currencyReceiver, currencyAmount - fee);  _checkAndDeliver(_amount, _tokenReceiver);  emit TokensBought(_msgSender(), _amount, currencyAmount);  So on the token transfer to the feeCollector above, the currency parameter can be changed by the owner. And the following token transfer (currency.safeTransferFrom(_msgSender(), currencyReceiver, currencyAmount - fee);) will be made in a different currency.  A possible scenario of the attack could look as follows:  Malicious owner sells tokens for a valuable currency. People are placing allowance for the tokens.  The owner changes the currency to a new one with a much lower price and re-entrancy during transfer.  When a victim wants to buy tokens, the owner reenters on fee transfer and returns the old currency.  The victim transfers the updated currency that is more expensive.  Recommendation  Save the currency in memory at the beginning of the function and use it further.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.3 Lack of Validation of PrivateOffer Initialization Parameters    ", "body": "  Resolution  Addressed by adding extra validation of the parameters.  Description  The PrivateOffer contract allows to create a customised deal for a specific investor. The initialize() function receives parameters to set up the PrivateOffer accordingly.  The following parameters lack of validation during initialization:  tokenAmount  token  currency  tokenAmount  contracts/PrivateOffer.sol:L81-L84  uint256 currencyAmount = Math.ceilDiv(  _arguments.tokenAmount * _arguments.tokenPrice,  10 ** _arguments.token.decimals()  );  tokenAmount is not validated at all. It should be verified to be greater than zero.  token  token is not validated at all. It should be verified to be different than zero address.  currency  currency is  not validated at all. The documentation mentions a restricted list of supported currencies. It should be enforced by checking this parameter against a whitelist of currency addresses.  Recommendation  Enhance the validation of the following parameters: tokenAmount, token, currency.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.4 Lack of Validation of Crowdinvesting Initialization Parameters    ", "body": "  Resolution  Mitigated by adding extra validation.  Description  The Crowdinvesting contract allows everyone who meets the requirements to buy tokens at a fixed price. The initialize() function receives parameters to set up the Crowdinvesting accordingly.  The following parameters lack of validation during initialization:  tokenPrice  minAmountPerBuyer  lastBuyDate  currency  tokenPrice  contracts/Crowdinvesting.sol:L160  require(_arguments.tokenPrice != 0, \"_tokenPrice needs to be a non-zero amount\");  tokenPrice is checked to be different to zero. It should be verified to be in between priceMin and priceMax when these parameters are provided.  minAmountPerBuyer  contracts/Crowdinvesting.sol:L156-L159  require(  _arguments.minAmountPerBuyer <= _arguments.maxAmountPerBuyer,  \"_minAmountPerBuyer needs to be smaller or equal to _maxAmountPerBuyer\"  );  minAmountPerBuyer is checked to be below or equal to maxAmountPerBuyer. It should be verified to not be zero.  lastBuyDate  contracts/Crowdinvesting.sol:L172  lastBuyDate = _arguments.lastBuyDate;  lastBuyDate is not validated at all. It should be verified to be greater than the current block.timestamp. Currently, a Crowdinvesting contract with lastBuyDate parameter set to a value (different than zero) below block.timestamp will not be able to sell any token.  contracts/Crowdinvesting.sol:L249-L265  function _checkAndDeliver(uint256 _amount, address _tokenReceiver) internal {  require(tokensSold + _amount <= maxAmountOfTokenToBeSold, \"Not enough tokens to sell left\");  require(tokensBought[_tokenReceiver] + _amount >= minAmountPerBuyer, \"Buyer needs to buy at least minAmount\");  require(  tokensBought[_tokenReceiver] + _amount <= maxAmountPerBuyer,  \"Total amount of bought tokens needs to be lower than or equal to maxAmount\"  );  if (lastBuyDate != 0 && block.timestamp > lastBuyDate) {  revert(\"Last buy date has passed: not selling tokens anymore.\");  tokensSold += _amount;  tokensBought[_tokenReceiver] += _amount;  token.mint(_tokenReceiver, _amount);  currency  contracts/Crowdinvesting.sol:L154  require(address(_arguments.currency) != address(0), \"currency can not be zero address\");  currency is checked to be different than zero. The documentation mentions a restricted list of supported currencies. It should be enforced by checking this parameter against a whitelist of currency addresses.  Recommendation  Enhance the validation of the following parameters: tokenPrice, tokenPrice,  lastBuyDate, currency.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.5 Keeping Denominators of Fees Is Redundant   ", "body": "  Resolution  Addressed by major code refactoring of the  Description  All the fees are stored as numerators and denominators. That requires storing two numbers instead of just one. That solution increases code size and makes a comparison of two different values more complicated:  ../code-a3/contracts/FeeSettings.sol:L264-L269  defaultTokenFeeNumerator = proposedDefaultFees.tokenFeeNumerator;  defaultTokenFeeDenominator = proposedDefaultFees.tokenFeeDenominator;  defaultCrowdinvestingFeeNumerator = proposedDefaultFees.crowdinvestingFeeNumerator;  defaultCrowdinvestingFeeDenominator = proposedDefaultFees.crowdinvestingFeeDenominator;  defaultPrivateOfferFeeNumerator = proposedDefaultFees.privateOfferFeeNumerator;  defaultPrivateOfferFeeDenominator = proposedDefaultFees.privateOfferFeeDenominator;  ../code-a3/contracts/FeeSettings.sol:L306-L313  function _isFractionAGreater(  uint32 aNumerator,  uint32 aDenominator,  uint32 bNumerator,  uint32 bDenominator  ) internal pure returns (bool) {  return uint256(aNumerator) * bDenominator > uint256(bNumerator) * aDenominator;  Recommendation  Instead, the common practice is to fix the denominator for all values as a constant, for example, 10e5. That would keep the code simpler, shorter, and cheaper.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.6 Non-Normalized Salt Computation   ", "body": "  Resolution                           Addressed by using   Description  Currently, the salt is computed doing a keccak256 hash of encoded parameters that will define the characteristics of the contract to be deployed. However, the encoding method is not consistent across different contracts.  CrowdinvestingCloneFactory salt computation is based on abi.encode encoding of parameters  contracts/factories/CrowdinvestingCloneFactory.sol:L62-L69  function _getSalt(  bytes32 _rawSalt,  address _trustedForwarder,  CrowdinvestingInitializerArguments memory _arguments  ) internal pure returns (bytes32) {  return keccak256(abi.encode(_rawSalt, _trustedForwarder, _arguments));  PriceLinearCloneFactory salt computation is based on abi.encodePacked encoding of parameters  contracts/factories/PriceLinearCloneFactory.sol:L122-L147  function _generateSalt(  bytes32 _rawSalt,  address _trustedForwarder,  address _owner,  uint64 _slopeEnumerator,  uint64 _slopeDenominator,  uint64 _startTimeOrBlockNumber,  uint32 _stepDuration,  bool _isBlockBased,  bool _isRising  ) internal pure returns (bytes32) {  return  keccak256(  abi.encodePacked(  _rawSalt,  _trustedForwarder,  _owner,  _slopeEnumerator,  _slopeDenominator,  _startTimeOrBlockNumber,  _stepDuration,  _isBlockBased,  _isRising  );  PrivateOfferFactory salt computation is based on abi.encode encoding of parameters  contracts/factories/PrivateOfferFactory.sol:L170-L182  function _getSalt(  bytes32 _rawSalt,  PrivateOfferArguments calldata _arguments,  uint64 _vestingStart,  uint64 _vestingCliff,  uint64 _vestingDuration,  address _vestingContractOwner  ) private pure returns (bytes32) {  return  keccak256(  abi.encode(_rawSalt, _arguments, _vestingStart, _vestingCliff, _vestingDuration, _vestingContractOwner)  );  TokenFactory salt computation is based on abi.encodePacked encoding of parameters  contracts/factories/TokenProxyFactory.sol:L125-L148  function _getSalt(  bytes32 _rawSalt,  address _trustedForwarder,  IFeeSettingsV2 _feeSettings,  address _admin,  AllowList _allowList,  uint256 _requirements,  string memory _name,  string memory _symbol  ) private pure returns (bytes32) {  return  keccak256(  abi.encodePacked(  _rawSalt,  _trustedForwarder,  _feeSettings,  _admin,  _allowList,  _requirements,  _name,  _symbol  );  VestingCloneFactory salt computation is based on abi.encodePacked encoding of parameters  contracts/factories/VestingCloneFactory.sol:L31  bytes32 salt = keccak256(abi.encodePacked(_rawSalt, _trustedForwarder, _owner, _token));  Recommendation  Normalize the computation of the salt that will be used along with create2 feature. Note, as a reminder, that it is preferable to use abi.encode in order to prevent any potential hash collisions when encoding dynamic types variables.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.7 Unused or Redundant Imports in Multiple Contracts   ", "body": "  Resolution  Fixed.  Description  Multiple contracts import unused or redundant libraries.  PrivateOffer  contracts/PrivateOffer.sol:L4  import \"@openzeppelin/contracts/token/ERC20/extensions/IERC20Metadata.sol\";  PriceLinear  contracts/PriceLinear.sol:L6  import \"@openzeppelin/contracts-upgradeable/security/ReentrancyGuardUpgradeable.sol\";  VestingCloneFactory  contracts/factories/VestingCloneFactory.sol:L7  import \"@openzeppelin/contracts/proxy/Clones.sol\";  PrivateOfferFactory  contracts/factories/PrivateOfferFactory.sol:L4-L5  import \"@openzeppelin/contracts/token/ERC20/extensions/IERC20Metadata.sol\";  import \"@openzeppelin/contracts/utils/math/SafeCast.sol\";  PriceLinearCloneFactory  contracts/factories/PriceLinearCloneFactory.sol:L7  import \"@openzeppelin/contracts/proxy/Clones.sol\";  CrowdinvestingCloneFactory.sol  contracts/factories/CrowdinvestingCloneFactory.sol:L7  import \"@openzeppelin/contracts/proxy/Clones.sol\";  Recommendation  Remove unused or redundant imports.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.8 Missing Events on Important State Changes   ", "body": "  Resolution                           Events   added.  Description  The setLastBuyDate() function from Crowdinvesting contract updates the lastBuyDate state variable without emitting an event.  setLastBuyDate()  contracts/Crowdinvesting.sol:L390-L393  function setLastBuyDate(uint256 _lastBuyDate) external onlyOwner whenPaused {  lastBuyDate = _lastBuyDate;  coolDownStart = block.timestamp;  Recommendation  Emit an event on important state change.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.9 DynamicPricingActivated Event Is Emitted Twice   ", "body": "  Resolution  Fixed.  Description  The DynamicPricingActivated(address,uint256,uint256) event is emitted twice when activating the dynamic price feature.  DynamicPricingActivated  contracts/Crowdinvesting.sol:L186-L212  function activateDynamicPricing(  IPriceDynamic _priceOracle,  uint256 _priceMin,  uint256 _priceMax  ) external onlyOwner whenPaused {  _activateDynamicPricing(_priceOracle, _priceMin, _priceMax);  coolDownStart = block.timestamp;  emit DynamicPricingActivated(address(_priceOracle), _priceMin, _priceMax);  /**  Activates dynamic pricing and sets the price oracle, as well as the minimum and maximum price.  @param _priceOracle this address is queried for the current price of a token  @param _priceMin price will never be less that this  @param _priceMax price will never be more than this  /  function _activateDynamicPricing(IPriceDynamic _priceOracle, uint256 _priceMin, uint256 _priceMax) internal {  require(address(_priceOracle) != address(0), \"_priceOracle can not be zero address\");  priceOracle = _priceOracle;  require(_priceMin <= priceBase, \"priceMin needs to be smaller or equal to priceBase\");  priceMin = _priceMin;  require(priceBase <= _priceMax, \"priceMax needs to be larger or equal to priceBase\");  priceMax = _priceMax;  coolDownStart = block.timestamp;  emit DynamicPricingActivated(address(_priceOracle), _priceMin, _priceMax);  Recommendation  Remove DynamicPricingActivated event emitted in the activateDynamicPricing() function.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/tokenize-it/"}, {"title": "4.1 Missing Input Validation for WalletAddress    ", "body": "  Resolution                           The client acknowledged the issue and fixed it by implementing a regex validation in PR#25   here - Snap shasum  Description  The snap prompts users to input the wallet address to be monitored. Users can set wallet addreses that do not adhere to the common Ethereum address format. The user input is not sanitized. This could lead to various injection vulnerabilities such as markdown or control character injections that could break other components. In particular, the address is sent to the API as a URL query parameter. A malicious attacker could try using that to mount URL injection attacks.  packages/snap/src/index.ts:L50-L61  if (  request.method === RpcRequestMethods.UpdateAccount &&  'walletAddress' in request.params &&  typeof request.params.walletAddress === 'string'  ) {  const { walletAddress } = request.params;  if (!walletAddress) {  throw new Error('no wallet address provided');  updateWalletAddress(walletAddress);  Recommendation  Sanitize the address string input by the user and reject all addresses that do not adhere to the Ethereum address format.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.2 Server Should Not Rely on Clients  Randomness    ", "body": "  Resolution  Severity decreased: Major  > Medium: The client acknowledged the issue, and let us know that the ID is only used for analytics purposes, to be compatible with the existing API. A future release of the API will improve UID handling.  Description  The snap code sends a request to the Wallet Guard API with a random UUID crypto.randomUUID() generated by the client. We would like to underline that the API should never trust clients  randomness nor assume any property about it. Relying on client-generated randomness for the API could lead to many vulnerabilities, such as replay attacks or collision issues due to the inability to ensure uniqueness. The varying algorithms used by clients may be subpar or even compromised. As this id is not used anywhere else in the snap code, we assume that it might be used on the API side. Because the API is not in scope for this review, we don t have access to the code and cannot tell whether this pseudo-random UUID is used in a safe way.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  Recommendation  Don t rely on clients  randomness on the API. Instead, the server should assign a unique ID to every incoming request.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.3 Properties of the transaction Object Might Be Undefined ", "body": "  Description  The Metamask Snaps API does not guarantee that the properties from and method of the transaction object are defined. Depending on the transaction type, it could happen that these properties are not defined. This would result in a runtime error when undefined is casted to string.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  Recommendation  One should check whether properties from, and method are defined, before explicitly casting them to a string. This could be done by introducing a hasProperty utility function for instance.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.4 AssetChangeComponent Displays  a Change With Value 0 if fiatValue < 0.005 ", "body": "  Description  The toFixed(2) method rounds the transaction value string to 2 decimals. For transactions with fiatValue < 0.005, the function returns 0, meaning the component will display a transaction with zero value to the user, even if the transaction has a small yet non-zero value. This is not a good idea as it might trick the user. In that case, it would be better to default to the smallest value that can represented (i.e. 0.01) instead of 0.  packages/snap/src/components/stateChanges/AssetChangeComponent.ts:L18  const fiatValue = Number(stateChange.fiatValue).toFixed(2);  Recommendation  If fiatValue < 0.005, consider displaying a value of 0.01 to the user, instead of 0.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.5 Incomplete NatSpec and General Documentation ", "body": "  Description  The code is missing NatSpec documentation in many places. NatSpec documentation plays an important role in improving code comprehension and maintenance. Adding NatSpec documentation to functions with significant logic that provides clear explanations of behavior, inputs, and outputs enhances code readability, transparency, and maintainability of the codebase.  Recommendation  We recommend adding NatSpec documentation to every function that contains significant logic. Especially all the Snaps handlers. This will improve the readability, transparency, and maintainability of the codebase. We also recommend adding a detailed high-level documentation about the Snaps features, components, and permissions in the README.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.6 formatFiatValue() Can Be Simplified ", "body": "  Description  The function formatFiatValue formats a number to a string that is displayed to the user. The function formats numbers with at most 2 decimal digits, removes the trailing zeros, and adds commas as thousands separators.  The function first converts the number to a string representing the number in fixed-point notation. Then, it uses regex to remove the trailing zeros if they exist. Finally, it adds the thousands separators.  packages/snap/src/utils/helpers.ts:L16-L26  export const formatFiatValue = (  fiatValue: string,  maxDecimals: number,  ): string => {  const fiatWithRoundedDecimals = Number(fiatValue)  .toFixed(maxDecimals) // round to maxDecimals  .replace(/\\.00$/u, ''); // removes 00 if it exists  const fiatWithCommas = numberWithCommas(fiatWithRoundedDecimals); // add commas  return `$${fiatWithCommas}`;  };  The design of the function is unnecessarily complex. The whole design could be simplified using the native toLocaleString() function with appropriate parameters.  Recommendation  Simplify the design by using the native toLocaleString function. For instance, the function could be used as follows toLocaleString('en-US',{minimumFractionDigits: 0, maximumFractionDigits: 2})  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.7 No Way to Disable Approvals Checking, and Transaction Analytics ", "body": "  Description  Currently, there is no easy way to disable wallet approval monitoring and/or transaction simulation apart from uninstalling the snap. Users might want to opt out of wallet monitoring or disable transaction simulation selectively e.g., for privacy concerns.  Recommendation  We would recommend implementing a mechanism that allows users to selectively disable the snap features.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.8 devDependencies Erroneously Listed as dependencies ", "body": "  Description  The following dependencies are only used for development purpose and should therefore be listed as  devDependencies  instead of  dependencies  in the package.json file. Indeed, the TypeScript code is compiled into a bundle, which is released. Meaning the snap  production  code should not contain any external dependency.  packages/snap/package.json:L28-L31  \"dependencies\": {  \"@metamask/snaps-types\": \"^0.32.2\",  \"@metamask/snaps-ui\": \"^0.32.2\"  },  Recommendation  List the dependencies as  devDependencies .  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.9 package.json - Missing Author ", "body": "  Description  The package.json file is missing the author name, the link to the project homepage, and to the bug tracker.  Recommendation  According to package publishing best practices, we recommend adding those elements to the package.json file.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.10 Extra  If  Statement", "body": "  Description  The onRpcRequest() handler returns early if walletAddress is not defined.  packages/snap/src/index.ts:L57-L59  if (!walletAddress) {  throw new Error('no wallet address provided');  Thus, the extra  if  check before calling snap.request() is superfluous and can be removed.  packages/snap/src/index.ts:L57-L64  if (!walletAddress) {  throw new Error('no wallet address provided');  updateWalletAddress(walletAddress);  if (walletAddress) {  await snap.request({  Recommendation  Remove the extra  if  check.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.11 Misleading Comment", "body": "  Description  The NatSpec comment indicates that onRpcRequest() returns  the result of snap_dialog  while the method either does not return anything, or returns the Ethereum address of the monitored wallet.  packages/snap/src/index.ts:L24-L34  /**  Handle incoming JSON-RPC requests, sent through `wallet_invokeSnap`.  @param args - The request handler args as object.  @param args.origin - The origin of the request, e.g., the website that  invoked the snap.  @param args.request - A validated JSON-RPC request object.  @returns The result of `snap_dialog`.  @throws If the request method is not valid for this snap.  /  Recommendation  Fix the comment.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.12 Wallet Monitoring Improvements", "body": "  Description  The snap allows the user to set an arbitrary wallet address to be monitored for dangerous approvals. This feature is only of limited use and could be improved by:  Allowing to specify multiple addresses to monitor (a wallet typically consists of many accounts that are managed under the wallet key)  Allowing users to fetch connected addresses via the ethereum API directly instead of requiring the user to input valid accounts  For privacy reasons, allowing users to opt out of transaction analytics on a per-account basis (Currently, every transaction and transaction origin is sent to the API, even if no monitored wallet address is set).  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "4.13 Consider Submitting Snap Version With Backend API Requests", "body": "  Description  Consider adding the snap package version to the API requests in order to get insights about what snap versions are used in the field. This could be useful for future debugging and forensics when multiple snap versions will coexist.  packages/snap/src/http/fetchTransaction.ts:L32-L40  const simulateRequest: SimulateRequestParams = {  id: crypto.randomUUID(),  chainID: mappedChainId,  signer: transaction.from as string,  origin: transactionOrigin as string,  method: transaction.method as string,  transaction,  source: 'SNAP',  };  packages/snap/src/types/simulateApi.ts:L25-L35  export type SimulateRequestParams = {  id: string;  chainID: string;  signer: string;  origin: string;  method: string;  transaction: {  [key: string]: Json;  };  source: 'SNAP';  };  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/wallet-guard/"}, {"title": "5.1 didTransferShares function has no access control modifier    ", "body": "  Resolution                           The concerned function has now been restricted to be only called by   146 with final commit hash as  Description  The staked tokens (shares) in Forta are meant to be transferable. Similarly, the rewards allocation for these shares for delegated staking is meant to be transferable as well. This allocation for the shares  owner is tracked in the StakeAllocator. To enable this, the Forta staking contract FortaStaking implements a _beforeTokenTransfer() function that calls _allocator.didTransferShares() when it is appropriate to transfer the underlying allocation.  code/contracts/components/staking/FortaStaking.sol:L572-L585  function _beforeTokenTransfer(  address operator,  address from,  address to,  uint256[] memory ids,  uint256[] memory amounts,  bytes memory data  ) internal virtual override {  for (uint256 i = 0; i < ids.length; i++) {  if (FortaStakingUtils.isActive(ids[i])) {  uint8 subjectType = FortaStakingUtils.subjectTypeOfShares(ids[i]);  if (subjectType == DELEGATOR_NODE_RUNNER_SUBJECT && to != address(0) && from != address(0)) {  _allocator.didTransferShares(ids[i], subjectType, from, to, amounts[i]);  Due to this, the StakeAllocator.didTransferShares() has an external visibility so it can be called from the FortaStaking contract to perform transfers. However, there is no access control modifier to allow only the staking contract to call this. Therefore, anyone can call this function with whatever parameters they want.  code/contracts/components/staking/allocation/StakeAllocator.sol:L341-L349  function didTransferShares(  uint256 sharesId,  uint8 subjectType,  address from,  address to,  uint256 sharesAmount  ) external {  _rewardsDistributor.didTransferShares(sharesId, subjectType, from, to, sharesAmount);  Recommendation  Apply access control modifiers as appropriate for this contract, for example onlyRole().  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.2 Incorrect reward epoch start date calculation    ", "body": "  Resolution                           The suggested recommendations have been implemented in a pull request   144 with a final hash as  Description  The Forta rewards system is based on epochs. A privileged address with the role REWARDER_ROLE calls the reward() function with a parameter for a specific epochNumber that consequently distributes the rewards for that epoch. Additionally, as users stake and delegate their stake, accounts in the Forta system accrue weight that is based on the active stake to distribute these rewards. Since accounts can modify their stake as well as delegate or un-delegate it, the rewards weight for each account can be modified, as seen, for example, in the didAllocate() function. In turn, this modifies the DelegatedAccRewards storage struct that stores the accumulated rewards for each share id. To keep track of changes done to the accumulated rewards, epochs with checkpoints are used to manage the accumulated rate of rewards, their value at the checkpoint, and the timestamp of the checkpoint.  For example, in the didAllocate() function the addRate() function is being called to modify the accumulated rewards.  code/contracts/components/staking/rewards/RewardsDistributor.sol:L89-L101  function didAllocate(  uint8 subjectType,  uint256 subject,  uint256 stakeAmount,  uint256 sharesAmount,  address staker  ) external onlyRole(ALLOCATOR_CONTRACT_ROLE) {  bool delegated = getSubjectTypeAgency(subjectType) == SubjectStakeAgency.DELEGATED;  if (delegated) {  uint8 delegatorType = getDelegatorSubjectType(subjectType);  uint256 shareId = FortaStakingUtils.subjectToActive(delegatorType, subject);  DelegatedAccRewards storage s = _rewardsAccumulators[shareId];  s.delegated.addRate(stakeAmount);  Then the function flow goes into setRate() that checks the existing accumulated rewards storage and modifies it based on the current timestamp.  code/contracts/components/staking/rewards/Accumulators.sol:L34-L36  function addRate(Accumulator storage acc, uint256 rate) internal {  setRate(acc, latest(acc).rate + rate);  code/contracts/components/staking/rewards/Accumulators.sol:L42-L50  function setRate(Accumulator storage acc, uint256 rate) internal {  EpochCheckpoint memory ckpt = EpochCheckpoint({ timestamp: SafeCast.toUint32(block.timestamp), rate: SafeCast.toUint224(rate), value: getValue(acc) });  uint256 length = acc.checkpoints.length;  if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {  acc.checkpoints[length - 1] = ckpt;  } else {  acc.checkpoints.push(ckpt);  Namely, it pushes epoch checkpoints to the list of account checkpoints based on its timestamp. If the last checkpoint s timestamp is during the current epoch, then the last checkpoint is replaced with the new one altogether. If the last checkpoint s timestamp is different from the current epoch, a new checkpoint is added to the list. However, the isCurrentEpoch() function calls a function getCurrentEpochTimestamp() that incorrectly determines the start date of the current epoch. In particular, it doesn t take the offset into account when calculating how many epochs have already passed.  code/contracts/components/staking/rewards/Accumulators.sol:L103-L110  function getCurrentEpochTimestamp() internal view returns (uint256) {  return ((block.timestamp / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET;  function isCurrentEpoch(uint256 timestamp) internal view returns (bool) {  uint256 currentEpochStart = getCurrentEpochTimestamp();  return timestamp > currentEpochStart;  Instead of ((block.timestamp / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET, it should be (((block.timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET. In fact, it should simply call the getEpochNumber() function that correctly provides the epoch number for any timestamp.  code/contracts/components/staking/rewards/Accumulators.sol:L95-L97  function getEpochNumber(uint256 timestamp) internal pure returns (uint32) {  return SafeCast.toUint32((timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH);  In other words, the resulting function would look something like the following:  code/contracts/components/staking/rewards/Accumulators.sol:L45-L48  if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {  acc.checkpoints[length - 1] = ckpt;  } else {  acc.checkpoints.push(ckpt);  This causes several checkpoints to be stored for the same epoch, which would cause issues in functions such as getAtEpoch(), that feeds into getValueAtEpoch() function that provides data for the rewards  share calculation. In the end, this would cause issues in the accounting for the rewards calculation resulting in incorrect distributions.  During the discussion with the Forta Foundation team, it was additionally discovered that there are edge cases around the limits of epochs. Specifically, epoch s end time and the subsequent epoch s start time are exactly the same, although it should be that it is only the start of the next epoch. Similarly, that start time isn t recognized as part of the epoch due to > sign instead of >=. In particular, the following changes need to be made:  Recommendation  A refactor of the epoch timestamp calculation functions is recommended to account for:  The correct epoch number to calculate the start and end timestamps of epochs.  The boundaries of epochs coinciding.  Clarity in functions  intent. For example, adding a function just to calculate any epoch s start time and renaming getCurrentEpochTimestamp() to getCurrentEpochStartTimestamp().  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.3 A single unfreeze dismisses all other slashing proposal freezes    ", "body": "  Resolution                           As per the recommendation, the Forta team modified the logic in favor of open proposals. Now, every   149 with a final hash  Description  In order to retaliate against malicious actors, the Forta staking system allows users to submit slashing proposals that are guarded by submitting along a deposit with a slashing reason. These proposals immediately freeze the proposal s subject s stake, blocking them from withdrawing that stake.  At the same time, there can be multiple proposals submitted against the same subject, which works out with freezing   the subject remains frozen with each proposal submitted. However, once any one of the active proposals against the subject gets to the end of its lifecycle, be it REJECTED, DISMISSED, EXECUTED, or REVERTED, the subject gets unfrozen altogether. The other proposals might still be active, but the stake is no longer frozen, allowing the subject to withdraw it if they would like.  In terms of impact, this allows bad actors to avoid punishment intended by the slashes and freezes. A malicious actor could, for example, submit a faulty proposal against themselves in the hopes that it will get quickly rejected or dismissed while the existing, legitimate proposals against them are still being considered. This would allow them to get unfrozen quickly and withdraw their stake. Similarly, in the event a bad staker has several proposals against them, they could withdraw right after a single slashing proposal goes through.  Examples  code/contracts/components/staking/slashing/SlashingController.sol:L174-L179  function dismissSlashProposal(uint256 _proposalId, string[] calldata _evidence) external onlyRole(SLASHING_ARBITER_ROLE) {  _transition(_proposalId, DISMISSED);  _submitEvidence(_proposalId, DISMISSED, _evidence);  _returnDeposit(_proposalId);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L187-L192  function rejectSlashProposal(uint256 _proposalId, string[] calldata _evidence) external onlyRole(SLASHING_ARBITER_ROLE) {  _transition(_proposalId, REJECTED);  _submitEvidence(_proposalId, REJECTED, _evidence);  _slashDeposit(_proposalId);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L215-L229  function reviewSlashProposalParameters(  uint256 _proposalId,  uint8 _subjectType,  uint256 _subjectId,  bytes32 _penaltyId,  string[] calldata _evidence  ) external onlyRole(SLASHING_ARBITER_ROLE) onlyInState(_proposalId, IN_REVIEW) onlyValidSlashPenaltyId(_penaltyId) onlyValidSubjectType(_subjectType) notAgencyType(_subjectType, SubjectStakeAgency.DELEGATOR) {  // No need to check for proposal existence, onlyInState will revert if _proposalId is in undefined state  if (!subjectGateway.isRegistered(_subjectType, _subjectId)) revert NonRegisteredSubject(_subjectType, _subjectId);  _submitEvidence(_proposalId, IN_REVIEW, _evidence);  if (_subjectType != proposals[_proposalId].subjectType || _subjectId != proposals[_proposalId].subjectId) {  _unfreeze(_proposalId);  _freeze(_subjectType, _subjectId);  code/contracts/components/staking/slashing/SlashingController.sol:L254-L259  function revertSlashProposal(uint256 _proposalId, string[] calldata _evidence) external {  _authorizeRevertSlashProposal(_proposalId);  _transition(_proposalId, REVERTED);  _submitEvidence(_proposalId, REVERTED, _evidence);  _unfreeze(_proposalId);  code/contracts/components/staking/slashing/SlashingController.sol:L267-L272  function executeSlashProposal(uint256 _proposalId) external onlyRole(SLASHER_ROLE) {  _transition(_proposalId, EXECUTED);  Proposal memory proposal = proposals[_proposalId];  slashingExecutor.slash(proposal.subjectType, proposal.subjectId, getSlashedStakeValue(_proposalId), proposal.proposer, slashPercentToProposer);  slashingExecutor.freeze(proposal.subjectType, proposal.subjectId, false);  code/contracts/components/staking/slashing/SlashingController.sol:L337-L339  function _unfreeze(uint256 _proposalId) private {  slashingExecutor.freeze(proposals[_proposalId].subjectType, proposals[_proposalId].subjectId, false);  Recommendation  Introduce a check in the unfreezing mechanics to first ensure there are no other active proposals for that subject.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.4 Storage gap variables slightly off from the intended size    ", "body": "  Resolution  The Forta Team worked on the storage layout to maintain a consistent storage buffer in the inheritance tree. The changes were made through multiple pull requests, also an easy-to-understand layout description has been added through a pull request 157. However, we still found some inconsistencies and recommend doing a thorough review of the buffer space again.  For instance, in FortaStaking (considering the latest commit)  the above-mentioned storage variables will be taking a single slot, however, separate slots are considered for the buffer space(referring to the storage layout description to determine __gap buffer).  Description  The Forta staking system is using upgradeable proxies for its deployment strategy. To avoid storage collisions between contract versions during upgrades, uint256[] private __gap array variables are introduced that create a storage buffer. Together with contract state variables, the storage slots should sum up to 50. For example, the  __gap variable is present in the BaseComponentUpgradeable component, which is the base of most Forta contracts, and there is a helpful comment in AgentRegistryCore that describes how its relevant __gap variable size was calculated:  code/contracts/components/BaseComponentUpgradeable.sol:L62  uint256[50] private __gap;  code/contracts/components/agents/AgentRegistryCore.sol:L196  uint256[41] private __gap; // 50 - 1 (frontRunningDelay) - 3 (_stakeThreshold) - 5 StakeSubjectUpgradeable  However, there are a few places where the __gap size was not computed correctly to get the storage slots up to 50. Some of these are:  code/contracts/components/scanners/ScannerRegistry.sol:L234  uint256[49] private __gap;  code/contracts/components/dispatch/Dispatch.sol:L333  uint256[47] private __gap;  code/contracts/components/node_runners/NodeRunnerRegistryCore.sol:L452  uint256[44] private __gap;  While these still provide large storage buffers, it is best if the __gap variables are calculated to hold the same buffer within contracts of similar types as per the initial intentions to avoid confusion.  During conversations with the Forta Foundation team, it appears that some contracts like ScannerRegistry and AgentRegistry should instead add up to 45 with their __gap variable due to the StakeSubject contracts they inherit from adding 5 from themselves. This is something to note and be careful with as well for future upgrades.  Recommendation  Provide appropriate sizes for the __gap variables to have a consistent storage layout approach that would help avoid storage issues with future versions of the system.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.5 AgentRegistryCore - Agent Creation DoS    ", "body": "  Resolution  The Forta team as per the recommendations modified the minting logic to allow users to mint an agentId only for their own address in a pull request 155 with final hash as 7426891222e2bcdf2bbbec669905d5041f9fb58e. Also, the team claims that the Agent Ids are generated through the Forta Bot SDK to minimize the collision risk. However, this has not been verified by the auditing team.  We still recommend notifying users to check whether an ID is already registered prior to making any commitment if a front-running delay is enabled, to avoid unintended DoS.  Description  AgentRegistryCore allows anyone to mint an agentID for the desired owner address. However, in some cases, it may fall prey to DoS, either deliberately or unintentionally.  For instance, let s assume the Front Running Protection is disabled or the frontRunningDelay is 0. It means anyone can directly create an agent without any prior commitment. Thus, anyone can observe pending transactions and try to front run them to mint an agentID prior to the victim s restricting it to mint a desired agentID.  Also, it may be possible that a malicious actor succeeds in frontrunning a transaction with manipulated data/chainIDs but with the same owner address and agentID. There is a good chance that victim still accepts the attacker s transaction as valid, even though its own transaction reverted, due to the fact that the victim is still seeing itself as the owner of that ID.  Taking an instance where let s assume the frontrunning protection is enabled. Still, there is a good chance that two users vouch for the same agentIDs and commits in the same block, thus getting the same frontrunning delay. Then, it will be a game of luck, whoever creates that agent first will get the ID minted to its address, and the other user s transaction will be reverted wasting the time they have spent on the delay.  As the agentIDs can be picked by users, the chances of collisions with an already minted ID will increase over time causing unnecessary reverts for others.  Adding to the fact that there is no restriction for owner address, anyone can spam mint any agentID to any address for any profitable reason.  Examples  code/contracts/components/agents/AgentRegistryCore.sol:L68-L77  function createAgent(uint256 agentId, address owner, string calldata metadata, uint256[] calldata chainIds)  public  onlySorted(chainIds)  frontrunProtected(keccak256(abi.encodePacked(agentId, owner, metadata, chainIds)), frontRunningDelay)  _mint(owner, agentId);  _beforeAgentUpdate(agentId, metadata, chainIds);  _agentUpdate(agentId, metadata, chainIds);  _afterAgentUpdate(agentId, metadata, chainIds);  Recommendation  Modify function prepareAgent to not commit an already registered agentID.  A better approach could be to allow sequential minting of agentIDs using some counters.  Only allow users to mint an agentID, either for themselves or for someone they are approved to.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.6 Lack of checks for rewarding an epoch that has already been rewarded    ", "body": "  Resolution                           The suggested recommendations have been implemented in a pull request   150 with final hash  Description  To give rewards to the participating stakers, the Forta system utilizes reward epochs for each shareId, i.e. a delegated staking share. Each epoch gets their own reward distribution, and then StakeAllocator and RewardsDistributor contracts along with the Forta staking shares determine how much the users get.  Although totalRewardsDistributed is essentially isolated to the sweep() function to allow transferring out the reward tokens without taking away those tokens reserved for the reward distribution, this still creates an inconsistency, albeit a minor one in the context of the current system.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L155-L167  function reward(  uint8 subjectType,  uint256 subjectId,  uint256 amount,  uint256 epochNumber  ) external onlyRole(REWARDER_ROLE) {  if (subjectType != NODE_RUNNER_SUBJECT) revert InvalidSubjectType(subjectType);  if (!_subjectGateway.isRegistered(subjectType, subjectId)) revert RewardingNonRegisteredSubject(subjectType, subjectId);  uint256 shareId = FortaStakingUtils.subjectToActive(getDelegatorSubjectType(subjectType), subjectId);  _rewardsPerEpoch[shareId][epochNumber] = amount;  totalRewardsDistributed += amount;  emit Rewarded(subjectType, subjectId, amount, epochNumber);  Recommendation  Implement checks as appropriate to the reward() function to ensure correct behavior of totalRewardsDistributed tracking. Also, implement necessary changes to the tracking of pending rewards, if necessary.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.7 Reentrancy in FortaStaking during ERC1155 mints    ", "body": "  Resolution                           The Forta team implemented a Reentrancy Guard in a pull request   151 with a final hash  Description  In the Forta staking system, the staking shares (both  active  and  inactive ) are represented as tokens implemented according to the ERC1155 standard. The specific implementation that is being used utilizes a smart contract acceptance check _doSafeTransferAcceptanceCheck() upon mints to the recipient.  code/contracts/components/staking/FortaStaking.sol:L54  contract FortaStaking is BaseComponentUpgradeable, ERC1155SupplyUpgradeable, SubjectTypeValidator, ISlashingExecutor, IStakeMigrator {  The specific implementation for ERC1155SupplyUpgradeable contracts can be found here, and the smart contract check can be found here.  This opens up reentrancy into the system s flow. In fact, the reentrancy occurs on all mints that happen in the below functions, and it happens before a call to another Forta contract for allocation is made via either _allocator.depositAllocation or _allocator.withdrawAllocation:  code/contracts/components/staking/FortaStaking.sol:L273-L295  function deposit(  uint8 subjectType,  uint256 subject,  uint256 stakeValue  ) external onlyValidSubjectType(subjectType) notAgencyType(subjectType, SubjectStakeAgency.MANAGED) returns (uint256) {  if (address(subjectGateway) == address(0)) revert ZeroAddress(\"subjectGateway\");  if (!subjectGateway.isStakeActivatedFor(subjectType, subject)) revert StakeInactiveOrSubjectNotFound();  address staker = _msgSender();  uint256 activeSharesId = FortaStakingUtils.subjectToActive(subjectType, subject);  bool reachedMax;  (stakeValue, reachedMax) = _getInboundStake(subjectType, subject, stakeValue);  if (reachedMax) {  emit MaxStakeReached(subjectType, subject);  uint256 sharesValue = stakeToActiveShares(activeSharesId, stakeValue);  SafeERC20.safeTransferFrom(stakedToken, staker, address(this), stakeValue);  _activeStake.mint(activeSharesId, stakeValue);  _mint(staker, activeSharesId, sharesValue, new bytes(0));  emit StakeDeposited(subjectType, subject, staker, stakeValue);  _allocator.depositAllocation(activeSharesId, subjectType, subject, staker, stakeValue, sharesValue);  return sharesValue;  code/contracts/components/staking/FortaStaking.sol:L303-L326  function migrate(  uint8 oldSubjectType,  uint256 oldSubject,  uint8 newSubjectType,  uint256 newSubject,  address staker  ) external onlyRole(SCANNER_2_NODE_RUNNER_MIGRATOR_ROLE) {  if (oldSubjectType != SCANNER_SUBJECT) revert InvalidSubjectType(oldSubjectType);  if (newSubjectType != NODE_RUNNER_SUBJECT) revert InvalidSubjectType(newSubjectType);  if (isFrozen(oldSubjectType, oldSubject)) revert FrozenSubject();  uint256 oldSharesId = FortaStakingUtils.subjectToActive(oldSubjectType, oldSubject);  uint256 oldShares = balanceOf(staker, oldSharesId);  uint256 stake = activeSharesToStake(oldSharesId, oldShares);  uint256 newSharesId = FortaStakingUtils.subjectToActive(newSubjectType, newSubject);  uint256 newShares = stakeToActiveShares(newSharesId, stake);  _activeStake.burn(oldSharesId, stake);  _activeStake.mint(newSharesId, stake);  _burn(staker, oldSharesId, oldShares);  _mint(staker, newSharesId, newShares, new bytes(0));  emit StakeDeposited(newSubjectType, newSubject, staker, stake);  _allocator.depositAllocation(newSharesId, newSubjectType, newSubject, staker, stake, newShares);  code/contracts/components/staking/FortaStaking.sol:L365-L387  function initiateWithdrawal(  uint8 subjectType,  uint256 subject,  uint256 sharesValue  ) external onlyValidSubjectType(subjectType) returns (uint64) {  address staker = _msgSender();  uint256 activeSharesId = FortaStakingUtils.subjectToActive(subjectType, subject);  if (balanceOf(staker, activeSharesId) == 0) revert NoActiveShares();  uint64 deadline = SafeCast.toUint64(block.timestamp) + _withdrawalDelay;  _lockingDelay[activeSharesId][staker].setDeadline(deadline);  uint256 activeShares = Math.min(sharesValue, balanceOf(staker, activeSharesId));  uint256 stakeValue = activeSharesToStake(activeSharesId, activeShares);  uint256 inactiveShares = stakeToInactiveShares(FortaStakingUtils.activeToInactive(activeSharesId), stakeValue);  SubjectStakeAgency agency = getSubjectTypeAgency(subjectType);  _activeStake.burn(activeSharesId, stakeValue);  _inactiveStake.mint(FortaStakingUtils.activeToInactive(activeSharesId), stakeValue);  _burn(staker, activeSharesId, activeShares);  _mint(staker, FortaStakingUtils.activeToInactive(activeSharesId), inactiveShares, new bytes(0));  if (agency == SubjectStakeAgency.DELEGATED || agency == SubjectStakeAgency.DELEGATOR) {  _allocator.withdrawAllocation(activeSharesId, subjectType, subject, staker, stakeValue, activeShares);  Although this doesn t seem to be an issue in the current Forta system of contracts since the allocator s logic doesn t seem to be manipulable, this could still be dangerous as it opens up an external execution flow.  Recommendation  Consider introducing a reentrancy check or emphasize this behavior in the documentation, so that both other projects using this system later and future upgrades along with maintenance work on the Forta staking system itself are implemented safely.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.8 Unnecessary code blocks that check the same condition    ", "body": "  Resolution                           The code block has been refactored under a single conditional block as per the suggested recommendation in a pull request   152 with a final hash as  Description  In the RewardsDistributor there is a function that allows to set delegation fees for a NodeRunner. It adjusts the fees[] array for that node as appropriate. However, during its checks, it performs the same check twice in a row.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L259-L264  if (fees[1].sinceEpoch != 0) {  if (Accumulators.getCurrentEpochNumber() < fees[1].sinceEpoch + delegationParamsEpochDelay) revert SetDelegationFeeNotReady();  if (fees[1].sinceEpoch != 0) {  fees[0] = fees[1];  Recommendation  Consider refactoring this under a single code block.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.9 Event spam in RewardsDistributor.claimRewards    ", "body": "  Resolution  Forta team has implemented the recommended check in a pull request 153, as: if (epochRewards == 0) revert ZeroAmount(\"epochRewards\");  The implemented check will now be reverting the transaction if there exists no reward for an epoch number. However, it may not be a gas-efficient approach for the user claiming rewards and accidentally passing an incorrect epoch number. A better approach could be to transfer any reward and emit any event only for a non-zero epochReward.  Description  The RewardsDistributor contract allows users to claim their rewards through the claimRewards() function. It does check to see whether or not the user has already claimed the rewards for a specific epoch that they are claiming for, but it does not check to see if the user has any associated rewards at all. This could lead to event ClaimedRewards being spammed by malicious users, especially on low gas chains.  Examples  code/contracts/components/staking/rewards/RewardsDistributor.sol:L224-L229  for (uint256 i = 0; i < epochNumbers.length; i++) {  if (_claimedRewardsPerEpoch[shareId][epochNumbers[i]][_msgSender()]) revert AlreadyClaimed();  _claimedRewardsPerEpoch[shareId][epochNumbers[i]][_msgSender()] = true;  uint256 epochRewards = _availableReward(shareId, isDelegator, epochNumbers[i], _msgSender());  SafeERC20.safeTransfer(rewardsToken, _msgSender(), epochRewards);  emit ClaimedRewards(subjectType, subjectId, _msgSender(), epochNumbers[i], epochRewards);  Recommendation  Add a check for rewards amounts being greater than 0.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.10 SubjectTypes.sol files unused    ", "body": "  Resolution                           The unused file has now been removed in commit   2548e0a4f7b38926362a759f4fa0611394348d6e  Description  There is a rogue file SubjectTypes.sol that is not being utilized. It appears that its intended functionality is being done by the SubjectTypeValidator.sol file as it even has a contract with the same name implemented there.  Examples  code/contracts/components/staking/SubjectTypes.sol:L4-L10  pragma solidity ^0.8.9;  uint8 constant SCANNER_SUBJECT = 0;  uint8 constant AGENT_SUBJECT = 1;  uint8 constant NODE_RUNNER_SUBJECT = 3;  contract SubjectTypeValidator {  Recommendation  Remove the SubjectTypes.sol file.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.11 Lack of a check for the subject s stake for reviewSlashProposalParameters    ", "body": "  Resolution                           The recommended check has now been added in a pull request   154 with final hash as  Description  While it may be assumed that the review function will be called by a privileged and knowledgeable actor, this additional check may avoid accidental mistakes.  Examples  code/contracts/components/staking/slashing/SlashingController.sol:L153  if (subjectGateway.totalStakeFor(_subjectType, _subjectId) == 0) revert ZeroAmount(\"subject stake\");  code/contracts/components/staking/slashing/SlashingController.sol:L226-L229  if (_subjectType != proposals[_proposalId].subjectType || _subjectId != proposals[_proposalId].subjectId) {  _unfreeze(_proposalId);  _freeze(_subjectType, _subjectId);  Recommendation  Add a check for the new subject having stake to slash.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "5.12 Comment and code inconsistencies    ", "body": "  Resolution                           The comments have now been found fixed as per the implemented logic, primarily in the pull request   156 and in another commit with hash  f4ee799ee192084965643b09b69f3cbeababd5ae  Description  During the audit a few inconsistencies were found between what the comments say and what the implemented code actually did.  Examples  Subject Type Agency for Scanner Subjects  In the SubjectTypeValidator, the comment says that the SCANNER_SUBJECT is of type DIRECT agency type, i.e. it can be directly staked on by multiple different stakers. However, we found a difference in the implementation, where the concerned subject is defined as type MANAGED agency type, which says that it cannot be staked on directly; instead it s a delegated type and the allocation is supposed to be managed by its manager.  code/contracts/components/staking/SubjectTypeValidator.sol:L21  code/contracts/components/staking/SubjectTypeValidator.sol:L66-L67  - SCANNER_SUBJECT --> DIRECT  code/contracts/components/staking/SubjectTypeValidator.sol:L66-L67  } else if (subjectType == SCANNER_SUBJECT) {  return SubjectStakeAgency.MANAGED;  Dispatch refers to ERC721 tokens as ERC1155  One of the comments describing the functionality to link and unlink agents and scanners refers to them as ERC1155 tokens, when in reality they are ERC721.  code/contracts/components/dispatch/Dispatch.sol:L179-L185  /**  @notice Assigns the job of running an agent to a scanner.  @dev currently only allowed for DISPATCHER_ROLE (Assigner software).  @dev emits Link(agentId, scannerId, true) event.  @param agentId ERC1155 token id of the agent.  @param scannerId ERC1155 token id of the scanner.  /  NodeRunnerRegistryCore comment that implies the reverse of what happens  A comment describing a helper function that returns address for a given scanner ID describes the opposite behavior. It is the same comment for the function just above that actually does what the comment says.  code/contracts/components/node_runners/NodeRunnerRegistryCore.sol:L259-L262  /// Converts scanner address to uint256 for FortaStaking Token Id.  function scannerIdToAddress(uint256 scannerId) public pure returns (address) {  return address(uint160(scannerId));  ScannerToNodeRunnerMigration comment that says that no NodeRunner tokens must be owned  For the migration from Scanners to NodeRunners, a comment in the beginning of the file implies that for the system to work correctly, there must be no NodeRunner tokens owned prior to migration. After a conversation with the Forta Foundation team, it appears that this was an early design choice that is no longer relevant.  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L69  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L91  @param nodeRunnerId If set as 0, a new NodeRunnerRegistry ERC721 will be minted to nodeRunner (but it must not own any prior),  code/contracts/components/scanners/ScannerToNodeRunnerMigration.sol:L91  Recommendation  @param nodeRunnerId If set as 0, a new NodeRunnerRegistry ERC721 will be minted to nodeRunner (but it must not own any prior),  Recommendation  Verify the operational logic and fix either the concerned comments or defined logic as per the need.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/11/forta-delegated-staking/"}, {"title": "3.1 The Hypervisor.deposit function does not check the msg.sender    ", "body": "  Resolution                           Partially fixed in   GammaStrategies/hypervisor@9a7a3dd, by allowing only  Description  Hypervisor.deposit pulls pre-approved ERC20 tokens from the from address to the contract. Later it mints shares to the to address. Attackers can determine both the from and to addresses as they wish, and thus steal shares (that can be redeemed to tokens immediately) from users that pre-approved the contract to spend ERC20 tokens on their behalf.  Recommendation  As described in issue 3.5, we recommend restricting access to this function only for UniProxy. Moreover, the UniProxy contract should validate that from == msg.sender.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.2 UniProxy.depositSwap - Tokens are not approved before calling Router.exactInput    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  the call to Router.exactInputrequires the sender to pre-approve the tokens. We could not find any reference for that, thus we assume that a call to UniProxy.depositSwap will always revert.  Examples  code/contracts/UniProxy.sol:L202-L234  router = ISwapRouter(_router);  uint256 amountOut;  uint256 swap;  if(swapAmount < 0) {  //swap token1 for token0  swap = uint256(swapAmount * -1);  IHypervisor(pos).token1().transferFrom(msg.sender, address(this), deposit1+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit0  );  else{  //swap token1 for token0  swap = uint256(swapAmount);  IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit1  );  Recommendation  Consider approving the exact amount of input tokens before the swap.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.3 Uniproxy.depositSwap - _router should not be determined by the caller    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  Uniproxy.depositSwap uses _router that is determined by the caller, which in turn might inject a  fake  contract, and thus may steal funds stuck in the UniProxy contract.  The UniProxy contract has certain trust assumptions regarding the router. The router is supposed to return not less than deposit1(or deposit0) amount of tokens but that fact is never checked.  Examples  code/contracts/UniProxy.sol:L168-L177  function depositSwap(  int256 swapAmount, // (-) token1, (+) token0 for token1; amount to swap  uint256 deposit0,  uint256 deposit1,  address to,  address from,  bytes memory path,  address pos,  address _router  ) external returns (uint256 shares) {  Recommendation  Consider removing the _router parameter from the function, and instead, use a storage variable that will be initialized in the constructor.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.4 Re-entrancy + flash loan attack can invalidate price check    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  The UniProxy contract has a price manipulation protection:  code/contracts/UniProxy.sol:L75-L82  if (twapCheck || positions[pos].twapOverride) {  // check twap  checkPriceChange(  pos,  (positions[pos].twapOverride ? positions[pos].twapInterval : twapInterval),  (positions[pos].twapOverride ? positions[pos].priceThreshold : priceThreshold)  );  But after that, the tokens are transferred from the user, if the token transfer allows an attacker to hijack the call-flow of the transaction inside, the attacker can manipulate the Uniswap price there, after the check happened. The Hypervisor s deposit function itself is vulnerable to the flash-loan attack.  Recommendation  Make sure the price does not change before the Hypervisor.deposit call. For example, the token transfers can be made at the beginning of the UniProxy.deposit function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.5 The deposit function of the Hypervisor contract should only be called from UniProxy    ", "body": "  Resolution                           Partially fixed in   GammaStrategies/hypervisor@9a7a3dd, by allowing only  Description  The deposit function is designed to be called only from the UniProxy  contract, but everyone can call it. This function does not have any protection against price manipulation in the Uniswap pair. A deposit can be frontrunned, and the depositor s funds may be  stolen .  Recommendation  Make sure only UniProxy can call the deposit function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.6 UniProxy.properDepositRatio - Proper ratio will not prevent liquidity imbalance for all possible scenarios    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  Examples  code/contracts/UniProxy.sol:L258-L275  function properDepositRatio(  address pos,  uint256 deposit0,  uint256 deposit1  ) public view returns (bool) {  (uint256 hype0, uint256 hype1) = IHypervisor(pos).getTotalAmounts();  if (IHypervisor(pos).totalSupply() != 0) {  uint256 depositRatio = deposit0 == 0 ? 10e18 : deposit1.mul(1e18).div(deposit0);  depositRatio = depositRatio > 10e18 ? 10e18 : depositRatio;  depositRatio = depositRatio < 10e16 ? 10e16 : depositRatio;  uint256 hypeRatio = hype0 == 0 ? 10e18 : hype1.mul(1e18).div(hype0);  hypeRatio = hypeRatio > 10e18 ? 10e18 : hypeRatio;  hypeRatio = hypeRatio < 10e16 ? 10e16 : hypeRatio;  return (FullMath.mulDiv(depositRatio, deltaScale, hypeRatio) < depositDelta &&  FullMath.mulDiv(hypeRatio, deltaScale, depositRatio) < depositDelta);  return true;  Recommendation  Consider removing the cap of [0.1,10] both for depositRatio and for hypeRatio.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.7 UniProxy - SafeERC20 is declared but safe functions are not used    ", "body": "  Resolution                           fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  The UniProxy contract declares the usage of the SafeERC20 library for functions of the IERC20 type. However, unsafe functions are used instead of safe ones.  Examples  Usage of approve instead of safeApprove  Usage of transferFrom instead of safeTransferFrom.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.8 Missing/wrong implementation    ", "body": "  Resolution  Fixed in GammaStrategies/hypervisor@9a7a3dd by introducing two new functions: toggleDepositOverride, setPriceThresholdPos.  Fixed in GammaStrategies/hypervisor@9a7a3dd by keeping only the version of deposit function with 4 parameters.  Fixed in GammaStrategies/hypervisor@9a7a3dd by removing the unreachable code.  Examples  The UniProxy contract has different functions used for setting the properties of a position. However, Position.priceThreshold, and Position.depositOverride are never assigned to, even though they are being used.  UniProxy.deposit is calling IHypervisor.deposit multiple times with different function signatures (3 and 4 parameters), while the Hypervisor contract only implements the version with 4 parameters, and does not implement the IHypervisor interface.  Hypervisor.uniswapV3MintCallback | uniswapV3SwapCallback - both these functions contain unreachable code, namely  the case where payer != address(this).  Recommendations  Consider adding functions to set these properties, or alternatively, a single function to set the properties of a position.  Consider supporting a single deposit function for IHypervisor, and make sure that the actual implementation adheres to this interface.  Consider deleting these lines.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.9 Hypervisor.withdraw - Possible reentrancy    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  Recommendation  Consider adding a ReentrancyGuard both to Hypervisor.withdraw and Hypervisor.deposit  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.10 UniProxy.depositSwap doesn t deposit all the users  funds    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  When executing the swap, the minimal amount out is passed to the router (deposit1  in this example), but the actual swap amount will be amountOut. But after the trade, instead of depositing amountOut, the contract tries to deposit deposit1, which is lower. This may result in some users  funds staying in the UniProxy contract.  code/contracts/UniProxy.sol:L220-L242  else{  //swap token1 for token0  swap = uint256(swapAmount);  IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit1  );  require(amountOut > 0, \"Swap failed\");  if (positions[pos].version < 2) {  // requires lp token transfer from proxy to msg.sender  shares = IHypervisor(pos).deposit(deposit0, deposit1, address(this));  IHypervisor(pos).transfer(to, shares);  Recommendation  Deposit all the user s funds to the Hypervisor.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.11 Hypervisor - Multiple  sandwiching  front running vectors    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by removing the call to  Description  The amount of tokens received from UniswapV3Pool functions might be manipulated by front-runners due to the decentralized nature of AMMs, where the order of transactions can not be pre-determined. A potential  sandwicher  may insert a buying order before the user s call to Hypervisor.rebalance for instance, and a sell order after.  More specifically, calls to pool.swap, pool.mint, pool.burn are susceptible to  sandwiching  vectors.  Examples  Hypervisor.rebalance  code/contracts/Hypervisor.sol:L278-L286  if (swapQuantity != 0) {  pool.swap(  address(this),  swapQuantity > 0,  swapQuantity > 0 ? swapQuantity : -swapQuantity,  swapQuantity > 0 ? TickMath.MIN_SQRT_RATIO + 1 : TickMath.MAX_SQRT_RATIO - 1,  abi.encode(address(this))  );  code/contracts/Hypervisor.sol:L348-L363  function _mintLiquidity(  int24 tickLower,  int24 tickUpper,  uint128 liquidity,  address payer  ) internal returns (uint256 amount0, uint256 amount1) {  if (liquidity > 0) {  (amount0, amount1) = pool.mint(  address(this),  tickLower,  tickUpper,  liquidity,  abi.encode(payer)  );  code/contracts/Hypervisor.sol:L365-L383  function _burnLiquidity(  int24 tickLower,  int24 tickUpper,  uint128 liquidity,  address to,  bool collectAll  ) internal returns (uint256 amount0, uint256 amount1) {  if (liquidity > 0) {  // Burn liquidity  (uint256 owed0, uint256 owed1) = pool.burn(tickLower, tickUpper, liquidity);  // Collect amount owed  uint128 collect0 = collectAll ? type(uint128).max : _uint128Safe(owed0);  uint128 collect1 = collectAll ? type(uint128).max : _uint128Safe(owed1);  if (collect0 > 0 || collect1 > 0) {  (amount0, amount1) = pool.collect(to, tickLower, tickUpper, collect0, collect1);  Recommendation  Consider adding an amountMin parameter(s) to ensure that at least the amountMin of tokens was received.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.12 Full test suite is necessary ", "body": "  Description  The test suite at this stage is not complete. It is crucial to have a full test coverage that includes the edge cases and failure scenarios, especially for complex system like Gamma.  As we ve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.  Some issues such as issue 3.8, issue 3.2 could be caught by a full-coverage test suite.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.13 Uniswap v3 callbacks access control should be hardened    ", "body": "  Resolution                           Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation for  Description  Examples  code/contracts/Hypervisor.sol:L407-L445  function uniswapV3MintCallback(  uint256 amount0,  uint256 amount1,  bytes calldata data  ) external override {  require(msg.sender == address(pool));  address payer = abi.decode(data, (address));  if (payer == address(this)) {  if (amount0 > 0) token0.safeTransfer(msg.sender, amount0);  if (amount1 > 0) token1.safeTransfer(msg.sender, amount1);  } else {  if (amount0 > 0) token0.safeTransferFrom(payer, msg.sender, amount0);  if (amount1 > 0) token1.safeTransferFrom(payer, msg.sender, amount1);  function uniswapV3SwapCallback(  int256 amount0Delta,  int256 amount1Delta,  bytes calldata data  ) external override {  require(msg.sender == address(pool));  address payer = abi.decode(data, (address));  if (amount0Delta > 0) {  if (payer == address(this)) {  token0.safeTransfer(msg.sender, uint256(amount0Delta));  } else {  token0.safeTransferFrom(payer, msg.sender, uint256(amount0Delta));  } else if (amount1Delta > 0) {  if (payer == address(this)) {  token1.safeTransfer(msg.sender, uint256(amount1Delta));  } else {  token1.safeTransferFrom(payer, msg.sender, uint256(amount1Delta));  Recommendation  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.14 Code quality comments    ", "body": "  Resolution  Fixed in GammaStrategies/hypervisor@9a7a3dd by removing the from parameter.  Fixed in GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Fixed in GammaStrategies/hypervisor@9a7a3dd by deleting depositSwap.  Examples  UniProxy.deposit - from parameter is never used.  UniProxy - MAX_INT should be changed to MAX_UINT.  Consider using compiler version >= 0.8.0, and make sure that the compiler version is specified explicitly for every .sol file in the repo.  UniProxy - Minimize code duplication in deposit and depositSwap.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "5.1 TokenFaucet refill can have an unexpected outcome ", "body": "  Description  The TokenFaucet contract can only disburse tokens to the users if it has enough balance. When the contract is running out of tokens, it stops dripping.  code/pool-contracts/contracts/token-faucet/TokenFaucet.sol:L119-L138  uint256 assetTotalSupply = asset.balanceOf(address(this));  uint256 availableTotalSupply = assetTotalSupply.sub(totalUnclaimed);  uint256 newSeconds = currentTimestamp.sub(lastDripTimestamp);  uint256 nextExchangeRateMantissa = exchangeRateMantissa;  uint256 newTokens;  uint256 measureTotalSupply = measure.totalSupply();  if (measureTotalSupply > 0 && availableTotalSupply > 0 && newSeconds > 0) {  newTokens = newSeconds.mul(dripRatePerSecond);  if (newTokens > availableTotalSupply) {  newTokens = availableTotalSupply;  uint256 indexDeltaMantissa = measureTotalSupply > 0 ? FixedPoint.calculateMantissa(newTokens, measureTotalSupply) : 0;  nextExchangeRateMantissa = nextExchangeRateMantissa.add(indexDeltaMantissa);  emit Dripped(  newTokens  );  The owners of the faucet can decide to refill the contract so it can disburse tokens again. If there s been a lot of time since the faucet was drained, the lastDripTimestamp value can be far behind the currentTimestamp. In that case, the users can instantly withdraw some amount (up to all the balance) right after the refill.  Recommendation  To avoid uncertainty, it s essential to call the drip function before the refill. If this call is made in a separate transaction, the owner should make sure that this transaction was successfully mined before sending tokens for the refill.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.2 Gas Optimization on transfers ", "body": "  Description  In TokenFaucet, on every transfer _captureNewTokensForUser is called twice. This function does a few calculations and writes the latest UserState to the storage. However, if lastExchangeRateMantissa == exchangeRateMantissa, or in other words, two transfers happen in the same block, there are no changes in the newToken amounts, so there is an extra storage store with the same values.  Examples  deltaExchangeRateMantissa will be 0 in case two transfers ( no matter from or to) are in the same block for a user.  /pool-contracts/contracts/token-faucet/TokenFaucet.sol  uint256 deltaExchangeRateMantissa = uint256(exchangeRateMantissa).sub(userState.lastExchangeRateMantissa);  uint128 newTokens = FixedPoint.multiplyUintByMantissa(userMeasureBalance, deltaExchangeRateMantissa).toUint128();  userStates[user] = UserState({  lastExchangeRateMantissa: exchangeRateMantissa,  balance: uint256(userState.balance).add(newTokens).toUint128()  });  Recommendation  Return without storage update if lastExchangeRateMantissa == exchangeRateMantissa, or by another method if deltaExchangeRateMantissa == 0. This reduces the gas cost for active users (high number of transfers that might be in the same block)  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.3 Handle transfer tokens where from == to ", "body": "  Description  In TokenFaucet, when calling beforeTokenTransfer it should also be optimized when to == from. This is to prevent any possible issues with internal accounting and token drip calculations.  /pool-contracts/contracts/token-faucet/TokenFaucet.sol  ...  if (token == address(measure) && from != address(0)) {  //add && from != to  drip();  ...  Recommendation  As ERC20 standard, from == to can be allowed but check in beforeTokenTransfer that if to == from, then do not call _captureNewTokensForUser(from); again.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.4 Redundant/Duplicate checks ", "body": "  Description  There are a few checks (require) in TokenFaucet that are redundant and/or checked twice.  Examples  _dripRatePerSecond > 0 checked twice, no need to check it in initialize pool-contracts/contracts/token-faucet/TokenFaucet.sol  require(_dripRatePerSecond > 0, \"TokenFaucet/dripRate-gt-zero\");  asset = _asset;  measure = _measure;  setDripRatePerSecond(_dripRatePerSecond);  function setDripRatePerSecond(uint256 _dripRatePerSecond) public onlyOwner {  require(_dripRatePerSecond > 0, \"TokenFaucet/dripRate-gt-zero\");  lastDripTimestamp == uint32(currentTimestamp) and newSeconds > 0 are basically the same check.  measureTotalSupply can never be < 0, as in the if statement enforces that /pool-contracts/contracts/token-faucet/TokenFaucet.sol#L111-L117  function drip() public returns (uint256) {  uint256 currentTimestamp = _currentTime();  // this should only run once per block.  if (lastDripTimestamp == uint32(currentTimestamp)) {  return 0;  ...  uint256 newSeconds = currentTimestamp.sub(lastDripTimestamp);  ...  if (measureTotalSupply > 0 && availableTotalSupply > 0 && newSeconds > 0) {  ...  uint256 indexDeltaMantissa = measureTotalSupply > 0 ? FixedPoint.calculateMantissa(newTokens, measureTotalSupply) : 0;  Recommendation  Remove the redundant checks to reduce the code size and complexity.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "5.5 Unnecessary use of upgradability   ", "body": "  Resolution                           These contracts are part of   OpenZeppelin Contracts Upgradeable.  Description  Libraries such as SafeMath and SafeCast should not be upgradable as they should be used as pure functions.  Upgradable libraries used in TokenFaucet contract:  SafeMathUpgradeable  SafeCastUpgradeable  IERC20Upgradeable  Recommendation  Remove the upgradability functionality from any part of the system that is unnecessary, as they add complexity and centralization power to the admins.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/02/pooltogether/"}, {"title": "4.1 Missing Events on Important State Changes    ", "body": "  Resolution  The client implemented a fix in commit 1be41a88a40125baf58d8904770cd9eb9e0732bb and provided the following statement:  RocketDAONodeTrusted is not a contract that is getting upgrade so this won t be fixed  RocketDAOProtocol has been updated to include events for each bootstrap function  RocketNetworkVoting has been updated to emit an event  RocketDAOSecurityProposals has been updated to emit events for all proposals  Description  Throughout the code base, various important settings-related state changes are not surfaced by events.  In RocketDAONodeTrusted:  contracts/contract/dao/node/RocketDAONodeTrusted.sol:L149-L165  function bootstrapMember(string memory _id, string memory _url, address _nodeAddress) override external onlyGuardian onlyBootstrapMode onlyRegisteredNode(_nodeAddress) onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {  // Ok good to go, lets add them  RocketDAONodeTrustedProposalsInterface(getContractAddress(\"rocketDAONodeTrustedProposals\")).proposalInvite(_id, _url, _nodeAddress);  // Bootstrap mode - Uint Setting  function bootstrapSettingUint(string memory _settingContractName, string memory _settingPath, uint256 _value) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {  // Ok good to go, lets update the settings  RocketDAONodeTrustedProposalsInterface(getContractAddress(\"rocketDAONodeTrustedProposals\")).proposalSettingUint(_settingContractName, _settingPath, _value);  // Bootstrap mode - Bool Setting  function bootstrapSettingBool(string memory _settingContractName, string memory _settingPath, bool _value) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {  // Ok good to go, lets update the settings  RocketDAONodeTrustedProposalsInterface(getContractAddress(\"rocketDAONodeTrustedProposals\")).proposalSettingBool(_settingContractName, _settingPath, _value);  In RocketDAOProtocol:  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L42-L51  function bootstrapSettingMulti(string[] memory _settingContractNames, string[] memory _settingPaths, SettingType[] memory _types, bytes[] memory _values) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {  // Ok good to go, lets update the settings  RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalSettingMulti(_settingContractNames, _settingPaths, _types, _values);  /// @notice Bootstrap mode - Uint Setting  function bootstrapSettingUint(string memory _settingContractName, string memory _settingPath, uint256 _value) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {  // Ok good to go, lets update the settings  RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalSettingUint(_settingContractName, _settingPath, _value);  Treasury address setter:  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L77-L79  function bootstrapTreasuryNewContract(string memory _contractName, address _recipientAddress, uint256 _amountPerPeriod, uint256 _periodLength, uint256 _startTime, uint256 _numPeriods) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {  RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalTreasuryNewContract(_contractName, _recipientAddress, _amountPerPeriod, _periodLength, _startTime, _numPeriods);  Bootstrap mode management:  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L97-L100  function bootstrapDisable(bool _confirmDisableBootstrapMode) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {  require(_confirmDisableBootstrapMode == true, \"You must confirm disabling bootstrap mode, it can only be done once!\");  setBool(keccak256(abi.encodePacked(daoNameSpace, \"bootstrapmode.disabled\")), true);  One-time treasury spends:  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L72-L74  function bootstrapSpendTreasury(string memory _invoiceID, address _recipientAddress, uint256 _amount) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {  RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalTreasuryOneTimeSpend(_invoiceID, _recipientAddress, _amount);  In RocketNetworkVoting.sol:  contracts/contract/network/RocketNetworkVoting.sol:L122  function setDelegate(address _newDelegate) external override onlyRegisteredNode(msg.sender) {  In RocketDAOSecurityProposals.sol:  contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L98-L99  function proposalSettingUint(string memory _settingNameSpace, string memory _settingPath, uint256 _value) override public onlyExecutingContracts() onlyValidSetting(_settingNameSpace, _settingPath) {  bytes32 namespace = keccak256(abi.encodePacked(protocolDaoSettingNamespace, _settingNameSpace));  contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L107-L108  function proposalSettingBool(string memory _settingNameSpace, string memory _settingPath, bool _value) override public onlyExecutingContracts() onlyValidSetting(_settingNameSpace, _settingPath) {  bytes32 namespace = keccak256(abi.encodePacked(protocolDaoSettingNamespace, _settingNameSpace));  contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L116-L117  function proposalSettingAddress(string memory _settingNameSpace, string memory _settingPath, address _value) override public onlyExecutingContracts() onlyValidSetting(_settingNameSpace, _settingPath) {  bytes32 namespace = keccak256(abi.encodePacked(protocolDaoSettingNamespace, _settingNameSpace));  contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L126-L127  function proposalInvite(string calldata _id, address _memberAddress) override public onlyLatestContract(\"rocketDAOProtocolProposals\", msg.sender) {  // Their proposal executed, record the block  Recommendation  We recommend emitting events on state changes, particularly when these are performed by an authorized party. The implementation of the recommendation should be analogous to the handling of events on state changes in the rest of the system, such as in the RocketMinipoolPenalty contract:  contracts/contract/minipool/RocketMinipoolPenalty.sol:L28-L33  function setMaxPenaltyRate(uint256 _rate) external override onlyGuardian {  // Update rate  maxPenaltyRate = _rate;  // Emit event  emit MaxPenaltyRateUpdated(_rate, block.timestamp);  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/rocket-pool-houston/"}, {"title": "4.2 RocketDAOProtocolProposal._propose() Should Revert if _blockNumber > block.number    ", "body": "  Resolution                           The client fixed this issue in commit   Description  Currently, the RocketDAOProtocolProposal._propose() function does not account for scenarios where _blockNumber is greater than block.number. This is a critical oversight, as voting power cannot be determined for future block numbers.  contracts/contract/dao/protocol/RocketDAOProtocolProposal.sol:L351  function _propose(string memory _proposalMessage, uint256 _blockNumber, uint256 _totalVotingPower, bytes calldata _payload) internal returns (uint256) {  Recommendation  We recommend updating the function to revert on transactions where _blockNumber exceeds block.number. This will prevent the creation of proposals with undefined voting power and maintain the integrity of the voting process.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/rocket-pool-houston/"}, {"title": "4.3 Unused Parameter and Improper Parameter Sanitization in RocketNetworkVoting.calculateVotingPower()    ", "body": "  Resolution  The client fixed the issue in commit aff5be87c2bc6fd4966be743cf8370fb43fac917 and provided the following statement:  matchedETH was left over from previous design, removed.  Added assertion for block number  The upgrade script ensures there is at least 1 snapshot of the RPL price  Description  The matchedETH parameter in RocketNetworkVoting.calculateVotingPower() is unused.  contracts/contract/network/RocketNetworkVoting.sol:L110-L111  // Get contracts  RocketDAOProtocolSettingsNodeInterface rocketDAOProtocolSettingsNode = RocketDAOProtocolSettingsNodeInterface(getContractAddress(\"rocketDAOProtocolSettingsNode\"));  contracts/contract/network/RocketNetworkVoting.sol:L102-L105  key = keccak256(abi.encodePacked(\"rpl.staked.node.amount\", _nodeAddress));  uint256 rplStake = uint256(rocketNetworkSnapshots.lookupRecent(key, uint32(_block), 5));  return calculateVotingPower(rplStake, ethMatched, ethProvided, rplPrice);  contracts/contract/network/RocketNetworkVoting.sol:L114-L114  uint256 maximumStake = providedETH * maximumStakePercent / rplPrice;  Recommendation  We recommend removing the unused parameter to enhance code clarity. The presence of unused parameters can lead to potential confusion for future developers.  Additionally, we recommend ensuring that the snapshotted rplPrice value exists before it is used to compute the maximumStake value.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/rocket-pool-houston/"}, {"title": "4.4 Wrong/Misleading NatSpec Documentation    ", "body": "  Resolution  The client acknowledged this issue, fixed the highlighted discrepancies, and notified us that they will continue reviewing the rest of codebase for inaccuracies  Description  The NatSpec documentation in several parts of the code base contains inaccuracies or is misleading. This issue can lead to misunderstandings about how the code functions, especially for developers who rely on these comments for clarity and guidance.  Examples  In RocketDAOProtocolProposal, the NatSpec comments are potentially misleading:  contracts/contract/dao/protocol/RocketDAOProtocolProposal.sol:L269-L270  /// @notice Get the votes against count of this proposal  /// @param _proposalID The ID of the proposal to query  contracts/contract/dao/protocol/RocketDAOProtocolProposal.sol:L282-L287  /// @notice Returns true if this proposal was supported by this node  /// @param _proposalID The ID of the proposal to query  /// @param _nodeAddress The node operator address to query  function getReceiptDirection(uint256 _proposalID, address _nodeAddress) override public view returns (VoteDirection) {  return VoteDirection(getUint(keccak256(abi.encodePacked(daoProposalNameSpace, \"receipt.direction\", _proposalID, _nodeAddress))));  In RocketDAOProtocolVerifier, the NatSpec documentation is incomplete, which might leave out critical information about the function s purpose and behavior:  contracts/contract/dao/protocol/RocketDAOProtocolVerifier.sol:L133-L135  /// @notice Used by a verifier to challenge a specific index of a proposal's voting power tree  /// @param _proposalID The ID of the proposal being challenged  /// @param _index The global index of the node being challenged  Recommendation  The NatSpec documentation should be thoroughly reviewed and corrected where necessary. We recommend ensuring it accurately reflects the code s functionality and provides complete information.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/rocket-pool-houston/"}, {"title": "4.5 RocketDAOProtocolSettingsRewards.setSettingRewardClaimPeriods() Cannot Be Invoked    ", "body": "  Resolution                           The client acknowledged this issue and let us know that this setting was meant to be adjustable via the   Description  contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L46  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"periods\")), _periods);  Recommendation  To make this function useful and align it with its intended purpose, we recommend integrating its functionality into RocketDAOProtocolProposals. In addition, we recommend that this function emit an event upon successful change of settings, enhancing the transparency of the operation.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/rocket-pool-houston/"}, {"title": "4.6 Redundant Comments   ", "body": "  Resolution  The client removed redundant comments from contracts that are being upgraded in this release.  Description  Throughout the code base, there are various redundant and duplicated comments. The following instances, and most likely a few more, can safely be removed to increase readability.  In RocketDaoProtocol:  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L43  // Ok good to go, lets update the settings  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L49  // Ok good to go, lets update the settings  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L55  // Ok good to go, lets update the settings  contracts/contract/dao/protocol/RocketDAOProtocol.sol:L61  // Ok good to go, lets update the settings  In RocketDAONodeTrusted*:  contracts/contract/dao/node/RocketDAONodeTrusted.sol:L42  // Construct  contracts/contract/dao/node/RocketDAONodeTrusted.sol:L157  // Ok good to go, lets update the settings  contracts/contract/dao/node/RocketDAONodeTrusted.sol:L163  // Ok good to go, lets update the settings  contracts/contract/dao/node/RocketDAONodeTrusted.sol:L170  // Ok good to go, lets update the settings  contracts/contract/dao/node/RocketDAONodeTrusted.sol:L186  // Ok good to go, lets update the settings  contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L150-L151  // Log it  emit ActionLeave(msg.sender, rplBondRefundAmount, block.timestamp);  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/rocket-pool-houston/"}, {"title": "4.7 Use calldata Storage Location Instead of memory for Large Read-Only Data   ", "body": "  Resolution                           The client applied the recommended optimisation in commit   Description  The current implementation stores data, such as the pollard node array in RocketDAOProtocolVerifier, in memory. Given this data structure s size and read-only nature, using memory can be less efficient regarding gas usage.  contracts/contract/dao/protocol/RocketDAOProtocolVerifier.sol:L65-L66  function submitProposalRoot(uint256 _proposalID, address _proposer, uint32 _blockNumber, Types.Node[] memory _treeNodes) external onlyLatestContract(\"rocketDAOProtocolProposal\", msg.sender) onlyLatestContract(\"rocketDAOProtocolVerifier\", address(this)) {  // Retrieve the node count at _blockNumber  contracts/contract/dao/protocol/RocketDAOProtocolVerifier.sol:L513-L514  function computeRootFromNodes(Types.Node[] memory _nodes) internal pure returns (Types.Node memory) {  uint256 len = _nodes.length / 2;  Recommendation  Consider modifying the storage location of such large read-only data arrays from memory to calldata. This will save gas as calldata is a cheaper storage location for data that does not need modification. This optimization is particularly relevant for contracts that frequently interact with large arrays or data structures.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/rocket-pool-houston/"}, {"title": "5.1 VotingMachine - tryToMoveToValidating can lock up proposals    ", "body": "  Resolution  Fixed per our recommendation.  Description  After a vote was received, the proposal can move to a validating state if any of the votes pass the proposal s precReq value, referred to as the minimum threshold.  code/contracts/governance/VotingMachine.sol:L391  tryToMoveToValidating(_proposalId);  Inside the method tryToMoveToValidating each of the vote options are checked to see if they pass precReq. In case that happens, the proposal goes into the next stage, specifically Validating.  code/contracts/governance/VotingMachine.sol:L394-L407  /// @notice Function to move to Validating the proposal in the case the last vote action  ///  was done before the required votingBlocksDuration passed  /// @param _proposalId The id of the proposal  function tryToMoveToValidating(uint256 _proposalId) public {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING_STATUS_REQUIRED\");  if (_proposal.currentStatusInitBlock.add(_proposal.votingBlocksDuration) <= block.number) {  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  The method internalMoveToValidating checks the proposal s status to be Voting and proceeds to moving the proposal into Validating state.  code/contracts/governance/VotingMachine.sol:L270-L278  /// @notice Internal function to change proposalStatus from Voting to Validating  /// @param _proposalId The id of the proposal  function internalMoveToValidating(uint256 _proposalId) internal {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  _proposal.currentStatusInitBlock = block.number;  emit StatusChangeToValidating(_proposalId);  The problem appears if multiple vote options go past the minimum threshold. This is because the loop does not stop after the first found option and the loop will fail when the method internalMoveToValidating is called a second time.  code/contracts/governance/VotingMachine.sol:L401-L405  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  The method internalMoveToValidating fails the second time because the first time it is called, the proposal goes into the Validating state and the second time it is called, the require check fails.  code/contracts/governance/VotingMachine.sol:L274-L275  require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  This can lead to proposal lock-ups if there are enough votes to at least one option that pass the minimum threshold.  Recommendation  After moving to the Validating state return successfully.  function tryToMoveToValidating(uint256 _proposalId) public {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING_STATUS_REQUIRED\");  if (_proposal.currentStatusInitBlock.add(_proposal.votingBlocksDuration) <= block.number) {  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  return; // <- this was added  An additional change can be done to internalMoveToValidating because it is called only in tryToMoveToValidating and the parent method already does the check.  /// @notice Internal function to change proposalStatus from Voting to Validating  /// @param _proposalId The id of the proposal  function internalMoveToValidating(uint256 _proposalId) internal {  Proposal storage _proposal = proposals[_proposalId];  // The line below can be removed  // require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  _proposal.currentStatusInitBlock = block.number;  emit StatusChangeToValidating(_proposalId);  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "5.2 VotingMachine - verifyNonce should only allow the next nonce    ", "body": "  Resolution  Fixed per our recommendation.  Description  When a relayer calls submitVoteByRelayer they also need to provide a nonce. This nonce is cryptographicly checked against the provided signature. It is also checked again to be higher than the previous nonce saved for that voter.  code/contracts/governance/VotingMachine.sol:L232-L239  /// @notice Verifies the nonce of a voter on a proposal  /// @param _proposalId The id of the proposal  /// @param _voter The address of the voter  /// @param _relayerNonce The nonce submitted by the relayer  function verifyNonce(uint256 _proposalId, address _voter, uint256 _relayerNonce) public view {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.voters[_voter].nonce < _relayerNonce, \"INVALID_NONCE\");  When the vote is saved, the previous nonce is incremented.  code/contracts/governance/VotingMachine.sol:L387  voter.nonce = voter.nonce.add(1);  This leaves the opportunity to use the same signature to vote multiple times, as long as the provided nonce is higher than the incremented nonce.  Recommendation  The check should be more restrictive and make sure the consecutive nonce was provided.  require(_proposal.voters[_voter].nonce + 1 == _relayerNonce, \"INVALID_NONCE\");  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "5.3 VoteMachine - Cancelling vote does not increase the nonce    ", "body": "  Resolution  Fixed per our recommendation.  Description  A vote can be cancelled by calling cancelVoteByRelayer with the proposal ID, nonce, voter s address, signature and a hash of the sent params.  The parameters are hashed and checked against the signature correctly.  The nonce is part of these parameters and it is checked to be valid.  code/contracts/governance/VotingMachine.sol:L238  require(_proposal.voters[_voter].nonce < _relayerNonce, \"INVALID_NONCE\");  Once the vote is cancelled, the data is cleared but the nonce is not increased.  code/contracts/governance/VotingMachine.sol:L418-L434  if (_cachedVoter.balance > 0) {  _proposal.votes[_cachedVoter.vote] = _proposal.votes[_cachedVoter.vote].sub(_cachedVoter.balance.mul(_cachedVoter.weight));  _proposal.totalVotes = _proposal.totalVotes.sub(1);  voter.weight = 0;  voter.balance = 0;  voter.vote = 0;  voter.asset = address(0);  emit VoteCancelled(  _proposalId,  _voter,  _cachedVoter.vote,  _cachedVoter.asset,  _cachedVoter.weight,  _cachedVoter.balance,  uint256(_proposal.proposalStatus)  );  This means that in the future, the same signature can be used as long as the nonce is still higher than the current one.  Recommendation  Considering the recommendation from issue https://github.com/ConsenSys/aave-governance-dao-audit-2020-01/issues/4 is implemented, the nonce should also increase when the vote is cancelled. Otherwise the same signature can be replayed again.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "5.4 Possible lock ups with SafeMath multiplication   ", "body": "  Resolution  The situation described is unlikely to occur, and does not justify mitigations which might introduce other risks.  Description  In some cases using SafeMath can lead to a situation where a contract is locked up due to an unavoidable overflow.  It is theoretically possible that both the internalSubmitVote() and internalCancelVote() functions could become unusable by voters with a high enough balance, if the asset weighting is set extremely high.  Examples  This line in internalSubmitVote() could overflow if the voter s balance and the asset weight were sufficiently high:  code/contracts/governance/VotingMachine.sol:L379  uint256 _votingPower = _voterAssetBalance.mul(_assetWeight);  A similar situation occurs in internalCancelVote():  code/contracts/governance/VotingMachine.sol:L419-L420  _proposal.votes[_cachedVoter.vote] = _proposal.votes[_cachedVoter.vote].sub(_cachedVoter.balance.mul(_cachedVoter.weight));  _proposal.totalVotes = _proposal.totalVotes.sub(1);  Recommendation  This could be protected against by setting a maximum value for asset weights. In practice it is very unlikely to occur in this situation, but it could be introduced at some point in the future.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "5.1 Similar token-to-token swap methods can yield very different results ", "body": "  Description  BPool s interface exposes several methods to perform token swaps. Because the formula used to calculate trade values varies depending on the method, we compared token swaps performed using two different methods:  BPool.swapExactAmountIn performs a direct token-to-token swap between two bound assets within the pool. Some amount tokenAmountIn of tokenIn is directly traded for some minimum amount minAmountOut of tokenOut. An additional parameter, maxPrice, allows the trader to specify the maximum amount of slippage allowed during the trade.  BPool.joinswapExternAmountIn allows a trader to exchange an amount tokenAmountIn of tokenIn for a minimum amount minPoolAmountOut of the pool s token. A subsequent call to BPool.exitswapPoolAmountIn allows a trader to exchange amount poolAmountIn of the pool s tokens for a minimum amount minAmountOut of tokenOut.  While the latter method performs a swap by way of the pool s token as an intermediary, both methods can be used in order to perform a token-to-token swap. Our comparison between the two tested the relative amount tokenAmountOut of tokenOut between the two methods with a variety of different parameters.  Examples  Each example made use of a testing contract, found here: https://gist.github.com/wadeAlexC/12ee22438e8028f5439c5f0faaf9b7f7  Additionally, BPool was modified; unneeded functions were removed so that deployment did not exceed the block gas limit.  1. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (50 BONE)  tokenAmountIn: 1 BONE  swapExactAmountIn tokenAmountOut: 980391195693945000  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 980391186207949598  Result: swapExactAmountIn gives 1.00000001x more tokens  2. tokenIn weight: 1 BONE  tokenOut weight: 49 BONE  tokenIn, tokenOut at equal balances (50 BONE)  tokenAmountIn: 1 BONE  swapExactAmountIn tokenAmountOut: 20202659955287800  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 20202659970818843  Result: joinswap/exitswap gives 1.00000001x more tokens  3. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (1 BONE)  tokenAmountIn: 0.5 BONE  swapExactAmountIn tokenAmountOut: 333333111111037037  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 333333055579388951  Result: swapExactAmountIn gives 1.000000167x more tokens  4. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9999993333331111110  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9999991667381668530  Result: swapExactAmountIn gives 1.000000167x more tokens  The final test raised the swap fee from MIN_FEE (0.0001%) to MAX_FEE (10%):    tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9310344827586206910  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9177966102628338740  Result: swapExactAmountIn gives 1.014423536x more tokens  Recommendation  Our final test showed that with equivalent balances and weights, raising the swap fee to 10% had a drastic effect on relative tokenAmountOut received, with swapExactAmountIn yielding >1.44% more tokens than the joinswap/exitswap method.  Reading through Balancer s provided documentation, our assumption was that these two swap methods were roughly equivalent. Discussion with Balancer clarified that the joinswap/exitswap method applied two swap fees: one for single asset deposit, and one for single asset withdrawal. With the minimum swap fee, this double application proved to have relatively little impact on the difference between the two methods. In fact, some parameters resulted in higher relative yield from the joinswap/exitswap method. With the maximum swap fee, the double application was distinctly noticeable.  Given the relative complexity of the math behind BPools, there is much that remains to be tested. There are alternative swap methods, as well as numerous additional permutations of parameters that could be used; these tests were relatively narrow in scope.  We recommend increasing the intensity of unit testing to cover a more broad range of interactions with BPool s various swap methods. In particular, the double application of the swap fee should be examined, as well as the differences between low and high swap fees.  Those using BPool should endeavor to understand as much of the underlying math as they can, ensuring awareness of the various options available for performing trades.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"}, {"title": "5.2 Commented code exists in BMath ", "body": "  Description  There are some instances of code being commented out in the BMath.sol that should be removed. It seems that most of the commented code is related to exit fee, however this is in contrast to BPool.sol code base that still has the exit fee code flow, but uses 0 as the fee.  Examples  code/contracts/BMath.sol:L137-L140  uint tokenInRatio = bdiv(newTokenBalanceIn, tokenBalanceIn);  // uint newPoolSupply = (ratioTi ^ weightTi) * poolSupply;  uint poolRatio = bpow(tokenInRatio, normalizedWeight);  code/contracts/BMath.sol:L206-L209  uint normalizedWeight = bdiv(tokenWeightOut, totalWeight);  // charge exit fee on the pool token side  // pAiAfterExitFee = pAi*(1-exitFee)  uint poolAmountInAfterExitFee = bmul(poolAmountIn, bsub(BONE, EXIT_FEE));  And many more examples.  Recommendation  Remove the commented code, or address them properly. If the code is related to exit fee, which is considered to be 0 in this version, this style should be persistent in other contracts as well.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"}, {"title": "5.3 Max weight requirement in rebind is inaccurate ", "body": "  Description  BPool.rebind enforces MIN_WEIGHT and MAX_WEIGHT bounds on the passed-in denorm value:  code/contracts/BPool.sol:L262-L274  function rebind(address token, uint balance, uint denorm)  public  _logs_  _lock_  require(msg.sender == _controller, \"ERR_NOT_CONTROLLER\");  require(_records[token].bound, \"ERR_NOT_BOUND\");  require(!_finalized, \"ERR_IS_FINALIZED\");  require(denorm >= MIN_WEIGHT, \"ERR_MIN_WEIGHT\");  require(denorm <= MAX_WEIGHT, \"ERR_MAX_WEIGHT\");  require(balance >= MIN_BALANCE, \"ERR_MIN_BALANCE\");  MIN_WEIGHT is 1 BONE, and MAX_WEIGHT is 50 BONE.  Though a token weight of 50 BONE may make sense in a single-token system, BPool is intended to be used with two to eight tokens. The sum of the weights of all tokens must not be greater than 50 BONE.  This implies that a weight of 50 BONE for any single token is incorrect, given that at least one other token must be present.  Recommendation  MAX_WEIGHT for any single token should be MAX_WEIGHT - MIN_WEIGHT, or 49 BONE.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"}, {"title": "5.4 Switch modifier order in BPool ", "body": "  Description  BPool functions often use modifiers in the following order: _logs_, _lock_. Because _lock_ is a reentrancy guard, it should take precedence over _logs_. See example:  Recommendation  Place _lock_ before other modifiers; ensuring it is the very first and very last thing to run when a function is called.  6 Document Change Log  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"}, {"title": "1.0", "body": "0000001x more tokens  2. tokenIn weight: 1 BONE  tokenOut weight: 49 BONE  tokenIn, tokenOut at equal balances (50 BONE)  tokenAmountIn: 1 BONE  swapExactAmountIn tokenAmountOut: 20202659955287800  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 20202659970818843  Result: joinswap/exitswap gives 1.00000001x more tokens  3. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (1 BONE)  tokenAmountIn: 0.5 BONE  swapExactAmountIn tokenAmountOut: 333333111111037037  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 333333055579388951  Result: swapExactAmountIn gives 1.000000167x more tokens  4. tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9999993333331111110  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9999991667381668530  Result: swapExactAmountIn gives 1.000000167x more tokens  The final test raised the swap fee from MIN_FEE (0.0001%) to MAX_FEE (10%):    tokenIn weight: 25 BONE  tokenOut weight: 25 BONE  tokenIn, tokenOut at equal balances (30 BONE)  tokenAmountIn: 15 BONE  swapExactAmountIn tokenAmountOut: 9310344827586206910  joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9177966102628338740  Result: swapExactAmountIn gives 1.014423536x more tokens  Recommendation  Our final test showed that with equivalent balances and weights, raising the swap fee to 10% had a drastic effect on relative tokenAmountOut received, with swapExactAmountIn yielding >1.44% more tokens than the joinswap/exitswap method.  Reading through Balancer s provided documentation, our assumption was that these two swap methods were roughly equivalent. Discussion with Balancer clarified that the joinswap/exitswap method applied two swap fees: one for single asset deposit, and one for single asset withdrawal. With the minimum swap fee, this double application proved to have relatively little impact on the difference between the two methods. In fact, some parameters resulted in higher relative yield from the joinswap/exitswap method. With the maximum swap fee, the double application was distinctly noticeable.  Given the relative complexity of the math behind BPools, there is much that remains to be tested. There are alternative swap methods, as well as numerous additional permutations of parameters that could be used; these tests were relatively narrow in scope.  We recommend increasing the intensity of unit testing to cover a more broad range of interactions with BPool s various swap methods. In particular, the double application of the swap fee should be examined, as well as the differences between low and high swap fees.  Those using BPool should endeavor to understand as much of the underlying math as they can, ensuring awareness of the various options available for performing trades.  5.2 Commented code exists in BMath Minor  Description  There are some instances of code being commented out in the BMath.sol that should be removed. It seems that most of the commented code is related to exit fee, however this is in contrast to BPool.sol code base that still has the exit fee code flow, but uses 0 as the fee.  Examples  code/contracts/BMath.sol:L137-L140  uint tokenInRatio = bdiv(newTokenBalanceIn, tokenBalanceIn);  // uint newPoolSupply = (ratioTi ^ weightTi) * poolSupply;  uint poolRatio = bpow(tokenInRatio, normalizedWeight);  code/contracts/BMath.sol:L206-L209  uint normalizedWeight = bdiv(tokenWeightOut, totalWeight);  // charge exit fee on the pool token side  // pAiAfterExitFee = pAi*(1-exitFee)  uint poolAmountInAfterExitFee = bmul(poolAmountIn, bsub(BONE, EXIT_FEE));  And many more examples.  Recommendation  Remove the commented code, or address them properly. If the code is related to exit fee, which is considered to be 0 in this version, this style should be persistent in other contracts as well.  5.3 Max weight requirement in rebind is inaccurate Minor  Description  BPool.rebind enforces MIN_WEIGHT and MAX_WEIGHT bounds on the passed-in denorm value:  code/contracts/BPool.sol:L262-L274  function rebind(address token, uint balance, uint denorm)  public  _logs_  _lock_  require(msg.sender == _controller, \"ERR_NOT_CONTROLLER\");  require(_records[token].bound, \"ERR_NOT_BOUND\");  require(!_finalized, \"ERR_IS_FINALIZED\");  require(denorm >= MIN_WEIGHT, \"ERR_MIN_WEIGHT\");  require(denorm <= MAX_WEIGHT, \"ERR_MAX_WEIGHT\");  require(balance >= MIN_BALANCE, \"ERR_MIN_BALANCE\");  MIN_WEIGHT is 1 BONE, and MAX_WEIGHT is 50 BONE.  Though a token weight of 50 BONE may make sense in a single-token system, BPool is intended to be used with two to eight tokens. The sum of the weights of all tokens must not be greater than 50 BONE.  This implies that a weight of 50 BONE for any single token is incorrect, given that at least one other token must be present.  Recommendation  MAX_WEIGHT for any single token should be MAX_WEIGHT - MIN_WEIGHT, or 49 BONE.  5.4 Switch modifier order in BPool Minor  Description  BPool functions often use modifiers in the following order: _logs_, _lock_. Because _lock_ is a reentrancy guard, it should take precedence over _logs_. See example:  Recommendation  Place _lock_ before other modifiers; ensuring it is the very first and very last thing to run when a function is called.  6 Document Change Log  1.0  2020-05-15  Initial report  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/05/balancer-finance/"}, {"title": "4.1 RPC eth_signMessage Allows Linked Dapps to Sign Messages With Any Wallet Account and W/O Explicit User Consent ", "body": "  Description  When a normal dapp requests MetaMask to sign an arbitrary message, the message is displayed to the user for confirmation before signing it. Requiring explicit user consent and displaying the message to be signed is crucial to ensure that the user has full control over what messages are signed on their behalf.  The shapeshift snap exposes an RPC endpoint for ethereum (and an undocumented one for AVAX) that allows bypassing user consent.  When invoking the shapeshift snap with the eth_signMessage RPC method, the snap signs the message right away, silently, without requiring consent from the wallet owner or notifying them of the fact that the dapp is signing with a HD wallet account on their behalf. This severely undermines security restrictions by the MetaMask that ensure that the end-user has full control over what is being signed, giving them the option to reject signing.  For comparison, eth_signTransaction  ask for user confirmation while eth_signMessage does not a.  The relevant code can be found here:  packages/snap/src/rpc/evm/common/EVMSigner.ts:L37-L47  async signMessage({ message }: SignMessageParamsType<T>): Promise<SignMessageResponseType<T>> {  try {  return (await this.signer.ethSignMessage(  message as SignerSignMessageType<T>,  )) as SignMessageResponseType<T>  } catch (error) {  this.logger.error(message, { fn: 'ethSignMessage' }, error)  return Promise.reject(error)  It also affects the avax implementation:  packages/snap/src/rpc/evm/avalanche/handlers.ts:L34-L45  export const avalancheSignMessage = async (  params: AvalancheSignMessageParams,  ): Promise<AvalancheSignMessageResponse> => {  try {  const avalancheSigner = new AvalancheSigner()  await avalancheSigner.initialize()  return await avalancheSigner.signMessage(params)  } catch (error) {  moduleLogger.error({ fn: 'avalancheSignMessage' }, error)  return Promise.reject(error)  Examples  A dapp can invoke eth_signMessage which will not require user confirmation and silently return a message that is signed with any of the users HD wallet accounts.  await window.ethereum.request({  method: 'wallet_invokeSnap',  params: {  snapId: \"local:http://localhost:9000\",  request: { method: 'eth_signMessage', params: {message: {addressNList: [0x80000000 + 44, 0x80000000 + 60, 0x80000000 + 0, 0, 1], message:\"hi\"}, snapId: \"local:http://localhost:9000\" }},  }});  {address: '0xBaB66CfA59757200c90c79BC6e2aEe4bFBe382Be', signature: '0x5c04bcc1ca73e9f9d4bf3642150407c01c189d784dd90349\u2026e03ca8ec026ec6062b3d708c5fedbca0f2427282620be8c1b'}  Recommendation  Follow exactly the same flow MetaMask already implemented when signing arbitrary messages. Log signing requests, surface them to the user, ask for confirmation, and reject them by default (timeout). Display the origin on signing request dialogues and present the data to be signed and the account in a human-readable, understandable way while showing the original data to be signed, too.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.2 ShapeShift Manages MetaMasks Ethereum Private Keys ", "body": "  Description  As the Snap Outline in this report mentions, the ShapeShift snap requests access to the BIP32 entropy for the Ethereum private keys. This effectively allows the ShapeShift snap to manage MetaMasks Ethereum keys directly, which comes with great responsibility. To avoid undermining established security controls put in place by the MetaMask team, the snap would have to replicate the same security functionality not to degrade the security posture of MetaMask altogether.  For reference, please take a look at issue 4.4, issue 4.1 , issue 4.9 .  Recommendation  We recommend using the Metamask provider exposed via the endowment:ethereum-provider RPC endpoint to perform Ethereum operations instead of managing the Ethereum keys and low-level operations directly. This avoids bypassing MetaMask security controls but falling back to proven and battle-tested user confirmation dialogs instead.  Moreover, we also asked the MM team to provide a more robust account management API that is not based on giving full low-level account access to Snaps. This would enable Snaps to perform signing operations with control over cryptographic parameters (e.g., BIP-44 derivation path) without accessing the root entropy. This will significantly decrease the risks for the end-user.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.3 Superfluous Permission endowment:network-access ", "body": "  Description  The snap requests permission endowment:network-access to interact with external entities over HTTP/fetch. While the sandbox/demo dapp may use the fetch API in its context as a web app, the requested permission is only relevant for the snap and the snap never calls the fetch() API. Hence, the permission is requested but never used.  Requesting more permissions than necessary should always be avoided following the principle of least privilege.  Examples  packages/snap/snap.manifest.json:L69  \"endowment:network-access\": {},  Recommendation  Remove superfluous permissions.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.4 RPC eth_getAddress Undermines MetaMask Security Features by Exposing All Accounts W/O Explicit User Consent ", "body": "  Description  Metamask by default protects wallet addresses from being exposed to connected websites. A user wishing to expose a wallet address to a dapp must explicitly connect that address with the dapp website. Here is an example of MetaMask Flask with a random test wallet managing 2 accounts. Account 1 is connected to metamask, test2 is not.  The dapp can query for connected addresses via the MetaMask injected provider RPC method eth_requestAccounts. Other non-connected addresses will not be returned:  await window.ethereum.request({ method: 'eth_requestAccounts' })  ['0x3d0c4e58b3ff2516455f79c1147eb95f125d56ae']  In contrast, the shapeshift snap requests low-level access for the ethereum root key. The snap exposes a similar RPC endpoint named eth_getAddress that returns all ethereum addresses. Any connected dapp can query the snap to retrieve ethereum addresses. The dapp - not necessarily trusted - can even silently interact with the snap to enumerate all ethereum addresses, whether they re  connected  to the dapp or not. This effectively bypasses MetaMask security measures where the user defines the addresses to expose.  Via the ShapeShift snap eth_getAddress RPC endpoint, the dapp can effectively enumerate all addresses even though they re not connected via the main wallet. This circumvents MetaMask security measures undermining established security principles of the wallet.  Note that all *_getAddress RPC endpoints exhibit this problem.  Examples  MetaMask APO only exposes connected addresses  await window.ethereum.request({ method: 'eth_requestAccounts' })  ['0x3d0c4e58b3ff2516455f79c1147eb95f125d56ae']  the same address can be retrieved via the ShapeShift snap RPC API  await window.ethereum.request({  method: 'wallet_invokeSnap',  params: {  snapId: \"local:http://localhost:9000\",  request: { method: 'eth_getAddress', params: {addressParams:{addressNList: [0x80000000 + 44, 0x80000000 + 60, 0x80000000 + 0, 0, 0]}, snapId: \"local:http://localhost:9000\" }},  }});  '0x3D0C4e58b3fF2516455f79c1147eB95F125d56aE'  MetaMask does not expose other addresses. However, the snap RPC API allows to enumerate non-connected addresses too, undermining the security of the main wallet.  await window.ethereum.request({  method: 'wallet_invokeSnap',  params: {  snapId: \"local:http://localhost:9000\",  request: { method: 'eth_getAddress', params: {addressParams:{addressNList: [0x80000000 + 44, 0x80000000 + 60, 0x80000000 + 0, 0, 1]}, snapId: \"local:http://localhost:9000\" }},  }});  '0xBaB66CfA59757200c90c79BC6e2aEe4bFBe382Be'  Recommendation  Ensure that the implemented security measures match those of the main wallet. Allow users to choose which addresses they want to expose to the dapp. Do not give low-level access to all wallet addresses without user consent.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.5 Signing Request Fails to Display Origin and User Account on Confirmation Message ", "body": "  Description  The signing request message does not display the user account used to sign the message. A malicious dapp may pretend to sign a message with one account while issuing an RPC call for a different account.  ShapeShift snap signing requests should implement similar security measures to how MetaMask signing requests work. Being fully transparent on  who signs what , and displaying the origin of the request. This is especially important on multi-dapp snaps to avoid users being tricked into signing transactions they did not intend to sign (wrong signer; dapp race condition).  Please note that we have also reported to the MM Snaps team that dialogs do not, by default, hint at the origin of the action. We hope this will be addressed commonly for all snaps in the future.  Recommendation  Display the signing account in a human-readable and expected format on the signing request. Also, display the origin of the RPC call.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.6 Control Character and Markdown Injection in snap_dialog ", "body": "  Description  On certain occasions, the snap may need to present a dialog to the user to request confirmation for an action or data verification. This step is crucial as dapps are not always trusted, and it s essential to prevent scenarios where they can silently sign data or perform critical operations using the user s keys without explicit permission. To create custom user-facing dialogs, MetaMask provides the Snaps UI package, equipped with style-specific components. However, some of these components have been found to have unintended side-effects.  For instance, the text() component can render Markdown or allow for control character injections. Specifically for the ShapeShift snap, this poses a concern because when the snap asks the user to sign structured data, that data might be mistakenly interpreted as Markdown. As a result, the user could inadvertently sign something they did not intend to sign. This means that if the message-to-be-signed contains Markdown renderable text, the displayed message for user approval will be inaccurate.  In the code snippet provided below, please note that the variable params is considered potentially untrusted. It may contain Markdown renderable strings or Control Characters that can disrupt the context of the user-displayed message.  Examples  UserConfirm does not sanitize params  await window.ethereum.request({  method: 'wallet_invokeSnap',  params: {  snapId: \"local:http://localhost:9000\",  request: { method: 'binance_signTransaction', params: {transaction:{\"hi **bold**\\n\\nnextline **test**\":1}}},  }});  packages/snap/src/rpc/common/utils.ts:L89-L110  export const userConfirm = async (params: userConfirmParam): Promise<boolean> => {  try {  /* eslint-disable-next-line no-undef */  const ret = await snap.request({  method: 'snap_dialog',  params: {  type: 'confirmation',  content: panel([  heading(`${params.prompt}: ${params.description}`),  text(params.textAreaContent),  ]),  },  })  if (!ret) {  return false  } catch (error) {  moduleLogger.error(error, { fn: 'userConfirm' }, 'Could not display confirmation dialog')  return false  return true  packages/snap/src/rpc/common/BaseSigner.ts:L54-L60  protected async confirmTransaction(transaction: any): Promise<boolean> {  return await userConfirm({  prompt: `Sign ${this.coin} Transaction?`,  description: 'Please verify the transaction data below',  textAreaContent: JSON.stringify(transaction, null, 2),  })  SnapDialog (not referenced anywhere but potentially vulnerable)  packages/adapter/src/metamask/metamask.ts:L114-L140  /**  TODO: This is a snap-native call - a handler must be added to the snap onRpcRequest() method to support this.  /  export const snapDialog = async ({  prompt,  description,  textAreaContent,  }: {  prompt: string  description: string  textAreaContent: string  }): Promise<boolean> => {  const provider = await getMetaMaskProvider()  if (provider === undefined) {  throw new Error('Could not get MetaMask provider')  if (provider.request === undefined) {  throw new Error('MetaMask provider does not define a .request() method')  try {  const ret = await provider.request({  method: 'snap_dialog',  params: {  type: 'confirmation',  content: panel([heading(`${prompt}: ${description}`), text(textAreaContent)]),  },  })  Please note that we have also reported the need for plaintext UI elements to the MM Snaps team. We hope this will be addressed commonly for all snaps in the future.  Recommendation  Validate inputs. Encode data in a safe way to be displayed to the user. Show the original data provided within a pre-text or code block. Show derived or decoded information (token recipient) as additional information to the user.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.7 Lack of High-Level and Inline Documentation ", "body": "  Description  The codebase currently lacks inline documentation, and the repository is missing high-level documentation explaining the Snap capabilities and features. This absence of documentation poses several concerns for future maintenance and transparency. Without inline documentation,  as the codebase grows, understanding the code s logic and functionality can be more challenging for developers, making maintenance and bug fixes more time-consuming and error-prone. Additionally, the absence of high-level documentation makes grasping the Snap s intended functionality and capabilities hard for end-users.  Recommendation  We recommend adding inline documentation throughout the codebase to facilitate comprehension of the code s behavior and contribute to its maintainability. We also recommend adding comprehensive high-level documentation in the repository, detailing the Snap s capabilities, features, and intended usage. This will offer insights to developers and end-users, promoting transparency for all parties.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.8 Notify on Chain Switches and Allow Users to Restrict Access to Chain Specific Functionality and Data ", "body": "  Description  MetaMask core is set to Ethereum as the default network. When switching to BNB or other Networks, Metamask asks the user to confirm the switch. This ensures that, at any point, the user is fully aware of the network they are currently operating on.  ShapeShift Snap exports multi-chain functionality, making it available to connected dapps via the MetaMask RPC. Connected dapps can request operations on various chains without requiring the users to confirm a chain switch. This deviates from the MetaMask security principles of always keeping the user informed about chain switches. Furthermore, the user does not have fine-grained control over what chain functionality is exposed to the dapp.  For example, since there is no origin check in the RPC handler onRpcRequest(), any connected dapp may access ShapeShift snap functionality. Some dapps may only require access to Avalanche or Thorchain-related functionality, while others may request access to functionality for several chains. Following the principle of least privilege, the user should be able to choose the chains dapps can access instead of granting access to every chain as soon as the dapp is connected to the snap. Indeed, this behavior poses a substantial phishing risk.  Recommendation  We recommend keeping an internal state of the last chain used. When a dapp requests to access functionality for a different chain, ask the user to confirm the chain switch. Give users control over what chains they want to expose to the dapp and keep a record of their choice. For example, the first time a dapp access Avalanche-specific features, the user should be able to accept or reject the dapp from accessing the network. Incorporate MetaMask s security measures without compromising or weakening them in any way.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.9 RPC *_signTransaction Endpoints Should Display Human Readable Transaction Data ", "body": "  Description  The transaction signing process lacks essential information to make sense of the transaction data object. The addressNList is assumed to be a BIP-32 path without proper explanation, and the contained information is presented in a non-human-readable format. As a result, the user cannot easily identify critical information, such as the signer s address. This leads to a non-user-friendly experience, which also poses security concerns.  Recommendation  Provide some means for the user to understand what they are signing. Display the signing request origin (multi-dapp usage). Additionally, show the raw data they re actually signing. Decode the BIP-32 key path to a user-readable address.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.10 Asynchronicity Might Lead to an Undefined Ethereum Provider ", "body": "  Description  The current logic to get the Ethereum provider implemented in getMetaMaskProvider does not have any timeout to wait for provider initialization. If this logic is used in the Snap initialization code, because of the asynchronous nature of the initialization code, the function might return an  undefined  provider because the provider is not yet initialized. This would throw an error and prevent the detection of the installed Metamask version. It would be better to provide a safe timeout and wait before deciding whether the provider is undefined. Alternatively, one could rely on the @metamask/detect-provider package (which is already part of the project dependencies) to get the provider.  packages/snap/src/rpc/common/utils.ts:L112-L119  const getMetaMaskProvider = async (): Promise<ExternalProvider> => {  try {  // eslint-disable-next-line no-undef  const provider = (window as any).ethereum  assert(provider, 'Could not detect Ethereum provider')  return provider  } catch (error) {  moduleLogger.error(  Recommendation  Rely on the @metamask/detect-provider package to get the Ethereum provider or implement a timeout before deciding whether the provider is undefined.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.11 Avalanche RPC Endpoints avax_* Are Enabled in the Snap but Disabled in the Sandbox App ", "body": "  Description  The AVAX functionality is enabled in the snap but disabled in the sandbox dapp. This is problematic as the functionality is present in the snap but not documented or surfaced anywhere.  packages/sandbox/src/components/AssetCardList/AssetCardListConfig.ts:L65-L69  name: 'Avalanche',  icon: 'avax.png',  symbol: 'AVAX',  enabled: false,  actions: {  packages/snap/src/index.ts:L88-L100  export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {  const { method, params } = request  switch (method) {  case 'avax_getAddress':  return await avalancheGetAddress(params)  case 'avax_signMessage':  return await avalancheSignMessage(params)  case 'avax_signTransaction':  return await avalancheSignTransaction(params)  case 'avax_verifyMessage':  return await avalancheVerifyMessage(params)  case 'avax_broadcastTransaction':  return await ethereumBroadcastTransaction(params)  Recommentation  Similarly to other networks, surface the AVAX functionality in the sandbox dapp.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.12 RPC [eth|avax]_signMessage Endpoints Return Errors ", "body": "  Description  The sandbox dapp returns an error when calling eth_signMessage:  Recommendation  Fix the default message to be signed. Include the from address/HD-path in the signing parameters.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.13 Unused Interface Declaration ", "body": "  Description  The interface type RPCRequest is declared but not referenced in the codebase.  Examples  packages/snap/src/index.ts:L83-L94  interface RPCRequest {  origin: string  request: ShapeShiftSnapRPCRequest  export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {  const { method, params } = request  switch (method) {  case 'avax_getAddress':  return await avalancheGetAddress(params)  case 'avax_signMessage':  return await avalancheSignMessage(params)  Recommendation  async ({ request } : RPCRequest ) => {  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.14 Every RPC Request Leads to the Creation of a New Signer Object", "body": "  Description  Every RPC request to the Snap follows the same pattern: it goes through a  handler  that creates a new signer for the proper chain, initializes it, and returns the result of the signer s operation. As an example, here is the handler for the cosmos_getAddress RPC request:  packages/snap/src/rpc/cosmossdk/cosmos/handlers.ts:L17-L28  export const cosmosGetAddress = async (  params: CosmosGetAddressParams,  ): Promise<CosmosGetAddressResponse> => {  try {  const cosmosSigner = new CosmosSigner()  await cosmosSigner.initialize()  return await cosmosSigner.getAddress(params)  } catch (error) {  moduleLogger.error({ fn: 'cosmosGetAddress' }, error)  return Promise.reject(error)  The signer is initialized by deriving the keys from the Metamask BIP32 entropy (in this.initializeSigner()) and creating a new unchained HTTP client for the appropriate RPC endpoint.  packages/snap/src/rpc/cosmossdk/cosmos/CosmosSigner.ts:L31-L46  async initialize(  { broadcastUrl }: SignerInitializeArgs = {  broadcastUrl: broadcastUrls.DEFAULT_UNCHAINED_COSMOS_HTTP_URL,  },  ) {  const httpProviderConfiguration = new unchained.cosmos.Configuration({  basePath: broadcastUrl,  })  try {  this.signer = await this.initializeSigner()  this.httpProvider = new unchained.cosmos.V1Api(httpProviderConfiguration)  this._initialized = true  } catch (error) {  this.logger.error(error, { fn: 'getSigner' }, `Failed to initialize ${this.coin}Signer`)  While this pattern works, it is sub-optimal performance-wise as the Snap re-creates new signer objects for every RPC request. Assuming RPC requests are generally sequential, e.g., get_address -> sign_tx -> broadcast_tx, this might lead to the creation/garbage collection of many objects with potentially expensive operations (key derivation, for instance). In that case, using the Singleton pattern would likely help as it would prevent creating a new signer to process every RPC request.  The presence of the initialization pattern with the  initialized  flag in the signer classes also indicates that the initial engineer s intentions were likely to use such a pattern. Yet, this flag is not used in the current codebase.  Recommendation  While the current code does not pose a problem from a security standpoint, we would recommend leveraging the Singleton pattern for the signer classes. This would prevent creating new signer objects for every RPC request but only a single instance of the signer class for the first request. It would positively impact the performance of the Snap when dealing with many RPC requests.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.15 Misleading Error Message (Copy-Paste)", "body": "  Description  snapDialog logs a misleading error message if confirmation times out. This seems to be a copy-past error from the walletSnap method.  packages/adapter/src/metamask/metamask.ts:L142-L147  } catch (error) {  /** User did not confirm the action or an error was encountered */  moduleLogger.error(error, { fn: 'walletSnap' }, `wallet_snap_* RPC call failed.`)  return Promise.reject(error)  packages/adapter/src/metamask/metamask.ts:L107-L111  return ret  } catch (error) {  moduleLogger.error(error, { fn: 'walletSnap' }, `wallet_snap_* RPC call failed.`)  return Promise.reject(error)  Recommendation  Log an accurate error message.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/07/metamask/partner-snaps-shapeshift-snap/"}, {"title": "4.1 Reentrancy vulnerability in MetaSwap.swap()    ", "body": "  Resolution                           This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  MetaSwap.swap() should have a reentrancy guard.  The adapters use this general process:  Collect the from token (or ether) from the user.  Execute the trade.  Transfer the contract s balance of tokens (from and to) and ether to the user.  If an attacker is able to reenter swap() before step 3, they can execute their own trade using the same tokens and get all the tokens for themselves.  This is partially mitigated by the check against amountTo in CommonAdapter, but note that the amountTo typically allows for slippage, so it may still leave room for an attacker to siphon off some amount while still returning the required minimum to the user.  code/contracts/adapters/CommonAdapter.sol:L57-L62  // Transfer remaining balance of tokenTo to sender  if (address(tokenTo) != Constants.ETH) {  uint256 balance = tokenTo.balanceOf(address(this));  require(balance >= amountTo, \"INSUFFICIENT_AMOUNT\");  _transfer(tokenTo, balance, recipient);  } else {  Examples  As an example of how this could be exploited, 0x supports an  EIP1271Wallet  signature type, which invokes an external contract to check whether a trade is allowed. A malicious maker might front run the swap to reduce their inventory. This way, the taker is sending more of the taker asset than necessary to MetaSwap. The excess can be stolen by the maker during the EIP1271 call.  Recommendation  Use a simple reentrancy guard, such as OpenZeppelin s ReentrancyGuard to prevent reentrancy in MetaSwap.swap(). It might seem more obvious to put this check in Spender.swap(), but the Spender contract intentionally does not use any storage to avoid interference between different adapters.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.2 A new malicious adapter can access users  tokens    ", "body": "  Resolution                           This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  The purpose of the MetaSwap contract is to save users gas costs when dealing with a number of different aggregators. They can just approve() their tokens to be spent by MetaSwap (or in a later architecture, the Spender contract). They can then perform trades with all supported aggregators without having to reapprove anything.  A downside to this design is that a malicious (or buggy) adapter has access to a large collection of valuable assets. Even a user who has diligently checked all existing adapter code before interacting with MetaSwap runs the risk of having their funds intercepted by a new malicious adapter that s added later.  Recommendation  There are a number of designs that could be used to mitigate this type of attack. After discussion and iteration with the client team, we settled on a pattern where the MetaSwap contract is the only contract that receives token approval. It then moves tokens to the Spender contract before that contract DELEGATECALLs to the appropriate adapter. In this model, newly added adapters shouldn t be able to access users  funds.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.3 Owner can front-run traders by updating adapters    ", "body": "  Resolution                           This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  MetaSwap owners can front-run users to swap an adapter implementation. This could be used by a malicious or compromised owner to steal from users.  Because adapters are DELEGATECALLed, they can modify storage. This means any adapter can overwrite the logic of another adapter, regardless of what policies are put in place at the contract level. Users must fully trust every adapter because just one malicious adapter could change the logic of all other adapters.  Recommendation  At a minimum, disallow modification of existing adapters. Instead, simply add new adapters and disable the old ones. (They should be deleted, but the aggregator IDs of deleted adapters should never be reused.)  This is, however, insufficient. A new malicious adapter could still overwrite the adapter mapping to modify existing adapters. To fully address this issue, the adapter registry should be in a separate contract. Through discussion and iteration with the client team, we settled on the following pattern:  MetaSwap contains the adapter registry. It calls into a new Spender contract.  The Spender contract has no storage at all and is just used to DELEGATECALL to the adapter contracts.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.4 Simplify fee calculation in WethAdapter    ", "body": "  Resolution  ConsenSys/metaswap-contracts@93bf5c6.  Description  WethAdapter does some arithmetic to keep track of how much ether is being provided as a fee versus as funds that should be transferred into WETH:  code/contracts/adapters/WethAdapter.sol:L41-L59  // Some aggregators require ETH fees  uint256 fee = msg.value;  if (address(tokenFrom) == Constants.ETH) {  // If tokenFrom is ETH, msg.value = fee + amountFrom (total fee could be 0)  require(amountFrom <= fee, \"MSG_VAL_INSUFFICIENT\");  fee -= amountFrom;  // Can't deal with ETH, convert to WETH  IWETH weth = getWETH();  weth.deposit{value: amountFrom}();  _approveSpender(weth, spender, amountFrom);  } else {  // Otherwise capture tokens from sender  // tokenFrom.safeTransferFrom(recipient, address(this), amountFrom);  _approveSpender(tokenFrom, spender, amountFrom);  // Perform the swap  aggregator.functionCallWithValue(abi.encodePacked(method, data), fee);  This code can be simplified by using address(this).balance instead.  Recommendation  Consider something like the following code instead:  if (address(tokenFrom) == Constants.ETH) {  getWETH().deposit{value: amountFrom}(); // will revert if the contract has an insufficient balance  _approveSpender(weth, spender, amountFrom);  } else {  tokenFrom.safeTransferFrom(recipient, address(this), amountFrom);  _approveSpender(tokenFrom, spender, amountFrom);  // Send the remaining balance as the fee.  aggregator.functionCallWithValue(abi.encodePacked(method, data), address(this).balance);  Aside from being a little simpler, this way of writing the code makes it obvious that the full balance is being properly consumed. Part is traded, and the rest is sent as a fee.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.5 Consider checking adapter existence in MetaSwap    ", "body": "  Resolution                           The MetaSwap team found that doing the check in   Description  MetaSwap doesn t check that an adapter exists before calling into Spender:  code/contracts/MetaSwap.sol:L87-L100  function swap(  string calldata aggregatorId,  IERC20 tokenFrom,  uint256 amount,  bytes calldata data  ) external payable whenNotPaused nonReentrant {  Adapter storage adapter = adapters[aggregatorId];  if (address(tokenFrom) != Constants.ETH) {  tokenFrom.safeTransferFrom(msg.sender, address(spender), amount);  spender.swap{value: msg.value}(  adapter.addr,  Then Spender performs the check and reverts if it receives address(0).  code/contracts/Spender.sol:L15-L16  function swap(address adapter, bytes calldata data) external payable {  require(adapter != address(0), \"ADAPTER_NOT_SUPPORTED\");  It can be difficult to decide where to put a check like this, especially when the operation spans multiple contracts. Arguments can be made for either choice (or even duplicating the check), but as a general rule it s a good idea to avoid passing invalid parameters internally. Checking for adapter existence in MetaSwap.swap() is a natural place to do input validation, and it means Spender can have a simpler model where it trusts its inputs (which always come from MetaSwap).  Recommendation  Drop the check from Spender.swap() and perform the check instead in MetaSwap.swap().  5 Second Assessment  We performed a second assessment between October 3rd and October 4th, 2020. The engagement was conducted primarily by Steve Marx. The total effort expended was 2 person-days.  This second assessment covered three new features added by the MetaSwap team:  Support for the CHI gas token   This allows users to offset their gas costs by burning gas tokens. These tokens can come from the user or from tokens that are owned by the MetaSwap contract itself.  Uniswap Adapter   This adapter allows swaps to be executed via the Uniswap v2 Router directly, rather than going through some other exchange first.  Fee collection   FeeCommonAdapter and FeeWethAdapter are fee-collecting versions of the original CommonAdapter and WethAdapter. They support an extra parameter fee, indicating the quantity of the from asset to be sent to a fee wallet.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "5.1 Scope for the Second Assessment", "body": "  The following files were in scope for the second assessment:  MetaSwap.sol  5d66ea56c131b3ad5246e9fc6c126a0b7ba497fa  adapters/FeeCommonAdapter.sol  1bb0e2b4f7fca8e0d98113cf152eeb6be4ff13c7  adapters/FeeWethAdapter.sol  f844d9e13bd2cbf52a81ae4637b35f214098f3b2  adapters/UniswapAdapter.sol  d0733f6f4567dc58d3caf4af8875e17824a97f2d  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "5.2 Security Specification", "body": "  The security specification hasn t change much from the original assessment, so please refer to that. There are two significant changes to the security model: fee collection and gas token ownership.  In the new code, fees are collected, but these fees can be seen as voluntary from the perspective of the smart contracts. Users are free to pass any value for the fee parameter, including 0 to avoid all fees. The assumption is that most users will not bother to change the fee suggested by the MetaSwap API.  The other significant change is the introduction of the CHI gas token. In particular, the ability to use gas tokens held by the MetaSwap contract opens a new potential attack surface. Indeed, we found that an attacker could use contract-held tokens for other purposes.  6 Second Assessment Issues  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "6.1 Attacker can abuse gas tokens stored in MetaSwap    ", "body": "  Resolution                           This function was removed in   ConsenSys/metaswap-contracts@75c4454.  Description  MetaSwap.swapUsingGasToken() allows users to make use of gas tokens held by the MetaSwap contract itself to reduce the gas cost of trades.  This mechanism is unsafe because any tokens held by the contract can be used by an attacker for other purposes.  Examples  An attack could also be made by using an existing token that makes external calls (e.g. an ERC777 token) or a mechanism in an aggregated exchange that makes external calls (e.g. wallet signatures in 0x).  Recommendation  The simplest way to avoid this vulnerability is to never transfer CHI gas tokens to MetaSwap at all. An alternative would be to only allow gas tokens to be used by approved transactions from the MetaSwap API. A possible mechanism for that would be to require a signature from the MetaSwap API. If such a signature were only provided in known-good situations (which are admittedly hard to define), it wouldn t be possible for an attacker to misuse the tokens.  7 Third Assessment  We performed a third assessment between November 7th and November 10th, 2020. The engagement was conducted primarily by Steve Marx. The total expended effort was 4 person-hours.  This third assessment covered the new FeeDistributor contract, which divides assets among a number of recipients. It s used in the MetaSwap system to distribute fees. Each recipient has a number of  shares , and assets are divided according to each recipients portion of share ownership. Potential assets include ether and ERC20-compatible tokens.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "7.1 Scope for the third assessment", "body": "  The only contract in scope was the FeeDistributor:  FeeDistributor.sol  23749a338461db92a96ae87a2fd454d1aa0cbb92  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "7.2 Security Specification", "body": "  At setup, the FeeDistributor is initialized with a number of recipients, each with a corresponding number of shares.  Recipients should be able to withdraw their fair share (<recipient's shares> / <total shares>) of any stored asset at any time.  No recipient should receive more than their fair share of an asset.  8 Third Assessment Recommendations  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "8.1 Document assumptions about ERC20 tokens", "body": "  Most ERC20-compatible tokens can be used with the FeeDistributor contract, but it s wise to document some assumptions made by the contract:  Token balances will not be too big (relative to the number of shares). Specifically, the total number of token units received by the contract must be able to be multiplied by the largest share amount held by a recipient.  Token balances will not be too small (relative to share amounts). It s impossible to divide a balance of 1 among more than 1 recipient. To be safe, it would be good to make sure that no one cares about losing less than totalShares token units. For example, if there are 1,000,000 total shares, an asset like ether would not be a problem because 1,000,000 wei is a trivial amount.  Token balances will not decrease without an explicit transfer. The contract makes the assumption that it can always compute the total received tokens by adding tokenBalance(token) and _totalWithdrawn[token]. This is not the case if the token balance can be manipulated externally.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "8.2 Only allow full withdrawal", "body": "  The current code has both   ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "8.3 Drop the recipient parameter", "body": "  Everywhere in the code, the   9 Third Assessment Issues  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "9.1 Simplify accounting and better handle remainders    ", "body": "  Resolution                           This was fixed in   ConsenSys/metaswap-contracts@f0a62e5. The accounting was reworked according to the recommendation here.  Description  The current code does some fairly complex and redundant calculations during withdrawal to keep track of various pieces of state. In particular, the pair of _available[recipient][token] and _totalOnLastUpdate[recipient][token] is difficult to describe and reason about.  Recommendation  For a given token and recipient, we recommend instead just tracking how much has already been withdrawn. The rest can be easily calculated:  function earned(IERC20 token, address recipient) public view returns (uint256) {  uint256 totalReceived = tokenBalance(token).add(_totalWithdrawn[token]);  return totalReceived.mul(shares[recipient]).div(totalShares);  function available(IERC20 token, address recipient) public view returns (uint256) {  return earned(token, recipient).sub(_withdrawn[token][recipient]);  function withdraw(IERC20[] calldata tokens) external {  for (uint256 i = 0; i < tokens.length; i++) {  IERC20 token = tokens[i];  uint256 amount = available(token, msg.sender);  _withdrawn[token][msg.sender] += amount;  _totalWithdrawn[token] += amount;  _transfer(token, msg.sender, amount);  emit Withdrawal(tokens, msg.sender);  This code is easier to reason about:  It s easy to see that withdrawn[token][msg.sender] is correct because it s only increased when there s a corresponding transfer.  It s easy to see that _totalWithdrawn[token] is correct for the same reason.  It s easy to see that earned() is correct under standard assumptions about ERC20 balances.  It s easy to see that available() is correct, as it s just the earned amount less the already-withdrawn amount.  Remainders are better handled. If 1 token unit is available and you own half the shares, nothing happens on withdrawal, and if there are later 2 token units available, you can withdraw 1. (Under the previous code, if you tried to withdraw when 1 token unit was available, you would be unable to withdraw when 2 were available.)  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "6.1 No Protection of Uninitialized Implementation Contracts From Attacker    ", "body": "  Description  In the contracts implement Openzeppelin s UUPS model, uninitialized implementation contract can be taken over by an attacker with initialize function, it s recommended to invoke the _disableInitializers function in the constructor to prevent the implementation contract from being used by the attacker. However all the contracts which implements OwnablePausableUpgradeable do not call _disableInitializers in the constructors  Examples  contracts/tokens/Rewards.sol:L25  contract Rewards is IRewards, OwnablePausableUpgradeable, ReentrancyGuardUpgradeable {  contracts/pool/Pool.sol:L20  contract Pool is IPool, OwnablePausableUpgradeable, ReentrancyGuardUpgradeable {  contracts/tokens/StakedLyxToken.sol:L46  contract StakedLyxToken is OwnablePausableUpgradeable, LSP4DigitalAssetMetadataInitAbstract, IStakedLyxToken, ReentrancyGuardUpgradeable {  etc.  Recommendation  Invoke _disableInitializers in the constructors of contracts which implement OwnablePausableUpgradeable including following:  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/09/leequid-staking/"}, {"title": "6.2 Unsafe Function receiveFees   ", "body": "  Description  In the Pool contract, function receiveFees is used for compensate a potential penalty/slashing in the protocol by sending LYX back to the pool without minting sLYX, but the side effect is that anyone can send LYX to the pool which could mess up pool balance after all validator exited, in fact it can be replaced by a another function receiveWithoutActivation with access control which does the same thing.  Examples  contracts/pool/Pool.sol:L153  function receiveFees() external payable override {}  contracts/pool/Pool.sol:L132-L134  function receiveWithoutActivation() external payable override {  require(msg.sender == address(stakedLyxToken) || hasRole(DEFAULT_ADMIN_ROLE, msg.sender), \"Pool: access denied\");  Recommendation  Remove function receiveFees  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/09/leequid-staking/"}, {"title": "6.3 Unnecessary Matching in Unstake Process    ", "body": "  Description  Function unstakeProcessed in StakedLyxToken contract, when unstakeAmount > totalPendingUnstake, all the unstake requests should be able to be processed, thus no need to go through the matching, as a result, extra gas in the matching can be saved.  Examples  contracts/tokens/StakedLyxToken.sol:L388-L411  if (unstakeAmount > totalPendingUnstake) {  pool.receiveWithoutActivation{value: unstakeAmount - totalPendingUnstake}();  unstakeAmount = totalPendingUnstake;  totalPendingUnstake -= unstakeAmount;  totalUnstaked += unstakeAmount;  uint256 amountToFill = unstakeAmount;  for (uint256 i = unstakeRequestCurrentIndex; i <= unstakeRequestCount; i++) {  UnstakeRequest storage request = _unstakeRequests[i];  if (amountToFill > (request.amount - request.amountFilled)) {  amountToFill -= (request.amount - request.amountFilled);  continue;  } else {  if (amountToFill == (request.amount - request.amountFilled) && i < unstakeRequestCount) {  unstakeRequestCurrentIndex = i + 1;  } else {  request.amountFilled += uint128(amountToFill);  unstakeRequestCurrentIndex = i;  break;  Recommendation  Put the matching part (line 393-411) into else branch of if unstakeAmount > totalPendingUnstake, change the if branch into following:  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/09/leequid-staking/"}, {"title": "6.4 Redundant Parameter in the Initialize Function", "body": "  Description  In the initialize function of the Pool contract, the parameter  _withdrawalCredentials is redundant, as all validator s withdrawal credentials are set to the address of the Reward contract and reward contract s address is already set by the initialization parameter _rewards .  Examples  contracts/pool/Pool.sol:L59-L76  function initialize(  address _admin,  address _stakedLyxToken,  address _rewards,  address _validators,  address _oracles,  bytes32 _withdrawalCredentials,  address _validatorRegistration,  uint256 _minActivatingDeposit,  uint256 _pendingValidatorsLimit  ) public initializer {  require(_stakedLyxToken != address(0), \"Pool: stakedLyxToken address cannot be zero\");  require(_rewards != address(0), \"Pool: rewards address cannot be zero\");  require(_admin != address(0), \"Pool: admin address cannot be zero\");  require(_oracles != address(0), \"Pool: oracles address cannot be zero\");  require(_validatorRegistration != address(0), \"Pool: validatorRegistration address cannot be zero\");  require(_validators != address(0), \"Pool: validators address cannot be zero\");  require(_withdrawalCredentials != bytes32(0), \"Pool: withdrawalCredentials cannot be zero\");  Recommendation  Remove the parameter  _withdrawalCredentials from intialize function  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/09/leequid-staking/"}, {"title": "3.1 Yearn: Re-entrancy attack during deposit ", "body": "  Description  During the deposit in the supplyTokenTo function, the token transfer is happening after the shares are minted and before tokens are deposited to the yearn vault:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128  function supplyTokenTo(uint256 _amount, address to) override external {  uint256 shares = _tokenToShares(_amount);  _mint(to, shares);  // NOTE: we have to deposit after calculating shares to mint  token.safeTransferFrom(msg.sender, address(this), _amount);  _depositInVault();  emit SuppliedTokenTo(msg.sender, shares, _amount, to);  If the token allows the re-entrancy (e.g., ERC-777), the attacker can do one more transaction during the token transfer and call the supplyTokenTo function again.  This second call will be done with already modified shares from the first deposit but non-modified token balances. That will lead to an increased amount of shares minted during the supplyTokenTo. By using that technique, it s possible to steal funds from other users of the contract.  Recommendation  Have the re-entrancy guard on all the external functions.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.2 Yearn: Partial deposits are not processed properly ", "body": "  Description  The deposit is usually made with all the token balance of the contract:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L171-L172  // this will deposit full balance (for cases like not enough room in Vault)  return v.deposit();  The Yearn vault contract has a limit of how many tokens can be deposited there. If the deposit hits the limit, only part of the tokens is deposited (not to exceed the limit). That case is not handled properly, the shares are minted as if all the tokens are accepted, and the  change  is not transferred back to the caller:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128  function supplyTokenTo(uint256 _amount, address to) override external {  uint256 shares = _tokenToShares(_amount);  _mint(to, shares);  // NOTE: we have to deposit after calculating shares to mint  token.safeTransferFrom(msg.sender, address(this), _amount);  _depositInVault();  emit SuppliedTokenTo(msg.sender, shares, _amount, to);  Recommendation  Handle the edge cases properly.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.3 Sushi: redeemToken redeems less than it should ", "body": "  Description  The redeemToken function takes as argument the amount of SUSHI to redeem. Because the SushiBar s leave function   which has to be called to achieve this goal   takes an amount of xSUSHI that is to be burned in exchange for SUSHI, redeemToken has to compute the amount of xSUSHI that will result in a return of as many SUSHI tokens as were requested.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L62-L87  /// @notice Redeems tokens from the yield source from the msg.sender, it burn yield bearing tokens and return token to the sender.  /// @param amount The amount of `token()` to withdraw.  Denominated in `token()` as above.  /// @return The actual amount of tokens that were redeemed.  function redeemToken(uint256 amount) public override returns (uint256) {  ISushiBar bar = ISushiBar(sushiBar);  ISushi sushi = ISushi(sushiAddr);  uint256 totalShares = bar.totalSupply();  uint256 barSushiBalance = sushi.balanceOf(address(bar));  uint256 requiredShares = amount.mul(totalShares).div(barSushiBalance);  uint256 barBeforeBalance = bar.balanceOf(address(this));  uint256 sushiBeforeBalance = sushi.balanceOf(address(this));  bar.leave(requiredShares);  uint256 barAfterBalance = bar.balanceOf(address(this));  uint256 sushiAfterBalance = sushi.balanceOf(address(this));  uint256 barBalanceDiff = barBeforeBalance.sub(barAfterBalance);  uint256 sushiBalanceDiff = sushiAfterBalance.sub(sushiBeforeBalance);  balances[msg.sender] = balances[msg.sender].sub(barBalanceDiff);  sushi.transfer(msg.sender, sushiBalanceDiff);  return (sushiBalanceDiff);  Recommendation  Calculate requiredShares based on the formula above (x2). We also recommend dealing in a clean way with the special cases  totalShares == 0 and barSushiBalance == 0.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.4 Sushi: balanceOfToken underestimates balance ", "body": "  Description  The balanceOfToken computation is too pessimistic, i.e., it can underestimate the current balance slightly.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L29-L45  /// @notice Returns the total balance (in asset tokens).  This includes the deposits and interest.  /// @return The underlying balance of asset tokens  function balanceOfToken(address addr) public override returns (uint256) {  if (balances[addr] == 0) return 0;  ISushiBar bar = ISushiBar(sushiBar);  uint256 shares = bar.balanceOf(address(this));  uint256 totalShares = bar.totalSupply();  uint256 sushiBalance =  shares.mul(ISushi(sushiAddr).balanceOf(address(sushiBar))).div(  totalShares  );  uint256 sourceShares = bar.balanceOf(address(this));  return (balances[addr].mul(sushiBalance).div(sourceShares));  Recommendation  The balanceOfToken function should use the formula above.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.5 Yearn: Redundant approve call ", "body": "  Description  The approval for token transfer is done in the following way:  code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L167-L170  if(token.allowance(address(this), address(v)) < token.balanceOf(address(this))) {  token.safeApprove(address(v), 0);  token.safeApprove(address(v), type(uint256).max);  Since the approval will be equal to the maximum value, there s no need to make zero-value approval first.  Recommendation  Change two safeApprove to one regular approve with the maximum value.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.6 Sushi: Some state variables should be immutable and have more specific types ", "body": "  Description  The state variables sushiBar and sushiAddr are initialized in the contract s constructor and never changed afterward.  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L12-L21  contract SushiYieldSource is IYieldSource {  using SafeMath for uint256;  address public sushiBar;  address public sushiAddr;  mapping(address => uint256) public balances;  constructor(address _sushiBar, address _sushiAddr) public {  sushiBar = _sushiBar;  sushiAddr = _sushiAddr;  Recommendation  Make these two state variables immutable and change their types as indicated above. Remove the corresponding explicit type conversions in the rest of the contract, and add explicit conversions to type address where necessary.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.7 Sushi: Unnecessary balance queries ", "body": "  Description  code/sushi-pooltogether/contracts/SushiYieldSource.sol:L73-L84  uint256 barBeforeBalance = bar.balanceOf(address(this));  uint256 sushiBeforeBalance = sushi.balanceOf(address(this));  bar.leave(requiredShares);  uint256 barAfterBalance = bar.balanceOf(address(this));  uint256 sushiAfterBalance = sushi.balanceOf(address(this));  uint256 barBalanceDiff = barBeforeBalance.sub(barAfterBalance);  uint256 sushiBalanceDiff = sushiAfterBalance.sub(sushiBeforeBalance);  balances[msg.sender] = balances[msg.sender].sub(barBalanceDiff);  Recommendation  Use requiredShares instead of barBalanceDiff, and remove the unnecessary queries and variables.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "3.8 Sushi: Unnecessary function declaration in interface ", "body": "  Description  The ISushiBar interface declares a transfer function.  code/sushi-pooltogether/contracts/ISushiBar.sol:L5-L17  interface ISushiBar {  function enter(uint256 _amount) external;  function leave(uint256 _share) external;  function totalSupply() external view returns (uint256);  function balanceOf(address account) external view returns (uint256);  function transfer(address recipient, uint256 amount)  external  returns (bool);  However, this function is never used, so it could be removed from the interface. Other functions that the SushiBar provides but are not used (approve, for example) aren t part of the interface either.  Recommendation  Remove the transfer declaration from the ISushiBar interface.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/pooltogether-sushi-and-yearn-v2-yield-sources/"}, {"title": "5.1 Warning about ERC20 handling function", "body": "  Description  There is something worth bringing up for discussion in the ERC20 disbursement function.  code/BitwaveMultiSend.sol:L55  assert(token.transferFrom(msg.sender, _to[i], _value[i]) == true);  In the above presented line, the external call is being compared to a truthful boolean. And, even though, this is clearly part of the ERC20 specification there have historically been cases where tokens with sizeable market caps and liquidity have erroneously not implemented return values in any of the transfer functions.  The question presents itself as to whether these non-ERC20-conforming tokens are meant to be supported or not.  The audit team believes that the purpose of this smart contract is to disburse OXT tokens and therefore, since its development was under the umbrella of the Orchid team, absolutely no security concerns should arise from this issue.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.2 Discussion on the permissioning of send functions", "body": "  Description  Since the disbursement of funds is all made atomically (i.e., the Ether funds held by the smart contract are transient) there is no need to permission the function with the restrictedToOwner modifier.  Even in the case of ERC20 tokens, there is no need to permission the function since the smart contract can only spend allowance attributed to it by the caller (msg.sender).  This being said there is value in permissioning this contract, specifically if attribution of the deposited funds in readily available tools like Etherscan is important. Because turning this into a publicly available tool for batch sends of Ether and ERC20 tokens would mean that someone could wrongly attribute some disbursement to Orchid Labs should they be ignorant to this fact.  A possible solution to this problem would be the usage of events to properly attribute the disbursements but it is, indeed, an additional burden to carefully analyse these for proper attribution.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.3 Improve function visibility ", "body": "  Description  The following methods are not called internally in the token contract and visibility can, therefore, be restricted to external rather than public. This is more gas efficient because less code is emitted and data does not need to be copied into memory. It also makes functions a bit simpler to reason about because there s no need to worry about the possibility of internal calls.  BitwaveMultiSend.sendEth()  BitwaveMultiSend.sendErc20()  Recommendation  Change visibility of these methods to external.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.4 Ether send function remainder handling ", "body": "  Description  The Ether send function depicted below implements logic to reimburse the sender if an extraneous amount is left in the contract after the disbursement.  code/BitwaveMultiSend.sol:L22-L43  function sendEth(address payable [] memory _to, uint256[] memory _value) public restrictedToOwner payable returns (bool _success) {  // input validation  require(_to.length == _value.length);  require(_to.length <= 255);  // count values for refunding sender  uint256 beforeValue = msg.value;  uint256 afterValue = 0;  // loop through to addresses and send value  for (uint8 i = 0; i < _to.length; i++) {  afterValue = afterValue.add(_value[i]);  assert(_to[i].send(_value[i]));  // send back remaining value to sender  uint256 remainingValue = beforeValue.sub(afterValue);  if (remainingValue > 0) {  assert(msg.sender.send(remainingValue));  return true;  It is also the only place where the SafeMath dependency is being used. More specifically to check there was no underflow in the arithmetic adding up the disbursed amounts.  However, since the individual sends would revert themselves should more Ether than what was available in the balance be specified these protection measures seem unnecessary.  Not only the above is true but the current codebase does not allow to take funds locked within the contract out in the off chance someone forced funds into this smart contract (e.g., by self-destructing some other smart contract containing funds into this one).  Recommendation  The easiest way to handle both retiring SafeMath and returning locked funds would be to phase out all the intra-function arithmetic and just transferring address(this).balance to msg.sender at the end of the disbursement. Since all the funds in there are meant to be from the caller of the function this serves the purpose of returning extraneous funds to him well and, adding to that, it allows for some front-running fun if someone  self-destructed  funds to this smart contract by mistake.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.5 Unneeded type cast of contract type ", "body": "  Description  The typecast being done on the address parameter in the lien below is unneeded.  code/BitwaveMultiSend.sol:L51  ERC20 token = ERC20(_tokenAddress);  Recommendation  Assign the right type at the function parameter definition like so:  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.6 Inadequate use of assert ", "body": "  Description  The usage of require vs assert has always been a matter of discussion because of the fine lines distinguishing these transaction-terminating expressions.  However, the usage of the assert syntax in this case is not the most appropriate.  Borrowing the explanation from the latest solidity docs (v. https://solidity.readthedocs.io/en/latest/control-structures.html#id4) :  Since assert-style exceptions (using the 0xfe opcode) consume all gas available to the call and require-style ones (using the 0xfd opcode) do not since the Metropolis release when the REVERT instruction was added, the usage of require in the lines depicted in the examples section would only result in gas savings and the same security assumptions.  In this case, even though the calls are being made to external contracts the supposedly abide to a predefined specification, this is by no means an invariant of the presented system since the component is external to the built system and its integrity cannot be formally verified.  Examples  code/BitwaveMultiSend.sol:L34  assert(_to[i].send(_value[i]));  code/BitwaveMultiSend.sol:L40  assert(msg.sender.send(remainingValue));  code/BitwaveMultiSend.sol:L55  assert(token.transferFrom(msg.sender, _to[i], _value[i]) == true);  Recommendation  Exchange the assert statements for require ones.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "6.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of a MythX Full Mode analysis was reviewed by the audit team and no relevant issues were raised as part of the process.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "6.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  code/BitwaveMultiSend.sol  3:7      error      \"./ERC20.sol\": Import statements must use double quotes only.                              quotes  22:16    error      There should be no whitespace between \"address payable\" and the opening square bracket.    array-declarations  24:8     warning    Provide an error message for require()                                                     error-reason  25:8     warning    Provide an error message for require()                                                     error-reason  34:19    warning    Consider using 'transfer' in place of 'send'.                                              security/no-send  40:19    warning    Consider using 'transfer' in place of 'send'.                                              security/no-send  47:8     warning    Provide an error message for require()                                                     error-reason  48:8     warning    Provide an error message for require()                                                     error-reason  \u2716 2 errors, 6 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "6.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Function Name  Visibility  Mutability  Modifiers  BitwaveMultiSend  Implementation  Public    NO   sendEth  Public    restrictedToOwner  sendErc20  Public    restrictedToOwner  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "4.1 TransactionManager - User might steal router s locked funds    ", "body": "  Resolution  This issue has been fixed.  Description  Recommendation  Consider using a data structure different than issuedShares for storing user deposits. This way, withdrawals by users will only be allowed when calling TransactionManager.cancel.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.2 TransactionManager - Receiver-side check also on sending side    Unverified Fix", "body": "  Resolution                           The Connext team claims to have fixed this in commit   4adbfd52703441ee5de655130fc2e0252eae4661. We have not reviewed this commit or, generally, the codebase at this point.  Description  The functions prepare, cancel, and fulfill in the TransactionManager all have a  common part  that is executed on both the sending and the receiving chain and side-specific parts that are only executed either on the sending or on the receiving side. The following lines occur in fulfill s common part, but this should only be checked on the receiving chain. In fact, on the sending chain, we might even compare amounts of different assets.  code2/packages/contracts/contracts/TransactionManager.sol:L476-L478  // Sanity check: fee <= amount. Allow `=` in case of only wanting to execute  // 0-value crosschain tx, so only providing the fee amount  require(relayerFee <= txData.amount, \"#F:023\");  This could prevent a legitimate fulfill on the sending chain, causing a loss of funds for the router.  Recommendation  Move these lines to the receiving-side part.  Remark  The callData supplied to fulfill is not used at all on the sending chain, but the check whether its hash matches txData.callDataHash happens in the common part.  code2/packages/contracts/contracts/TransactionManager.sol:L480-L481  // Check provided callData matches stored hash  require(keccak256(callData) == txData.callDataHash, \"#F:024\");  In principle, this check could also be moved to the receiving-chain part, allowing the router to save some gas by calling sending-side fulfill with empty callData and skip the check. Note, however, that the TransactionFulfilled event will then also emit the  wrong  callData on the sending chain, so the off-chain code has to be able to deal with that if you want to employ this optimization.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.3 TransactionManager - Flawed shares arithmetic    ", "body": "  Resolution  Comment from Connext:  We removed the shares logic completely, and are instead only focusing on standard tokens (i.e. rebasing, inflationary, and deflationary tokens are not supported directly). If users want to transfer non-standard tokens, they do so at their own risk.  Description  To support a wide variety of tokens, the TransactionManager uses a per-asset shares system to represent fractional ownership of the contract s balance in a token. There are several flaws in the shares-related arithmetic, such as:  addLiquidity and sender-side prepare convert asset amounts 1:1 to shares, instead of taking the current value of a share into account. A 1:1 conversion is only appropriate if the number of shares is 0, for example, for the first deposit.  The WadRayMath library is not used correctly (and maybe not the ideal tool for the task in the first place): rayMul and rayDiv each operate on two rays (decimal numbers with 27 digits) but are not used according to that specification in getAmountFromIssuedShares and getIssuedSharesFromAmount. The scaling errors cancel each other out, though.  The WadRayMath library rounds to the nearest representable number. That might not be desirable for NXTP; for example, converting a token amount to shares and back to tokens might lead to a higher amount than we started with.  The WadRayMath library reverts on overflows, which might not be acceptable behavior. For instance, a receiver-side fulfill might fail due to an overflow in the conversion from shares to a token amount. The corresponding fulfill on the sending chain might very well succeed, though, and it is possible that, at a later point, the receiver-side transaction can be canceled. (Note that canceling does not involve actually converting shares into a token amount, but the calculation is done anyway for the event.)  The amount emitted in the TransactionPrepared event on the receiving chain can, depending on the granularity of the shares, differ considerably from what a user can expect to receive when the shares are converted back into tokens. The reason for this is the double conversion from the initial token amount \u2014 which is emitted \u2014 to shares and, later, back to tokens.  Special cases might have to be taken into account. As a more subtle example, converting a non-zero token amount to shares is not possible (or at least not with the usual semantics) if the contract s balance is zero, but the number of already existing shares is strictly greater than zero, as any number of shares will give you back less than the original amount. Whether this situation is possible depends on the token contract.  Recommendation  The shares logic was added late to the contract and is still in a pretty rough shape. While providing a full-fledged solution is beyond the scope of this review, we hope that the points raised above provide pointers and guidelines to inform a major overhaul.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.4 Router - handleMetaTxRequest - gas griefing / race conditions / missing validations / free meta transactions ", "body": "  Description  There s a comment in handleMetaTxRequest that asks whether data needs to be validated before interacting with the contract and the answer is yes, always, or else this opens up a gas griefing vector on the router side.  For example, someone might flood broadcast masses of metaTx requests (*.*.metatx) and all online routers will race to call TransactionManager.fulfill(). Even if only one transaction should be able to successfully go through all the others will loose on gas (until up to the first require failing).  Given that there is no rate limiting and it is a broadcast that is very cheap to perform on the client-side (I can just spawn a lot of nodes spamming messages) this can be very severe, keeping the router busy sending transactions that are deemed to fail until the routers balance falls below the min gas limit configured.  Even if the router would check the contracts current state first (performing read-only calls that can be done offline) to check if the transaction has a chance to succeed, it will still compete in a race for the current block (mempool).  Examples  code/packages/router/src/handler.ts:L459-L477  const fulfillData: MetaTxFulfillPayload = data.data;  // Validate that metatx request matches with known data about fulfill  // Is this needed? Can we just submit to chain without validating?  // Technically this is ok, but perhaps we want to validate only for our own  // logging purposes.  // Would also be bad if router had no gas here  // Next, prepare the tx object  // - Get chainId from data  // - Get fulfill fee from data and validate it covers gas  // - etc.  // Send to txService  // Update metrics  // TODO: make sure fee is something we want to accept  this.logger.info({ method, methodId, requestContext, chainId, data }, \"Submitting tx\");  const res = await this.txManager  .fulfill(  chainId,  Recommendation  For state-changing transactions that actually cost gas there is no way around implementing strict validation whenever possible and avoid performing the transaction in case validation fails. Contract state should always be validated before issuing new online transactions but this might not fix the problem that routers still compete for their transaction to be included in the next block (mempool not monitored). The question therefore is, whether it would be better to change the metaTX flow to have a router confirm that they will send the tx via the messaging service first so others know they do not even have to try to send it. However, even this scenario may allow to DoS the system by maliciously responding with such a method.  In general, there re a lot of ways to craft a message that forces the router to issue an on-chain transaction that may fail with no consequences for the sender of the metaTx message.  Additionally, the relayerFee is currently unchecked which may lead to the router loosing funds because they effectively accept zero-fee relays.  As noted in issue 4.6, the missing return after detecting that the metatx is destined for a TransactionManager that is not supported allows for explicit gas griefing attacks (deploy a fake TransactionManager.fulfill that mines all the gas for a beneficiary).  The contract methods should additionally validate that the sender balance can cover for the gas required to send the transaction.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.5 Router - subgraphLoop may process transactions the router was not configured for (code fragility) ", "body": "  Description  subgraphLoop gets all sending transactions for the router, chain, status triplet.  code/packages/router/src/subgraph.ts:L155-L159  allSenderPrepared = await sdk.GetSenderTransactions({  routerId: this.routerAddress.toLowerCase(),  sendingChainId: chainId,  status: TransactionStatus.Prepared,  });  and then sorts the results by receiving chain id. Note that this keeps track of chainID s the router was not configured for.  code/packages/router/src/subgraph.ts:L168-L176  // create list of txIds for each receiving chain  const receivingChains: Record<string, string[]> = {};  allSenderPrepared.router?.transactions.forEach(({ transactionId, receivingChainId }) => {  if (receivingChains[receivingChainId]) {  receivingChains[receivingChainId].push(transactionId);  } else {  receivingChains[receivingChainId] = [transactionId];  });  In a next step, transactions are resolved from the various chains. This filters out chainID s the router was not configured for (and just returns an empty array), however, the GetTransactions query assumes that transactionID s are unique across the subgraph which might not be true!  code/packages/router/src/subgraph.ts:L179-L193  let correspondingReceiverTxs: any[];  try {  const queries = await Promise.all(  Object.entries(receivingChains).map(async ([cId, txIds]) => {  const _sdk = this.sdks[Number(cId)];  if (!_sdk) {  this.logger.error({ chainId: cId, method, methodId }, \"No config for chain, this should not happen\");  return [];  const query = await _sdk.GetTransactions({ transactionIds: txIds.map((t) => t.toLowerCase()) });  return query.transactions;  }),  );  correspondingReceiverTxs = queries.flat();  } catch (err) {  In the last step, all chainID s (even the one s the router was not configured for) are iterated again (which might be unnecessary). TransactionID s are loosely matched from the previously flattened results from all the various chains. Since transactionID s don t necessarily need to be unique across chains or within the chain, it is likely that the subsequent matching of transactionID s (correspondingReceiverTxs.find) returns more than 1 entry.  However, find() just returns the first item and covers up the fact that there might be multiple matches. Also, since the code returned an empty array for chains it was not configured for, the find will return undef satisfying the !corresponding branch and fire an SenderTransactionPrepared triggering the handler to perform an on-chain action that will most definitely fail at some point.  Recommendation  The code in this module is generally very fragile. It is based on assumptions that can likely be exploited by a third party re-using transactionID s (or other values). It is highly recommended to rework the code making it more resilient to potential corner cases.  Filter receivingChains for chainID s that are not supported by the router  Avoid having to integrating the allSenderPrepared array twice and use a filtered list instead  Change the very broad query in _sdk.GetTransactions() that assumes transactionID s are unique across all chains to a specific query that selects transactions specific to the chain and this router. The more specific the better!  When matching the transactions also match the source/receiver chains instead of only matching the transactionID. Additionally, check if more than one entry matches the condition instead of silently taking the first result (this is what array.find() does)  Also see issue 5.2  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.6 Router - handler reports an error condition but continues execution instead of aborting it ", "body": "  Description  There are some code paths that detect and log an error but then continue the execution flow instead of returning the error condition to the caller. This may allow for a variety of griefing vectors (e.g. gas griefing).  Examples  reports an error because the received address does not match our configured transaction manager, but then proceeds. This means the router would accept a transaction manager it was not configured for.  code/packages/router/src/handler.ts:L448-L458  if (utils.getAddress(data.to) !== utils.getAddress(chainConfig.transactionManagerAddress)) {  const err = new HandlerError(HandlerError.reasons.ConfigError, {  requestContext,  calling: \"chainConfig.transactionManagerAddress\",  methodId,  method,  configError: `Provided transactionManagerAddress does not map to our configured transactionManagerAddress`,  });  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");  If chainConfig is undef this should return or else it will bail later  code/packages/router/src/handler.ts:L436-L445  if (!chainConfig) {  const err = new HandlerError(HandlerError.reasons.ConfigError, {  requestContext,  calling: \"getConfig\",  methodId,  method,  configError: `No chainConfig for ${chainId}`,  });  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");  if data is not fulfill this silently returns, while it should probably raise an error instead (unexpected message)  code/packages/router/src/handler.ts:L447-L447  if (data.type === \"Fulfill\") {  Recommendation  Implement strict validation of untrusted data.  Be explicit and raise error conditions on unexpected messages (e.g. type is not fulfill) instead of silently skipping the message.  Add the missing returns after reporting an error instead of continuing the execution flow on errors.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.7 Router - spawns unauthenticated admin API endpoint listening on all interfaces ", "body": "  Description  unauthenticated  listening on allips  pot. allows any local or remote unpriv user with access to the endpoint to steal the routers liquidity /remove-liquidity -> req.body.recipientAddress  Examples  code/packages/router/src/index.ts:L123-L130  server.listen(8080, \"0.0.0.0\", (err, address) => {  if (err) {  console.error(err);  process.exit(1);  console.log(`Server listening at ${address}`);  });  Recommendation  require authentication  should only bind to localhost by default  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.8 TODO comments should be resolved ", "body": "  Description  As part of the process of bringing the application to production readiness, dev comments (especially TODOs) should be resolved. In many cases, these comments indicate a missing functionality that should be implemented, or some missing necessary validation checks.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.9 TransactionManager - Missing nonReentrant modifier on removeLiquidity    ", "body": "  Resolution  This issue has been fixed.  Description  The removeLiquidity function does not have a nonReentrant modifier.  code/packages/contracts/contracts/TransactionManager.sol:L274-L329  /**  @notice This is used by any router to decrease their available  liquidity for a given asset.  @param shares The amount of liquidity to remove for the router in shares  @param assetId The address (or `address(0)` if native asset) of the  asset you're removing liquidity for  @param recipient The address that will receive the liquidity being removed  /  function removeLiquidity(  uint256 shares,  address assetId,  address payable recipient  ) external override {  // Sanity check: recipient is sensible  require(recipient != address(0), \"#RL:007\");  // Sanity check: nonzero shares  require(shares > 0, \"#RL:035\");  // Get stored router shares  uint256 routerShares = issuedShares[msg.sender][assetId];  // Get stored outstanding shares  uint256 outstanding = outstandingShares[assetId];  // Sanity check: owns enough shares  require(routerShares >= shares, \"#RL:018\");  // Convert shares to amount  uint256 amount = getAmountFromIssuedShares(  shares,  outstanding,  Asset.getOwnBalance(assetId)  );  // Update router issued shares  // NOTE: unchecked due to require above  unchecked {  issuedShares[msg.sender][assetId] = routerShares - shares;  // Update the total shares for asset  outstandingShares[assetId] = outstanding - shares;  // Transfer from contract to specified recipient  Asset.transferAsset(assetId, recipient, amount);  // Emit event  emit LiquidityRemoved(  msg.sender,  assetId,  shares,  amount,  recipient  );  Assuming we re dealing with a token contract that allows execution of third-party-supplied code, that means it is possible to leave the TransactionManager contract in one of the functions that call into the token contract and then reenter via removeLiquidity. Alternatively, we can leave the contract in removeLiquidity and reenter through an arbitrary external function, even if it has a nonReentrant modifier.  Example  Recommendation  While tokens that behave as described in the example might be rare or not exist at all, caution is advised when integrating with unknown tokens or calling untrusted code in general. We strongly recommend adding a nonReentrant modifier to removeLiquidity.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.10 TransactionManager - Relayer may use user s cancel after expiry signature to steal user s funds by colluding with a router   ", "body": "  Resolution                           This has been acknowledged by the Connext team. As discussed below in the  Recommendation , it is not a flaw in the contracts   Description  Users that are willing to have a lower trust dependency on a relayer should have the ability to opt-in only for the service that allows the relayer to withdraw back users  funds from the sending chain after expiry. However, in practice, a user is forced to opt-in for the service that refunds the router before the expiry, since the same signature is used for both services (lines 795,817 use the same signature).  Let s consider the case of a user willing to call fulfill on his own, but to use the relayer only to withdraw back his funds from the sending chain after expiry. In this case, the relayer can collude with the router and use the user s cancel signature (meant for withdrawing his only after expiry) as a front-running transaction for a user call to fulfill. This way the router will be able to withdraw both his funds and the user s funds since the user s fulfill signature is now public data residing in the mem-pool.  Examples  code/packages/contracts/contracts/TransactionManager.sol:L795-L817  require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");  Asset.transferAsset(txData.sendingAssetId, payable(msg.sender), relayerFee);  // Get the amount to refund the user  uint256 toRefund;  unchecked {  toRefund = amount - relayerFee;  // Return locked funds to sending chain fallback  if (toRefund > 0) {  Asset.transferAsset(txData.sendingAssetId, payable(txData.sendingChainFallback), toRefund);  } else {  // Receiver side, router liquidity is returned  if (txData.expiry >= block.timestamp) {  // Timeout has not expired and tx may only be cancelled by user  // Validate signature  require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");  Recommendation  The crucial point here is that the user must never sign a  cancel  that could be used on the receiving chain while fulfillment on the sending chain is still a possibility. Or, to put it differently: A user may only sign a  cancel  that is valid on the receiving chain after sending-chain expiry or if they never have and won t ever sign a  fulfill  (or at least won t sign until sending-chain expiry \u2014 but it is pointless to sign a  fulfill  after that, so  never  is a reasonable simplification). Or, finally, a more symmetric perspective on this requirement: If a user has signed  fulfill , they must not sign a receiving-chain-valid  cancel  until sending-chain expiry, and if they have signed a receiving-chain-valid  cancel , they must not sign a  fulfill  (until sending-chain expiry).  In this sense,  cancel  signatures that are valid on the receiving chain are dangerous, while sending-side cancellations are not. So the principle stated in the previous paragraph might be easier to follow with different signatures for sending- and receiving-chain cancellations.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.11 Router - handleSenderPrepare - missing validation, unchecked bidExpiry, unchecked expiry, unchecked chainids/swaps, race conidtions ", "body": "  Description  This finding highlights a collection of issues with the handleSenderPrepare method. The code and coding style appears fragile. Validation should be strictly enforced and protective measures against potential race conditions should be implemented.  The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:  unchecked bidExpiry might allow using bids even after expiration.  code/packages/router/src/handler.ts:L612-L626  .andThen(() => {  // TODO: anything else? seems unnecessary to validate everything  if (!BigNumber.from(bid.amount).eq(amount) || bid.transactionId !== txData.transactionId) {  return err(  new HandlerError(HandlerError.reasons.PrepareValidationError, {  method,  methodId,  calling: \"\",  requestContext,  prepareError: \"Bid params not equal to tx data\",  }),  );  return ok(undefined);  });  unchecked txdata.expiry might lead to router preparing for an already expired prepare. However, this is rather unlikely easily exploitable as the data source is a subgraph.  a bid might not be fulfillable anymore due to changes to the router (e.g. removing a chainconfig or assets) but the router would still attempt it. Make sure to always verify chainid/assets/the configured system parameters.  potential race condition. make sure to lock the txID in the beginning.  code/packages/router/src/handler.ts:L663-L669  // encode the data for contract call  // Send to txService  this.receiverPreparing.set(txData.transactionId, true);  this.logger.info(  { method, methodId, requestContext, transactionId: txData.transactionId },  \"Sending receiver prepare tx\",  );  Note that transactionID s as they are used in the system must be unique across chains.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.12 Router - handleNewAuction - fragile code ", "body": "  Description  This finding highlights a collection of issues with the handleNewAuction. The code and coding style appears fragile. Validation should be strictly enforced, debugging code should be removed or disabled in production and protective measures should be taken from abusive clients.  The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:  router bids on zero-amount requests (this will fail later when calling the contract, thus a potential gas griefing attack vector)  code/packages/router/src/handler.ts:L197-L201  // validate that assets/chains are supported and there is enough liquidity  // and gas on both sender and receiver side.  // TODO: will need to track this offchain  const amountReceived = mutateAmount(amount);  duplicate constant var assignment (subfunction const shadowing and unchecked initial config!)  code/packages/router/src/handler.ts:L202-L204  const config = getConfig();  const sendingConfig = config.chainConfig[sendingChainId];  const receivingConfig = config.chainConfig[receivingChainId];  code/packages/router/src/handler.ts:L231-L240  // validate config  const config = getConfig();  const sendingConfig = config.chainConfig[sendingChainId];  const receivingConfig = config.chainConfig[receivingChainId];  if (  !sendingConfig.providers ||  sendingConfig.providers.length === 0 ||  !receivingConfig.providers ||  receivingConfig.providers.length === 0  ) {  actual estimated gas required to fuel transaction is never checked. current balance might be outdated, especially in race condition scenarios.  code/packages/router/src/handler.ts:L315-L318  .andThen((balances) => {  const [senderBalance, receiverBalance] = balances as BigNumber[];  if (senderBalance.lt(sendingConfig.minGas) || receiverBalance.lt(receivingConfig.minGas)) {  return errAsync(  remove debug code from production build (dry-run)  code/packages/router/src/handler.ts:L194-L194  dryRun,  code/packages/router/src/handler.ts:L385-L385  this.messagingService.publishAuctionResponse(inbox, { bid, bidSignature: dryRun ? undefined : bidSignature }),  signer address might be different for different chains  code/packages/router/src/handler.ts:L290-L312  return combine([  ResultAsync.fromPromise(  this.txService.getBalance(sendingChainId, this.signer.address),  (err) =>  new HandlerError(HandlerError.reasons.TxServiceError, {  calling: \"txService.getBalance => sending\",  method,  methodId,  requestContext,  txServiceError: jsonifyError(err as NxtpError),  }),  ),  ResultAsync.fromPromise(  this.txService.getBalance(receivingChainId, this.signer.address),  (err) =>  new HandlerError(HandlerError.reasons.TxServiceError, {  calling: \"txService.getBalance => receiving\",  method,  methodId,  requestContext,  txServiceError: jsonifyError(err as NxtpError),  }),  ),  no rate limiting. potential DoS vector when someone floods the node with auction requests (significant work to be done, handler is async, will trigger a reply message). user might force the router to sign the same message multiple times.  missing validation of bid parameters (expiriy within valid range, \u2026)  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.13 Router - Cancel is not implemented ", "body": "  Description  Canceling of failed/expired swaps does not seem to be implemented in the router. This may allow a user to trick the router into preparing all its funds which will not automatically be reclaimed after expiration (router DoS).  Examples  cancelExpired is never called  code/packages/sdk/src/sdk.ts:L873-L885  // TODO: this just cancels a transaction, it is misnamed, has nothing to do with expiries  public async cancelExpired(cancelParams: CancelParams, chainId: number): Promise<providers.TransactionResponse> {  const method = this.cancelExpired.name;  const methodId = getRandomBytes32();  this.logger.info({ method, methodId, cancelParams, chainId }, \"Method started\");  const cancelRes = await this.transactionManager.cancel(chainId, cancelParams);  if (cancelRes.isOk()) {  this.logger.info({ method, methodId }, \"Method complete\");  return cancelRes.value;  } else {  throw cancelRes.error;  disabled code  code/packages/router/src/handler.ts:L719-L733  \"Do not cancel ATM, figure out why we are in this case first\",  );  // const cancelRes = await this.txManager.cancel(txData.sendingChainId, {  //   txData,  //   signature: \"0x\",  //   relayerFee: \"0\",  // });  // if (cancelRes.isOk()) {  //   this.logger.warn(  //     { method, methodId, transactionHash: cancelRes.value.transactionHash },  //     \"Cancelled transaction\",  //   );  // } else {  //   this.logger.error({ method, methodId }, \"Could not cancel transaction after error!\");  // }  Recommendation  Implement the cancel flow.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.14 TransactionManager.prepare - Possible griefing/denial of service by front-running    Unverified Fix", "body": "  Resolution  Comment from Connext:  We see this as a highly unlikely attack vector and have chosen not to mitigate it, but it is possible. Users can always and easily generate a new key prepare from a new account, and performing this attack will always cost gas and some dust amount. Further, adding in the suggested require(msg.sender == invariantData.user) will lock out many contract-based use cases and requiring an additional signature/user interaction (auth, approve, prepare, fulfill) is not desirable.  The Connext team claims to have implemented this solution in commit 6811bb2681f44f34ce28906cb842db49fb73d797. We have not reviewed this commit or, generally, the codebase at this point.  Description  A call to TransactionManager.prepare might be front-run with a transaction using the same invariantData but with a different amount and/or expiry values. By choosing a tiny amount of assets, the attacker may prevent the user from locking his original desired amount. The attacker can repeat this process for any new transactionId presented by the user, thus effectively denying the service for him.  Recommendation  Consider adding a require(msg.sender == invariantData.user) restriction to TransactionManager.prepare.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.15 Router - Provide and enforce safe defaults (config) ", "body": "  Description  Chain confirmations default to 1 which is not safe. In case of a re-org the router might (temporarily) get out of sync with the chain and perform actions it should not perform. This may put funds at risk.  Examples  the schema requires an unsafe minimum of 1 confirmation  code/packages/router/src/config.ts:L33-L36  export const TChainConfig = Type.Object({  providers: Type.Array(Type.String()),  confirmations: Type.Number({ minimum: 1 }),  subgraph: Type.String(),  the default configuration uses 1 confirmation  code/packages/router/config.json.example:L1-L17  \"adminToken\": \"blahblah\",  \"chainConfig\": {  \"4\": {  \"providers\": [\"https://rinkeby.infura.io/v3/\"],  \"confirmations\": 1,  \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-rinkeby\"  },  \"5\": {  \"providers\": [\"https://goerli.infura.io/v3/\"],  \"confirmations\": 1,  \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-goerli\"  },  \"logLevel\": \"info\",  \"mnemonic\": \"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\"  Recommendation  Give guidance, provide and enforce safe defaults.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.16 ProposedOwnable - two-step ownership transfer should be confirmed by the new owner    ", "body": "  Resolution  All recommendations given below have been implemented. In addition to that, the privilege to manage assets and the privilege to manage routers can now be renounced separately.  Description  In order to avoid losing control of the contract, the two-step ownership transfer should be confirmed by the new owner s address instead of the current owner.  Examples  acceptProposedOwner is restricted to onlyOwner while ownership should be accepted by the newOwner  code/packages/contracts/contracts/ProposedOwnable.sol:L89-L96  /**  @notice Transfers ownership of the contract to a new account (`newOwner`).  Can only be called by the current owner.  /  function acceptProposedOwner() public virtual onlyOwner {  require((block.timestamp - _proposedTimestamp) > _delay, \"#APO:030\");  _setOwner(_proposed);  move renounced() to ProposedOwnable as this is where it logically belongs to  code/packages/contracts/contracts/TransactionManager.sol:L160-L162  function renounced() public view override returns (bool) {  return owner() == address(0);  onlyOwner can directly access state-var _owner instead of spending more gas on calling owner()  code/packages/contracts/contracts/ProposedOwnable.sol:L76-L79  modifier onlyOwner() {  require(owner() == msg.sender, \"#OO:029\");  _;  Recommendation  onlyOwner can directly access _owner (gas optimization)  add a method to explicitly renounce ownership of the contract  move TransactionManager.renounced() to ProposedOwnable as this is where it logically belongs to  change the access control for acceptProposedOwner from onlyOwner to require(msg.sender == _proposed) (new owner).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.17 FulfillInterpreter - Wrong order of actions in fallback handling ", "body": "  Description  code2/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L68-L90  bool isNative = LibAsset.isNativeAsset(assetId);  if (!isNative) {  LibAsset.increaseERC20Allowance(assetId, callTo, amount);  // Check if the callTo is a contract  bool success;  bytes memory returnData;  if (Address.isContract(callTo)) {  // Try to execute the callData  // the low level call will return `false` if its execution reverts  (success, returnData) = callTo.call{value: isNative ? amount : 0}(callData);  // Handle failure cases  if (!success) {  // If it fails, transfer to fallback  LibAsset.transferAsset(assetId, fallbackAddress, amount);  // Decrease allowance  if (!isNative) {  LibAsset.decreaseERC20Allowance(assetId, callTo, amount);  For the fallback scenario, i.e., the call isn t executed or fails, the funds are first transferred to fallbackAddress, and the previously increased allowance is decreased after that. If the token supports it, the recipient of the direct transfer could try to exploit that the approval hasn t been revoked yet, so the logically correct order is to decrease the allowance first and transfer the funds later. However, it should be noted that the FulfillInterpreter should, at any point in time, only hold the funds that are supposed to be transferred as part of the current transaction; if there are any excess funds, these are leftovers from a previous failure to withdraw everything that could have been withdrawn, so these can be considered up for grabs. Hence, this is only a minor issue.  Recommendation  We recommend reversing the order of actions for the fallback case: Decrease the allowance first, and transfer later. Moreover, it would be better to increase the allowance only in case a call will actually be made, i.e., if Address.isContract(callTo) is true.  Remark  This issue was already present in the original version of the code but was missed initially and only found during the re-audit.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.18 FulfillInterpreter - Executed event can t be linked to TransactionFulfilled event    ", "body": "  Resolution  This issue has been fixed. We d like to point out, though:  Based on the data emitted by the TransactionFulfilled event, it is currently not possible to distinguish between: (A) No call to callTo has been made because the address didn t contain code. (B) Address callTo did contain code, a call was made, and it failed with empty return data. If this distinction seems relevant, an additional bool should be returned from FulfillInterpreter.execute and emitted in TransactionFulfilled, indicating which of the two scenarios were encountered.  The Executed event isn t needed anymore and could be removed.  Description  While it is in the user s best interest not to reuse a transactionId they have used before, unique transaction IDs are not enforced, and a user seeking to wreak havoc might choose to reuse an ID if it helps them accomplish their goal. In this case, event-monitoring software might get confused by several Executed events with the same transactionId and not be able to match the event with its TransactionFulfilled counterpart.  Recommendation  Generally, the following rules apply to transaction IDs:  A user must, in their own best interest, never reuse a transactionId they have used before \u2014 not even across different chains and no matter whether the transaction was successful or not.  This per-user uniqueness of transaction IDs is not enforced, though \u2014 not even per TransactionManager deployment. Hence, the code may not rely on this assumption, and no harm must come from a reused transaction ID for the system or anyone else than the user who reused the ID.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.19 Sdk.finishTransfer - missing validation ", "body": "  Description  Sdk.finishTransfer should validate that the router that locks liquidity in the receiving chain, should be the same router the user had committed to in the sending chain.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.20 FulfillInterpreter - Missing check whether callTo address contains code    ", "body": "  Resolution  This issue has been fixed.  Description  The receiver-side prepare checks whether the callTo address is either zero or a contract:  code/packages/contracts/contracts/TransactionManager.sol:L466-L470  // Check that the callTo is a contract  // NOTE: This cannot happen on the sending chain (different chain  // contexts), so a user could mistakenly create a transfer that must be  // cancelled if this is incorrect  require(invariantData.callTo == address(0) || Address.isContract(invariantData.callTo), \"#P:031\");  However, as a contract may selfdestruct and the check is not repeated later, there is no guarantee that callTo still contains code when the call to this address (assuming it is non-zero) is actually executed in FulfillInterpreter.execute:  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L71-L82  // Try to execute the callData  // the low level call will return `false` if its execution reverts  (bool success, bytes memory returnData) = callTo.call{value: isEther ? amount : 0}(callData);  if (!success) {  // If it fails, transfer to fallback  Asset.transferAsset(assetId, fallbackAddress, amount);  // Decrease allowance  if (!isEther) {  Asset.decreaseERC20Allowance(assetId, callTo, amount);  As a result, if the contract at callTo self-destructs between prepare and fulfill (both on the receiving chain), success will be true, and the funds will probably be lost to the user.  A user could currently try to avoid this by checking that the contract still exists before calling fulfill on the receiving chain, but even then, they might get front-run by selfdestruct, and the situation is even worse with a relayer, so this provides no reliable protection.  Recommendation  Repeat the Address.isContract check on callTo before making the external call in FulfillInterpreter.execute and send the funds to the fallbackAddress if the result is false.  Remark  It should be noted that an unsuccessful call, i.e., a revert, is the only behavior that is recognized by FulfillInterpreter.execute as failure. While it is prevalent to indicate failure by reverting, this doesn t have to be the case; a well-known example is an ERC20 token that indicates a failing transfer by returning false. A user who wants to utilize this feature has to make sure that the called contract behaves accordingly; if that is not the case, an intermediary contract may be employed, which, for example, reverts for return value false.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.21 TransactionManager - Adherence to EIP-712   ", "body": "  Resolution  Comment from Connext:  We did not fully adopt EIP712 because hardware wallet support is still not universal. Additionally, we chose not to address this issue in the recommended fashion (using address(this), block.chainId) because the fulfill signature must be usable across both the sending and receiving chain. Instead, we made sure the transactionManagerReceivingAddress, receivingChainId was signed.  We advise users of the system not to use their key and address for other systems that operate with signed messages unless they can rule out the possibility of replay attacks. Regarding the signed receivingChainId and receivingChainTxManagerAddress, we d like to mention that even for receiver-side fulfillment, these are not verified against the current chain ID and address of the contract.  Description  fulfill function requires the user signature on a transactionId. While currently, the user SDK code is using a cryptographically secured pseudo-random function to generate the transactionId, it should not be counted upon and measures should be placed on the smart-contract level to ensure replay-attack protection.  Examples  code/packages/contracts/contracts/TransactionManager.sol:L918-L933  function recoverSignature(  bytes32 transactionId,  uint256 relayerFee,  string memory functionIdentifier,  bytes calldata signature  ) internal pure returns (address) {  // Create the signed payload  SignedData memory payload = SignedData({  transactionId: transactionId,  relayerFee: relayerFee,  functionIdentifier: functionIdentifier  });  // Recover  return ECDSA.recover(ECDSA.toEthSignedMessageHash(keccak256(abi.encode(payload))), signature);  Recommendation  Consider adhering to EIP-712, or at least including address(this), block.chainId as part of the data signed by the user.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.22 TransactionManager - Hard-coded chain ID might lead to problems after a chain split   Pending", "body": "  Resolution  The recommendation below has been implemented, but the current codebase doesn t handle chain splits correctly. On the chain that gets a new chain ID, funds may be lost or frozen.  More specifically, after a chain split, we may find ourselves in the situation that the current chain ID is neither the sendingChainId nor the receivingChainId stored in the invariant transaction data. If that is the case, we re on the chain that got a new chain ID. fulfill should always revert in this situation, but cancellation should be possible to release locked funds. We don t know, however, whether we should send the funds back to the user (that is, we re on a fork of the sending chain) or whether they should be given back to the router (that is, we re on a fork of the receiving chain). Our recommendation to solve this is to store in the variant transaction data explicitly whether this is the sending chain or the receiving chain; with this information, we can disambiguate the situation and implement cancel correctly.  Description  The ID of the chain on which the contract is deployed is supplied as a constructor argument and stored as an immutable state variable:  code/packages/contracts/contracts/TransactionManager.sol:L104-L107  /**  @dev The chain id of the contract, is passed in to avoid any evm issues  /  uint256 public immutable chainId;  code/packages/contracts/contracts/TransactionManager.sol:L125-L128  constructor(uint256 _chainId) {  chainId = _chainId;  interpreter = new FulfillInterpreter(address(this));  Hence, chainId can never change, and even after a chain split, both contracts would continue to use the same chain ID. That can have undesirable consequences. For example, a transaction that was prepared before the split could be fulfilled on both chains.  Recommendation  It would be better to query the chain ID directly from the chain via block.chainId. However, the development team informed us that they had encountered problems with this approach as some chains apparently are not implementing this correctly. They resorted to the method described above, a constructor-supplied, hard-coded value. For chains that do indeed not inform correctly about their chain ID, this is a reasonable solution. However, for the reasons outlined above, we still recommend querying the chain ID via block.chainId for chains that do support that \u2014 which should be the vast majority \u2014 and using the fallback mechanism only when necessary.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.23 Router - handling of native assetID (0x000..00, e.g. ETH) not implemented ", "body": "  Description  Additionally, handleSenderPrepare does not manage approvals for ERC20 transfers.  Examples  harcoded zero amount  code/packages/router/src/contract.ts:L137-L147  return ResultAsync.fromPromise(  this.txService.sendTx(  to: this.config.chainConfig[chainId].transactionManagerAddress,  data: encodedData,  value: constants.Zero,  chainId,  from: this.signerAddress,  },  requestContext,  ),  code/packages/router/src/contract.ts:L206-L215  this.txService.sendTx(  chainId,  data: fulfillData,  to: nxtpContractAddress,  value: 0,  from: this.signerAddress,  },  requestContext,  ),  approveTokensIfNeeded will fail when using native assets  code/packages/sdk/src/transactionManager.ts:L329-L333  ).andThen((signerAddress) => {  const erc20 = new Contract(  assetId,  ERC20.abi,  this.signer.provider ? this.signer : this.signer.connect(config.provider),  Recommendation  Remove complexity by requiring ERC20 compliant wrapped native assets (e.g.WETH instead of native ETH).  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.24 Router - config file is missing the swapPools attribute and credentials are leaked to console in case of invalid config ", "body": "  Description  Node startup fails due to missing swapPools configuration in config.json.example. Confidential secrets are leaked to console in the event that the config file is invalid.  Examples  yarn workspace @connext/nxtp-router dev  [app] [nodemon] 2.0.12  [app] [nodemon] to restart at any time, enter `rs`  [app] [nodemon] watching path(s): .env dist/**/* ../@connext/nxtp-txservice/dist ../@connext/nxtp-contracts/dist ../@connext/nxtp-utils/dist  [app] [nodemon] watching extensions: js,json  [app] [nodemon] starting `node --enable-source-maps ./dist/index.js | pino-pretty`  [tsc]  [tsc] 13:52:29 - Starting compilation in watch mode...  [tsc]  [tsc]  [tsc] 13:52:29 - Found 0 errors. Watching for file changes.  [app] Found configFile  [app] Invalid config: {  [app]   \"mnemonic\": \"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\",  [app]   \"authUrl\": \"https://auth.connext.network\",  [app]   \"natsUrl\": \"nats://nats1.connext.provide.network:4222,nats://nats2.connext.provide.network:4222,nats://nats3.connext.provide.network:4222\",  [app]   \"adminToken\": \"blahblah\",  [app]   \"chainConfig\": {  [app]     \"4\": {  [app]       \"providers\": [  [app]         \"https://rinkeby.infura.io/v3/\"  [app]       ],  [app]       \"confirmations\": 1,  [app]       \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-rinkeby\",  [app]       \"transactionManagerAddress\": \"0x29E81453AAe28A63aE12c7ED7b3F8BC16629A4Fd\",  [app]       \"minGas\": \"100000000000000000\"  [app]     },  [app]     \"5\": {  [app]       \"providers\": [  [app]         \"https://goerli.infura.io/v3/\"  [app]       ],  [app]       \"confirmations\": 1,  [app]       \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-goerli\",  [app]       \"transactionManagerAddress\": \"0xbF0F4f639cDd010F38CeBEd546783BD71c9e5Ea0\",  [app]       \"minGas\": \"100000000000000000\"  [app]     }  [app]   },  [app]   \"logLevel\": \"info\"  [app] }  [app] Error: must have required property 'swapPools'  [app]     at Object.getEnvConfig (code/packages/router/dist/config.js:135:15)  [app]         -> code/packages/router/src/config.ts:145:11  [app]     at Object.getConfig (code/packages/router/dist/config.js:149:30)  [app]         -> code/packages/router/src/config.ts:161:18  [app]     at Object.<anonymous> (code/packages/router/dist/index.js:19:25)  [app]         -> code/packages/router/src/index.ts:23:16  [app]     at Module._compile (internal/modules/cjs/loader.js:1063:30)  [app]     at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)  [app]     at Module.load (internal/modules/cjs/loader.js:928:32)  [app]     at Function.Module._load (internal/modules/cjs/loader.js:763:16)  [app]     at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:72:12)  [app]     at internal/main/run_main_module.js:17:47  Confidential information is only cleared in case the config file is valid but not in the event of an error  code/packages/router/src/config.ts:L143-L149  if (!valid) {  console.error(`Invalid config: ${JSON.stringify(nxtpConfig, null, 2)}`);  throw new Error(validate.errors?.map((err) => err.message).join(\",\"));  console.log(JSON.stringify({ ...nxtpConfig, mnemonic: \"********\" }, null, 2));  return nxtpConfig;  Recommendation  Provide a valid default example config. Fix integration tests.  Always remove confidential information before logging on screen.  Avoid providing default credentials as it is very likely that someone might end up using them. Consider asking the user to provide missing credentials on first run or autogenerate it for them.  Note that the adminToken is not cleared before it is being printed to screen. If this is a credential it should be blanked out before being printed. Consider separating application-specific configuration from credentials/secrets.  5 Recommendations  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.1 Router - Logging Consistency", "body": "  Description  Avoid using console.*() in favor of the logger.*() family to provide a consistent timestamped log trail. Note that console.* might have different buffering behavior than logger.log which may mix up output lines.  Examples  code/packages/router/src/index.ts:L124-L131  server.listen(8080, \"0.0.0.0\", (err, address) => {  if (err) {  console.error(err);  process.exit(1);  console.log(`Server listening at ${address}`);  });  code/packages/router/src/contract.ts:L415-L416  const decoded = this.txManagerInterface.decodeFunctionResult(\"getRouterBalance\", encodedData);  console.log(\"decoded: \", decoded);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.2 Router - Always perform strict validation of data received from third-parties or untrusted sources", "body": "  Description  For example, in subgraph.ts an external resource is queried to return transactions that match the router s ID, sending Chain, and status. An honest external party will only return items that match this filter. However, in case of the third-party misbehaving (or being breached), it might happen that entries that do not belong to this node or chain configuration are returned.  Examples  code/packages/router/src/subgraph.ts:L153-L175  let allSenderPrepared: GetSenderTransactionsQuery;  try {  allSenderPrepared = await sdk.GetSenderTransactions({  routerId: this.routerAddress.toLowerCase(),  sendingChainId: chainId,  status: TransactionStatus.Prepared,  });  } catch (err) {  this.logger.error(  { method, methodId, error: jsonifyError(err) },  \"Error in sdk.GetSenderTransactions, aborting loop interval\",  );  return;  // create list of txIds for each receiving chain  const receivingChains: Record<string, string[]> = {};  allSenderPrepared.router?.transactions.forEach(({ transactionId, receivingChainId }) => {  if (receivingChains[receivingChainId]) {  receivingChains[receivingChainId].push(transactionId);  } else {  receivingChains[receivingChainId] = [transactionId];  Recommendation  It is recommended to implement a defense-in-depth approach always validating inputs that come from third-parties or untrusted sources. Especially because the resources spent on performing the checks are negligible and significantly reduce the risk posed by third-party data providers.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.3 FulfillInterpreter - ReentrancyGuard can be removed  Pending", "body": "  Resolution                           The   Description and Recommendation  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L22-L28  /**  @notice Errors if the sender is not the transaction manager  /  modifier onlyTransactionManager {  require(msg.sender == _transactionManager, \"#OTM:027\");  _;  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L54-L61  function execute(  bytes32 transactionId,  address payable callTo,  address assetId,  address payable fallbackAddress,  uint256 amount,  bytes calldata callData  ) override external payable nonReentrant onlyTransactionManager {  Consequently, if the TransactionManager contract can t be reentered, the FulfillInterpreter is automatically protected against reentrancy. Hence, if issue 4.9 is fixed, the reentrancy guard can be removed from FulfillInterpreter.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.4 FulfillInterpreter - _transactionManager state variable can be immutable   ", "body": "  Resolution  This recommendation has been implemented.  Description and Recommendation  The _transactionManager state variable in the FulfillInterpreter is set in the constructor and never changed afterward. Hence, it can be immutable.  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L16-L20  address private _transactionManager;  constructor(address transactionManager) {  _transactionManager = transactionManager;  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.5 TransactionManager - Risk mitigation for addLiquidity   ", "body": "  Resolution  This recommendation has been implemented.  Description and Recommendation  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "6.1 Staking node can be inappropriately removed from the tree    ", "body": "  Resolution                           This is fixed in   OrchidProtocol/orchid@8c586f2.  Description  The following code in OrchidDirectory.pull() is responsible for reattaching a child from a removed tree node:  code/dir-ethereum/directory.sol:L275-L281  if (name(stake.left_) == key) {  current.right_ = stake.right_;  current.after_ = stake.after_;  } else {  current.left_ = stake.left_;  current.before_ = stake.before_;  The condition name(stake.left_) == key can never hold because key is the key for stake itself.  The result of this bug is somewhat catastrophic. The child is not reattached, but it still has a link to the rest of the tree via its  parent_  pointer. This means reducing the stake of that node can underflow the ancestors  before/after amounts, leading to improper random selection or failing altogether.  The node replacing the removed node also ends up with itself as a child, which violates the basic tree structure and is again likely to produce integer underflows and other failures.  Recommendation  As a simple fix, use if(name(stake.left_) == name(last)) as already suggested by the development team when this bug was first shared.  Two suggestions for better long-term fixes:  Use a strict interface for tree operations. It should be impossible to update a node s parent without simultaneously updating that parent s child pointer.  As suggested in (https://github.com/ConsenSys/orchid-audit-2019-10/issues/7), simplify the logic in pull() to avoid this logic altogether.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.2 Verifiers need to be pure, but it s very difficult to validate pureness    ", "body": "  Resolution  This is addressed in OrchidProtocol/orchid@1b405fb. With this change, the contract checks that the verifier s code doesn t change (via extcodehash). If the code does change, the contract  fails open  by skipping the verifier and allowing all payments.  Because the code can no longer change, the server can use the (relatively) simple method of executing the contract locally and only allowing a whitelist of opcodes that don t depend on or modify state.  The server already has mitigations for denial of service attacks, including limiting the amount of computing resources that can be used for validating code.  Description  After the initial audit, a  verifier  was introduced to the OrchidLottery code. Each Pot can have an associated OrchidVerifier. This is a contract with a good() function that accepts three parameters:  code/lot-ethereum/lottery.sol:L28  function good(bytes calldata shared, address target, bytes calldata receipt) external pure returns (bool);  The verifier returns a boolean indicating whether a given micropayment should be allowed or not. An example use case is a verifier that only allows certain target addresses to be paid. In this case, shared (a single value for a given Pot) is a merkle root, target is (as always) the address being paid, and receipt (specified by the payment recipient) is a merkle proof that the target address is within the merkle tree with the given root.  Unfortunately, this simple scheme is insufficient. As a simple example, a verifier contract could be created with the CREATE2 opcode. It could be demonstrated that it reads no state when good() is called. Then the contract could be destroyed by calling a function that performs a SELFDESTRUCT, and it could be replaced via another CREATE2 call with different code.  This could be mitigated by rejecting any verifier contract that contains the SELFDESTRUCT opcode, but this would also catch harmless occurrences of that particular byte. https://gist.github.com/Arachnid/e8f0638dc9f5687ff8170a95c47eac1e attempts to find SELFDESTRUCT opcodes but fails to account for tricks where the SELFDESTRUCT appears to be data but can actually be executed. (See Recmo s comment.) In general, this approach is difficult to get right and probably requires full data flow analysis to be correct.  Another possible mitigation is to use a factory contract to deploy the verifiers, guaranteeing that they re not created with CREATE2. This should render SELFDESTRUCT harmless, but there s no guarantee that future forks won t introduce new vectors here.  Finally, requiring servers to implement potentially complex contract validation opens up potential for denial-of-service attacks. A server will have to implement mitigations to prevent repeatedly checking the same verifier or spending inordinate resources checking a maliciously crafted contract (e.g. one with high branching factors).  Recommendation  The verifiers add quite a bit of complexity and risk. We recommend looking for an alternative approach, such as including a small number of vetted verifiers (e.g. a merkle proof verifier) or having servers use their own  allow list  for verifiers that they trust.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.3 Simplify the logic in OrchidDirectory.pull()    ", "body": "  Resolution  This was addressed in the following commits:  OrchidProtocol/orchid@0ad2484  OrchidProtocol/orchid@8b3e821  OrchidProtocol/orchid@affbf93  OrchidProtocol/orchid@e506c0f  OrchidProtocol/orchid@f864e60  Description  pull() is the most complex function in OrchidDirectory, due to its need to handle removing a node altogether when its stake amount reaches 0.  The current logic for removing an interior node is roughly this:  Given a node to be remove called old, walk down the tree, always stepping towards the  heavier  (in terms of total stake) subtree, until you reach a leaf node (called target).  If target is a direct child of old:  Set target to be a child of old.parent. Move the remaining child of old to be under target.  If target is not a direct child of old:  Swap target and old in the tree. Walk up the tree from old (now a leaf node) to target to subtract target s staked amount from the nodes in between. Detach old from the tree.  The code for this is fairly complex, and one serious bug (issue 6.1) was identified in this code.  This logic can be simplified by combining the two cases (direct child and not) and thinking of it as roughly a two-step operation of  detach leaf node  and  replace interior node with leaf node .  Given a node to be removed called old, walk the tree to find target as before.  Walk back up to old, subtracting target s staked amount from the nodes in between.  Detach target from the tree.  Replace old with target.  (Note that in the code,  old  above is called stake and  target  is calledcurrent.)  Recommendation  Replace this code:  code/dir-ethereum/directory.sol:L266-L297  bytes32 direct = current.parent_;  copy(pivot, last);  current.parent_ = stake.parent_;  if (direct == key) {  Primary storage other = stake.before_ > stake.after_ ? stake.right_ : stake.left_;  if (!nope(other))  stakes_[name(other)].parent_ = name(last);  if (name(stake.left_) == key) {  current.right_ = stake.right_;  current.after_ = stake.after_;  } else {  current.left_ = stake.left_;  current.before_ = stake.before_;  } else {  if (!nope(stake.left_))  stakes_[name(stake.left_)].parent_ = name(last);  if (!nope(stake.right_))  stakes_[name(stake.right_)].parent_ = name(last);  current.right_ = stake.right_;  current.after_ = stake.after_;  current.left_ = stake.left_;  current.before_ = stake.before_;  stake.parent_ = direct;  copy(last, staker, stakee);  step(key, stake, -current.amount_, current.parent_);  kill(last);  with something like this code:  // Remember this key so we can update `pivot` later  bytes32 currentKey = name(last);  // Remove `current` from the subtree rooted at `stake`  step(currentKey, current, -current.amount_, stake.parent_);  kill(last);  // Replace `stake` with `current`  current.left_ = stake.left_;  if (!nope(current.left_))  stakes_[name(current.left_)].parent_ = currentKey;  current.right_ = stake.right_;  if (!nope(current.right_))  stakes_[name(current.right_)].parent_ = currentKey;  current.before_ = stake.before_;  current.after_ = stake.after_;  current.parent_ = stake.parent_;  pivot.value_ = currentKey; // `pivot` was parent's pointer to `stake`  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.4 Remove unnecessary address payable   ", "body": "  Resolution                           The development team decided to leave this as-is.   Description  The address payable type is only needed for transferring ether to an address. The OrchidDirectory and OrchidLottery contracts work with tokens, not ether, so there s no need for any parameters to be of type address payable.  Recommendation  Use simply address instead of address payable everywhere.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.5 Use consistent staker, stakee ordering in OrchidDirectory    ", "body": "  Resolution                           This is fixed in   OrchidProtocol/orchid@1cfef88.  Description  code/dir-ethereum/directory.sol:L156  function lift(bytes32 key, Stake storage stake, uint128 amount, address stakee, address staker) private {  OrchidDirectory.lift() has a parameter stakee that precedes staker, while the rest of the code always places staker first. Because Solidity doesn t have named parameters, it s a good idea to use a consistent ordering to avoid mistakes.  Recommendation  Switch lift() to follow the  staker then stakee  ordering convention of the rest of the contract.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.6 Use more descriptive function and variable names   ", "body": "  Resolution  This issue is about readability. Even though the audit team firmly believes that improved readability would increase trust in Orchid from its clients, this is not a correctness issue.  The Orchid team believes that making this change, particularly this late in their development cycle, would be too risky. The development team is very familiar with the current terminology, and bugs may accidentally be introduced with the change.  Description  Throughout OrchidDirectory and OrchidLottery, function and variable names are quite obscure. This makes it harder for a reader to understand the code.  Examples  OrchidDirectory:  heft() returns the total staked for a given stakee (perhaps totalForStakee()) Primary is a pointer to a tree node (perhaps NodePointer), and its member value_ could be named key name() gives the key for a given (staker, stakee) pair or a Primary (perhaps getKey()) copy() writes a key to a node pointer (probably better to remove this and just do pointer.key = ...) kill() sets a node pointer to zero (probably better to just remove this and use delete pointer) nope() checks whether a node pointer exists (probably better to just do pointer.key == 0) have() returns the total number of staked tokens (perhaps totalStaked) scan() finds a node, given a random 128-bit number (perhaps selectNode(uint128 random)) turn() is only used in one place and is likely better just inlined step() walks up a subtree, adjusting before/after amounts along the way (perhaps propagate() or bubbleUp()) lift() updates the stake for a given node and then calls step() (perhaps updateNodeStake()) more() is really just the body for push(), so it should probably be moved inside push() instead push() is the external method for staking (perhaps increaseStake() or just stake()) wait() increases the withdrawal delay for the sender s stake for a given stakee (increaseDelay()) Pending could be called PendingWithdrawal take() could be called completeWithdrawal() stop() could be called cancelWithdrawal() delay_ could be withdrawalDelay pull() decreases stake and establishes a pending withdrawal (perhaps decreaseStake(), unstake() or startWithdrawal()) Within pull():  pivot could be pointerToStake last could be pointerToLeaf current could be leaf direct could be leafParent other could be sibling  OrchidLottery:  Pot could perhaps be Fund send() just emits an Update event (perhaps log() or logUpdate()) Track is a struct that keeps track of a ticket that has already been redeemed to prevent replay (perhaps RedeemedTicket) kill() is overloaded to delete funds and used tickets (perhaps deleteFund() and forgetTicket()) take() could be called transferTokens() grab() redeems a winning ticket (perhaps redeem() or redeemTicket()) give() and pull() both transfer tokens from a given Pot, but one is used by the signer and one by the funder. Perhaps better would be a single transferFromPot(address funder, address signer, address target, uint128 amount) with require(msg.sender == funder || msg.sender == signer). warn() could be startWithdrawal() lock() could be cancelWithdrawal() pull() could be completeWithdrawal()  Recommendation  Consider using longer, more descriptive names to make it easier to understand the code. Where there s no particularly good name, add comments explaining the meaning.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.7 In OrchidDirectory.step() and OrchidDirectory.lift(), use a signed amount   ", "body": "  Resolution                           The variables in question are now   Description  step() and lift() both accept a uint128 parameter called amount. This amount is added to various struct fields, which are also of type uint128.  The contract intentionally underflows this amount to represent negative numbers. This is roughly equivalent to using a signed integer, except that:  Unsigned integers aren t sign extended when they re cast to a larger integer type, so care must be taken to avoid this.  Tools that look for integer overflow/underflow will detect this possibility as a bug. It s then hard to determine which overflows are intentional and which are not.  Examples  code/dir-ethereum/directory.sol:L247  lift(key, stake, -amount, stakee, staker);  code/dir-ethereum/directory.sol:L296  step(key, stake, -current.amount_, current.parent_);  Recommendation  Use int128 instead, and ensure that amounts can never exceed the maximum int128 value. (This is trivially achieved by limiting the total number of tokens that can exist.)  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.8 Document that math in OrchidDirectory assumes a maximum number of tokens    ", "body": "  Resolution                           This is fixed in   OrchidProtocol/orchid@f2efe42 by using  Description  OrchidDirectory relies on mathematical operations being unable to overflow due to the particular ERC20 token being used being capped at less than 2**128.  Examples  The following code in step() assumes that no before/after amount can reach 2**128:  code/dir-ethereum/directory.sol:L145-L148  if (name(stake.left_) == key)  stake.before_ += amount;  else  stake.after_ += amount;  The following code in lift() assumes that no staked amount (or total amount for a given stakee) can reach 2**128:  code/dir-ethereum/directory.sol:L157-L164  uint128 local = stake.amount_;  local += amount;  stake.amount_ = local;  emit Update(staker, stakee, local);  uint128 global = stakees_[stakee].amount_;  global += amount;  stakees_[stakee].amount_ = global;  The following code in have() assumes that the total amount staked cannot reach 2**128:  code/dir-ethereum/directory.sol:L103  return stake.before_ + stake.after_ + stake.amount_;  Recommendation  Document this assumption in the form of code comments where potential overflows exist.  Consider also asserting the ERC20 token s total supply in the constructor to attempt to block using a token that violates this constraint and/or checking in push() that the total amount staked will remain less than 2**128. This recommendation is in line with the mitigation proposed for issue 6.7.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.9 Unneeded named return parameter    ", "body": "  Resolution                           Fixed in   OrchidProtocol/orchid@21d56d5  Description  In the heft function in the OrchidDirectory contract, there is an unused and unneeded named return parameter (that actually instantiates a new variable in memory which is not used).  Remediation  Change returns (uint128 amount) to returns (uint128).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.10 Improve function visibility    ", "body": "  Resolution                           Fixed in   OrchidProtocol/orchid@68fb26a  Description  The following methods are not called internally in the token contract and visibility can, therefore, be restricted to external rather than public. This is more gas efficient because less code is emitted and data does not need to be copied into memory. It also makes functions a bit simpler to reason about because there s no need to worry about the possibility of internal calls.  OrchidDirectory.heft()  OrchidDirectory.scan()  OrchidDirectory.push()  OrchidDirectory.wait()  OrchidDirectory.take()  OrchidDirectory.stop()  OrchidDirectory.pull()  OrchidLocation.move()  OrchidLocation.look()  OrchidLottery.size()  OrchidLottery.keys()  OrchidLottery.seek()  OrchidLottery.look()  OrchidLottery.push()  OrchidLottery.move()  OrchidLottery.kill()  OrchidLottery.grab()  OrchidLottery.pull()  OrchidLottery.warn()  OrchidLottery.lock()  OrchidLottery.pull()  OrchidCurator.list()  OrchidCurator.good()  OrchidUntrusted.good()  Recommendation  Change visibility of these methods to external.  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of the MythX Pro vulnerability scan was reviewed by the audit team and no vulnerabilities were identified as part of the process.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/curator.sol  35:8    warning    Provide an error message for require().    error-reason  contracts/directory.sol  107:8     warning    Provide an error message for require().            error-reason  141:4     error      \"step\": Avoid assigning to function parameters.    security/no-assign-params  141:4     error      \"step\": Avoid assigning to function parameters.    security/no-assign-params  176:8     warning    Provide an error message for require().            error-reason  180:12    warning    Provide an error message for require().            error-reason  202:8     warning    Provide an error message for require().            error-reason  209:8     warning    Provide an error message for require().            error-reason  211:8     warning    Provide an error message for require().            error-reason  226:8     warning    Provide an error message for require().            error-reason  226:35    warning    Avoid using 'block.timestamp'.                     security/no-block-members  228:8     warning    Provide an error message for require().            error-reason  233:8     warning    Provide an error message for require().            error-reason  233:35    warning    Avoid using 'block.timestamp'.                     security/no-block-members  244:8     warning    Provide an error message for require().            error-reason  245:8     warning    Provide an error message for require().            error-reason  305:8     warning    Provide an error message for require().            error-reason  306:26    warning    Avoid using 'block.timestamp'.                     security/no-block-members  contracts/location.sol  38:24    warning    Avoid using 'block.timestamp'.    security/no-block-members  contracts/lottery.sol  66:8      warning    Provide an error message for require().            error-reason  104:8     warning    Provide an error message for require().            error-reason  111:8     warning    Provide an error message for require().            error-reason  117:8     warning    Provide an error message for require().            error-reason  131:8     warning    Provide an error message for require().            error-reason  131:32    warning    Avoid using 'block.timestamp'.                     security/no-block-members  140:4     error      \"take\": Avoid assigning to function parameters.    security/no-assign-params  153:12    warning    Provide an error message for require().            error-reason  156:4     error      \"grab\": Avoid assigning to function parameters.    security/no-assign-params  156:4     warning    Line exceeds the limit of 145 characters           max-len  157:8     warning    Provide an error message for require().            error-reason  158:8     warning    Provide an error message for require().            error-reason  163:12    error      Only use indent of 8 spaces.                       indentation  165:12    error      Only use indent of 8 spaces.                       indentation  166:12    error      Only use indent of 8 spaces.                       indentation  167:12    error      Only use indent of 8 spaces.                       indentation  167:12    warning    Provide an error message for require().            error-reason  167:28    warning    Avoid using 'block.timestamp'.                     security/no-block-members  168:12    error      Only use indent of 8 spaces.                       indentation  168:12    warning    Provide an error message for require().            error-reason  169:12    error      Only use indent of 8 spaces.                       indentation  171:12    error      Only use indent of 8 spaces.                       indentation  172:0     error      Only use indent of 8 spaces.                       indentation  175:20    warning    Avoid using 'block.timestamp'.                     security/no-block-members  176:64    warning    Avoid using 'block.timestamp'.                     security/no-block-members  182:8     warning    Provide an error message for require().            error-reason  200:22    warning    Avoid using 'block.timestamp'.                     security/no-block-members  214:8     warning    Provide an error message for require().            error-reason  215:8     warning    Provide an error message for require().            error-reason  215:31    warning    Avoid using 'block.timestamp'.                     security/no-block-members  219:8     warning    Provide an error message for require().            error-reason  \u2716 12 errors, 38 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "7.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  8 S\u016brya s Description Report  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "8.1 Files Description Table", "body": "  contracts/curator.sol  5ea6c8374cec289bcf4dfe1adb3bb157ca9bab74  contracts/directory.sol  811ab3b049d570c4236ee965d76ed1a9f5cb929e  contracts/location.sol  1ea56960f41ca3a299c4fd35fab9ef1fdd494d5b  contracts/lottery.sol  e63f3c86b3abba57d0a7e3ca36436bfee4d9ac1b  contracts/token.sol  faf15f117ac160641adfe56c2a01ad14bff931f3  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "8.2 Contracts Description Table", "body": "  Function Name  Visibility  Mutability  Modifiers  OrchidCurator  Implementation  <Constructor>  Public    list  Public    NO   good  Public    NO   OrchidUntrusted  Implementation  good  Public    NO   IOrchidDirectory  Interface  have  External    NO   OrchidDirectory  Implementation  IOrchidDirectory  <Constructor>  Public    heft  Public    NO   name  Public    NO   name  Private \ud83d\udd10  copy  Private \ud83d\udd10  copy  Private \ud83d\udd10  kill  Private \ud83d\udd10  nope  Private \ud83d\udd10  have  Public    NO   scan  Public    NO   turn  Private \ud83d\udd10  step  Private \ud83d\udd10  lift  Private \ud83d\udd10  more  Private \ud83d\udd10  push  Public    NO   wait  Public    NO   take  Public    NO   stop  Public    NO   pull  Public    NO   OrchidLocation  Implementation  move  Public    NO   look  Public    NO   OrchidLottery  Implementation  <Constructor>  Public    send  Private \ud83d\udd10  find  Private \ud83d\udd10  kill  Private \ud83d\udd10  size  Public    NO   keys  Public    NO   seek  Public    NO   page  Public    NO   look  Public    NO   push  Public    NO   move  Public    NO   kill  Private \ud83d\udd10  kill  Public    NO   take  Private \ud83d\udd10  grab  Public    NO   give  Public    NO   pull  Public    NO   warn  Public    NO   lock  Public    NO   pull  Public    NO   OrchidToken  Implementation  ERC20, ERC20Detailed  <Constructor>  Public    ERC20Detailed  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "8.3 Legend", "body": "  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "5.1 Random task execution    ", "body": "  Resolution                           Fixed in   DecenterApps/defisaver-v3-contracts@478e9cd by adding  Description  In a scenario where user takes a flash loan, _parseFLAndExecute() gives the flash loan wrapper contract (FLAaveV2, FLDyDx) the permission to execute functions on behalf of the user s DSProxy. This execution permission is revoked only after the entire recipe execution is finished, which means that in case that any of the external calls along the recipe execution is malicious, it might call executeAction() back and inject any task it wishes (e.g. take user s funds out, drain approved tokens, etc)  Examples  code/contracts/actions/flashloan/FLAaveV2.sol:L105-L136  function executeOperation(  address[] memory _assets,  uint256[] memory _amounts,  uint256[] memory _fees,  address _initiator,  bytes memory _params  ) public returns (bool) {  require(msg.sender == AAVE_LENDING_POOL, ERR_ONLY_AAVE_CALLER);  require(_initiator == address(this), ERR_SAME_CALLER);  (Task memory currTask, address proxy) = abi.decode(_params, (Task, address));  // Send FL amounts to user proxy  for (uint256 i = 0; i < _assets.length; ++i) {  _assets[i].withdrawTokens(proxy, _amounts[i]);  address payable taskExecutor = payable(registry.getAddr(TASK_EXECUTOR_ID));  // call Action execution  IDSProxy(proxy).execute{value: address(this).balance}(  taskExecutor,  abi.encodeWithSelector(CALLBACK_SELECTOR, currTask, bytes32(_amounts[0] + _fees[0]))  );  // return FL  for (uint256 i = 0; i < _assets.length; i++) {  _assets[i].approveToken(address(AAVE_LENDING_POOL), _amounts[i] + _fees[i]);  return true;  Recommendation  A reentrancy guard (mutex) that covers the entire content of FLAaveV2.executeOperation/FLDyDx.callFunction should be used to prevent such attack.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.2 Tokens with more than 18 decimal points will cause issues    ", "body": "  Resolution                           Fixed in   DecenterApps/defisaver-v3-contracts@de22007 by using  Description  It is assumed that the maximum number of decimals for each token is 18. However uncommon, but it is possible to have tokens with more than 18 decimals, as an Example YAMv2 has 24 decimals. This can result in broken code flow and unpredictable outcomes (e.g. an underflow will result with really high rates).  Examples  contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol  function getSellRate(address _srcAddr, address _destAddr, uint _srcAmount, bytes memory) public override view returns (uint rate) {  (rate, ) = KyberNetworkProxyInterface(KYBER_INTERFACE)  .getExpectedRate(IERC20(_srcAddr), IERC20(_destAddr), _srcAmount);  // multiply with decimal difference in src token  rate = rate * (10**(18 - getDecimals(_srcAddr)));  // divide with decimal difference in dest token  rate = rate / (10**(18 - getDecimals(_destAddr)));  code/contracts/views/AaveView.sol :  also used in getLoanData()  Recommendation  Make sure the code won t fail in case the token s decimals is more than 18.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.3 Error codes of Compound s Comptroller.enterMarket, Comptroller.exitMarket are not checked    ", "body": "  Resolution                           Fixed in   DecenterApps/defisaver-v3-contracts@7075e49 by reverting in the case the return value is non zero.  Description  Compound s enterMarket/exitMarket functions return an error code instead of reverting in case of failure. DeFi Saver smart contracts never check for the error codes returned from Compound smart contracts, although the code flow might revert due to unavailability of the CTokens, however early on checks for Compound errors are suggested.  Examples  code/contracts/actions/compound/helpers/CompHelper.sol:L26-L37  function enterMarket(address _cTokenAddr) public {  address[] memory markets = new address[](1);  markets[0] = _cTokenAddr;  IComptroller(COMPTROLLER_ADDR).enterMarkets(markets);  /// @notice Exits the Compound market  /// @param _cTokenAddr CToken address of the token  function exitMarket(address _cTokenAddr) public {  IComptroller(COMPTROLLER_ADDR).exitMarket(_cTokenAddr);  Recommendation  Caller contract should revert in case the error code is not 0.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.4 Reversed order of parameters in allowance function call    ", "body": "  Resolution                           Fixed in   DecenterApps/defisaver-v3-contracts@8b5657b by swapping the order of function call parameters.  Description  When trying to pull the maximum amount of tokens from an approver to the allowed spender, the parameters that are used for the allowance function call are not in the same order that is used later in the call to safeTransferFrom.  Examples  code/contracts/utils/TokenUtils.sol:L26-L44  function pullTokens(  address _token,  address _from,  uint256 _amount  ) internal returns (uint256) {  // handle max uint amount  if (_amount == type(uint256).max) {  uint256 allowance = IERC20(_token).allowance(address(this), _from);  uint256 balance = getBalance(_token, _from);  _amount = (balance > allowance) ? allowance : balance;  if (_from != address(0) && _from != address(this) && _token != ETH_ADDR && _amount != 0) {  IERC20(_token).safeTransferFrom(_from, address(this), _amount);  return _amount;  Recommendation  Reverse the order of parameters in allowance function call to fit the order that is in the safeTransferFrom function call.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.5 Full test suite is recommended   Pending", "body": "  Description  The test suite at this stage is not complete and many of the tests fail to execute. For complicated systems such as DeFi Saver, which uses many different modules and interacts with different DeFi protocols, it is crucial to have a full test coverage that includes the edge cases and failed scenarios. Especially this helps with safer future development and upgrading each modules.  As we ve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.  Some issues such as issue 5.4 could be caught by a full coverage test suite.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.6 Kyber getRates code is unclear ", "body": "  Description  In contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol the function names don t reflect their true functionalities, and the code uses some undocumented assumptions.  Examples  getSellRate can be converted into one function to get the rates, which then for buy or sell can swap input and output tokens  getBuyRate uses a 3% slippage that is not documented.  function getSellRate(address _srcAddr, address _destAddr, uint _srcAmount, bytes memory) public override view returns (uint rate) {  (rate, ) = KyberNetworkProxyInterface(KYBER_INTERFACE)  .getExpectedRate(IERC20(_srcAddr), IERC20(_destAddr), _srcAmount);  // multiply with decimal difference in src token  rate = rate * (10**(18 - getDecimals(_srcAddr)));  // divide with decimal difference in dest token  rate = rate / (10**(18 - getDecimals(_destAddr)));  /// @notice Return a rate for which we can buy an amount of tokens  /// @param _srcAddr From token  /// @param _destAddr To token  /// @param _destAmount To amount  /// @return rate Rate  function getBuyRate(address _srcAddr, address _destAddr, uint _destAmount, bytes memory _additionalData) public override view returns (uint rate) {  uint256 srcRate = getSellRate(_destAddr, _srcAddr, _destAmount, _additionalData);  uint256 srcAmount = wmul(srcRate, _destAmount);  rate = getSellRate(_srcAddr, _destAddr, srcAmount, _additionalData);  // increase rate by 3% too account for inaccuracy between sell/buy conversion  rate = rate + (rate / 30);  Recommendation  Refactoring the code to separate getting rate functionality with getSellRate and getBuyRate. Explicitly document any assumptions in the code ( slippage, etc)  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.7 Missing check in IOffchainWrapper.takeOrder implementation ", "body": "  Description  IOffchainWrapper.takeOrder wraps an external call that is supposed to perform a token swap. As for the two different implementations ZeroxWrapper and ScpWrapper this function validates that the destination token balance after the swap is greater than the value before. However, it is not sufficient, and the user-provided minimum amount for swap should be taken in consideration as well. Besides, the external contract should not be trusted upon, and SafeMath should be used for the subtraction operation.  Examples  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L42-L50  uint256 tokensBefore = _exData.destAddr.getBalance(address(this));  (success, ) = _exData.offchainData.exchangeAddr.call{value: _exData.offchainData.protocolFee}(_exData.offchainData.callData);  uint256 tokensSwaped = 0;  if (success) {  // get the current balance of the swaped tokens  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  require(tokensSwaped > 0, ERR_TOKENS_SWAPED_ZERO);  code/contracts/exchangeV3/offchainWrappersV3/ScpWrapper.sol:L43-L51  uint256 tokensBefore = _exData.destAddr.getBalance(address(this));  (success, ) = _exData.offchainData.exchangeAddr.call{value: _exData.offchainData.protocolFee}(_exData.offchainData.callData);  uint256 tokensSwaped = 0;  if (success) {  // get the current balance of the swaped tokens  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  require(tokensSwaped > 0, ERR_TOKENS_SWAPED_ZERO);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.8 Unused code present in the codebase ", "body": "  Resolution                           Some of the unused code were removed in   DecenterApps/defisaver-v3-contracts@61b0c09.  Description  There are a few instances of unused code (dead code) in the code base, that is suggested to be removed .  Examples  DFSExchange.sol contract is not used  /contracts/utils/ZrxAllowlist.sol these functions are not used in the codebase:  nonPayableAddrs mapping addNonPayableAddr() removeNonPayableAddr() isNonPayableAddr()  DSProxy.execute(bytes memory _code, bytes memory _data) is not intended to used.  There might be more instances of unused code in the codebase.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.9 Return values not used for DFSExchangeCore.onChainSwap ", "body": "  Description  Return values from DFSExchangeCore.onChainSwap are not used.  Examples  code/contracts/exchangeV3/DFSExchangeCore.sol:L37-L73  function _sell(ExchangeData memory exData) internal returns (address, uint256) {  uint256 amountWithoutFee = exData.srcAmount;  address wrapper = exData.offchainData.wrapper;  bool offChainSwapSuccess;  uint256 destBalanceBefore = exData.destAddr.getBalance(address(this));  // Takes DFS exchange fee  exData.srcAmount -= getFee(  exData.srcAmount,  exData.user,  exData.srcAddr,  exData.dfsFeeDivider  );  // Try 0x first and then fallback on specific wrapper  if (exData.offchainData.price > 0) {  (offChainSwapSuccess, ) = offChainSwap(exData, ExchangeActionType.SELL);  // fallback to desired wrapper if 0x failed  if (!offChainSwapSuccess) {  onChainSwap(exData, ExchangeActionType.SELL);  wrapper = exData.wrapper;  uint256 destBalanceAfter = exData.destAddr.getBalance(address(this));  uint256 amountBought = sub(destBalanceAfter, destBalanceBefore);  // check slippage  require(amountBought >= wmul(exData.minPrice, exData.srcAmount), ERR_SLIPPAGE_HIT);  // revert back exData changes to keep it consistent  exData.srcAmount = amountWithoutFee;  return (wrapper, amountBought);  code/contracts/exchangeV3/DFSExchangeCore.sol:L79-L117  function _buy(ExchangeData memory exData) internal returns (address, uint256) {  require(exData.destAmount != 0, ERR_DEST_AMOUNT_MISSING);  uint256 amountWithoutFee = exData.srcAmount;  address wrapper = exData.offchainData.wrapper;  bool offChainSwapSuccess;  uint256 destBalanceBefore = exData.destAddr.getBalance(address(this));  // Takes DFS exchange fee  exData.srcAmount -= getFee(  exData.srcAmount,  exData.user,  exData.srcAddr,  exData.dfsFeeDivider  );  // Try 0x first and then fallback on specific wrapper  if (exData.offchainData.price > 0) {  (offChainSwapSuccess, ) = offChainSwap(exData, ExchangeActionType.BUY);  // fallback to desired wrapper if 0x failed  if (!offChainSwapSuccess) {  onChainSwap(exData, ExchangeActionType.BUY);  wrapper = exData.wrapper;  uint256 destBalanceAfter = exData.destAddr.getBalance(address(this));  uint256 amountBought = sub(destBalanceAfter, destBalanceBefore);  // check slippage  require(amountBought >= exData.destAmount, ERR_SLIPPAGE_HIT);  // revert back exData changes to keep it consistent  exData.srcAmount = amountWithoutFee;  return (wrapper, amountBought);  Recommendation  The return value can be used for verification of the swap or used in the event data.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.10 Return value is not used for TokenUtils.withdrawTokens    ", "body": "  Resolution                           Fixed in   DecenterApps/defisaver-v3-contracts@37dabff by storing the return value locally and use its value throughout the execution.  Description  The return value of TokenUtils.withdrawTokens which represents the actual amount of tokens that were transferred is never used throughout the repository. This might cause discrepancy in the case where the original value of _amount was type(uint256).max.  Examples  code/contracts/actions/aave/AaveBorrow.sol:L70-L97  function _borrow(  address _market,  address _tokenAddr,  uint256 _amount,  uint256 _rateMode,  address _to,  address _onBehalf  ) internal returns (uint256) {  ILendingPoolV2 lendingPool = getLendingPool(_market);  // defaults to onBehalf of proxy  if (_onBehalf == address(0)) {  _onBehalf = address(this);  lendingPool.borrow(_tokenAddr, _amount, _rateMode, AAVE_REFERRAL_CODE, _onBehalf);  _tokenAddr.withdrawTokens(_to, _amount);  logger.Log(  address(this),  msg.sender,  \"AaveBorrow\",  abi.encode(_market, _tokenAddr, _amount, _rateMode, _to, _onBehalf)  );  return _amount;  code/contracts/utils/TokenUtils.sol:L46-L53  function withdrawTokens(  address _token,  address _to,  uint256 _amount  ) internal returns (uint256) {  if (_amount == type(uint256).max) {  _amount = getBalance(_token, address(this));  Recommendation  The return value can be used to validate the withdrawal or used in the event emitted.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.11 Missing access control for DefiSaverLogger.Log", "body": "  Description  DefiSaverLogger is used as a logging aggregator within the entire dapp, but anyone can create logs.  Examples  code/contracts/utils/DefisaverLogger.sol:L14-L21  function Log(  address _contract,  address _caller,  string memory _logName,  bytes memory _data  ) public {  emit LogEvent(_contract, _caller, _logName, _data);  6 Recommendations  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "6.1 Use a single file for all system-wide constants", "body": "  Description  There are many addresses and constants using in the system. It is suggested to put the most used ones in one file (e.g. constants.sol and use inheritance to access these values. This will help with the readability and easier maintenance for future changes. As some of these hardcoded values are admin addresses, this also helps with any possible incident response.  Examples  Logger:  DFSRegistry  TaskExecutor  ActionBase  DefisaverLogger public constant logger = DefisaverLogger(  0x5c55B921f590a89C1Ebe84dF170E655a82b62126  );  Admin Vault:  AdminAuth  AdminVault public constant adminVault = AdminVault(0xCCf3d848e08b94478Ed8f46fFead3008faF581fD);  REGISTRY_ADDR  SubscriptionProxy  StrategyExecutor  TaskExecutor  ActionBase  address public constant REGISTRY_ADDR = 0xB0e1682D17A96E8551191c089673346dF7e1D467;  Any other constant in the system also can be moved to this contract.  Recommendation  Use constants.sol and import this file in the contracts that require access to these values. This is just a recommendation, as discussed with the team, on some use cases this might result in higher gas usage on deployment.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "6.2 Code quality & Styling", "body": "  Description  Here are some examples that the code style does not follow the best practices:  Examples  Public/external function names should not be prefixed with _  code/contracts/core/TaskExecutor.sol:L56  function _executeActionsFromFL(Task memory _currTask, bytes32 _flAmount) public payable {  Function parameters are being overriden  code/contracts/exchangeV3/DFSExchange.sol:L24-L37  function sell(ExchangeData memory exData, address payable _user) public payable   {  exData.dfsFeeDivider = SERVICE_FEE;  exData.user = _user;  // Perform the exchange  (address wrapper, uint destAmount) = _sell(exData);  // send back any leftover ether or tokens  sendLeftover(exData.srcAddr, exData.destAddr, _user);  // log the event  logger.Log(address(this), msg.sender, \"ExchangeSell\", abi.encode(wrapper, exData.srcAddr, exData.destAddr, exData.srcAmount, destAmount));  MAX_SERVICE_FEE should be MIN_SERVICE_FEE  code/contracts/utils/Discount.sol:L28-L33  function setServiceFee(address _user, uint256 _fee) public {  require(msg.sender == owner, \"Only owner\");  require(_fee >= MAX_SERVICE_FEE || _fee == 0, \"Wrong fee value\");  serviceFees[_user] = CustomServiceFee({active: true, amount: _fee});  Functions with a get prefix should not modify state  code/contracts/exchangeV3/DFSExchangeCore.sol:L182-L206  function getFee(  uint256 _amount,  address _user,  address _token,  uint256 _dfsFeeDivider  ) internal returns (uint256 feeAmount) {  if (_dfsFeeDivider != 0 && Discount(DISCOUNT_ADDRESS).isCustomFeeSet(_user)) {  _dfsFeeDivider = Discount(DISCOUNT_ADDRESS).getCustomServiceFee(_user);  if (_dfsFeeDivider == 0) {  feeAmount = 0;  } else {  feeAmount = _amount / _dfsFeeDivider;  // fee can't go over 10% of the whole amount  if (feeAmount > (_amount / 10)) {  feeAmount = _amount / 10;  address walletAddr = feeRecipient.getFeeAddr();  _token.withdrawTokens(walletAddr, feeAmount);  Protocol fee value should be validated against msg.value and not against contract s balance  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L25-L31  function takeOrder(  ExchangeData memory _exData,  ExchangeActionType _type  ) override public payable returns (bool success, uint256) {  // check that contract have enough balance for exchange and protocol fee  require(_exData.srcAddr.getBalance(address(this)) >= _exData.srcAmount, ERR_SRC_AMOUNT);  require(TokenUtils.ETH_ADDR.getBalance(address(this)) >= _exData.offchainData.protocolFee, ERR_PROTOCOL_FEE);  Remove deprecation warning (originated in OpenZeppelin s implementation) in comment, as the issue has been solved  code/contracts/utils/SafeERC20.sol:L33-L44  /**  @dev Deprecated. This function has issues similar to the ones found in  {ERC20-approve}, and its usage is discouraged.  /  function safeApprove(  IERC20 token,  address spender,  uint256 value  ) internal {  _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, 0));  _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, value));  Typo RECIPIE_FEE instead of RECIPE_FEE  code/contracts/actions/exchange/DfsSell.sol:L15  uint internal constant RECIPIE_FEE = 400;  Code duplication : sendLeftOver is identical both in UniswapWrapperV3 and in KyberWrapperV3, and thus can be shared in a base class.  code/contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol:L127-L133  function sendLeftOver(address _srcAddr) internal {  msg.sender.transfer(address(this).balance);  if (_srcAddr != KYBER_ETH_ADDRESS) {  IERC20(_srcAddr).safeTransfer(msg.sender, IERC20(_srcAddr).balanceOf(address(this)));  Code duplication : sliceUint function is identical both in DFSExchangeHelper and in DFSPrices  DFSPricesV3.getBestPrice, DFSPricesV3.getExpectedRate should be view functions  Fix the code comments from User borrows tokens to to User borrows tokens from  code/contracts/actions/aave/AaveBorrow.sol:L63-L77  /// @notice User borrows tokens to the Aave protocol  /// @param _market Address provider for specific market  /// @param _tokenAddr The address of the token to be borrowed  /// @param _amount Amount of tokens to be borrowed  /// @param _rateMode Send 1 for stable rate and 2 for variable  /// @param _to The address we are sending the borrowed tokens to  /// @param _onBehalf From what user we are borrow the tokens, defaults to proxy  function _borrow(  address _market,  address _tokenAddr,  uint256 _amount,  uint256 _rateMode,  address _to,  address _onBehalf  ) internal returns (uint256) {  code/contracts/actions/compound/CompBorrow.sol:L51-L59  /// @notice User borrows tokens to the Compound protocol  /// @param _cTokenAddr Address of the cToken we are borrowing  /// @param _amount Amount of tokens to be borrowed  /// @param _to The address we are sending the borrowed tokens to  function _borrow(  address _cTokenAddr,  uint256 _amount,  address _to  ) internal returns (uint256) {  IExchangeV3.sell, IExchangeV3.buy should not be payable  TaskExecutor._executeAction should not forward contract s balance within the IDSProxy.execute call, as the funds are being sent to the same contract.  code/contracts/core/TaskExecutor.sol:L90-L105  function _executeAction(  Task memory _currTask,  uint256 _index,  bytes32[] memory _returnValues  ) internal returns (bytes32 response) {  response = IDSProxy(address(this)).execute{value: address(this).balance}(  registry.getAddr(_currTask.actionIds[_index]),  abi.encodeWithSignature(  \"executeAction(bytes[],bytes[],uint8[],bytes32[])\",  _currTask.callData[_index],  _currTask.subData[_index],  _currTask.paramMapping[_index],  _returnValues  );  Unsafe arithmetic operations  code/contracts/actions/compound/CompClaim.sol:L73  uint256 compClaimed = compBalanceAfter - compBalanceBefore;  code/contracts/actions/compound/CompWithdraw.sol:L84  _amount = tokenBalanceAfter - tokenBalanceBefore;  code/contracts/actions/uniswap/UniSupply.sol:L82-L83  _uniData.tokenA.withdrawTokens(_uniData.to, (_uniData.amountADesired - amountA));  _uniData.tokenB.withdrawTokens(_uniData.to, (_uniData.amountBDesired - amountB));  code/contracts/actions/flashloan/FLAaveV2.sol:L125-L133  IDSProxy(proxy).execute{value: address(this).balance}(  taskExecutor,  abi.encodeWithSelector(CALLBACK_SELECTOR, currTask, bytes32(_amounts[0] + _fees[0]))  );  // return FL  for (uint256 i = 0; i < _assets.length; i++) {  _assets[i].approveToken(address(AAVE_LENDING_POOL), _amounts[i] + _fees[i]);  code/contracts/exchangeV3/DFSExchangeCore.sol:L45  exData.srcAmount -= getFee(  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L48  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "6.3 Gas optimization", "body": "  Description  Use address(this) instead of external call for registry when possible.  Examples  code/contracts/actions/flashloan/FLAaveV2.sol:L82-L102  function _flAaveV2(FLAaveV2Data memory _flData, bytes memory _params) internal returns (uint) {  ILendingPoolV2(AAVE_LENDING_POOL).flashLoan(  payable(registry.getAddr(FL_AAVE_V2_ID)),  _flData.tokens,  _flData.amounts,  _flData.modes,  _flData.onBehalfOf,  _params,  AAVE_REFERRAL_CODE  );  logger.Log(  address(this),  msg.sender,  \"FLAaveV2\",  abi.encode(_flData.tokens, _flData.amounts, _flData.modes, _flData.onBehalfOf)  );  return _flData.amounts[0];  code/contracts/actions/flashloan/dydx/FLDyDx.sol:L76-L107  function _flDyDx(  uint256 _amount,  address _token,  bytes memory _data  ) internal returns (uint256) {  address payable receiver = payable(registry.getAddr(FL_DYDX_ID));  ISoloMargin solo = ISoloMargin(SOLO_MARGIN_ADDRESS);  // Get marketId from token address  uint256 marketId = _getMarketIdFromTokenAddress(SOLO_MARGIN_ADDRESS, _token);  uint256 repayAmount = _getRepaymentAmountInternal(_amount);  IERC20(_token).safeApprove(SOLO_MARGIN_ADDRESS, repayAmount);  Actions.ActionArgs[] memory operations = new Actions.ActionArgs[](3);  operations[0] = _getWithdrawAction(marketId, _amount, receiver);  operations[1] = _getCallAction(_data, receiver);  operations[2] = _getDepositAction(marketId, repayAmount, address(this));  Account.Info[] memory accountInfos = new Account.Info[](1);  accountInfos[0] = _getAccountInfo();  solo.operate(accountInfos, operations);  logger.Log(address(this), msg.sender, \"FLDyDx\", abi.encode(_amount, _token));  return _amount;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "3.1 Memory corruption in Buffer    ", "body": "  Resolution                           Issue has been closed in   ensdomains/buffer#3  Description  Although out of scope for this audit, the audit team noticed a memory corruption issue in the Buffer library. The init function is as follows:  contracts/Buffer.sol:L22-L41  /**  @dev Initializes a buffer with an initial capacity.  @param buf The buffer to initialize.  @param capacity The number of bytes of space to allocate the buffer.  @return The buffer, for chaining.  /  function init(buffer memory buf, uint capacity) internal pure returns(buffer memory) {  if (capacity % 32 != 0) {  capacity += 32 - (capacity % 32);  // Allocate space for the buffer data  buf.capacity = capacity;  assembly {  let ptr := mload(0x40)  mstore(buf, ptr)  mstore(ptr, 0)  mstore(0x40, add(32, add(ptr, capacity)))  return buf;  Note that memory is reserved only for capacity bytes, but the bytes actually requires capacity + 32 bytes to account for the prefixed array length. Other functions in Buffer assume correct allocation and therefore corrupt nearby memory.  Although we didn t immediately spot an ENS exploit for this vulnerability, we consider any memory corruption issue to be important to address.  Example  A simple test shows the memory corruption issue:  contract Test {  using Buffer for Buffer.buffer;  function test() external pure {  Buffer.buffer memory buffer;  buffer.init(1);  // foo immediately follows buffer.buf in memory  bytes memory foo = new bytes(0);  assert(foo.length == 0);  buffer.append(\"A\");  // \"A\" == 65, gets written to the high order byte of foo.length  assert(foo.length == 65 * 256**31);  Remediation  Allocate an additional 32 bytes as follows, to account for storing the uint256 size of the bytes array:  mstore(0x40, add(ptr, add(capacity, 32)))  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.2 SimplePriceOracle.price is susceptible to integer overflow    ", "body": "  Resolution                           Issue has been closed in   ensdomains/ethregistrar#17 by using  Description  SimplePriceOracle.price is as follows:  ethregistrar/contracts/SimplePriceOracle.sol:L26-L28  function price(string calldata /*name*/, uint /*expires*/, uint duration) external view returns(uint) {  return duration * rentPrice;  This is susceptible to a simple overflow attack, e.g. setting the duration to 2**256/rentPrice to give yourself a price of 0.  Severity note: It s unclear whether the SimplePriceOracle is expected to be used in practice, but the severity is set here under the assumption that the code may be used somewhere.  Remediation  Use SafeMath or explicitly check for the overflow.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.3 ETHRegistrarController.register is vulnerable to front running    ", "body": "  Resolution                           Issue has been closed in   ensdomains/ethregistrar#18  Description  commit() and then register() appears to serve the purpose of preventing front running. However, because the commitment is not tied to a specific owner, it serves equally well as a commitment for a front-running attacker.  Example  Alice calls commit(makeCommitment(\"mydomain\", <secret>)).  10 minutes later, Alice submits a transaction to register(\"mydomain\", Alice, ..., <secret>).  Eve observes this transaction in the transaction pool.  Eve submits register(\"mydomain\", Eve, ..., <secret>) with a higher gas price and wins the race.  Remediation  Commitments should commit to owners in addition to names. This way an attacker can t repurpose a previous commitment. (They would have to buy on behalf of the original committer.)  As an alternative, if it s undesirable to pin down owner, the commitment could include msg.sender instead (only allowing the original committer to call register).  E.g. the following (and corresponding changes to callers):  function makeCommitment(  string memory name,  address owner, /* or perhaps committer/sender */  bytes32 secret  pure  public  returns(bytes32)  bytes32 label = keccak256(bytes(name));  return keccak256(abi.encodePacked(label, owner, secret));  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.4 SOA record check on the wrong domain    ", "body": "  Resolution                           During the audit, this issue was discovered by the client development team and already fixed in   ensdomains/root#25.  Description  The SOA record check in Root.getAddress is meant to happen on the root TLD, but in the version of the code audited, it is performed instead on _ens.nic.<tld>.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.5 Work towards a trustless model for ENS   ", "body": "  Resolution  Acknowledged by client team. As stated, this is a long-term issue for which there is no immediate fix, but work is already in progress.  Description  The ENS registry itself is owned by a multisig wallet owned by a number of reputable Ethereum community members. That multisig wallet can do just about anything, up to and including directly taking over any existing or future registered names.  It s important to note that even if we as a community trust the current owners of the multisig wallet, we also need to consider the possibility of their Ethereum private keys being compromised by malicious actors.  Remediation  This centralized control is by design, and the multisig owners have been chosen carefully. However, we do recommend\u2014as is already the plan\u2014that the multisig wallet s power be reduced in future updates to the system. Changes made by that wallet are already quite transparent to the community, but future enhancements might include requiring a waiting period for any changes or disallowing certain types of changes altogether.  In the meantime, wherever possible, the trust model should be made clear so that users understand what guarantees they do and do not have when interacting with ENS.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.6 Consider replacing the Buffer implementation    ", "body": "  Resolution                           There will be no immediate fix for this, but the client team is working on collaborating to get a better audited   Description  The audit team uncovered two bugs in the Buffer library, one each in the only two functions that were looked at. (The library was in general not in scope for this audit.) One bug was a critical memory corruption bug. This calls into question how safe this library is to use in general.  Remediation  Consider using a different library, ideally one that has been fully tested and audited and that minimizes the use of inline assembly, particularly around memory allocation.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.7 Overzealous resizing in Buffer    ", "body": "  Resolution                           Issue has been closed in   ensdomains/buffer#4  Description  In the following code, the buffer is resized even when sufficient capacity is available to perform the write. The buf.buf.length term is unnecessary and leads to unnecessary resizing:  contracts/Buffer.sol:L91-L95  function write(buffer memory buf, uint off, bytes memory data, uint len) internal pure returns(buffer memory) {  require(len <= data.length);  if (off + len > buf.capacity) {  resize(buf, max(buf.capacity, len + off) * 2);  Contrast with the calculation in a similar function:  contracts/Buffer.sol:L206-L209  function write(buffer memory buf, uint off, bytes32 data, uint len) private pure returns(buffer memory) {  if (len + off > buf.capacity) {  resize(buf, (len + off) * 2);  Remediation  Check just the condition if (off + len > buf.capacity) when deciding whether to resize the buffer. This will be a significant gas savings in the common case of reserving exactly the right capacity and then performing two append operations.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.8 Pending auctions in the legacy registrar don t result in proper ownership in ENS    ", "body": "  Resolution                           Addressed in   ensdomains/ethregistrar#23 by reducing the waiting period to 28 days.  Description  If an auction has yet to be finalized in the legacy HashRegistrar at the time that the new, permanent .eth registrar is put in place, the auction winner doesn t get actual ownership of the ENS entry.  The sequence of events would look like:  Auction is started in the HashRegistrar for the name something.eth  The new BaseRegistrarImplementation becomes the owner of the .eth root node in ENS.  The auction is won.  The auction winner calls finalizeAuction, which calls trySetSubnodeOwner, which fails to actually set subnode ownership (as the HashRegistrar no longer has ownership of the .eth root node).  At this point, there s an owner of the deed for the name something.eth in the HashRegistrar, but the ENS subnode is unowned. It can t be transferred to the new registrar for 183 days, and the name can t be registered in the new registrar.  The owner can get themselves out of this situation by calling releaseDeed in the HashRegistrar. If they want to avoid potentially losing their domain in the process, they can transfer the deed to a smart contract which can then release the deed and rent the same name in the new registrar atomically.  Remediation  Here are a few ideas of improvements to help in this situation:  Discourage (or prevent, if possible) new auctions very close to the launch of the new registrar.  Allow domains to be transferred before the 183-day waiting period but require rent payment in those cases. (Perhaps just use the existing grace period to have people renew?)  Document the process for rescuing names that get stuck in this state, or better yet provide a tool for doing so.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.9 BaseRegistrarImplementation.acceptRegistrarTransfer should probably use the live modifier    ", "body": "  Resolution                           Issue has been closed in   ensdomains/ethregistrar#19.  Description  Most external functions in BaseRegistrarImplementation have the live modifier, which ensures that they can only be called on the current ENS owner of the registrar s base address. The acceptRegistrarTransfer function does not have this modifier, which means names can be transferred to the new registrar even if it s not the proper registry owner.  It s hard to think of a real-world example of why this is problematic, especially because the interim registrar appears to protect against this by only transferring to the ens.owner, but it seems safer to include the live modifier unless there s a specific reason not to.  Remediation  Add the live modifier to acceptRegistrarTransfer.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.10 Reconsider use of inline assembly in BytesUtils.sol    ", "body": "  Resolution                           Issue has been closed in   ensdomains/dnssec-oracle#55  Description  Root.sol imports and uses @ensdomains/dnssec-oracle/contracts/BytesUtils.sol for byte operations.  BytesUtils.sol is mainly written in assembly. In general, inline assembly is concerning from a security perspective because it bypasses compiler checks and inhibits human code reasoning.  e.g.readUint8():  function readUint8(bytes memory self, uint idx) internal pure returns (uint8 ret) {  require(idx + 1 <= self.length);  assembly {  ret := and(mload(add(add(self, 1), idx)), 0xFF)  Remediation  Some of the functions in BytesUtil.sol can be written in Solidity without affecting the gas costs.  readUint8() can be written as following Solidity code which functions the same:  function readUint8(bytes memory self, uint idx) internal pure returns (uint8 ret) {  return uint8(self[idx]);  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.11 BaseRegistrarImplementation.acceptRegistrarTransfer does not check for invalid names    ", "body": "  Resolution  Short names will be manually canceled in the old registrar during the migration period. Note that this is still feasible with the reduced 28-day lock-up period.  Description  BaseRegistrarImplementation.acceptRegistrarTransfer does not explicitly check for invalid names.  In the old registrar it is possible to register domain names with length less than 7 characters. However anyone can call HashRegistrar.invalidateName() to invalidate the registration and get half of the deed amount as an incentive.  Assume that an invalid domain is registered in the old registrar and no one invalidates the registration (within the 183 days between the registrationDate and the transfer ETHRegistrarController.acceptRegistrarTransfer), it is possible to transfer the invalid domain to the new ENS registrar.  Remediation  Given that it is easy to check for invalid domains using a rainbow table for all possible <7 character domains, anyone can invalidate them before the new registrar goes live. Note that for the auctions starting right before the new registrar goes live, there will be a 183 days window in which anyone can call HashRegistrar.invalidateName() to invalidate the domain names.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.12 Sanity check around transferPeriodEnds    ", "body": "  Resolution                           Issue has been closed in   ensdomains/ethregistrar#23  Description  BaseRegistrarImplementation.acceptRegistrarTransfer has a hardcoded limit such that only domains registered 183 days ago can be transferred in.  This imposes an implicit constraint on the transferPeriodEnds state variable. If the transfer period ends too soon after the new registrar is put in place, names that were just registered won t be transferrable during the transfer period (and will thus become available to be rented by another user).  Remediation  A sanity check in the constructor would help here, e.g.:  require(_transferPeriodEnds > now + 183 days);  Note that the true requirement is something more like  The time between when this registrar becomes the ENS node owner of the .eth domain and the time of transferPeriodEnds must be at least 183 days plus a sufficient time window for late registrants to have a chance to perform the transfer.  But it s hard to see a way to encode this precisely. A broad sanity check will at least avoid simple timing mistakes.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.13 StablePriceOracle.price has an unimportant integer underflow    ", "body": "  Resolution                           Issue has been closed in   ensdomains/ethregistrar#20  Description  ethregistrar/contracts/StablePriceOracle.sol:L57-L63  function price(string calldata name, uint /*expires*/, uint duration) view external returns(uint) {  uint len = name.strlen();  require(len > 0);  if(len > rentPrices.length) {  len = rentPrices.length;  uint priceUSD = rentPrices[len - 1].mul(duration);  If the length of the rentPrices array is 0, then the last line above attempts to access rentPrices[2**256-1]. This will assert, but it might be more friendly (from a gas perspective) to revert in this case.  Remediation  A simple fix would be to move the require(len > 0) down until just before the array access.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.14 ETHRegistrarController.register should  revert rather than silently fail    ", "body": "  Resolution                           Issue has been closed in   ensdomains/ethregistrar#22  Description  When called with an invalid commitment or unavailable domain, ETHRegistrarController.register refunds the sent ether and silently fails rather than reverting:  ethregistrar/contracts/ETHRegistrarController.sol:L56-L64  function register(string calldata name, address owner, uint duration, bytes32 secret) external payable {  // Require a valid commitment  bytes32 commitment = makeCommitment(name, secret);  require(commitments[commitment] + MIN_COMMITMENT_AGE <= now);  // If the commitment is too old, or the name is registered, stop  if(commitments[commitment] + MAX_COMMITMENT_AGE < now || !available(name))  {  msg.sender.transfer(msg.value);  return;  register also has no return value, so it s difficult for a caller to know whether the register action succeeded or failed.  Remediation  It s probably better to use require(...) to handle these invalid cases. This is roughly equivalent because no state changes have been made before this early return, but it seems less error prone and clearer to callers about what happened.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "3.15 StringUtils.strlen could be rewritten without assembly    ", "body": "  Resolution                           Issue has been closed in   ensdomains/ethregistrar#21  Description  StringUtils.strlen uses inline assembly to walk through a UTF-8 string and count its character length. In general, inline assembly is concerning from a security perspective because it bypasses compiler checks and inhibits human code reasoning.  Remediation  Consider rewriting in Solidity, something similar to the following:  function strlen(string memory s) internal pure returns (uint256) {  uint256 i = 0;  uint256 len;  for (len = 0; i < bytes(s).length; len++) {  byte b = bytes(s)[i];  if (b < 0x80) {  i += 1;  } else if (b < 0xE0) {  i += 2;  ...  return len;  4 Threat Model  The creation of a threat model is beneficial when building smart contract systems as it helps to understand the potential security threats, assess risk, and identify appropriate mitigation strategies. This is especially useful during the design and development of a contract system as it allows to create a more resilient design which is more difficult to change post-development.  A threat model was created during the audit process in order to analyze the attack surface of the contract system and to focus review and testing efforts on key areas that a malicious actor would likely also attack. It consists of two parts: a high-level analysis that help to understand the attack surface and a list of threats that exist for the contract system.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "4.1 Overview", "body": "  The following assets are managed by contracts and likely targets for an attacker:  Registered domain names (e.g. foo.eth)  Ether, in the form of rent paid to the ETHRegistrarController  The following actors have access to the system to perform an attack:  System owners (ENS itself, registrars, controllers, price oracles)  DNS domain/subdomain owners, who can update DNSSEC records  Users who are registering, renewing, and transferring domains  The following describes the surface area available to attackers:  DNSSEC records  Registrars and controllers  Root contract  Ethereum private keys  Because they were out of scope for this audit, we did not consider some interesting targets such as the DNSSEC oracle, DNSSEC-based registrar, the interim .eth registrar, or the multisig wallet used for ENS ownership.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "4.2 Threat Analysis", "body": "  The following table contains a list of identified threats, along with their mitigations:  user may try to register/renew a domain for less than the expected price  overflow on the rent price / manipulate the price oracle  SafeMath mitigates some potential math errors  user may try to mount denial-of-service attacks on other users (e.g. censor their purchases/renewals)  network DoS  long purchase windows and grace periods  user may try to snipe a domain  front-running  a commit/reveal scheme attempts to prevent this but is ineffective (see section 3), a generous grace period prevents race conditions on expiration  user may try to register .eth TLD  update DNSSEC records  Root disallows changes to that node  Root owner may steal domains, manipulate prices, etc.  ENS root swaps the controller/registrar with malicious code  such manipulation would be transparent today, and future updates may limit the root owners  powers  domain owners may take over already-owned subdomains  change DNSSEC to replace registrar for a domain  this is allowed by design  5 Tool-based analysis  The issues from the tool based analysis have been reviewed and the relevant issues have been listed in chapter 3 - Issues.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Where possible, we ran the full MythX analysis. MythX is still in beta, and where analysis failed, we fell back to running Mythril Classic, a large subset of the functionality of MythX.  Below is the raw output of MythX and Mythril Classic vulnerability scans:  In order to run MythX, Root.sol contract was flattened. flat_root.sol line numbers reflect on the output of truffle-flattener contracts/Root.sol.  Title: Floating Pragma  Head: A floating pragma is set.  Description: It is recommended to make a conscious choice on what version of Solidity is used for compilation. Currently any version equal or greater than \"0.4.24\" is allowed.  Source code:  flat_root.sol 1:0  --------------------------------------------------  pragma solidity ^0.4.24;  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: State variable shadows another state variable.  Description: The state variable \"CLASS_INET\" in contract \"DNSClaimChecker\" shadows another state variable with the same name \"CLASS_INET\" in contract \"Root\".  Source code:  flat_root.sol 869:4  --------------------------------------------------  uint16 constant CLASS_INET = 1  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: State variable shadows another state variable.  Description: The state variable \"TYPE_TXT\" in contract \"DNSClaimChecker\" shadows another state variable with the same name \"TYPE_TXT\" in contract \"Root\".  Source code:  flat_root.sol 870:4  --------------------------------------------------  uint16 constant TYPE_TXT = 16  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"owner\" in contract \"ENS\" shadows the state variable with the same name \"owner\" in contract \"Ownable\".  Source code:  flat_root.sol 20:58  --------------------------------------------------  address owner  --------------------------------------------------  flat_root.sol 22:36  --------------------------------------------------  address owner  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"owner\" in contract \"Root\" shadows the state variable with the same name \"owner\" in contract \"Ownable\".  Source code:  flat_root.sol 1012:44  --------------------------------------------------  address owner  --------------------------------------------------  flat_root.sol 1037:36  --------------------------------------------------  address owner  --------------------------------------------------  ==================================================  Title: Shadowing State Variables  Head: Local variable shadows a state variable.  Description: The local variable \"oracle\" in contract \"DNSClaimChecker\" shadows the state variable with the same name \"oracle\" in contract \"Root\".  Source code:  flat_root.sol 881:29  --------------------------------------------------  DNSSEC oracle  --------------------------------------------------  ==================================================  BaseRegistrarImplementation  Mythril Classic results for BaseRegistrarImplementation are as follows. flat_BaseRegistrarImplementation.sol line numbers reflect on the output of truffle-flattener contracts/BaseRegistrarImplementation.sol  ETHRegistrarController  Mythril Classic results for ETHRegistrarController are as follows. flat_ETHRegistrarController.sol line numbers reflect on the output of truffle-flattener contracts/ETHRegistrarController.sol.  ==== Multiple Calls in a Single Transaction ====  SWC ID: 113  Severity: Medium  Contract: ETHRegistrarController  Function name: rentPrice(string,uint256)  PC address: 996  Estimated Gas Usage: 5179 - 79151  Multiple sends are executed in one transaction.  Consecutive calls are executed at the following bytecode offsets:  Offset: 2947  Offset: 3202  Try to isolate each external call into its own transaction, as external calls can fail accidentally or deliberately.  --------------------  In file: flat_ETHRegistrarController.sol:1467  function rentPrice(string memory name, uint duration) view public returns(uint) {  bytes32 hash = keccak256(bytes(name));  return prices.price(name, base.nameExpires(uint256(hash)), duration);  --------------------  ==== Dependence on predictable environment variable ====  SWC ID: 116  Severity: Low  Contract: ETHRegistrarController  Function name: register(string,address,uint256,bytes32)  PC address: 3552  Estimated Gas Usage: 2056 - 6247  Sending of Ether depends on a predictable variable.  The contract sends Ether depending on the values of the following variables:  block.timestamp  block.timestamp  block.timestamp  Note that the values of variables like coinbase, gaslimit, block number and timestamp are predictable and/or can be manipulated by a malicious miner. Don't use them for random number generation or to make critical decisions.  --------------------  In file: flat_ETHRegistrarController.sol:1498  msg.sender.transfer(msg.value)  --------------------  ==== Integer Overflow ====  SWC ID: 101  Severity: High  Contract: ETHRegistrarController  Function name: register(string,address,uint256,bytes32)  PC address: 5332  Estimated Gas Usage: 2333 - 9254  The binary addition can overflow.  The operands of the addition operation are not sufficiently constrained. The addition could therefore result in an integer overflow. Prevent the overflow by checking inputs or ensure sure that the overflow is caught by an assertion.  --------------------  In file: flat_ETHRegistrarController.sol:1411  add(mload(s), ptr)  --------------------  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  ethregistrar  contracts/BaseRegistrarImplementation.sol  36:36     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  60:42     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  65:15     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  73:16     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  73:48     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  75:23     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  83:39     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  85:15     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  89:47     warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  114:37    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  118:35    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/ETHRegistrarController.sol  53:63    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  54:34    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  61:64    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  64:58    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/StringUtils.sol  15:8     error    Avoid using Inline Assembly.    security/no-inline-assembly  22:12    error    Avoid using Inline Assembly.    security/no-inline-assembly  \u2716 2 errors, 15 warnings found.  root  No issues found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  root/contracts/Migrations.sol  eac3bb098bace681296263c037b30123fd46e01a  root/contracts/Ownable.sol  b596da7ad9b5c92a119268e05a5f2190659de8d3  root/contracts/Root.sol  94c5fd45635c6d78cae15903bdf565e2c587bdfa  ethregistrar/contracts/BaseRegistrar.sol  dfadfc8a35024069ff66cbc4a82b67dc48129eab  ethregistrar/contracts/BaseRegistrarImplementation.sol  a1e04ce66a9588063155591b59cd695d2d35cabe  ethregistrar/contracts/DummyOracle.sol  e1dab33211d55e02874ae2510e5e773e13056939  ethregistrar/contracts/ETHRegistrarController.sol  7cb180a1d5102efd2acc04b0b518848b6127846e  ethregistrar/contracts/Migrations.sol  b6732a145e4cb6841945488f591b1cf383a6441e  ethregistrar/contracts/PriceOracle.sol  3257acda730f294f19984163f9fe4a19eabdef4d  ethregistrar/contracts/SafeMath.sol  5effc6db2209b2bf2d49abe4ad1ac247e106f8d9  ethregistrar/contracts/SimplePriceOracle.sol  fc11bff8c93e8471b8d8478f1a14b7f43fff2eef  ethregistrar/contracts/StablePriceOracle.sol  892333542a757ba6089c5c3d19d00b337cb0da78  ethregistrar/contracts/StringUtils.sol  4d784bb26b409cfd8ed841f43c4e0ffbfddc450b  ethregistrar/contracts/_TestDeps.sol  2077d541fedbd889d2f814c5c51aa046078f566d  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  Migrations  Implementation  <Constructor>  Public    setCompleted  Public    restricted  upgrade  Public    restricted  Ownable  Implementation  <Constructor>  Public    transferOwnership  Public    onlyOwner  isOwner  Public    NO   Root  Implementation  Ownable  <Constructor>  Public    proveAndRegisterTLD  External    NO   setSubnodeOwner  External    onlyOwner  setRegistrar  External    onlyOwner  registerTLD  Public    NO   setResolver  Public    onlyOwner  setOwner  Public    onlyOwner  setTTL  Public    onlyOwner  getLabel  Internal \ud83d\udd12  getAddress  Internal \ud83d\udd12  getSOAHash  Internal \ud83d\udd12  BaseRegistrar  Implementation  ERC721, Ownable  addController  External    NO   removeController  External    NO   nameExpires  External    NO   available  Public    NO   register  External    NO   renew  External    NO   reclaim  External    NO   acceptRegistrarTransfer  External    NO   BaseRegistrarImplementation  Implementation  BaseRegistrar  <Constructor>  Public    ownerOf  Public    NO   addController  External    onlyOwner  removeController  External    onlyOwner  nameExpires  External    NO   available  Public    NO   register  External    live onlyController  renew  External    live onlyController  reclaim  External    live  acceptRegistrarTransfer  External    NO   DummyOracle  Implementation  <Constructor>  Public    set  Public    NO   read  External    NO   ETHRegistrarController  Implementation  Ownable  <Constructor>  Public    rentPrice  Public    NO   valid  Public    NO   available  Public    NO   makeCommitment  Public    NO   commit  Public    NO   register  External    NO   renew  External    NO   setPriceOracle  Public    onlyOwner  withdraw  Public    onlyOwner  Migrations  Implementation  <Constructor>  Public    setCompleted  Public    restricted  upgrade  Public    restricted  PriceOracle  Interface  price  External    NO   SafeMath  Library  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  add  Internal \ud83d\udd12  mod  Internal \ud83d\udd12  SimplePriceOracle  Implementation  Ownable, PriceOracle  <Constructor>  Public    setPrice  Public    onlyOwner  price  External    NO   DSValue  Interface  read  External    NO   StablePriceOracle  Implementation  Ownable, PriceOracle  <Constructor>  Public    setOracle  Public    onlyOwner  setPrices  Public    onlyOwner  price  External    NO   StringUtils  Library  strlen  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  Root Control Flow  6 Test Coverage Measurement  Testing is implemented using Truffle. 12 tests are included for the Root contract, and they all pass. 30 tests are included for the .eth permanent registrar, and they all pass.  We were unable to obtain code coverage numbers for the tests, but the audit team s overall impression is that testing covers a high percentage of code branches. That said, the testing is weak, in particular regarding negative test cases and edge cases. As a specific example, changing the following in ETHRegistrarController.renew causes no test failures, which shows a serious lack of coverage:  // OLD: require(msg.value >= cost);  // NEW:  require(msg.value > 0);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/03/ens-permanent-registrar/"}, {"title": "5.1 uint overflow may lead to stealing funds    Addressed", "body": "  Resolution  safeMath was added in SKALE-215. At the time of the writing this comment, the review has not been comprehensive to all arithmetic calculations in the scope.  Note that in some cases usage of safeMath due to reverts can result in unexpected halting of the system, that too should be reviewed again.  Description  It s possible to create a delegation with a very huge amount which may result in a lot of critically bad malicious usages:  code/contracts/delegation/DelegationRequestManager.sol:L74-L76  uint holderBalance = SkaleToken(contractManager.getContract(\"SkaleToken\")).balanceOf(holder);  uint lockedToDelegate = tokenState.getLockedCount(holder) - tokenState.getPurchasedAmount(holder);  require(holderBalance >= amount + lockedToDelegate, \"Delegator hasn't enough tokens to delegate\");  amount is passed by a user as a parameter, so if it s close to uint max value, amount + lockedToDelegate would overflow and this requirement would pass.  Having delegation with an almost infinite amount of tokens can lead to many various attacks on the system up to stealing funds and breaking everything.  Recommendation  Using SafeMath everywhere should prevent this and other similar issues. There should be more critical attacks caused by overflows/underflows, so SafeMath should be used everywhere in the codebase.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.2 Holders can burn locked funds    Addressed", "body": "  Resolution                           Fixed in   SKALE-2144 by adding proper checks in  Description  Skale token is a modified ERC-777 that allows locking some part of the balance. Locking is checked during every transfer:  code/contracts/ERC777/LockableERC777.sol:L433-L441  // Property of the company SKALE Labs inc.---------------------------------  uint locked = _getLockedOf(from);  if (locked > 0) {  require(_balances[from] >= locked + amount, \"Token should be unlocked for transferring\");  //-------------------------------------------------------------------------  _balances[from] = _balances[from].sub(amount);  _balances[to] = _balances[to].add(amount);  But it s not checked during burn function and it s possible to  burn  locked tokens. Tokens will be burned, but locked amount will remain the same. That will result in having more locked tokens than the balance which may have very unpredictable behaviour.  Recommendation  Allow burning only unlocked tokens.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.3 Node can unlink validator    Addressed", "body": "  Resolution                           Fixed in   SKALE-2145-unlink-node by adding a check in  Description  Validators can link a node address to them by calling linkNodeAddress function:  code/contracts/delegation/ValidatorService.sol:L109-L119  function linkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {  uint validatorId = getValidatorId(validatorAddress);  require(_validatorAddressToId[nodeAddress] == 0, \"Validator cannot override node address\");  _validatorAddressToId[nodeAddress] = validatorId;  function unlinkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {  uint validatorId = getValidatorId(validatorAddress);  require(_validatorAddressToId[nodeAddress] == validatorId, \"Validator hasn't permissions to unlink node\");  _validatorAddressToId[nodeAddress] = 0;  After that, the node has the same rights and is almost indistinguishable from the validator. So the node can even remove validator s address from _validatorAddressToId list and take over full control over validator. Additionally, the node can even remove itself by calling unlinkNodeAddress, leaving validator with no control at all forever.  Also, even without nodes, a validator can initially call unlinkNodeAddress to remove itself.  Recommendation  Linked nodes (and validator) should not be able to unlink validator s address from the _validatorAddressToId mapping.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.4 Unlocking funds after slashing    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  The initial funds can be unlocked if 51+% of them are delegated. However if any portion of the funds are slashed, the rest of the funds will not be unlocked at the end of the delegation period.  code/contracts/delegation/TokenState.sol:L258-L263  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Consider slashed tokens as delegated, or include them in the calculation for process to unlock in endingDelegatedToUnlocked  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.5 Bounties and fees should only be locked for the first 3 months    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  Bounties are currently locked for the first 3 months after delegation:  code/contracts/delegation/DelegationService.sol:L315  skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));  Instead, they should be locked for the first 3 months after the token launch.  Recommendation  It s better just to forbid any withdrawals for the first 3 months, no need to track it separately for every delegation. This recommendation is mainly to simplify the process.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.6 getLockedCount is iterating over all history of delegations    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  getLockedCount is iterating over all delegations of a specific holder and may even change the state of these delegations by calling getState.  code/contracts/delegation/TokenState.sol:L60-L71  function getLockedCount(address holder) external returns (uint amount) {  amount = 0;  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  uint[] memory delegationIds = delegationController.getDelegationsByHolder(holder);  for (uint i = 0; i < delegationIds.length; ++i) {  uint id = delegationIds[i];  if (isLocked(getState(id))) {  amount += delegationController.getDelegation(id).amount;  return amount + getPurchasedAmount(holder) + this.getSlashedAmount(holder);  This problem is major because delegations number is growing over time and may even potentially grow more than the gas limit and lock all tokens forever. getLockedCount is called during every transfer which makes any token transfer much more expensive than it should be.  Recommendation  Remove iterations over a potentially unlimited amount of tokens. All the necessary data can be precalculated before and getLockedCount function can have O(1) complexity.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.7 Tokens are unlocked only when delegation ends    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  After the first 3 months since at least 50% of tokens are delegated, all tokens should be unlocked. In practice, they are only unlocked if at least 50% of tokens, that were bought on the initial launch, are undelegated.  code/contracts/delegation/TokenState.sol:L258-L264  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Implement lock mechanism according to the legal requirement.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.8 Tokens after delegation should not be unlocked automatically    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  When some amount of tokens are delegated to a validator when the delegation period ends, these tokens are unlocked. However these tokens should be added to _purchased as they were in that state before their delegation.  code/contracts/delegation/TokenState.sol:L258-L264  if (_isPurchased[delegationId]) {  address holder = delegation.holder;  _totalDelegated[holder] += delegation.amount;  if (_totalDelegated[holder] >= _purchased[holder]) {  purchasedToUnlocked(holder);  Recommendation  Tokens should only be unlocked if the main legal requirement (_totalDelegated[holder] >= _purchased[holder]) is satisfied, which in the above case this has not happened.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.9 Some unlocked tokens can become locked after delegation is rejected    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  When some amount of tokens are requested to be delegated to a validator, the validator can reject the request. The previous status of these tokens should be intact and not changed (locked or unlocked).  Here the initial status of tokens gets stored and it s either completely locked or unlocked:  code/contracts/delegation/TokenState.sol:L205-L214  if (_purchased[delegation.holder] > 0) {  _isPurchased[delegationId] = true;  if (_purchased[delegation.holder] > delegation.amount) {  _purchased[delegation.holder] -= delegation.amount;  } else {  _purchased[delegation.holder] = 0;  } else {  _isPurchased[delegationId] = false;  The problem is that if some amount of these tokens are locked at the time of the request and the rest tokens are unlocked, they will all be considered as locked after the delegation was rejected.  code/contracts/delegation/TokenState.sol:L272-L278  function _cancel(uint delegationId, DelegationController.Delegation memory delegation) internal returns (State state) {  if (_isPurchased[delegationId]) {  state = purchasedProposedToPurchased(delegationId, delegation);  } else {  state = proposedToUnlocked(delegationId);  Recommendation  Don t change the status of the rejected tokens.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.10 Gas limit for bounty and slashing distribution    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  After every bounty payment (should be once per month) to a validator, the bounty is distributed to all delegators. In order to do that, there is a for loop that iterates over all active delegators and sends their bounty to SkaleBalances contract:  code/contracts/delegation/DelegationService.sol:L310-L316  for (uint i = 0; i < shares.length; ++i) {  skaleToken.send(address(skaleBalances), shares[i].amount, abi.encode(shares[i].holder));  uint created = delegationController.getDelegation(shares[i].delegationId).created;  uint delegationStarted = timeHelpers.getNextMonthStartFromDate(created);  skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));  There are also few more loops over all the active delegators. This leads to a huge gas cost of distribution mechanism. A number of active delegators that can be processed before hitting the gas limit is limited and not big enough.  The same issue is with slashing:  code/contracts/delegation/DelegationService.sol:L95-L106  function slash(uint validatorId, uint amount) external allow(\"SkaleDKG\") {  ValidatorService validatorService = ValidatorService(contractManager.getContract(\"ValidatorService\"));  require(validatorService.validatorExists(validatorId), \"Validator does not exist\");  Distributor distributor = Distributor(contractManager.getContract(\"Distributor\"));  TokenState tokenState = TokenState(contractManager.getContract(\"TokenState\"));  Distributor.Share[] memory shares = distributor.distributePenalties(validatorId, amount);  for (uint i = 0; i < shares.length; ++i) {  tokenState.slash(shares[i].delegationId, shares[i].amount);  Recommendation  The best solution would require major changes to the codebase, but would eventually make it simpler and safer. Instead of distributing and centrally calculating bounty for each delegator during one call it s better to just store all the necessary values, so delegator would be able to calculate the bounty on withdrawal. Amongst the necessary values, there should be history of total delegated amounts per validator during each bounty payment and history of all delegations with durations of their active state.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.11 ERC-777 callback issue    Partially fixed", "body": "  Resolution  SKALE-2153 to  skalenetwork/skale-manager#128  This report raises this as an unfixed minor issue. This issue will be fixed if the upgrade capability for _getAndUpdateLockedAmount() is revoked by SKALE network governance in the future.  Description  ERC-777 token comes with callback functions to the receiver and the sender on every token transfer. This gives re-entrancy opportunities for everyone who s using this token. There is a chance that other systems might not handle ERC-777 correctly.  Examples  Uniswap reentrancy critical bug: https://medium.com/consensys-diligence/uniswap-audit-b90335ac007  Recommendation  Use ERC-20 standard or remove callback function calls.  Remove callback function usage from the system and replace them with a standard ERC-20 flow:  code/contracts/delegation/SkaleBalances.sol:L55-L68  function tokensReceived(  address operator,  address from,  address to,  uint256 amount,  bytes calldata userData,  bytes calldata operatorData  external  allow(\"SkaleToken\")  address recipient = abi.decode(userData, (address));  stashBalance(recipient, amount);  code/contracts/delegation/DelegationService.sol:L275-L289  function tokensReceived(  address operator,  address from,  address to,  uint256 amount,  bytes calldata userData,  bytes calldata operatorData  external  allow(\"SkaleToken\")  require(userData.length == 32, \"Data length is incorrect\");  uint validatorId = abi.decode(userData, (uint));  distributeBounty(amount, validatorId);  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.12 Rename functions    Addressed", "body": "  Resolution                           Fixed in   SKALE-2154-naming by renaming the functions. The functions that are not solely getters and update the state of the smart contract are renamed to have  Description  The naming of the functions should reflect their nature, such as functions starting with  get  should be only getters and do not change state. This will result in confusion developments and the implicit state changes might not be noticed.  Other than getters, some other function or variable names are misleading.  Examples  The following functions are a few examples that are named as getters but they change the state.  getState -> updateState  getDelegationsTotal getDelegationsForValidator getDelegationsByHolder  Some other naming that does not reflect the nature of the functionality:  getPurchasedAmount -> getPurchasedUnlocked  tokenState.Sold -> lock  Recommendation  For functions that get and update variables use getAndUpdate naming. Similarly use variable names that reflect the nature of the values they store.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.13 Delegations might stuck in non-active validator   Pending", "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions.  Description  If a validator does not get enough funds to run a node (MSR - Minimum staking requirement), all token holders that delegated tokens to the validator cannot switch to a different validator, and might result in funds getting stuck with the nonfunctioning validator for up to 12 months.  Example  code/contracts/delegation/ValidatorService.sol:L166  require((validatorNodes.length + 1) * msr <= delegationsTotal, \"Validator has to meet Minimum Staking Requirement\");  Recommendation  Allow token holders to withdraw delegation earlier if the validator didn t get enough funds for running nodes.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.14 Disabled Validators still have delegated funds   Pending", "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions.  Description  The owner of ValidatorService contract can enable and disable validators. The issue is that when a validator is disabled, it still has its delegations, and delegated funds will be locked until the end of their delegation period (up to 12 months).  code/contracts/delegation/ValidatorService.sol:L84-L90  function enableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {  trustedValidators[validatorId] = true;  function disableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {  trustedValidators[validatorId] = false;  Recommendation  It might make sense to release all delegations and stop validator s nodes if it s not trusted anymore. However, the rationale behind disabling the validators might be different that what we think, in any case there should be a way to handle this scenario, where the validator is disabled but there are funds delegated to it.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.15 Fees can be > 100%    Addressed", "body": "  Resolution                           Added a check to prevent fee rates equal or higher than 100% in   SKALE-2157-fee-check.  Description  A validator can be created with feeRate > 1000 which would mean that the fee rate would be higher than 100%. Severity is not high because that validator will most likely be not whitelisted.  Also, 100%+ fees would still somehow work and not revert because of the absence of SafeMath.  Recommendation  Add sanity check for the input values in registerValidator, and do not allow adding a validator with a fee rate higher than 100%.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.16 getState changes state implicitly    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  getState function is checking and changing the state of a delegation struct. This function is called in many places in the codebase. Every delegation has a lot of different possible states and all of them are changed implicitly during other transactions, which makes it hard to track the logic in the code and make future changes in the code close to impossible without breaking some functionalities.  Recommendation  The general suggestion would be to minimize the number of implicit storage changes. Many states can be either changed explicitly or be calculated without additional storage changes.  As an option, it s possible to get rid of state storage slot at all. startDate and endDate fields may set the current state:  initProposed can be called during the creation of the proposal.  no need to explicitly change states between ACCEPTED and DELEGATED, you can set the start date on acceptance and no further changes are required.  no need to switch states between DELEGATED and ENDING_DELEGATED, when delegation is set to end, it s fine to just have end_date storage slot and make assign the date there when undelegate function is called.  unlocking funds from delegation (or not accepted request) can be explicit.  Also see issue 5.19 for other suggestions regarding getState usage in the code  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.17 _endingDelegations list is redundant    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  _endingDelegations is a list of delegations that is created for optimisation purposes. But the only place it s used is in getPurchasedAmount function, so only a subset of all delegations is going to be updated.  code/contracts/delegation/TokenState.sol:L159-L164  function getPurchasedAmount(address holder) public returns (uint amount) {  // check if any delegation was ended  for (uint i = 0; i < _endingDelegations[holder].length; ++i) {  getState(_endingDelegations[holder][i]);  return _purchased[holder];  But getPurchasedAmount function is mostly used after iterating over all delegations of the holder.  Recommendation  Remove _endingDelegations and switch to a mechanism that does not require looping through delegations list of potentially unlimited size.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.18 Some functions are defined but not implemented    Addressed", "body": "  Resolution                           Fixed by removing the empty functions and implementing some others in   SKALE-2160. At the time of the writing this comment, the review has not been comprehensive to all functions in the scope.  Description  There are many functions that are defined but not implemented. They have a revert with a message as not implemented.  This results in complex code and reduces readability. Here is a some of these functions within the scope of this audit:  DelegationService.setMinimumStakingRequirement()  DelegationService.getAllDelegationRequests()  DelegationService.getDelegationRequestsForValidator()  DelegationService.listDelegationRequests()  DelegationService.getDelegationRequestsForValidator() Many more functions in DelegationService.sol  Examples  code/contracts/delegation/DelegationService.sol:L152-L158  function getAllDelegationRequests() external returns(uint[] memory) {  revert(\"Not implemented\");  function getDelegationRequestsForValidator(uint validatorId) external returns (uint[] memory) {  revert(\"Not implemented\");  Recommendation  If these functions are needed for this release, they must be implemented. If they are for future plan, it s better to remove the extra code in the smart contracts.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.19 tokenState.setState redundant checks    Addressed", "body": "  Resolution                           Issue is fixed as a part of the major code changes in   skalenetwork/skale-manager#92  Description  tokenState.setState is used to change the state of the token from:  PROPOSED to ACCEPTED (in accept())  DELEGATED to ENDING_DELEGATED (in requestUndelegation()  The if/else statement in setState is too complicated and can be simplified, both to optimize gas usage and to increase readability.  Examples  code/contracts/delegation/TokenState.sol:L173-L197  function setState(uint delegationId, State newState) internal {  TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  require(newState != State.PROPOSED, \"Can't set state to proposed\");  if (newState == State.ACCEPTED) {  State currentState = getState(delegationId);  require(currentState == State.PROPOSED, \"Can't set state to accepted\");  _state[delegationId] = State.ACCEPTED;  _timelimit[delegationId] = timeHelpers.getNextMonthStart();  } else if (newState == State.DELEGATED) {  revert(\"Can't set state to delegated\");  } else if (newState == State.ENDING_DELEGATED) {  require(getState(delegationId) == State.DELEGATED, \"Can't set state to ending delegated\");  DelegationController.Delegation memory delegation = delegationController.getDelegation(delegationId);  _state[delegationId] = State.ENDING_DELEGATED;  _timelimit[delegationId] = timeHelpers.calculateDelegationEndTime(delegation.created, delegation.delegationPeriod, 3);  _endingDelegations[delegation.holder].push(delegationId);  } else {  revert(\"Unknown state\");  Recommendation  Some of the changes that do not change the functionality of the setState function:  Remove reverts() and add the valid states to the require() at the beginning of the function  Remove multiple calls to getState()  Remove final else/revert as this is an internal function and States passed should be valid More optimization can be done which requires further understanding of the system and the state machine.  function setState(uint delegationId, State newState) internal {  TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));  require(newState != State.PROPOSED || newState != State.DELEGATED, \"Invalid state change\");  State currentState = getState(delegationId);  if (newState == State.ACCEPTED) {  require(currentState == State.PROPOSED, \"Can't set state to accepted\");  _state[delegationId] = State.ACCEPTED;  _timelimit[delegationId] = timeHelpers.getNextMonthStart();  } else if (newState == State.ENDING_DELEGATED) {  require(currentState == State.DELEGATED, \"Can't set state to ending delegated\");  DelegationController.Delegation memory delegation = delegationController.getDelegation(delegationId);  _state[delegationId] = State.ENDING_DELEGATED;  _timelimit[delegationId] = timeHelpers.calculateDelegationEndTime(delegation.created, delegation.delegationPeriod, 3);  _endingDelegations[delegation.holder].push(delegationId);  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.20 Validator should be able to remove delegator    Addressed", "body": "  Resolution                           Code added in   SKALE-2162, If the delegation is not in  Description  In order to delegate tokens to a validator, the validator should accept the delegation request, however it s not possible to remove the delegator for the next period.  Recommendation  For consistency, either allow a validator to undelegate delegators for the next period or remove acceptance mechanism if it s not needed.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.21 Lack of logs and events on state changes   Pending", "body": "  Resolution  Skale team acknowledged this issue and will address this in future versions, but given the minor level and need to begin remediation, it s left out of scope from the re-mediation tag.  Description  Events in Solidity are used to log major state changes in the system, as for tracebility and also trigger UI changes or user notifications. It is a good practice to use events for every value storage change to be able to trace back the system.  Recommendation  emit events whenever a state change happens. As an example slashing does not emit any events and cannot notify a user unless a service is polling the system state regularly.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.22 DelegationService redundancy    Addressed", "body": "  Resolution  pull/114 and the functionality is distributed in  Description  DelegationService acts as a gateway for every external call. The problem is that it adds extra complexity to the code, which makes it harder to read and add a new code. Also, it costs more gas because of extra calls between contracts.  Recommendation  The same functionality of DelegationService can be added through UI to allow direct calls to each contract. However, as the whole system is modular and upgradable, it is understandable why using one main contract as the point of interaction might make sense as well.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "5.23 Add timelock for some onlyOwner functions   Pending", "body": "  Resolution  Skale team acknowledged this issue and gave us the following response:  The SKALE Network Upgrade key will soon transition to an on-chain voting mechanism therefore making the ownership a function of community governance. It will be centrally managed through a multi-sig process for the initial 3 months to prioritize agility for resolving critical issues prior to becoming a community owned on-chain function. Successful Ethereum projects such as Maker have given clear data points on successful voting mechanism and community control which the SKALE Network will employ as soon as possible.  Description  The system is trusted in a way that there are some owners have the power to do major changes in the system. The most powerful is owner of ContractManager which can update any contract in any way. Even though the system is trusted and this is intended behaviour, it s possible to mitigate this trust a bit.  Recommendation  Add timelock to major admin functions, so people would know about it beforehand (2 weeks before) and would be able to react somehow.  Severity is minor because if owners of SKALE would want to attack the system in that way, tokens would lose the value anyway, and security of SKALE chains would be unreliable. So it s unclear what can be done even having that knowledge beforehand.  6 Mitigation issues  This section lists the issues found in the mitigation phase. The audit team, reviewed the code fixes after the initial report was delivered,  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.1 Users can burn delegated tokens using re-entrancy attack    Addressed", "body": "  Resolution                           Mitigated in   skalenetwork/skale-manager#128  Description  When a user burns tokens, the following code is called:  new_code/contracts/ERC777/LockableERC777.sol:L413-L426  uint locked = _getAndUpdateLockedAmount(from);  if (locked > 0) {  require(_balances[from] >= locked.add(amount), \"Token should be unlocked for burning\");  //-------------------------------------------------------------------------  _callTokensToSend(  operator, from, address(0), amount, data, operatorData  );  // Update state variables  _totalSupply = _totalSupply.sub(amount);  _balances[from] = _balances[from].sub(amount);  There is a callback function right after the check that there are enough unlocked tokens to burn. In this callback, the user can delegate all the tokens right before burning them without breaking the code flow.  Recommendation  _callTokensToSend  should be called before checking for the unlocked amount of tokens, which is better defined as Checks-Effects-Interactions Pattern.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.2 Rounding errors after slashing    Addressed", "body": "  Resolution                           Mitigated in   skalenetwork/skale-manager#130.  Description  When slashing happens _delegatedToValidator and _effectiveDelegatedToValidator values are reduced.  new_code/contracts/delegation/DelegationController.sol:L349-L355  function confiscate(uint validatorId, uint amount) external {  uint currentMonth = getCurrentMonth();  Fraction memory coefficient = reduce(_delegatedToValidator[validatorId], amount, currentMonth);  reduce(_effectiveDelegatedToValidator[validatorId], coefficient, currentMonth);  putToSlashingLog(_slashesOfValidator[validatorId], coefficient, currentMonth);  _slashes.push(SlashingEvent({reducingCoefficient: coefficient, validatorId: validatorId, month: currentMonth}));  When holders process slashings, they reduce _delegatedByHolderToValidator, _delegatedByHolder, _effectiveDelegatedByHolderToValidator values.  new_code/contracts/delegation/DelegationController.sol:L892-L904  if (oldValue > 0) {  reduce(  _delegatedByHolderToValidator[holder][validatorId],  _delegatedByHolder[holder],  _slashes[index].reducingCoefficient,  month);  reduce(  _effectiveDelegatedByHolderToValidator[holder][validatorId],  _slashes[index].reducingCoefficient,  month);  slashingSignals[index.sub(begin)].holder = holder;  slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId, month));  Also when holders are undelegating, they are calculating how many tokens from delegations[delegationId].amount were slashed.  new_code/contracts/delegation/DelegationController.sol:L316  uint amountAfterSlashing = calculateDelegationAmountAfterSlashing(delegationId);  If rounding error reduces amount not that much as other values, we can have uint underflow. This is especially dangerous because all calculations are delayed and we will know about underflow and SafeMath revert in the next month or later. Developers already made sure that rounding errors are aligned in a correct way, and that the reduced value should always be larger than the subtracted, so there should not be underflow. This solution is very unstable because it s hard to verify it and keep in mind even during a small code change.  If rounding errors make amount smaller then it should be, when other values should be zero (for example, when all the delegations are undelegated), these values will become some very small values. The problem here is that it would be impossible to compare values to zero.  Recommendation  Consider not calling revert on these subtractions and make result value be equals to zero if underflow happens.  Consider comparing to some small epsilon value instead of zero. Or similar to the previous point, on every subtraction check if the value is smaller then epsilon, and make it zero if it is.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.3 Slashes do not affect bounty distribution    Addressed", "body": "  Resolution                           Mitigated in   skalenetwork/skale-manager#118  Description  When slashes are processed by a holder, only _delegatedByHolderToValidator and _delegatedByHolder values are reduced. But _effectiveDelegatedByHolderToValidator value remains the same. This value is used to distribute bounties amongst delegators. So slashing will not affect that distribution.  contracts/delegation/DelegationController.sol:L863-L873  uint oldValue = getAndUpdateDelegatedByHolderToValidator(holder, validatorId);  if (oldValue > 0) {  uint month = _slashes[index].month;  reduce(  _delegatedByHolderToValidator[holder][validatorId],  _delegatedByHolder[holder],  _slashes[index].reducingCoefficient,  month);  slashingSignals[index.sub(begin)].holder = holder;  slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId));  Recommendation  Reduce _effectiveDelegatedByHolderToValidator and _effectiveDelegatedToValidator when slashes are processed.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.4 Iterations over slashes    Addressed", "body": "  Resolution                           Partially mitigated in   skalenetwork/skale-manager#163 .  Description  Every user should iterate over each slash (but only once) and process them in order to determine whether this slash impacted his delegations or not.  However, the check is done during almost every action that the user does because it updates the current state of the user s balance. The downside of this method is that if there are a lot of slashes in the system, every user would be forced to iterate over all of them even if the user is only trading tokens and only calls transfer function.  If the number of slashes is huge, checking them all in one function would impossible due to the block gas limit. It s possible to call the checking function separately and process slashes in batches. So this attack should not result in system halt and can be mitigated with manual intervention.  Also, there are two separate pipelines for iterating over slashes. One pipeline is for iterating over months to determine amount of slashed tokens in separate delegations. This one can potentially hit gas limit in many-many years. The other one is for modifying aggregated delegation values.  Recommendation  Try to avoid all the unnecessary iterations over a potentially unlimited number of items. Additionally, it s possible to optimize some calculations:  When slashing signals are processed, all of them always have the same holder. There s no reason for having an array of signals with the same holder (always with predefined length and values will most likely be zero). It seems possible to remove signals functionality and just aggregate the changes for the Punisher.  Try merge two pipelines into one.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.5 Storage operations optimization    Addressed", "body": "  Resolution                           Mitigated in   skalenetwork/skale-manager#179  Description  There are a lot of operations that write some value to the storage (uses SSTORE opcode) without actually changing it.  Examples  In getAndUpdateValue  function of DelegationController and TokenLaunchLocker:  new_code/contracts/delegation/DelegationController.sol:L711-L715  for (uint i = sequence.firstUnprocessedMonth; i <= month; ++i) {  sequence.value = sequence.value.add(sequence.addDiff[i]).sub(sequence.subtractDiff[i]);  delete sequence.addDiff[i];  delete sequence.subtractDiff[i];  In handleSlash function of Punisher contract amount will be zero in most cases:  new_code/contracts/delegation/Punisher.sol:L66-L68  function handleSlash(address holder, uint amount) external allow(\"DelegationController\") {  _locked[holder] = _locked[holder].add(amount);  Recommendation  Check if the value is the same and don t write it to the storage in that case.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.6 Duplicate function implementation addMonths()    Addressed", "body": "  Resolution                           Fixed in   skalenetwork/skale-manager#127  Description  TimeHelpers.addMonths() implementation is redundant as it can directly use BokkyPooBahsDateTimeLibrary.addMonths() function.  Recommendation  Simply use return BokkyPooBahsDateTimeLibrary.addMonths() on the same function to prevent further code changes, it s still a good idea to call addMonth through TimeHelpers contract.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "6.7 Function overloading    Addressed", "body": "  Resolution                           Fixed in   skalenetwork/skale-manager#181  Description  Some functions in the codebase are overloaded. That makes code less readable and increases the probability of missing bugs.  For example, there are a lot of reduce function implementations in DelegationController:  new_code/contracts/delegation/DelegationController.sol:L722-L820  function reduce(PartialDifferencesValue storage sequence, uint amount, uint month) internal returns (Fraction memory) {  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  if (sequence.firstUnprocessedMonth == 0) {  return createFraction(0);  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return createFraction(0);  uint _amount = amount;  if (value < amount) {  _amount = value;  Fraction memory reducingCoefficient = createFraction(value.sub(_amount), value);  reduce(sequence, reducingCoefficient, month);  return reducingCoefficient;  function reduce(PartialDifferencesValue storage sequence, Fraction memory reducingCoefficient, uint month) internal {  reduce(  sequence,  sequence,  reducingCoefficient,  month,  false);  function reduce(  PartialDifferencesValue storage sequence,  PartialDifferencesValue storage sumSequence,  Fraction memory reducingCoefficient,  uint month) internal  reduce(  sequence,  sumSequence,  reducingCoefficient,  month,  true);  function reduce(  PartialDifferencesValue storage sequence,  PartialDifferencesValue storage sumSequence,  Fraction memory reducingCoefficient,  uint month,  bool hasSumSequence) internal  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  if (hasSumSequence) {  require(month.add(1) >= sumSequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  require(reducingCoefficient.numerator <= reducingCoefficient.denominator, \"Increasing of values is not implemented\");  if (sequence.firstUnprocessedMonth == 0) {  return;  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return;  uint newValue = sequence.value.mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  if (hasSumSequence) {  subtract(sumSequence, sequence.value.sub(newValue), month);  sequence.value = newValue;  for (uint i = month.add(1); i <= sequence.lastChangedMonth; ++i) {  uint newDiff = sequence.subtractDiff[i].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  if (hasSumSequence) {  sumSequence.subtractDiff[i] = sumSequence.subtractDiff[i].sub(sequence.subtractDiff[i].sub(newDiff));  sequence.subtractDiff[i] = newDiff;  function reduce(  PartialDifferences storage sequence,  Fraction memory reducingCoefficient,  uint month) internal  require(month.add(1) >= sequence.firstUnprocessedMonth, \"Can't reduce value in the past\");  require(reducingCoefficient.numerator <= reducingCoefficient.denominator, \"Increasing of values is not implemented\");  if (sequence.firstUnprocessedMonth == 0) {  return;  uint value = getAndUpdateValue(sequence, month);  if (value == 0) {  return;  sequence.value[month] = sequence.value[month].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  for (uint i = month.add(1); i <= sequence.lastChangedMonth; ++i) {  sequence.subtractDiff[i] = sequence.subtractDiff[i].mul(reducingCoefficient.numerator).div(reducingCoefficient.denominator);  Recommendation  Avoid function overloading as a general guideline.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/skale-token/"}, {"title": "3.1 Dependencies With Publicly Known Vulnerabilities (Out of Scope) ", "body": "  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/04/mobymask-mvp-snap/"}, {"title": "3.2 Description", "body": "  The snaps project defines a dependency (@truffle/hdwallet-provider@2.1.1 within the yarn.lock file vulnerable to publicly known weaknesses rated as High or Medium in the CVSS scoring system. It should be noted that the identified areas were not directly in the scope of the code review and are listed for the sake of completeness.  The following @truffle/hdwallet-provider@2.1.1 weaknesses were identified:  Denial of Service decode-uri-element CVE-2022-38900 (CVSSv3 7.5)  Regular Expression Denial of Service  http-cache-semantics CVE-2022-25881(CVSSv3 7.5)  cookiejar CVE-2022-25901(CVSSv3 7.5 - 5.3)  ws CVE-2021-32640 (CVSSv3 5.3)  Server Request Forgery request CVE-2023-28155  (CVSSv3 6.5)  Open Redirect got CVE-2022-33987 (CVSSv3 5.3)  Insecure Credential Storage web3 SNYK-JS-WEB3-174533 (CVSSv3 3.3)  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/04/mobymask-mvp-snap/"}, {"title": "3.3 Recommendation", "body": "  Review all identified dependencies and update the newest, stable version where applicable. Additionally, review the current patch policy to ensure the components are updated as soon as a fix exists. For the identified vulnerable components, the following versions provide fixes:  decode-uri-component@0.2.2  http-cache-semantics@4.1.1  cookiejar@2.1.4  got@11.8.5, @12.1.0  ws@7.4.6, @6.2.2, @5.2.3  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/04/mobymask-mvp-snap/"}, {"title": "4.1 Lockup Plans Are Not Well Suited for Trading on Traditional OTC Platforms ", "body": "  Description  For most of the OTC trading platforms with RFQ style the maker or the taker creates an order that is valid for some time and is expecting a specific token ID. In case of a lockup period a trade participants can request to buy a specific plan ID and then give a fixed amount of time to fill that order, assuming that anything past that time that is unvested is guaranteed to go to them. In reality, the taker of such an order can batch two transactions in one block:  Segment the planId the order is expecting into two: one with just 1 wei to vest and the other with the rest. The large plan will have an incremented plan ID. The small plan will have the old ID.  Fill the order and get the full payment for what is now a worthless plan token.  People should be aware of such a possibility before attempting to purchase any lockup plans over OTC platforms.  Recommendation  One way to solve this is to assign both plans a new ID during the segmentation process.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.2 Architectural Pattern of Internal and External Functions Increases Attack Surface    ", "body": "  Resolution                           Fixed as of commit   Description  There is an architectural pattern throughout the code of functions being defined in two places: an external wrapper (name) that verifies authorization and validates parameters, and an internal function (_name) that contains the implementation logic.  This pattern separates concerns and avoids redundancy in the case that more than one external function reuses the same internal logic.  For example, VotingTokenLockupPlans.setupVoting calls an internal function _setupVoting and sets the holder parameter to msg.sender.  contracts/LockupPlans/VotingTokenLockupPlans.sol:L164-L165  function setupVoting(uint256 planId) external nonReentrant returns (address votingVault) {  votingVault = _setupVoting(msg.sender, planId);  contracts/LockupPlans/VotingTokenLockupPlans.sol:L436-L437  function _setupVoting(address holder, uint256 planId) internal returns (address) {  require(ownerOf(planId) == holder, '!owner');  Other Examples  contracts/LockupPlans/TokenLockupPlans.sol:L107-L113  function segmentPlan(  uint256 planId,  uint256[] memory segmentAmounts  ) external nonReentrant returns (uint256[] memory newPlanIds) {  newPlanIds = new uint256[](segmentAmounts.length);  for (uint256 i; i < segmentAmounts.length; i++) {  uint256 newPlanId = _segmentPlan(msg.sender, planId, segmentAmounts[i]);  contracts/LockupPlans/TokenLockupPlans.sol:L244-L245  function _segmentPlan(address holder, uint256 planId, uint256 segmentAmount) internal returns (uint256 newPlanId) {  require(ownerOf(planId) == holder, '!owner');  contracts/VestingPlans/TokenVestingPlans.sol:L115-L117  function revokePlans(uint256[] memory planIds) external nonReentrant {  for (uint256 i; i < planIds.length; i++) {  _revokePlan(msg.sender, planIds[i]);  contracts/VestingPlans/TokenVestingPlans.sol:L226-L228  function _revokePlan(address vestingAdmin, uint256 planId) internal {  Plan memory plan = plans[planId];  require(vestingAdmin == plan.vestingAdmin, '!vestingAdmin');  Recommendation  To reduce the attack surface, consider hard coding parameters such as holder to msg.sender in internal functions when extra flexibility isn t needed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.3 Vesting Admin Could Prevent the Recipient From Redeeming    ", "body": "  Resolution                           Fixed as of commit   Description  In the vesting part of the protocol, each plan has a vesting admin who can transfer tokens on behalf of the plan holder. However, this setup poses a risk of centralization. For instance, a plan holder might leave their tokens vested for a long time without claiming them. Then, if the vesting admin decides to transfer the plan to a different wallet, the recipient may never be able to claim those tokens.  We understand that this feature is meant to assist novice users who might lose their private keys and need a safety net. Nevertheless, we suggest giving the plan recipient the option to toggle the adminTransferOBO on and off. This way, they can protect themselves better against any potentially malicious actions from the vesting admin, all without triggering a taxable event.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.4 Revoking Vesting Will Trigger a Taxable Event    ", "body": "  Resolution                           Fixed as of commit   Description  From the previous conversations with the Hedgey team, we identified that users should be in control of when taxable events happen. For that reason,  one could redeem a plan in the past. Unfortunately, the recipient of the vesting plan can not always be in control of the redemption process. If for one reason or another the administrator of the vesting plan decides to revoke it, any vested funds will be sent to the vesting plan holder, triggering the taxable event and burning the NFT.  Examples  contracts/VestingPlans/TokenVestingPlans.sol:L226-L237  function _revokePlan(address vestingAdmin, uint256 planId) internal {  Plan memory plan = plans[planId];  require(vestingAdmin == plan.vestingAdmin, '!vestingAdmin');  (uint256 balance, uint256 remainder, ) = planBalanceOf(planId, block.timestamp, block.timestamp);  require(remainder > 0, '!Remainder');  address holder = ownerOf(planId);  delete plans[planId];  _burn(planId);  TransferHelper.withdrawTokens(plan.token, vestingAdmin, remainder);  TransferHelper.withdrawTokens(plan.token, holder, balance);  emit PlanRevoked(planId, balance, remainder);  contracts/VestingPlans/VotingTokenVestingPlans.sol:L245-L263  function _revokePlan(address vestingAdmin, uint256 planId) internal {  Plan memory plan = plans[planId];  require(vestingAdmin == plan.vestingAdmin, '!vestingAdmin');  (uint256 balance, uint256 remainder, ) = planBalanceOf(planId, block.timestamp, block.timestamp);  require(remainder > 0, '!Remainder');  address holder = ownerOf(planId);  delete plans[planId];  _burn(planId);  address vault = votingVaults[planId];  if (vault == address(0)) {  TransferHelper.withdrawTokens(plan.token, vestingAdmin, remainder);  TransferHelper.withdrawTokens(plan.token, holder, balance);  } else {  delete votingVaults[planId];  VotingVault(vault).withdrawTokens(vestingAdmin, remainder);  VotingVault(vault).withdrawTokens(holder, balance);  emit PlanRevoked(planId, balance, remainder);  Recommendation  One potential workaround is to only withdraw the unvested portion to the vesting admin while keeping the vested part in the contract. That being said amount and rate variables would need to be updated in order not to allow any additional vesting for the given plan. This way plan holders will not be entitled to more funds but will be able to redeem them at the time they choose.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.5 Use of selfdestruct Deprecated in VotingVault    ", "body": "  Resolution                           Fixed as of commit   Description  The VotingVault.withdrawTokens function invokes the selfdestruct operation when the vault is empty so that it can t be used again.  The use ofselfdestruct has been deprecated and a breaking change in its future behavior is expected.  Examples  contracts/sharedContracts/VotingVault.sol:L36-L39  function withdrawTokens(address to, uint256 amount) external onlyController {  TransferHelper.withdrawTokens(token, to, amount);  if (IERC20(token).balanceOf(address(this)) == 0) selfdestruct;  Recommendation  Remove the line that invokes selfdestruct and consider changing internal state so that future calls to delegateTokens always revert.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.6 Balance of msg.sender Is Used Instead of the from Address    ", "body": "  Resolution                           Fixed as of commit   Description  The TransferHelper library has methods that allow transferring tokens directly or on behalf of a different wallet that previously approved the transfer. Those functions also check the sender balance before conducting the transfer. In the second case, where the transfer happens on behalf of someone the code is checking not the actual token spender balance, but the msg.sender balance instead.  Examples  contracts/libraries/TransferHelper.sol:L18-L25  function transferTokens(  address token,  address from,  address to,  uint256 amount  ) internal {  uint256 priorBalance = IERC20(token).balanceOf(address(to));  require(IERC20(token).balanceOf(msg.sender) >= amount, 'THL01');  Recommendation  Use the from parameter instead of msg.sender.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.7 Refactor Large Functions for Readability", "body": "  Description  Function bodies larger than a typical screenful of text are harder to read and to reason about security properties.  Examples  VotingTokenLockupPlans._combinePlans is 98 lines long.  VotingTokenLockupPlans._segmentPlan is 72 lines long.  TokenLockupPlans._segmentPlan is 66 lines long.  Recommendation  Refactor large functions into compositions of smaller, easier-to-understand functions.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.8 Unused Code in Source Files   ", "body": "  Resolution                           Fixed as of commit   Description  There is unused code in the source files.  Examples  The function TimelockLibrary.totalPeriods isn t used and appears to be incorrect (rounds down instead of up).  contracts/libraries/TimelockLibrary.sol:L28-L31  /// @notice function to calculate the total periods in a given plan based on the rate and amount  function totalPeriods(uint256 rate, uint256 amount) internal pure returns (uint256 periods) {  periods = amount / rate;  ERC721Delegate.wasTransferred is written but not read:  contracts/ERC721Delegate/ERC721Delegate.sol:L26-L27  // Mapping if a token has been transferred  mapping(uint256 => bool) public wasTransferred;  Recommendation  Remove unused code to reduce confusion and to decrease the attack surface.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.9 Use Custom Errors to Save Gas", "body": "  Description  As of Solidity 0.8.4 it is possible to save gas when reporting error conditions by using custom errors instead of strings.  Examples  contracts/ERC721Delegate/ERC721Delegate.sol:L40  require(index < ERC721.balanceOf(owner), 'ERC721Enumerable: owner index out of bounds');  contracts/ERC721Delegate/ERC721Delegate.sol:L59  require(index < totalSupply(), 'ERC721Enumerable: global index out of bounds');  Recommendation  We recommend using custom errors to save gas.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.10 Use _beforeTokenTransfer to Override Behavior in OpenZeppelin Token Contracts  Partially Addressed", "body": "  Resolution  Client response:  Did not implement for the Vesting plans because the impact would override and complicate functionality desired through the vestingAdminTransferOBO, because of the way the hooks process before and after it would require significant and possibly risky changes  Description  For example, in the unreleased version of OpenZeppelin s contracts, the ERC20._transfer function is no longer virtual and contains the warning:  NOTE: This function is not virtual, {_update} should be overridden instead.  Examples  contracts/VestingPlans/TokenVestingPlans.sol:L282  function transferFrom(address from, address to, uint256 tokenId) public override(IERC721, ERC721) {  contracts/VestingPlans/TokenVestingPlans.sol:L291  function _safeTransfer(address from, address to, uint256 tokenId, bytes memory data) internal override {  contracts/LockupPlans/NonTransferable/TokenLockupPlans_Bound.sol:L21  function _transfer(address from, address to, uint256 tokenId) internal virtual override {  Recommendation  OpenZeppelin recognizes this as a common use case and provides a hook for cleaner control over transfer behavior.  Use the  _beforeTokenTransfer hook with version 4 contracts to enforce transfer conditions.  Please note however that the _beforeTokenTransfer hook will be deprecated in the next release of OpenZeppelin s contracts in favor of a new function called _update.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "4.11 Use calldata Instead of memory for External Function Arguments Data Location  Partially Addressed", "body": "  Resolution                           Fixed for some functions but not others, e.g.,   Description  Reference types (e.g., arrays, mappings, and structs) in function arguments must declare the  data location  for where they are stored.  There are two options for external functions: calldata or memory.  calldata arguments are immutable which reduces complexity and improves code readability.  memory arguments are mutable and add an implicit copy operation.  Examples  contracts/LockupPlans/TokenLockupPlans.sol:L72  function redeemPlans(uint256[] memory planIds) external nonReentrant {  contracts/VestingPlans/VotingTokenVestingPlans.sol:L123  function revokePlans(uint256[] memory planIds) external nonReentrant {  contracts/LockupPlans/TokenLockupPlans.sol:L107-L110  function segmentPlan(  uint256 planId,  uint256[] memory segmentAmounts  ) external nonReentrant returns (uint256[] memory newPlanIds) {  Recommendation  The Solidity documentation makes the following recommendation:  If you can, try to use calldata as data location because it will avoid copies and also makes sure that the data cannot be modified.  We recommend always using calldata for external function parameters unless doing so would incur a serious performance penalty or make code harder to read.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/hedgey-token-lockup-and-vesting-plans/"}, {"title": "6.1 IdleCDO._deposit() allows re-entrancy from hookable tokens. ", "body": "  Resolution                           The development team has addressed this concern in commit   5fbdc0506c94a172abbd4122276ed2bd489d1964. This change has not been reviewed by the audit team.  Description  The function IdleCDO._deposit() updates the system s internal accounting and mints shares to the caller, then transfers the deposited funds from the user. Some token standards, such as ERC777, allow a callback to the source of the funds before the balances are updated in transferFrom(). This callback could be used to re-enter the protocol while already holding the minted tranche tokens and at a point where the system accounting reflects a receipt of funds that has not yet occurred.  While an attacker could not interact with IdleCDO.withdraw() within this callback because of the _checkSameTx() restriction, they would be able to interact with the rest of the protocol.  code/contracts/IdleCDO.sol:L230-L245  function _deposit(uint256 _amount, address _tranche) internal returns (uint256 _minted) {  // check that we are not depositing more than the contract available limit  _guarded(_amount);  // set _lastCallerBlock hash  _updateCallerBlock();  // check if strategyPrice decreased  _checkDefault();  // interest accrued since last depositXX/withdrawXX/harvest is splitted between AA and BB  // according to trancheAPRSplitRatio. NAVs of AA and BB are updated and tranche  // prices adjusted accordingly  _updateAccounting();  // mint tranche tokens according to the current tranche price  _minted = _mintShares(_amount, msg.sender, _tranche);  // get underlyings from sender  IERC20Detailed(token).safeTransferFrom(msg.sender, address(this), _amount);  Recommendation  Move the transferFrom() action in _deposit() to immediately after _updateCallerBlock().  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.2 IdleCDO.virtualPrice() and _updatePrices() yield different prices in a number of cases ", "body": "  Resolution  The development team implemented a new version of both functions using a third method, virtualPricesAux(), to perform the primary price calculation. Additionally, _updatePrices() was renamed to _updateAccounting().  This change was incorporated in commit ff0b69380828657f16df8683c35703b325a6b656.  Description  The function IdleCDO.virtualPrice() is used to determine the current price of a tranche. Similarly, IdleCDO._updatePrices() is used to store the latest price of a tranche, as well as update other parts of the system accounting. There are a number of cases where the prices yielded by these two functions differ. While these are primarily corner cases that are not obviously exploitable in practice, potential violations of key accounting invariants should always be considered serious.  Additionally, the use of two separate implementations of the same calculation suggest the potential for more undiscovered discrepancies, possibly of higher consequence.  As an example, in _updatePrices() the precision loss from splitting the strategy returns favors BB tranche holders. In virtualPrice() both branches of the price calculation incur precision loss, favoring the IdleCDO contract itself.  _updatePrices()  code/contracts/IdleCDO.sol:L331-L341  if (BBTotSupply == 0) {  // if there are no BB holders, all gain to AA  AAGain = gain;  } else if (AATotSupply == 0) {  // if there are no AA holders, all gain to BB  BBGain = gain;  } else {  // split the gain between AA and BB holders according to trancheAPRSplitRatio  AAGain = gain * trancheAPRSplitRatio / FULL_ALLOC;  BBGain = gain - AAGain;  virtualPrice()  code/contracts/IdleCDO.sol:L237-L245  if (_tranche == AATranche) {  // calculate gain for AA tranche  // trancheGain (AAGain) = gain * trancheAPRSplitRatio / FULL_ALLOC;  trancheNAV = lastNAVAA + (gain * _trancheAPRSplitRatio / FULL_ALLOC);  } else {  // calculate gain for BB tranche  // trancheGain (BBGain) = gain * (FULL_ALLOC - trancheAPRSplitRatio) / FULL_ALLOC;  trancheNAV = lastNAVBB + (gain * (FULL_ALLOC - _trancheAPRSplitRatio) / FULL_ALLOC);  Recommendation  Implement a single method that determines the current price for a tranche, and use this same implementation anywhere the price is needed.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.3 IdleCDO.harvest() allows price manipulation in certain circumstances ", "body": "  Resolution                           The development team has addressed this concern in a pull request with a final commit hash of   5341a9391f9c42cadf26d72c9f804ca75a15f0fb. This change has not been reviewed by the audit team.  Description  The function IdleCDO.harvest() uses Uniswap to liquidate rewards earned by the contract s strategy, then updates the relevant positions and internal accounting. This function can only be called by the contract owner or the designated rebalancer address, and it accepts an array which indicates the minimum buy amounts for the liquidation of each reward token.  The purpose of permissioning this method and specifying minimum buy amounts is to prevent a sandwiching attack from manipulating the reserves of the Uniswap pools and forcing the IdleCDO contract to incur loss due to price slippage.  However, this does not effectively prevent price manipulation in all cases. Because the contract sells it s entire balance of redeemed rewards for the specified minimum buy amount, this approach does not enforce a minimum price for the executed trades. If the balance of IdleCDO or the amount of claimable rewards increases between the submission of the harvest() transaction and its execution, it may be possible to perform a profitable sandwiching attack while still satisfying the required minimum buy amounts.  The viability of this exploit depends on how effectively an attacker can increase the amount of rewards tokens to be sold without incurring an offsetting loss. The strategy contracts used by IdleCDO are expected to vary widely in their implementations, and this manipulation could potentially be done either through direct interaction with the protocol or as part of a flashbots bundle containing a large position adjustment from an honest user.  code/contracts/IdleCDO.sol:L564-L565  function harvest(bool _skipRedeem, bool _skipIncentivesUpdate, bool[] calldata _skipReward, uint256[] calldata _minAmount) external {  require(msg.sender == rebalancer || msg.sender == owner(), \"IDLE:!AUTH\");  code/contracts/IdleCDO.sol:L590-L599  // approve the uniswap router to spend our reward  IERC20Detailed(rewardToken).safeIncreaseAllowance(address(_uniRouter), _currentBalance);  // do the uniswap trade  _uniRouter.swapExactTokensForTokensSupportingFeeOnTransferTokens(  _currentBalance,  _minAmount[i],  _path,  address(this),  block.timestamp + 1  );  Recommendation  Update IdleCDO.harvest() to enforce a minimum price rather than a minimum buy amount. One method of doing so would be taking an additional array parameter indicating the amount of each token to sell in exchange for the respective buy amount.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.4 Prevent zero amount transfers/minting ", "body": "  Resolution                           The development team has addressed this concern in commit   a72747da8c0ca71274f3a1506c6faf724cf82dd2. This change has not been reviewed by the audit team.  Description  Many of the functions in the system can be called with amount = 0. This is not a security issue, however a  defense in depth  approach in this and similar cases may prevent an undiscovered bug from being exploitable. Most of the functionalities that were reviewed in this audit won t create an exploitable state transition in these cases, however they will trigger a 0 token transfer or minting.  Examples  depositAA()  depositBB()  stake()  unstake()  Recommendation  Check and return early (or revert) on requests with zero amount.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.5 Missing Sanity checks ", "body": "  Resolution                           The development team has addressed this concern in commit   a1d5dac0ad5f562d4c75bff99e770d92bcc2a72f. This change has not been reviewed by the audit team.  Description  The implementation of initialize() functions are missing some sanity checks. The proper checks are implemented in some of the setter functions but missing in some others.  Examples  Missing sanity check for != address(0)  code/contracts/IdleCDO.sol:L54-L57  token = _guardedToken;  strategy = _strategy;  strategyToken = IIdleCDOStrategy(_strategy).strategyToken();  rebalancer = _rebalancer;  code/contracts/IdleCDO.sol:L84-L84  guardian = _owner;  code/contracts/IdleCDO.sol:L672-L673  address _currAAStaking = AAStaking;  address _currBBStaking = BBStaking;  code/contracts/IdleCDOTrancheRewards.sol:L50-L53  idleCDO = _idleCDO;  tranche = _trancheToken;  rewards = _rewards;  governanceRecoveryFund = _governanceRecoveryFund;  Recommendation  Add sanity checks before assigning system variables.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.6 IdleCDO.virtualPrice() & _updatePrices() too complicated to verify ", "body": "  Resolution  These methods were revisited in the continuation of the original review and more time was allotted to them than was possible previously. Some refactoring also occurred during that time (see 6.2). However, the development team elected to maintain the general approach used in these functions.  The primary challenge in verifying their correctness remains, which is their heavy reliance on external interactions with contracts whose expected semantics are poorly defined.  Description  IdleCDO.virtualPrice() and _updatePrices() functions are used for many important functionality in the Idle system. They also have nested external calls to many other contracts (e.g. IdleTokenGovernance, IdleCDOStrategy and strategy token, IdleCDOTranche on both Tranche tokens, etc). This level of complexity for a vital function is not recommended and is considered dangerous implementation.  Examples  Recommendation  Consider refactoring the code to use less complicated logic and code flow.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "5.1 VaultConfig.setVaultConfig doesn t check all critical arguments ", "body": "  Resolution  Remediated per Notional s team notes in commit by adding the following checks:  Checks to ensure borrow currency and secondary currencies cannot change once set  Check to ensure liquidationRate does not exceed minCollateralRatioBPS  Check for maxBorrowMarketIndex was not added. The Notional team will review this parameter on a case-by-case basis as for some vaults borrowing idiosyncratic fCash may not be an issue  Description  The Notional Strategy Vaults need to get whitelisted and have specific Notional parameters set in order to interact with the rest of the Notional system. This is done through VaultAction.updateVault() where the owner address can provide a VaultConfigStorage calldata vaultConfig argument to either whitelist a new vault or change an existing one. While this is to be performed by a trusted privileged actor (the owner), and it could be assumed they are careful with their updates, the contracts themselves don t perform enough checks on the validity of the parameters, either in isolation or when compared against the existing vault state. Below are examples of arguments that should be better checked.  borrowCurrencyId  The borrowCurrencyId parameter gets provided to TokenHandler.getAssetToken() and TokenHandler.getUnderlyingToken() to retrieve its associated TokenStorage object and verify that the currency doesn t have transfer fees.  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L162-L164  Token memory assetToken = TokenHandler.getAssetToken(vaultConfig.borrowCurrencyId);  Token memory underlyingToken = TokenHandler.getUnderlyingToken(vaultConfig.borrowCurrencyId);  require(!assetToken.hasTransferFee && !underlyingToken.hasTransferFee);  However, these calls retrieve data from the mapping from storage which returns an empty struct for an unassigned currency ID. This would pass the check in the last require statement regarding the transfer fees and would successfully allow to set the currency even if isn t actually registered in Notional. The recommendation would be to check that the returned TokenStorage object has data inside of it, perhaps by checking the decimals on the token.  In the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the borrow currency without checking that the existing borrow and lending accounting has been cleared. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended.  liquidationRate and minCollateralRatioBPS  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L283  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  vaultAccount.vaultShares = vaultAccount.vaultShares.sub(vaultSharesToLiquidator);  maxBorrowMarketIndex  The current Strategy Vault implementation does not allow for idiosyncratic cash because it causes issues during exits as there are no active markets for the account s maturity. Therefore, the configuration shouldn t be set with maxBorrowMarketIndex >=3 as that would open up the 1 Year maturity for vault accounts that could cause idiosyncratic fCash. The recommendation would be to add that check.  secondaryBorrowCurrencies  Similarly to the borrowCurrencyId, there are few checks that actually determine that the secondaryBorrowCurrencies[] given are actually registered in Notional. This is, however, more inline with how some vaults are supposed to work as they may have no secondary currencies at all, such as when the secondaryBorrowCurrencies[] id is given as 0. In the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the secondary borrow currency without checking that the existing borrow and lending accounting has been cleared. For example, the VaultAction.updateSecondaryBorrowCapacity() function could be invoked on the new set of secondary currencies and simply increase the borrow there. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.2 Handle division by 0 ", "body": "  Resolution  Remediated per Notional s team notes in commit by adding the following checks:  Check to account for div by zero in settle vault account  Short circuit to ensure debtSharesToRepay is never zero. Divide by zero may still occur but this would signal a critical accounting issue  The Notional team also acknowledged that the contract will revert when vaultShareValue = 0. The team decided to not make any changes related to that since liquidation will not accomplish anything for an account with no vault share value.  Description  There are a few places in the code where division by zero may occur but isn t handled.  Examples  If the vault settles at exactly 0 value with 0 remaining strategy token value, there may be an unhandled division by zero trying to divide claims on the settled assets:  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L424-L436  int256 settledVaultValue = settlementRate.convertToUnderlying(residualAssetCashBalance)  .add(totalStrategyTokenValueAtSettlement);  // If the vault is insolvent (meaning residualAssetCashBalance < 0), it is necessarily  // true that totalStrategyTokens == 0 (meaning all tokens were sold in an attempt to  // repay the debt). That means settledVaultValue == residualAssetCashBalance, strategyTokenClaim == 0  // and assetCashClaim == totalAccountValue. Accounts that are still solvent will be paid from the  // reserve, accounts that are insolvent will have a totalAccountValue == 0.  strategyTokenClaim = totalAccountValue.mul(vaultState.totalStrategyTokens.toInt())  .div(settledVaultValue).toUint();  assetCashClaim = totalAccountValue.mul(residualAssetCashBalance)  .div(settledVaultValue);  If a vault account is entirely insolvent and its vaultShareValue is zero, there will be an unhandled division by zero during liquidation:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L281  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  If a vault account s secondary debt is being repaid when there is none, there will be an unhandled division by zero:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L661-L666  VaultSecondaryBorrowStorage storage balance =  LibStorage.getVaultSecondaryBorrow()[vaultConfig.vault][maturity][currencyId];  uint256 totalfCashBorrowed = balance.totalfCashBorrowed;  uint256 totalAccountDebtShares = balance.totalAccountDebtShares;  fCashToLend = debtSharesToRepay.mul(totalfCashBorrowed).div(totalAccountDebtShares).toInt();  While these cases may be unlikely today, this code could be reutilized in other circumstances later that could cause reverts and even disrupt operations more frequently.  Recommendation  Handle the cases where the denominator could be zero appropriately.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.3 Increasing a leveraged position in a vault with secondary borrow currency will revert ", "body": "  Resolution                           Per Notional team s notes, they have rearranged if statement to ensure that increasing an existing position will work. The proposed solution was skipped as it creates issues with the   Commit  Description  From the client s specifications for the strategy vaults, we know that accounts should be able to increase their leveraged positions before maturity. This property will not hold for the vaults that require borrowing a secondary currency to enter a position. When an account opens its position in such vault for the first time, the VaultAccountSecondaryDebtShareStorage.maturity is set to the maturity an account has entered. When the account is trying to increase the debt position, an accounts current maturity will be checked, and since it is not set to 0, as in the case where an account enters the vault for the first time, nor it is smaller than the new maturity passed by an account as in case of a rollover, the code will revert.  Examples  contracts-v2/contracts/external/actions/VaultAction.sol:L226-L228  if (accountMaturity != 0) {  // Cannot roll to a shorter term maturity  require(accountMaturity < maturity);  Recommendation  In order to fix this issue, we recommend that < is replaced with <= so that account can enter the vault maturity the account is already in as well as the future once.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.4 Secondary Currency debt is not managed by the Notional Controller ", "body": "  Resolution                           Remediated per Notional s team notes in   commit by adding valuation for secondary borrow within the vault.  Description  Some of the Notional Strategy Vaults may allow for secondary currencies to be borrowed as part of the same strategy. For example, a strategy may allow for USDC to be its primary borrow currency as well as have ETH as its secondary borrow currency.  In order to enter the vault, a user would have to deposit depositAmountExternal of the primary borrow currency when calling VaultAccountAction.enterVault(). This would allow the user to borrow with leverage, as long as the vaultConfig.checkCollateralRatio() check on that account succeeds, which is based on the initial deposit and borrow currency amounts. This collateral ratio check is then performed throughout that user account s lifecycle in that vault, such as when they try to roll their maturity, or when liquidators try to perform collateral checks to ensure there is no bad debt.  However, in the event that the vault has a secondary borrow currency as well, that additional secondary debt is not calculated as part of the checkCollateralRatio() check. The only debt that is being considered is the vaultAccount.fCash that corresponds to the primary borrow currency debt:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L313-L319  function checkCollateralRatio(  VaultConfig memory vaultConfig,  VaultState memory vaultState,  VaultAccount memory vaultAccount  ) internal view {  (int256 collateralRatio, /* */) = calculateCollateralRatio(  vaultConfig, vaultState, vaultAccount.account, vaultAccount.vaultShares, vaultAccount.fCash  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L278-L292  function calculateCollateralRatio(  VaultConfig memory vaultConfig,  VaultState memory vaultState,  address account,  uint256 vaultShares,  int256 fCash  ) internal view returns (int256 collateralRatio, int256 vaultShareValue) {  vaultShareValue = vaultState.getCashValueOfShare(vaultConfig, account, vaultShares);  // We do not discount fCash to present value so that we do not introduce interest  // rate risk in this calculation. The economic benefit of discounting will be very  // minor relative to the added complexity of accounting for interest rate risk.  // Convert fCash to a positive amount of asset cash  int256 debtOutstanding = vaultConfig.assetRate.convertFromUnderlying(fCash.neg());  Whereas the value of strategy tokens that belong to that user account are being calculated by calling IStrategyVault(vault).convertStrategyToUnderlying() on the associated strategy vault:  contracts-v2/contracts/internal/vaults/VaultState.sol:L314-L324  function getCashValueOfShare(  VaultState memory vaultState,  VaultConfig memory vaultConfig,  address account,  uint256 vaultShares  ) internal view returns (int256 assetCashValue) {  if (vaultShares == 0) return 0;  (uint256 assetCash, uint256 strategyTokens) = getPoolShare(vaultState, vaultShares);  int256 underlyingInternalStrategyTokenValue = _getStrategyTokenValueUnderlyingInternal(  vaultConfig.borrowCurrencyId, vaultConfig.vault, account, strategyTokens, vaultState.maturity  );  contracts-v2/contracts/internal/vaults/VaultState.sol:L296-L311  function _getStrategyTokenValueUnderlyingInternal(  uint16 currencyId,  address vault,  address account,  uint256 strategyTokens,  uint256 maturity  ) private view returns (int256) {  Token memory token = TokenHandler.getUnderlyingToken(currencyId);  // This will be true if the the token is \"NonMintable\" meaning that it does not have  // an underlying token, only an asset token  if (token.decimals == 0) token = TokenHandler.getAssetToken(currencyId);  return token.convertToInternal(  IStrategyVault(vault).convertStrategyToUnderlying(account, strategyTokens, maturity)  );  From conversations with the Notional team, it is assumed that this call returns the strategy token value subtracted against the secondary currencies debt, as is the case in the Balancer2TokenVault for example. In other words, when collateral ratio checks are performed, those strategy vaults that utilize secondary currency borrows would need to calculate the value of strategy tokens already accounting for any secondary debt. However, this is a dependency for a critical piece of the Notional controller s strategy vaults collateral checks.  Therefore, even though the strategy vaults  code and logic would be vetted before their whitelisting into the Notional system, they would still remain an external dependency with relatively arbitrary code responsible for the liquidation infrastructure that could lead to bad debt or incorrect liquidations if the vaults give inaccurate information, and thus potential loss of funds.  Recommendation  Specific strategy vault implementations using secondary borrows were not in scope of this audit. However, since the core Notional Vault system was, and it includes secondary borrow currency functionality, from the point of view of the larger Notional system it is recommended to include secondary debt checks within the Notional controller contract to reduce external dependency on the strategy vaults  logic.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.5 Vaults are unable to borrow single secondary currency ", "body": "  Resolution  Remediated per Notional s team notes.  Description  As was previously mentioned some strategies require borrowing one or two secondary currencies. All secondary currencies have to be whitelisted in the VaultConfig.secondaryBorrowCurrencies. Borrow operation on secondary currencies is performed in the borrowSecondaryCurrencyToVault(...) function. Due to a require statement in that function, vaults will only be able to borrow secondary currencies if both of the currencies are whitelisted in VaultConfig.secondaryBorrowCurrencies. Considering that many strategies will have just one secondary currency, this will prevent those strategies from borrowing any secondary assets.  Examples  contracts-v2/contracts/external/actions/VaultAction.sol:L214  require(currencies[0] != 0 && currencies[1] != 0);  Recommendation  contracts-v2/contracts/external/actions/VaultAction.sol:L202-L208  function borrowSecondaryCurrencyToVault(  address account,  uint256 maturity,  uint256[2] calldata fCashToBorrow,  uint32[2] calldata maxBorrowRate,  uint32[2] calldata minRollLendRate  ) external override returns (uint256[2] memory underlyingTokensTransferred) {  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.6 An account roll may be impossible if the vault is already at the maximum borrow capacity. ", "body": "  Resolution                           Remediated per Notional s team notes in   commit by adding the ability for accounts to deposit during a roll vault position call to offset any additional cost that would put them over the maximum borrow capacity.  Description  One of the actions allowed in Notional Strategy Vaults is to roll an account s maturity to a later one by borrowing from a later maturity and repaying that into the debt of the earlier maturity.  However, this could cause an issue if the vault is at maximum capacity at the time of the roll. When an account performs this type of roll, the new borrow would have to be more than the existing debt simply because it has to at least cover the existing debt and pay for the borrow fees that get added on every new borrow. Since the whole vault was already at max borrow capacity before with the old, smaller borrow, this process would revert at the end after the new borrow as well once the process gets to VaultAccount.updateAccountfCash and VaultConfiguration.updateUsedBorrowCapacity:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L243-L257  function updateUsedBorrowCapacity(  address vault,  uint16 currencyId,  int256 netfCash  ) internal returns (int256 totalUsedBorrowCapacity) {  VaultBorrowCapacityStorage storage cap = LibStorage.getVaultBorrowCapacity()[vault][currencyId];  // Update the total used borrow capacity, when borrowing this number will increase (netfCash < 0),  // when lending this number will decrease (netfCash > 0).  totalUsedBorrowCapacity = int256(uint256(cap.totalUsedBorrowCapacity)).sub(netfCash);  if (netfCash < 0) {  // Always allow lending to reduce the total used borrow capacity to satisfy the case when the max borrow  // capacity has been reduced by governance below the totalUsedBorrowCapacity. When borrowing, it cannot  // go past the limit.  require(totalUsedBorrowCapacity <= int256(uint256(cap.maxBorrowCapacity)), \"Max Capacity\");  The result is that users won t able to roll while the vault is at max capacity. However, users may exit some part of their position to reduce their borrow, thereby reducing the overall vault borrow capacity, and then could execute the roll. A bigger problem would occur if the vault configuration got updated to massively reduce the borrow capacity, which would force users to exit their position more significantly with likely a much smaller chance at being able to roll.  Recommendation  Document this case so that users can realise that rolling may not always be an option. Perhaps consider adding ways where users can pay a small deposit, like on enterVault, to offset the additional difference in borrows  and pay for fees so they can remain with essentially the same size position within Notional.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.7 Rollover might introduce economically impractical deposits of dust into a strategy ", "body": "  Resolution  Acknowledged with a note from the Notional s team:   This is true, however, vaults with secondary borrows may need to execute logic in order to roll positions forward. We will opt to not do any handling for dust amounts on the vault controller side and allow each vault to set its own dust thresholds.   Description  During the rollover of the strategy position into a longer maturity, several things happen:  Funds are borrowed from the longer maturity to pay off the debt and fees of the current maturity.  Strategy tokens that are associated with the current maturity are moved to the new maturity.  Any additional funds provided by the account are deposited into the strategy into a new longer maturity.  In reality, due to the AMM nature of the protocol, the funds borrowed from the new maturity could exceed the debt the account has in the current maturity, resulting in a non-zero vaultAccount.tempCashBalance. In that case, those funds will be deposited into the strategy. That would happen even if there are no external funds supplied by the account for the deposit.  It is possible that the dust in the temporary account balance will not cover the gas cost of triggering a full deposit call of the strategy.  Examples  contracts-v2/contracts/internal/vaults/VaultState.sol:L244-L246  uint256 strategyTokensMinted = vaultConfig.deposit(  vaultAccount.account, vaultAccount.tempCashBalance, vaultState.maturity, additionalUnderlyingExternal, vaultData  );  Recommendation  We suggest that additional checks are introduced that would check that on rollover vaultAccount.tempCashBalance + additionalUnderlyingExternal > 0 or larger than a certain threshold like minAccountBorrowSize for example.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.8 Significantly undercollateralized accounts will revert on liquidation ", "body": "  Resolution                           Remediated per Notional s team notes in   commit by updating the calculations within  Description  The Notional Strategy Vaults utilise collateral to allow leveraged borrowing as long as the account passes the checkCollateralRatio check that ensures the overall account value is at least minCollateralRatio greater than its debts. If the account doesn t have sufficient collateral, it goes through a liquidation process where some of the collateral is sold to liquidators for the account s borrowed currency in attempt to improve the collateral ratio. However, if the account is severely undercollateralised, the entire account position is liquidated and given over to the liquidator:  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L282-L289  int256 depositRatio = maxLiquidatorDepositAssetCash.mul(vaultConfig.liquidationRate).div(vaultShareValue);  // Use equal to so we catch potential off by one issues, the deposit amount calculated inside the if statement  // below will round the maxLiquidatorDepositAssetCash down  if (depositRatio >= Constants.RATE_PRECISION) {  maxLiquidatorDepositAssetCash = vaultShareValue.divInRatePrecision(vaultConfig.liquidationRate);  // Set this to true to ensure that the account gets fully liquidated  mustLiquidateFullAmount = true;  Here, the liquidator will need to deposit exactly maxLiquidatorDepositAssetCash=vaultShareValue/liquidationRate in order to get all of account s assets, i.e. all of vaultShareValue in the form of vaultAccount.vaultShares. In fact, later this deposit will be set in vaultAccount.tempCashBalance:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L361-L380  int256 maxLiquidatorDepositExternal = assetToken.convertToExternal(maxLiquidatorDepositAssetCash);  // NOTE: deposit amount external is always positive in this method  if (depositAmountExternal < maxLiquidatorDepositExternal) {  // If this flag is set, the liquidator must deposit more cash in order to liquidate the account  // down to a zero fCash balance because it will fall under the minimum borrowing limit.  require(!mustLiquidateFull, \"Must Liquidate All Debt\");  } else {  // In the other case, limit the deposited amount to the maximum  depositAmountExternal = maxLiquidatorDepositExternal;  // Transfers the amount of asset tokens into Notional and credit it to the account's temp cash balance  int256 assetAmountExternalTransferred = assetToken.transfer(  liquidator, vaultConfig.borrowCurrencyId, depositAmountExternal  );  vaultAccount.tempCashBalance = vaultAccount.tempCashBalance.add(  assetToken.convertToInternal(assetAmountExternalTransferred)  );  Then the liquidator will get:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L281  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  And if (except for precision and conversions) vaultAccount.tempCashBalance=maxLiquidatorDepositAssetCash=vaultShareValue/liquidationRate, then vaultSharesToLiquidator = (vaultAccount.tempCashBalance * liquidationRate * vaultAccount.vaultShares) / (vaultShareValue) becomes vaultSharesToLiquidator = ((vaultShareValue/liquidationRate)* liquidationRate * vaultAccount.vaultShares) / (vaultShareValue) = vaultAccount.vaultShares  In other words, the liquidator needed to deposit exactly vaultShareValue/liquidationRate to get all vaultAccount.vaultShares. However, the liquidator deposit (what would be in vaultAccount.tempCashBalance) needs to cover all of that account s debt, i.e. vaultAccount.fCash. At the end of the liquidation process, the vault account has its fCash and tempCash balances updated:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L289-L290  int256 fCashToReduce = vaultConfig.assetRate.convertToUnderlying(vaultAccount.tempCashBalance);  vaultAccount.updateAccountfCash(vaultConfig, vaultState, fCashToReduce, vaultAccount.tempCashBalance.neg());  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L77-L88  function updateAccountfCash(  VaultAccount memory vaultAccount,  VaultConfig memory vaultConfig,  VaultState memory vaultState,  int256 netfCash,  int256 netAssetCash  ) internal {  vaultAccount.tempCashBalance = vaultAccount.tempCashBalance.add(netAssetCash);  // Update fCash state on the account and the vault  vaultAccount.fCash = vaultAccount.fCash.add(netfCash);  require(vaultAccount.fCash <= 0);  While the vaultAccount.tempCashBalance gets cleared to 0, the vaultAccount.fCash amount only gets to vaultAccount.fCash = vaultAccount.fCash.add(netfCash), and netfCash=fCashToReduce = vaultConfig.assetRate.convertToUnderlying(vaultAccount.tempCashBalance), which, based on the constraints above essentially becomes:  vaultAccount.fCash=vaultAccount.fCash+vaultConfig.assetRate.convertToUnderlying(assetToken.convertToExternal(vaultShareValue/vaultConfig.liquidationRate))  However, later this account is set on storage, and, considering it is going through 100% liquidation, the account will necessarily be below minimum borrow size and will need to be at vaultAccount.fCash==0.  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L52-L62  function setVaultAccount(VaultAccount memory vaultAccount, VaultConfig memory vaultConfig) internal {  mapping(address => mapping(address => VaultAccountStorage)) storage store = LibStorage  .getVaultAccount();  VaultAccountStorage storage s = store[vaultAccount.account][vaultConfig.vault];  // The temporary cash balance must be cleared to zero by the end of the transaction  require(vaultAccount.tempCashBalance == 0); // dev: cash balance not cleared  // An account must maintain a minimum borrow size in order to enter the vault. If the account  // wants to exit under the minimum borrow size it must fully exit so that we do not have dust  // accounts that become insolvent.  require(vaultAccount.fCash == 0 || vaultConfig.minAccountBorrowSize <= vaultAccount.fCash.neg(), \"Min Borrow\");  The case where vaultAccount.fCash>0 is taken care of by taking any extra repaid value and assigning it to the protocol, zeroing out the account s balances:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L293  if (vaultAccount.fCash > 0) vaultAccount.fCash = 0;  The case where vaultAccount.fCash < 0 is however not addressed, and instead the process will revert. This will occur whenever the vaultShareValue discounted with the liquidation rate is less than the fCash debt after all the conversions between external and underlying accounting. So, whenever the below is true, the account will not be liquidate-able. fCash>vaultShareValue/liquidationRate  This is an issue because the account is still technically solvent even though it is undercollateralized, but the current implementation would simply revert until the account is entirely insolvent (still without liquidation options) or its balances are restored enough to be liquidated fully.  Consider implementing a dynamic liquidation rate that becomes smaller the closer the account is to insolvency, thereby encouraging liquidators to promptly liquidate the accounts.  6 Strategy Vaults  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "6.1 Strategy vault swaps can be frontrun ", "body": "  Resolution  Acknowledged with a note from the Notional s team:  This is a large part of the diligence process for writing strategies   Description  Some strategy vaults utilize borrowing one currency, swapping it for another, and then using the new currency somewhere to generate yield. For example, the CrossCurrencyfCash strategy vault could borrow USDC, swap it for DAI, and then deposit that DAI back into Notional if the DAI lending interest rates are greater than USDC borrowing interest rates. However, during vault settlement the assets would need to be swapped back into the original borrow currency.  Since these vaults control the borrowed assets that go only into white-listed strategies, the Notional system allows users to borrow multiples of their posted collateral and claim the yield from a much larger position. As a result, these strategy vaults would likely have significant funds being borrowed and managed into these strategies.  However, as mentioned above, these strategies usually utilize a trading mechanism to swap borrowed currencies into whatever is required by the strategy, and these trades may be quite large. In fact, the BaseStrategyVault implementation contains functions that interact with Notional s trading module to assist with those swaps:  strategy-vaults/contracts/vaults/BaseStrategyVault.sol:L100-L127  /// @notice Can be used to delegate call to the TradingModule's implementation in order to execute  /// a trade.  function _executeTrade(  uint16 dexId,  Trade memory trade  ) internal returns (uint256 amountSold, uint256 amountBought) {  (bool success, bytes memory result) = nProxy(payable(address(TRADING_MODULE))).getImplementation()  .delegatecall(abi.encodeWithSelector(ITradingModule.executeTrade.selector, dexId, trade));  require(success);  (amountSold, amountBought) = abi.decode(result, (uint256, uint256));  /// @notice Can be used to delegate call to the TradingModule's implementation in order to execute  /// a trade.  function _executeTradeWithDynamicSlippage(  uint16 dexId,  Trade memory trade,  uint32 dynamicSlippageLimit  ) internal returns (uint256 amountSold, uint256 amountBought) {  (bool success, bytes memory result) = nProxy(payable(address(TRADING_MODULE))).getImplementation()  .delegatecall(abi.encodeWithSelector(  ITradingModule.executeTradeWithDynamicSlippage.selector,  dexId, trade, dynamicSlippageLimit  );  require(success);  (amountSold, amountBought) = abi.decode(result, (uint256, uint256));  Although some strategies may manage stablecoin <-> stablecoin swaps that typically would incur low slippage, large size trades could still suffer from low on-chain liquidity and end up getting frontrun and  sandwiched  by MEV bots or other actors, thereby extracting maximum amount from the strategy vault swaps as slippage permits. This could be especially significant during vaults  settlements, that can be initiated by anyone, as lending currencies may be swapped in large batches and not do it on a per-account basis. For example with the CrossCurrencyfCash vault, it can only enter settlement if all strategy tokens (lending currency in this case) are gone and swapped back into the borrow currency:  strategy-vaults/contracts/vaults/CrossCurrencyfCashVault.sol:L141-L143  if (vaultState.totalStrategyTokens == 0) {  NOTIONAL.settleVault(address(this), maturity);  As a result, in addition to the risk of stablecoins  getting off-peg, unfavorable market liquidity conditions and arbitrage-seeking actors could eat into the profits generated by this strategy as per the maximum allowed slippage. However, during settlement the strategy vaults don t have the luxury of waiting for the right conditions to perform the trade as the borrows need to repaid at their maturities.  So, the profitability of the vaults, and therefore users, could suffer due to potential low market liquidity allowing high slippage and risks of being frontrun with the chosen strategy vaults  currencies.  Recommendation  Ensure that the currencies chosen to generate yield in the strategy vaults have sufficient market liquidity on exchanges allowing for low slippage swaps.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "6.2 Cross currency strategy should not have same lend and borrow currencies ", "body": "  Description  Cross currency strategy currently takes lend and borrow currencies as the initialization arguments. Due to the way strategy and TradingModule are implemented, the strategy will not operate correctly if lend and borrow currencies are the same. Despite those arguments being passed exclusively by the Notional team, there is still a possibility of incorrect arguments being used.  Examples  strategy-vaults/contracts/vaults/CrossCurrencyfCashVault.sol:L77-L82  function initialize(  string memory name_,  uint16 borrowCurrencyId_,  uint16 lendCurrencyId_,  uint64 settlementSlippageLimit_  ) external initializer {  Recommendation  We suggest adding a require check in the initialization function of the CrossCurrencyfCashVault.sol that will ensure that lend and borrow currencies are different.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "6.1 in3-server - amplified DDoS on incubed requests on proof with signature    ", "body": "  Resolution                           Mitigated by adding   merge_requests/101. The full extent of this fix is outside the scope of this audit.  Description  It is possible for a client to send a request to each node of the network to request a signature with proof for every other node in the network. This can result in DDoSing the network as there are no costs for the client to request this and client can send the same request to all the nodes in the network, resulting in n^2 requests.  Examples  Client asks each node for in3_nodeList to get all the signer addresses, this could also be done using NodeRegistry contract  Client asks each node for a proof with signature, e.g.:  \"jsonrpc\": \"2.0\",  \"id\": 2,  \"method\": \"eth_getTransactionByHash\",  \"params\": [\"0xf84cfb78971ebd940d7e4375b077244e93db2c3f88443bb93c561812cfed055c\"],  \"in3\": {  \"chainId\": \"0x1\",  \"verification\": \"proofWithSignature\",  \"signatures\":[\"0x784bfa9eb182C3a02DbeB5285e3dBa92d717E07a\", ALL OTHER SIGNERS HERE]  All the nodes are now sending requests to each other with signature required which is an expensive computation. This can go on for more transactions (or blocks, or other Eth_ requests) and can result in DDoS of the network.  Recommendation  Limit the number of signers in proof with signature requests. Also exclude self.signer from the list. This combined with the remediation of issue 6.6 can partially mitigate the attack vector.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.2 BlockProof - Node conviction race condition may trick all but one node into losing funds    ", "body": "  Resolution  Mitigated by:  80bb6ecf by checking if blockhash exists and prevent an overwrite, saving gas  Client will blacklist the server if the signature is missing, has a wrong signer or is invalid.  6cc0dbc0 Removing nodes from local available nodes list when the server detects wrong responses  Other commits to mitigate the mentioned vulnerable scenarios  With the new handling, the client will not call convict immediately (as this could be exploited again). Instead, the client will do the calculation whether it s worth convicting the server before even calling convict.  It should be noted that the changes are scattered and modified in the final source code, and this behaviour of IN3-server code is outside the scope of this audit.  Description  TLDR; One node can force all other nodes to convict a specific malicious signer controlled by the attacker and spend gas on something they are not going to be rewarded for. The attacker loses deposit but all other nodes that try to convict and recreate in the same block will lose the fees less or equal to deposit/2. Another variant forces the same node to recreate the blockheaders multiple times within the same block as the node does not check if it is already convicting/recreating blockheaders.  Nodes can request various types of proofs from other nodes. For example, if a node requests a proof when calling one of the eth_getBlock* methods, the in3-server s method handleBlock will be called. The request should contain a list of addresses registered to the NodeRegistry that are requested to sign the blockhash.  code/in3-server/src/modules/eth/EthHandler.ts:L105-L112  // handle special jspn-rpc  if (request.in3.verification.startsWith('proof'))  switch (request.method) {  case 'eth_getBlockByNumber':  case 'eth_getBlockByHash':  case 'eth_getBlockTransactionCountByHash':  case 'eth_getBlockTransactionCountByNumber':  return handleBlock(this, request)  in3-server will subsequently reach out to it s connected blockchain node execute the eth_getBlock* call to get the block data. If the block data is available the in3-server, it will try to collect signatures from the nodes that signature was requested from (request.in3.signatures, collectSignatures())  code/in3-server/src/modules/eth/proof.ts:L237-L243  // create the proof  response.in3 = {  proof: {  type: 'blockProof',  signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)  If the node does not find the address it will throw an exception. Note that if this exception is not caught it will actually allow someone to boot nodes off the network - which is critical.  code/in3-server/src/chains/signatures.ts:L58-L60  const config = nodes.nodes.find(_ => _.address.toLowerCase() === adr.toLowerCase())  if (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')  If the address is valid and existent in the NodeRegistry the in3-node will ask the node to sign the blockhash of the requested blocknumber:  code/in3-server/src/chains/signatures.ts:L69-L84  // send the sign-request  let response: RPCResponse  try {  response = (blocksToRequest.length  ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3_sign', params: blocksToRequest })  : { result: [] }) as RPCResponse  if (response.error) {  //throw new Error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(_ => _.blockNumber).join() + ':' + response.error)  logger.error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(_ => _.blockNumber).join() + ':' + response.error)  return null  } catch (error) {  logger.error(error.toString())  return null  For all the signed blockhashes that have been returned the in3-server will subsequently check if one of the nodes provided a wrong blockhash.  We note that nodes might:  decided to not follow the in_3sign request and just not provide a signed response  a node might sign with a different key  a node might sign a different blockheader  a node might sign a previous blocknumber  In all these cases, the node will not be convicted, even though it was able to request other nodes to perform work.  If another node signed a wrong blockhash the in3-server will automatically try to convict it. If the block is within the most recent 255 it will directly call convict() on the NodeRegistry (takes less gas). if it is an older block, it will try to recreate the blockchain in the RlockhashRegistry (takes more gas).  code/in3-server/src/chains/signatures.ts:L128-L152  const convictSignature: Buffer = keccak(Buffer.concat([bytes32(s.blockHash), address(singingNode.address), toBuffer(s.v, 1), bytes32(s.r), bytes32(s.s)]))  if (diffBlocks < 255) {  await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {  privateKey: handler.config.privateKey,  gas: 500000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  handler.watcher.futureConvicts.push({  convictBlockNumber: latestBlockNumber,  signer: singingNode.address,  wrongBlockHash: s.blockHash,  wrongBlockNumber: s.block,  v: s.v,  r: s.r,  s: s.s,  recreationDone: true  })  else {  await handleRecreation(handler, nodes, singingNode, s, diffBlocks)  The recreation and convict is only done if it is profitable for the node. (Note the issue mentioned in issue 6.13)  code/in3-server/src/chains/signatures.ts:L209-L213  const costPerBlock = 86412400000000  const blocksMissing = latestSS - s.block  const costs = blocksMissing * costPerBlock * 1.25  if (costs > (deposit / 2)) {  A malicious node can exploit the hardcoded profit economics and the fact that in3-server implementation will try to auto-convict nodes in the following scenario:  malicious node requests a blockproof with an eth_getBlock* call from the victim node (in3-server) for a block that is not in the most recent 256 blocks (to maximize effort for the node). This equals to spending more gas in order to convict the node (costs <= (deposit / 2)).  the malicious node prepares the BlockhashRegistry to contain a blockhash that would maximize the gas needed to convict the malicious node (can be calculated offline; must fulfill costs <= (deposit /2).  with the blockproof request the malicious node asks the in3-server to get the signature from a specific signer. The signer will also be malicious and is going to sign a wrong blockhash with a valid signature.  the malicious signer is going to lose it s deposit but the deposit also incentivizes other nodes to spend gas on the conviction process. The higher the deposit, the more an in3-server is willing to spend on the conviction.  In this scenario one malicious node tries to trick another node into convicting a malicious signer while having to spend the maximum amount of gas to make it profitable for the node.  The problem is, that the malicious node can ask multiple (or even all other nodes in the registry) to provide a blockproof and ask the malicious signer for a signed blockhash. All nodes will come to the conclusion that the signer returned an invalid hash and will try to convict the node. They will try to recreate the blockchain in the BlockhashRegistry for a barely profitable scenario. Since in3-nodes do not monitor the tx-pool they will not know that other nodes are already trying to convict the node. All nodes are going to spend gas on recreating the same blockchain in the BlockhashRegistry leading to all but the first transaction in the block to lose funds (up to deposit/2 based on the hardcoded costPerBlock)  Another variant of the same issue is that nodes do not check if they already convicted another node (or recreated blockheaders). An attacker can therefore force a specific node to convict a malicious node multiple times before the nodes transactions are actually in a block as the nodes does not check if it is already convicting that node. The node might lose gas on the recreation/conviction process multiple times.  Recommendation  To reduce the impact of multiple nodes trying to update the blockhashRegistry at the same time and avoid nodes losing gas by recreating the same blocks over and over again, the BlockhashRegistry should require that the target blockhash for the blocknumber does not yet exist in the registry (similar to the issue mentioned in https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/24).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.3 NodeRegistry Front-running attack on convict()    ", "body": "  Resolution  Blocknumber is removed from convict function, which removes any signal for an attacker in the scenario provided. However, the order of the transactions to convict a wrong signed hash is necessary to prevent any front-running attacks:  Convict(_Blockhash)  recreate Blockheaders  RevealConvict (minimum 2 blocks after convict but as soon as recreateBlockheaders is confirmed)  The fixes were introduced in ecf2c6a6 and f4250c9a, although later on NodeRegistry contract was split in two other contracts NodeRegistryLogic and NodeRegistryData and further changes were done in the conviction flow in different commits.  Description  convict(uint _blockNumber, bytes32 _hash) and revealConvict() are designed to prevent front-running and they do so for the purpose they are designed for. However, if the malicious node, is still sending out the wrong blockhash for the convicted block, anyone seeing the initial convict transaction, can check the convicted blocknumber with the nodes and send his own revealConvict before the original sender.  The original sender will be the one updating the block headers recreateBlockheaders(_blockNumber, _blockheaders), and the attacker can just watch for the update headers to perform this attack.  Recommendation  For the first attack vector, remove the blocknumber from the convict(uint _blockNumber, bytes32 _hash) inputs and just use the hash.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.4 NodeRegistry - URL can be arbitrary dns resolvable names, IP s and even localhost or private subnets    ", "body": "  Resolution  This issue has been addressed with the following commits:  4c93a10f adding 48 hours delay in the server code before they communicate with the newly registered nodes.  merge_requests/111 adding a whole new smart contract to the IN3 system, IN3WhiteList.sol, and supporting code in the server.  issues/94 To prevent attacker to use nodes as a DoS network, a DNS record verification is discussed to be implemented.  It is a design decision to base the Node registry on URLs (DNS resolvable names). This has the implications outlined in this issue and they cannot easily be mitigated. Adding a delay until nodes can be used after registration only delays the problem. Assuming that an entity curates the registry or a whitelist is in place centralizes the system. Adding DNS record verification still allows an owner of a DNS entry to point its name to any IP address they would like it to point to. It certainly makes it harder to add RPC URLs with DNS names that are not in control of the attacker but it also adds a whole lot more complexity to the system (including manual steps performed by the node operator). In the end, the system allows IP based URLs in the registry which cannot be used for DNS validation.  Note that the server code changes, and the new smart contract IN3WhiteList.sol are outside the scope of the original audit. We strongly recommend to reduce complexity and audit the final codebase before mainnet deployment.  Description  As outlined in issue 6.9 the NodeRegistry allows anyone to register nodes with arbitrary URLs. The url is then used by in3-server or clients to connect to other nodes in the system. Signers can only be convicted if they sign wrong blockhashes. However, if they never provide any signatures they can stay in the registry for as long as they want and sabotage the network. The Registry implements an admin functionality that is available for the first year to remove misbehaving nodes (or spam entries) from the Registry. However, this is insufficient as an attacker might just re-register nodes after the minimum timeout they specify or spend some more finneys on registering more nodes. Depending on the eth-price this will be more or less profitable.  From an attackers perspective the NodeRegistry is a good source of information for reconnaissance, allows to de-anonymize and profile nodes based on dns entries or netblocks or responses to in3_stats (https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/49), makes a good list of target for DoS attacks on the system or makes it easy to exploit nodes for certain yet unknown security vulnerabilities.  Since nodes and potentially clients (not in scope) do not validate the rpc URL received from the NodeRegistry they will try to connect to whatever is stored in a nodes url entry.  code/in3-server/src/chains/signatures.ts:L58-L75  const config = nodes.nodes.find(_ => _.address.toLowerCase() === adr.toLowerCase())  if (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')  // get cache signatures and remaining blocks that have no signatures  const cachedSignatures: Signature[] = []  const blocksToRequest = blocks.filter(b => {  const s = signatureCaches.get(b.hash) && false  return s ? cachedSignatures.push(s) * 0 : true  })  // send the sign-request  let response: RPCResponse  try {  response = (blocksToRequest.length  ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3_sign', params: blocksToRequest })  : { result: [] }) as RPCResponse  if (response.error) {  This allows for a wide range of attacks not limited to:  An attacker might register a node with an empty or invalid URL. The in3-server does not validate the URL and therefore will attempt to connect to the invalid URL, spending resources (cpu, file-descriptors, ..) to find out that it is invalid.  An attacker might register a node with a URL that is pointing to another node s rpc endpoint and specify weights that suggest that it is capable of service a lot of requests to draw more traffic towards that node in an attempt to cause a DoS situation.  An attacker might register a node for a http/https website at any port in an extortion attempt directed to website owners. The incubed network nodes will have to learn themselves that the URL is invalid and they will at least attempt to connect the website once.  An attacker might update the node information in the NodeRegistry for a specific node every block, providing a new url (or a slightly different URLs issue 6.9) to avoid client/node URL blacklists.  An attacker might provide IP addresses instead of DNS resolvable names with the url in an attempt to draw traffic to targets, avoiding canonicalization and blacklisting features.  An attacker might provide a URL that points to private IP netblocks for IPv4 or IPv6 in various formats. Combined with the ability to ask another node to connect to an attacker defined url (via blockproof, signatures[] -> signer_address -> signer.url) this might allow an attacker to enumerate services in the LAN of node operators.  An attacker might provide the loopback IPv4, IPv6 or resolvable name as the URL in an attempt to make the node connect to local loopback services (service discovery, bypassing authentication for some local running services - however this is very limited to the requests nodes may execute).  URLs may be provided in various formats: resolvable dns names, IPv4, IPv6 and depending on the http handler implementation even in Decimal, Hex or Octal form (i.e. http://2130706433/)  A valid DNS resolvable name might point to a localhost or private IP netblock.  Since none of the rpc endpoints provide signatures they cannot be convicted or removed (unless the unregisterKey does it within the first year. However, that will not solve the problem that someone can re-register the same URLs over and over again)  Recommendation  It is a fundamental design decision of the system architecture to allow rpc urls in the Node Registry, therefore this issue can only be partially mitigated unless the system design is reworked. It is therefore suggested to add checks to both the registry contract (coarse validation to avoid adding invalid urls) and node implementations (rigorous validation of URL s and resolved IP addresses) and filter out any potentially harmful destinations.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.5 Malicious clients can use forks or reorgs to convict honest nodes   ", "body": "  Resolution  Default value for past signed blocks is changed to 10 blocks. Slockit plans to use their off-chain channels to notify clients for planned forks. They also looking into using fork oracles in the future releases to detect planned hardforks to mitigate risks.  Description  In case of reorgs it is possible to have more than 6 blocks in a node that gets replaced by a new longer chain. Also for forks, such as upcoming Istanbul fork, it s common to have some nodes taking some time to update and they will be in the wrong chain for the time being. In both cases, in3-nodes are prone to sign blocks that are considered invalid in the main chain. Malicious nodes can catch these instances and convict the honest users in the main chain to get 50% of their deposits.  Recommendation  No perfect solution comes to mind at this time. One possible mitigation method for forks could be to disable the network on the time of the fork but this is most certainly going to be a threat to the system itself.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.6 in3-server - should protect itself from abusive clients    ", "body": "  Resolution  Slockit implemented their own DOS protection for incubed server in merge_requests/99. The variant of this implementation adds more complexity to the code base. The benchmark and testing of the new DOS protection is not in scope for this audit.  The incubed server has now an additional DOS-Protection build in. Here we first estimate a Weight of such a request and add them together for all incoming requests per IP of the client per Minute. Since we estimate the execution, we can prevent a client running DOS-Attacks from the same IP with heavy requests (such as eth_getLogs)  Description  The in3-node implementation should provide features for client request throttling to avoid that a client can consume most of the nodes resources by causing a lot of resource intensive requests.  This is a general problem to the system which is designed to make sure that low resource clients can verify blockchain properties. What this means is that almost all of the client requests are very lightweight. Clients can request nodes to sign data for them. A sign request involves cryptographic operations and a http-rpc request to a back-end blockchain node. The imbalance is clearly visible in the case of blockProofs where a client may request another node to interact with a smart contract (NodeRegistry) and ask other nodes to sign blockhashes. All other nodes will have to get the requested block data from their local blockchain nodes and the incubed node requesting the signatures will have to wait for all responses. The client instead only has to send out that request once and may just leave that tcp connection open. It might even consume more resources from a specific node by requesting the same signatures again and again not even waiting for a response but causing a lot of work on the node that has to collect all the signatures. This combined with unbound requests for signatures or other properties can easily be exploited by a powerful client implementation with a mission to stall the whole incubed network.  Recommendation  According to the threat model outlines a general DDoS scenario specific to rpcUrls. It discusses that the nodes are themselves responsible for DDoS protection. However, DDoS protection is a multi-layer approach and it is highly unlikely that every node-operator will hide their nodes behind a DDoS CDN like cloudflare. We therefore suggest to also build in strict limitations for clients that can be checked in code. Similar to checkPerformanceLimits which is just checking for some specific it is suggested to implement a multi-layer throttling mechanism that prevents nodes from being abused by single clients. Methods must be designed with (D)DoS scenarios in mind to avoid that third parties are abusing the network for DDoS campaigns or trying to DoS the incubed network.  code/in3-server/src/modules/eth/EthHandler.ts:L74-L91  private checkPerformanceLimits(request: RPCRequest) {  if (request.method === 'eth_call') {  if (!request.params || request.params.length < 2) throw new Error('eth_call must have a transaction and a block as parameters')  const tx = request.params as TxRequest  if (!tx || (tx.gas && toNumber(tx.gas) > 10000000)) throw new Error('eth_call with a gaslimit > 10M are not allowed')  else if (request.method === 'eth_getLogs') {  if (!request.params || request.params.length < 1) throw new Error('eth_getLogs must have a filter as parameter')  const filter: LogFilter = request.params[0]  let toB = filter && filter.toBlock  if (toB === 'latest' || toB === 'pending' || !toB) toB = this.watcher && this.watcher.block && this.watcher.block.number  let fromB = toB && filter && filter.fromBlock  if (fromB === 'earliest') fromB = 1;  const range = fromB && (toNumber(toB) - toNumber(fromB))  if (range > (request.in3.verification.startsWith('proof') ? 1000 : 10000))  throw new Error('eth_getLogs for a range of ' + range + ' blocks is not allowed. limits: with proof: 1000, without 10000 ')  implement request throttling per client  implement caching mechanism for similar requests if it is expected that the same response is to be delivered multiple times  implement general performance limits and reject further requests if the node is close to exhausting its resources (soft DoS)  make sure the node does not exhaust the systems resources  implement throttling per request method  design methods to prevent (D)DoS in the first place. Methods that allow a client to send one request that causes a node to perform multiple client controlled requests must be avoided or at least bound and throttled (issue 6.7, https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50).  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.7 in3-server - DoS on in3.sign and other requests    ", "body": "  Resolution  Similar to issue 6.1, Mitigated by adding maxBlocksSigned and maxSignatures for requests of any client.  The Numbers of signatures a client can ask to fetch is now limited to maxSignatures which defaults to 5  in merge_requests/101. The full extent of this fix is outside the scope of this audit.  We have limited the number of block you can ask to sign in the in3_sign-request. The default is 10, because this function is also used for eth_getLogs to provide proof for all events. This limit will also limit the result of logs returned to include only max 10 different blocks.  Description  It is free for the client to ask the nodes to sign block hashes (and also other requests). in3.sign([{\"blockNumber\": 123}]) Takes an array of objects that will result in multiple requests in the node. This sample request has (at least) two internal requests, one eth_getBlockByNumber and signing the block hash.  These requests can be continuously sent out to clients and result in using computation power of the nodes without any expense from the client.  Examples  Request to get and sign the first 200 blocks:  web3.manager.request_blocking(\"in3_sign\", [{'blockNumber':i} for i in range(200)])  Recommendation  Limit the number of blocks (input), or do not accept arrays for input.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.8 in3-server - key management   Pending", "body": "  Resolution  The breakdown of the fixes addressed with git.slock.it/PR/13 are as follows:  Keys should never be stored or accepted in plaintext format Keys should only be accepted in an encrypted and protected format  The private key in code/in3-server/config.json has been removed. The repository still contains private keys at least in the following locations:  package.json  vscode/launch.json  example_docker-compose.yml  Note that private keys indexed by a git repository can be restored from the repository history.  The following statement has been provided to address this issue:  We have removed all examples and usage of plain private keys and replaced them with json-keystore files. Also in the documentation we added warnings on how to deal with keys, especially with hints to the bash history or enviroment  A single key should be used for only one purpose. Keys should not be shared.  The following statement has been provided to address this issue:  This is why we seperated the owner and signer-key. This way you can use a multisig to securly protect the owner-key. The signer-key is used to sign blocks (and convict) and is not able to do anything else (not even changing its own url)  The application should support developers in understanding where cryptographic keys are stored within the application as well as in which memory regions they might be accessible for other applications  Addressed by wrapping the private key in an object that stores the key in encrypted form and only decrypts it when signing. The key is cleared after usage. The IN3-server still allows raw private keys to be configured. A warning is printed if that is the case. The loaded raw private key is temporarily assigned to a local variable and not explicitly cleared by the method.  While we used to keep the unlocked key as part of the config, we have now removed the key from the config and store them in a special signer-function. https://git.slock.it/in3/ts/in3-server/merge_requests/113  Keys should be protected in memory and only decrypted for the duration of time they are actively used. Keys should not be stored with the applications source-code repository  see previous remediation note.  After unlocking the signer key, we encrypt it again and keep it encrypted only decrypting it when signing. This way the raw private key only exist for a very short time in memory and will be filled with 0 right after. ( https://git.slock.it/in3/ts/in3-server/merge_requests/113/diffs#653b04fa41e35b55181776b9f14620b661cff64c_54_73 )  Use standard libraries for cryptographic operations  The following statement has been provided to address this issue  We are using ethereumjs-libs.  Use the system keystore and API to sign and avoid to store key material at all  The following statement has been provided to address this issue  We are looking into using different signer-apis, even supporting hardware-modules like HSMs. But this may happen in future releases.  The application should store the keys eth-address (util.getAddress()) instead of re-calculating it multiple times from the private key.  Fixed by generating the address for a private key once and storing it in a private key wrapper object.  Do not leak credentials and key material in debug-mode, to local log-output or external log aggregators.  txArgs still contains a field privateKey as outlined in the issue description. However, this privateKey now represents the wrapper object noted in a previous comment which only provides access to the ETH address generated from the raw private key.  The following statement has been provided to address this issue:  since the private key and the passphrase are actually deleted from the config, logoutputs or even debug will not be able to leak this information.  Description  Secure and efficient key management is a challenge for any cryptographic system. Incubed nodes for example require an account on the ethereum blockchain to actively participate in the incubed network. The account and therefore a private-key is used to sign transactions on the ethereum blockchain and to provide signed proofs to other in3-nodes.  This means that an attacker that is able to discover the keys used by an in3-server by any mechanism may be able to impersonate that node, steal the nodes funds or sign wrong data on behalf of the node which might also lead to a loss of funds.  The private key for the in3-server can be specified in a configuration file called config.json residing in the program working dir. Settings from the config.json can be overridden via command-line options. The application keeps configuration parameters available internally in an IN3RPCConfig object and passes this object as an initialization parameter to other objects.  The key can either be provided in plaintext as a hex-string starting with 0x or within an ethereum keystore format compatible protected keystore file. Either way it is provided it will be held in plaintext in the object.  The application accepts plaintext private keys and the keys are stored unprotected in the applications memory in JavaScript objects. The in3-server might even re-use the nodes private key which may weaken the security provided by the node. The repository leaks a series of presumably  test private keys  and the default config file already comes with a private key set that might be shared across unvary users that fail to override it.  code/in3-server/config.json:L1-L4  \"privateKey\": \"0xc858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3\",  \"rpcUrl\": \"http://rpc-kovan.slock.it\"  code/in3-server/package.json:L20-L31  \"docker-run\": \"docker run -p 8500:8500 docker.slock.it/slockit/in3-server:latest --privateKey=0x3858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3 --chain=0x2a --rpcUrl=https://kovan.infura.io/HVtVmCIHVgqHGUgihfhX --minBlockHeight=6 --registry=0x013b82355a066A31427df3140C5326cdE9c64e3A --persistentFile=false --logging-host=logs7.papertrailapp.com --logging-name=Papertrail --logging-port=30571 --logging-type=winston-papertrail\",  \"docker-setup\": \"docker run -p 8500:8500 slockit/in3-server:latest --privateKey=0x3858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3 --chain=0x2a --rpcUrl=https://kovan.infura.io/HVtVmCIHVgqHGUgihfhX --minBlockHeight=6 --registry=0x013b82355a066A31427df3140C5326cdE9c64e3A --persistentFile=false --autoRegistry-url=https://in3.slock.it/kovan1 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=1\",  \"local\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xD231FCF9349A296F555A060A619235F88650BBA795E5907CFD7F5442876250E4 --chain=0x2a --rpcUrl=https://rpc.slock.it/kovan --minBlockHeight=6 --registry=0x27a37a1210df14f7e058393d026e2fb53b7cf8c1 --persistentFile=false\",  \"ipfs\": \"docker run -d -p 5001:5001 jbenet/go-ipfs  daemon --offline\",  \"linkIn3\": \"cd node_modules; rm -rf in3; ln -s ../../in3 in3; cd ..\",  \"lint:solium\": \"node node_modules/ethlint/bin/solium.js -d contracts/\",  \"lint:solium:fix\": \"node node_modules/ethlint/bin/solium.js -d contracts/ --fix\",  \"lint:solhint\": \"node node_modules/solhint/solhint.js \\\"contracts/**/*.sol\\\" -w 0\",  \"local-env\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0x9e53e6933d69a28a737943e227ad013c7489e366f33281d350c77f089d8411a6 --chain=0x111 --rpcUrl=http://localhost:8545 --minBlockHeight=6 --registry=0x31636f91297C14A8f1E7Ac271f17947D6A5cE098 --persistentFile=false --autoRegistry-url=http://127.0.0.1:8500 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=0\",  \"local-env2\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x111 --rpcUrl=http://localhost:8545 --minBlockHeight=6 --registry=0x31636f91297C14A8f1E7Ac271f17947D6A5cE098 --persistentFile=false --autoRegistry-url=http://127.0.0.1:8501 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=0\",  \"local-env3\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x5 --rpcUrl=https://rpc.slock.it/goerli --minBlockHeight=6 --registry=0x85613723dB1Bc29f332A37EeF10b61F8a4225c7e --persistentFile=false\",  \"local-env4\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x2a --rpcUrl=https://rpc.slock.it/kovan --minBlockHeight=6 --registry=0x27a37a1210df14f7e058393d026e2fb53b7cf8c1 --persistentFile=false\"  The private key is also passed as arguments to other functions. In error cases these may leak the private key to log interfaces or remote log aggregation instances (sentry). See txargs.privateKey in the example below:  code/in3-server/src/util/tx.ts:L100-L100  const key = toBuffer(txargs.privateKey)  code/in3-server/src/util/tx.ts:L134-L140  const txHash = await transport.handle(url, {  jsonrpc: '2.0',  id: idCount++,  method: 'eth_sendRawTransaction',  params: [toHex(tx.serialize())]  }).then((_: RPCResponse) => _.error ? Promise.reject(new SentryError('Error sending tx', 'tx_error', 'Error sending the tx ' + JSON.stringify(txargs) + ':' + JSON.stringify(_.error))) as any : _.result + '')  Recommendation  Keys should never be stored or accepted in plaintext format.  Keys should not be stored in plaintext on the file-system as they might easily be exposed to other users. Credentials on the file-system must be tightly restricted by access control. Keys should not be provided as plaintext via environment variables as this might make them available to other processes sharing the same environment (child-processes, e.g. same shell session) Keys should not be provided as plaintext via command-line arguments as they might persist in the shell s command history or might be available to privileged system accounts that can query other processes startup parameters.  Keys should only be accepted in an encrypted and protected format.  A single key should be used for only one purpose. Keys should not be shared.  The use of the same key for two different cryptographic processes may weaken the security provided by one or both of the processes. The use of the same key for two different applications may weaken the security provided by one or both of the applications. Limiting the use of a key limits the damage that could be done if the key is compromised. Node owners keys should not be re-used as signer keys.  The application should support developers in understanding where cryptographic keys are stored within the application as well as in which memory regions they might be accessible for other applications.  Keys should be protected in memory and only decrypted for the duration of time they are actively used.  Keys should not be stored with the applications source-code repository.  Use standard libraries for cryptographic operations.  Use the system keystore and API to sign and avoid to store key material at all.  The application should store the keys eth-address (util.getAddress()) instead of re-calculating it multiple times from the private key.  Do not leak credentials and key material in debug-mode, to local log-output or external log aggregators.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.9 NodeRegistry - Multiple nodes can share slightly different RPC URL    ", "body": "  Resolution                           Same mitigation as   issue 6.4.  Description  One of the requirements for Node registration is to have a unique URL which is not already used by a different owner. The uniqueness check is done by hashing the provided _url and checking if someone already registered with that hash of _url.  However, byte-equality checks (via hashing in this case) to enforce uniqueness will not work for URLs. For example, while the following URLs are not equal and will result in different urlHashes they can logically be the same end-point:  https://some-server.com/in3-rpc  https://some-server.com:443/in3-rpc  https://some-server.com/in3-rpc/  https://some-server.com/in3-rpc///  https://some-server.com/in3-rpc?something  https://some-server.com/in3-rpc?something&something  https://www.some-server.com/in3-rpc?something  (if www resolves to the same ip)  code/in3-contracts/contracts/NodeRegistry.sol:L547-L553  bytes32 urlHash = keccak256(bytes(_url));  // make sure this url and also this owner was not registered before.  // solium-disable-next-line  require(!urlIndex[urlHash].used && signerIndex[_signer].stage == Stages.NotInUse,  \"a node with the same url or signer is already registered\");  This leads to the following attack vectors:  A user signs up multiple nodes that resolve to the same end-point (URL). A minimum deposit of 0.01 ether is required for each registration. Registering multiple nodes for the same end-point might allow an attacker to increase their chance of being picked to provide proofs. Registering multiple nodes requires unique signer addresses per node.  Also one node can have multiple accounts, hence one node can have slightly different URL and different accounts as the signers.  DoS - A user might register nodes for URLs that do not serve in3-clients in an attempt to DDoS e.g. in an attempt to extort web-site operators. This is kind of a reflection attack where nodes will request other nodes from the contract and try to contact them over RPC. Since it is http-rpc it will consume resources on the receiving end.  DoS - A user might register Nodes with RPC URLs of other nodes, manipulating weights to cause more traffic than the node can actually handle. Nodes will try to communicate with that node. If no proof is requested the node will not even know that someone else signed up other nodes with their RPC URL to cause problems. If they request proof the original signer will return a signed proof and the node will fail due to a signature mismatch. However, the node cannot be convicted and therefore forced to lose the deposit as conviction is bound the signer and the block was not signed by the rogue node entry. There will be no way to remove the node from the registry other than the admin functionality.  Recommendation  Canonicalize URLs, but that will not completely prevent someone from registering nodes for other end-points or websites. Nodes can be removed by an admin in the first year but not after that. Rogue owners cannot be prevented from registering random nodes with high weights and minimum deposit. They cannot be convicted as they do not serve proofs. Rogue owners can still unregister to receive their deposit after messing with the system.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.10 in3-server - should enforce safe settings for minBlockHeight   ", "body": "  Resolution  The default block is changed to 10 and minBlockHeight is added to the registry (as part of the properties) in 8c72633e, but allow the user to define a minBlockHeight lower than this number. The client is responsible to review the settings depending on how secure they want their nodes to be.  Client response:  We have discussed this, but decided to keep it flexible. This means:  We have put the minBlockHeight into the registry (as part of the properties). Because these properties indicate the limit and capabilities of the node and give the client a chance to filter out nodes if they don t match the requirements. So each client is able to filter out node who are not willing to take the risk and sign for example latest-6. Of course these nodes will most likely only store a low deposit ( you can not have a signature of a young block and a high deposit), but if you need a high security the nodes with a deposit will propably wait at least 10 or more blocks. In order to protect the owner of a node of using insecure settings, we will use our wizard to check the deposit and minBlockHeights and warn or educate the user. The reason why this flexibility is important, is because there use cases where dapps will not accept the let user wait 10 blocks before confirming a transaction. If the dapp developer needs a signature of a younger block, he will need to live with the fact, that he won t be able to find a high deposit to secure it.  We also changed the default to 10 blocks, but allow the user to define a minBlockHeight lower than this number. In this case the node would write a warning in the logfile, but still accepts the user configuration. This allows to use incubed also on different chains other than the mainnet.  The safeMinBlockHeight is now dependend on different chains, which is one single function, so we don t have hardcoded values in different places anymore.  Description  A node that is signing wrong blockhashes might get their deposit slashed from the registry. The entity that is convicting a node that signs a wrong blockhash is awarded half of the deposit.  A threat to this kind of system is that blocks might constantly be reorganized in the chain, especially with the latest block. Allowing a node to sign the latest block will definitely put the node s deposit at stake with every signature they provide.  A node can configure the minBlockHeight it is about to sign with a configurative option. The option defaults to a minBlockHeight of 6 in the default config:  code/in3-server/src/server/config.ts:L32-L32  minBlockHeight: 6,  And again in the signing function for blockheaders:  code/in3-server/src/chains/signatures.ts:L189-L189  const blockHeight = handler.config.minBlockHeight === undefined ? 6 : handler.config.minBlockHeight  handleSign will refuse to sign any block that is within the last 5 blocks. The 6th block will be signed.  code/in3-server/src/chains/signatures.ts:L190-L193  const tooYoungBlock = blockData.find(block => toNumber(blockNumber) - toNumber(block.number) < blockHeight)  if (tooYoungBlock)  throw new Error(' cannot sign for block ' + tooYoungBlock.number + ', because the blockHeight must be at least ' + blockHeight)  However, a user is not prevented from configuring an insecure minBlockHeight (e.g. 0) which will very likely lead to the loss of funds because the node will be signing the latest block.  Kraken requires at least 30 confirmation (abt. 6 minutes) until a transaction is confirmed. For Bitcoin it is said to be safe to wait more than 6 blocks (abt. 1 hr) for a transaction to be confirmed. ETC even underwent a  deep chain reorg that could have caused many nodes to lose their deposits. The  ethereum whitepaper defines an uncle that can be referenced in a block to have the following property:  Bitfinex requires a minimum of 10 confirmations. Some blockchain explorers and analytics tools also require a minimum of 10 confirmations. Scraped data from  https://etherscan.io/blocks_forked?ps=100 shows 3 forks of depth 3 since they started keeping records 115 days ago, and no forks deeper than 3. So some applications might legitimately pick a number somewhere between 5 and 20, trading some security for better UX. However, it should be re-evaluated whether the current default provides enough security to protect the nodes funds with a trade-off of lag to the network.  Given these values it is suggested to revalidate the default of a minBlockHeight of 6 in favor of a more secure depth to make sure that - with a default setting - nodes will not lose funds in case of re-orgs.  Recommendation  config.minBlockHeight should always be set to a sane value when loading the configuration. There should be no need to reset it to a hardcoded default value of 6 in handleSign. Do not hardcode the values in various places in the config.  normalize and sanitize the settings to make sure that after loading they are always valid and within reasonable bounds. the application should refuse to run with a minBlockHeader set to 0 as this is a guarantee for losing funds. Other nodes can enumerate nodes that are misconfigured (e.g. with minBlockHeight being 0) to request signatures just to convict them on micro-forks.  assume a secure default setting for every chain (note that this might be different for every chain). allow to override the value by the user. warn the user of less secure settings and do not allow to set settings that are obviously leading to the loss of funds.  re-evaluate the minBlockHeight of 6 for the ethereum blockchain and choose a koservative secure default.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.11 in3-server - rpc proof handler specification inconsistency    ", "body": "  Resolution                           Addressed with   https://git.slock.it/in3/ts/in3-server/issues/100. Checks for  Description  According to the specification incubed requests must specify whether they want to have a proof or not. There are three variants of proofs that can be requested:  never - no proof appended  proof - proof but no signed blockhashes  proofWithSignature- proof and a request to sign blockhashes from the list of addresses provided in signatures.  Note that the name signatures for the array of signers a blockhash signature is requested from is misleading. It is actually signer addresses as listed in the NodeRegistry and not signatures.  Following the in3-server we found at least one inconsistency (and suspect more) with the proof requested by a client. The graceful check for the existence of something starting with proof will pass proof and proofWithSignature but also any other proofXYZ to the blockproof handler.  code/in3-server/src/modules/eth/EthHandler.ts:L106-L112  if (request.in3.verification.startsWith('proof'))  switch (request.method) {  case 'eth_getBlockByNumber':  case 'eth_getBlockByHash':  case 'eth_getBlockTransactionCountByHash':  case 'eth_getBlockTransactionCountByNumber':  return handleBlock(this, request)  Following through handleBlock we cannot find any check for proofWithSignature. The string is not found in the whole codebase which also suggests it is not tested. However, the code assumes that because request.in3.signatures is not empty, signatures were requested. This is inconsistent with the specification and a protocol violation.  code/in3-server/src/modules/eth/proof.ts:L237-L244  // create the proof  response.in3 = {  proof: {  type: 'blockProof',  signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)  The same is valid for all other types of proofs. proofWithSignature is never checked and it is assumed that proofWithSignature was requested  just because request.in3.signatures is present non-empty.  The same is true for  never  which is actually never handled in code.  Recommendation  The protocol should be strictly enforced without allowing any ambiguities and unsharpness. Ambiguities and gracefulness in the protocol can lead to severe inconsistencies and encourage client authors to not strictly adhere to the protocol. This makes it hard to update and maintain the protocol in the future and may allow potential attackers enough freedom to exploit the protocol. Furthermore the specification must be kept up-to-date at all times. The specification is to lead development and code must always be verified against the specification.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.12 in3-server - hardcoded gas limit could result in failed transactions/requests    ", "body": "  Resolution                           Fixed by using web3   merge_requests/109 to dynamically price the gas according to the network state.  Description  There are many instances of hardcoded gas limit in in3-server that depending on the complexity of the transaction or gas cost changes in Ethereum could result in failed transactions.  Examples  convict():  code/in3-server/src/chains/signatures.ts:L132-L137  await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {  privateKey: handler.config.privateKey,  gas: 500000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  recreateBlockheaders():  code/in3-server/src/chains/signatures.ts:L275-L280  await callContract(handler.config.rpcUrl, blockHashRegistry, 'recreateBlockheaders(uint,bytes[])', [latestSS - diffBlock, txArray], {  privateKey: handler.config.privateKey,  gas: 8000000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  Other instances of hard coded gasLimit or gasPrice:  code/in3-server/src/modules/eth/EthHandler.ts:L78-L79  if (!tx || (tx.gas && toNumber(tx.gas) > 10000000)) throw new Error('eth_call with a gaslimit > 10M are not allowed')  Recommendation  Use web3 gas estimate instead. To be sure, there can be an additional gas added to the estimated value or max(HARDCODED_GAS, estimated_amount)  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.13 in3-server - handleRecreation tries to recreate blockchain if no block is available to recreate it from    ", "body": "  Resolution  Mitigated by 502b5528 by falling back to using the current block in case searchForAvailableBlock returns 0. Costs can be zero, but cannot be negative anymore.  The behaviour of the IN3-server code is outside the scope of this audit. However, while verifying the fixes for this specific issue it was observed that the watch.ts:handleConvict() relies on a static hardcoded cost calculation. We further note that the cost calculation formula has an error and is missing parentheses to avoid that costs can be zero. We did not see a reason for the costs not to be allowed to be zero. Furthermore, costs are calculated based on the difference of the conviction block to the latest block. Actual recreation costs can be less if there is an available block in blockhashRegistry to recreate it from that is other than the latest block.  Description  A node that wants to convict another node for false proof must update the BlockhashRegistry for signatures provided in blocks older than the most recent 256 blocks. Only when the smart contract is able to verify that the signed blockhash is wrong the convicting node will be able to receive half of its deposit.  The in3-server implements an automated mechanism to recreate blockhashes. It first searches for an existing blockhash within a range of blocks. If one is found and it is profitable (gas spend vs. amount awarded) the node will try to recreate the blockchain updating the registry.  The call to searchForAvailableBlock might return 0 (default) because no block is actually found within the range, this will cause costs to be negative and the code will proceed trying to convict the node even though it cannot work.  The call to searchForAvailableBlock might also return the convict block number (latestSS==s.block) in which case costs will be 0 and the code will still proceed trying to recreate the blockheaders and convict the node.  code/in3-server/src/chains/signatures.ts:L207-L231  const [, deposit, , , , , , ,] = await callContract(handler.config.rpcUrl, nodes.contract, 'nodes(uint):(string,uint,uint64,uint64,uint128,uint64,address,bytes32)', [toNumber(singingNode.index)])  const latestSS = toNumber((await callContract(handler.config.rpcUrl, blockHashRegistry, 'searchForAvailableBlock(uint,uint):(uint)', [s.block, diffBlocks]))[0])  const costPerBlock = 86412400000000  const blocksMissing = latestSS - s.block  const costs = blocksMissing * costPerBlock * 1.25  if (costs > (deposit / 2)) {  console.log(\"not worth it\")  //it's not worth it  return  else {  // it's worth convicting the server  const blockrequest = []  for (let i = 0; i < blocksMissing; i++) {  blockrequest.push({  jsonrpc: '2.0',  id: i + 1,  method: 'eth_getBlockByNumber', params: [  toHex(latestSS - i), false  })  Please note that certain parts of the code rely on hardcoded gas values. Gas economics might change with future versions of the evm and have to be re-validated with every version. It is also good practice to provide inline comments about how and on what base certain values were selected.  Recommendation  Verify that the call succeeds and returns valid values. Check if the block already exists in the BlockhashRegistry and avoid recreation. Also note that searchForAvailableBlock can wrap with values close to uint_max even though that is unlikely to happen. In general, return values for external calls should be validated more rigorously.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.14 Impossible to remove malicious nodes after the initial period    ", "body": "  Resolution  This issue has been addressed with a large change-set that splits the NodeRegistry into two contracts, which results in a code flow that mitigates this issue by making the logic contract upgradable (after 47 days of notice). The resolution adds more complexity to the system, and this complexity is not covered by the original audit. Splitting up the contracts has the side-effect of events being emitted by two different contracts, requiring nodes to subscribe to both contracts  events.  The need for removing malicious nodes from the registry, arises from the design decision to allow anyone to register any URL. These URLs might not actually belong to the registrar of the URL and might not be IN3 nodes. This is partially mitigated by a centralization feature introduced in the mitigation phase that implements whitelist functionality for adding nodes.  We generally advocate against adding complexity, centralization and upgrading mechanisms that can allow one party to misuse functionalities of the contract system for their benefit (e.g. adminSetNodeDeposit is only used to reset the deposit but allows the Logic contract to set any deposit; the logic contract is set by the owner and there is a 47 day timelock).  We believe the solution to this issue, should have not been this complex. The trust model of the system is changed with this solution, now the logic contract can allow the admin a wide range of control over the system state and data.  The following statement has been provided with the change-set:  During the 1st year, we will keep the current mechanic even though it s a centralized approach. However, we changed the structure of the smart contracts and separated the NodeRegistry into two different smart contracts: NodeRegistryLogic and NodeRegistryData. After a successful deployment only the NodeRegistryLogic-contract is able to write data into the NodeRegistryData-contract. This way, we can keep the stored data (e.g. the nodeList) in the NodeRegistryData-contract while changing the way the data gets added/updated/removed is handled in the NodeRegistryLogic-contract. We also provided a function to update the NodeRegistryLogic-contract, so that we are able to change to a better solution for removing nodes in an updated contract.  Description  The system has centralized power structure for the first year after deployment. An unregisterKey (creator of the contract) is allowed to remove Nodes that are in state Stages.Active from the registry, only in 1st year.  However, there is no possibility to remove malicious nodes from the registry after that.  code/in3-contracts/contracts/NodeRegistry.sol:L249-L264  /// @dev only callable in the 1st year after deployment  function removeNodeFromRegistry(address _signer)  external  onlyActiveState(_signer)  // solium-disable-next-line security/no-block-members  require(block.timestamp < (blockTimeStampDeployment + YEAR_DEFINITION), \"only in 1st year\");// solhint-disable-line not-rely-on-time  require(msg.sender == unregisterKey, \"only unregisterKey is allowed to remove nodes\");  SignerInformation storage si = signerIndex[_signer];  In3Node memory n = nodes[si.index];  unregisterNodeInternal(si, n);  Recommendation  Provide a solution for the network to remove fraudulent node entries. This could be done by voting mechanism (with staking, etc).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.15 NodeRegistry.registerNodeFor() no replay protection and expiration   ", "body": "  Resolution  This issue was addressed with the following statement:  In our understanding of the relationship between node-owner and signer the owner both are controlled by the very same entity, thus the owner should always know the privateKey of the signer. With this in mind a replay-protection would be useless, as the owner could always sign the necessary message. The reason why we separated the signer from the owner was to enable the possibility of owning an in3-node as with a multisig-account, as due to the nature of the exposal of the signer-key the possibility of it being leaked somehow is given (e.g. someone  hacks  the server), making the signer-key more unsecure. In addition, even though it s possible to replay the register as an owner it would unfeasable, as the owner would have to pay for the deposit anyway thus rendering the attack useless as there would be no benefit for an owner to do it.  Description  An owner can register a node with the signer not being the owner by calling registerNodeFor. The owner submits a message signed for the owner including the properties of the node including the url.  The signed data does not include the registryID nor the NodeRegistry s address and can therefore be used by the owner to submit the same node to multiple registries or chains without the signers consent.  The signed data does not expire and can be re-used by the owner indefinitely to submit the same node again to future contracts or the same contract after the node has been removed.  Arguments are not validated in the external function (also see issue 6.17)  code/in3-contracts/contracts/NodeRegistry.sol:L215-L223  bytes32 tempHash = keccak256(  abi.encodePacked(  _url,  _props,  _timeout,  _weight,  msg.sender  );  Recommendation  Include registryID and an expiration timestamp that is checked in the contract with the signed data. Validate function arguments.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.16 BlockhashRegistry - Structure of provided blockheaders should be validated    ", "body": "  Resolution  Mitigated by:  99f35fce - validating the block number in the provided RLP encoded input data  79e5a302 - fixes the potential out of bounds access for parentHash by requiring the the input to contain at least data up until including the parentHash in the user provided RLP blob. However, this check does not enforce that the minimum amount of data is available to extract the blockNumber  Additionally we would like to note the following:  While the code decodes the RLPLongList structure that contains the blockheader fields it does not decode the RLPLongString parentHash and just assumes one length-byte for it.  The length of the RLPLongString parentHash is never used but skipped instead.  The decoding is incomplete and fragile. The method does not attempt to decode other fields in the struct to verify that they are indeed valid RLP data. For the blockNumber extraction a fixed offset of 444 is assumed to access the difficulty RLP field (this might through as the minimum input length up to this field is not enforced). difficulty is then skipped and the blockNumber is accessed.  The minimum input data length enforced is shorter than a typical blockheader.  The code relies on implicit exceptions for out of bounds array access instead of verifying early on that enough input bytes are available to extract the required data.  We would also like to note that the commit referenced as mitigation does not appear to be based on the audit code.  Description  getParentAndBlockhash takes an rlp-encoded blockheader blob, extracts the parent parent hash and returns both the parent hash and the calculated blockhash of the provided data. The method is used to add blockhashes to the registry that are older than 256 blocks as they are not available to the evm directly. This is done by establishing a trust-chain from a blockhash that is already in the registry up to an older block  The method assumes that valid rlp encoded data is provided but the structure is not verified (rlp decodes completely; block number is correct; timestamp is younger than prevs, \u2026), giving a wide range of freedom to an attacker with enough hashing power (or exploiting potential future issues with keccak) to forge blocks that would never be accepted by clients, but may be accepted by this smart contract. (threat: mining pool forging arbitrary non-conformant blocks to exploit the BlockhashRegistry)  It is not checked that input was actually provided. However, accessing an array at an invalid index will raise an exception in the EVM. Providing a single byte > 0xf7 will yield a result and succeed even though it would have never been accepted by a real node.  It is assumed that the first byte is the rlp encoded length byte and an offset into the provided _blockheader bytes-array is calculated. Memory is subsequently accessed via a low-level mload at this calculated offset. However, it is never validated that the offset actually lies within the provided range of bytes _blockheader leading to an out-of-bounds memory read access.  The rlp encoded data is only partially decoded. For the first rlp list the number of length bytes is extracted. For the rlp encoded long string a length byte of 1 is assumed. The inline comment appears to be inaccurate or might be misleading. // we also have to add \"2\" = 1 byte to it to skip the length-information  Invalid intermediary blocks (e.g. with parent hash 0x00) will be accepted potentially allowing an attacker to optimize the effort needed to forge invalid blocks skipping to the desired blocknumber overwriting a certain blockhash (see issue 6.18)  With one collisions (very unlikely) an attacker can add arbitrary or even random values to the BlockchainRegistry. The parent-hash of the starting blockheader cannot be verified by the contract ([target_block_random]<--parent_hash--[rnd]<--parent_hash--[rnd]<--parent_hash--...<--parent_hash--[collision]<--parent_hash_collission--[anchor_block]). While nodes can verify block structure and bail on invalid structure and check the first blocks hash and make sure the chain is in-tact the contract can t. Therefore one cannot assume the same trust in the blockchain registry when recreating blocks compared to running a full node.  code/in3-contracts/contracts/BlockhashRegistry.sol:L98-L126  function getParentAndBlockhash(bytes memory _blockheader) public pure returns (bytes32 parentHash, bytes32 bhash) {  /// we need the 1st byte of the blockheader to calculate the position of the parentHash  uint8 first = uint8(_blockheader[0]);  /// calculates the offset  /// by using the 1st byte (usually f9) and substracting f7 to get the start point of the parentHash information  /// we also have to add \"2\" = 1 byte to it to skip the length-information  require(first > 0xf7, \"invalid offset\");  uint8 offset = first - 0xf7 + 2;  /// we are using assembly because it's the most efficent way to access the parent blockhash within the rlp-encoded blockheader  // solium-disable-next-line security/no-inline-assembly  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  mstore(0x20, _blockheader)  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  mload(0x20), 0x20  ), offset)  bhash = keccak256(_blockheader);  Recommendation  Validate that the provided data is within a sane range of bytes that is expected (min/max blockheader sizes).  Validate that the provided data is actually an rlp encoded blockheader.  Validate that the offset for the parent Hash is within the provided data.  Validate that the parent Hash is non zero.  Validate that blockhashes do not repeat.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.17 Registries - Incomplete input validation and inconsistent order of validations   Pending", "body": "  Resolution  This issue describes general inconsistencies of the smart contract code base. The inconsistencies have been addressed with multiple change-sets:  Issues that have been addressed by the development team:  BlockhashRegistry.reCalculateBlockheaders - bhash can be zero; blockheaders can be empty  Fixed in 8d2bfa40 by adding the missing checks.  BlockhashRegistry.recreateBlockheaders - blockheaders can be empty; Arguments should be validated before calculating values that depend on them.  Fixed in 8d2bfa40 by adding the missing checks.  NodeRegistry.removeNode - should check require(_nodeIndex < nodes.length) first before any other action.  Fixed in 47255587 by adding the missing checks.  NodeRegistry.registerNodeFor - Signature version v should be checked to be either 27 || 28 before verifying it.  The fix in 47255587 introduced a serious typo (v != _v) that has been fixed with 4a0377c5 .  NodeRegistry.revealConvict - unchecked signer  Addressed with the comment that signer gets checked by ecrecover (slock.it/issue/10).  NodeRegistry.revealConvict - signer status can be checked earlier. Addressed with the following comment (slock.it/issue/10):  Due to the seperation of the contracts we will now check check the signatures and whether the blockhash is right. Only after this steps we will call into the NodeRegistryData contracts, thus potentially saving gas  NodeRegistry.updateNode - the check if the newURL is registered can be done earlier  Fixed in 4786a966.  BlockhashRegistry.getParentAndBlockhash- blockheader structure can be random as long as parenthash can be extracted  This issue has been reviewed as part of issue 6.16 (99f35fce).  Issues that have not been addressed by the development team and still persist:  BlockhashRegistry.searchForAvailableBlock - _startNumber + _numBlocks can be > block.number; _startNumber + _numBlocks can overflow. This issue has not been addressed.  General Notes:  Ideally commits directly reference issues that were raised during the audit. During the review of the mitigations provided with the change-sets for the listed issues we observed that change-sets  contain changes that are not directly related to the issues. (e.g. 79e5a302)  Description  Methods and Functions usually live in one of two worlds:  public API - methods declared with visibility public or external exposed for interaction by other parties  internal API - methods declared with visibility internal, private that are not exposed for interaction by other parties  While it is good practice to visually distinguish internal from public API by following commonly accepted naming convention e.g. by prefixing internal functions with an underscore (_doSomething vs. doSomething) or adding the keyword unsafe to unsafe functions that are not performing checks and may have a dramatic effect to the system (_unsafePayout vs. RequestPayout), it is important to properly verify that inputs to methods are within expected ranges for the implementation.  Input validation checks should be explicit and well documented as part of the code s documentation. This is to make sure that smart-contracts are robust against erroneous inputs and reduce the potential attack surface for exploitation.  It is good practice to verify the methods input as early as possible and only perform further actions if the validation succeeds. Methods can be split into an external or public API that performs initial checks and subsequently calls an internal method that performs the action.  The following lists some public API methods that are not properly checking the provided data:  BlockhashRegistry.reCalculateBlockheaders - bhash can be zero; blockheaders can be empty  BlockhashRegistry.getParentAndBlockhash- blockheader structure can be random as long as parenthash can be extracted  BlockhashRegistry.recreateBlockheaders - blockheaders can be empty; Arguments should be validated before calculating values that depend on them:  code/in3-contracts/contracts/BlockhashRegistry.sol:L70-L70  assert(_blockNumber > _blockheaders.length);  BlockhashRegistry.searchForAvailableBlock - _startNumber + _numBlocks can be > block.number; _startNumber + _numBlocks can overflow.  NodeRegistry.removeNode - should check require(_nodeIndex < nodes.length) first before any other action.  code/in3-contracts/contracts/NodeRegistry.sol:L602-L609  function removeNode(uint _nodeIndex) internal {  // trigger event  emit LogNodeRemoved(nodes[_nodeIndex].url, nodes[_nodeIndex].signer);  // deleting the old entry  delete urlIndex[keccak256(bytes(nodes[_nodeIndex].url))];  uint length = nodes.length;  assert(length > 0);  NodeRegistry.registerNodeFor - Signature version v should be checked to be either 27 || 28 before verifying it.  code/in3-contracts/contracts/NodeRegistry.sol:L200-L212  function registerNodeFor(  string calldata _url,  uint64 _props,  uint64 _timeout,  address _signer,  uint64 _weight,  uint8 _v,  bytes32 _r,  bytes32 _s  external  payable  NodeRegistry.revealConvict - unchecked signer  code/in3-contracts/contracts/NodeRegistry.sol:L321-L321  SignerInformation storage si = signerIndex[_signer];  NodeRegistry.revealConvict - signer status can be checked earlier.  code/in3-contracts/contracts/NodeRegistry.sol:L344-L344  require(si.stage != Stages.Convicted, \"node already convicted\");  NodeRegistry.updateNode - the check if the newURL is registered can be done earlier  code/in3-contracts/contracts/NodeRegistry.sol:L444-L444  require(!urlIndex[newURl].used, \"url is already in use\");  Recommendation  Use Checks-Effects-Interactions pattern for all functions.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.18 BlockhashRegistry - recreateBlockheaders allows invalid parent hashes for intermediary blocks    ", "body": "  Resolution  Fixed by requiring valid parent hashes for blockheaders.  Description  It is assumed that a blockhash of 0x00 is invalid, but the method accepts intermediary parent hashes extracted from blockheaders that are zero when establishing the trust chain.  This may allow an attacker with enough hashing power to store a blockheader hash that is actually invalid on the real chain but accepted within this smart contract. This may even only be done temporarily to overwrite an existing hash for a short period of time (see https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/24).  code/in3-contracts/contracts/BlockhashRegistry.sol:L141-L147  for (uint i = 0; i < _blockheaders.length; i++) {  (calcParent, calcBlockhash) = getParentAndBlockhash(_blockheaders[i]);  if (calcBlockhash != currentBlockhash) {  return 0x0;  currentBlockhash = calcParent;  Recommendation  Stop processing the array of _blockheaders immediately if a blockheader is invalid.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.19 BlockhashRegistry - recreateBlockheaders succeeds and emits an event even though no blockheaders have been provided    ", "body": "  Resolution  Fixed the vulnerable scenarios by adding proper checks to:  Prevent passing empty _blockheaders in 8d2bfa40  Prevent storing the same blockhash twice in 80bb6ecf  Description  The method is used to re-create blockhashes from a list of rlp-encoded _blockheaders. However, the method never checks if _blockheaders actually contains items. The result is, that the method will unnecessarily store the same value that is already in the blockhashMapping at the same location and wrongly log LogBlockhashAdded even though nothing has been added nor changed.  assume _blockheaders is empty and the registry already knows the blockhash of _blockNumber  code/in3-contracts/contracts/BlockhashRegistry.sol:L61-L67  function recreateBlockheaders(uint _blockNumber, bytes[] memory _blockheaders) public {  bytes32 currentBlockhash = blockhashMapping[_blockNumber];  require(currentBlockhash != 0x0, \"parentBlock is not available\");  bytes32 calculatedHash = reCalculateBlockheaders(_blockheaders, currentBlockhash);  require(calculatedHash != 0x0, \"invalid headers\");  An attempt is made to re-calculate the hash of an empty _blockheaders array (also passing the currentBlockhash from the registry)  code/in3-contracts/contracts/BlockhashRegistry.sol:L66-L66  bytes32 calculatedHash = reCalculateBlockheaders(_blockheaders, currentBlockhash);  The following loop in reCalculateBlockheaders is skipped and the currentBlockhash is returned.  code/in3-contracts/contracts/BlockhashRegistry.sol:L134-L149  function reCalculateBlockheaders(bytes[] memory _blockheaders, bytes32 _bHash) public pure returns (bytes32 bhash) {  bytes32 currentBlockhash = _bHash;  bytes32 calcParent = 0x0;  bytes32 calcBlockhash = 0x0;  /// save to use for up to 200 blocks, exponential increase of gas-usage afterwards  for (uint i = 0; i < _blockheaders.length; i++) {  (calcParent, calcBlockhash) = getParentAndBlockhash(_blockheaders[i]);  if (calcBlockhash != currentBlockhash) {  return 0x0;  currentBlockhash = calcParent;  return currentBlockhash;  The assertion does not fire, the bnr to store the calculatedHash is the same as the one initially provided to the method as an argument.. Nothing has changed but an event is emitted.  code/in3-contracts/contracts/BlockhashRegistry.sol:L69-L74  /// we should never fail this assert, as this would mean that we were able to recreate a invalid blockchain  assert(_blockNumber > _blockheaders.length);  uint bnr = _blockNumber - _blockheaders.length;  blockhashMapping[bnr] = calculatedHash;  emit LogBlockhashAdded(bnr, calculatedHash);  Recommendation  The method is crucial for the system to work correctly and must be tightly controlled by input validation. It should not be allowed to overwrite an existing value in the contract (issue 6.29) or emit an event even though nothing has happened. Therefore validate that user provided input is within safe bounds. In this case, that at least one _blockheader has been provided. Validate that _blockNumber is less than block.number and do not expect that parts of the code will throw and safe the contract from exploitation.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.20 NodeRegistry.updateNode replaces signer with owner and emits inconsistent events    ", "body": "  Resolution  Reviewed merged changes at in3-contracts/5cb54165.  The method now emits a distinct event twice when node properties are updated.  The event correctly emits the signer.  When updating a node URL, the new URLInformation now correctly sets the signer.  However, there is a discrepancy between the process of registering a node and updating  node s properties. When registering a node the owner has to provide a signed message containing the registration properties from the signer. Once the node is registered it can be unilaterally updated by the owner without requiring the signers permission to do so. According to slock.it it is assumed that the node owner and the signer are in control of the same entity and therefore this is not a concern.  Description  https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/36).  code/in3-contracts/contracts/NodeRegistry.sol:L438-L452  if (newURl != keccak256(bytes(node.url))) {  // deleting the old entry  delete urlIndex[keccak256(bytes(node.url))];  // make sure the new url is not already in use  require(!urlIndex[newURl].used, \"url is already in use\");  UrlInformation memory ui;  ui.used = true;  ui.signer = msg.sender;  urlIndex[newURl] = ui;  node.url = _url;  Furthermore, the method emits a LogNodeRegistered event when the node structure is updated. However, the event will always emit msg.sender as the signer even though that might not be true. For example, if the url does not change, the signer can still be another account that was previously registered with registerNodeFor and is not necessarily the owner.  code/in3-contracts/contracts/NodeRegistry.sol:L473-L478  emit LogNodeRegistered(  node.url,  _props,  msg.sender,  node.deposit  );  code/in3-contracts/contracts/NodeRegistry.sol:L30-L30  event LogNodeRegistered(string url, uint props, address signer, uint deposit);  Recommendation  The updateNode() function gets the signer as an input used to reference the node structure and this signer should be set for the UrlInformation.  function updateNode(  address _signer,  string calldata _url,  uint64 _props,  uint64 _timeout,  uint64 _weight  The method should actually only allow to change node properties when owner==signer otherwise updateNode is bypassing the strict requirements enforced with registerNodeFor where e.g. the url needs to be signed by the signer in order to register it.  The emitted event should always emit node.signer instead of msg.signer which can be wrong.  The method should emit its own distinct event LogNodeUpdated for audit purposes and to be able to distinguish new node registrations from node structure updates. This might also require software changes to client/node implementations to listen for node updates.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.21 NodeRegistry - In3Node memory n is never used    ", "body": "  Resolution                           Fixed by removing the modifier and move the node-signer check to functions in   f1fd7943  Description  NodeRegistry In3Node memory n is never used inside the modifier onlyActiveState.  code/in3-contracts/contracts/NodeRegistry.sol:L125-L133  modifier onlyActiveState(address _signer) {  SignerInformation memory si = signerIndex[_signer];  require(si.stage == Stages.Active, \"address is not an in3-signer\");  In3Node memory n = nodes[si.index];  assert(nodes[si.index].signer == _signer);  _;  Recommendation  Use n in the assertion to access the node signer assert(n.signer == _signer);  or directly access it from storage and avoid copying the struct.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.22 NodeRegistry - returnDeposit and transferOwnership should emit an event    ", "body": "  Resolution                           Fixed in   f1fd7943 by adding new events (  Description  Important state changing functions should emit an event for the purpose of having an audit trail and being able to monitor the smart contract usage and performance.  Recommendation  Emit events for returnDeposit and transferOwnership.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.23 in3-server - in3_stats leaks information   ", "body": "  Resolution                           There is a config-option   Description  in3_stat shows information from node activities in the currentMonth, currentDay, currentHour which can result in leaking information about the functionality that node is being used for. This information might be valuable when an attacker wants to find out how utilized a node is and if any reflection attacks are successful (https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50).  Examples  'profile': AttributeDict({  'name': 'Slockit2',  'icon': 'https://slock.it/assets/slock_logo.png',  'url': 'https://slock.it'  }),  'stats': Attr  ibuteDict({  'upSince': 1568400626355,  'currentMonth': AttributeDict({  'requests': 47618,  'lastRequest': 1569422025347,  'methods': AttributeDict({  'nd-2': 2,  'eth_call': 940,  'eth_blockNumber': 25,  'eth_getBlockByNumber': 45395,  'web3_clientVersion': 386,  'admin_datadir': 7,  'admin_peers': 11,  'shh_version': 7,  'shh_info': 14,  'admin_nodeInfo ': 9, '  txpool_status ': 9, '  personal_listAccounts ': 3, '  eth_chainId ': 12, '  eth_protocolVersion ': 6, '  net_listening ': 6, '  net_peerCount ': 6, '  eth_syncing ': 6,  'eth_mining': 6,  'eth_hashrate': 6,  'eth_gasPrice': 18,  'eth_coinbase': 44,  'eth_accounts': 54,  'eth_getBalance': 321,  'personal_unlockAccount': 61,  'personal_  importRawKey ': 5, '  personal_newAccount ': 8, '  eth_estimateGas ': 16, '  eth_sendRawTransaction ': 9, '  eth_getTransactionReceipt ': 49, '  in3_sign ': 59, '  eth_getCode ': 33,  'eth_getTransactionCount': 15,  'eth_getLogs': 8,  'in3_stats': 16,  'in3_validatorlist': 15,  'in3_nodeList': 15,  'in3_call': 15,  'proof_in3_sign': 1  })  }),  'currentDay': AttributeDict({  'requests': 144,  'lastRequest': 1569422025347,  'methods': AttributeDict({  'eth_getBlockByNumber': 135,  'web3_clientVersion': 6,  'eth_coinbase': 1,  'eth_accounts': 1,  'in3_stats': 1  })  }),  'currentHour': AttributeDict({  'requests': 144,  'lastRequest': 1569422025346,  'methods': AttributeDict({  'eth_getBlockByNumber': 135,  'web3_clientVersion': 6,  'eth_coinbase': 1,  'eth_accounts': 1,  'in3_stats': 1  })  })  })  Recommendation  Make sure if this information is needed, if not enable it just for debugging purposes.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.24 NodeRegistry - removeNode unnecessarily casts the nodeIndex to uint64 potentially truncating its value    ", "body": "  Resolution                           Fixed as per recommendation   https://git.slock.it/in3/in3-contracts/commit/6c35dd422e27eec1b1d2f70e328268014cadb515.  Description  removeNode removes a node from the Nodes array. This is done by copying the last node of the array to the _nodeIndex of the node that is to be removed. Finally the node array size is decreased.  A Node s index is also referenced in the SignerInformation struct. This index needs to be adjusted when removing a node from the array as the last node is copied to the index of the node that is to be removed.  code/in3-contracts/contracts/NodeRegistry.sol:L60-L69  struct SignerInformation {  uint64 lockedTime;                  /// timestamp until the deposit of an in3-node can not be withdrawn after the node was removed  address owner;                      /// the owner of the node  Stages stage;                       /// state of the address  uint depositAmount;                 /// amount of deposit to be locked, used only after a node had been removed  uint index;                         /// current index-position of the node in the node-array  code/in3-contracts/contracts/NodeRegistry.sol:L614-L620  // move the last entry to the removed one.  In3Node memory m = nodes[length - 1];  nodes[_nodeIndex] = m;  SignerInformation storage si = signerIndex[m.signer];  si.index = uint64(_nodeIndex);  nodes.length--;  Recommendation  Do not cast and therefore truncate the index.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.25 Registries - general inconsistencies   Pending", "body": "  Resolution  The breakdown of the fixes are as follows:  NodeRegistry - check for addr(0) being passed. This is anyway only done in the constructor and will not require a lot of gas.  The proper checks for registry addresses are added in 4786a966.  NodeRregistry - unnecessary payable  Removed payable modifier everywhere, as ERC20 support is added to the system. ERC20 support is not part of this audit.  NodeRegistry - deposit checks can be combined into one function to make the code more readable. The min deposit amount could be exposed as public const to allow other entities to query the contracts minimum deposit similar to the max ether amount. MAX_ETHER_LIMIT should make clear that this limit is only applicable in the first year (e.g. MAX_ETHER_LIMIT_FIRST_YEAR) .  Fixed and variables renamed.  NodeRegistry - require(si.owner == msg.sender) can be checked before accessing the nodes array  Added proper checks in c9e75b35.  NodeRegistry - removeNode resets index to a valid node array index of 0. Even though  the code will access the index it is good practice to set this to an invalid value to make sure it raises an error condition if it is wrongly accessed in a future revision of the code. This is mainly a safeguard. Another option is to invalidate the 0 index.  Although the index is not set to 0, this issue is not yet fixed (Follow up here).  NodeRegistry - implicitly set defaults are hard to maintain. This should be a constant state variable that can be queried to be transparent about minimum and maximum values. Prefer throwing an exception instead of automatically setting the value to a minimum as this might be unexpected by a client and can cover error conditions.  timeout has been removed, so this is obsolete as it is not in the new code anymore.  NodeRegistry - one year startup period: instead of storing the deployment timestamp the contract should store the end-of-admin-timestamp.  Fixed as recommended timestampAdminKeyActive = block.timestamp + YEAR_DEFINITION;  NodeRegistry - inefficient re-calculation of hash  Fixed (issues/16).  NodeRegistry - weight should be part of proofHash  Added in 9fa5548d.  NodeRegistry - updateNode if the new timeout is smaller than the current timeout it will silently be ignored. This may be unexpected by the caller and cover error conditions where a client provides wrong inputs. Raising an exception should  be preferred in such cases instead of gracefully assuming values.  Fixed by removing timeout variable (issues/16).  NodeRegistry - admin functionality should be clearly named as such for transparency reasons (e.g. adminRemovenodeFromRegistry) .  Renamed all admin function in both contracts with prefix admin.  Description  NodeRegistry - check for addr(0) being passed. This is anyway only done in the constructor and will not require a lot of gas.  code/in3-contracts/contracts/NodeRegistry.sol:L138-L139  constructor(BlockhashRegistry _blockRegistry) public {  blockRegistry = _blockRegistry;  NodeRregistry - unnecessary payable  code/in3-contracts/contracts/NodeRegistry.sol:L535-L535  address payable _owner,  NodeRegistry - deposit checks can be combined into one function to make the code more readable. The min deposit amount could be exposed as public const to allow other entities to query the contracts minimum deposit similar to the max ether amount. MAX_ETHER_LIMIT should make clear that this limit is only applicable in the first year (e.g. MAX_ETHER_LIMIT_FIRST_YEAR) .  code/in3-contracts/contracts/NodeRegistry.sol:L543-L545  require(_deposit >= 10 finney, \"not enough deposit\");  checkNodeProperties(_deposit, _timeout);  code/in3-contracts/contracts/NodeRegistry.sol:L120-L120  uint constant internal MAX_ETHER_LIMIT = 50 ether;  NodeRegistry - require(si.owner == msg.sender) can be checked before accessing the nodes array  code/in3-contracts/contracts/NodeRegistry.sol:L402-L404  SignerInformation storage si = signerIndex[_signer];  In3Node memory n = nodes[si.index];  require(si.owner == msg.sender, \"only for the in3-node owner\");  NodeRegistry - removeNode resets index to a valid node array index of 0. Even though  the code will access the index it is good practice to set this to an invalid value to make sure it raises an error condition if it is wrongly accessed in a future revision of the code. This is mainly a safeguard. Another option is to invalidate the 0 index.  code/in3-contracts/contracts/NodeRegistry.sol:L612-L612  signerIndex[nodes[_nodeIndex].signer].index = 0;  NodeRegistry - implicitly set defaults are hard to maintain. This should be a constant state variable that can be queried to be transparent about minimum and maximum values. Prefer throwing an exception instead of automatically setting the value to a minimum as this might be unexpected by a client and can cover error conditions.  code/in3-contracts/contracts/NodeRegistry.sol:L565-L565  m.timeout = _timeout > 1 hours ? _timeout : 1 hours;  NodeRegistry - one year startup period: instead of storing the deployment timestamp the contract should store the end-of-admin-timestamp.  code/in3-contracts/contracts/NodeRegistry.sol:L256-L256  require(block.timestamp < (blockTimeStampDeployment + YEAR_DEFINITION), \"only in 1st year\");// solhint-disable-line not-rely-on-time  NodeRegistry - inefficient re-calculation of hash  code/in3-contracts/contracts/NodeRegistry.sol:L438-L441  if (newURl != keccak256(bytes(node.url))) {  // deleting the old entry  delete urlIndex[keccak256(bytes(node.url))];  NodeRegistry - weight should be part of proofHash  code/in3-contracts/contracts/NodeRegistry.sol:L490-L502  function calcProofHash(In3Node memory _node) internal pure returns (bytes32) {  return keccak256(  abi.encodePacked(  _node.deposit,  _node.timeout,  _node.registerTime,  _node.props,  _node.signer,  _node.url  );  NodeRegistry - updateNode if the new timeout is smaller than the current timeout it will silently be ignored. This may be unexpected by the caller and cover error conditions where a client provides wrong inputs. Raising an exception should  be preferred in such cases instead of gracefully assuming values.  code/in3-contracts/contracts/NodeRegistry.sol:L463-L465  if (_timeout > node.timeout) {  node.timeout = _timeout;  NodeRegistry - admin functionality should be clearly named as such for transparency reasons (e.g. adminRemovenodeFromRegistry) .  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.26 BlockhashRegistry- assembly code can be optimized    ", "body": "  Resolution                           Fixed as per recommendation with   https://git.slock.it/in3/in3-contracts/commit/87f02a7c4f5c30d2b4be42f331c1306e85d42ca6.  Description  The following code can be optimized by removing mload and mstore:  code/in3-contracts/contracts/BlockhashRegistry.sol:L106-L125  require(first > 0xf7, \"invalid offset\");  uint8 offset = first - 0xf7 + 2;  /// we are using assembly because it's the most efficent way to access the parent blockhash within the rlp-encoded blockheader  // solium-disable-next-line security/no-inline-assembly  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  mstore(0x20, _blockheader)  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  mload(0x20), 0x20  ), offset)  Recommendation  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  //mstore(0x20, _blockheader)  //@audit should assign 0x20ptr to variable first and use it.  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  _blockheader, 0x20  ), offset)  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.27 Experimental Compiler features are enabled - ABIEncoderV2   ", "body": "  Resolution  This issue has been addressed with the following statement:  In order to pass structs between contracts we need that new ABIEncoder. [..] The old NodeRegistry did not require the ABIEncoderV2. [..] But due to the separation of the contracts in Logic and Data we are passing certain data-structures between contracts.  Description  The smart contracts enable experimental compiler features. Please note that these features are experimental for a reason and should be avoided unless explicitly required.  code/in3-contracts/contracts/BlockhashRegistry.sol:L21-L21  pragma experimental ABIEncoderV2;  Seems that NodeRegistry does not require any ABIEncoderV2 specific functionality.  code/in3-contracts/contracts/NodeRegistry.sol:L21-L21  pragma experimental ABIEncoderV2;  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.28 BlockhashRegistry - recreateBlockheaders() should use the evm provided blockhash when applicable   Pending", "body": "  Resolution  The provided code-change at 79fa3ef1 is not addressing the raised concerns. As noted in the recommendation it is suggested to completely skip the recreation routine if the target blockhash (_blockNumber.sub(_blockheaders.length)) is available to the evm. The method should call saveBlockNumber(_blockNumber) instead.  The commit attempts to add a verification for extracted blockhashes from the user provided RLP data if the blockhash for the block is available. However, the variable name currentBlock is misleading making it hard to follow the authors intent.  Description  There are different levels of trust attached to blockhashes stored in the BlockhashRegistry. On one side there are blockhashes which data-source is the evm ( blockhash(blocknumber)) and on the other side there are blockhashes that have been fed into the system by recalculating block-headers and establishing a trust chain to an already existing blockhash in the contract. While the contract can trust the result of blockhash(blocknumber) for the most recent 256 blocks because the information is coming directly from the evm, blockhashes that are re-created by calling recreateBlockheaders are manually verified and trust relies on the proper validation of the chain of block-headers provided.  Side-effect: Also saves gas by avoiding unnecessary calculations within the recreateBlockheaders() codepath as blockhash is already available via evm.  Recommendation  recreateBlockheaders() should prefer to use blockhash(number) by calling saveBlockNumber() instead of re-calculating the blockhash from the user provided chain of blockheaders, if the blockhash can easily be accessed by the evm (most recent 256 blocks, except current block). Check if _blockheaders.length > 0 && _blockNumber.sub(_blockheaders.length) < block.number-256.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.29 BlockhashRegistry - Existing blockhashes can be overwritten    ", "body": "  Resolution                           Addressed with   80bb6ecf and  17d450cf by checking if blockhash exists and changing the  Description  Last 256 blocks, that are available in the EVM environment, are stored in BlockhashRegistry by calling snapshot() or saveBlockNumber(uint _blockNumber) functions. Older blocks are recreated by calling recreateBlockheaders.  The methods will overwrite existing blockhashes.  code/in3-contracts/contracts/BlockhashRegistry.sol:L79-L87  function saveBlockNumber(uint _blockNumber) public {  bytes32 bHash = blockhash(_blockNumber);  require(bHash != 0x0, \"block not available\");  blockhashMapping[_blockNumber] = bHash;  emit LogBlockhashAdded(_blockNumber, bHash);  code/in3-contracts/contracts/BlockhashRegistry.sol:L72  blockhashMapping[bnr] = calculatedHash;  Recommendation  If a block is already saved in the smart contract, it can be checked and a SSTORE can be prevented to save gas. Require that blocknumber hash is not stored.  require(blockhashMapping[_blockNumber] == 0x0, \"block already saved\");  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Below is the raw output of the MythX vulnerability scan:  \"issues\": [  \"swcID\": \"SWC-127\",  \"swcTitle\": \"\",  \"description\": {  \"head\": \"jump to arbitrary destination\",  \"tail\": \"A caller can trigger a jump to an arbitrary destination. Make sure this does not enable unintended control flow.\"  },  \"severity\": \"High\",  \"locations\": [  \"sourceMap\": \"20901:1:1\",  \"sourceType\": \"raw-bytecode\",  \"sourceFormat\": \"evm-byzantium-bytecode\",  \"sourceList\": [  \"0x63ad5e3d2c8551cf64f6d0425940efdeb79801907fcad157d1c82922919c13cb\",  \"0xd8d70c0998b3293c364b1cde922c80081d70d02726e74091c8aae6fa6a10b892\"  },  \"sourceMap\": \"23263:248:-1\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 5593089348,  \"testCases\": [  \"initialState\": {  \"accounts\": {  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\": {  \"nonce\": 0,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa1\": {  \"nonce\": 1,  \"balance\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa2\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa3\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x00\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa4\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0xfd\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa5\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x608060405260005600a165627a7a72305820466f8a1bdae15c60b8e998fe04836ef505803cfbd8edd29bd4679531357576530029\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa6\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x608060405273aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa63081146038578073ffffffffffffffffffffffffffffffffffffffff16ff5b5000fea165627a7a723058205e8b906b72ad42c69b05acf4542283b6080ae82562bc74baac467daac2fb0e0e0029\",  \"storage\": {}  },  \"0xaffeaffeaffeaffeaffeaffeaffeaffeaffeaffe\": {  \"nonce\": 0,  \"balance\": \"0x0000000000000000000000000000000000ffffffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"steps\": [  \"address\": \"\",  \"gasLimit\": \"0xffffff\",  \"gasPrice\": \"0x773594000\",  \"input\": REMOVED,  \"origin\": \"0xaffeaffeaffeaffeaffeaffeaffeaffeaffeaffe\",  \"value\": \"0x0\",  \"blockCoinbase\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"blockDifficulty\": \"0xa7d7343662e26\",  \"blockGasLimit\": \"0xffffff\",  \"blockNumber\": \"0x661a55\",  \"blockTime\": \"0x5be99aa8\"  },  \"address\": \"0x0901d12ebe1b195e5aa8748e62bd7734ae19b51f\",  \"gasLimit\": \"0x7d00\",  \"gasPrice\": \"0x773594000\",  \"input\": \"0xac48987300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\",  \"origin\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"value\": \"0x9\",  \"blockCoinbase\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"blockDifficulty\": \"0x52c054bfb494c\",  \"blockGasLimit\": \"0x7d0000\",  \"blockNumber\": \"0x661a55\",  \"blockTime\": \"0x5be99aa8\"  ],  \"toolName\": \"harvey\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"blockhash\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"blockhash\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"6746:25:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666071716,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"blockhash\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"blockhash\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"13158:23:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666159516,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"6756:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666984722,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"7532:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1667012822,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"13711:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1667019122,  \"toolName\": \"maru\"  ],  \"sourceType\": \"raw-bytecode\",  \"sourceFormat\": \"evm-byzantium-bytecode\",  \"sourceList\": [  \"0x63ad5e3d2c8551cf64f6d0425940efdeb79801907fcad157d1c82922919c13cb\",  \"0xd8d70c0998b3293c364b1cde922c80081d70d02726e74091c8aae6fa6a10b892\"  ],  \"meta\": {  \"selectedCompiler\": \"Unknown\",  \"logs\": [],  \"toolName\": \"maru\",  \"coveredPaths\": 91,  \"coveredInstructions\": 7058  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/BlockhashRegistry.sol  21:0     warning    Avoid using experimental features in production code    no-experimental  46:12    warning    Line exceeds the limit of 145 characters                max-len  61:1     error      Line contains trailing whitespace                       no-trailing-whitespace  79:4     warning    Line exceeds the limit of 145 characters                max-len  81:1     error      Line contains trailing whitespace                       no-trailing-whitespace  98:4     warning    Line exceeds the limit of 145 characters                max-len  134:4    warning    Line exceeds the limit of 145 characters                max-len  142:1    error      Line contains trailing whitespace                       no-trailing-whitespace  contracts/NodeRegistry.sol  21:0     warning    Avoid using experimental features in production code    no-experimental  117:1    error      Line contains trailing whitespace                       no-trailing-whitespace  123:1    error      Line contains trailing whitespace                       no-trailing-whitespace  128:8    warning    Line exceeds the limit of 145 characters                max-len  143:1    error      Line contains trailing whitespace                       no-trailing-whitespace  143:8    warning    Line exceeds the limit of 145 characters                max-len  152:1    error      Line contains trailing whitespace                       no-trailing-whitespace  152:4    warning    Line exceeds the limit of 145 characters                max-len  197:1    error      Line contains trailing whitespace                       no-trailing-whitespace  200:1    error      Line contains trailing whitespace                       no-trailing-whitespace  215:1    error      Line contains trailing whitespace                       no-trailing-whitespace  224:1    error      Line contains trailing whitespace                       no-trailing-whitespace  324:1    error      Line contains trailing whitespace                       no-trailing-whitespace  342:1    error      Line contains trailing whitespace                       no-trailing-whitespace  448:1    error      Line contains trailing whitespace                       no-trailing-whitespace  555:2    error      Line contains trailing whitespace                       no-trailing-whitespace  555:8    warning    Line exceeds the limit of 145 characters                max-len  568:1    error      Line contains trailing whitespace                       no-trailing-whitespace  571:1    error      Line contains trailing whitespace                       no-trailing-whitespace  602:1    error      Line contains trailing whitespace                       no-trailing-whitespace  615:1    error      Line contains trailing whitespace                       no-trailing-whitespace  \u2716 19 errors, 10 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Function Name  Visibility  Mutability  Modifiers  NodeRegistry  Implementation  <Constructor>  Public    convict  External    NO   registerNode  External    NO   registerNodeFor  External    NO   removeNodeFromRegistry  External    onlyActiveState  returnDeposit  External    NO   revealConvict  External    NO   transferOwnership  External    onlyActiveState  unregisteringNode  External    onlyActiveState  updateNode  External    onlyActiveState  totalNodes  External    NO   calcProofHash  Internal \ud83d\udd12  checkNodeProperties  Internal \ud83d\udd12  registerNodeInternal  Internal \ud83d\udd12  unregisterNodeInternal  Internal \ud83d\udd12  removeNode  Internal \ud83d\udd12  BlockhashRegistry  Implementation  <Constructor>  Public    searchForAvailableBlock  External    NO   recreateBlockheaders  Public    NO   saveBlockNumber  Public    NO   snapshot  Public    NO   getParentAndBlockhash  Public    NO   reCalculateBlockheaders  Public    NO   Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.4 Other Tools", "body": "  Other security tools such as Slither was also used to identify problems in the smart contract.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.5 Test Coverage", "body": "  Code coverage metrics indicate the amount of lines/statements/branches that are covered by the test-suite. It s important to note that  100% test coverage  does not indicate the code has no vulnerabilities. Be aware that code coverage does not provide information about the individual test-cases quality.  A fork of the Solidity-Coverage tool was used to measure the portion of the code base exercised by the test suite, and identify areas with little or no coverage. Specific sections of the code where necessary test coverage is missing are included in the Issue Details section.  The project is using the automated testing framework provided by Truffle. The test-suite is evaluating 62 individual tests and the test-suite passed without errors. The corresponding console output can be found here.  A code coverage report was generated and is provided along other tool output. The test coverage results for NodeRegistry.sol can be viewed here. The test coverage results for BlockhashRegistry.sol can be viewed here. Please find a summary of the coverage results below.  BlockhashRegistry.sol  100%  30/30  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "92.86%", "body": "  13/14  100%  7/7  100%  31/31  NodeRegistry.sol  100%  123/123  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "95.45%", "body": "  63/66  100%  17/17  100%  129/129  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.1 readOnlyMode is ineffective and may result in a false sense of security    Addressed", "body": "  Resolution                           This was addressed in   PegaSysEng/permissioning-smart-contracts@ed2d4a2 by adding comments to clarify that  Description  AccountRules and NodeRules can both enter and exit a mode of operation called readOnlyMode.  The only effect of readOnlyMode is to prevent admins (who are the only users able to change rules) from changing rules.  Those same admins can disable readOnlyMode, so this mode will not prevent a determined actor from doing something they want to do.  Recommendation  Either readOnlyMode should be removed to prevent it from providing a false sense of security, or the authorization required to toggle readOnlyMode should be separated from the authorization required to change rules.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.2 Ingress.setContractAddress() can cause duplicate entries in contractKeys    ", "body": "  Resolution                           This is fixed in   PegaSysEng/permissioning-smart-contracts@faff726.  Description  code/contracts/Ingress.sol:L39-L62  function setContractAddress(bytes32 name, address addr) public returns (bool) {  require(name > 0x0000000000000000000000000000000000000000000000000000000000000000, \"Contract name must not be empty.\");  require(isAuthorized(msg.sender), \"Not authorized to update contract registry.\");  ContractDetails memory info = registry[name];  // create info if it doesn't exist in the registry  if (info.contractAddress == address(0)) {  info = ContractDetails({  owner: msg.sender,  contractAddress: addr  });  // Update registry indexing  contractKeys.push(name);  } else {  info.contractAddress = addr;  // update record in the registry  registry[name] = info;  emit RegistryUpdated(addr,name);  return true;  If, however, a contract is actually added with the address 0, which is currently allowed in the code, then the contract does already exists, and adding the name to contractKeys again will result in a duplicate.  Mitigation  An admin can call removeContract repeatedly with the same name to remove multiple duplicate entries.  Recommendation  Either disallow a contract address of 0 or check for existence via the owner field instead (which can never be 0).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.3 Use specific contract types instead of address where possible    ", "body": "  Resolution                           This is fixed in   PegaSysEng/permissioning-smart-contracts@05d33ae and  PegaSysEng/permissioning-smart-contracts@2728bac.  Description  For clarity and to get more out of the Solidity type checker, it s generally preferred to use a specific contract type for variables rather than the generic address.  Examples  AccountRules.ingressContractAddress could instead be AccountRules.ingressContract and use the type IngressContract:  code/contracts/AccountRules.sol:L16  address private ingressContractAddress;  code/contracts/AccountRules.sol:L24  AccountIngress ingressContract = AccountIngress(ingressContractAddress);  code/contracts/AccountRules.sol:L32  constructor (address ingressAddress) public {  This same pattern is found in NodeRules:  code/contracts/NodeRules.sol:L32  address private nodeIngressContractAddress;  Recommendation  Where possible, use a specific contract type rather than address.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.4 Ingress should use a set    ", "body": "  Resolution                           This is fixed in   PegaSysEng/permissioning-smart-contracts@2978bd0 and  PegaSysEng/permissioning-smart-contracts@f973035.  Description  The AdminList, AccountRulesList, and NodeRulesList contracts have been recently rewritten to use a set. Ingress has the semantics of a set but has not been written the same way.  This leads to some inefficiencies. In particular, Ingress.removeContract is an O(n) operation:  code/contracts/Ingress.sol:L68-L74  for (uint i = 0; i < contractKeys.length; i++) {  // Delete the key from the array + mapping if it is present  if (contractKeys[i] == name) {  delete registry[contractKeys[i]];  contractKeys[i] = contractKeys[contractKeys.length - 1];  delete contractKeys[contractKeys.length - 1];  contractKeys.length--;  Recommendation  Use the same set implementation for Ingress: an array of ContractDetails and a mapping of names to indexes in that array.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.5 Use a specific Solidity compiler version    ", "body": "  Resolution                           This is fixed in   PegaSysEng/permissioning-smart-contracts@acf5a22 by pinning to Solidity 0.5.9 everywhere except the  Description  A number of files use a  floating  pragma as follows:  pragma solidity >=0.4.22 <0.6.0;  It s better to use a specific Solidity compiler version (preferably a current version). This removes any confusion about which compiler was used when the contract is deployed, and it makes sure the code is never subjected to older compiler bugs.  It s still a good idea to upgrade the compiler version in the future as compiler bugs are fixed, but this way you must explicitly choose the new compiler version in your code when you do so.  Recommendation  Based on the Truffle configuration, the code is currently compiled with Solidity 0.5.9. Consider changing the existing pragmas to the following:  pragma solidity 0.5.9;  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "7.6 ContractDetails.owner is never read    ", "body": "  Resolution                           This is fixed in   PegaSysEng/permissioning-smart-contracts@d3f505e.  Description  The ContractDetails struct used by Ingress contracts has an owner field that is written to, but it is never read.  code/contracts/Ingress.sol:L14-L19  struct ContractDetails {  address owner;  address contractAddress;  mapping(bytes32 => ContractDetails) registry;  Recommendation  If owner is not (yet) needed, the ContractDetails struct should be removed altogether and the type of Ingress.registry should change to mapping(bytes32 => address)  8 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Below is the raw output of the MythX vulnerability scan:  Summary  40 problems (0 errors, 40 warnings)  Warnings  SWC-108  XXXXX  SWC-131  27  XXXXXXXXXXXXXXXXXXXX  SWC-110  XXX  SWC-128  XXX  SWC-123  XX  Details  AccountRules.sol - 7 problems (0 errors, 7 warnings)  Warning  12:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"readOnlyMode\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  14:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"version\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  61:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  62:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  63:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  64:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  65:8  Unused local variable \"\" The local variable \"\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  AccountRulesList.sol - 2 problems (0 errors, 2 warnings)  Warning  15:4  A reachable exception has been detected. It is possible to trigger an exception (opcode 0xfe). Exceptions can be caused by type errors, division by zero, out-of-bounds array access, or assert violations. Note that explicit assert() should only be used to check invariants. Use require() for regular input checking.  SWC-110  Warning  36:8  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  AccountRulesProxy.sol - 12 problems (0 errors, 12 warnings)  Warning  5:8  Unused local variable \"sender\" The local variable \"sender\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  5:8  Unused local variable \"sender\" The local variable \"sender\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"target\" The local variable \"target\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"target\" The local variable \"target\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"value\" The local variable \"value\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"value\" The local variable \"value\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"gasPrice\" The local variable \"gasPrice\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"gasPrice\" The local variable \"gasPrice\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"gasLimit\" The local variable \"gasLimit\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"gasLimit\" The local variable \"gasLimit\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"payload\" The local variable \"payload\" is created within the contract \"AccountRules\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"payload\" The local variable \"payload\" is created within the contract \"AccountRulesProxy\" but does not seem to be used anywhere.  SWC-131  AdminList.sol - 3 problems (0 errors, 3 warnings)  Warning  17:4  A reachable exception has been detected. It is possible to trigger an exception (opcode 0xfe). Exceptions can be caused by type errors, division by zero, out-of-bounds array access, or assert violations. Note that explicit assert() should only be used to check invariants. Use require() for regular input checking.  SWC-110  Warning  38:8  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  Warning  42:23  Potential denial-of-service if block gas limit is reached. A storage modification is executed in a loop. Be aware that the transaction may fail to execute if the loop is unbounded and the necessary gas exceeds the block gas limit.  SWC-128  AdminProxy.sol - 3 problems (0 errors, 3 warnings)  Warning  4:10  precondition violation A precondition was violated. Make sure valid inputs are provided to both callees (e.g, via passed arguments) and callers (e.g., via return values).  SWC-123  Warning  4:26  Unused local variable \"source\" The local variable \"source\" is created within the contract \"Admin\" but does not seem to be used anywhere.  SWC-131  Warning  4:26  Unused local variable \"source\" The local variable \"source\" is created within the contract \"AdminProxy\" but does not seem to be used anywhere.  SWC-131  Ingress.sol - 3 problems (0 errors, 3 warnings)  Warning  12:14  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"contractKeys\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  19:40  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"registry\" is internal. Other possible visibility values are public and private.  SWC-108  Warning  35:19  precondition violation A precondition was violated. Make sure valid inputs are provided to both callees (e.g, via passed arguments) and callers (e.g., via return values).  SWC-123  NodeRulesList.sol - 1 problem (0 errors, 1 warning)  Warning  15:4  assertion violation An assertion was violated. Make sure your program logic is correct (e.g., no division by zero) and that you add appropriate validation for inputs from both callers (e.g, passed arguments) and callees (e.g., return values).  SWC-110  NodeIngress.sol - 1 problem (0 errors, 1 warning)  Warning  9:9  The state variable visibility is not set. It is best practice to set the visibility of state variables explicitly. The default visibility for \"version\" is internal. Other possible visibility values are public and private.  SWC-108  NodeRulesProxy.sol - 8 problems (0 errors, 8 warnings)  Warning  5:8  Unused local variable \"sourceEnodeHigh\" The local variable \"sourceEnodeHigh\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  6:8  Unused local variable \"sourceEnodeLow\" The local variable \"sourceEnodeLow\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  7:8  Unused local variable \"sourceEnodeIp\" The local variable \"sourceEnodeIp\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  8:8  Unused local variable \"sourceEnodePort\" The local variable \"sourceEnodePort\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  9:8  Unused local variable \"destinationEnodeHigh\" The local variable \"destinationEnodeHigh\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  10:8  Unused local variable \"destinationEnodeLow\" The local variable \"destinationEnodeLow\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  11:8  Unused local variable \"destinationEnodeIp\" The local variable \"destinationEnodeIp\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  Warning  12:8  Unused local variable \"destinationEnodePort\" The local variable \"destinationEnodePort\" is created within the contract \"NodeRulesProxy\" but does not seem to be used anywhere.  SWC-131  AccountIngress.sol - 0 problems  Admin.sol - 0 problems  ExposedAccountRulesList.sol - 0 problems  ExposedAdminList.sol - 0 problems  ExposedNodeRulesList.sol - 0 problems  Generated on Thu Aug 29 2019 15:16:37 GMT-0700 (Pacific Daylight Time)  MythX Logs:  AccountRules.sol  UUID: 6db36465-5d19-43b8-8318-20d038616ffb  info: skipped automated fuzz testing due to incompatible bytecode input  AccountRulesList.sol  UUID: 17faa2da-60ed-4e9c-8f76-c9d87ebfa025  AccountRulesProxy.sol  UUID: 0579eb33-82ef-4ac7-99e1-948ba46955df  Admin.sol  UUID: da4012ea-98e3-4116-9ee9-896da7904e7c  AdminList.sol  UUID: 6a5da947-d87d-4f3a-b3e6-94d76712aa73  AdminProxy.sol  UUID: ef18baac-d986-4bcd-aefc-1d0801e214d2  ExposedAccountRulesList.sol  UUID: 706738e2-35a4-4def-b7c4-f680920db1a1  ExposedAdminList.sol  UUID: ea78f05f-c0f2-46e0-84eb-0168d35fccc4  ExposedNodeRulesList.sol  UUID: 33d4d1a4-2f85-4d6d-99c7-dd76c738d305  Ingress.sol  UUID: 162745bf-308e-4cc8-a07b-b5e1564f7764  NodeIngress.sol  UUID: a40eebe4-d51e-40fd-8b0b-913491e63411  NodeRulesList.sol  UUID: c5d7bd11-591d-4bd6-8364-883d8db35bb9  NodeRulesProxy.sol  UUID: 851c3349-b9f2-459c-a3ec-5bbd7cb6d616  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Ethlint didn t find any issues.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  AccountIngress.sol  57207a6878535bc2f3d40216d96f07eef9bbdfd9  AccountRulesList.sol  73ffd92be5b6c3b1e18d1b860344dac578c9aa31  Admin.sol  e13931323093f1555f4dfcc74fad6a2c457c1082  AdminProxy.sol  eecd073b4e05a4445fb00888074b48c443c5bbf4  Ingress.sol  b0fcff06fa7d55136cfe483331280e4e9bb9def4  NodeIngress.sol  3f46f78e4c1b9a546287135a13ffa303f62a826b  NodeRulesList.sol  fa9382c4cf3f4d800aa3d0e89bb9a712d5aa5f0c  AccountRules.sol  c730212300e070ed22b1490f6e67347d1f36c051  AccountRulesProxy.sol  1024d00149ee0258f5ee4c0671a09ada723c3645  AdminList.sol  0304e06bfc4c87abc4d2f4c0361633590c5ef830  NodeRules.sol  8f0dc9efd5bc09a8c6346495e23a398c907baf21  NodeRulesProxy.sol  01967d8481a3f1497ecdfcfcd5e7dd2ea9f9c17e  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AccountIngress  Implementation  Ingress  getContractVersion  Public    NO   emitRulesChangeEvent  Public    NO   transactionAllowed  Public    NO   AccountRulesList  Implementation  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  addAll  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  Admin  Implementation  AdminProxy, AdminList  <Constructor>  Public    isAuthorized  Public    NO   addAdmin  Public    onlyAdmin  removeAdmin  Public    onlyAdmin notSelf  getAdmins  Public    NO   addAdmins  Public    onlyAdmin  AdminProxy  Interface  isAuthorized  External    NO   Ingress  Implementation  getContractAddress  Public    NO   isAuthorized  Public    NO   setContractAddress  Public    NO   removeContract  Public    NO   getAllContractKeys  Public    NO   NodeIngress  Implementation  Ingress  getContractVersion  Public    NO   emitRulesChangeEvent  Public    NO   connectionAllowed  Public    NO   NodeRulesList  Implementation  calculateKey  Internal \ud83d\udd12  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  AccountRules  Implementation  AccountRulesProxy, AccountRulesList  <Constructor>  Public    getContractVersion  Public    NO   isReadOnly  Public    NO   enterReadOnly  Public    onlyAdmin  exitReadOnly  Public    onlyAdmin  transactionAllowed  Public    NO   accountInWhitelist  Public    NO   addAccount  Public    onlyAdmin onlyOnEditMode  removeAccount  Public    onlyAdmin onlyOnEditMode  getSize  Public    NO   getByIndex  Public    NO   getAccounts  Public    NO   addAccounts  Public    onlyAdmin  AccountRulesProxy  Interface  transactionAllowed  External    NO   AdminList  Implementation  size  Internal \ud83d\udd12  exists  Internal \ud83d\udd12  add  Internal \ud83d\udd12  addAll  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  NodeRules  Implementation  NodeRulesProxy, NodeRulesList  <Constructor>  Public    getContractVersion  Public    NO   isReadOnly  Public    NO   enterReadOnly  Public    onlyAdmin  exitReadOnly  Public    onlyAdmin  connectionAllowed  Public    NO   enodeInWhitelist  Public    NO   addEnode  Public    onlyAdmin onlyOnEditMode  removeEnode  Public    onlyAdmin onlyOnEditMode  getSize  Public    NO   getByIndex  Public    NO   triggerRulesChangeEvent  Public    NO   NodeRulesProxy  Interface  connectionAllowed  External    NO   Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "8.4 Slither", "body": "  Slither is a Solidity static analysis framework written in Python 3. It runs a suite of vulnerability detectors.  Below is the raw output of the Slither scan:  INFO:Detectors:  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedAdminList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRules.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Ingress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRules.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AdminProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRulesProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedNodeRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AdminList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (ExposedAccountRulesList.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Admin.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (Migrations.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeIngress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountRulesProxy.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (AccountIngress.sol#1)  Pragma version \">=0.4.22<0.6.0\" allows old versions (NodeRulesList.sol#1)  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#incorrect-versions-of-solidity  INFO:Detectors:  Function 'ExposedAdminList._size()' (ExposedAdminList.sol#9-11) is not in mixedCase  Function 'ExposedAdminList._exists(address)' (ExposedAdminList.sol#13-15) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#13) is not in mixedCase  Function 'ExposedAdminList._add(address)' (ExposedAdminList.sol#17-19) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#17) is not in mixedCase  Function 'ExposedAdminList._remove(address)' (ExposedAdminList.sol#21-23) is not in mixedCase  Parameter '_address' of _address (ExposedAdminList.sol#21) is not in mixedCase  Function 'ExposedAdminList._addBatch(address[])' (ExposedAdminList.sol#25-27) is not in mixedCase  Parameter '_addresses' of _addresses (ExposedAdminList.sol#25) is not in mixedCase  Parameter '_account' of _account (AccountRules.sol#77) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#22) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#26) is not in mixedCase  Parameter '_account' of _account (AccountRulesList.sol#45) is not in mixedCase  Variable 'Ingress.RULES_CONTRACT' (Ingress.sol#8) is not in mixedCase  Variable 'Ingress.ADMIN_CONTRACT' (Ingress.sol#9) is not in mixedCase  Function 'ExposedNodeRulesList._calculateKey(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#8-10) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#8) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#8) is not in mixedCase  Function 'ExposedNodeRulesList._size()' (ExposedNodeRulesList.sol#12-14) is not in mixedCase  Function 'ExposedNodeRulesList._exists(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#16-18) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#16) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#16) is not in mixedCase  Function 'ExposedNodeRulesList._add(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#20-22) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#20) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#20) is not in mixedCase  Function 'ExposedNodeRulesList._remove(bytes32,bytes32,bytes16,uint16)' (ExposedNodeRulesList.sol#24-26) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_ip' of _ip (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_port' of _port (ExposedNodeRulesList.sol#24) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#24) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#28) is not in mixedCase  Parameter '_account' of _account (AdminList.sol#56) is not in mixedCase  Function 'ExposedAccountRulesList._size()' (ExposedAccountRulesList.sol#8-10) is not in mixedCase  Function 'ExposedAccountRulesList._exists(address)' (ExposedAccountRulesList.sol#12-14) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#12) is not in mixedCase  Function 'ExposedAccountRulesList._add(address)' (ExposedAccountRulesList.sol#16-18) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#16) is not in mixedCase  Function 'ExposedAccountRulesList._addAll(address[])' (ExposedAccountRulesList.sol#20-22) is not in mixedCase  Function 'ExposedAccountRulesList._remove(address)' (ExposedAccountRulesList.sol#24-26) is not in mixedCase  Parameter '_account' of _account (ExposedAccountRulesList.sol#24) is not in mixedCase  Parameter '_address' of _address (Admin.sol#22) is not in mixedCase  Parameter '_address' of _address (Admin.sol#26) is not in mixedCase  Parameter '_address' of _address (Admin.sol#38) is not in mixedCase  Parameter 'new_address' of new_address (Migrations.sol#20) is not in mixedCase  Variable 'Migrations.last_completed_migration' (Migrations.sol#6) is not in mixedCase  Struct 'NodeRulesList.enode' (NodeRulesList.sol#8-13) is not in CapWords  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#18) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#18) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#18) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#18) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#26) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#26) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#26) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#26) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#30) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#30) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#30) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#30) is not in mixedCase  Parameter '_enodeHigh' of _enodeHigh (NodeRulesList.sol#39) is not in mixedCase  Parameter '_enodeLow' of _enodeLow (NodeRulesList.sol#39) is not in mixedCase  Parameter '_ip' of _ip (NodeRulesList.sol#39) is not in mixedCase  Parameter '_port' of _port (NodeRulesList.sol#39) is not in mixedCase  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#conformance-to-solidity-naming-conventions  INFO:Detectors:  AccountRules.slitherConstructorVariables (AccountRules.sol#9-113) uses literals with too many digits:  version = 1000000  NodeRules.slitherConstructorVariables (NodeRules.sol#9-170) uses literals with too many digits:  version = 1000000  NodeIngress.getContractAddress (Ingress.sol#26-29) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.setContractAddress (Ingress.sol#39-62) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.removeContract (Ingress.sol#64-81) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  RULES_CONTRACT = 0x72756c6573000000000000000000000000000000000000000000000000000000  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  ADMIN_CONTRACT = 0x61646d696e697374726174696f6e000000000000000000000000000000000000  NodeIngress.slitherConstructorVariables (NodeIngress.sol#7-49) uses literals with too many digits:  version = 1000000  AccountIngress.getContractAddress (Ingress.sol#26-29) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.setContractAddress (Ingress.sol#39-62) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.removeContract (Ingress.sol#64-81) uses literals with too many digits:  require(bool,string)(name > 0x0000000000000000000000000000000000000000000000000000000000000000,Contract name must not be empty.)  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  RULES_CONTRACT = 0x72756c6573000000000000000000000000000000000000000000000000000000  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  ADMIN_CONTRACT = 0x61646d696e697374726174696f6e000000000000000000000000000000000000  AccountIngress.slitherConstructorVariables (AccountIngress.sol#7-40) uses literals with too many digits:  version = 1000000  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#too-many-digits  INFO:Detectors:  AccountIngress.version should be constant (AccountIngress.sol#9)  AccountRules.version should be constant (AccountRules.sol#14)  Ingress.ADMIN_CONTRACT should be constant (Ingress.sol#9)  Ingress.RULES_CONTRACT should be constant (Ingress.sol#8)  NodeIngress.version should be constant (NodeIngress.sol#9)  NodeRules.version should be constant (NodeRules.sol#30)  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#state-variables-that-could-be-declared-constant  INFO:Detectors:  ExposedAdminList._size() (ExposedAdminList.sol#9-11) should be declared external  ExposedAdminList._exists(address) (ExposedAdminList.sol#13-15) should be declared external  ExposedAdminList._add(address) (ExposedAdminList.sol#17-19) should be declared external  ExposedAdminList._remove(address) (ExposedAdminList.sol#21-23) should be declared external  ExposedAdminList._addBatch(address[]) (ExposedAdminList.sol#25-27) should be declared external  AccountRules.getContractVersion() (AccountRules.sol#38-40) should be declared external  AccountRules.isReadOnly() (AccountRules.sol#43-45) should be declared external  AccountRules.enterReadOnly() (AccountRules.sol#47-51) should be declared external  AccountRules.exitReadOnly() (AccountRules.sol#53-57) should be declared external  AccountRules.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountRules.sol#59-74) should be declared external  AccountRulesProxy.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountRulesProxy.sol#4-11) should be declared external  AccountRules.addAccount(address) (AccountRules.sol#82-88) should be declared external  AccountRules.removeAccount(address) (AccountRules.sol#90-96) should be declared external  AccountRules.getSize() (AccountRules.sol#98-100) should be declared external  AccountRules.getByIndex(uint256) (AccountRules.sol#102-104) should be declared external  AccountRules.getAccounts() (AccountRules.sol#106-108) should be declared external  AccountRules.addAccounts(address[]) (AccountRules.sol#110-112) should be declared external  Ingress.setContractAddress(bytes32,address) (Ingress.sol#39-62) should be declared external  Ingress.removeContract(bytes32) (Ingress.sol#64-81) should be declared external  Ingress.getAllContractKeys() (Ingress.sol#83-85) should be declared external  NodeRules.getContractVersion() (NodeRules.sol#53-55) should be declared external  NodeRules.isReadOnly() (NodeRules.sol#58-60) should be declared external  NodeRules.enterReadOnly() (NodeRules.sol#62-66) should be declared external  NodeRules.exitReadOnly() (NodeRules.sol#68-72) should be declared external  NodeRulesProxy.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeRulesProxy.sol#4-13) should be declared external  NodeRules.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeRules.sol#74-101) should be declared external  NodeRules.addEnode(bytes32,bytes32,bytes16,uint16) (NodeRules.sol#112-132) should be declared external  NodeRules.removeEnode(bytes32,bytes32,bytes16,uint16) (NodeRules.sol#134-154) should be declared external  NodeRules.getSize() (NodeRules.sol#156-158) should be declared external  NodeRules.getByIndex(uint256) (NodeRules.sol#160-165) should be declared external  ExposedNodeRulesList._calculateKey(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#8-10) should be declared external  ExposedNodeRulesList._size() (ExposedNodeRulesList.sol#12-14) should be declared external  ExposedNodeRulesList._exists(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#16-18) should be declared external  ExposedNodeRulesList._add(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#20-22) should be declared external  ExposedNodeRulesList._remove(bytes32,bytes32,bytes16,uint16) (ExposedNodeRulesList.sol#24-26) should be declared external  ExposedAccountRulesList._size() (ExposedAccountRulesList.sol#8-10) should be declared external  ExposedAccountRulesList._exists(address) (ExposedAccountRulesList.sol#12-14) should be declared external  ExposedAccountRulesList._add(address) (ExposedAccountRulesList.sol#16-18) should be declared external  ExposedAccountRulesList._addAll(address[]) (ExposedAccountRulesList.sol#20-22) should be declared external  ExposedAccountRulesList._remove(address) (ExposedAccountRulesList.sol#24-26) should be declared external  Admin.addAdmin(address) (Admin.sol#26-36) should be declared external  Admin.removeAdmin(address) (Admin.sol#38-42) should be declared external  Admin.getAdmins() (Admin.sol#44-46) should be declared external  Admin.addAdmins(address[]) (Admin.sol#48-50) should be declared external  Migrations.setCompleted(uint256) (Migrations.sol#16-18) should be declared external  Migrations.upgrade(address) (Migrations.sol#20-23) should be declared external  NodeIngress.getContractVersion() (NodeIngress.sol#15-17) should be declared external  NodeIngress.emitRulesChangeEvent(bool) (NodeIngress.sol#19-22) should be declared external  NodeIngress.connectionAllowed(bytes32,bytes32,bytes16,uint16,bytes32,bytes32,bytes16,uint16) (NodeIngress.sol#24-48) should be declared external  AccountIngress.getContractVersion() (AccountIngress.sol#15-17) should be declared external  AccountIngress.emitRulesChangeEvent(bool) (AccountIngress.sol#19-22) should be declared external  AccountIngress.transactionAllowed(address,address,uint256,uint256,uint256,bytes) (AccountIngress.sol#24-39) should be declared external  Reference: https://github.com/crytic/slither/wiki/Detector-Documentation#public-function-that-could-be-declared-as-external  INFO:Slither:. analyzed (16 contracts), 157 result(s) found  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/08/pegasys-permissioning/"}, {"title": "4.1 VaultsRegistry Allows Owner to Bypass VaultFactory   ", "body": "  Resolution  Acknowledged - By Design. While a registry owner (StakeWise DAO) can register factories, and these factories, in turn, can register vaults, adding vaults out-of-band doesn t increase trust in the system.  Description  Creating a Vault with an approved VaultFactory ensures that only approved implementations of Vaults can be registered with the system. For example, when a Vault Owner creates a new Vault via VaultFactory.createVault() the function deploys a proxy pointing to a fixed implementation. Additionally, the newly added Vault address is registered with the VaultRegistry. Only approved factories are allowed to register Vaults with the VaultRegistry.addVault().  However, the guarantee that only approved Factories (with approved implementations) can register Vaults is undermined by permissions the VaultRegistry owner has. They can unilaterally register Vaults that have not been created by the VaultFactory.  It is not clear if the registered Vault is actually a vault contract (or even EOA), nor if it s been properly initialized within one transaction (see issue 4.5) as it can be registered out of band by the owner.  Examples  contracts/vaults/VaultsRegistry.sol:L31-L37  /// @inheritdoc IVaultsRegistry  function addVault(address vault) external override {  if (!factories[msg.sender] && msg.sender != owner()) revert Errors.AccessDenied();  vaults[vault] = true;  emit VaultAdded(msg.sender, vault);  Recommendation  Ensure and provide guarantees on the origin of Vaults by enforcing that they ve been created with an approved factory. If the owner is a multi-sig or DAO, ensure that everyone understands the implications of allowing the owner to add vaults to the registry out-of-band (SharedMevRewards) and the scrutiny required to avoid that a malicious Vault is added to the registry.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.2 KeeperRewards.canUpdateRewards() and VaultMev._harvestAssets() Can Theoretically Overflow   ", "body": "  Resolution                           Acknowledged -  By Design. The Keeper contract is immutable and has already been deployed/verified   here with rewardsDelay set to 43200  Description  KeeperRewards.canUpdateRewards()  The inline comment mentions that the unchecked block cannot overflow as  lastRewardsTimestamp & rewardsDelay are uint64 . Yet,  rewardsDelay is a uint256 meaning the result of lastRewardsTimestamp + rewardsDelay could overflow if rewardsDelay > 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF  - lastRewardsTimestamp. If this operation resulted in an overflow, canUpdateRewards() would likely return true in invalid scenarios. This should not happen if the contract is initialized with reasonable values. However, in case the contract is incorrectly initialized or upgraded, this operation could overflow and lead to unintended effects.  contracts/keeper/KeeperRewards.sol:L132-L137  function canUpdateRewards() public view override returns (bool) {  unchecked {  // cannot overflow as lastRewardsTimestamp & rewardsDelay are uint64  return lastRewardsTimestamp + rewardsDelay < block.timestamp;  contracts/keeper/KeeperRewards.sol:L33  uint256 public immutable override rewardsDelay;  VaultMev._harvestAssets()  An integer overflow may occur with an unsafe cast to int256(uint256) for values exceeding uint256.MAX/2+1. Although the likelihood of this happening within the current system and chain (ETH/Mainnet) is extremely low, requiring the contract to return an asset equivalent to over half of the ETH total supply, which is practically unfeasible. However, if this contract is reused, either wholly or partially, on different chains with different configurations or custom tokens, there could be a heightened risk. Therefore, to ensure robust security practices, it is advisable to invest a small amount of gas by implementing SafeCast to enforce secure coding standards.  contracts/vaults/modules/VaultMev.sol:L59-L63  // execution rewards are always equal to what was accumulated in own MEV escrow  return (totalAssetsDelta + int256(IOwnMevEscrow(_mevEscrow).harvest()), harvested);  contracts/vaults/ethereum/mev/OwnMevEscrow.sol:L23-L32  function harvest() external returns (uint256 assets) {  if (msg.sender != vault) revert Errors.HarvestFailed();  assets = address(this).balance;  if (assets == 0) return 0;  emit Harvested(assets);  // slither-disable-next-line arbitrary-send-eth  IVaultEthStaking(msg.sender).receiveFromMevEscrow{value: assets}();  Recommendation  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.3 Inconsistent Interface Between Similar Functionality: Whitelist / Blocklist   ", "body": "  Resolution  The client acknowledged the issue and provided the following statement: Acknowledged -  Feature Enhancement  Description  VaultBlocklist and VaultWhitelist implement similar functionality. However, their handling from a management perspective is quite different.  setBlocklistManager and setWhitelister implementations are the same. However, we would suggest using the same terminology for both functions (i.e.  BlocklistManager ,  WhitelistManager ).  __VaultBlocklist_init takes a blocklistManager and stores it, while __VaultWhitelist_init also whitelists the whitelistManager. Given that a whitelister should only be used as a management account it is unclear why it needs to be whitelisted for e.g. token transfers?  Blocklist exports a _checkBlocklist() method, while Whitelist requires callers to check the whitelistedAccounts mapping directly.  updateBlocklist() silently proceeds even if the same address is blocklisted twice while _updateWhitelist() would revert  The differing management approaches between VaultBlocklist and VaultWhitelist introduce inconsistency and potential confusion for developers and users. This lack of uniformity in functionality and handling could lead to errors in implementation and management of blocklisted and whitelisted accounts, impacting the overall security and usability of the system.  Examples  Blocklist updateBlocklist():  contracts/vaults/modules/VaultBlocklist.sol:L23-L29  function updateBlocklist(address account, bool isBlocked) public virtual override {  if (msg.sender != blocklistManager) revert Errors.AccessDenied();  if (blockedAccounts[account] == isBlocked) return;  blockedAccounts[account] = isBlocked;  emit BlocklistUpdated(msg.sender, account, isBlocked);  Whitelist updateWhitelist():  contracts/vaults/modules/VaultWhitelist.sol:L23-L27  function updateWhitelist(address account, bool approved) external override {  if (msg.sender != whitelister) revert Errors.AccessDenied();  _updateWhitelist(account, approved);  contracts/vaults/modules/VaultWhitelist.sol:L34-L43  /**  @notice Internal function for updating whitelist  @param account The address of the account to update  @param approved Defines whether account is added to the whitelist or removed  /  function _updateWhitelist(address account, bool approved) private {  if (whitelistedAccounts[account] == approved) revert Errors.WhitelistAlreadyUpdated();  whitelistedAccounts[account] = approved;  emit WhitelistUpdated(msg.sender, account, approved);  Recommendation  Consider using the same or a similar interface for both Blocklist and Whitelist features.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.4 User Can Prevent Blocklist Manager From Ejecting Them by Reverting on ETH Transfer   ", "body": "  Resolution  Acknowledged -  Can be avoided by collateralising the vault  Description  In the scenario where the Blocklist Manager intends to remove a user from the vault, particularly when the vault is not collateralized, the Blocklist Manager initiates the removal process by invoking the ejectUser() function within the EthFoxVault contract. Subsequently, this action triggers the execution of the redeem() path for the respective user.  contracts/vaults/ethereum/custom/EthFoxVault.sol:L87-L103  /// @inheritdoc IEthFoxVault  function ejectUser(address user) external override {  // add user to blocklist  updateBlocklist(user, true);  // fetch shares of the user  uint256 userShares = _balances[user];  if (userShares == 0) return;  if (_isCollateralized()) {  // send user shares to exit queue  _enterExitQueue(user, userShares, user);  } else {  // redeem user shares  _redeem(user, userShares, user);  contracts/vaults/modules/VaultEnterExit.sol:L164-L190  function _redeem(  address user,  uint256 shares,  address receiver  ) internal returns (uint256 assets) {  _checkNotCollateralized();  if (shares == 0) revert Errors.InvalidShares();  if (receiver == address(0)) revert Errors.ZeroAddress();  // calculate amount of assets to burn  assets = convertToAssets(shares);  if (assets == 0) revert Errors.InvalidAssets();  // reverts in case there are not enough withdrawable assets  if (assets > withdrawableAssets()) revert Errors.InsufficientAssets();  // update total assets  _totalAssets -= SafeCast.toUint128(assets);  // burn owner shares  _burnShares(user, shares);  // transfer assets to the receiver  _transferVaultAssets(receiver, assets);  emit Redeemed(user, receiver, assets, shares);  This operation eventually leads to the execution of _transferVaultAssets(), a function responsible for facilitating a low-level value-contract-call to the designated recipient. In the event of an unsuccessful call, the operation reverts, ensuring the integrity of the transaction.  contracts/vaults/modules/VaultEthStaking.sol:L124-L130  /// @inheritdoc VaultEnterExit  function _transferVaultAssets(  address receiver,  uint256 assets  ) internal virtual override nonReentrant {  return Address.sendValue(payable(receiver), assets);  function sendValue(address payable recipient, uint256 amount) internal {  if (address(this).balance < amount) {  revert AddressInsufficientBalance(address(this));  (bool success, ) = recipient.call{value: amount}(\"\");  if (!success) {  revert FailedInnerCall();  Upon sending value to the designated recipient, the recipient s fallback function, if available, is invoked, effectively transferring control to the recipient. At this juncture, the recipient possesses the option to revert the call within their fallback function. Should the recipient choose to revert the call, the outer call initiated at sendValue() would also revert accordingly.  In scenarios where the vault is not collateralized, the recipient can evade being ejected from the vault by intentionally reverting within their fallback function. This strategic maneuver allows the recipient to thwart the expulsion attempt initiated by the EthFoxVault contract during the redemption process.  Recommendation  Change from push to pull transfers. Document that a user can still be ejected by collateralizing the vault.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.5 Front-Running Vulnerability During Initialization of Vault Contracts    ", "body": "  Resolution  The client mentioned the issue was fixed in this PR  Description  Vault contracts like EthFoxVault are deployed as proxy contracts, which point to a specific implementation. The contract follows the OpenZeppelin (OZ) Initializable pattern without permissioning the initialize() function. This pattern necessitates that the deployment of the proxy contract and its initialization occur immediately, within the same transaction. Failure to do so exposes the contract to the risk of front-running, where any user could potentially manipulate the initialization process and claim administrative control over the contract.  contracts/vaults/ethereum/custom/EthFoxVault.sol:L49-L75  /// @custom:oz-upgrades-unsafe-allow constructor  constructor(  address _keeper,  address _vaultsRegistry,  address _validatorsRegistry,  address sharedMevEscrow,  uint256 exitedAssetsClaimDelay  VaultImmutables(_keeper, _vaultsRegistry, _validatorsRegistry)  VaultEnterExit(exitedAssetsClaimDelay)  VaultMev(sharedMevEscrow)  _disableInitializers();  /// @inheritdoc IEthFoxVault  function initialize(bytes calldata params) external payable virtual override initializer {  EthFoxVaultInitParams memory initParams = abi.decode(params, (EthFoxVaultInitParams));  __EthFoxVault_init(initParams);  emit EthFoxVaultCreated(  initParams.admin,  initParams.ownMevEscrow,  initParams.capacity,  initParams.feePercent,  initParams.metadataIpfsHash  );  Users deploying contracts via the VaultFactory are inherently protected, as both the deployment of the proxy and the initialization are executed within the same transaction.  contracts/vaults/ethereum/EthVaultFactory.sol:L38-L58  /// @inheritdoc IEthVaultFactory  function createVault(  bytes calldata params,  bool isOwnMevEscrow  ) external payable override returns (address vault) {  // create vault  vault = address(new ERC1967Proxy(implementation, ''));  // create MEV escrow contract if needed  address _mevEscrow;  if (isOwnMevEscrow) {  _mevEscrow = address(new OwnMevEscrow(vault));  // set MEV escrow contract so that it can be initialized in the Vault  ownMevEscrow = _mevEscrow;  // set admin so that it can be initialized in the Vault  vaultAdmin = msg.sender;  // initialize Vault  IEthVault(vault).initialize{value: msg.value}(params);  However, it s worth noting that the task scripts located in the ./tasks/ directory do not utilize the VaultFactory, MultiCall, or Hardhat code to deploy and initialize in the same transaction. Consequently, vaults deployed via these task scripts are vulnerable to front-running by any party.  Examples  FoxVault - vulnerable  tasks/eth-full-deploy.ts:L255-L278  const foxVault = foxVaultFactory.attach(foxVaultAddress)  // Initialize EthFoxVault  const ownMevEscrowFactory = await ethers.getContractFactory('OwnMevEscrow')  const ownMevEscrow = await ownMevEscrowFactory.deploy(foxVaultAddress)  await callContract(  foxVault.initialize(  ethers.AbiCoder.defaultAbiCoder().encode(  'tuple(address admin, address ownMevEscrow, uint256 capacity, uint16 feePercent, string metadataIpfsHash)',  ],  networkConfig.foxVault.admin,  await ownMevEscrow.getAddress(),  networkConfig.foxVault.capacity,  networkConfig.foxVault.feePercent,  networkConfig.foxVault.metadataIpfsHash,  ],  ),  { value: networkConfig.securityDeposit }  Keeper, VaultsRegistry - not vulnerable due to function initialize() onlyOwner  tasks/eth-full-deploy.ts:L319-L324  // transfer ownership to governor  await callContract(vaultsRegistry.initialize(networkConfig.governor))  console.log('VaultsRegistry ownership transferred to', networkConfig.governor)  await callContract(keeper.initialize(networkConfig.governor))  console.log('Keeper ownership transferred to', networkConfig.governor)  Test Suite does not even use Factory.createVault but deploy-initializes manually  test/shared/fixtures.ts:L715-L731  await vault.initialize(  ethers.AbiCoder.defaultAbiCoder().encode(  'tuple(address admin, address ownMevEscrow, uint256 capacity, uint16 feePercent, string metadataIpfsHash)',  ],  adminAddr,  await ownMevEscrow.getAddress(),  vaultParams.capacity,  vaultParams.feePercent,  vaultParams.metadataIpfsHash,  ],  ),  { value: SECURITY_DEPOSIT }  Recommendation  Note: According to the StakeWise team there is no intention to create a factory for the EthFoxVault. We recommend implementing safe deploy&initialize procedures instead of solely relying on verifying contract parameterization after deployment.  Change the task scripts to use hardhat deploy & initialize code that performs all action in a single transaction.  Switch to multicall deployment & initialize pattern.  Enforce permission on initialize() to onlyOwner as seen with Keeper and VaultsRegistry  Monitor and verify correct contract parameterisation after deployment  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.6 Consider Emitting a Specific Event in ejectUser()    ", "body": "  Resolution                           Fixed in this   PR  Description  The function ejectUser() in EthFoxVault allows the BlocklistManager to ban and eject a user from the system. Consider making this function emit a specific event for transparency and auditability reasons.  Examples  contracts/vaults/ethereum/custom/EthFoxVault.sol:L87-L88  /// @inheritdoc IEthFoxVault  function ejectUser(address user) external override {  Recommendation  Emit a specific event when a user is ejected from the system.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.7 VaultFactory/VaultsRegistry - Should Check That New Implementation Is a Contract   ", "body": "  Resolution                           The client provided the following statement: Acknowledged -  The necessary checks are already performed during the proxy deployment/upgrade process   here  Description  Currently, the addVaultImpl() function in the VaultsRegistry contract lacks validation that address of newImpl actually has code. If an invalid address, devoid of code, is set as an implementation, vaults wishing to upgrade to that implementation will not be able to upgrade.  Example  EthVaultFactory.implementation may not be a contract. This will cause a revert in createVault when ERC1967Proxy() is instantiated.  contracts/vaults/ethereum/EthVaultFactory.sol:L28-L36  /**  @dev Constructor  @param _implementation The implementation address of Vault  @param vaultsRegistry The address of the VaultsRegistry contract  /  constructor(address _implementation, IVaultsRegistry vaultsRegistry) {  implementation = _implementation;  _vaultsRegistry = vaultsRegistry;  VaultsRegistry.addVaultImpl does not validate newImpl. This will cause an implicit revert in UUPSUpgradeable._upgradeToAndCallUUPS during upgrade.  contracts/vaults/VaultsRegistry.sol:L40-L44  function addVaultImpl(address newImpl) external override onlyOwner {  if (vaultImpls[newImpl]) revert Errors.AlreadyAdded();  vaultImpls[newImpl] = true;  emit VaultImplAdded(newImpl);  Recommendation  Enforce a check on newImpl.code.size > 0 early on.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.8 Consider Using abi.encodeCall Instead of Low-Level bytes4(keccak(..)) and abi.encodeWithSelector   ", "body": "  Resolution  Acknowledged -  By Design  Description  Consider using Solidity contract type interfaces for building low-level contract calls with arguments, instead of constructing them manually from function declaration strings.  Examples  contracts/vaults/modules/VaultVersion.sol:L26-L27  bytes4 private constant _initSelector = bytes4(keccak256('initialize(bytes)'));  contracts/vaults/modules/VaultVersion.sol:L34-L39  function upgradeToAndCall(  address newImplementation,  bytes memory data  ) public payable override onlyProxy {  super.upgradeToAndCall(newImplementation, abi.encodeWithSelector(_initSelector, data));  Recommendation  Use abi.encodeCall(IVault.initialize, (bytes data)) instead of falling back to low-level function selector calculations.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.9 Consider Reverting on Ineffective Calls   ", "body": "  Resolution  Acknowledged -  By Design. Reducing the byte code size of the vault contract means that these  validations can be omitted.  Description  Examples  Blocklist  contracts/vaults/modules/VaultBlocklist.sol:L31-L35  /// @inheritdoc IVaultBlocklist  function setBlocklistManager(address _blocklistManager) external override {  _checkAdmin();  _setBlocklistManager(_blocklistManager);  contracts/vaults/modules/VaultBlocklist.sol:L49-L53  function _setBlocklistManager(address _blocklistManager) private {  // update blocklist manager address  blocklistManager = _blocklistManager;  emit BlocklistManagerUpdated(msg.sender, _blocklistManager);  Whitelist  contracts/vaults/modules/VaultWhitelist.sol:L28-L32  /// @inheritdoc IVaultWhitelist  function setWhitelister(address _whitelister) external override {  _checkAdmin();  _setWhitelister(_whitelister);  contracts/vaults/modules/VaultWhitelist.sol:L45-L53  /**  @dev Internal function for updating the whitelister externally or from the initializer  @param _whitelister The address of the new whitelister  /  function _setWhitelister(address _whitelister) private {  // update whitelister address  whitelister = _whitelister;  emit WhitelisterUpdated(msg.sender, _whitelister);  Recommendation  Consider reverting the  _setWhitelister() (resp. _setBlocklistManager()) function execution in case one attempts to set the same _whitelister (resp. _blocklistManager ).  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.10 Vault Admin Is Non-Transferable   ", "body": "  Resolution  Acknowledged -  By Design  Description  The Vault admin is set on initialization.  There is no address(0) check preventing no admin from being set at initialization.  Admin access can only be set on initialization and not be transferred (2-step). This may leave the Vault vulnerable and the admins unable to react in case of a vault admin address compromise.  contracts/vaults/modules/VaultAdmin.sol:L24-L34  /**  @dev Initializes the VaultAdmin contract  @param _admin The address of the Vault admin  /  function __VaultAdmin_init(  address _admin,  string memory metadataIpfsHash  ) internal onlyInitializing {  admin = _admin;  emit MetadataUpdated(msg.sender, metadataIpfsHash);  Example  For reference, Fee recipient invalidates address(0)  contracts/vaults/modules/VaultFee.sol:L36-L39  function _setFeeRecipient(address _feeRecipient) private {  _checkHarvested();  if (_feeRecipient == address(0)) revert Errors.InvalidFeeRecipient();  Recommendation  Consider implementing a 2-step vault admin transfer and checking that the admin is not address(0) to prevent any mistake.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.11 Where Possible, a Specific Contract Type Should Be Used Rather Than Address   ", "body": "  Resolution  Acknowledged -  By Design  Description  Declare state variables with the best type available and downcast to address if needed. Typecasting inside the corpus of a function is unneeded when the parameter s type is known beforehand. Declare the best type in function arguments and state variables. Always return the best type available instead of resorting to address by default.  Examples  There are more instances of this pattern, but here s a list of samples:  contracts/vaults/ethereum/EthGenesisVault.sol:L71-L75  _poolEscrow = IPoolEscrow(poolEscrow);  _rewardEthToken = IRewardEthToken(rewardEthToken);  contracts/vaults/ethereum/EthGenesisVault.sol:L51-L61  constructor(  address _keeper,  address _vaultsRegistry,  address _validatorsRegistry,  address osTokenVaultController,  address osTokenConfig,  address sharedMevEscrow,  address poolEscrow,  address rewardEthToken,  uint256 exitingAssetsClaimDelay  contracts/vaults/modules/VaultImmutables.sol:L13-L22  abstract contract VaultImmutables {  /// @custom:oz-upgrades-unsafe-allow state-variable-immutable  address internal immutable _keeper;  /// @custom:oz-upgrades-unsafe-allow state-variable-immutable  address internal immutable _vaultsRegistry;  /// @custom:oz-upgrades-unsafe-allow state-variable-immutable  address internal immutable _validatorsRegistry;  contracts/vaults/modules/VaultVersion.sol:L41-L53  /// @inheritdoc UUPSUpgradeable  function _authorizeUpgrade(address newImplementation) internal view override {  _checkAdmin();  if (  newImplementation == address(0) ||  ERC1967Utils.getImplementation() == newImplementation || // cannot reinit the same implementation  IVaultVersion(newImplementation).vaultId() != vaultId() || // vault must be of the same type  IVaultVersion(newImplementation).version() != version() + 1 || // vault cannot skip versions between  !IVaultsRegistry(_vaultsRegistry).vaultImpls(newImplementation) // new implementation must be registered  ) {  revert Errors.UpgradeFailed();  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.12 A Malicious Adversary Could Theoretically DoS the Approval of New Validators.", "body": "  Description  To mitigate the withdrawal credentials front-running vulnerability, Stakewise requires oracles to sign the validators registry s Merkle tree root when approving new validators. This ensures that if a malicious operator attempts to front-run a legitimate deposit transaction with different withdrawal credentials, the Merkle tree root of the deposit contract will change, invalidating the legitimate deposit transaction through a check in the KeeperValidators contract:  contracts/keeper/KeeperValidators.sol:L53-L55  if (_validatorsRegistry.get_deposit_root() != params.validatorsRegistryRoot) {  revert Errors.InvalidValidatorsRegistryRoot();  It should be noted that this mechanism potentially opens the door to a DoS attack. Specifically, a malicious actor could theoretically disrupt validator approval by front-running legitimate deposit transactions with a deposit of at least 1 ETH into the validator registry contract. However, such an attack would likely be costly and resource-intensive to sustain over time.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.13 Follow Ethereum Secure Coding and Style Guidelines", "body": "  Description  Follow the solidity style guide. Specifically, constants should be named with all capital letters with underscores separating words. Examples: MAX_BLOCKS, TOKEN_NAME, TOKEN_TICKER, CONTRACT_VERSION.  Examples  contracts/vaults/modules/VaultEthStaking.sol:L31  uint256 private constant _securityDeposit = 1e9;  contracts/vaults/modules/VaultFee.sol:L18-L19  uint256 internal constant _maxFeePercent = 10_000; // @dev 100.00 %  contracts/vaults/modules/VaultValidators.sol:L26-L27  uint256 internal constant _validatorLength = 176;  contracts/vaults/modules/VaultVersion.sol:L26-L27  bytes4 private constant _initSelector = bytes4(keccak256('initialize(bytes)'));  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.14 _processTotalAssetsDelta() Should Return Early on totalAssetsDelta == 0", "body": "  Description  Consider taking the if-branch for totalAssetsDelta less than or equal zero and return early instead of consuming gas on mulDiv and performing accounting.  contracts/vaults/modules/VaultState.sol:L101-L116  /**  @dev Internal function for processing rewards and penalties  @param totalAssetsDelta The number of assets earned or lost  /  function _processTotalAssetsDelta(int256 totalAssetsDelta) internal {  // SLOAD to memory  uint256 newTotalAssets = _totalAssets;  if (totalAssetsDelta < 0) {  // add penalty to total assets  newTotalAssets -= uint256(-totalAssetsDelta);  // update state  _totalAssets = SafeCast.toUint128(newTotalAssets);  return;  Recommendation  The logical error can be addressed by modifying the conditional check to if (totalAssetsDelta <= 0). This accommodates when the totalAssetsDelta parameter is 0. When it is so, neither a fee should be deducted from nor should the function proceed with the reward processing. This if-statement can simply return in such a case.  function _processTotalAssetsDelta(int256 totalAssetsDelta) internal {  // SLOAD to memory  uint256 newTotalAssets = _totalAssets;  if (totalAssetsDelta =< 0) {  // ...  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "4.15 Unused or Duplicate Imports", "body": "  Description  Several source units contain imports for libraries or contracts that are not utilized within the codebase.  Unused imports contribute to code clutter and may confuse developers about the library s use in the contract. Keeping the codebase clean can help with maintainability and readability.  Example  Unused Import  contracts/vaults/modules/VaultEnterExit.sol:L8  import {IKeeperRewards} from '../../interfaces/IKeeperRewards.sol';  Unused Import  contracts/vaults/ethereum/EthPrivVault.sol:L14  import {VaultVersion} from '../modules/VaultVersion.sol';  Duplicate Import  contracts/vaults/ethereum/EthErc20Vault.sol:L8-L10  import {IEthVaultFactory} from '../../interfaces/IEthVaultFactory.sol';  import {IEthErc20Vault} from '../../interfaces/IEthErc20Vault.sol';  import {IEthVaultFactory} from '../../interfaces/IEthVaultFactory.sol';  Recommendation  Adhering to best practices, it is recommended to eliminate any unused imports to ensure the cleanliness of the codebase. Consequently, the removal of these unused imports from the contracts is advisable to maintain codebase integrity and enhance overall code quality.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2024/03/stakewise-v3-vaults-/-ethfoxvault/"}, {"title": "6.1 Collaterals are not guaranteed to be returned after a batch is cancelled    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising#162  Description  When traders open buy orders, they also transfer collateral tokens to the market maker contract. If the current batch is going to be cancelled, there is a chance that these collateral tokens will not be returned to the traders.  Examples  If a current collateralsToBeClaimed value is zero on a batch initialization and in this new batch only buy orders are submitted, collateralsToBeClaimed value will still stay zero.  At the same time if in Tap contract tapped amount was bigger than _maximumWithdrawal() on batch initialisation, _maximumWithdrawal() will most likely increase when the traders transfer new collateral tokens with the buy orders. And a beneficiary will be able to withdraw part of these tokens. Because of that, there might be not enough tokens to withdraw by the traders if the batch is cancelled.  It s partially mitigated by having floor value in Tap contract, but if there are more collateral tokens in the batch than floor, the issue is still valid.  Recommendation  Ensure that tapped is not bigger than _maximumWithdrawal()  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.2 Fees can be changed during the batch    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@0941f53 by storing current fee in meta batch.  Description  Shareholders can vote to change the fees. For buy orders, fees are withdrawn immediately when order is submitted and the only risk is frontrunning by the shareholder s voting contract.  For sell orders, fees are withdrawn when a trader claims an order and withdraws funds in _claimSellOrder  function:  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L790-L792  if (fee > 0) {  reserve.transfer(_collateral, beneficiary, fee);  Fees can be changed between opening order and claiming this order which makes the fees unpredictable.  Recommendation  Fees for an order should not be updated during its lifetime.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.3 Bancor formula should not be updated during the batch    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@a8c2e21 by storing a ref to the Formula with the meta batch.  Description  Shareholders can vote to change the bancor formula contract. That can make a price in the current batch unpredictable.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L212-L216  function updateFormula(IBancorFormula _formula) external auth(UPDATE_FORMULA_ROLE) {  require(isContract(_formula), ERROR_CONTRACT_IS_EOA);  _updateFormula(_formula);  Recommendation  Bancor formula update should be executed in the next batch or with a timelock that is greater than batch duration.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.4 Maximum slippage shouldn t be updated for the current batch    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@aa4f03e by storing slippage with the batch.  Description  When anyone submits a new order, the batch price is updated and it s checked whether the price slippage is acceptable. The problem is that the maximum slippage can be updated during the batch and traders cannot be sure that price is limited as they initially expected.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L487-L489  function _slippageIsValid(Batch storage _batch, address _collateral) internal view returns (bool) {  uint256 staticPricePPM = _staticPricePPM(_batch.supply, _batch.balance, _batch.reserveRatio);  uint256 maximumSlippage = collaterals[_collateral].slippage;  Additionally, if a maximum slippage is updated to a lower value, some of the orders that should lower the current slippage will also revert.  Recommendation  Save a slippage value on batch initialization and use it during the current batch.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.5 AragonFundraisingController - an untapped address in toReset can block attempts of opening Trading after presale    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@9451147 by checking if token is tapped. Gas consumption is increased due to external call to Tap to check if token is actually tapped. The number of tokens to be reset is capped.  Description  AragonFundraisingController can be initialized with a list of token addresses _toReset that are to be reset when trading opens after the presale. These addresses are supposed to be addresses of tapped tokens. However, the list needs to be known when initializing the contract but the tapped tokens are added after initialization when calling addCollateralToken (and tapped with _rate>0). This can lead to an inconsistency that blocks openTrading.  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L99-L102  for (uint256 i = 0; i < _toReset.length; i++) {  require(_tokenIsContractOrETH(_toReset[i]), ERROR_INVALID_TOKENS);  toReset.push(_toReset[i]);  In case a token address makes it into the list of toReset tokens that is not tapped it will be impossible to openTrading as tap.resetTappedToken(toReset[i]); throws for untapped tokens. According to the permission setup in FundraisingMultisigTemplate only Controller can call Marketmaker.open  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L163-L169  function openTrading() external auth(OPEN_TRADING_ROLE) {  for (uint256 i = 0; i < toReset.length; i++) {  tap.resetTappedToken(toReset[i]);  marketMaker.open();  Recommendation  Instead of initializing the Controller with a list of tapped tokens to be reset when trading opens, add a flag to addCollateralToken to indicate that the token should be reset when calling openTrading, making sure only tapped tokens are added to this list. This also allows adding tapped tokens that are to be reset at a later point in time.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.6 Tap payments inconsistency    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising#162  Description  Every time project managers want to withdraw tapped funds, the maximum amount of withdrawable funds is calculated in tap._maximumWithdrawal function. The method ensures that project managers can only withdraw unlocked funds (balance exceeding the collaterals minimum comprised of the collaterals configured floor including the minimum tokens to hold) even though their allowance might be higher.  if there are no unlocked funds available, the maximum withdrawal is zero (balance <= minimum).  if there are unlocked funds available (balance > minimum) and the allowance (tapped) would result in a balance >= minimum, the maximum withdrawal amount is the calculated allowance tapped.  if there are unlocked funds available (balance > minimum) and the allowance (tapped) would result in a balance < minimum, the maximum withdrawal amount tapped is capped to balance - minimum to ensure that the remaining collateral balance is at least at the minimum and not below.  This means that in the case of (3) if there are not enough funds to withdraw tapped(time*tap_rate) amount of tokens, it gets truncated and only a part of tapped tokens gets withdrawn.  code/apps/tap/contracts/Tap.sol:L239-L255  function _maximumWithdrawal(address _token) internal view returns (uint256) {  uint256 toBeClaimed = controller.collateralsToBeClaimed(_token);  uint256 floor = floors[_token];  uint256 minimum = toBeClaimed.add(floor);  uint256 balance = _token == ETH ? address(reserve).balance : ERC20(_token).staticBalanceOf(reserve);  uint256 tapped = (_currentBatchId().sub(lastWithdrawals[_token])).mul(rates[_token]);  if (minimum >= balance) {  return 0;  if (balance >= tapped.add(minimum)) {  return tapped;  return balance.sub(minimum);  The problem is that the remaining tokens  (tapped - capped_tapped) cannot be claimed afterward and tapped value is reset to zero.  Remediation  In case the maximum withdrawal amount gets capped, the information about the remaining tokens that the project team should have been able to withdraw should be kept to allow them to withdraw the tokens at a later point in time when there are enough funds for it.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.7 [New] Tapped collaterals can be bought by traders   ", "body": "  Resolution  This behaviour is intentional and if there is not a lot of funds in the pool, shareholders have a priority to buy tokens even if these tokens can already be withdrawn by the beneficiary. It is done in order to protect shareholders in case if the project is dying and running out of funds. The downside of this behaviour is that it creates an additional incentive for the beneficiary to withdraw tapped tokens as soon and as often as possible which creates a race condition.  Description  When a trader submits a sell order, _openSellOrder() function checks that there are enough tokens in reserve by calling _poolBalanceIsSufficient function  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L483-L485  function _poolBalanceIsSufficient(address _collateral) internal view returns (bool) {  return controller.balanceOf(address(reserve), _collateral) >= collateralsToBeClaimed[_collateral];  the problem is that because collateralsToBeClaimed[_collateral] has increased, controller.balanceOf(address(reserve), _collateral) could also increase. It happens so because controller.balanceOf() function subtracts tapped amount from the reserve s balance.  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L358-L366  function balanceOf(address _who, address _token) public view isInitialized returns (uint256) {  uint256 balance = _token == ETH ? _who.balance : ERC20(_token).staticBalanceOf(_who);  if (_who == address(reserve)) {  return balance.sub(tap.getMaximumWithdrawal(_token));  } else {  return balance;  And tap.getMaximumWithdrawal(_token) could decrease because it depends on collateralsToBeClaimed[_collateral]  apps/tap/contracts/Tap.sol:L231-L264  function _tappedAmount(address _token) internal view returns (uint256) {  uint256 toBeKept = controller.collateralsToBeClaimed(_token).add(floors[_token]);  uint256 balance = _token == ETH ? address(reserve).balance : ERC20(_token).staticBalanceOf(reserve);  uint256 flow = (_currentBatchId().sub(lastTappedAmountUpdates[_token])).mul(rates[_token]);  uint256 tappedAmount = tappedAmounts[_token].add(flow);  /**  whatever happens enough collateral should be  kept in the reserve pool to guarantee that  its balance is kept above the floor once  all pending sell orders are claimed  /  /**  the reserve's balance is already below the balance to be kept  the tapped amount should be reset to zero  /  if (balance <= toBeKept) {  return 0;  /**  the reserve's balance minus the upcoming tap flow would be below the balance to be kept  the flow should be reduced to balance - toBeKept  /  if (balance <= toBeKept.add(tappedAmount)) {  return balance.sub(toBeKept);  /**  the reserve's balance minus the upcoming flow is above the balance to be kept  the flow can be added to the tapped amount  /  return tappedAmount;  That means that the amount that beneficiary can withdraw has just decreased, which should not be possible.  Recommendation  Ensure that tappedAmount cannot be decreased once updated.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.8 Presale - contributionToken double cast and invalid comparison    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@61f5803.  Description  Examples  contribute - invalid comparison of contract type against address(0x00). Even though this is accepted in solidity <0.5.0 it is going to raise a compiler error with newer versions (>=0.5.0).  code/apps/presale/contracts/Presale.sol:L163-L170  function contribute(address _contributor, uint256 _value) external payable nonReentrant auth(CONTRIBUTE_ROLE) {  require(state() == State.Funding, ERROR_INVALID_STATE);  if (contributionToken == ETH) {  require(msg.value == _value, ERROR_INVALID_CONTRIBUTE_VALUE);  } else {  require(msg.value == 0,      ERROR_INVALID_CONTRIBUTE_VALUE);  _transfer - double cast token to ERC20 if it is the contribution token.  code/apps/presale/contracts/Presale.sol:L344-L344  require(ERC20(_token).safeTransfer(_to, _amount), ERROR_TOKEN_TRANSFER_REVERTED);  Recommendation  contributionToken can either be ETH or a valid ERC20 contract address. It is therefore recommended to store the token as an address type instead of the more precise contract type to resolve the double cast and the invalid contract type to address comparison or cast the ERC20 type to address() before comparison.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.9 Fees are not returned for buy orders if a batch is canceled   ", "body": "  Resolution  This issue has been addressed with the following statement:  The only situation where a batch can be cancelled is when a collateral is un-whitelisted. This is obviously a very critical operation that we introduced just in case the collateral happened to be malicious token. Handling the ability to return fees in case a batch order is cancelled would thus add a lot of computation overhead for: a. a very unlikely situation b. where the fees would anyhow be returned in a malicious token. c. given a small amount [it s a fee and not the main amount]. We figured out that it was a bad decision to add gas overhead to all orders just to prevent this situation.  Description  Every trader pays fees on each buy order and transfers it directly to the beneficiary.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L706-L713  uint256 fee = _value.mul(buyFeePct).div(PCT_BASE);  uint256 value = _value.sub(fee);  // collect fee and collateral  if (fee > 0) {  _transfer(_buyer, beneficiary, _collateral, fee);  _transfer(_buyer, address(reserve), _collateral, value);  If the batch is canceled, fees are not returned to the traders because there is no access to the beneficiary account.  Additionally, fees are returned to traders for all the sell orders if the batch is canceled.  Recommendation  Consider transferring fees to a beneficiary only after the batch is over.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.10 Tap - Controller should not be updateable    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@f6054443 by removing update functionality.  Description  Similar to the issue 6.11, Tap allows updating the Controller contract it is using. The permission is currently not assigned in the FundraisingMultisigTemplate but might be used in custom deployments.  code/apps/tap/contracts/Tap.sol:L117-L125  /**  @notice Update controller to `_controller`  @param _controller The address of the new controller contract  /  function updateController(IAragonFundraisingController _controller) external auth(UPDATE_CONTROLLER_ROLE) {  require(isContract(_controller), ERROR_CONTRACT_IS_EOA);  _updateController(_controller);  Recommendation  To avoid inconsistencies, we suggest to remove this functionality and provide a guideline on how to safely upgrade components of the system.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.11 Tap - reserve can be updated in Tap but not in MarketMaker or Controller    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@987720b1 by removing update functionality.  Description  The address of the pool/reserve contract can be updated in Tap if someone owns the UPDATE_RESERVE_ROLE permission. The permission is currently not assigned in the template.  The reserve is being referenced by multiple Contracts. Tap interacts with it to transfer funds to the beneficiary, Controller adds new protected tokens, and MarketMaker transfers funds when someone sells their Shareholder token.  Updating reserve only in Tap is inconsistent with the system as the other contracts are still referencing the old reserve unless they are updated via the Aragon Application update mechanisms.  code/apps/tap/contracts/Tap.sol:L127-L135  /**  @notice Update reserve to `_reserve`  @param _reserve The address of the new reserve [pool] contract  /  function updateReserve(Vault _reserve) external auth(UPDATE_RESERVE_ROLE) {  require(isContract(_reserve), ERROR_CONTRACT_IS_EOA);  _updateReserve(_reserve);  Recommendation  Remove the possibility to update reserve in Tap to keep the system consistent. Provide information about update mechanisms in case the reserve needs to be updated for all components.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.12 Presale can be opened earlier than initially assigned date    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@0726e29.  Description  There are 2 ways how presale opening date can be assigned. Either it s defined on initialization or the presale will start when open() function is executed.  code/apps/presale/contracts/Presale.sol:L144-L146  if (_openDate != 0) {  _setOpenDate(_openDate);  The problem is that even if openDate is assigned to some non-zero date, it can still be opened earlier by calling open() function.  code/apps/presale/contracts/Presale.sol:L152-L156  function open() external auth(OPEN_ROLE) {  require(state() == State.Pending, ERROR_INVALID_STATE);  _open();  Recommendation  Require that openDate is not set (0) when someone manually calls the open() function.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.13 Presale - should not allow zero value contributions    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@6a6e222.  Description  The Presale accepts zero value contributions emitting a contribution event if none of the Aragon components (TokenManager, MinimeToken) raises an exception.  code/apps/presale/contracts/Presale.sol:L163-L173  function contribute(address _contributor, uint256 _value) external payable nonReentrant auth(CONTRIBUTE_ROLE) {  require(state() == State.Funding, ERROR_INVALID_STATE);  if (contributionToken == ETH) {  require(msg.value == _value, ERROR_INVALID_CONTRIBUTE_VALUE);  } else {  require(msg.value == 0,      ERROR_INVALID_CONTRIBUTE_VALUE);  _contribute(_contributor, _value);  Recommendation  Reject zero value ETH or ERC20 contributions.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.14 Compiler Warnings - Function state mutability can be restricted to view    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@cfd677a.  Description  The following methods are not state-changing and can, therefore, be restricted to view.  Recommendation  Restrict function state mutability of the listed methods to view.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.15 FundraisingMultisigTemplate - should use BaseTemplate._createPermissionForTemplate() to assign permissions to itself    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@dd153e0.  Description  The template temporarily assigns permissions to itself to be able to configure parts of the system. This can either be done by calling acl.createPermission(address(this), app, role, manager) or by using a distinct method provided with the DAO-Templates BaseTemplate _createPermissionForTemplate.  We suggest that in order to make it clear that permissions are assigned to the template and make it easier to audit that permissions are either revoked or transferred before the DAO is transferred to the new user, the method provided and used with the default Aragon DAO-Templates should be used.  use createPermission if permissions are assigned to an entity other than the template contract.  use _createPermissionForTemplate when creating permissions for the template contract.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L333-L334  // create and grant ADD_PROTECTED_TOKEN_ROLE to this template  acl.createPermission(this, controller, controller.ADD_COLLATERAL_TOKEN_ROLE(), this);  Sidenote: pass address(this) instead of the contract instance to createPermission.  Recommendation  Use BaseTemplate._createPermissionForTemplate to assign permissions to the template.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.16 FundraisingMultisigTemplate - misleading comments    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@3b4700a and  AragonBlack/fundraising@40c465fc.  Description  The comment mentionsADD_PROTECTED_TOKEN_ROLE but permissions for ADD_COLLATERAL_TOKEN_ROLE are created.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L333-L334  // create and grant ADD_PROTECTED_TOKEN_ROLE to this template  acl.createPermission(this, controller, controller.ADD_COLLATERAL_TOKEN_ROLE(), this);  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L355-L356  // transfer ADD_PROTECTED_TOKEN_ROLE  _transferPermissionFromTemplate(acl, controller, shareVoting, controller.ADD_COLLATERAL_TOKEN_ROLE(), shareVoting);  Recommendation  ADD_PROTECTED_TOKEN_ROLE in the comment should be ADD_COLLATERAL_TOKEN_ROLE.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.17 FundraisingMultisigTemplate - unnecessary cast to address    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@0e00269.  Description  The addresses of DAI (argument address _dai) and AND (argument address _ant) are unnecessarily cast to address.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L58-L76  constructor(  DAOFactory              _daoFactory,  ENS                     _ens,  MiniMeTokenFactory      _miniMeFactory,  IFIFSResolvingRegistrar _aragonID,  address                 _dai,  address                 _ant  BaseTemplate(_daoFactory, _ens, _miniMeFactory, _aragonID)  public  _ensureAragonIdIsValid(_aragonID);  _ensureMiniMeFactoryIsValid(_miniMeFactory);  _ensureTokenIsContractOrETH(_dai);  _ensureTokenIsContractOrETH(_ant);  collaterals.push(address(_dai));  collaterals.push(address(_ant));  Recommendation  Both arguments are already of type address, therefore remove the explicit cast to address() when pushing to the collaterals array.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.18 FundraisingMultisigTemplate - unused import ERC20    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@73481d1.  Description  The interface ERC20 is imported but never used.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L4-L4  import \"@aragon/os/contracts/lib/token/ERC20.sol\";  Recommendation  Remove the unused import.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.19 FundraisingMultisigTemplate - DAI/ANT token address cannot be zero    ", "body": "  Resolution                           Fixed with   AragonBlack/fundraising@da561ce.  Description  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L71-L72  _ensureTokenIsContractOrETH(_dai);  _ensureTokenIsContractOrETH(_ant);  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L572-L575  function _ensureTokenIsContractOrETH(address _token) internal view returns (bool) {  require(isContract(_token) || _token == ETH, ERROR_BAD_SETTINGS);  Recommendation  Use isContract() instead of _ensureTokenIsContractOrETH() and optionally require that collateral[0] != collateral[1] as an additional check to prevent that the fundraising template is being deployed with an invalid configuration.  7 Tool-Based Analysis  Several tools were used to perform an automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open-source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium --version  Solium version 1.2.5  $ solium -d  apps/aragon-fundraising/contracts/AragonFundraisingController.sol  370:5    error    Only use indent of 4 spaces.    indentation  templates/multisig/contracts/FundraisingMultisigTemplate.sol  573:5    error    Only use indent of 4 spaces.    indentation  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers (please use horizontal scroll to view all columns):  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AragonFundraisingController  Implementation  EtherTokenConstant, IsContract, IAragonFundraisingController, AragonApp  initialize  External    onlyInit  updateBeneficiary  External    auth  updateFees  External    auth  openPresale  External    auth  closePresale  External    isInitialized  contribute  External    auth  refund  External    isInitialized  openTrading  External    auth  openBuyOrder  External    auth  openSellOrder  External    auth  claimBuyOrder  External    isInitialized  claimSellOrder  External    isInitialized  addCollateralToken  External    auth  reAddCollateralToken  External    auth  removeCollateralToken  External    auth  updateCollateralToken  External    auth  updateMaximumTapRateIncreasePct  External    auth  updateMaximumTapFloorDecreasePct  External    auth  addTokenTap  External    auth  updateTokenTap  External    auth  withdraw  External    auth  token  Public    isInitialized  contributionToken  Public    isInitialized  getMaximumWithdrawal  Public    isInitialized  collateralsToBeClaimed  Public    isInitialized  balanceOf  Public    isInitialized  _tokenIsContractOrETH  Internal \ud83d\udd12  BancorFormula  Implementation  IBancorFormula, Utils  <Constructor>  Public    calculatePurchaseReturn  Public    NO   calculateSaleReturn  Public    NO   calculateCrossConnectorReturn  Public    NO   power  Internal \ud83d\udd12  generalLog  Internal \ud83d\udd12  floorLog2  Internal \ud83d\udd12  findPositionInMaxExpArray  Internal \ud83d\udd12  generalExp  Internal \ud83d\udd12  optimalLog  Internal \ud83d\udd12  optimalExp  Internal \ud83d\udd12  BatchedBancorMarketMaker  Implementation  EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  open  External    auth  updateFormula  External    auth  updateBeneficiary  External    auth  updateFees  External    auth  addCollateralToken  External    auth  removeCollateralToken  External    auth  updateCollateralToken  External    auth  openBuyOrder  External    auth  openSellOrder  External    auth  claimBuyOrder  External    nonReentrant isInitialized  claimSellOrder  External    nonReentrant isInitialized  claimCancelledBuyOrder  External    nonReentrant isInitialized  claimCancelledSellOrder  External    nonReentrant isInitialized  getCurrentBatchId  Public    isInitialized  getCollateralToken  Public    isInitialized  getBatch  Public    isInitialized  getStaticPricePPM  Public    isInitialized  _staticPricePPM  Internal \ud83d\udd12  _currentBatchId  Internal \ud83d\udd12  _beneficiaryIsValid  Internal \ud83d\udd12  _feeIsValid  Internal \ud83d\udd12  _reserveRatioIsValid  Internal \ud83d\udd12  _tokenManagerSettingIsValid  Internal \ud83d\udd12  _collateralValueIsValid  Internal \ud83d\udd12  _bondAmountIsValid  Internal \ud83d\udd12  _collateralIsWhitelisted  Internal \ud83d\udd12  _batchIsOver  Internal \ud83d\udd12  _batchIsCancelled  Internal \ud83d\udd12  _userIsBuyer  Internal \ud83d\udd12  _userIsSeller  Internal \ud83d\udd12  _poolBalanceIsSufficient  Internal \ud83d\udd12  _slippageIsValid  Internal \ud83d\udd12  _buySlippageIsValid  Internal \ud83d\udd12  _sellSlippageIsValid  Internal \ud83d\udd12  _currentBatch  Internal \ud83d\udd12  _open  Internal \ud83d\udd12  _updateBeneficiary  Internal \ud83d\udd12  _updateFormula  Internal \ud83d\udd12  _updateFees  Internal \ud83d\udd12  _cancelCurrentBatch  Internal \ud83d\udd12  _addCollateralToken  Internal \ud83d\udd12  _removeCollateralToken  Internal \ud83d\udd12  _updateCollateralToken  Internal \ud83d\udd12  _openBuyOrder  Internal \ud83d\udd12  _openSellOrder  Internal \ud83d\udd12  _claimBuyOrder  Internal \ud83d\udd12  _claimSellOrder  Internal \ud83d\udd12  _claimCancelledBuyOrder  Internal \ud83d\udd12  _claimCancelledSellOrder  Internal \ud83d\udd12  _updatePricing  Internal \ud83d\udd12  _transfer  Internal \ud83d\udd12  Presale  Implementation  EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  open  External    auth  contribute  External    nonReentrant auth  refund  External    nonReentrant isInitialized  close  External    nonReentrant isInitialized  contributionToTokens  Public    isInitialized  state  Public    isInitialized  _timeSinceOpen  Internal \ud83d\udd12  _setOpenDate  Internal \ud83d\udd12  _setVestingDatesWhenOpenDateIsKnown  Internal \ud83d\udd12  _open  Internal \ud83d\udd12  _contribute  Internal \ud83d\udd12  _refund  Internal \ud83d\udd12  _close  Internal \ud83d\udd12  _transfer  Internal \ud83d\udd12  Tap  Implementation  TimeHelpers, EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  updateController  External    auth  updateReserve  External    auth  updateBeneficiary  External    auth  updateMaximumTapRateIncreasePct  External    auth  updateMaximumTapFloorDecreasePct  External    auth  addTappedToken  External    auth  removeTappedToken  External    auth  updateTappedToken  External    auth  resetTappedToken  External    auth  withdraw  External    auth  getMaximumWithdrawal  Public    isInitialized  _currentBatchId  Internal \ud83d\udd12  _maximumWithdrawal  Internal \ud83d\udd12  _beneficiaryIsValid  Internal \ud83d\udd12  _maximumTapFloorDecreasePctIsValid  Internal \ud83d\udd12  _tokenIsContractOrETH  Internal \ud83d\udd12  _tokenIsTapped  Internal \ud83d\udd12  _tapRateIsValid  Internal \ud83d\udd12  _tapUpdateIsValid  Internal \ud83d\udd12  _tapRateUpdateIsValid  Internal \ud83d\udd12  _tapFloorUpdateIsValid  Internal \ud83d\udd12  _updateController  Internal \ud83d\udd12  _updateReserve  Internal \ud83d\udd12  _updateBeneficiary  Internal \ud83d\udd12  _updateMaximumTapRateIncreasePct  Internal \ud83d\udd12  _updateMaximumTapFloorDecreasePct  Internal \ud83d\udd12  _addTappedToken  Internal \ud83d\udd12  _removeTappedToken  Internal \ud83d\udd12  _updateTappedToken  Internal \ud83d\udd12  _resetTappedToken  Internal \ud83d\udd12  _withdraw  Internal \ud83d\udd12  FundraisingMultisigTemplate  Implementation  EtherTokenConstant, BaseTemplate  <Constructor>  Public    BaseTemplate  prepareInstance  External    NO   installShareApps  External    NO   installFundraisingApps  External    NO   finalizeInstance  External    NO   _installBoardApps  Internal \ud83d\udd12  _installShareApps  Internal \ud83d\udd12  _installFundraisingApps  Internal \ud83d\udd12  _proxifyFundraisingApps  Internal \ud83d\udd12  _initializePresale  Internal \ud83d\udd12  _initializeMarketMaker  Internal \ud83d\udd12  _initializeTap  Internal \ud83d\udd12  _initializeController  Internal \ud83d\udd12  _setupCollaterals  Internal \ud83d\udd12  _setupBoardPermissions  Internal \ud83d\udd12  _setupSharePermissions  Internal \ud83d\udd12  _setupFundraisingPermissions  Internal \ud83d\udd12  _cacheDao  Internal \ud83d\udd12  _cacheBoardApps  Internal \ud83d\udd12  _cacheShareApps  Internal \ud83d\udd12  _cacheFundraisingApps  Internal \ud83d\udd12  _daoCache  Internal \ud83d\udd12  _boardAppsCache  Internal \ud83d\udd12  _shareAppsCache  Internal \ud83d\udd12  _fundraisingAppsCache  Internal \ud83d\udd12  _clearCache  Internal \ud83d\udd12  _vaultCache  Internal \ud83d\udd12  _shareTMCache  Internal \ud83d\udd12  _reserveCache  Internal \ud83d\udd12  _presaleCache  Internal \ud83d\udd12  _controllerCache  Internal \ud83d\udd12  _ensureTokenIsContractOrETH  Internal \ud83d\udd12  _ensureBoardAppsCache  Internal \ud83d\udd12  _ensureShareAppsCache  Internal \ud83d\udd12  _ensureFundraisingAppsCache  Internal \ud83d\udd12  _registerApp  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.4 Test Coverage Measurement", "body": "  Testing is implemented using Truffle an all provided test cases pass. However, the Presale contract fails to generate coverage statistics.  MarketMaker  Controller  Tap  Presale  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "5.1 Inconsistency Between Actual IAccount Interface and Published Interface ID    ", "body": "  Resolution                           After the official end of this engagement, the community has come to the decision that   Description  As an analog to Ethereum Improvement Proposals (EIPs), there are StarkNet Improvement Proposals (SNIPs), and SNIP-5   similar in intention and technique to ERC-165   defines how to publish and detect what interfaces a smart contract implements. As in ERC-165, this is achieved with the help of an interface identifier.  contracts/account/src/argent_account.cairo:L506-L522  fn supports_interface(self: @ContractState, interface_id: felt252) -> bool {  if interface_id == ERC165_IERC165_INTERFACE_ID {  true  } else if interface_id == ERC165_ACCOUNT_INTERFACE_ID {  true  } else if interface_id == ERC165_OUTSIDE_EXECUTION_INTERFACE_ID {  true  } else if interface_id == ERC165_IERC165_INTERFACE_ID_OLD {  true  } else if interface_id == ERC165_ACCOUNT_INTERFACE_ID_OLD_1 {  true  } else if interface_id == ERC165_ACCOUNT_INTERFACE_ID_OLD_2 {  true  } else {  false  In this issue, we re interested in ERC165_ACCOUNT_INTERFACE_ID, which is defined as follows:  contracts/lib/src/account.cairo:L3-L4  const ERC165_ACCOUNT_INTERFACE_ID: felt252 =  0x32a450d0828523e159d5faa1f8bc3c94c05c819aeb09ec5527cd8795b5b5067;  This ID corresponds to an interface with the following function signatures:  fn __validate__(Array<Call>) -> felt252;  fn __execute__(Array<Call>) -> Array<Span<felt252>>;  fn is_valid_signature(felt252, Array<felt252>) -> bool;  Note that is_valid_signature returns a bool. However, in the actual IAccount interface, this function returns a felt252:  contracts/lib/src/account.cairo:L10-L17  // InterfaceID: 0x32a450d0828523e159d5faa1f8bc3c94c05c819aeb09ec5527cd8795b5b5067  trait IAccount<TContractState> {  fn __validate__(ref self: TContractState, calls: Array<Call>) -> felt252;  fn __execute__(ref self: TContractState, calls: Array<Call>) -> Array<Span<felt252>>;  fn is_valid_signature(  self: @TContractState, hash: felt252, signatures: Array<felt252>  ) -> felt252;  If we check out the implementation of is_valid_signature, we see that it returns the magic value 0x1626ba7e known from ERC-1271 if the signature is valid and 0 otherwise:  contracts/account/src/argent_account.cairo:L214-L222  fn is_valid_signature(  self: @ContractState, hash: felt252, signatures: Array<felt252>  ) -> felt252 {  if self.is_valid_span_signature(hash, signatures.span()) {  ERC1271_VALIDATED  } else {  contracts/lib/src/account.cairo:L8  const ERC1271_VALIDATED: felt252 = 0x1626ba7e;  The ID for this interface would be 0x2ceccef7f994940b3962a6c67e0ba4fcd37df7d131417c604f91e03caecc1cd. Note that, unlike in ERC-165, in SNIP-5, the return type of a function does matter for the interface identifier. Hence, the actual IAccount interface defined and implemented and the published interface ID do not match.  Recommendation  At the end of this engagement, the community has not come to a decision yet whether is_valid_signature should return a bool or a felt252 (i.e., the magic value 0x1626ba7e in the affirmative case  and 0 otherwise). Depending on the outcome, either the actual interface and its implementation or the published interface ID must be changed to achieve consistency between the two.  Remark  SNIP-5 is not very clear on how to deal with the new Cairo syntax introduced in v2.0.0 of the compiler. Specifically, with this new syntax, interface traits have a generic parameter TContractState, and all non-static functions in the interface have a first parameter self of type TContractState or @TContractState for view functions. How to deal with this parameter in the derivation of the interface identifier is not (yet) explicitly specified in the proposal, but the Argent team has assured us that the understanding in the community is to ignore this parameter for the extended function selectors and hence the interface ID.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/argent-account-multisig-for-starknet/"}, {"title": "5.2 Wrong ID for OutsideExecution Interface    ", "body": "  Description  While not standardized across the community, the Argent team has decided to isolate the  outside execution  functionality in a separate interface, so other teams in the ecosystem can choose to implement that interface as well.  contracts/lib/src/outside_execution.cairo:L10-L29  /// Interface ID: 0x3a8eb057036a72671e68e4bad061bbf5740d19351298b5e2960d72d76d34cb9  // get_outside_execution_message_hash is not part of the standard interface  #[starknet::interface]  trait IOutsideExecution<TContractState> {  /// @notice This method allows anyone to submit a transaction on behalf of the account as long as they have the relevant signatures  /// @param outside_execution The parameters of the transaction to execute  /// @param signature A valid signature on the Eip712 message encoding of `outside_execution`  /// @notice This method allows reentrancy. A call to `__execute__` or `execute_from_outside` can trigger another nested transaction to `execute_from_outside`.  fn execute_from_outside(  ref self: TContractState, outside_execution: OutsideExecution, signature: Array<felt252>  ) -> Array<Span<felt252>>;  /// Get the status of a given nonce, true if the nonce is available to use  fn is_valid_outside_execution_nonce(self: @TContractState, nonce: felt252) -> bool;  /// Get the message hash for some `OutsideExecution` following Eip712. Can be used to know what needs to be signed  fn get_outside_execution_message_hash(  self: @TContractState, outside_execution: OutsideExecution  ) -> felt252;  SNIP-5   as already mentioned in issue 5.1   is a StarkNet Improvement Proposal that describes how to publish and detect what interfaces a contract implements. To briefly summarize, the interface ID is defined as the XOR of the extended selectors of the functions in the interface, and a function s extended selector is the starknet_keccak hash of the function signature, where some special rules define how to deal with the different data types. Deriving the input for starknet_keccak can be done manually, but it is tedious, error-prone, and can even be somewhat involved, as it may require knowledge of some Cairo internals, depending on the types used in the function.  When we tried to verify the ID for the OutsideExecution interface, we noticed a mismatch between the result of our own calculations and the ID the Argent team had arrived at:  contracts/lib/src/outside_execution.cairo:L7-L8  const ERC165_OUTSIDE_EXECUTION_INTERFACE_ID: felt252 =  0x3a8eb057036a72671e68e4bad061bbf5740d19351298b5e2960d72d76d34cb9;  Together with the client, we were able to identify a mistake that was made in the manual derivation of the input to the hash function, leading to a wrong extended function selector and, therefore, an incorrect interface identifier.  The correct extended function selector for execute_from_outside is:  (The line breaks were only inserted for better readability in this document. The string does not contain any whitespace.)  Recommendation  Together with the extended function selector for is_valid_outside_execution_nonce,  0x3ae284922d559e87220df9c5a51dae59c391ce8f3b4fabb572275e210299df4, the resulting interface ID for OutsideExecution is 0x68cfd18b92d1907b8ba3cc324900277f5a3622099431ea85dd8089255e4181, and the definition of ERC165_OUTSIDE_EXECUTION_INTERFACE_ID should be changed accordingly.  Note that the Argent team has deliberately omitted get_outside_execution_message_hash from the interface (in the sense of SNIP-5).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/argent-account-multisig-for-starknet/"}, {"title": "5.3 change_owner Selector Test Missing   ", "body": "  Description  The Argent Account contract employs many hard-coded constants in its logic, for example for the function selectors. Consequently, there is a test for each one of these selectors. Although the tests were not in scope for this audit, we noticed that one selector test is missing    the change_owner selector with value 658036363289841962501247229249022783727527757834043681434485756469236076608.  contracts/account/src/argent_account.cairo:L46-L47  const CHANGE_OWNER_SELECTOR: felt252 =  658036363289841962501247229249022783727527757834043681434485756469236076608; // starknet_keccak('change_owner')  contracts/account/src/tests/test_argent_account.cairo:L222  fn test_selectors() {  Recommendation  Add the test for the change_owner selector.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/argent-account-multisig-for-starknet/"}, {"title": "5.4 Suggestions for  Code Quality Improvements   ", "body": "  Descriptions and Recommendations  A. There are two opportunities for minor code quality improvements in the execute_multicall function:  contracts/lib/src/calls.cairo:L12-L39  fn execute_multicall(calls: Span<Call>) -> Array<Span<felt252>> {  let mut result: Array<Span<felt252>> = ArrayTrait::new();  let mut calls = calls;  let mut idx = 0;  loop {  match calls.pop_front() {  Option::Some(call) => {  match call_contract_syscall(*call.to, *call.selector, call.calldata.span()) {  Result::Ok(mut retdata) => {  result.append(retdata);  idx = idx + 1;  },  Result::Err(revert_reason) => {  let mut data = ArrayTrait::new();  data.append('argent/multicall-failed');  data.append(idx);  data.append_all(revert_reason);  panic(data);  },  },  Option::None(_) => {  break ();  },  };  };  result  retdata is never changed and doesn t have be mutable.  Instead of using an immutable calls parameter for the function that is later shadowed by a mutable variable of the same name, it would make more sense to make the function parameter mutable right away.  B.  The append_all function appends the elements of one array to another:  contracts/lib/src/array_ext.cairo:L8-L19  fn append_all(ref self: Array<T>, mut value: Array<T>) {  loop {  match value.pop_front() {  Option::Some(reason) => {  self.append(reason);  },  Option::None(()) => {  break ();  },  };  };  6 Recommendations  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/argent-account-multisig-for-starknet/"}, {"title": "6.1 Hard-Coded Grace Period Lengths for Escapes Might Not Suit Every User", "body": "  Description  Owner and Guardian can each trigger an escape of the other party. Before the escape can be activated, there is a grace period during which it can be canceled (if both parties agree) or during which the owner can unilaterally override an owner escape (triggered by the Guardian) with a Guardian escape. The length of this grace period is hard-coded to 7 days for both types of escape.  Obviously, there is a trade-off for the length of the grace periods:  Owner escape: If the owner has lost their private key and asks the Guardian to move the Account to a new owner, they d want that to happen as quickly as possible to regain control of the account. On the other hand, if the Guardian is malicious and wants to replace the owner without the latter s consent, the owner would like to have as much time as possible to react and override this takeover attempt.  Guardian escape: If the Guardian becomes malicious or inactive, the owner would like to replace or remove this Guardian as fast as possible. On the other hand, if the owner s private key gets compromised, the Guardian might prevent the worst (e.g., via transfer limits) even before the owner has knowledge of the compromise and can work with the Guardian to move the Account to a new owner. Therefore, in this scenario, actually replacing or removing the Guardian should be delayed as long as possible.  Depending on factors such as trust in the Guardian and own availability/attention, different users of the Argent Account might have different preferences with regard to these trade-offs.  Recommendation  Even taking into account that an explicit design goal is simplicity, configurable grace period lengths would make Argent Account more adaptable to different types of users and different usage scenarios. In any case, the default setting and its implications should be clearly communicated to users.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/argent-account-multisig-for-starknet/"}, {"title": "6.2 If the Guardian and Backup Guardian Are Both Compromised, the Escape Gas Attack Spam Can Be Repeated", "body": "  Description  The Argent Account has optional entities called Guardian and Backup Guardian that act as a fail-safe layer in monitoring and signing of transactions and in recovery of access to the account in the event of loss of keys to the owner.  To achieve the latter use case, the Guardians can execute an escape_owner process which initiates a security period (currently 7 days) after which the ownership of the account will go to a new owner.  contracts/account/src/argent_account.cairo:L294  fn trigger_escape_owner(new_owner: felt252) {  However, this method simply starts the process and can be restarted many times (even if the original process doesn t finish) as long as it isn t overridden by the owner with an escape_guardian process, which achieves the same outcome except the change will affect the Guardian instead of the owner. And since in StarkNet the gas costs for processing the transaction are paid for by the account, in this case our Argent Account, the escape_owner function can be spammed to waste coins from the account.  In particular, if the Guardian and/or the Backup Guardian are compromised, they could do this for malicious reasons. Thankfully, this is limited by counting the amount of attempts to start the escape process. In particular, this is currently limited to just 5 attempts. Similarly, there is a limit on the maximum fee as well:  contracts/account/src/argent_account.cairo:L49-L52  /// Limit escape attempts by only one party  const MAX_ESCAPE_ATTEMPTS: u32 = 5;  /// Limits fee in escapes  const MAX_ESCAPE_MAX_FEE: u128 = 50000000000000000; // 0.05 ETH  As a result, the damage is limited. Once the owner notices this, they can simply replace the Guardian through an escape with either a good actor or disable it altogether.  However, when the Guardian is replaced, the number of escape attempts is reset to 0. So if there is also a malicious Backup Guardian, they could start the gas attack again. The owner would then have to race the Backup Guardian to change/remove them as well before they start the gas attack to avoid any additional damage.  It is important to note that this would still be limited to just another MAX_ESCAPE_ATTEMPTS (currently 5) worth of escape transactions with MAX_ESCAPE_MAX_FEE (currently 5e16 of the smallest decimal of the gas coin, for example 0.05 ETH for ether) , and the Backup Guardian won t be able to repeat this process, so the damage is again minimized.  Recommendation  In a conversation with the Argent team, this has been acknowledged as a design choice and, due to its minimized impact, will remain. It might be beneficial to describe this scenario in the docs when describing the possible gas attack by the malicious Guardian so the users are aware, especially for those with smaller wallets.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/argent-account-multisig-for-starknet/"}, {"title": "5.1 Code readability - Rename priceDeviation to maxPriceDeviation   ", "body": "  Resolution  The variable was renamed.  Description  Improve code readability by renaming the state variable priceDeviation to maxPriceDeviation, distinguishing it from the local variable price_deviation and indicating that the variable is a limit as outlined in the specification (MAX_DEVIATION).  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L124-L129  if (  price_deviation > (BONE + priceDeviation) ||  price_deviation < (BONE - priceDeviation)  ) {  return true;  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L83-L95  if (  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  ) {  return true;  price_deviation = Math.bdiv(ethTotal_1, ethTotal_0);  if (  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  ) {  return true;  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "5.2 Improve Input Validation   ", "body": "  Resolution  the recommended checks have been added to the constructor.  Description  The constructor does not validate whether the provided price provider arguments actually make sense. In the worst-case someone might be able to deploy the contract that cannot be used. It is recommended to fail the contract creation early if invalid arguments are detected.  Consider implementing the following checks to detect whether a non-viable price provider is being deployed:  tokens.length > 1 and less than the maximum supported tokens (note that hasDeviation requires token.length**2 iterations if no deviation is detected)  _isPeggedToEth.length == tokens.length  _decimals.length == tokens.length  approximationMatrix.length && approximationMatrix[0][0].length == tokens.length +1  _priceDeviation is within bounds (less than 100%, i.e. less than 1 * BONE) otherwise the calculation might underflow.  _powerPrecision is within bounds  address(_priceOracle) != address(0)  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L38-L63  constructor(  BPool _pool,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation,  uint256 _K,  uint256 _powerPrecision,  uint256[][] memory _approximationMatrix  ) public {  pool = _pool;  //Get token list  tokens = pool.getFinalTokens(); //This already checks for pool finalized  //Get token normalized weights  uint256 length = tokens.length;  for (uint8 i = 0; i < length; i++) {  weights.push(pool.getNormalizedWeight(tokens[i]));  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  K = _K;  powerPrecision = _powerPrecision;  approximationMatrix = _approximationMatrix;  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L35-L50  constructor(  IUniswapV2Pair _pair,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation  ) public {  pair = _pair;  //Get tokens  tokens.push(pair.token0());  tokens.push(pair.token1());  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "5.3 Use SafeMath consistently   ", "body": "  Resolution  All arithmetic operations now use SafeMath.  Description  Even though the Uniswap price provider imports the SafeMath library, the SafeMath library functions aren t always used for integer arithmetic operations. Note that plain Solidity arithmetic operators do not check for integer underflows and overflows.  Examples  Example 1:  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L66  uint256 missingDecimals = 18 - decimals[index];  Example 2 (same in line 91-92):  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L84-L85  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  Example 3:  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L164-L165  uint256 liquidity = numerator / denominator;  totalSupply += liquidity;  Recommendation  In some cases, this issue is cosmetic because the values are assumed to be within certain ranges. Nevertheless, we recommend accepting the slightly higher gas cost for SafeMath functions for consistency and to prevent potential issues.  6 Issues  The issues are presented in approximate order of priority from highest to lowest.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "6.1 Unchecked Specification requirement - token limit Closed", "body": "  Description  According to the Balancer Shared Pool Price Provider that was provided with the audit code-base the price provide must fulfill the following requirements:  Pool token price cannot be manipulated  Chainlink will be used as the main oracle  It should use as less gas as possible  Limited to Balancer s shared pools where the weights cannot be changed  Limited to a pool containing 2 to 3 tokens  However, the constructor of the price provider does not enforce the limit of 2 to 3 tokens.  Examples  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L38-L63  constructor(  BPool _pool,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation,  uint256 _K,  uint256 _powerPrecision,  uint256[][] memory _approximationMatrix  ) public {  pool = _pool;  //Get token list  tokens = pool.getFinalTokens(); //This already checks for pool finalized  //Get token normalized weights  uint256 length = tokens.length;  for (uint8 i = 0; i < length; i++) {  weights.push(pool.getNormalizedWeight(tokens[i]));  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  K = _K;  powerPrecision = _powerPrecision;  approximationMatrix = _approximationMatrix;  Recommendation  Require that the number of tokens returned by pool.getFinalTokens() is 2<= len <=3.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "6.2 Integer underflow if a token specifies more than 18 decimals Closed", "body": "  Description  Decimals are provided by the account deploying the price provider contract. In getEthBalanceByToken the assumption is made that decimals[index] is less or equal to 18 decimals, however, the deployer may provide decimals that are not within normal operating bounds. Contract creation succeeds, while the contract is not viable.  Examples  The value underflows if the contract is used with a token decimals > 18.  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L69-L78  function getEthBalanceByToken(uint256 index)  internal  view  returns (uint256)  uint256 pi = isPeggedToEth[index]  ? BONE  : uint256(priceOracle.getAssetPrice(tokens[index]));  require(pi > 0, \"ERR_NO_ORACLE_PRICE\");  uint256 missingDecimals = 18 - decimals[index];  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L57-L66  function getEthBalanceByToken(uint256 index, uint112 reserve)  internal  view  returns (uint256)  uint256 pi = isPeggedToEth[index]  ? Math.BONE  : uint256(priceOracle.getAssetPrice(tokens[index]));  require(pi > 0, \"ERR_NO_ORACLE_PRICE\");  uint256 missingDecimals = 18 - decimals[index];  Recommendation  Add a check to the constructor to ensure that none of the provided decimals is greater than 18.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "4.1 _WETH private constant address in UnoswapRouter makes for tricky deployments to other chains ", "body": "  Resolution  The 1inch team acknowledged but is unable to implement changes due to stack-too-deep errors that would require a large refactor of this already well-used and tested library.  To interact with WETH, the UnoswapRouter uses a hardcoded _WETH private constant in the contract. Therefore, this currently needs changing every time the contract is deployed to a different chain, as noted by the comment within the contract:  1inch-contract/contracts/routers/UnoswapRouter.sol:L24-L26  /// @dev WETH address is network-specific and needs to be changed before deployment.  /// It can not be moved to immutable as immutables are not supported in assembly  address private constant _WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  As the comment also points out, the choice to not make it an immutable variable is not possible since they are not supported in assembly, and the UnoswapRouter contract is highly efficient and almost entirely written in assembly. However, the other contracts within the scope of this audit, do utilize setting a private immutable variable for WETH in the constructor, and some of them then initialize a new address variable derived from this private immutable variable, thereby allowing the address variable to be used in the assembly blocks instead:  UnoswapV3Router  1inch-contract/contracts/routers/UnoswapV3Router.sol:L33-L37  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  constructor(IWETH weth) {  _WETH = weth;  ClipperRouter  1inch-contract/contracts/routers/ClipperRouter.sol:L18-L24  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  IClipperExchangeInterface private immutable _clipperExchange;  constructor(IWETH weth, IClipperExchangeInterface clipperExchange) {  _clipperExchange = clipperExchange;  _WETH = weth;  1inch-contract/contracts/routers/ClipperRouter.sol:L101  address weth = address(_WETH);  1inch-contract/contracts/routers/ClipperRouter.sol:L112  if iszero(call(gas(), weth, 0, ptr, 0x64, 0, 0)) {  OrderMixin  limit-order-protocol/contracts/OrderMixin.sol:L63-L70  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  /// @notice Stores unfilled amounts for each order plus one.  /// Therefore 0 means order doesn't exist and 1 means order was filled  mapping(bytes32 => uint256) private _remaining;  constructor(IWETH weth) {  _WETH = weth;  Normalizing this process across all smart contracts in the 1inch system could help avoid accidental mistakes when the deployer could forget to first edit the unoswap contract to have the correct address.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "4.2 Selfdestruct may be removed as an opcode in future ", "body": "  Resolution  The 1inch team acknowledged and noted.  The AggregationRouterV5 contract implements a function called destroy that calls a selfdestuct on the contract with the msg.sender as the argument, that is checked by the onlyOwner modifier on the function.  1inch-contract/contracts/AggregationRouterV5.sol:L35-L37  function destroy() external onlyOwner {  selfdestruct(payable(msg.sender));  However, there are discussions currently around removing the selfdestruct functionality from the EVM altogether with various motivations and rationale provided, such as this being not possible with Verkle trees and it being a requirement for stateleness. Link to the EIP is below: https://eips.ethereum.org/EIPS/eip-4758  It appears that the suggested remediation of this functionality per the EIP-4758 will not significantly change the results, for example all of the funds will still be sent to the specified address, but the destruction of the actual contract will not occur. So this is just an advisory note for the 1inch team to notify of this potential change in the future.  5 Limit Order Protocol  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.1 Malicious maker can take more takers funds than taker expected ", "body": "  Resolution                           Remediated as per the 1inch team in   1inch/limit-order-protocol@9ddc086 by adding a check that reverts when  OrderMixin contract allows users to match makers(sellers) and takers(buyers) in an orderbook-like manner. One additional feature this contract has is that both makers and takers are allowed to integrate hooks into their orders to better react to market conditions and manage funds on the fly. Two out of many of these hooks are called: _getMakingAmount and _getTakingAmount. Those particular hooks allow the maker to dynamically respond to the making or taking amounts supplied by the taker. Essentially they allow overriding the rate that was initially set by the maker when creating an order up to a certain extent. To make sure that the newly suggested maker rate is reasonable taker also provides a threshold value or in other words the minimum amount of assets the taker is going to be fine receiving.  Generally speaking, the maker can override the taking amount offered to the taker if the buyer passed a specific making amount in the fill transaction and vice versa. But there is one special case where the maker will be able to override both, which when done right will force the taker to spend an amount larger than the one intended. Specifically, this happens when the taker passed the desired taking amount and the maker returns a suggested making amount that is larger than the remaining order size. In this case, the making amount is being set to the remaining order amount and the taking is being recomputed.  limit-order-protocol/contracts/OrderMixin.sol:L214-L217  actualMakingAmount = _getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);  if (actualMakingAmount > remainingMakingAmount) {  actualMakingAmount = remainingMakingAmount;  actualTakingAmount = _getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);  Essentially this allows the maker to override the taker amount and as long as the maker keeps the price intact or changed within a certain threshold like described in this issue, they can take all taking tokens of the buyer up to an amount of the token balance or approval limit whatever comes first.  Consider the following example scenario:  The maker has a large order to sell 100 ETH on the order book for 100 DAI each.  The taker then wants to partially fill the order and buy as much ETH as 100 DAI will buy. At the same time taker has 100,000 DAI in the wallet.  When taker tries to fill this order taker passes the takingAmount to be 100. Since OrderMixin received the taking amount we go this route:  limit-order-protocol/contracts/OrderMixin.sol:L214  actualMakingAmount = _getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);  However, note that when executing the _getMakingAmount() function, it first evaluates the order.getMakingAmount() argument which is evaluated as bytes calldata _getter within the function.  limit-order-protocol/contracts/OrderMixin.sol:L324-L337  function _getMakingAmount(  bytes calldata getter,  uint256 orderTakingAmount,  uint256 requestedTakingAmount,  uint256 orderMakingAmount,  uint256 remainingMakingAmount,  bytes32 orderHash  ) private view returns(uint256) {  if (getter.length == 0) {  // Linear proportion  return getMakingAmount(orderMakingAmount, orderTakingAmount, requestedTakingAmount);  return _callGetter(getter, orderTakingAmount, requestedTakingAmount, orderMakingAmount, remainingMakingAmount, orderHash);  That is because the Order struct that is made and signed by the maker actually contains the necessary bytes within it that can be decoded to construct a target and calldata for static calls, which in this case are supposed to be used to return the making asset amounts that the maker determines to be appropriate, as seen in the comments under the uint256 offsets part of the struct.  limit-order-protocol/contracts/OrderLib.sol:L7-L27  library OrderLib {  struct Order {  uint256 salt;  address makerAsset;  address takerAsset;  address maker;  address receiver;  address allowedSender;  // equals to Zero address on public orders  uint256 makingAmount;  uint256 takingAmount;  uint256 offsets;  // bytes makerAssetData;  // bytes takerAssetData;  // bytes getMakingAmount; // this.staticcall(abi.encodePacked(bytes, swapTakerAmount)) => (swapMakerAmount)  // bytes getTakingAmount; // this.staticcall(abi.encodePacked(bytes, swapMakerAmount)) => (swapTakerAmount)  // bytes predicate;       // this.staticcall(bytes) => (bool)  // bytes permit;          // On first fill: permit.1.call(abi.encodePacked(permit.selector, permit.2))  // bytes preInteraction;  // bytes postInteraction;  bytes interactions; // concat(makerAssetData, takerAssetData, getMakingAmount, getTakingAmount, predicate, permit, preIntercation, postInteraction)  Finally, if these bytes indeed contain data (i.e. length>0), they are passed to the _callGetter() function that asks the previously mentioned target for the data.  limit-order-protocol/contracts/OrderMixin.sol:L354-L377  function _callGetter(  bytes calldata getter,  uint256 orderExpectedAmount,  uint256 requestedAmount,  uint256 orderResultAmount,  uint256 remainingMakingAmount,  bytes32 orderHash  ) private view returns(uint256) {  if (getter.length == 1) {  if (OrderLib.getterIsFrozen(getter)) {  // On \"x\" getter calldata only exact amount is allowed  if (requestedAmount != orderExpectedAmount) revert WrongAmount();  return orderResultAmount;  } else {  revert WrongGetter();  } else {  (address target, bytes calldata data) = getter.decodeTargetAndCalldata();  (bool success, bytes memory result) = target.staticcall(abi.encodePacked(data, requestedAmount, remainingMakingAmount, orderHash));  if (!success || result.length != 32) revert GetAmountCallFailed();  return abi.decode(result, (uint256));  However, since the getter is set in the Order struct, and the Order is set by the maker, the getter itself is entirely under the maker s control and can return whatever the maker wants, with no regard for the taker s passed actualTakingAmount or any arguments at all for that matter. So, in our example, the return value could be 100.1 ETH, i.e. just above the total order size. That will get us on the route of recomputing the taking amount since 100.1 is over the 100ETH remaining in the order.  limit-order-protocol/contracts/OrderMixin.sol:L217  actualTakingAmount = _getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);  This branch will set the actualMakingAmount  to 100ETH and then the malicious maker will say the actualTakingAmount is 10000 DAI, this can be done via the _getTakingAmount static call in the same exact way as the making amount was manipulated.  Then the threshold check would look like this as defined by its formula:  limit-order-protocol/contracts/OrderMixin.sol:L222  if (actualMakingAmount * takingAmount < thresholdAmount * actualTakingAmount) revert MakingAmountTooLow();  ActualMakingAmount - 100ETH  ActualTakingAmount - 10000DAI  threshold - 1 ETH  takingAmount - 100 DAI  then: 100ETH * 100DAI < 1ETH*10000DAI This condition will be false so we will pass this check.  Then we proceed to taker interaction. Assuming the taker did not pass any interaction, the actualTakingAmount will not change.  Then we proceed to exchange tokens between maker and taker in the amount of actualTakingAmount and actualMakingAmount.  The scenario allows the maker to take the taker s funds up to an amount of taker s approval or balance. Essentially while taker wanted to only spend 100 DAI, potentially they ended up spending much more. This paired with infinite approvals that are currently enabled on the 1inch UI could lead to funds being lost.  While this does not introduce a price discrepancy this attack can be profitable to the malicious actor. The attacker could put an order to sell a large amount of new not trustworthy tokens for sale who s supply the attacker controls. Then after a short marketing campaign when people will cautiously try to buy a small amount of those tokens for let s say a small amount of USDC due to this bug attacker could drain all of their USDC.  We advise that 1inch team treats this issue with extra care since a similar issue is present in a currently deployed production version of 1inch OrderMixin. One potential solution to this bug is introducing a global threshold that would represent by how much the actual taking amount can differ from the taker provided taking amount.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.2 Invalidating users orders ", "body": "  1inch team has implemented a more streamlined version of the order book that is called OrderRFQMixin. This version has no hooks and is meant to be more straightforward than the main order book contract.  One significant difference between those contracts is that the RFQ version invalidates the orders even after they have been only partially filled.  limit-order-protocol/contracts/OrderRFQMixin.sol:L197-L203  {  // Stack too deep  uint256 info = order.info;  // Check time expiration  uint256 expiration = uint128(info) >> 64;  if (expiration != 0 && block.timestamp > expiration) revert OrderExpired(); // solhint-disable-line not-rely-on-time  _invalidateOrder(maker, info, 0);  Since makers have to sign the orders, only makers can place the remainder of the original order as a new one. Given that information, an attacker could take all the orders and fill them with 1 wei of taking assets. While this will cost an attacker gas, on some chains it would be possible to make the operations of the protocol unreliable and impractical for makers.  One way to fix that without making significant changes to the logic is to introduce a threshold that will determine the smallest taking amount for each order. That could be a percent of the taking amount specified in the order. This change will make the attack more expensive and less likely to happen.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.3 Reentrancy potential issue for contracts building on top RFQOrderMixin ", "body": "  Resolution                           Remediated as per the 1inch team in   1inch/limit-order-protocol@d3957fe by forwarding a limited amount of gas to guard against complex execution at the target but still allow for smart contract receipt that may require a bit more gas than usual EOA receipts.  The RFQOrderMixin contract is used to facilitate transfer of assets in RFQ orders between makers and takers. Naturally, one of such possible assets could be the native coin of the chain, such as ETH. In order to perform these transfers, the contract currently utilizes the target.call(){value:X} method to transfer X ETH to the target address. However, this also calls into the target address and opens up arbitrary code execution that could lead to significant problems, that often times result in a reentrancy attack.  limit-order-protocol/contracts/OrderRFQMixin.sol:L233  (bool success, ) = target.call{value: makingAmount}(\"\");  // solhint-disable-line avoid-low-level-calls  While 1inch s RFQOrderMixin contract doesn t have a clear reentrancy attack vector, other smart contract systems that might utilize 1inch RFQ orders will have to handle a potential reentrancy due to this problem. The impact for downstream systems could be critical.  This could be changed to .transfer() or .send() methods of transferring ETH, or at least heavily noted in documentation for any and all developers who may fork/utilize this code so reentrancy risks are made aware of.  This does not seem to be a general-purpose use library for other systems, so likelihood of this issue happening isn t as high as in issue 6.4, so the severity is lower.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.4 Order cancellation event spam in orderMixin ", "body": "  The 1inch Limit Order protocol s contracts utilize mechanisms to allow creation of orders without posting the orders on chain. Indeed, the orders are created by signing Order struct hashes off-chain by the maker, and then having takers pass the signatures associated with those hashes to fill in those orders. However, a maker needs to be able to cancel their order if they change their mind, which would require them to execute an on-chain transaction marking that order hash as invalid:  limit-order-protocol/contracts/OrderMixin.sol:L113-L121  function cancelOrder(OrderLib.Order calldata order) external returns(uint256 orderRemaining, bytes32 orderHash) {  if (order.maker != msg.sender) revert AccessDenied();  orderHash = hashOrder(order);  orderRemaining = _remaining[orderHash];  if (orderRemaining == _ORDER_FILLED) revert AlreadyFilled();  emit OrderCanceled(msg.sender, orderHash, orderRemaining);  _remaining[orderHash] = _ORDER_FILLED;  Unfortunately, since the OrderMixin contract is not aware of order hashes before interacting with them for the first time, it can not verify that the order was actually ever seriously present or intended to be executed. As a result, this would allow users to cancel non-existent orders and create event spam. While this would be costly to the spammer, it would nonetheless be possible.  The impact of this would need systems that rely on the OrderCanceled event log to be aware of potential spam attacks with fake order cancellation and not use them, for example, for analytics, potential volume forecasting, tracking order created -> order cancelled metrics and so on.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.5 Order book slippage ", "body": "  OrderMixin contract allows users to match makers and takers in an orderbook-like manner. One additional feature this contract has is that both makers and takers are allowed to integrate hooks into their orders to better react to market conditions and manage funds on the fly. Two of these hooks are called: _getMakingAmount and _getTakingAmount.  When trying to fill an order taker is required to provide either the making amount or taking amount as well as the threshold or in other words the minimum amount of assets the taker is going to be fine receiving. During the fill transaction, an order maker is given the opportunity to update the offer by the means of the _getMakingAmount and _getTakingAmount. A threshold checks are then used in order to make sure that the updated values are within taker s acceptable bounds:  limit-order-protocol/contracts/OrderMixin.sol:L222  if (actualMakingAmount * takingAmount < thresholdAmount * actualTakingAmount) revert MakingAmountTooLow();  limit-order-protocol/contracts/OrderMixin.sol:L212  if (actualTakingAmount * makingAmount > thresholdAmount * actualMakingAmount) revert TakingAmountTooHigh();  It is reasonable to assume that if the maker knows the threshold the taker selected, the maker will attempt to update the making or taking amount to maximize profits. While _getMakingAmount and _getTakingAmount do not pass the threshold selected by the taker directly, it is still possible for the maker to obtain this information and act accordingly.  A malicious maker could listen to the mempool and wait for a transaction that is meant to fill his order obtaining the threshold value.  Maker would then update the state of the contract that responds to the static call of the _getMakingAmount and _getTakingAmount hooks.  If the maker is using FlashBots or a similar service, the maker can ensure that the above actions are performed before the transaction that would fill the order.  While there is no good way to alleviate this issue given the current design we believe it is important to be aware of this issue and allow the 1inch users to know that some analogy of slippage is still possible even on the orderbook-like system. This will allow them to choose tighter and more secure threshold values.  6 Solidity Utils  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.1 ECDSA library has a vulnerability for signature malleability of EIP-2098 compact signatures ", "body": "  Resolution                           Remediated as per the 1inch team in   1inch/solidity-utils@166353b by adding a warning note in the comments of the library code.  The 1inch ECDSA library supports several types of signatures and forms in which they could be provided. However, for compact signatures there is a recently found malleability attack vector. Specifically, the issue arises when contracts use transaction replay protection through signature uniqueness (i.e. by marking it as used). While this may not be the case in the scope of other contracts of this audit, this ECDSA library is meant to be a general use library so it should be fixed so as to not mislead others who might use this.  For more details and context, find below the advisory notice and fix in the OpenZeppelin s ECDSA library: https://github.com/OpenZeppelin/openzeppelin-contracts/security/advisories/GHSA-4h98-2769-gh6h OpenZeppelin/openzeppelin-contracts@d693d89  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.2 Ethereum reimbursements sent to an incorrect address ", "body": "  Resolution                           Remediated as per the 1inch team as of   1inch/solidity-utils@6b1a3df by adding the correct recipient of the refund.  1inch team has written a library called UniERC20 that extends the traditional ERC20 standard to also support eth transfers seamlessly. In the case of the uniTransferFrom function call, the library checks that the msg.value of the transaction is bigger or equal to the amount passed in the function argument. If the msg.value is larger than the amount required, the difference, or extra funds, should be sent to the sender. In the actual implementation Instead of returning the funds to the sender, extra funds are actually sent to the destination.  solidity-utils/contracts/libraries/UniERC20.sol:L59-L65  if (msg.value > amount) {  // Return remainder if exist  unchecked {  (bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls  if (!success) revert ETHSendFailed();  Given that this code is packed as a library and allows for easy reusability by the 1inch team and outside developers it is crucial that this logic is written well and well tested.  We recommend reconsidering reimbursing the sender when an incorrect amount is being sent because it introduces an easy-to-oversee reentrancy backdoor with call() that is mentioned in issue 6.4. Reverting was a default behavior in similar cases across the rest of the 1inch contracts.  If this functionality is required, a fix we could recommend is replacing the to with from. We can also suggest running a fuzzing campaign against this library.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.3 ECDSA incorrect size provided for calldata in the static call ", "body": "  Resolution                           Remediated as per the 1inch team in   1inch/solidity-utils@cfdc889 by passing the correct data size.  The ECDSA library implements support for IERC1271 interfaces that verify provided signature for the data through the different isValidSignature functions that depend on the type of signature used.  However, the library passes an incorrect size for the calldata in the static call for signatures that are of the form (bytes32 r, bytes32 vs). It should be 0xa4 (164 bytes) instead of 0xa5 (165 bytes).  solidity-utils/contracts/libraries/ECDSA.sol:L178  if staticcall(gas(), signer, ptr, 0xa5, 0, 0x20) {  The impact could vary and depends on the signature verifier. For example, it could be significant if the signature verifier performs a check on the calldatasize for this specific type of signature and reverts on incorrect sizes, thereby having valid signatures return false when passed to isValidSignature.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.4 Re-entrancy risk in UniERC20 ", "body": "  Resolution                           Remediated as per the 1inch team in   1inch/solidity-utils@6b1a3df by forwarding a limited amount of gas to guard against complex execution at the target but still allow for smart contract receipt that may require a bit more gas than usual EOA receipts.  UniERC20 is a general library for facilitating transfers of any ERC20 or native coin assets. It features gas-efficient code and could be easily integrated into large systems of contract, such as those that are used in this audit   1inch routers and limit order protocol.  However, it also utilizes .call(){value:X} method of transferring chain native assets, such as ETH. This introduces a large risk in the form of re-entrancy attacks, so any system implementing this library would have to handle them. While 1inch s projects in the scope of this audit do not seem to have re-entrancy attack vectors, other projects that could be utilizing this library might. Since this is an especially efficient and convenient library, the likelihood that some other project using this suffers and then sufferring a re-entrancy attack is significant.  solidity-utils/contracts/libraries/UniERC20.sol:L45  (bool success, ) = to.call{value: amount}(\"\");  // solhint-disable-line avoid-low-level-calls  solidity-utils/contracts/libraries/UniERC20.sol:L62  (bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls  Consider instead implementing transfer() or send() methods for transferring chain native assets, such as ETH, instead of performing a .call()  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.1 zBanc - DynamicLiquidTokenConverter ineffective reentrancy protection    ", "body": "  Resolution                           Fixed with   zer0-os/zBanc@ff3d913 by following the recommendation.  Description  reduceWeight calls _protected() in an attempt to protect from reentrant calls but this check is insufficient as it will only check for the locked statevar but never set it. A potential for direct reentrancy might be present when an erc-777 token is used as reserve.  It is assumed that the developer actually wanted to use the protected modifier that sets the lock before continuing with the method.  Examples  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L123-L128  function reduceWeight(IERC20Token _reserveToken)  public  validReserve(_reserveToken)  ownerOnly  _protected();  contract ReentrancyGuard {  // true while protected code is being executed, false otherwise  bool private locked = false;  /**  @dev ensures instantiation only by sub-contracts  /  constructor() internal {}  // protects a function against reentrancy attacks  modifier protected() {  _protected();  locked = true;  _;  locked = false;  // error message binary size optimization  function _protected() internal view {  require(!locked, \"ERR_REENTRANCY\");  Recommendation  To mitigate potential attack vectors from reentrant calls remove the call to _protected() and decorate the function with protected instead. This will properly set the lock before executing the function body rejecting reentrant calls.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.2 zBanc - DynamicLiquidTokenConverter input validation    ", "body": "  Resolution                           fixed with   zer0-os/zBanc@ff3d913 by checking that the provided values are at least 0% < p <= 100%.  Description  Check that the value in PPM is within expected bounds before updating system settings that may lead to functionality not working correctly. For example, setting out-of-bounds values for stepWeight or setMinimumWeight may make calls to reduceWeight fail. These values are usually set in the beginning of the lifecycle of the contract and misconfiguration may stay unnoticed until trying to reduce the weights. The settings can be fixed, however, by setting the contract inactive and updating it with valid settings. Setting the contract to inactive may temporarily interrupt the normal operation of the contract which may be unfavorable.  Examples  Both functions allow the full uint32 range to be used, which, interpreted as PPM would range from 0% to  4.294,967295%  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L75-L84  function setMinimumWeight(uint32 _minimumWeight)  public  ownerOnly  inactive  //require(_minimumWeight > 0, \"Min weight 0\");  //_validReserveWeight(_minimumWeight);  minimumWeight = _minimumWeight;  emit MinimumWeightUpdated(_minimumWeight);  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L92-L101  function setStepWeight(uint32 _stepWeight)  public  ownerOnly  inactive  //require(_stepWeight > 0, \"Step weight 0\");  //_validReserveWeight(_stepWeight);  stepWeight = _stepWeight;  emit StepWeightUpdated(_stepWeight);  Recommendation  Reintroduce the checks for _validReserveWeight to check that a percent value denoted in PPM is within valid bounds _weight > 0 && _weight <= PPM_RESOLUTION. There is no need to separately check for the value to be >0 as this is already ensured by _validReserveWeight.  Note that there is still room for misconfiguration (step size too high, min-step too high), however, this would at least allow to catch obviously wrong and often erroneously passed parameters early.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.3 zBanc - DynamicLiquidTokenConverter introduces breaking changes to the underlying bancorprotocol base    ", "body": "  Resolution  Addressed with zer0-os/zBanc@ff3d913 by removing the modifications in favor of surgical and more simple changes, keeping the factory and upgrade components as close as possible to the forked bancor contracts.  Additionally, the client provided the following statement:  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.14 Removed excess functionality from factory and restored the bancor factory pattern.", "body": "  Description  Introducing major changes to the complex underlying smart contract system that zBanc was forked from(bancorprotocol) may result in unnecessary complexity to be added. Complexity usually increases the attack surface and potentially introduces software misbehavior. Therefore, it is recommended to focus on reducing the changes to the base system as much as possible and comply with the interfaces and processes of the system instead of introducing diverging behavior.  For example, DynamicLiquidTokenConverterFactory does not implement the ITypedConverterFactory while other converters do. Furthermore, this interface and the behavior may be expected to only perform certain tasks e.g. when called during an upgrade process. Not adhering to the base systems expectations may result in parts of the system failing to function for the new convertertype. Changes introduced to accommodate the custom behavior/interfaces may result in parts of the system failing to operate with existing converters. This risk is best to be avoided.  In the case of DynamicLiquidTokenConverterFactory the interface is imported but not implemented at all (unused import). The reason for this is likely because the function createConverter in DynamicLiquidTokenConverterFactory does not adhere to the bancor-provided interface anymore as it is doing way more than  just  creating and returning a new converter. This can create problems when trying to upgrade the converter as the upgraded expected the shared interface to be exposed unless the update mechanisms are modified as well.  In general, the factories createConverter method appears to perform more tasks than comparable type factories. It is questionable if this is needed but may be required by the design of the system. We would, however, highly recommend to not diverge from how other converters are instantiated unless it is required to provide additional security guarantees (i.e. the token was instantiated by the factory and is therefore trusted).  The ConverterUpgrader changed in a way that it now can only work with the DynamicLiquidTokenconverter instead of the more generalized IConverter interface. This probably breaks the update for all other converter types in the system.  The severity is estimated to be medium based on the fact that the development team seems to be aware of the breaking changes but the direction of the design of the system was not yet decided.  Examples  unused import  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L6-L6  import \"../../interfaces/ITypedConverterFactory.sol\";  converterType should be external as it is not called from within the same or inherited contracts  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L144-L146  function converterType() public pure returns (uint16) {  return 3;  createToken can be external and is actually creating a token and converter that is using that token (the converter is not returned)(consider renaming to createTokenAndConverter)  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L54-L74  DSToken token = new DSToken(_name, _symbol, _decimals);  token.issue(msg.sender, _initialSupply);  emit NewToken(token);  createConverter(  token,  _reserveToken,  _reserveWeight,  _reserveBalance,  _registry,  _maxConversionFee,  _minimumWeight,  _stepWeight,  _marketCapThreshold  );  return token;  the upgrade interface changed and now requires the converter to be a DynamicLiquidTokenConverter. Other converters may potentially fail to upgrade unless they implement the called interfaces.  zBanc/solidity/contracts/converter/ConverterUpgrader.sol:L96-L122  function upgradeOld(DynamicLiquidTokenConverter _converter, bytes32 _version) public {  _version;  DynamicLiquidTokenConverter converter = DynamicLiquidTokenConverter(_converter);  address prevOwner = converter.owner();  acceptConverterOwnership(converter);  DynamicLiquidTokenConverter newConverter = createConverter(converter);  copyReserves(converter, newConverter);  copyConversionFee(converter, newConverter);  transferReserveBalances(converter, newConverter);  IConverterAnchor anchor = converter.token();  // get the activation status before it's being invalidated  bool activate = isV28OrHigherConverter(converter) && converter.isActive();  if (anchor.owner() == address(converter)) {  converter.transferTokenOwnership(address(newConverter));  newConverter.acceptAnchorOwnership();  handleTypeSpecificData(converter, newConverter, activate);  converter.transferOwnership(prevOwner);  newConverter.transferOwnership(prevOwner);  emit ConverterUpgrade(address(converter), address(newConverter));  solidity/contracts/converter/ConverterUpgrader.sol:L95-L101  function upgradeOld(  IConverter _converter,  bytes32 /* _version */  ) public {  // the upgrader doesn't require the version for older converters  upgrade(_converter, 0);  Recommendation  It is a fundamental design decision to either follow the bancorsystems converter API or diverge into a more customized system with a different design, functionality, or even security assumptions. From the current documentation, it is unclear which way the development team wants to go.  However, we highly recommend re-evaluating whether the newly introduced type and components should comply with the bancor API (recommended; avoid unnecessary changes to the underlying system,) instead of changing the API for the new components. Decide if the new factory should adhere to the usually commonly shared ITypedConverterFactory (recommended) and if not, remove the import and provide a new custom shared interface. It is highly recommended to comply and use the bancor systems extensibility mechanisms as intended, keeping the previously audited bancor code in-tact and voiding unnecessary re-assessments of the security impact of changes.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.4 zBanc - DynamicLiquidTokenConverter isActive should only be returned if converter is fully configured and converter parameters should only be updateable while converter is inactive    ", "body": "  Resolution  Addressed with zer0-os/zBanc@ff3d913 by removing the custom ACL modifier falling back to checking whether the contract is configured (isActive, inactive modifiers). When a new contract is deployed it will be inactive until the main vars are set by the owner (upgrade contract). The upgrade path is now aligned with how the LiquidityPoolV2Converter performs upgrades.  Additionally, the client provided the following statement:  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.13 - upgrade path resolved - inactive modifier back on the setters, and upgrade path now mirrors lpv2 path. An important note here is that lastWeightAdjustmentMarketCap setting isn t included in the inActive() override, since it has a valid state of 0. So it must be set before the others settings, or it will revert as inactive", "body": "  Description  By default, a converter is active once the anchor ownership was transferred. This is true for converters that do not require to be properly set up with additional parameters before they can be used.  zBanc/solidity/contracts/converter/ConverterBase.sol:L272-L279  /**  @dev returns true if the converter is active, false otherwise  @return true if the converter is active, false otherwise  /  function isActive() public view virtual override returns (bool) {  return anchor.owner() == address(this);  For a simple converter, this might be sufficient. If a converter requires additional setup steps (e.g. setting certain internal variables, an oracle, limits, etc.) it should return inactive until the setup completes. This is to avoid that users are interacting with (or even pot. frontrunning) a partially configured converter as this may have unexpected outcomes.  For example, the LiquidityPoolV2Converter overrides the isActive method to require additional variables be set (oracle) to actually be in active state.  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L79-L85  Additionally, settings can only be updated while the contract is inactive which will be the case during an upgrade. This ensures that the owner cannot adjust settings at will for an active contract.  @dev returns true if the converter is active, false otherwise  @return true if the converter is active, false otherwise  /  function isActive() public view override returns (bool) {  return super.isActive() && address(priceOracle) != address(0);  Additionally, settings can only be updated while the contract is inactive which will be the case during an upgrade. This ensures that the owner cannot adjust settings at will for an active contract.  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L97-L109  function activate(  IERC20Token _primaryReserveToken,  IChainlinkPriceOracle _primaryReserveOracle,  IChainlinkPriceOracle _secondaryReserveOracle)  public  inactive  ownerOnly  validReserve(_primaryReserveToken)  notThis(address(_primaryReserveOracle))  notThis(address(_secondaryReserveOracle))  validAddress(address(_primaryReserveOracle))  validAddress(address(_secondaryReserveOracle))  The DynamicLiquidTokenConverter is following a different approach. It inherits the default isActive which sets the contract active right after anchor ownership is transferred. This kind of breaks the upgrade process for DynamicLiquidTokenConverter as settings cannot be updated while the contract is active (as anchor ownership might be transferred before updating values). To unbreak this behavior a new authentication modifier was added, that allows updates for the upgrade contradict while the contract is active. Now this is a behavior that should be avoided as settings should be predictable while a contract is active. Instead it would make more sense initially set all the custom settings of the converter to zero (uninitialized) and require them to be set and only the return the contract as active. The behavior basically mirrors the upgrade process of LiquidityPoolV2Converter.  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L44-L50  modifier ifActiveOnlyUpgrader(){  if(isActive()){  require(owner == addressOf(CONVERTER_UPGRADER), \"ERR_ACTIVE_NOTUPGRADER\");  _;  Pre initialized variables should be avoided. The marketcap threshold can only be set by the calling entity as it may be very different depending on the type of reserve (eth, token).  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L17-L20  uint32 public minimumWeight = 30000;  uint32 public stepWeight = 10000;  uint256 public marketCapThreshold = 10000 ether;  uint256 public lastWeightAdjustmentMarketCap = 0;  Here s one of the setter functions that can be called while the contract is active (only by the upgrader contract but changing the ACL commonly followed with other converters).  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L67-L74  function setMarketCapThreshold(uint256 _marketCapThreshold)  public  ownerOnly  ifActiveOnlyUpgrader  marketCapThreshold = _marketCapThreshold;  emit MarketCapThresholdUpdated(_marketCapThreshold);  Recommendation  Align the upgrade process as much as possible to how LiquidityPoolV2Converter performs it. Comply with the bancor API.  override isActive and require the contracts main variables to be set.  do not pre initialize the contracts settings to  some  values. Require them to be set by the caller (and perform input validation)  mirror the upgrade process of LiquidityPoolV2Converter and instead of activate call the setter functions that set the variables. After setting the last var and anchor ownership been transferred, the contract should return active.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.5 zBanc - DynamicLiquidTokenConverter frontrunner can grief owner when calling reduceWeight   ", "body": "  Resolution  The client acknowledged this issue by providing the following statement:  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.12 - admin by a DAO will mitigate the owner risks here", "body": "  Description  The owner of the converter is allowed to reduce the converters weights once the marketcap surpasses a configured threshhold. The thresshold is configured on first deployment. The marketcap at the beginning of the call is calculated as reserveBalance / reserve.weight and stored as lastWeightAdjustmentMarketCap after reducing the weight.  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L130-L138  function reduceWeight(IERC20Token _reserveToken)  public  validReserve(_reserveToken)  ownerOnly  _protected();  uint256 currentMarketCap = getMarketCap(_reserveToken);  require(currentMarketCap > (lastWeightAdjustmentMarketCap.add(marketCapThreshold)), \"ERR_MARKET_CAP_BELOW_THRESHOLD\");  The reserveBalance can be manipulated by buying (adding reserve token) or selling liquidity tokens (removing reserve token). The success of a call to reduceWeight is highly dependant on the marketcap. A malicious actor may, therefore, attempt to grief calls made by the owner by sandwiching them with buy and sell calls in an attempt to (a) raise the barrier for the next valid payout marketcap or (b) temporarily lower the marketcap if they are a major token holder in an attempt to fail the reduceWeights call.  In both cases the griefer may incur some losses due to conversion errors, bancor fees if they are set, and gas spent. It is, therefore, unlikely that a third party may spend funds on these kinds of activities. However, the owner as a potential major liquid token holder may use this to their own benefit by artificially lowering the marketcap to the absolute minimum (old+threshold) by selling liquidity and buying it back right after reducing weights.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.6 zBanc - outdated fork   ", "body": "  Description  According to the client the system was forked off bancor v0.6.18 (Oct 2020). The current version 0.6.x is v0.6.36 (Apr 2021).  Recommendation  It is recommended to check if relevant security fixes were released after v0.6.18 and it should be considered to rebase with the current stable release.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.7 zBanc - inconsistent DynamicContractRegistry, admin risks    ", "body": "  Resolution  The client acknowledged the admin risk and addressed the itemCount concerns by exposing another method that only returns the overridden entries. The following statement was provided:  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.10 - keeping this pattern which matches the bancor pattern, and noting the DCR should be owned by a DAO, which is our plan. solved itemCount issue - Added dcrItemCount and made itemCount call the bancor registry s itemCount, so unpredictable behavior due to the count should be eliminated.", "body": "  Description  DynamicContractRegistry is a wrapper registry that allows the zBanc to use the custom upgrader contract while still providing access to the normal bancor registry.  For this to work, the registry owner can add or override any registry setting. Settings that don t exist in this contract are attempted to be retrieved from an underlying registry (contractRegistry).  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L66-L70  function registerAddress(bytes32 _contractName, address _contractAddress)  public  ownerOnly  validAddress(_contractAddress)  If the item does not exist in the registry, the request is forwarded to the underlying registry.  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L52-L58  function addressOf(bytes32 _contractName) public view override returns (address) {  if(items[_contractName].contractAddress != address(0)){  return items[_contractName].contractAddress;  }else{  return contractRegistry.addressOf(_contractName);  According to the documentation this registry is owned by zer0 admins and this means users have to trust zer0 admins to play fair.  To handle this, we deploy our own ConverterUpgrader and ContractRegistry owned by zer0 admins who can register new addresses  The owner of the registry (zer0 admins) can change the underlying registry contract at will. The owner can also add new or override any settings that already exist in the underlying registry. This may for example allow a malicious owner to change the upgrader contract in an attempt to potentially steal funds from a token converter or upgrade to a new malicious contract. The owner can also front-run registry calls changing registry settings and thus influencing the outcome. Such an event will not go unnoticed as events are emitted.  It should also be noted that itemCount will return only the number of items in the wrapper registry but not the number of items in the underlying registry. This may have an unpredictable effect on components consuming this information.  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L36-L43  /**  @dev returns the number of items in the registry  @return number of items  /  function itemCount() public view returns (uint256) {  return contractNames.length;  Recommendation  Require the owner/zer0 admins to be a DAO or multisig and enforce 2-step (notify->wait->upgrade) registry updates (e.g. by requiring voting or timelocks in the admin contract). Provide transparency about who is the owner of the registry as this may not be clear for everyone. Evaluate the impact of itemCount only returning the number of settings in the wrapper not taking into account entries in the subcontract (including pot. overlaps).  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.8 zBanc - DynamicLiquidTokenConverter consider using PPM_RESOLUTION instead of hardcoding integer literals    ", "body": "  Resolution                           This issue was present in the initial commit under review (  zer0-os/zBanc@48da0ac) but has since been addressed with  zer0-os/zBanc@3d6943e.  Description  getMarketCap calculates the reserve s market capitalization as reserveBalance  * 1e6 /  weight where 1e6 should be expressed as the constant PPM_RESOLUTION.  Examples  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L157-L164  function getMarketCap(IERC20Token _reserveToken)  public  view  returns(uint256)  Reserve storage reserve = reserves[_reserveToken];  return reserveBalance(_reserveToken).mul(1e6).div(reserve.weight);  Recommendation  Avoid hardcoding integer literals directly into source code when there is a better expression available. In this case 1e6 is used because weights are denoted in percent to base PPM_RESOLUTION (=100%).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.9 zBanc - DynamicLiquidTokenConverter avoid potential converter type overlap with bancor   ", "body": "  Resolution  Acknowledged by providing the following statement:  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.24 the converterType relates to an array selector in the test helpers, so would be inconvenient to make a higher value. we will have to maintain the value when rebasing in DynamicLiquidTokenConverter & Factory, ConverterUpgrader, and the ConverterUpgrader.js test file and Converter.js test helper file.", "body": "  Description  The system is forked frombancorprotocol/contracts-solidity. As such, it is very likely that security vulnerabilities reported to bancorprotocol upstream need to be merged into the zer0/zBanc fork if they also affect this codebase. There is also a chance that security fixes will only be available with feature releases or that the zer0 development team wants to merge upstream features into the zBanc codebase.  Note that the current master of the bancorprotocol already appears to defined converterType 3 and 4: https://github.com/bancorprotocol/contracts-solidity/blob/5f4c53ebda784751c3a90b06aa2c85e9fdb36295/solidity/test/helpers/Converter.js#L51-L54  Examples  The new custom converter  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L50-L52  function converterType() public pure override returns (uint16) {  return 3;  ConverterTypes from the bancor base system  zBanc/solidity/contracts/converter/types/liquidity-pool-v1/LiquidityPoolV1Converter.sol:L71-L73  function converterType() public pure override returns (uint16) {  return 1;  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L73-L76  Recommendation  /  function converterType() public pure override returns (uint16) {  return 2;  Recommendation  Choose a converterType id for this custom implementation that does not overlap with the codebase the system was forked from. e.g. uint16(-1) or 1001 instead of 3 which might already be used upstream.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.10 zBanc - unnecessary contract duplication    ", "body": "  Resolution                           fixed with   zer0-os/zBanc@ff3d913 by removing the duplicate contract.  Description  DynamicContractRegistryClient is an exact copy of ContractRegistryClient. Avoid unnecessary code duplication.  < contract DynamicContractRegistryClient is Owned, Utils {  ---  > contract ContractRegistryClient is Owned, Utils {  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.1 Delegated transactions can be executed for multiple accounts    ", "body": "  Resolution  Comment from the client: The issue has been solved  Description  The Gateway contract allows users to create meta transactions triggered by the system s backend. To do so, one of the owners of the account should sign the message in the following format:  code/src/gateway/Gateway.sol:L125-L131  address sender = _hashPrimaryTypedData(  _hashTypedData(  nonce,  to,  data  ).recoverAddress(senderSignature);  The message includes a nonce, destination address, and call data. The problem is that this message does not include the account address. So if the sender is the owner of multiple accounts, this meta transaction can be called for multiple accounts.  Recommendation  Add the account field in the signed message or make sure that any address can be the owner of only one account.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.2 Removing an owner does not work in PersonalAccountRegistry    ", "body": "  Resolution  Comment from the client: The issue has been solved  Description  An owner of a personal account can be added/removed by other owners. When removing the owner, only removedAtBlockNumber value is updated. accounts[account].owners[owner].added remains true:  code/src/personal/PersonalAccountRegistry.sol:L116-L121  accounts[account].owners[owner].removedAtBlockNumber = block.number;  emit AccountOwnerRemoved(  account,  owner  );  But when the account is checked whether this account is the owner, only accounts[account].owners[owner].added  is actually checked:  code/src/personal/PersonalAccountRegistry.sol:L255-L286  function _verifySender(  address account  private  returns (address)  address sender = _getContextSender();  if (!accounts[account].owners[sender].added) {  require(  accounts[account].salt == 0  );  bytes32 salt = keccak256(  abi.encodePacked(sender)  );  require(  account == _computeAccountAddress(salt)  );  accounts[account].salt = salt;  accounts[account].owners[sender].added = true;  emit AccountOwnerAdded(  account,  sender  );  return sender;  So the owner will never be removed, because accounts[account].owners[owner].added will always be `true.  Recommendation  Properly check if the account is still the owner in the _verifySender  function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.3 The withdrawal mechanism is overcomplicated    ", "body": "  Resolution  Comment from the client:  The withdrawal mechanism has been refactored. In current version user can withdraw funds from the deposit account in two ways:  with guardian signature - withdrawDeposit  using  deposit exit  process  Description  To withdraw the funds, anyone who has the account in PaymentRegistry should call the withdrawDeposit function and go through the withdrawal process. After the lockdown period (30 days), the user will withdraw all the funds from the account.  code/src/payment/PaymentRegistry.sol:L160-L210  function withdrawDeposit(  address token  external  address owner = _getContextAccount();  uint256 lockedUntil = deposits[owner].withdrawalLockedUntil[token];  /* solhint-disable not-rely-on-time */  if (lockedUntil != 0 && lockedUntil <= now) {  deposits[owner].withdrawalLockedUntil[token] = 0;  address depositAccount = deposits[owner].account;  uint256 depositValue;  if (token == address(0)) {  depositValue = depositAccount.balance;  } else {  depositValue = ERC20Token(token).balanceOf(depositAccount);  _transferFromDeposit(  depositAccount,  owner,  token,  depositValue  );  emit DepositWithdrawn(  depositAccount,  owner,  token,  depositValue  );  } else {  _deployDepositAccount(owner);  lockedUntil = now.add(depositWithdrawalLockPeriod);  deposits[owner].withdrawalLockedUntil[token] = lockedUntil;  emit DepositWithdrawalRequested(  deposits[owner].account,  owner,  token,  lockedUntil  );  /* solhint-enable not-rely-on-time */  During that period, everyone who has a channel with the user is forced to commit their channels or lose money from that channel. When doing so, every user will reset the initial lockdown period and the withdrawer should start the process again.  code/src/payment/PaymentRegistry.sol:L479-L480  if (deposits[sender].withdrawalLockedUntil[token] > 0) {  deposits[sender].withdrawalLockedUntil[token] = 0;  There is no way for the withdrawer to close the channel by himself. If the withdrawer has N channels, it s theoretically possible to wait for up to N*(30 days) period and make N+2 transactions.  Recommendation  There may be some minor recommendations on how to improve that without major changes:  When committing a payment channel, do not reset the lockdown period to zero. Two better option would be either not change it at all or extend to now + depositWithdrawalLockPeriod  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.4 A malicious guardian can steal funds   ", "body": "  Resolution  Comment from the client: The etherspot payment system is semi-trusted by design.  Description  A guardian is signing every message that should be submitted as a payment channel update. A guardian s two main things to verify are: blockNumber and the fact that the sender has enough funds.  There are two main attack vectors for the malicious guardian:  It s possible to conspire with the previous owner of the account and submit the old blockNumber. This allows them to drain the account.  A guardian can also conspire with the sender and send more funds to multiple channels than funds in the account.  Recommendation  Reduce the system s reliance on single points of failure like the guardians.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.5 Upgrade solidity version    ", "body": "  Resolution  Solidity version has been upgraded to 0.6.12  Description  The current minimal solidity version is 0.6.0. But some parts of the code use features from the later versions of solidity, like the high-level version of CREATE2 to create accounts.  Recommendation  Upgrade solidity version to the latest stable (0.6.12).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.6 The lockdown period shouldn t be extended when called multiple times    ", "body": "  Resolution  Comment from the client: The issue has been solved  Description  In order to withdraw a deposit from the PaymentRegistry, the account owner should call the withdrawDeposit function and wait for depositWithdrawalLockPeriod (30 days) before actually transferring all the tokens from the account.  The issue is that if the withdrawer accidentally calls it for the second time before these 30 days pass, the waiting period gets extended for 30 days again.  code/src/payment/PaymentRegistry.sol:L170-L199  if (lockedUntil != 0 && lockedUntil <= now) {  deposits[owner].withdrawalLockedUntil[token] = 0;  address depositAccount = deposits[owner].account;  uint256 depositValue;  if (token == address(0)) {  depositValue = depositAccount.balance;  } else {  depositValue = ERC20Token(token).balanceOf(depositAccount);  _transferFromDeposit(  depositAccount,  owner,  token,  depositValue  );  emit DepositWithdrawn(  depositAccount,  owner,  token,  depositValue  );  } else {  _deployDepositAccount(owner);  lockedUntil = now.add(depositWithdrawalLockPeriod);  Recommendation  Only extend the waiting period when a withdrawal is requested for the first time.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.7 Missing documentation    ", "body": "  Resolution  Comment from the client: Code has been documented - We will work on white paper, graphs later  Description  The code base as is, is missing proper documentations to understand the code work flow and logic. The most important pieces are high-level diagrams, user work flows, and updated white paper.  It is important for readability and maintainability of the codebase to add in-line documentations. The Pillar code base under the audit lacks any type of inline documentation and it makes the code reviewer s job much harder. We highly recommend to provide inline documentation using Solidity s natspec format, as this will be easier to maintain.  As an example PaymentRegistry.sol without the documentation is really hard to read and understand. There are many assumptions or off-chain dependencies and it s impossible to understand the flows simply by reading the solidity code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.8 Gateway can call any contract   ", "body": "  Resolution  Comment from the client: That s right Gateway can call any contract, we want to keep it open for any external contract.  Description  The Gateway contract is used as a gateway for meta transactions and batched transactions. It can currently call any contract, while is  only intended to call specific contracts in the system that implemented GatewayRecipient interface:  code/src/gateway/Gateway.sol:L280-L292  for (uint256 i = 0; i < data.length; i++) {  require(  to[i] != address(0)  );  // solhint-disable-next-line avoid-low-level-calls  (succeeded,) = to[i].call(abi.encodePacked(data[i], account, sender));  require(  succeeded  );  There are currently no restrictions for to value.  Recommendation  Make sure, only intended contracts can be called by the Gateway : PersonalAccountRegistry, PaymentRegistry, ENSController.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.9 Remove unused code    ", "body": "  Resolution  Comment from the client: Unused code has been removed  Description  In account/AccountController.sol when deploying an account, the function _deployAccount() gets an extra input value which is always 0 and not set in any other method.  Examples  code/src/common/account/AccountController.sol:L24-L38  return _deployAccount(  salt,  );  function _deployAccount(  bytes32 salt,  uint256 value  internal  returns (address)  return address(new Account{salt: salt, value: value}());  Recommendation  It is recommended to remove this value as there are no use cases for it at the moment, however if it is planned to be used in the future, it should be well documented in the code to prevent confusion.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.10 Using ENS subdomains introduces possible privacy issues   ", "body": "  Resolution                           Comment from the client: This is a known issue - we also added   Description  Using ENS names by default introduces a privacy issue for users. The current implementation leaks all user addresses and their associated username. This is possibly known issue, however it is worth to mention as part of this audit.  Examples  Here s a sample of already registered addresses on mainnet fetched using Legions:  sbeta> ens listSubdomains name=\"pillar.eth\"  > Subdomains for 'pillar.eth'  > NameHash: '0x5bb02333b1f96385ba28fd63408843cfeee095b32196b718786a56e491e33387'  mrsirio  0x6d2ce500f82e20cdeb733ec0530360d2e761f44d  coinstacker  0x60cc065f860682fb899a385b9af66fe82b412b29  dadang  0x904e88eb2602d947ded5c0c5b84c32109255a5f2  ramaido  0x1ee590464e00780ab1c620de41545e74c0731521  tongkol  0x3cbbf43f7a449d54a71bf97c779186f183d1e9eb  kell  0x3d48c65ddfb5bed5980b40974416b55eceed6fab  sipa  0x944972562ea6a07ee0f77bf6ce89559214347774  joyboy  0x4660b09e45930d5ffaedf36bad4a37705303970b  ryanc  0x0c58b9d8b6bdfcd7fb33ab1ecc6b0db4fa94a7b8  hammad  0xe94bb8ea91bfa791cf632e2353cabb87a93713d6  nicolas  0x12ce0a744ccf8958b6859aff1e85bca797e4f742  timmy2shoes  0xafad99c454d97b0130da64179e1a5a7b516ae225  sergvind  0xd5164fe7b9b1d44dd4eb35ef312ada6bce2878ff  0x7384e49fdf540de561f0dc810cc9ad87e909afbe  0x2e496c59c5a0f525d82cf0402851f361ac879c63  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "4.1 No Limit for Minting Amount    ", "body": "  Description  In token contract FiatTokenV1, there is no limit set for amount of tokens can be minted, as a result, the minter can mint unlimited tokens, disrupting the token supply and value.  Examples  ../src/v1/FiatTokenV1.sol:L77-L79  function mint(address to, uint256 amount) public onlyRole(MINTER_ROLE) {  _mint(to, amount);  Recommendation  Add a limit for the number of tokens the minter can mint.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/02/trillion-network/"}, {"title": "4.2 Private Key Is Exposed in the Deployment and Upgrade Script    ", "body": "  Description  In the contract deploying and upgrading script, private key is used to broadcast the transaction, this would expose private key of the deployer and upgrader account on the machine running the script, therefore compromising these accounts.  Examples  ../script/DeployFiatToken.s.sol:L12  uint256 deployerPrivateKey = vm.envUint(\"PRIVATE_KEY\");  ../script/UpgradeFiatToken.s.sol:L13-L14  uint256 deployerPrivateKey = vm.envUint(\"PRIVATE_KEY\");  vm.startBroadcast(deployerPrivateKey);  Recommendation  Have Forge sending a raw transaction to the cold wallet of the account, the wallet signs the transaction then return the signed transactions to Forge and broadcaster. Alternatively use different wallet for deployment and upgrade and stop using the wallet after the script is complete  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/02/trillion-network/"}, {"title": "4.3  Functions Are Public and Without Access Control    ", "body": "  Description  Examples  ../src/v1/RescuableV1.sol:L30  function rescue(IERC20 token, address to, uint256 amount) public virtual {  ../src/v1/BlacklistableV1.sol:L51-L63  function blacklist(address account) public virtual {  _blacklisted[account] = true;  emit Blacklisted(account);  /**  @dev Removes account from blacklist  @param account The address to remove from the blacklist  /  function unBlacklist(address account) public virtual {  _blacklisted[account] = false;  emit UnBlacklisted(account);  Recommendation  Make these functions internal and in the child contract add correspondent public function with authentication to call the inherited functions  ", "labels": ["Consensys", "Medium", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/02/trillion-network/"}, {"title": "4.4 Unecessary Parent Contracts    ", "body": "  Description  Contract BlacklistableV1 and RescuableV1 extends ContextUpgradeable and ERC20Upgradeable, which are not used in any of contract functions and are already inherited by the child contract FiatTokenV1.  Examples  ../src/v1/BlacklistableV1.sol:L16  abstract contract BlacklistableV1 is Initializable, ContextUpgradeable, ERC20Upgradeable {  ../src/v1/RescuableV1.sol:L16  abstract contract RescuableV1 is Initializable, ContextUpgradeable, ERC20Upgradeable {  ../src/v1/FiatTokenV1.sol:L15-L25  contract FiatTokenV1 is  Initializable,  ERC20Upgradeable,  ERC20PausableUpgradeable,  ERC20BurnableUpgradeable,  AccessControlUpgradeable,  ERC20PermitUpgradeable,  UUPSUpgradeable,  BlacklistableV1,  RescuableV1  Recommendation  Remove the unnecessary parent contracts  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/02/trillion-network/"}, {"title": "4.5 Redundant  _disableInitializers in Constructor    ", "body": "  Description  Examples  ../src/v1/FiatTokenV1.sol:L38-L40  constructor() {  _disableInitializers();  Recommendation  Remove constructor from FiatTokenV1  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/02/trillion-network/"}, {"title": "5.1 Superfluous Permission endowment:ethereum-provider    ", "body": "  Resolution                           fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by removing the ethereum provider permission from the manifest.  Description  The snap requests permission endowment:ethereum-provider but window.ethereum is never accessed from within the snap s context.  snap/snap.manifest.json:L39  \"endowment:ethereum-provider\": {}  Recommendation  Remove superfluous permissions.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.2 A Trusted Website Can Add Any Address to the Snaps Address Storage; No Control Over Added Addresses; Confirmation Is a Notification    ", "body": "  Resolution  partially addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by only allowing trusted origins to interact with the snap.  Update: user confirmation for address management (add/remove current account) added with ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Description  Trusted websites can add addresses to the list of addresses the user wants to receive notifications for. However, the user has no control over the addresses, and even though the code suggests that the snap user must confirm new address addition, this confirmation is merely a notification that the address has been added.  The lack of address management may lead to a self-DoS when too many addresses are added to the extension.  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: \"local:http://localhost:8080\",  request: { method: 'hello', params: { address: \"\\nhi\\nho\" } },  }})  push-snap-site/components/buttons/ConfirmButton.tsx:L6-L35  export default function ConfirmButton() {  const { address, isConnecting, isDisconnected } = useAccount();  const defaultSnapOrigin = `local:http://localhost:8080`;  const sendHello = async (address: string) => {  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: defaultSnapOrigin,  request: { method: 'hello', params: { address: address } },  },  });  };  const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({  message:  `Confirm your Address ${address}, \\n this will be added to MetaMask for sending notifications`,  });  function sleep(ms: number) {  return new Promise((resolve) => setTimeout(resolve, ms));  const confirmAddition=async()=>{  signMessage();  if(isSuccess){  await sleep(5000);  await sendHello(String(address));  The same is true for configuration settings. Any connected dap may set togglepopup. This may be problematic in multi-dapp scenarios where multiple dapps request to set togglepopup.  Recommendation  Note that dapps are not necessarily completely trusted. They can be modified, or malicious behavior may be added later by the dapp deployer (unless used locally or via IPFS). Therefore, the snap should always notify the wallet owner of important state changes and allow them to reject them or, in this case, manage addresses that ve been added previously.  Consider checking the origin in onRPC if this dapp is only meant to be called from a specific dapp address. Otherwise, any connected dapp may change configuration settings.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.3 Lax Input Validation, Control Char, URI, and Markdown Injection    ", "body": "  Resolution  addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 validating the address with ethers.utils.isAddress.  Update 1:  Markdown rendering of newlines fixed with: ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Major: Markdown Injection in Confirmation Dialogue re-introduced with ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Update 2:  Markdown Injection in Confirmation Dialogue fixed with ethereum-push-notification-service/push-protocol-snaps@b40e141243c77bfd7ec109408b326607b19314c8  Description  There is no input validation on the address to be added. The input may be an ethereum address but can be anything, potentially breaking security assumptions in the code and leading to unwanted side effects.  request.params may be null, and  request.params.address may not be an ethereum address.  snap/src/index.ts:L18  await addAddress(request.params.address || \"0x0\");  Example  await window.ethereum?.request({  method: \"wallet_invokeSnap\",  params: {  snapId: \"local:http://localhost:8080\",  request: { method: 'hello', params: { address: \"Hi \ud83d\ude4c\\n\\n \ud83d\udd38 **boom**\" } },  }})  URI injection if address contains ?#/  snap/src/utils/fetchnotifs.ts:L3-L13  export const getNotifications=async(address:string)=>{  const url = `https://backend-prod.epns.io/apis/v1/users/eip155:5:${address}/feeds`;  const response = await fetch(url, {  method: 'get',  headers: {  'Content-Type': 'application/json',  },  });  const data = await response.json();  return data;  Injection in notifications  snap/src/utils/popupHelper.ts:L3-L12  export const popupHelper = (notifs: String[]) => {  let msg = [];  if (notifs.length > 0) {  notifs.forEach((notif) => {  let str = `\\n\ud83d\udd14` + notif + \"\\n\";  msg.push(str);  });  return msg;  };  Markdown injection  snap/src/utils/fetchAddress.ts:L45-L52  const data = persistedData.addresses;  const popup = persistedData.popuptoggle;  let msg='';  for(let i = 0; i < data!.length; i++){  msg = msg + '\ud83d\udd39' + data![i] + '\\n';  return snap.request({  method: 'snap_dialog',  Also, note that the currently rendered markdown that lists addresses appears wrong, as markdown newlines require \\n\\n instead of \\n.  Recommendation  Strictly validate inputs from external origins. Ensure that the provided address is a valid ethereum address. Optionally check the addresses checksum to detect typos. Ensure that inputs may not lead to renderable markdown. Fix the rendered list of addresses to properly display as a newline d list. Ensure untrusted inputs cannot inject context-sensitive information into fetch urls.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.4 persistedData Race Where snap_manageState.get Returnsnull    ", "body": "  Resolution                           addressed with   ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829 by introducing a wrapper function that ensures that snapstate returns sane defaults. This function is not used everywhere, but in places where it is not, custom checks are employed.  Description  Metamask Error:  snap.request(, {method: 'snap_manageState', params: {operation: 'get'}}) may return null. Snap state is only initialized on rpc request method hello via addAddress().  This is the only method that checks if the retrieved state is null:  snap/src/utils/fetchAddress.ts:L5-L20  export const addAddress = async (address:string) => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  if(persistedData == null){  const data = {  addresses: [address],  popuptoggle: 0,  };  await snap.request({  method: 'snap_manageState',  params: { operation: 'update', newState:data },  });  snap/src/index.ts:L12-L21  export const onRpcRequest: OnRpcRequestHandler = async ({  origin,  request,  }) => {  switch (request.method) {  case \"hello\": {  await addAddress(request.params.address || \"0x0\");  await confirmAddress();  break;  If the state was never initialized or there was a race where rpc-hello() was not called first, then the snap may run into a null deref exception (here rpc-togglepopup):  snap/src/utils/toggleHelper.ts:L2-L12  let persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  let popuptoggle = notifcount;  const data = {  addresses: persistedData.addresses,  popuptoggle: popuptoggle,  };  Recommendation  Wrap snap_manageState with a function that always falls back to safe defaults if the snap state was never set. This also obsoleted the future need to check if persistedData is null as the new method ensures safe non-null defaults.  This should also silence some of the type errors reported by tslint that warn that attributes of persistentdata are read while it might be null (see issue 5.6 ).  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.5 User Flow - Request to Sign Message Does Not Provide Security Guarantee    ", "body": "  Resolution                           obsolete, removed with   ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829.  Description  A connected dapp can add any address to the snap via the RPC method hello. There is no added security by requesting the user to sign with their address as the backend API gives access to any address notification (they are not private) and the dapps request is a front-end-only solution. A user may add any other address by creating their dapp which allows custom addresses.  In light of this, the front-end (dapp) security check requiring the user to prove that they are in possession of the private key appears not to add any security guarantees to the snap. Instead, the snap may want to enumerate wallet account addresses internally instead and remove the hello API altogether, or, allow any address to be added without requiring a proof of ownership of an address.  Examples  push-snap-site/components/buttons/ConfirmButton.tsx:L20-L23  const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({  message:  `Confirm your Address ${address}, \\n this will be added to MetaMask for sending notifications`,  });  Recommendation  Remove the signature check, and add linked accounts from within the snaps context. Be transparent that notification texts are not private, and anyone can subscribe to the back-end API. If notifications are private to the recipient, we suggest encrypting them for the target account and adding logic in the snap to allow the recipient to decrypt them within the context of the snap.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.6 TypeScript Errors    ", "body": "  Resolution  partially addressed in ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761:  toggleHelper not addressed.  popupHelper addressed as per recommendation.  fetchAllAddrNotifs fixed by forcing fetchAddress() to return empty array instead.  persistedData partially addressed. might still null-deref at persistedData.addresses in index.ts  Update: toggleHelper and persistedData addressed with the snap data check wrapper function in ethereum-push-notification-service/push-protocol-snaps@7ee018947303014e8c14e9413a5edd9fd29f9829  Description  toggleHelper  persistedData should be checked for null and default to a sane initial config. notifcount:Number should be notifcount:number.  Type '{ addresses: Json; popuptoggle: Number; }' is not assignable to type 'Record<string, Json>'.  Property 'popuptoggle' is incompatible with index signature.  Type 'Number' is not assignable to type 'Json'.  Type 'Number' is not assignable to type '{ [prop: string]: Json; }'.  Index signature for type 'string' is missing in type 'Number'.ts(2322)  snap/src/utils/toggleHelper.ts:L7-L16  let popuptoggle = notifcount;  const data = {  addresses: persistedData.addresses,  popuptoggle: popuptoggle,  };  await snap.request({  method: 'snap_manageState',  params: { operation: 'update', newState:data },  });  popupHelper  let msg = [] should be let msg = [] as String[];  Variable 'msg' implicitly has an 'any[]' type.ts(7005)  addresses can be null  snap/src/utils/fetchnotifs.ts:L34-L37  export const fetchAllAddrNotifs = async () => {  const addresses = await fetchAddress();  let notifs:String[] = [];  for(let i = 0; i < addresses.length; i++){  persistedData can be null  snap/src/index.ts:L63-L68  let persistedData = await snap.request({  method: \"snap_manageState\",  params: { operation: \"get\" },  });  let popuptoggle = Number(persistedData.popuptoggle) + msgs.length;  Recommendation  Fix the typescript configuration (see issue 5.13 ). Fix all reported ts-lint errors. Avoid using any types and use safe types instead.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.7 Avoid Hardcoding the Local Snap ID ", "body": "  Description  The local snap-id is hardcoded in various places. Local snap IDs should not be used in production. Hence, we recommend defining and importing the snap id from a single source file within the project, setting it to local:http://localhost:8080 and npm:push-v1 depending on whether the build is set to be production or development (e.g., using an environment variable).  push-snap-site/components/buttons/ConfirmButton.tsx:L6-L8  export default function ConfirmButton() {  const { address, isConnecting, isDisconnected } = useAccount();  const defaultSnapOrigin = `local:http://localhost:8080`;  push-snap-site/components/buttons/ReconnectButton.tsx:L4-L6  export default function ReconnectButton() {  const defaultSnapOrigin = `local:http://localhost:8080`;  push-snap-site/components/buttons/SendMessageButton.tsx:L1-L3  export default function ReconnectButton() {  const defaultSnapOrigin = `local:http://localhost:8080`;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.8 package.json - Invalid License    ", "body": "  Resolution                           fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by changing the license to GPLv2.  Description  The license field in package.json is invalid.  snap/package.json:L9  \"license\": \"(MIT-0 OR Apache-2.0)\",  Recommendation  Update the license field.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.9 fetchAddress  - Inaccurate Function Name ", "body": "  Description  Function fetchAddress returns an array of addresses and should, therefore, be named fetchAddresses  snap/src/utils/fetchAddress.ts:L66-L73  export const fetchAddress = async () => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  const addresses = persistedData!.addresses;  return addresses;  };  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.10 currentepoch  - Unnecesary Conversion From/to String    ", "body": "  Resolution                           fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by not converting  Description  It is unclear why currentepoch is declared as String while calculations require it to be numerical.  Examples  snap/src/utils/fetchnotifs.ts:L15-L31  export const filterNotifications=async(address:string)=>{  let fetchedNotifications = await getNotifications(address);  fetchedNotifications = fetchedNotifications?.feeds;  let notiffeeds:String[] = [];  const currentepoch:string = Math.floor(Date.now() / 1000).toString();  if(fetchedNotifications.length > 0){  for(let i = 0; i < fetchedNotifications.length; i++){  let feedepoch = fetchedNotifications[i].payload.data.epoch;  feedepoch = Number(feedepoch).toFixed(0);  if(feedepoch > parseInt(currentepoch)-60) {  let msg = fetchedNotifications[i].payload.data.app+' : '+fetchedNotifications[i].payload.data.amsg;  notiffeeds.push(msg);  notiffeeds = notiffeeds.reverse();  return notiffeeds;  Recommendation  currentepoch should be numerical.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.11 Dead Code popup    ", "body": "  Resolution                           fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by removing the used code.  Description  const popup is retrieved from the snap state but never used within the context of confirmAddress(). This might be an indicator of an incomplete implementation of the togglePopup setting or dead code.  snap/src/utils/fetchAddress.ts:L40-L47  export const confirmAddress = async () => {  const persistedData = await snap.request({  method: 'snap_manageState',  params: { operation: 'get' },  });  const data = persistedData.addresses;  const popup = persistedData.popuptoggle;  let msg='';  Recommendation  Double check if this setting is meant to be read (unlikely) or else clean up and remove unused code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.12 Unused Import ethers, @metamask/snaps-ui    ", "body": "  Resolution                           fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by using  Description  ethers  ethers is listed as a dependency and imported by fetchAddress.ts but is never used.  snap/src/utils/fetchAddress.ts:L3  const {ethers} = require('ethers');  @metamask/snaps-ui  @metamask/snaps-ui is imported in popupHelper but the imported components are never used.  snap/src/utils/popupHelper.ts:L1  import { heading, panel, text } from \"@metamask/snaps-ui\";  Recommendation  Remove the unused import/dependency.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.13 Non-Existent Base Config (Eslint, Tsconfig) ", "body": "  Description  .eslintrc.js points to a base configuration outside of this repository.  snap/.eslintrc.js:L2  extends: ['../../.eslintrc.js'],  .eslintrc.js  tsconfig.json  snap/tsconfig.json:L2  \"extends\": \"../../tsconfig.json\",  Recommendation  Provide the eslint base configuration with the repository to allow for reproducible lint runs. Run the linter as part of github commit checks.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.14 Performance - await in for Loop   ", "body": "  Resolution                           fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by using  Description  Performing an await as part of each operation is an indication that the program is not taking full advantage of the parallelization benefits of async/await:  snap/src/utils/fetchnotifs.ts:L38  let temp = await filterNotifications(addresses[i]);  Recommendation  Using Promise.all() fully utilizes parallelism and improves performance  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "5.15 API Design - Consider Using Consistent RPC Method Names   ", "body": "  Resolution                           fixed in   ethereum-push-notification-service/push-protocol-snaps@1a6a32ef760088ca59f73e555f41b5b5d871f761 by changing the rpc method names as per recommendation.  Description  Consider using descriptive RPC method names with a distinct prefix, e.g. pushproto_initialize, pushproto_addaddress, pushprotoc_togglepopup:  snap/src/index.ts:L17  case \"hello\": {  snap/src/index.ts:L22  case \"init\": {  snap/src/index.ts:L36  case \"togglepopup\": {  Note that init can be called multiple times and is not initializing anything.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/07/push-protocol-snap-for-metamask/"}, {"title": "4.1 Dapp May Force a Sign Approval Dialog Without Showing the Message to Be Signed    ", "body": "  Resolution  Addressed by always displaying the raw message to be signed to the user. This allows them to independently verify that this is indeed what they want to sign off on. Additionally, the client provided the following statement and changesets addressing the finding:  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.1 - always display raw transaction payload", "body": "  Changesets:  solflare-wallet/solflare-snap@9537098  solflare-wallet/aptos-snap@b857c16  solflare-wallet/sui-snap@d28c4e3  Description  With the request.params.displayMessage parameter in requests to signTransaction and signAllTransactions the dapp controls if the message to be signed is displayed to the user or not. Allowing the dapp to control if the data to be signed is displayed to the user is dangerous as the dapp may silently ask for a signature to sign data the user did not intend to sign. This has potential to undermine security controls and procedures implemented by MetaMask which generally enforce clarity of what data the user is requested to sign.  Note that the snap as an extension to the MetaMask trust module should not have to trust the dapp that is requesting signature.  Examples  Affects all snaps under review.  Solflare Snap  ../aptos-snap/src/index.js:L39-L51  const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertInput(message);  assertIsString(message);  assertIsArray(simulationResult);  assertAllStrings(simulationResult);  assertIsBoolean(displayMessage);  const accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);  assertConfirmation(accepted);  ../aptos-snap/src/ui.js:L28-L33  text(host),  ...(simulationResultItems.length > 0 || displayMessage ? [divider()] : []),  ...simulationResultItems,  ...(displayMessage ? [copyable(message)] : [])  ])  Recommendation  In accordance with how MetaMask signing works, it is highly recommended to remove the displayMessage toggle and consistently enforce the message to be signed to be displayed. Else, there is no way for the user to verify if they are signing the correct data/transaction.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.2 CtrlChar/Markdown Injection in renderSignTransaction, renderSignAllTransactions    ", "body": "  Resolution  The identified vulnerability has been remedied by excluding the simulation result from the dialog. It is essential to emphasize that the extension should ideally generate the simulation result as a validated source of information. The transmission of this information from a less trustworthy entity (even though it pertains to the dapp linked to the snap) to be displayed within the secure context of a snap, influencing users to endorse raw data based on this simulation result, may inherently introduce security risks. Eliminating the simulation result parameter from the RPC request effectively mitigates this specific injection opportunity. Additionally, the client has furnished the subsequent statement and alterations to address the aforementioned issue:  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.2 - fully remove simulation results. This removes the possibility to inject markdown or control characters in unsafe places. Given the multi-entrypoint argument and the lack of a straightforward way to parse and simulate transactions within the snap, we think raw transaction payloads provide an undeniable source of truth and let users parse those transactions with external tools. It is on our roadmap to see how this can be improved in the future", "body": "  Changesets:  solflare-wallet/solflare-snap@9537098  solflare-wallet/aptos-snap@b857c16  solflare-wallet/sui-snap@d28c4e3  Description  On certain occasions, the snap may need to present a dialog to the user to request confirmation for an action or data verification. This step is crucial as dapps are not always trusted, and it s essential to prevent scenarios where they can silently sign data or perform critical operations using the user s keys without explicit permission. To create custom user-facing dialogs, MetaMask provides the Snaps UI package, equipped with style-specific components. However, some of these components have been found to have unintended side-effects.  For instance, the text() component can render Markdown or allow for control character injections.  In the code snippet provided below, please note that request.params is considered untrusted. For example, request.params.simulationResult[] may contain Markdown renderable strings or Control Characters that can disrupt the context of the user-displayed message.  request.params.origin is validated via (new URL(dappOrigin))?.host; However, note, that the dapp may misrepresent the original origin!  requests.params.message is safely put into a copyable  request.params.simulationResult is unchecked  Please also note that the user might decide whether to sign or reject the transaction based on the simulation that is being displayed. This simulation result, however, is directly provided by the dapp which is less trusted than the metamask security module/snap. This might lead to users signing data based on potentially false information if the dapp provides malicious information. It should, therefore, be considered to generate the simulation information within the snap itself!  Examples  This affects all snaps under review.  Solflare Snap  signTransaction  ../solflare-snap/src/index.js:L41-L53  const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertInput(message);  assertIsString(message);  assertIsArray(simulationResult);  assertAllStrings(simulationResult);  assertIsBoolean(displayMessage);  const accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);  assertConfirmation(accepted);  ../solflare-snap/src/ui.js:L19-L35  export function renderSignTransaction(host, message, simulationResult, displayMessage = true) {  const simulationResultItems = simulationResult.map((item) => text(item));  return snap.request({  method: 'snap_dialog',  params: {  type: 'confirmation',  content: panel([  heading('Sign transaction'),  text(host),  ...(simulationResultItems.length > 0 || displayMessage ? [divider()] : []),  ...simulationResultItems,  ...(displayMessage ? [copyable(message)] : [])  ])  });  renderSignAllTransactions  ../solflare-snap/src/ui.js:L51-L55  simulationResults[i].forEach((item) => uiElements.push(text(item)));  if (displayMessage) {  uiElements.push(copyable(messages[i]));  Sui Snap  ../sui-snap/src/index.js:L41-L52  const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertInput(message);  assertIsString(message);  assertIsArray(simulationResult);  assertAllStrings(simulationResult);  assertIsBoolean(displayMessage);  const accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);  assertConfirmation(accepted);  ../sui-snap/src/index.js:L64-L77  const { derivationPath, messages, simulationResults = [], displayMessage = true } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertInput(messages);  assertIsArray(messages);  assertInput(messages.length);  assertAllStrings(messages);  assertIsArray(simulationResults);  assertInput(messages.length === simulationResults.length);  assertIsBoolean(displayMessage);  const accepted = await renderSignAllTransactions(dappHost, messages, simulationResults, displayMessage);  assertConfirmation(accepted);  Aptos Snap  ../aptos-snap/src/index.js:L39-L50  const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertInput(message);  assertIsString(message);  assertIsArray(simulationResult);  assertAllStrings(simulationResult);  assertIsBoolean(displayMessage);  const accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);  assertConfirmation(accepted);  ../aptos-snap/src/index.js:L62-L75  const { derivationPath, messages, simulationResults = [], displayMessage = true } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertInput(messages);  assertIsArray(messages);  assertInput(messages.length);  assertAllStrings(messages);  assertIsArray(simulationResults);  assertInput(messages.length === simulationResults.length);  assertIsBoolean(displayMessage);  const accepted = await renderSignAllTransactions(dappHost, messages, simulationResults, displayMessage);  assertConfirmation(accepted);  Recommendation  Validate inputs. Encode data in a safe way to be displayed to the user. Show the original data provided within a pre-text or code block (copyable). Show derived or decoded information (token recipient) as additional information to the user. If possible, generate a trusted simulation result within the snap.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.3 Insufficient Input Validation deriveKeyPair() ", "body": "  Resolution  The client has issued the following statement:  Solflare Comment/Background  We have inspected the @metamask/key-tree library and made sure that all incorrect inputs throw errors. This is the reason why we didn t duplicate the checks. The reason why we supply full paths is because the APIs of our Wallet forward full paths and to avoid sensitive splitting of paths in various places.  Solflare Resolution  We respect the recommendation, but due to architectural decisions up the stack, we would like to keep the deriveKeyPair() API as-is.  Statement from the Assessment Team:  The principle of defense in depth necessitates comprehensive scrutiny across all layers of the system. It is unwise to rely solely on a third-party library to handle error propagation, as you lack control over the underlying codebase. We strongly advocate for the pervasive implementation of input validation mechanisms in adherence to secure coding practices, thereby fortifying the defense in depth strategy. Given that this module pertains to wallet trust, it is imperative that coding standards adhere to the highest levels of security, even if it results in redundant checks. Prioritizing safety over convenience is paramount.  Regarding the API design, we acknowledge the intent for code reuse. Nevertheless, we propose the adoption of a well-defined API structure that employs structured data instead of parseable strings. This approach enhances security and simplifies maintenance, mitigating potential complications.  Description  path is checked for correct type: string  ../solflare-snap/src/index.js:L23-L30  case 'getPublicKey': {  const { derivationPath, confirm = false } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertIsBoolean(confirm);  const keyPair = await deriveKeyPair(derivationPath);  but is not checked for valid key derivation path format which may lead to unexpected outcomes or unhandled exceptions.  ../solflare-snap/src/privateKey.js:L20-L20  const segments = path.split('/').slice(3).filter(Boolean);  For example, the function allows non alpha-num 0-9+', slip:x prefixes, or empty elements (\"m/44'/784'\".split(\"/\").slice(3).filter(Boolean) => []).  Affects all snaps under review.  Recommendation  In general, the API design should be re-designed with the RPC functions receiving and validating only the last part of the key part, enforcing the format to be valid for the use case.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.4 Dapp May Suppress User Confirmation on Request to Extract Pubkey; May Extract Any Net-Key ", "body": "  Resolution  The client has issued the following statement:  Solflare Resolution  Given that arbitrary dApps can not suppress user confirmation (facilitated by the Wallet), that essential wallet flows are enabled by the feature and the comparison with the security model of a hardware wallet, Solflare would like to retain the functionality.  Statement from the Assessment Team:  As discussed, it s crucial to recognize that the snap represents the trusted component in this configuration, while the dapp s trustworthiness may not always be guaranteed. It s imperative that your dapp does not assume control over the user s trust solely by displaying a confirmation message. The ideal approach should align with the security controls employed by the MetaMask (MM) implementation, which consists of (1) establishing a connection between MM and the website and (2) granting explicit approval for all accounts accessible by the website. This is in stark contrast to assuming that connecting the snap to a dapp implicitly authorizes the dapp to enumerate all accounts managed by the snap. Such a deviation from the MM design not only strays from user expectations but also introduces security concerns. It s worth noting that we have also reported this issue to the MM Development Team, as it necessitates a more comprehensive solution.  Recommendation: The dapp should not exert control over the presentation of dialogs to the user. Users must always remain cognizant of every action occurring within the trust module boundary (the snap). There should be no provision for  silent  execution of trust module actions, thereby ensuring transparency and user awareness at all times.  Description  With the request.params.confirm parameter in requests to signTransaction and signAllTransactions the dapp controls if the user is requested confirmation to return the public key. If the dapp sets confirm=false the user will not be informed that the dapp accessed their pubkey information (any account).  Allowing the dapp to control if the user is asked to extract certain (derived) information from the snap is intransparent and may leak sensitive information. Especially in a setting where the snap is gatekeeping access to user specific information.  Examples  Affects all snaps under review.  ../solflare-snap/src/index.js:L23-L36  case 'getPublicKey': {  const { derivationPath, confirm = false } = request.params || {};  assertInput(derivationPath);  assertIsString(derivationPath);  assertIsBoolean(confirm);  const keyPair = await deriveKeyPair(derivationPath);  const pubkey = bs58.encode(keyPair.publicKey);  if (confirm) {  const accepted = await renderGetPublicKey(dappHost, pubkey);  assertConfirmation(accepted);  Recommendation  The snap should strictly enforce user confirmation on the first time the pubkey is requested from an origin. A potentially untrusted dapp (even though origin restricted; a dapp might turn malicious and should therefore be treated as untrusted) should never be able to silently dictate what security measures be enabled with a snap request.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.5 Production Builds Allow Development and Localhost Origins; Snap Does Not Enforce Transport Security   Partially Addressed", "body": "  Resolution  The client has issued the following statement:  Solflare Comment/Background  When implementing RPC access restrictions, our assumption was that MetaMask will always forward valid domains as the origin thus making http\ua789//..solflare.com not possible.  Solflare Resolution  Solflare will implement enforcing Transport Security by removing the option to proceed with the http protocol. Solflare will be deploying a development build snap under a different package name that will allow the localhost origin. This development snap does not need to be in the allow list and we can consume it purely through Flask. We would need to keep both *solflare.com and *solflare.dev origins as these represent our production and staging environments and we would like to retain the possibility of using the production snap with our staging environment. The reason that subdomains are wildcarded is that we are going to make dApp specific changes and deploy them on different subdomains (for example - a swap dApp would not want to have a widget with swap inside of it). We don t consider this a major risk as our processes around DNS management are very strict and limited  Changesets:  solflare-wallet/solflare-snap@749d2b0  solflare-wallet/aptos-snap@eef10b5  solflare-wallet/sui-snap@898295f  Statement from the Assessment Team:  The core tenet here is a robust defense-in-depth strategy that operates on the premise of making no assumptions. We must underscore the criticality of this principle in the context of trust module code. The same level of scrutiny should apply to wildcard origins. The rationale is rooted in the potential vulnerabilities associated with subdomain takeover scenarios, such as Azure instance compromise or DNS hijacking, among others. If an attacker can manipulate a subdomain, leading users to visit a site like malicious.solflare.com, there is a substantial risk of successfully conducting phishing attacks, potentially tricking users into permitting dapp actions on the snap. This not only jeopardizes user funds but also poses a severe threat to your organization s reputation. It falls squarely on your shoulders to ensure that such scenarios do not materialize through robust security controls.  Recommendation: To mitigate these risks, it is imperative to maintain a curated list of official production hosts and validate requests against these origins. For development builds of the extension, you may consider allowing dev and wildcard hosts. However, when introducing new subdomains, it is crucial to manage them meticulously and refrain from merely deploying new subdomain websites, as this approach can become challenging to control. Ultimately, the decision rests with your organization regarding the level of risk tolerance. Based on experience, we would strongly advise against taking unnecessary risks in this regard.  Description  The snaps RPC access is restricted to certain origins only. However, there is no logic that disables development/test domains from origin checks in production builds.  This means, that, any localhost app is allowed to connect to snap (any port, not hardcoded to snap id; should not allow dev domain). The origin enforcing regex allows non-transport security-enabled connections, i.e. http://wallet.solflare.com is allowed while it should be enforced as https://wallet.solflare.com. Furthermore, the origin check allows potentially insecure subdomains, i.e. https://beta.test.solflare.com. Additionally, invalid domains are allowed as well, i.e. http://..solflare.com  Examples  Solflare Snap  ../solflare-snap/src/index.js:L7-L17  module.exports.onRpcRequest = async ({ origin, request }) => {  if (  !origin ||  !origin.match(/^https?:\\/\\/localhost:[0-9]{1,4}$/) &&  !origin.match(/^https?:\\/\\/(?:\\S+\\.)?solflare\\.com$/) &&  !origin.match(/^https?:\\/\\/(?:\\S+\\.)?solflare\\.dev$/)  ) {  throw new Error('Invalid origin');  Aptos Snap  ../aptos-snap/src/index.js:L6-L15  module.exports.onRpcRequest = async ({ origin, request }) => {  if (  !origin ||  !origin.match(/^https?:\\/\\/localhost:[0-9]{1,4}$/) &&  !origin.match(/^https?:\\/\\/(?:\\S+\\.)?risewallet\\.dev$/)  ) {  throw new Error('Invalid origin');  Sui Snap  ../sui-snap/src/index.js:L8-L17  module.exports.onRpcRequest = async ({ origin, request }) => {  if (  !origin ||  !origin.match(/^https?:\\/\\/localhost:[0-9]{1,4}$/) &&  !origin.match(/^https?:\\/\\/(?:\\S+\\.)?elliwallet\\.dev$/)  ) {  throw new Error('Invalid origin');  Recommendation  Implement logic that removes development/localhost origin from the allow list for production builds. Employ strict checks on the format of provided origin. Do not by default allow all subdomains.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.6 Misleading HTML Entity and Function Name getPrivkey    ", "body": "  Resolution  Addressed by following renaming traces of  private key . Additionally, the client has issued the following statement:  Solflare Comment/Background  The index.html file is used for testing and naming hasn t been updated since the snap was initially developed. This file doesn t compile into the bundle.  Solflare Resolution  Solflare will be renaming the function  Changesets:  solflare-wallet/solflare-snap@aa629cb  solflare-wallet/aptos-snap@e603b9f  solflare-wallet/sui-snap@5cbd097  Description  Several lines in the code refer to a private key while the functionality always returns a public key. I.e. getPrivkey actually calls getPublicKey.  ../solflare-snap/index.html:L17-L21  const getPrivkeyButton = document.querySelector('button.getPrivkey')  connectButton.addEventListener('click', connect)  getPrivkeyButton.addEventListener('click', getPrivkey)  ../solflare-snap/index.html:L30-L50  async function getPrivkey () {  try {  const response = await ethereum.request({  method: 'wallet_invokeSnap',  params: {  snapId,  request: {  method: 'getPublicKey',  params: {  derivationPath: `m/44'/501'/0'/0'`,  confirm: true  })  console.log(response);  } catch (err) {  console.error(err)  alert('Problem happened: ' + err.message || err)  Affects all snaps under review.  Recommendation  Never return or extract a private key from the snap. Rename the variables in code to accurately reflect what type of data is being handled.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.7 Use of Outdated snap.config.json Instead of snap.config.js    ", "body": "  Resolution  Addressed by switching to the snap.config.js. Additionally, the client has issued the following statement:  Solflare Resolution  Solflare will be moving to snap.config.js  Changesets:  solflare-wallet/solflare-snap@5228b37  solflare-wallet/aptos-snap@62ec7b4  solflare-wallet/sui-snap@3420723  Description  According to the MetaMask-CLI Documentation the use of snap.config.json is discouraged and projects should switch to snap.config.js instead.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.8 Inconsistent or Blank Fields in package.json    ", "body": "  Resolution  Addressed by switching to the snap.config.js. Additionally, the client has issued the following statement:  Solflare Resolution  Solflare will be adding missing information  Changesets:  solflare-wallet/solflare-snap@f91a27d  solflare-wallet/aptos-snap@cb61e41  solflare-wallet/sui-snap@9819463  Description  ../solflare-snap/package.json:L2-L13  \"name\": \"@solflare-wallet/solana-snap\",  \"version\": \"1.0.0\",  \"description\": \"\",  \"main\": \"src/index.js\",  \"scripts\": {  \"start\": \"mm-snap build && mm-snap serve\",  \"build\": \"mm-snap build\",  \"deploy\": \"npm run build && npm publish\"  },  \"author\": \"\",  \"license\": \"ISC\",  \"files\": [  Affects all snaps under review.  Recommendation  Provide meta information for bugs, homepage, repository, author (non-blank), description (non-blank).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.9 Consider Using @metamask/detect-provider", "body": "  Resolution  The client has issued the following statement:  Solflare Comment/Background  The index.html file is used for testing and naming hasn t been updated since the snap was initially developed. This file doesn t compile into the bundle.  Solflare Resolution  Since this is just a testing file and we use proper detection mechanisms in the Wallet, Solflare will not implement a fix in the helper file.  Description  Consider using the Metamask provided library @metamask/detect-provider for inpage metamask/flask detection.  ../solflare-snap/index.html:L23-L28  async function connect () {  await ethereum.request({  method: 'wallet_requestSnaps',  params: { [snapId]: {} }  })  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.10 Consider Prefixing RPC Calls With solana_*, sui_*, aptos_*", "body": "  Resolution  The client has issued the following statement:  Solflare Comment/Background  The naming convention we have adopted was based on the structure of snap method invocations. Namely, when a snap invocation is happening, the wallet_invokeSnap method is called with a strictly specified snapId (which we consider scope) and the request method. Our thinking is that prefixing the methods in addition to explicitly stating the snapId might lead to unnecessary duplication.  Solflare Resolution  Solflare will not implement prefixes to both avoid naming duplication and because our wallets share a common architecture/codebase and are scoped by the snapId, we would like to avoid needing to call different methods for different wallets  Description  APIs (Application Programming Interfaces) play a crucial role in modern software development, enabling interoperability and communication between different software components. One often overlooked aspect of API design is the naming of API functions. Poorly named API functions can lead to security vulnerabilities and confusion, compromising user information integrity and security.  Consider prefixing the RPC method handlers with the respective protocols they are serving.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.11 Consider Moving to TypeScript", "body": "  Resolution  The client has issued the following statement:  Solflare Comment/Background  Only comment is that we began developing snaps before there was a way to write them in TypeScript. Also, the initial audit was done on a JavaScript codebase and in the benefit of reducing the gap audit and saving time we opted to stay in JavaScript. We definitely recognize all the TypeScript benefits, especially as most of our stack is written in TypeScript.  Solflare Resolution  It is definitely on our roadmap to migrate to TypeScript.  Description  JavaScript, as a dynamically typed language, lacks the stringent type checks and static analysis that can catch a wide range of programming errors during compile time. While JavaScript is widely adopted due to its simplicity and ubiquity, it poses certain security challenges that could be mitigated by transitioning to TypeScript.  Type-Related Vulnerabilities: JavaScript s loose typing allows developers to perform operations on variables without explicit type declarations. This freedom can inadvertently lead to vulnerabilities such as type coercion attacks, where an attacker manipulates input data to trigger unintended behaviors. TypeScript enforces strong typing, which can catch type-related issues early in the development process, reducing the risk of such vulnerabilities.  Null and Undefined Errors: JavaScript s permissiveness with null and undefined values can lead to runtime errors and crashes. Insecure access to null or undefined properties can open doors to injection attacks, privilege escalation, or application crashes. TypeScript s strict null checks force developers to handle these cases explicitly, minimizing the chances of overlooking critical input validation  Misconfigured Objects and Inheritance: JavaScript s prototype-based inheritance can lead to unforeseen object interactions and security breaches when not carefully managed. TypeScript s class-based object-oriented approach enhances code organization and provides better control over inheritance, reducing the likelihood of unexpected security vulnerabilities.  Reduced Maintenance Complexity: Maintaining a JavaScript codebase can become increasingly complex as a project grows. With TypeScript, the inclusion of type annotations and stricter rules aids in code comprehension, refactoring, and identifying security vulnerabilities. This streamlined development process indirectly contributes to a more secure software ecosystem.  Remember, this project is implementing an extension to the MetaMask Trust Module. As such, we recommend implementing compile time security controls by moving the codebase to TypeScript with strict linting rules enabled.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/solflare-metamask-snaps-solflare-sui-aptos/"}, {"title": "4.1 RPC starkNet_sendTransaction - The User Displayed Message Generated With getSigningTxnText() Is Prone to Markdown/Control Chars Injection From contractCallData    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by rendering untrusted user input with the copyable UI component, preventing markdown injection. Additionally, the client provided the following statement:  restructure dialog ui by using MM copyable field, it can ignore any markdown or tag block  validate send transaction calldata has to be able to convert to bigInt  Description  In the code snippet below, contractCallData is potentially untrusted and may contain Markdown renderable strings or strings containing Control Characters that break the context of the message displayed to the user. This can lead to misrepresenting the transaction data to be signed, which should be avoided.  packages/starknet-snap/src/utils/snapUtils.ts:L163-L195  export function getSigningTxnText(  state: SnapState,  contractAddress: string,  contractFuncName: string,  contractCallData: string[],  senderAddress: string,  maxFee: number.BigNumberish,  network: Network,  ): string {  // Retrieve the ERC-20 token from snap state for confirmation display purpose  const token = getErc20Token(state, contractAddress, network.chainId);  let tokenTransferStr = '';  if (token && contractFuncName === 'transfer') {  try {  let amount = '';  if ([3, 6, 9, 12, 15, 18].includes(token.decimals)) {  amount = convert(contractCallData[1], -1 * token.decimals, 'ether');  } else {  amount = (Number(contractCallData[1]) * Math.pow(10, -1 * token.decimals)).toFixed(token.decimals);  tokenTransferStr = `\\n\\nSender Address: ${senderAddress}\\n\\nRecipient Address: ${contractCallData[0]}\\n\\nAmount(${token.symbol}): ${amount}`;  } catch (err) {  console.error(`getSigningTxnText: error found in amount conversion: ${err}`);  return (  `Contract: ${contractAddress}\\n\\nCall Data: [${contractCallData.join(', ')}]\\n\\nEstimated Gas Fee(ETH): ${convert(  maxFee,  'wei',  'ether',  )}\\n\\nNetwork: ${network.name}` + tokenTransferStr  );  packages/starknet-snap/src/sendTransaction.ts:L60-L80  const signingTxnText = getSigningTxnText(  state,  contractAddress,  contractFuncName,  contractCallData,  senderAddress,  maxFee,  network,  );  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([  heading('Do you want to sign this transaction ?'),  text(`It will be signed with address: ${senderAddress}`),  text(signingTxnText),  ]),  },  });  Please note that we have also reported to the MM Snaps team, that dialogues do not by default hint the origin of the action. We hope this will be addressed in a common way for all snaps in the future,  Recommendation  Validate inputs. Encode data in a safe way to be displayed to the user. Show the original data provided within a pre-text or code-block. Show derived or decoded information (token recipient) as additional information to the user.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.2 Lax Validation Using@starknet::validateAndParseAddress Allows Short Addresses and Does Not Verify Checksums    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by wrapping validateAndParseAddress() with an implicit length check. Additionally, the client provided the following statement:  Add validation on the snap side for address length  Checksum will not implement as some users are going to call the Snap directly without going through the dApp  As per the client s decision, checksummed addresses are not enforced.  Description  Address inputs in RPC calls are validated using @starknet::validateAndParseAddress().  packages/starknet-snap/src/getErc20TokenBalance.ts:L19-L28  try {  validateAndParseAddress(requestParamsObj.tokenAddress);  } catch (err) {  throw new Error(`The given token address is invalid: ${requestParamsObj.tokenAddress}`);  try {  validateAndParseAddress(requestParamsObj.userAddress);  } catch (err) {  throw new Error(`The given user address is invalid: ${requestParamsObj.userAddress}`);  While the message validates the general structure for valid addresses, it does not strictly enforce address length and may silently add padding to the inputs before validation. This can be problematic as it may hide user input errors when a user provides an address that is too short and silently gets left-padded with zeroes. This may unintentionally cause a user to request action on the wrong address without them recognizing it.  ../src/utils/address.ts:L14-L24  export function validateAndParseAddress(address: BigNumberish): string {  assertInRange(address, ZERO, MASK_251, 'Starknet Address');  const result = addAddressPadding(address);  if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {  throw new Error('Invalid Address Format');  return result;  export function validateAndParseAddress(address: BigNumberish): string {  assertInRange(address, ZERO, MASK_251, 'Starknet Address');  const result = addAddressPadding(address);  if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {  throw new Error('Invalid Address Format');  return result;  Recommendation  The exposed Snap API should strictly validate inputs. User input must be provided in a safe canonical form (exact address length, checksum) by the dapp.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.3 RPC starkNet_signMessage - Fails to Display the User Account That Is Used for Signing the Message    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by displaying the signing accounts address with the dialog. All user-provided fields are copyable, preventing any markdown injection. Additionally, the client provided the following statement:  add signer address add bottom of the dialog  We want to note that the origin of the RPC call is not visible in the dialog. However, we recommend addressing this with the MM Snap SDK by generically showing the origin of MM popups with the dialog.  Description  The signing request dialogue does not display the user account that is being used to sign the message. A malicious dapp may pretend to sign a message with one account while issuing an RPC call for a different account.  Note that StarkNet signing requests should implement similar security measures to how MetaMask signing requests work. Being fully transparent on  who signs what , also displaying the origin of the request. This is especially important on multi-dapp snaps to avoid users being tricked into signing transactions they did not intend to sign (wrong signer).  packages/starknet-snap/src/signMessage.ts:L34-L42  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([heading('Do you want to sign this message ?'), text(JSON.stringify(typedDataMessage))]),  },  });  if (!response) return false;  Examples  UI does not show the signing accounts address. Hence, the user cannot be sure what account is used to sign the message.  Recommendation  Show what account is requested to sign a message. Display the origin of the RPC call.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.4 RPC starkNet_signMessage - Inconsistency When Previewing the Signed Message (Markdown Injection)    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by rendering user-provided information with the copyable UI component. Additionally, the client provided the following statement:  restructure dialog ui by using MM copyable field, it can ignore any markdown or tag block  Description  The snap displays an dialogue to the user requesting them to confirm that they want to sign a message when a dapp performs a request to starkNet_signMessage. However, the MetaMask Snaps UI text() component will render Markdown. This means that the message-to-be-signed displayed to the user for approval will be inaccurate if it contains Markdown renderable text.  packages/starknet-snap/src/signMessage.ts:L35-L41  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([heading('Do you want to sign this message ?'), text(JSON.stringify(typedDataMessage))]),  },  });  Examples  {\"a **mykey**\":\"this should not render **markdown** <pre>test</pre><b>bbb</b><strong>strongstrong</strong>[visit oststrom](https://oststrom.com) _ital_\"}  Recommendation  Render signed message contents in a code block or preformatted text blocks.  Note: we ve also reported this to the MetaMask Snaps team to provide further guidance.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.5 UI/AlertView - Unnecessary Use of dangerouslySetInnerHTML    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by not using dangerouslySetInnerHTML. Additionally, the client provided the following statement:  Remove dangerouslySetInnerHTML from UI  Description  AlertView is populated by setting innerHTML instead of the component s value, which would be auto-escaped. This only makes sense if the component is supposed to render HTML. However, the component is never used with HTML as input, and the attribute name text is misleading.  packages/wallet-ui/src/components/ui/atom/Alert/Alert.view.tsx:L11-L36  export function AlertView({ text, variant, ...otherProps }: Props) {  const paragraph = useRef<HTMLParagraphElement | null>(null);  const [isMultiline, setIsMultiline] = useState(false);  useEffect(() => {  if (paragraph.current) {  const height = paragraph.current.offsetHeight;  setIsMultiline(height > 20);  }, []);  return (  <Wrapper isMultiline={isMultiline} variant={variant} {...otherProps}>  <>  {variant === VariantOptions.SUCCESS && <LeftIcon icon={['fas', 'check-circle']} />}  {variant === VariantOptions.INFO && <LeftIcon icon={['fas', 'info-circle']} color={theme.palette.info.dark} />}  {variant === VariantOptions.ERROR && (  <LeftIcon icon={['fas', 'exclamation-circle']} color={theme.palette.error.main} />  )}  {variant === VariantOptions.WARNING && (  <LeftIcon icon={['fas', 'exclamation-triangle']} color={theme.palette.warning.main} />  )}  <Parag ref={paragraph} color={variant} dangerouslySetInnerHTML={{ __html: text }} />  </>  </Wrapper>  );  packages/wallet-ui/src/components/ui/organism/NoFlaskModal/NoFlaskModal.view.tsx:L4-L25  export const NoFlaskModalView = () => {  return (  <Wrapper>  <StarknetLogo />  <Title>You don't have the MetaMask Flask extension</Title>  <DescriptionCentered>  You need to install MetaMask Flask extension in order to use the StarkNet Snap.  <br />  <br />  <AlertView  text=\"Please make sure that the regular MetaMask extension is disabled or use a different browser profile\"  variant=\"warning\"  />  </DescriptionCentered>  <a href=\"https://metamask.io/flask\" target=\"_blank\" rel=\"noreferrer noopener\">  <ConnectButton customIconLeft={<FlaskIcon />} onClick={() => {}}>  Download MetaMask Flask  </ConnectButton>  </a>  </Wrapper>  );  };  Setting HTML from code is risky because it s easy to inadvertently expose users to a cross-site scripting (XSS) attack.  Recommendation  Do not use dangerouslySetInnerHTML unless there is a specific requirement that passed in HTML be rendered. If so, rename the attribute name to html instead of text to set clear expectations regarding how the input is treated. Nevertheless, since the component is not used with HTML input, we recommend removing dangerouslySetInnerHTML altogether.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.6 RPC starkNet_addErc20Token - Should Ask for User Confirmation    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by requesting user confirmation for adding new ERC20 Tokens. Additionally, the client provided the following statement:  Adding confirm dialog with MM copyable field, it can ignore any markdown or tag block  Disable loading frame when user reject the add ec220 token request on UI  Description  The RPC method upserts ERC20 tokens received via RPC without asking the user for confirmation. This would allow a connected dapp to insert/change ERC20 token information anytime. This can even be more problematic when multiple dapps are connected to the StarkNet-Snap (race conditions).  packages/starknet-snap/src/addErc20Token.ts:L30-L47  validateAddErc20TokenParams(requestParamsObj, network);  const erc20Token: Erc20Token = {  address: tokenAddress,  name: tokenName,  symbol: tokenSymbol,  decimals: tokenDecimals,  chainId: network.chainId,  };  await upsertErc20Token(erc20Token, wallet, saveMutex);  console.log(`addErc20Token:\\nerc20Token: ${JSON.stringify(erc20Token)}`);  return erc20Token;  } catch (err) {  console.error(`Problem found: ${err}`);  throw err;  Recommendation  Ask the user for confirmation when changing the snaps state.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.7 getKeysFromAddress - Possible Unchecked Null Dereference When Looking Up Private Key    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by throwing an exception on error. Additionally, the client provided the following statement:  instead of return null, raise err in getKeysFromAddress, caller will catch the exception  Description  getKeysFromAddress() may return null if an invalid address was provided but most callers of the function do not check for the null condition and blindly dereference or unpack the return value causing an exception.  packages/starknet-snap/src/utils/starknetUtils.ts:L453-L455  return null;  };  Examples  packages/starknet-snap/src/signMessage.ts:L44-L46  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);  const signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);  const typedDataSignature = getTypedDataMessageSignature(signerKeyPair, typedDataMessage, signerAddress);  packages/starknet-snap/src/extractPrivateKey.ts:L37  const { privateKey: userPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);  packages/starknet-snap/src/extractPublicKey.ts:L31-L32  const { publicKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);  userPublicKey = publicKey;  packages/starknet-snap/src/sendTransaction.ts:L48-L52  const {  privateKey: senderPrivateKey,  publicKey,  addressIndex,  } = await getKeysFromAddress(keyDeriver, network, state, senderAddress);  packages/starknet-snap/src/signMessage.ts:L44-L45  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);  const signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);  packages/starknet-snap/src/verifySignedMessage.ts:L38  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, verifySignerAddress);  packages/starknet-snap/src/estimateFee.ts:L48-L53  const { privateKey: senderPrivateKey, publicKey } = await getKeysFromAddress(  keyDeriver,  network,  state,  senderAddress,  );  Recommendation  Explicitly check for the null or {} case. Consider returning {} to not allow unpacking followed by an explicit null check.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.8 RPC starkNet_getStoredTransactions - Lax or Missing Input Validation   ", "body": "  Resolution  Won t fix. The client provided the following statement:  not fix, minor impact  We want to note that strict input validation should be performed on all untrusted inputs for read/write and read-only methods. Just because the method is read-only now does not necessarily mean it will stay that way. Leaving untrusted inputs unchecked may lead to more severe security vulnerabilities with a growing codebase in the future.  Description  Potentially untrusted inputs, e.g. addresses received via RPC calls, are not always checked to conform to the StarkNet address format. For example, requestParamsObj.senderAddress is never checked to be a valid StarkNet address.  packages/starknet-snap/src/getStoredTransactions.ts:L18-L26  const transactions = getTransactions(  state,  network.chainId,  requestParamsObj.senderAddress,  requestParamsObj.contractAddress,  requestParamsObj.txnType,  undefined,  minTimeStamp,  );  Recommendation  This method is read-only, and therefore, severity is estimated as Minor. However, it is always suggested to perform strict input validation on all user-provided inputs for read-only and read-write methods.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.9 Disable Debug Log for Production Build    ", "body": "  Resolution  Addressed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by introducing a configurable logger. Additionally, the client provided the following statement:  add custom logger to replace console.log, and log message base on debug level, when debug level is off, it will not log anything  update production CICD pipeline to build project with debug level = off/disabled  There re still some instances of console.log(). However, internal state or full requests are not logged anymore. We would still recommend replacing the remaining console.log calls (e.g. the one in addERC20Token).  Description  Throughout the codebase, there are various places where debug log output is being printed to the console. This should be avoided for production builds.  Examples  packages/starknet-snap/src/index.ts:L45-L46  // Switch statement for methods not requiring state to speed things up a bit  console.log(origin, request);  packages/starknet-snap/src/index.ts:L91-L92  console.log(`${request.method}:\\nrequestParams: ${JSON.stringify(requestParams)}`);  packages/starknet-snap/src/index.ts:L103  console.log(`Snap State:\\n${JSON.stringify(state, null, 2)}`);  Recommendation  Remove the debug output or create a custom log method that allows to enable/disable logging to console.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.10 package.json - Dependecy Mixup    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a as per recommendation. Additionally, the client provided the following statement:  Move development dependencies to package.json::devDependencies  Description  The following dependencies are only used for testing or development purposes and should therefore be listed as devDependencies in package.json, otherwise they may be installed for production builds, too.  https://sinonjs.org/  https://www.chaijs.com/  packages/starknet-snap/package.json:L50  \"chai\": \"^4.3.6\",  packages/starknet-snap/package.json:L53-L54  \"sinon\": \"^13.0.2\",  \"sinon-chai\": \"^3.7.0\",  Recommendation  Move development dependencies to package.json::devDependencies.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.11 package.json - Invalid License    Invalid", "body": "  Resolution  Invalid. Legal clarified that it is perfectly fine to allow MIT+Apache2. Additionally, client provided the following statement:  not fix, choose to stick with dual license  Description  The license field in package.json is invalid.  packages/starknet-snap/package.json:L4  \"license\": \"(Apache-2.0 OR MIT)\",  Recommendation  Update the license field.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.12 RPC starkNet_extractPrivateKey - Should Be Renamed to starkNet_displayPrivateKey  ", "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, the extractPrivateKey is not for display purpose  We want to note that we still encourage changing the method name and return value to explicitly return null in the RPC handler for the sake of good secure coding practices discouraging future devs to return implementing key extraction RPC endpoints that may expose wallet credentials to a linked dapp.  Description  It is recommended to rename starkNet_extractPrivateKey  to starkNet_displayPrivateKey as this more accurately describes what the RPC method is doing.  Also, the way the method handler is implemented makes it appear as if it returns the private key to the RPC origin while the submethod returns null. Consider changing this to an explicit empty return to clearly mark in the outer call that no private key is exposed to the caller. Not to confuse this with how starkNet_extractPublicKey  works which actually returns the pubkey to the RPC caller.  packages/starknet-snap/src/index.ts:L123-L127  case 'starkNet_extractPrivateKey':  apiParams.keyDeriver = await getAddressKeyDeriver(snap);  return extractPrivateKey(apiParams);  ", "labels": ["Consensys", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}]