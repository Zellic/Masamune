[{"title": "5.8 Gateway can call any contract   ", "body": "  Resolution  Comment from the client: That s right Gateway can call any contract, we want to keep it open for any external contract.  Description  The Gateway contract is used as a gateway for meta transactions and batched transactions. It can currently call any contract, while is only intended to call specific contracts in the system that implemented GatewayRecipient interface:  code/src/gateway/Gateway.sol:L280-L292  for (uint256 i = 0; i < data.length; i++) {  require(  to[i] != address(0)  );  // solhint-disable-next-line avoid-low-level-calls  (succeeded,) = to[i].call(abi.encodePacked(data[i], account, sender));  require(  succeeded  );  There are currently no restrictions for to value.  Recommendation  Make sure, only intended contracts can be called by the Gateway : PersonalAccountRegistry, PaymentRegistry, ENSController.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.9 Remove unused code    ", "body": "  Resolution  Comment from the client: Unused code has been removed  Description  In account/AccountController.sol when deploying an account, the function _deployAccount() gets an extra input value which is always 0 and not set in any other method.  Examples  code/src/common/account/AccountController.sol:L24-L38  return _deployAccount(  salt,  );  function _deployAccount(  bytes32 salt,  uint256 value  internal  returns (address)  return address(new Account{salt: salt, value: value}());  Recommendation  It is recommended to remove this value as there are no use cases for it at the moment, however if it is planned to be used in the future, it should be well documented in the code to prevent confusion.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.10 Using ENS subdomains introduces possible privacy issues   ", "body": "  Resolution   Comment from the client: This is a known issue - we also added   Description  Using ENS names by default introduces a privacy issue for users. The current implementation leaks all user addresses and their associated username. This is possibly known issue, however it is worth to mention as part of this audit.  Examples  Here s a sample of already registered addresses on mainnet fetched using Legions:  sbeta> ens listSubdomains name=\"pillar.eth\"  > Subdomains for 'pillar.eth'  > NameHash: '0x5bb02333b1f96385ba28fd63408843cfeee095b32196b718786a56e491e33387'  mrsirio  0x6d2ce500f82e20cdeb733ec0530360d2e761f44d  coinstacker  0x60cc065f860682fb899a385b9af66fe82b412b29  dadang  0x904e88eb2602d947ded5c0c5b84c32109255a5f2  ramaido  0x1ee590464e00780ab1c620de41545e74c0731521  tongkol  0x3cbbf43f7a449d54a71bf97c779186f183d1e9eb  kell  0x3d48c65ddfb5bed5980b40974416b55eceed6fab  sipa  0x944972562ea6a07ee0f77bf6ce89559214347774  joyboy  0x4660b09e45930d5ffaedf36bad4a37705303970b  ryanc  0x0c58b9d8b6bdfcd7fb33ab1ecc6b0db4fa94a7b8  hammad  0xe94bb8ea91bfa791cf632e2353cabb87a93713d6  nicolas  0x12ce0a744ccf8958b6859aff1e85bca797e4f742  timmy2shoes  0xafad99c454d97b0130da64179e1a5a7b516ae225  sergvind  0xd5164fe7b9b1d44dd4eb35ef312ada6bce2878ff  0x7384e49fdf540de561f0dc810cc9ad87e909afbe  0x2e496c59c5a0f525d82cf0402851f361ac879c63  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/11/pillar/"}, {"title": "5.1 Random task execution    ", "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@478e9cd by adding  Description  In a scenario where user takes a flash loan, _parseFLAndExecute() gives the flash loan wrapper contract (FLAaveV2, FLDyDx) the permission to execute functions on behalf of the user s DSProxy. This execution permission is revoked only after the entire recipe execution is finished, which means that in case that any of the external calls along the recipe execution is malicious, it might call executeAction() back and inject any task it wishes (e.g. take user s funds out, drain approved tokens, etc)  Examples  code/contracts/actions/flashloan/FLAaveV2.sol:L105-L136  function executeOperation(  address[] memory _assets,  uint256[] memory _amounts,  uint256[] memory _fees,  address _initiator,  bytes memory _params  ) public returns (bool) {  require(msg.sender == AAVE_LENDING_POOL, ERR_ONLY_AAVE_CALLER);  require(_initiator == address(this), ERR_SAME_CALLER);  (Task memory currTask, address proxy) = abi.decode(_params, (Task, address));  // Send FL amounts to user proxy  for (uint256 i = 0; i < _assets.length; ++i) {  _assets[i].withdrawTokens(proxy, _amounts[i]);  address payable taskExecutor = payable(registry.getAddr(TASK_EXECUTOR_ID));  // call Action execution  IDSProxy(proxy).execute{value: address(this).balance}(  taskExecutor,  abi.encodeWithSelector(CALLBACK_SELECTOR, currTask, bytes32(_amounts[0] + _fees[0]))  );  // return FL  for (uint256 i = 0; i < _assets.length; i++) {  _assets[i].approveToken(address(AAVE_LENDING_POOL), _amounts[i] + _fees[i]);  return true;  Recommendation  A reentrancy guard (mutex) that covers the entire content of FLAaveV2.executeOperation/FLDyDx.callFunction should be used to prevent such attack.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.2 Tokens with more than 18 decimal points will cause issues    ", "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@de22007 by using  Description  It is assumed that the maximum number of decimals for each token is 18. However uncommon, but it is possible to have tokens with more than 18 decimals, as an Example YAMv2 has 24 decimals. This can result in broken code flow and unpredictable outcomes (e.g. an underflow will result with really high rates).  Examples  contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol  function getSellRate(address _srcAddr, address _destAddr, uint _srcAmount, bytes memory) public override view returns (uint rate) {  (rate, ) = KyberNetworkProxyInterface(KYBER_INTERFACE)  .getExpectedRate(IERC20(_srcAddr), IERC20(_destAddr), _srcAmount);  // multiply with decimal difference in src token  rate = rate * (10**(18 - getDecimals(_srcAddr)));  // divide with decimal difference in dest token  rate = rate / (10**(18 - getDecimals(_destAddr)));  code/contracts/views/AaveView.sol : also used in getLoanData()  Recommendation  Make sure the code won t fail in case the token s decimals is more than 18.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.3 Error codes of Compound s Comptroller.enterMarket, Comptroller.exitMarket are not checked    ", "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@7075e49 by reverting in the case the return value is non zero.  Description  Compound s enterMarket/exitMarket functions return an error code instead of reverting in case of failure. DeFi Saver smart contracts never check for the error codes returned from Compound smart contracts, although the code flow might revert due to unavailability of the CTokens, however early on checks for Compound errors are suggested.  Examples  code/contracts/actions/compound/helpers/CompHelper.sol:L26-L37  function enterMarket(address _cTokenAddr) public {  address[] memory markets = new address[](1);  markets[0] = _cTokenAddr;  IComptroller(COMPTROLLER_ADDR).enterMarkets(markets);  /// @notice Exits the Compound market  /// @param _cTokenAddr CToken address of the token  function exitMarket(address _cTokenAddr) public {  IComptroller(COMPTROLLER_ADDR).exitMarket(_cTokenAddr);  Recommendation  Caller contract should revert in case the error code is not 0.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.4 Reversed order of parameters in allowance function call    ", "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@8b5657b by swapping the order of function call parameters.  Description  When trying to pull the maximum amount of tokens from an approver to the allowed spender, the parameters that are used for the allowance function call are not in the same order that is used later in the call to safeTransferFrom.  Examples  code/contracts/utils/TokenUtils.sol:L26-L44  function pullTokens(  address _token,  address _from,  uint256 _amount  ) internal returns (uint256) {  // handle max uint amount  if (_amount == type(uint256).max) {  uint256 allowance = IERC20(_token).allowance(address(this), _from);  uint256 balance = getBalance(_token, _from);  _amount = (balance > allowance) ? allowance : balance;  if (_from != address(0) && _from != address(this) && _token != ETH_ADDR && _amount != 0) {  IERC20(_token).safeTransferFrom(_from, address(this), _amount);  return _amount;  Recommendation  Reverse the order of parameters in allowance function call to fit the order that is in the safeTransferFrom function call.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.5 Full test suite is recommended   Pending", "body": "  Description  The test suite at this stage is not complete and many of the tests fail to execute. For complicated systems such as DeFi Saver, which uses many different modules and interacts with different DeFi protocols, it is crucial to have a full test coverage that includes the edge cases and failed scenarios. Especially this helps with safer future development and upgrading each modules.  As we ve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.  Some issues such as issue 5.4 could be caught by a full coverage test suite.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.6 Kyber getRates code is unclear ", "body": "  Description  In contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol the function names don t reflect their true functionalities, and the code uses some undocumented assumptions.  Examples  getSellRate can be converted into one function to get the rates, which then for buy or sell can swap input and output tokens  getBuyRate uses a 3% slippage that is not documented.  function getSellRate(address _srcAddr, address _destAddr, uint _srcAmount, bytes memory) public override view returns (uint rate) {  (rate, ) = KyberNetworkProxyInterface(KYBER_INTERFACE)  .getExpectedRate(IERC20(_srcAddr), IERC20(_destAddr), _srcAmount);  // multiply with decimal difference in src token  rate = rate * (10**(18 - getDecimals(_srcAddr)));  // divide with decimal difference in dest token  rate = rate / (10**(18 - getDecimals(_destAddr)));  /// @notice Return a rate for which we can buy an amount of tokens  /// @param _srcAddr From token  /// @param _destAddr To token  /// @param _destAmount To amount  /// @return rate Rate  function getBuyRate(address _srcAddr, address _destAddr, uint _destAmount, bytes memory _additionalData) public override view returns (uint rate) {  uint256 srcRate = getSellRate(_destAddr, _srcAddr, _destAmount, _additionalData);  uint256 srcAmount = wmul(srcRate, _destAmount);  rate = getSellRate(_srcAddr, _destAddr, srcAmount, _additionalData);  // increase rate by 3% too account for inaccuracy between sell/buy conversion  rate = rate + (rate / 30);  Recommendation  Refactoring the code to separate getting rate functionality with getSellRate and getBuyRate. Explicitly document any assumptions in the code ( slippage, etc)  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.7 Missing check in IOffchainWrapper.takeOrder implementation ", "body": "  Description  IOffchainWrapper.takeOrder wraps an external call that is supposed to perform a token swap. As for the two different implementations ZeroxWrapper and ScpWrapper this function validates that the destination token balance after the swap is greater than the value before. However, it is not sufficient, and the user-provided minimum amount for swap should be taken in consideration as well. Besides, the external contract should not be trusted upon, and SafeMath should be used for the subtraction operation.  Examples  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L42-L50  uint256 tokensBefore = _exData.destAddr.getBalance(address(this));  (success, ) = _exData.offchainData.exchangeAddr.call{value: _exData.offchainData.protocolFee}(_exData.offchainData.callData);  uint256 tokensSwaped = 0;  if (success) {  // get the current balance of the swaped tokens  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  require(tokensSwaped > 0, ERR_TOKENS_SWAPED_ZERO);  code/contracts/exchangeV3/offchainWrappersV3/ScpWrapper.sol:L43-L51  uint256 tokensBefore = _exData.destAddr.getBalance(address(this));  (success, ) = _exData.offchainData.exchangeAddr.call{value: _exData.offchainData.protocolFee}(_exData.offchainData.callData);  uint256 tokensSwaped = 0;  if (success) {  // get the current balance of the swaped tokens  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  require(tokensSwaped > 0, ERR_TOKENS_SWAPED_ZERO);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.8 Unused code present in the codebase ", "body": "  Resolution   Some of the unused code were removed in   DecenterApps/defisaver-v3-contracts@61b0c09.  Description  There are a few instances of unused code (dead code) in the code base, that is suggested to be removed .  Examples  DFSExchange.sol contract is not used  /contracts/utils/ZrxAllowlist.sol these functions are not used in the codebase:  nonPayableAddrs mapping addNonPayableAddr() removeNonPayableAddr() isNonPayableAddr()  DSProxy.execute(bytes memory _code, bytes memory _data) is not intended to used.  There might be more instances of unused code in the codebase.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.9 Return values not used for DFSExchangeCore.onChainSwap ", "body": "  Description  Return values from DFSExchangeCore.onChainSwap are not used.  Examples  code/contracts/exchangeV3/DFSExchangeCore.sol:L37-L73  function _sell(ExchangeData memory exData) internal returns (address, uint256) {  uint256 amountWithoutFee = exData.srcAmount;  address wrapper = exData.offchainData.wrapper;  bool offChainSwapSuccess;  uint256 destBalanceBefore = exData.destAddr.getBalance(address(this));  // Takes DFS exchange fee  exData.srcAmount -= getFee(  exData.srcAmount,  exData.user,  exData.srcAddr,  exData.dfsFeeDivider  );  // Try 0x first and then fallback on specific wrapper  if (exData.offchainData.price > 0) {  (offChainSwapSuccess, ) = offChainSwap(exData, ExchangeActionType.SELL);  // fallback to desired wrapper if 0x failed  if (!offChainSwapSuccess) {  onChainSwap(exData, ExchangeActionType.SELL);  wrapper = exData.wrapper;  uint256 destBalanceAfter = exData.destAddr.getBalance(address(this));  uint256 amountBought = sub(destBalanceAfter, destBalanceBefore);  // check slippage  require(amountBought >= wmul(exData.minPrice, exData.srcAmount), ERR_SLIPPAGE_HIT);  // revert back exData changes to keep it consistent  exData.srcAmount = amountWithoutFee;  return (wrapper, amountBought);  code/contracts/exchangeV3/DFSExchangeCore.sol:L79-L117  function _buy(ExchangeData memory exData) internal returns (address, uint256) {  require(exData.destAmount != 0, ERR_DEST_AMOUNT_MISSING);  uint256 amountWithoutFee = exData.srcAmount;  address wrapper = exData.offchainData.wrapper;  bool offChainSwapSuccess;  uint256 destBalanceBefore = exData.destAddr.getBalance(address(this));  // Takes DFS exchange fee  exData.srcAmount -= getFee(  exData.srcAmount,  exData.user,  exData.srcAddr,  exData.dfsFeeDivider  );  // Try 0x first and then fallback on specific wrapper  if (exData.offchainData.price > 0) {  (offChainSwapSuccess, ) = offChainSwap(exData, ExchangeActionType.BUY);  // fallback to desired wrapper if 0x failed  if (!offChainSwapSuccess) {  onChainSwap(exData, ExchangeActionType.BUY);  wrapper = exData.wrapper;  uint256 destBalanceAfter = exData.destAddr.getBalance(address(this));  uint256 amountBought = sub(destBalanceAfter, destBalanceBefore);  // check slippage  require(amountBought >= exData.destAmount, ERR_SLIPPAGE_HIT);  // revert back exData changes to keep it consistent  exData.srcAmount = amountWithoutFee;  return (wrapper, amountBought);  Recommendation  The return value can be used for verification of the swap or used in the event data.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.10 Return value is not used for TokenUtils.withdrawTokens    ", "body": "  Resolution   Fixed in   DecenterApps/defisaver-v3-contracts@37dabff by storing the return value locally and use its value throughout the execution.  Description  The return value of TokenUtils.withdrawTokens which represents the actual amount of tokens that were transferred is never used throughout the repository. This might cause discrepancy in the case where the original value of _amount was type(uint256).max.  Examples  code/contracts/actions/aave/AaveBorrow.sol:L70-L97  function _borrow(  address _market,  address _tokenAddr,  uint256 _amount,  uint256 _rateMode,  address _to,  address _onBehalf  ) internal returns (uint256) {  ILendingPoolV2 lendingPool = getLendingPool(_market);  // defaults to onBehalf of proxy  if (_onBehalf == address(0)) {  _onBehalf = address(this);  lendingPool.borrow(_tokenAddr, _amount, _rateMode, AAVE_REFERRAL_CODE, _onBehalf);  _tokenAddr.withdrawTokens(_to, _amount);  logger.Log(  address(this),  msg.sender,  \"AaveBorrow\",  abi.encode(_market, _tokenAddr, _amount, _rateMode, _to, _onBehalf)  );  return _amount;  code/contracts/utils/TokenUtils.sol:L46-L53  function withdrawTokens(  address _token,  address _to,  uint256 _amount  ) internal returns (uint256) {  if (_amount == type(uint256).max) {  _amount = getBalance(_token, address(this));  Recommendation  The return value can be used to validate the withdrawal or used in the event emitted.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.11 Missing access control for DefiSaverLogger.Log", "body": "  Description  DefiSaverLogger is used as a logging aggregator within the entire dapp, but anyone can create logs.  Examples  code/contracts/utils/DefisaverLogger.sol:L14-L21  function Log(  address _contract,  address _caller,  string memory _logName,  bytes memory _data  ) public {  emit LogEvent(_contract, _caller, _logName, _data);  6 Recommendations  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "6.1 Use a single file for all system-wide constants", "body": "  Description  There are many addresses and constants using in the system. It is suggested to put the most used ones in one file (e.g. constants.sol and use inheritance to access these values. This will help with the readability and easier maintenance for future changes. As some of these hardcoded values are admin addresses, this also helps with any possible incident response.  Examples  Logger:  DFSRegistry  TaskExecutor  ActionBase  DefisaverLogger public constant logger = DefisaverLogger(  0x5c55B921f590a89C1Ebe84dF170E655a82b62126  );  Admin Vault:  AdminAuth  AdminVault public constant adminVault = AdminVault(0xCCf3d848e08b94478Ed8f46fFead3008faF581fD);  REGISTRY_ADDR  SubscriptionProxy  StrategyExecutor  TaskExecutor  ActionBase  address public constant REGISTRY_ADDR = 0xB0e1682D17A96E8551191c089673346dF7e1D467;  Any other constant in the system also can be moved to this contract.  Recommendation  Use constants.sol and import this file in the contracts that require access to these values. This is just a recommendation, as discussed with the team, on some use cases this might result in higher gas usage on deployment.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "6.2 Code quality & Styling", "body": "  Description  Here are some examples that the code style does not follow the best practices:  Examples  Public/external function names should not be prefixed with _  code/contracts/core/TaskExecutor.sol:L56  function _executeActionsFromFL(Task memory _currTask, bytes32 _flAmount) public payable {  Function parameters are being overriden  code/contracts/exchangeV3/DFSExchange.sol:L24-L37  function sell(ExchangeData memory exData, address payable _user) public payable   {  exData.dfsFeeDivider = SERVICE_FEE;  exData.user = _user;  // Perform the exchange  (address wrapper, uint destAmount) = _sell(exData);  // send back any leftover ether or tokens  sendLeftover(exData.srcAddr, exData.destAddr, _user);  // log the event  logger.Log(address(this), msg.sender, \"ExchangeSell\", abi.encode(wrapper, exData.srcAddr, exData.destAddr, exData.srcAmount, destAmount));  MAX_SERVICE_FEE should be MIN_SERVICE_FEE  code/contracts/utils/Discount.sol:L28-L33  function setServiceFee(address _user, uint256 _fee) public {  require(msg.sender == owner, \"Only owner\");  require(_fee >= MAX_SERVICE_FEE || _fee == 0, \"Wrong fee value\");  serviceFees[_user] = CustomServiceFee({active: true, amount: _fee});  Functions with a get prefix should not modify state  code/contracts/exchangeV3/DFSExchangeCore.sol:L182-L206  function getFee(  uint256 _amount,  address _user,  address _token,  uint256 _dfsFeeDivider  ) internal returns (uint256 feeAmount) {  if (_dfsFeeDivider != 0 && Discount(DISCOUNT_ADDRESS).isCustomFeeSet(_user)) {  _dfsFeeDivider = Discount(DISCOUNT_ADDRESS).getCustomServiceFee(_user);  if (_dfsFeeDivider == 0) {  feeAmount = 0;  } else {  feeAmount = _amount / _dfsFeeDivider;  // fee can't go over 10% of the whole amount  if (feeAmount > (_amount / 10)) {  feeAmount = _amount / 10;  address walletAddr = feeRecipient.getFeeAddr();  _token.withdrawTokens(walletAddr, feeAmount);  Protocol fee value should be validated against msg.value and not against contract s balance  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L25-L31  function takeOrder(  ExchangeData memory _exData,  ExchangeActionType _type  ) override public payable returns (bool success, uint256) {  // check that contract have enough balance for exchange and protocol fee  require(_exData.srcAddr.getBalance(address(this)) >= _exData.srcAmount, ERR_SRC_AMOUNT);  require(TokenUtils.ETH_ADDR.getBalance(address(this)) >= _exData.offchainData.protocolFee, ERR_PROTOCOL_FEE);  Remove deprecation warning (originated in OpenZeppelin s implementation) in comment, as the issue has been solved  code/contracts/utils/SafeERC20.sol:L33-L44  /**  @dev Deprecated. This function has issues similar to the ones found in  {ERC20-approve}, and its usage is discouraged.  /  function safeApprove(  IERC20 token,  address spender,  uint256 value  ) internal {  _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, 0));  _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, spender, value));  Typo RECIPIE_FEE instead of RECIPE_FEE  code/contracts/actions/exchange/DfsSell.sol:L15  uint internal constant RECIPIE_FEE = 400;  Code duplication : sendLeftOver is identical both in UniswapWrapperV3 and in KyberWrapperV3, and thus can be shared in a base class.  code/contracts/exchangeV3/wrappersV3/KyberWrapperV3.sol:L127-L133  function sendLeftOver(address _srcAddr) internal {  msg.sender.transfer(address(this).balance);  if (_srcAddr != KYBER_ETH_ADDRESS) {  IERC20(_srcAddr).safeTransfer(msg.sender, IERC20(_srcAddr).balanceOf(address(this)));  Code duplication : sliceUint function is identical both in DFSExchangeHelper and in DFSPrices  DFSPricesV3.getBestPrice, DFSPricesV3.getExpectedRate should be view functions  Fix the code comments from User borrows tokens to to User borrows tokens from  code/contracts/actions/aave/AaveBorrow.sol:L63-L77  /// @notice User borrows tokens to the Aave protocol  /// @param _market Address provider for specific market  /// @param _tokenAddr The address of the token to be borrowed  /// @param _amount Amount of tokens to be borrowed  /// @param _rateMode Send 1 for stable rate and 2 for variable  /// @param _to The address we are sending the borrowed tokens to  /// @param _onBehalf From what user we are borrow the tokens, defaults to proxy  function _borrow(  address _market,  address _tokenAddr,  uint256 _amount,  uint256 _rateMode,  address _to,  address _onBehalf  ) internal returns (uint256) {  code/contracts/actions/compound/CompBorrow.sol:L51-L59  /// @notice User borrows tokens to the Compound protocol  /// @param _cTokenAddr Address of the cToken we are borrowing  /// @param _amount Amount of tokens to be borrowed  /// @param _to The address we are sending the borrowed tokens to  function _borrow(  address _cTokenAddr,  uint256 _amount,  address _to  ) internal returns (uint256) {  IExchangeV3.sell, IExchangeV3.buy should not be payable  TaskExecutor._executeAction should not forward contract s balance within the IDSProxy.execute call, as the funds are being sent to the same contract.  code/contracts/core/TaskExecutor.sol:L90-L105  function _executeAction(  Task memory _currTask,  uint256 _index,  bytes32[] memory _returnValues  ) internal returns (bytes32 response) {  response = IDSProxy(address(this)).execute{value: address(this).balance}(  registry.getAddr(_currTask.actionIds[_index]),  abi.encodeWithSignature(  \"executeAction(bytes[],bytes[],uint8[],bytes32[])\",  _currTask.callData[_index],  _currTask.subData[_index],  _currTask.paramMapping[_index],  _returnValues  );  Unsafe arithmetic operations  code/contracts/actions/compound/CompClaim.sol:L73  uint256 compClaimed = compBalanceAfter - compBalanceBefore;  code/contracts/actions/compound/CompWithdraw.sol:L84  _amount = tokenBalanceAfter - tokenBalanceBefore;  code/contracts/actions/uniswap/UniSupply.sol:L82-L83  _uniData.tokenA.withdrawTokens(_uniData.to, (_uniData.amountADesired - amountA));  _uniData.tokenB.withdrawTokens(_uniData.to, (_uniData.amountBDesired - amountB));  code/contracts/actions/flashloan/FLAaveV2.sol:L125-L133  IDSProxy(proxy).execute{value: address(this).balance}(  taskExecutor,  abi.encodeWithSelector(CALLBACK_SELECTOR, currTask, bytes32(_amounts[0] + _fees[0]))  );  // return FL  for (uint256 i = 0; i < _assets.length; i++) {  _assets[i].approveToken(address(AAVE_LENDING_POOL), _amounts[i] + _fees[i]);  code/contracts/exchangeV3/DFSExchangeCore.sol:L45  exData.srcAmount -= getFee(  code/contracts/exchangeV3/offchainWrappersV3/ZeroxWrapper.sol:L48  tokensSwaped = _exData.destAddr.getBalance(address(this)) - tokensBefore;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "6.3 Gas optimization", "body": "  Description  Use address(this) instead of external call for registry when possible.  Examples  code/contracts/actions/flashloan/FLAaveV2.sol:L82-L102  function _flAaveV2(FLAaveV2Data memory _flData, bytes memory _params) internal returns (uint) {  ILendingPoolV2(AAVE_LENDING_POOL).flashLoan(  payable(registry.getAddr(FL_AAVE_V2_ID)),  _flData.tokens,  _flData.amounts,  _flData.modes,  _flData.onBehalfOf,  _params,  AAVE_REFERRAL_CODE  );  logger.Log(  address(this),  msg.sender,  \"FLAaveV2\",  abi.encode(_flData.tokens, _flData.amounts, _flData.modes, _flData.onBehalfOf)  );  return _flData.amounts[0];  code/contracts/actions/flashloan/dydx/FLDyDx.sol:L76-L107  function _flDyDx(  uint256 _amount,  address _token,  bytes memory _data  ) internal returns (uint256) {  address payable receiver = payable(registry.getAddr(FL_DYDX_ID));  ISoloMargin solo = ISoloMargin(SOLO_MARGIN_ADDRESS);  // Get marketId from token address  uint256 marketId = _getMarketIdFromTokenAddress(SOLO_MARGIN_ADDRESS, _token);  uint256 repayAmount = _getRepaymentAmountInternal(_amount);  IERC20(_token).safeApprove(SOLO_MARGIN_ADDRESS, repayAmount);  Actions.ActionArgs[] memory operations = new Actions.ActionArgs[](3);  operations[0] = _getWithdrawAction(marketId, _amount, receiver);  operations[1] = _getCallAction(_data, receiver);  operations[2] = _getDepositAction(marketId, repayAmount, address(this));  Account.Info[] memory accountInfos = new Account.Info[](1);  accountInfos[0] = _getAccountInfo();  solo.operate(accountInfos, operations);  logger.Log(address(this), msg.sender, \"FLDyDx\", abi.encode(_amount, _token));  return _amount;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/defi-saver/"}, {"title": "5.1 Every node gets a full validator s bounty    ", "body": "  Resolution  This issue is addressed in Bug/skale 3273 formula fix 435 and SKALE-3273 Fix BountyV2 populating error 438.  The main change is related to how bounties are calculated for each validator. Below are a few notes on these pull requests:  nodesByValidator mapping is no longer used in the codebase and the non-zero values are deleted when calculateBounty() is called for a specific validator. The mapping is kept in the code for compatible storage layout in upgradable proxies.  Some functions such as populate() was developed for the transition to the upgraded contracts (rewrite _effectiveDelegatedSum values based on the new calculation formula). This function is not part of this review and will be removed in the future updates.  Unlike the old architecture, nodesByValidator[validatorId] is no longer used within the system to calculate _effectiveDelegatedSum and bounties. This is replaced by using overall staked amount and duration.  If a validator does not claim their bounty during a month, it is considered as a misbehave and her bounty goes to the bounty pool for the next month.  Description  To get the bounty, every node calls the getBounty function of the SkaleManager contract. This function can be called once per month. The size of the bounty is defined in the BountyV2 contract in the _calculateMaximumBountyAmount function:  code/contracts/BountyV2.sol:L213-L221  return epochPoolSize  .add(_bountyWasPaidInCurrentEpoch)  .mul(  delegationController.getAndUpdateEffectiveDelegatedToValidator(  nodes.getValidatorId(nodeIndex),  currentMonth  .div(effectiveDelegatedSum);  The problem is that this amount actually represents the amount that should be paid to the validator of that node. But each node will get this amount. Additionally, the amount of validator s bounty should also correspond to the number of active nodes, while this formula only uses the amount of delegated funds.  Recommendation  Every node should get only their parts of the bounty.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.2 A node exit prevents some other nodes from exiting for some period   Pending", "body": "  Resolution  Skale team s comment:  Description  When a node wants to exit, the nodeExit function should be called as many times, as there are schains in the node. Each time one schain is getting removed from the node. During every call, all the active schains are getting frozen for 12 hours.  code/contracts/NodeRotation.sol:L84-L105  function freezeSchains(uint nodeIndex) external allow(\"SkaleManager\") {  SchainsInternal schainsInternal = SchainsInternal(contractManager.getContract(\"SchainsInternal\"));  bytes32[] memory schains = schainsInternal.getActiveSchains(nodeIndex);  for (uint i = 0; i < schains.length; i++) {  Rotation memory rotation = rotations[schains[i]];  if (rotation.nodeIndex == nodeIndex && now < rotation.freezeUntil) {  continue;  string memory schainName = schainsInternal.getSchainName(schains[i]);  string memory revertMessage = \"Node cannot rotate on Schain \";  revertMessage = revertMessage.strConcat(schainName);  revertMessage = revertMessage.strConcat(\", occupied by Node \");  revertMessage = revertMessage.strConcat(rotation.nodeIndex.uint2str());  string memory dkgRevert = \"DKG process did not finish on schain \";  ISkaleDKG skaleDKG = ISkaleDKG(contractManager.getContract(\"SkaleDKG\"));  require(  skaleDKG.isLastDKGSuccessful(keccak256(abi.encodePacked(schainName))),  dkgRevert.strConcat(schainName));  require(rotation.freezeUntil < now, revertMessage);  _startRotation(schains[i], nodeIndex);  Because of that, no other node that is running one of these schains can exit during that period. In the worst-case scenario, one malicious node has 128 Schains and calls nodeExit every 12 hours. That means that some nodes will not be able to exit for 64 days.  Recommendation  Make node exiting process less synchronous.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.3 Removing a node require multiple transactions and may be very expensive   Pending", "body": "  Resolution  Skale team s comment:  Description  When removing a node from the network, the owner should redistribute all the schains that are currently on that node to the other nodes. To do so, the validator should call the nodeExit function of the SkaleManager contract. In this function, only one schain is going to be removed from the node. So the node would have to call the nodeExit function as many times as there are schains in the node. Every call iterates over every potential node that can be used as a replacement (like in https://github.com/ConsenSys/skale-network-audit-2020-10/issues/3).  In addition to that, the first call will iterate over all schains in the node, make 4 SSTORE operations and external calls for each schain:  code/contracts/NodeRotation.sol:L204-L210  function _startRotation(bytes32 schainIndex, uint nodeIndex) private {  ConstantsHolder constants = ConstantsHolder(contractManager.getContract(\"ConstantsHolder\"));  rotations[schainIndex].nodeIndex = nodeIndex;  rotations[schainIndex].newNodeIndex = nodeIndex;  rotations[schainIndex].freezeUntil = now.add(constants.rotationDelay());  waitForNewNode[schainIndex] = true;  This may hit the block gas limit even easier than issue 5.4.  If the first transaction does not hit the block s gas limit, the maximum price of deleting a node would be BLOCK_GAS_COST * 128. At the moment, it s around $50,000.  Recommendation  Optimize the process of deleting a node, so it can t hit the gas limit in one transaction, and the overall price should be cheaper.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.4 Adding a new schain may potentially hit the gas limit   Pending", "body": "  Resolution  Skale team s comment:  Description  When adding a new schain, a group of random 16 nodes is randomly selected to run that schain. In order to do so, the _generateGroup function iterates over all the nodes that can be used for that purpose:  code/contracts/SchainsInternal.sol:L522-L541  function _generateGroup(bytes32 schainId, uint numberOfNodes) private returns (uint[] memory nodesInGroup) {  Nodes nodes = Nodes(contractManager.getContract(\"Nodes\"));  uint8 space = schains[schainId].partOfNode;  nodesInGroup = new uint[](numberOfNodes);  uint[] memory possibleNodes = isEnoughNodes(schainId);  require(possibleNodes.length >= nodesInGroup.length, \"Not enough nodes to create Schain\");  uint ignoringTail = 0;  uint random = uint(keccak256(abi.encodePacked(uint(blockhash(block.number.sub(1))), schainId)));  for (uint i = 0; i < nodesInGroup.length; ++i) {  uint index = random % (possibleNodes.length.sub(ignoringTail));  uint node = possibleNodes[index];  nodesInGroup[i] = node;  _swap(possibleNodes, index, possibleNodes.length.sub(ignoringTail).sub(1));  ++ignoringTail;  _exceptionsForGroups[schainId][node] = true;  addSchainForNode(node, schainId);  require(nodes.removeSpaceFromNode(node, space), \"Could not remove space from Node\");  If the total number of nodes exceeds around a few thousands, adding a schain may hit the block gas limit.  Recommendation  Avoid iterating over all nodes when selecting a random node for a schain.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.5 Typos ", "body": "  Description  There are a few typos in the contract source code. This could result in unforeseeable issues in the future development cycles.  Examples  succesful instead of successful:  code/contracts/SkaleDKG.sol:L77-L78  mapping(bytes32 => uint) public lastSuccesfulDKG;  code/contracts/SkaleDKG.sol:L372-L373  _setSuccesfulDKG(schainId);  and many other instances of succesful through out the code.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.6 Redundant Checks in some flows", "body": "  Description  The workflows in the Skale network are complicated and multi layers (multiple calls to different modules). Some checks are done in this process that are redundant and can be removed, based on the current code and the workflow.  Examples  An example of this redundancy, is when completing a node exit procedure. completeExit() checks if the node status is leaving and if so continues:  code/contracts/Nodes.sol:L309-L311  require(isNodeLeaving(nodeIndex), \"Node is not Leaving\");  _setNodeLeft(nodeIndex);  However, in _setNodeLeft() it has an if clause for the status being Active, which will never be true.  code/contracts/Nodes.sol:L795-L803  function _setNodeLeft(uint nodeIndex) private {  nodesIPCheck[nodes[nodeIndex].ip] = false;  nodesNameCheck[keccak256(abi.encodePacked(nodes[nodeIndex].name))] = false;  delete nodesNameToIndex[keccak256(abi.encodePacked(nodes[nodeIndex].name))];  if (nodes[nodeIndex].status == NodeStatus.Active) {  numberOfActiveNodes--;  } else {  numberOfLeavingNodes--;  Recommendation  To properly check the code flows for unreachable code and remove redundant checks.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.7 Presence of empty function   ", "body": "  Resolution   Implemented in   Bug/skale 3273 formula fix 435.  Description  estimateBounty() is declared but neither implemented nor used in any part of the current code base.  Examples  code/contracts/BountyV2.sol:L142-L159  function estimateBounty(uint /* nodeIndex */) external pure returns (uint) {  revert(\"Not implemented\");  // ConstantsHolder constantsHolder = ConstantsHolder(contractManager.getContract(\"ConstantsHolder\"));  // Nodes nodes = Nodes(contractManager.getContract(\"Nodes\"));  // TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  // uint stagePoolSize;  // uint nextStage;  // (stagePoolSize, nextStage) = _getEpochPool(timeHelpers.getCurrentMonth(), timeHelpers, constantsHolder);  // return _calculateMaximumBountyAmount(  //     stagePoolSize,  //     nextStage.sub(1),  //     nodeIndex,  //     constantsHolder,  //     nodes  // );  Recommendation  It is suggested to remove dead code from the code base, or fully implement it before the next step.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.8 Presence of TODO tags in the codebase", "body": "  Description  A few TODO tags are present in the codebase.  Examples  code/contracts/SchainsInternal.sol:L153-L160  // TODO:  // optimize  for (uint i = 0; i + 1 < schainsAtSystem.length; i++) {  if (schainsAtSystem[i] == schainId) {  schainsAtSystem[i] = schainsAtSystem[schainsAtSystem.length.sub(1)];  break;  code/contracts/SchainsInternal.sol:L294-L301  /**  @dev Checks whether schain name is available.  TODO Need to delete - copy of web3.utils.soliditySha3  /  function isSchainNameAvailable(string calldata name) external view returns (bool) {  bytes32 schainId = keccak256(abi.encodePacked(name));  return schains[schainId].owner == address(0) && !usedSchainNames[schainId];  code/contracts/Nodes.sol:L81-L89  // TODO: move outside the contract  struct NodeCreationParams {  string name;  bytes4 ip;  bytes4 publicIp;  uint16 port;  bytes32[2] publicKey;  uint16 nonce;  And a few others in test scripts.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "6.1 Collaterals are not guaranteed to be returned after a batch is cancelled    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising#162  Description  When traders open buy orders, they also transfer collateral tokens to the market maker contract. If the current batch is going to be cancelled, there is a chance that these collateral tokens will not be returned to the traders.  Examples  If a current collateralsToBeClaimed value is zero on a batch initialization and in this new batch only buy orders are submitted, collateralsToBeClaimed value will still stay zero.  At the same time if in Tap contract tapped amount was bigger than _maximumWithdrawal() on batch initialisation, _maximumWithdrawal() will most likely increase when the traders transfer new collateral tokens with the buy orders. And a beneficiary will be able to withdraw part of these tokens. Because of that, there might be not enough tokens to withdraw by the traders if the batch is cancelled.  It s partially mitigated by having floor value in Tap contract, but if there are more collateral tokens in the batch than floor, the issue is still valid.  Recommendation  Ensure that tapped is not bigger than _maximumWithdrawal()  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.2 Fees can be changed during the batch    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@0941f53 by storing current fee in meta batch.  Description  Shareholders can vote to change the fees. For buy orders, fees are withdrawn immediately when order is submitted and the only risk is frontrunning by the shareholder s voting contract.  For sell orders, fees are withdrawn when a trader claims an order and withdraws funds in _claimSellOrder  function:  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L790-L792  if (fee > 0) {  reserve.transfer(_collateral, beneficiary, fee);  Fees can be changed between opening order and claiming this order which makes the fees unpredictable.  Recommendation  Fees for an order should not be updated during its lifetime.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.3 Bancor formula should not be updated during the batch    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@a8c2e21 by storing a ref to the Formula with the meta batch.  Description  Shareholders can vote to change the bancor formula contract. That can make a price in the current batch unpredictable.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L212-L216  function updateFormula(IBancorFormula _formula) external auth(UPDATE_FORMULA_ROLE) {  require(isContract(_formula), ERROR_CONTRACT_IS_EOA);  _updateFormula(_formula);  Recommendation  Bancor formula update should be executed in the next batch or with a timelock that is greater than batch duration.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.4 Maximum slippage shouldn t be updated for the current batch    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@aa4f03e by storing slippage with the batch.  Description  When anyone submits a new order, the batch price is updated and it s checked whether the price slippage is acceptable. The problem is that the maximum slippage can be updated during the batch and traders cannot be sure that price is limited as they initially expected.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L487-L489  function _slippageIsValid(Batch storage _batch, address _collateral) internal view returns (bool) {  uint256 staticPricePPM = _staticPricePPM(_batch.supply, _batch.balance, _batch.reserveRatio);  uint256 maximumSlippage = collaterals[_collateral].slippage;  Additionally, if a maximum slippage is updated to a lower value, some of the orders that should lower the current slippage will also revert.  Recommendation  Save a slippage value on batch initialization and use it during the current batch.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.5 AragonFundraisingController - an untapped address in toReset can block attempts of opening Trading after presale    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@9451147 by checking if token is tapped. Gas consumption is increased due to external call to Tap to check if token is actually tapped. The number of tokens to be reset is capped.  Description  AragonFundraisingController can be initialized with a list of token addresses _toReset that are to be reset when trading opens after the presale. These addresses are supposed to be addresses of tapped tokens. However, the list needs to be known when initializing the contract but the tapped tokens are added after initialization when calling addCollateralToken (and tapped with _rate>0). This can lead to an inconsistency that blocks openTrading.  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L99-L102  for (uint256 i = 0; i < _toReset.length; i++) {  require(_tokenIsContractOrETH(_toReset[i]), ERROR_INVALID_TOKENS);  toReset.push(_toReset[i]);  In case a token address makes it into the list of toReset tokens that is not tapped it will be impossible to openTrading as tap.resetTappedToken(toReset[i]); throws for untapped tokens. According to the permission setup in FundraisingMultisigTemplate only Controller can call Marketmaker.open  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L163-L169  function openTrading() external auth(OPEN_TRADING_ROLE) {  for (uint256 i = 0; i < toReset.length; i++) {  tap.resetTappedToken(toReset[i]);  marketMaker.open();  Recommendation  Instead of initializing the Controller with a list of tapped tokens to be reset when trading opens, add a flag to addCollateralToken to indicate that the token should be reset when calling openTrading, making sure only tapped tokens are added to this list. This also allows adding tapped tokens that are to be reset at a later point in time.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.6 Tap payments inconsistency    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising#162  Description  Every time project managers want to withdraw tapped funds, the maximum amount of withdrawable funds is calculated in tap._maximumWithdrawal function. The method ensures that project managers can only withdraw unlocked funds (balance exceeding the collaterals minimum comprised of the collaterals configured floor including the minimum tokens to hold) even though their allowance might be higher.  if there are no unlocked funds available, the maximum withdrawal is zero (balance <= minimum).  if there are unlocked funds available (balance > minimum) and the allowance (tapped) would result in a balance >= minimum, the maximum withdrawal amount is the calculated allowance tapped.  if there are unlocked funds available (balance > minimum) and the allowance (tapped) would result in a balance < minimum, the maximum withdrawal amount tapped is capped to balance - minimum to ensure that the remaining collateral balance is at least at the minimum and not below.  This means that in the case of (3) if there are not enough funds to withdraw tapped(time*tap_rate) amount of tokens, it gets truncated and only a part of tapped tokens gets withdrawn.  code/apps/tap/contracts/Tap.sol:L239-L255  function _maximumWithdrawal(address _token) internal view returns (uint256) {  uint256 toBeClaimed = controller.collateralsToBeClaimed(_token);  uint256 floor = floors[_token];  uint256 minimum = toBeClaimed.add(floor);  uint256 balance = _token == ETH ? address(reserve).balance : ERC20(_token).staticBalanceOf(reserve);  uint256 tapped = (_currentBatchId().sub(lastWithdrawals[_token])).mul(rates[_token]);  if (minimum >= balance) {  return 0;  if (balance >= tapped.add(minimum)) {  return tapped;  return balance.sub(minimum);  The problem is that the remaining tokens (tapped - capped_tapped) cannot be claimed afterward and tapped value is reset to zero.  Remediation  In case the maximum withdrawal amount gets capped, the information about the remaining tokens that the project team should have been able to withdraw should be kept to allow them to withdraw the tokens at a later point in time when there are enough funds for it.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.7 [New] Tapped collaterals can be bought by traders   ", "body": "  Resolution  This behaviour is intentional and if there is not a lot of funds in the pool, shareholders have a priority to buy tokens even if these tokens can already be withdrawn by the beneficiary. It is done in order to protect shareholders in case if the project is dying and running out of funds. The downside of this behaviour is that it creates an additional incentive for the beneficiary to withdraw tapped tokens as soon and as often as possible which creates a race condition.  Description  When a trader submits a sell order, _openSellOrder() function checks that there are enough tokens in reserve by calling _poolBalanceIsSufficient function  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L483-L485  function _poolBalanceIsSufficient(address _collateral) internal view returns (bool) {  return controller.balanceOf(address(reserve), _collateral) >= collateralsToBeClaimed[_collateral];  the problem is that because collateralsToBeClaimed[_collateral] has increased, controller.balanceOf(address(reserve), _collateral) could also increase. It happens so because controller.balanceOf() function subtracts tapped amount from the reserve s balance.  code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L358-L366  function balanceOf(address _who, address _token) public view isInitialized returns (uint256) {  uint256 balance = _token == ETH ? _who.balance : ERC20(_token).staticBalanceOf(_who);  if (_who == address(reserve)) {  return balance.sub(tap.getMaximumWithdrawal(_token));  } else {  return balance;  And tap.getMaximumWithdrawal(_token) could decrease because it depends on collateralsToBeClaimed[_collateral]  apps/tap/contracts/Tap.sol:L231-L264  function _tappedAmount(address _token) internal view returns (uint256) {  uint256 toBeKept = controller.collateralsToBeClaimed(_token).add(floors[_token]);  uint256 balance = _token == ETH ? address(reserve).balance : ERC20(_token).staticBalanceOf(reserve);  uint256 flow = (_currentBatchId().sub(lastTappedAmountUpdates[_token])).mul(rates[_token]);  uint256 tappedAmount = tappedAmounts[_token].add(flow);  /**  whatever happens enough collateral should be  kept in the reserve pool to guarantee that  its balance is kept above the floor once  all pending sell orders are claimed  /  /**  the reserve's balance is already below the balance to be kept  the tapped amount should be reset to zero  /  if (balance <= toBeKept) {  return 0;  /**  the reserve's balance minus the upcoming tap flow would be below the balance to be kept  the flow should be reduced to balance - toBeKept  /  if (balance <= toBeKept.add(tappedAmount)) {  return balance.sub(toBeKept);  /**  the reserve's balance minus the upcoming flow is above the balance to be kept  the flow can be added to the tapped amount  /  return tappedAmount;  That means that the amount that beneficiary can withdraw has just decreased, which should not be possible.  Recommendation  Ensure that tappedAmount cannot be decreased once updated.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.8 Presale - contributionToken double cast and invalid comparison    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@61f5803.  Description  Examples  contribute - invalid comparison of contract type against address(0x00). Even though this is accepted in solidity <0.5.0 it is going to raise a compiler error with newer versions (>=0.5.0).  code/apps/presale/contracts/Presale.sol:L163-L170  function contribute(address _contributor, uint256 _value) external payable nonReentrant auth(CONTRIBUTE_ROLE) {  require(state() == State.Funding, ERROR_INVALID_STATE);  if (contributionToken == ETH) {  require(msg.value == _value, ERROR_INVALID_CONTRIBUTE_VALUE);  } else {  require(msg.value == 0,      ERROR_INVALID_CONTRIBUTE_VALUE);  _transfer - double cast token to ERC20 if it is the contribution token.  code/apps/presale/contracts/Presale.sol:L344-L344  require(ERC20(_token).safeTransfer(_to, _amount), ERROR_TOKEN_TRANSFER_REVERTED);  Recommendation  contributionToken can either be ETH or a valid ERC20 contract address. It is therefore recommended to store the token as an address type instead of the more precise contract type to resolve the double cast and the invalid contract type to address comparison or cast the ERC20 type to address() before comparison.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.9 Fees are not returned for buy orders if a batch is canceled   ", "body": "  Resolution  This issue has been addressed with the following statement:  The only situation where a batch can be cancelled is when a collateral is un-whitelisted. This is obviously a very critical operation that we introduced just in case the collateral happened to be malicious token. Handling the ability to return fees in case a batch order is cancelled would thus add a lot of computation overhead for: a. a very unlikely situation b. where the fees would anyhow be returned in a malicious token. c. given a small amount [it s a fee and not the main amount]. We figured out that it was a bad decision to add gas overhead to all orders just to prevent this situation.  Description  Every trader pays fees on each buy order and transfers it directly to the beneficiary.  code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L706-L713  uint256 fee = _value.mul(buyFeePct).div(PCT_BASE);  uint256 value = _value.sub(fee);  // collect fee and collateral  if (fee > 0) {  _transfer(_buyer, beneficiary, _collateral, fee);  _transfer(_buyer, address(reserve), _collateral, value);  If the batch is canceled, fees are not returned to the traders because there is no access to the beneficiary account.  Additionally, fees are returned to traders for all the sell orders if the batch is canceled.  Recommendation  Consider transferring fees to a beneficiary only after the batch is over.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.10 Tap - Controller should not be updateable    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@f6054443 by removing update functionality.  Description  Similar to the issue 6.11, Tap allows updating the Controller contract it is using. The permission is currently not assigned in the FundraisingMultisigTemplate but might be used in custom deployments.  code/apps/tap/contracts/Tap.sol:L117-L125  /**  @notice Update controller to `_controller`  @param _controller The address of the new controller contract  /  function updateController(IAragonFundraisingController _controller) external auth(UPDATE_CONTROLLER_ROLE) {  require(isContract(_controller), ERROR_CONTRACT_IS_EOA);  _updateController(_controller);  Recommendation  To avoid inconsistencies, we suggest to remove this functionality and provide a guideline on how to safely upgrade components of the system.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.11 Tap - reserve can be updated in Tap but not in MarketMaker or Controller    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@987720b1 by removing update functionality.  Description  The address of the pool/reserve contract can be updated in Tap if someone owns the UPDATE_RESERVE_ROLE permission. The permission is currently not assigned in the template.  The reserve is being referenced by multiple Contracts. Tap interacts with it to transfer funds to the beneficiary, Controller adds new protected tokens, and MarketMaker transfers funds when someone sells their Shareholder token.  Updating reserve only in Tap is inconsistent with the system as the other contracts are still referencing the old reserve unless they are updated via the Aragon Application update mechanisms.  code/apps/tap/contracts/Tap.sol:L127-L135  /**  @notice Update reserve to `_reserve`  @param _reserve The address of the new reserve [pool] contract  /  function updateReserve(Vault _reserve) external auth(UPDATE_RESERVE_ROLE) {  require(isContract(_reserve), ERROR_CONTRACT_IS_EOA);  _updateReserve(_reserve);  Recommendation  Remove the possibility to update reserve in Tap to keep the system consistent. Provide information about update mechanisms in case the reserve needs to be updated for all components.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.12 Presale can be opened earlier than initially assigned date    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@0726e29.  Description  There are 2 ways how presale opening date can be assigned. Either it s defined on initialization or the presale will start when open() function is executed.  code/apps/presale/contracts/Presale.sol:L144-L146  if (_openDate != 0) {  _setOpenDate(_openDate);  The problem is that even if openDate is assigned to some non-zero date, it can still be opened earlier by calling open() function.  code/apps/presale/contracts/Presale.sol:L152-L156  function open() external auth(OPEN_ROLE) {  require(state() == State.Pending, ERROR_INVALID_STATE);  _open();  Recommendation  Require that openDate is not set (0) when someone manually calls the open() function.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.13 Presale - should not allow zero value contributions    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@6a6e222.  Description  The Presale accepts zero value contributions emitting a contribution event if none of the Aragon components (TokenManager, MinimeToken) raises an exception.  code/apps/presale/contracts/Presale.sol:L163-L173  function contribute(address _contributor, uint256 _value) external payable nonReentrant auth(CONTRIBUTE_ROLE) {  require(state() == State.Funding, ERROR_INVALID_STATE);  if (contributionToken == ETH) {  require(msg.value == _value, ERROR_INVALID_CONTRIBUTE_VALUE);  } else {  require(msg.value == 0,      ERROR_INVALID_CONTRIBUTE_VALUE);  _contribute(_contributor, _value);  Recommendation  Reject zero value ETH or ERC20 contributions.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.14 Compiler Warnings - Function state mutability can be restricted to view    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@cfd677a.  Description  The following methods are not state-changing and can, therefore, be restricted to view.  Recommendation  Restrict function state mutability of the listed methods to view.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.15 FundraisingMultisigTemplate - should use BaseTemplate._createPermissionForTemplate() to assign permissions to itself    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@dd153e0.  Description  The template temporarily assigns permissions to itself to be able to configure parts of the system. This can either be done by calling acl.createPermission(address(this), app, role, manager) or by using a distinct method provided with the DAO-Templates BaseTemplate _createPermissionForTemplate.  We suggest that in order to make it clear that permissions are assigned to the template and make it easier to audit that permissions are either revoked or transferred before the DAO is transferred to the new user, the method provided and used with the default Aragon DAO-Templates should be used.  use createPermission if permissions are assigned to an entity other than the template contract.  use _createPermissionForTemplate when creating permissions for the template contract.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L333-L334  // create and grant ADD_PROTECTED_TOKEN_ROLE to this template  acl.createPermission(this, controller, controller.ADD_COLLATERAL_TOKEN_ROLE(), this);  Sidenote: pass address(this) instead of the contract instance to createPermission.  Recommendation  Use BaseTemplate._createPermissionForTemplate to assign permissions to the template.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.16 FundraisingMultisigTemplate - misleading comments    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@3b4700a and  AragonBlack/fundraising@40c465fc.  Description  The comment mentionsADD_PROTECTED_TOKEN_ROLE but permissions for ADD_COLLATERAL_TOKEN_ROLE are created.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L333-L334  // create and grant ADD_PROTECTED_TOKEN_ROLE to this template  acl.createPermission(this, controller, controller.ADD_COLLATERAL_TOKEN_ROLE(), this);  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L355-L356  // transfer ADD_PROTECTED_TOKEN_ROLE  _transferPermissionFromTemplate(acl, controller, shareVoting, controller.ADD_COLLATERAL_TOKEN_ROLE(), shareVoting);  Recommendation  ADD_PROTECTED_TOKEN_ROLE in the comment should be ADD_COLLATERAL_TOKEN_ROLE.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.17 FundraisingMultisigTemplate - unnecessary cast to address    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@0e00269.  Description  The addresses of DAI (argument address _dai) and AND (argument address _ant) are unnecessarily cast to address.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L58-L76  constructor(  DAOFactory              _daoFactory,  ENS                     _ens,  MiniMeTokenFactory      _miniMeFactory,  IFIFSResolvingRegistrar _aragonID,  address                 _dai,  address                 _ant  BaseTemplate(_daoFactory, _ens, _miniMeFactory, _aragonID)  public  _ensureAragonIdIsValid(_aragonID);  _ensureMiniMeFactoryIsValid(_miniMeFactory);  _ensureTokenIsContractOrETH(_dai);  _ensureTokenIsContractOrETH(_ant);  collaterals.push(address(_dai));  collaterals.push(address(_ant));  Recommendation  Both arguments are already of type address, therefore remove the explicit cast to address() when pushing to the collaterals array.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.18 FundraisingMultisigTemplate - unused import ERC20    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@73481d1.  Description  The interface ERC20 is imported but never used.  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L4-L4  import \"@aragon/os/contracts/lib/token/ERC20.sol\";  Recommendation  Remove the unused import.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "6.19 FundraisingMultisigTemplate - DAI/ANT token address cannot be zero    ", "body": "  Resolution   Fixed with   AragonBlack/fundraising@da561ce.  Description  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L71-L72  _ensureTokenIsContractOrETH(_dai);  _ensureTokenIsContractOrETH(_ant);  code/templates/multisig/contracts/FundraisingMultisigTemplate.sol:L572-L575  function _ensureTokenIsContractOrETH(address _token) internal view returns (bool) {  require(isContract(_token) || _token == ETH, ERROR_BAD_SETTINGS);  Recommendation  Use isContract() instead of _ensureTokenIsContractOrETH() and optionally require that collateral[0] != collateral[1] as an additional check to prevent that the fundraising template is being deployed with an invalid configuration.  7 Tool-Based Analysis  Several tools were used to perform an automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open-source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  $ solium --version  Solium version 1.2.5  $ solium -d  apps/aragon-fundraising/contracts/AragonFundraisingController.sol  370:5    error    Only use indent of 4 spaces.    indentation  templates/multisig/contracts/FundraisingMultisigTemplate.sol  573:5    error    Only use indent of 4 spaces.    indentation  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers (please use horizontal scroll to view all columns):  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  AragonFundraisingController  Implementation  EtherTokenConstant, IsContract, IAragonFundraisingController, AragonApp  initialize  External    onlyInit  updateBeneficiary  External    auth  updateFees  External    auth  openPresale  External    auth  closePresale  External    isInitialized  contribute  External    auth  refund  External    isInitialized  openTrading  External    auth  openBuyOrder  External    auth  openSellOrder  External    auth  claimBuyOrder  External    isInitialized  claimSellOrder  External    isInitialized  addCollateralToken  External    auth  reAddCollateralToken  External    auth  removeCollateralToken  External    auth  updateCollateralToken  External    auth  updateMaximumTapRateIncreasePct  External    auth  updateMaximumTapFloorDecreasePct  External    auth  addTokenTap  External    auth  updateTokenTap  External    auth  withdraw  External    auth  token  Public    isInitialized  contributionToken  Public    isInitialized  getMaximumWithdrawal  Public    isInitialized  collateralsToBeClaimed  Public    isInitialized  balanceOf  Public    isInitialized  _tokenIsContractOrETH  Internal \ud83d\udd12  BancorFormula  Implementation  IBancorFormula, Utils  <Constructor>  Public    calculatePurchaseReturn  Public    NO   calculateSaleReturn  Public    NO   calculateCrossConnectorReturn  Public    NO   power  Internal \ud83d\udd12  generalLog  Internal \ud83d\udd12  floorLog2  Internal \ud83d\udd12  findPositionInMaxExpArray  Internal \ud83d\udd12  generalExp  Internal \ud83d\udd12  optimalLog  Internal \ud83d\udd12  optimalExp  Internal \ud83d\udd12  BatchedBancorMarketMaker  Implementation  EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  open  External    auth  updateFormula  External    auth  updateBeneficiary  External    auth  updateFees  External    auth  addCollateralToken  External    auth  removeCollateralToken  External    auth  updateCollateralToken  External    auth  openBuyOrder  External    auth  openSellOrder  External    auth  claimBuyOrder  External    nonReentrant isInitialized  claimSellOrder  External    nonReentrant isInitialized  claimCancelledBuyOrder  External    nonReentrant isInitialized  claimCancelledSellOrder  External    nonReentrant isInitialized  getCurrentBatchId  Public    isInitialized  getCollateralToken  Public    isInitialized  getBatch  Public    isInitialized  getStaticPricePPM  Public    isInitialized  _staticPricePPM  Internal \ud83d\udd12  _currentBatchId  Internal \ud83d\udd12  _beneficiaryIsValid  Internal \ud83d\udd12  _feeIsValid  Internal \ud83d\udd12  _reserveRatioIsValid  Internal \ud83d\udd12  _tokenManagerSettingIsValid  Internal \ud83d\udd12  _collateralValueIsValid  Internal \ud83d\udd12  _bondAmountIsValid  Internal \ud83d\udd12  _collateralIsWhitelisted  Internal \ud83d\udd12  _batchIsOver  Internal \ud83d\udd12  _batchIsCancelled  Internal \ud83d\udd12  _userIsBuyer  Internal \ud83d\udd12  _userIsSeller  Internal \ud83d\udd12  _poolBalanceIsSufficient  Internal \ud83d\udd12  _slippageIsValid  Internal \ud83d\udd12  _buySlippageIsValid  Internal \ud83d\udd12  _sellSlippageIsValid  Internal \ud83d\udd12  _currentBatch  Internal \ud83d\udd12  _open  Internal \ud83d\udd12  _updateBeneficiary  Internal \ud83d\udd12  _updateFormula  Internal \ud83d\udd12  _updateFees  Internal \ud83d\udd12  _cancelCurrentBatch  Internal \ud83d\udd12  _addCollateralToken  Internal \ud83d\udd12  _removeCollateralToken  Internal \ud83d\udd12  _updateCollateralToken  Internal \ud83d\udd12  _openBuyOrder  Internal \ud83d\udd12  _openSellOrder  Internal \ud83d\udd12  _claimBuyOrder  Internal \ud83d\udd12  _claimSellOrder  Internal \ud83d\udd12  _claimCancelledBuyOrder  Internal \ud83d\udd12  _claimCancelledSellOrder  Internal \ud83d\udd12  _updatePricing  Internal \ud83d\udd12  _transfer  Internal \ud83d\udd12  Presale  Implementation  EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  open  External    auth  contribute  External    nonReentrant auth  refund  External    nonReentrant isInitialized  close  External    nonReentrant isInitialized  contributionToTokens  Public    isInitialized  state  Public    isInitialized  _timeSinceOpen  Internal \ud83d\udd12  _setOpenDate  Internal \ud83d\udd12  _setVestingDatesWhenOpenDateIsKnown  Internal \ud83d\udd12  _open  Internal \ud83d\udd12  _contribute  Internal \ud83d\udd12  _refund  Internal \ud83d\udd12  _close  Internal \ud83d\udd12  _transfer  Internal \ud83d\udd12  Tap  Implementation  TimeHelpers, EtherTokenConstant, IsContract, AragonApp  initialize  External    onlyInit  updateController  External    auth  updateReserve  External    auth  updateBeneficiary  External    auth  updateMaximumTapRateIncreasePct  External    auth  updateMaximumTapFloorDecreasePct  External    auth  addTappedToken  External    auth  removeTappedToken  External    auth  updateTappedToken  External    auth  resetTappedToken  External    auth  withdraw  External    auth  getMaximumWithdrawal  Public    isInitialized  _currentBatchId  Internal \ud83d\udd12  _maximumWithdrawal  Internal \ud83d\udd12  _beneficiaryIsValid  Internal \ud83d\udd12  _maximumTapFloorDecreasePctIsValid  Internal \ud83d\udd12  _tokenIsContractOrETH  Internal \ud83d\udd12  _tokenIsTapped  Internal \ud83d\udd12  _tapRateIsValid  Internal \ud83d\udd12  _tapUpdateIsValid  Internal \ud83d\udd12  _tapRateUpdateIsValid  Internal \ud83d\udd12  _tapFloorUpdateIsValid  Internal \ud83d\udd12  _updateController  Internal \ud83d\udd12  _updateReserve  Internal \ud83d\udd12  _updateBeneficiary  Internal \ud83d\udd12  _updateMaximumTapRateIncreasePct  Internal \ud83d\udd12  _updateMaximumTapFloorDecreasePct  Internal \ud83d\udd12  _addTappedToken  Internal \ud83d\udd12  _removeTappedToken  Internal \ud83d\udd12  _updateTappedToken  Internal \ud83d\udd12  _resetTappedToken  Internal \ud83d\udd12  _withdraw  Internal \ud83d\udd12  FundraisingMultisigTemplate  Implementation  EtherTokenConstant, BaseTemplate  <Constructor>  Public    BaseTemplate  prepareInstance  External    NO   installShareApps  External    NO   installFundraisingApps  External    NO   finalizeInstance  External    NO   _installBoardApps  Internal \ud83d\udd12  _installShareApps  Internal \ud83d\udd12  _installFundraisingApps  Internal \ud83d\udd12  _proxifyFundraisingApps  Internal \ud83d\udd12  _initializePresale  Internal \ud83d\udd12  _initializeMarketMaker  Internal \ud83d\udd12  _initializeTap  Internal \ud83d\udd12  _initializeController  Internal \ud83d\udd12  _setupCollaterals  Internal \ud83d\udd12  _setupBoardPermissions  Internal \ud83d\udd12  _setupSharePermissions  Internal \ud83d\udd12  _setupFundraisingPermissions  Internal \ud83d\udd12  _cacheDao  Internal \ud83d\udd12  _cacheBoardApps  Internal \ud83d\udd12  _cacheShareApps  Internal \ud83d\udd12  _cacheFundraisingApps  Internal \ud83d\udd12  _daoCache  Internal \ud83d\udd12  _boardAppsCache  Internal \ud83d\udd12  _shareAppsCache  Internal \ud83d\udd12  _fundraisingAppsCache  Internal \ud83d\udd12  _clearCache  Internal \ud83d\udd12  _vaultCache  Internal \ud83d\udd12  _shareTMCache  Internal \ud83d\udd12  _reserveCache  Internal \ud83d\udd12  _presaleCache  Internal \ud83d\udd12  _controllerCache  Internal \ud83d\udd12  _ensureTokenIsContractOrETH  Internal \ud83d\udd12  _ensureBoardAppsCache  Internal \ud83d\udd12  _ensureShareAppsCache  Internal \ud83d\udd12  _ensureFundraisingAppsCache  Internal \ud83d\udd12  _registerApp  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "7.4 Test Coverage Measurement", "body": "  Testing is implemented using Truffle an all provided test cases pass. However, the Presale contract fails to generate coverage statistics.  MarketMaker  Controller  Tap  Presale  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/aragonblack-fundraising/"}, {"title": "4.1 StableSwapOperatorV1 - resistantFei value is not correct in the resistantBalanceAndFei function ", "body": "  Description  The resistantBalanceAndFei function of a PCVDeposit contract is supposed to return the amount of funds that the contract controls; it is then used to evaluate the total value of PCV (collateral in the protocol). Additionally, this function returns the number of FEI tokens that are protocol-controlled. These FEI tokens are  temporarily minted ; they are not backed up by the collateral and shouldn t be used in calculations that determine the collateralization of the protocol.  Ideally, the amount of these FEI tokens should be the same during the deposit, withdrawal, and the resistantBalanceAndFei function call. In the StableSwapOperatorV1  contract, all these values are totally different:  during the deposit, the amount of required FEI tokens is calculated. It s done in a way so the values of FEI and 3pool tokens in the metapool should be equal after the deposit. So if there is the initial imbalance of FEI and 3pool tokens, the deposit value of these tokens will be different: code/contracts/pcv/curve/StableSwapOperatorV1.sol:L156-L171 // get the amount of tokens in the pool (uint256 _3crvAmount, uint256 _feiAmount) = (     IStableSwap2(pool).balances(_3crvIndex),     IStableSwap2(pool).balances(_feiIndex) ); // ... and the expected amount of 3crv in it after deposit uint256 _3crvAmountAfter = _3crvAmount + _3crvBalanceAfter;  // get the usd value of 3crv in the pool uint256 _3crvUsdValue = _3crvAmountAfter * IStableSwap3(_3pool).get_virtual_price() / 1e18;  // compute the number of FEI to deposit uint256 _feiToDeposit = 0; if (_3crvUsdValue > _feiAmount) {     _feiToDeposit = _3crvUsdValue - _feiAmount; }  during the withdrawal, the FEI and 3pool tokens are withdrawn in the same proportion as they are present in the metapool: code/contracts/pcv/curve/StableSwapOperatorV1.sol:L255-L258 uint256[2] memory _minAmounts; // [0, 0] IERC20(pool).approve(pool, _lpToWithdraw); uint256 _3crvBalanceBefore = IERC20(_3crv).balanceOf(address(this)); IStableSwap2(pool).remove_liquidity(_lpToWithdraw, _minAmounts);  in the resistantBalanceAndFei function, the value of protocol-controlled FEI tokens and the value of 3pool tokens deposited are considered equal: code/contracts/pcv/curve/StableSwapOperatorV1.sol:L348-L349 resistantBalance = _lpPriceUSD / 2; resistantFei = resistantBalance;  Some of these values may be equal under some circumstances, but that is not enforced. After one of the steps (deposit or withdrawal), the total PCV value and collateralization may be changed significantly.  Recommendation  Make sure that deposit, withdrawal, and the resistantBalanceAndFei are consistent and won t instantly change the PCV value significantly.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.2 CollateralizationOracle - Fei in excluded deposits contributes to userCirculatingFei ", "body": "  Description  CollateralizationOracle.pcvStats iterates over all deposits, queries the resistant balance and FEI for each deposit, and accumulates the total value of the resistant balances and the total resistant FEI. Any Guardian or Governor can exclude (and re-include) a deposit that has become problematic in some way, for example, because it is reporting wrong numbers. Finally, the pcvStats function computes the userCirculatingFei as the total FEI supply minus the accumulated resistant FEI balances; the idea here is to determine the amount of  free  FEI, or FEI that is not PCV. However, the FEI balances from excluded deposits contribute to the userCirculatingFei, although they are clearly not  free  FEI. That leads to a wrong protocolEquity and a skewed collateralization ratio and might therefore have a significant impact on the economics of the system.  It should be noted that even the exclusion from the total PCV leads to a protocolEquity and a collateralization ratio that could be considered skewed (again, it might depend on the exact reasons for exclusion), but  adding  the missing FEI to the userCirculatingFei distorts these numbers even more.  In the extreme scenario that all deposits have been excluded, the entire Fei supply is currently reported as userCirculatingFei.  code/contracts/oracle/CollateralizationOracle.sol:L278-L328  /// @notice returns the Protocol-Controlled Value, User-circulating FEI, and  ///         Protocol Equity.  /// @return protocolControlledValue : the total USD value of all assets held  ///         by the protocol.  /// @return userCirculatingFei : the number of FEI not owned by the protocol.  /// @return protocolEquity : the difference between PCV and user circulating FEI.  ///         If there are more circulating FEI than $ in the PCV, equity is 0.  /// @return validityStatus : the current oracle validity status (false if any  ///         of the oracles for tokens held in the PCV are invalid, or if  ///         this contract is paused).  function pcvStats() public override view returns (  uint256 protocolControlledValue,  uint256 userCirculatingFei,  int256 protocolEquity,  bool validityStatus  ) {  uint256 _protocolControlledFei = 0;  validityStatus = !paused();  // For each token...  for (uint256 i = 0; i < tokensInPcv.length(); i++) {  address _token = tokensInPcv.at(i);  uint256 _totalTokenBalance  = 0;  // For each deposit...  for (uint256 j = 0; j < tokenToDeposits[_token].length(); j++) {  address _deposit = tokenToDeposits[_token].at(j);  // ignore deposits that are excluded by the Guardian  if (!excludedDeposits[_deposit]) {  // read the deposit, and increment token balance/protocol fei  (uint256 _depositBalance, uint256 _depositFei) = IPCVDepositBalances(_deposit).resistantBalanceAndFei();  _totalTokenBalance += _depositBalance;  _protocolControlledFei += _depositFei;  // If the protocol holds non-zero balance of tokens, fetch the oracle price to  // increment PCV by _totalTokenBalance * oracle price USD.  if (_totalTokenBalance != 0) {  (Decimal.D256 memory _oraclePrice, bool _oracleValid) = IOracle(tokenToOracle[_token]).read();  if (!_oracleValid) {  validityStatus = false;  protocolControlledValue += _oraclePrice.mul(_totalTokenBalance).asUint256();  userCirculatingFei = fei().totalSupply() - _protocolControlledFei;  protocolEquity = int256(protocolControlledValue) - int256(userCirculatingFei);  Recommendation  It is unclear how to fix this. One might want to exclude the FEI in excluded deposits entirely from the calculation, but not knowing the amount was the reason to exclude the deposit in the first place. One option could be to let the entity that excludes a deposit specify substitute values that should be used instead of querying the numbers from the deposit. However, it is questionable whether this approach is practical if the numbers we d like to see as substitute values change quickly or repeatedly over time. Ultimately, the querying function itself should be fixed. Moreover, as the substitute values can dramatically impact the system economics, we d only like to trust the Governor with this and not give this permission to a Guardian. However, the original intention was to give a role with less trust than the Governor the possibility to react quickly to a deposit that reports wrong numbers; if the exclusion of deposits becomes the Governor s privilege, such a quick and lightweight intervention isn t possible anymore.  Independently, we recommend taking proper care of the situation that all deposits   or just too many   have been excluded, for example, by setting the returned validityStatus to false, as in this case, there is not enough information to compute the collateralization ratio even as a crude approximation.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.3 StableSwapOperatorV1 - the _minLpOut value is not accurate ", "body": "  Description  When depositing, the expected minimum amount of the output LP tokens is calculated:  code/contracts/pcv/curve/StableSwapOperatorV1.sol:L194-L200  // slippage check on metapool deposit  uint256 _balanceDeposited = IERC20(pool).balanceOf(address(this)) - _balanceBefore;  uint256 _metapoolVirtualPrice = IStableSwap2(pool).get_virtual_price();  uint256 _minLpOut = (_feiToDeposit + _3crvBalanceAfter) * 1e18 / _metapoolVirtualPrice * (Constants.BASIS_POINTS_GRANULARITY - depositMaxSlippageBasisPoints) / Constants.BASIS_POINTS_GRANULARITY;  require(_balanceDeposited >= _minLpOut, \"StableSwapOperatorV1: metapool deposit slippage too high\");  The problem is that the get_virtual_price function returns a valid price only if the tokens in the pool are expected to have a price equal to $1 which is not the case. Also, the balances of deposited FEI and 3pool lp tokens are just added to each other while they have a different price: _feiToDeposit + _3crvBalanceAfter.  The price of the 3pool lp tokens is currently very close to 1$ so this difference is not that visible at the moment, but this can slowly change over time.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.4 StableSwapOperatorV1 - FEI tokens in the contract are not considerred as protocol-owned ", "body": "  Description  Every PCVDeposit contract should return the amount of PCV controlled by this contract in the resistantBalanceAndFei. In addition to that, this function returns the amount of protocol-controlled FEI, which is not supposed to be collateralized. These values are crucial for evaluating the collateralization of the protocol.  Unlike some other PCVDeposit contracts, protocol-controlled FEI is not minted during the deposit and not burnt during the withdrawal. These FEI tokens are transferred beforehand, so when depositing, all the FEI that are instantly becoming protocol-controlled and heavily impact the collateralization rate. The opposite impact, but as much significant, happens during the withdrawal.  The amount of FEI needed for the deposited is calculated dynamically, it is hard to predict the exact amount beforehand. There may be too many FEI tokens in the contract and the leftovers will be considered as the user-controlled FEI.  Recommendation  There may be different approaches to solve this issue. One of them would be to make sure that the Fei transfers to/from the contract and the deposit/withdraw calls are happening in a single transaction. These FEI should be minted, burnt, or re-used as the protocol-controlled FEI in the same transaction. Another option would be to consider all the FEI balance in the contract as the protocol-controlled FEI.  If the intention is to have all these FEI collateralized, the other solution is needed: make sure that resistantBalanceAndFei always returns resistantFei equals zero.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.5 BalancerLBPSwapper - init() can be front-run to potentially steal tokens ", "body": "  Description  The deployment process for BalancerLBPSwapper appears to be the following:  deploy BalancerLBPSwapper.  run ILiquidityBootstrappingPoolFactory.create() proving the newly deployed swapper address as the owner of the pool.  initialize BalancerLBPSwapper.init() with the address of the newly created pool.  This process may be split across multiple transactions as in the v2Phase1.js deployment scenario.  Between step (1) and (3) there is a window of opportunity for someone to maliciously initialize contract. This should be easily detectable because calling init() twice should revert the second transaction. If this is not caught in the deployment script this may have more severe security implications. Otherwise, this window can be used to grief the deployment initializing it before the original initializer does forcing them to redeploy the contract or to steal any tokenSpent/tokenReceived that are owned by the contract at this time.  Note: It is assumed that the contract will not own a lot of tokens right after deployment rendering the scenario of stealing tokens more unlikely. However, that highly depends on the deployment script for the contract system.  Examples  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L107-L117  function init(IWeightedPool _pool) external {  require(address(pool) == address(0), \"BalancerLBPSwapper: initialized\");  pool = _pool;  IVault _vault = _pool.getVault();  vault = _vault;  // Check ownership  require(_pool.getOwner() == address(this), \"BalancerLBPSwapper: contract not pool owner\");  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L159-L160  IERC20(tokenSpent).approve(address(_vault), type(uint256).max);  IERC20(tokenReceived).approve(address(_vault), type(uint256).max);  Recommendation  protect BalancerLBPSwapper.init() and only allow a trusted entity (e.g. the initial deployer) to call this method.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.6 PCVEquityMinter and BalancerLBPSwapper - desynchronisation race ", "body": "  Description  There is nothing that prevents other actors from calling BalancerLBPSwapper.swap() afterTime but right before PCVEquityMinter.mint() would as long as the minAmount required for the call to pass is deposited to BalancerLBPSwapper.  instead of taking the newly minted FEI from PCVEquityMinter, existing FEI from the malicious user will be used with the pool. (instead of inflating the token the malicious actor basically pays for it)  the Timed modifiers of both contracts will be out of sync with BalancerLBPSwapper.swap() being reset (and failing until it becomes available again) and PCVEquityMinter.mint() still being available. Furthermore, keeper-scripts (or actors that want to get the incentive) might continue to attempt to mint() while the call will ultimately fail in .swap() due to the resynchronization of timed (unless they simulate the calls first).  Note: There are not a lot of incentives to actually exploit this other than preventing protocol inflation (mint) and potentially griefing users. A malicious user will lose out on the incentivized call and has to ensure that the minAmount required for .swap() to work is available. It is, however, in the best interest of security to defuse the unpredictable racy character of the contract interaction.  Examples  code/contracts/token/PCVEquityMinter.sol:L91-L93  function _afterMint() internal override {  IPCVSwapper(target).swap();  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L172-L181  function swap() external override afterTime whenNotPaused {  uint256 spentReserves,  uint256 receivedReserves,  uint256 lastChangeBlock  ) = getReserves();  // Ensures no actor can change the pool contents earlier in the block  require(lastChangeBlock < block.number, \"BalancerLBPSwapper: pool changed this block\");  Recommendation  If BalancerLBPSwapper.swap() is only to be called within the flows of action from a PCVEquityMinter.mint() it is suggested to authenticate the call and only let PCVEquityMinter call .swap()  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.7 CollateralizationOracleWrapper - the deviation threshold check in update() always returns false ", "body": "  Description  A call to update() returns a boolean flag indicating whether the update was performed on outdated data. This flag is being checked in updateIfOutdated() which is typically called by an incentivized keeper function.  There may currently be no incentive (e.g. from the keeper side) to call update() if the values are not outdated but they deviated too much from the target. However, anyone can force an update by calling the non-incentivized public update() method instead.  Examples  code/contracts/oracle/CollateralizationOracleWrapper.sol:L156-L177  require(_validityStatus, \"CollateralizationOracleWrapper: CollateralizationOracle is invalid\");  // set cache variables  cachedProtocolControlledValue = _protocolControlledValue;  cachedUserCirculatingFei = _userCirculatingFei;  cachedProtocolEquity = _protocolEquity;  // reset time  _initTimed();  // emit event  emit CachedValueUpdate(  msg.sender,  cachedProtocolControlledValue,  cachedUserCirculatingFei,  cachedProtocolEquity  );  return outdated  || _isExceededDeviationThreshold(cachedProtocolControlledValue, _protocolControlledValue)  || _isExceededDeviationThreshold(cachedUserCirculatingFei, _userCirculatingFei);  Recommendation  Add unit tests to check for all three return conditions (timed, deviationA, deviationB)  Make sure to compare the current to the stored value before updating the cached values when calling _isExceededDeviationThreshold.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.8 ChainlinkOracleWrapper - latestRoundData might return stale results ", "body": "  Description  The oracle wrapper calls out to a chainlink oracle receiving the latestRoundData(). It then checks freshness by verifying that the answer is indeed for the last known round. The returned updatedAt timestamp is not checked.  If there is a problem with chainlink starting a new round and finding consensus on the new value for the oracle (e.g. chainlink nodes abandon the oracle, chain congestion, vulnerability/attacks on the chainlink system) consumers of this contract may continue using outdated stale data (if oracles are unable to submit no new round is started)  Examples  code/contracts/oracle/ChainlinkOracleWrapper.sol:L49-L58  /// @notice read the oracle price  /// @return oracle price  /// @return true if price is valid  function read() external view override returns (Decimal.D256 memory, bool) {  (uint80 roundId, int256 price,,, uint80 answeredInRound) = chainlinkOracle.latestRoundData();  bool valid = !paused() && price > 0 && answeredInRound == roundId;  Decimal.D256 memory value = Decimal.from(uint256(price)).div(oracleDecimalsNormalizer);  return (value, valid);  code/contracts/oracle/ChainlinkOracleWrapper.sol:L42-L47  /// @notice determine if read value is stale  /// @return true if read value is stale  function isOutdated() external view override returns (bool) {  (uint80 roundId,,,, uint80 answeredInRound) = chainlinkOracle.latestRoundData();  return answeredInRound != roundId;  Recommendation  Consider checking the oracle responses updatedAt value after calling out to chainlinkOracle.latestRoundData() verifying that the result is within an allowed margin of freshness.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.9 CollateralizationOracle - missing events and incomplete event information ", "body": "  Description  The CollateralizationOracle.setDepositExclusion function is used to exclude and re-include deposits from collateralization calculations. Unlike the other state-changing functions in this contract, it doesn t emit an event to inform about the exclusion or re-inclusion.  code/contracts/oracle/CollateralizationOracle.sol:L111-L113  function setDepositExclusion(address _deposit, bool _excluded) external onlyGuardianOrGovernor {  excludedDeposits[_deposit] = _excluded;  The DepositAdd event emits not only the deposit address but also the deposit s token. Despite the symmetry, the DepositRemove event does not emit the token.  code/contracts/oracle/CollateralizationOracle.sol:L25-L26  event DepositAdd(address from, address indexed deposit, address indexed token);  event DepositRemove(address from, address indexed deposit);  Recommendation  setDepositInclusion should emit an event that informs about the deposit and whether it was included or excluded.  For symmetry reasons and because it is indeed useful information, the DepositRemove event could include the deposit s token.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.10 RateLimited - Contract starts with a full buffer at deployment ", "body": "  Description  A contract that inherits from RateLimited starts out with a full buffer when it is deployed.  code/contracts/utils/RateLimited.sol:L35  _bufferStored = _bufferCap;  That means the full bufferCap is immediately available after deployment; it doesn t have to be built up over time. This behavior might be unexpected.  Recommendation  We recommend starting with an empty buffer, or   if there are valid reasons for the current implementation   at least document it clearly.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.11 StableSwapOperatorV1 - the contract relies on the 1$ price of every token in 3pool ", "body": "  Description  To evaluate the price of the 3pool lp token, the built-in get_virtual_price function is used. This function is supposed to be a manipulation-resistant pricing function that works under the assumption that all the tokens in the pool are worth 1$. If one of the tokens is broken and is priced less, the price is harder to calculate. For example, Chainlink uses the following function to calculate at least the lower boundary of the lp price: https://blog.chain.link/using-chainlink-oracles-to-securely-utilize-curve-lp-pools/  The withdrawal and the controlled value calculation are always made in DAI instead of other stablecoins of the 3pool. So if DAI gets compromised but other tokens aren t, there is no way to switch to them.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.12 BalancerLBPSwapper - tokenSpent and tokenReceived should be immutable ", "body": "  Description  Acc. to the inline comment both tokenSpent and tokenReceived should be immutable but they are not declared as such.  Examples  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L92-L94  // tokenSpent and tokenReceived are immutable  tokenSpent = _tokenSpent;  tokenReceived = _tokenReceived;  code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L40-L44  /// @notice the token to be auctioned  address public override tokenSpent;  /// @notice the token to buy  address public override tokenReceived;  Recommendation  Declare both variable immutable.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.13 CollateralizationOracle - potentially unsafe casts ", "body": "  Description  protocolControlledValue is the cumulative USD token value of all tokens in the PCV. The USD value is determined using external chainlink oracles. To mitigate some effects of attacks on chainlink to propagate to this protocol it is recommended to implement a defensive approach to handling values derived from the external source. Arithm. overflows are checked by the compiler (0.8.4), however, it does not guarantee safe casting from unsigned to signed integer. The scenario of this happening might be rather unlikely, however, there is no guarantee that the external price-feed is not taken over by malicious actors and this is when every line of defense counts.  //solidity 0.8.7  \u00bb  int(uint(2**255))  57896044618658097711785492504343953926634992332820282019728792003956564819968  \u00bb  int(uint(2**255-2))  57896044618658097711785492504343953926634992332820282019728792003956564819966  Examples  code/contracts/oracle/CollateralizationOracle.sol:L327-L327  protocolEquity = int256(protocolControlledValue) - int256(userCirculatingFei);  code/contracts/oracle/CollateralizationOracle.sol:L322-L322  protocolControlledValue += _oraclePrice.mul(_totalTokenBalance).asUint256();  Recommendation  Perform overflow checked SafeCast as another line of defense against oracle manipulation.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.14 FeiTimedMinter - constructor does not enforce the same boundaries as setter for frequency ", "body": "  Description  The setter method for frequency enforced upper and lower bounds while the constructor does not. Users cannot trust that the frequency is actually set to be within bounds on deployment.  Examples  code/contracts/token/FeiTimedMinter.sol:L32-L48  constructor(  address _core,  address _target,  uint256 _incentive,  uint256 _frequency,  uint256 _initialMintAmount  CoreRef(_core)  Timed(_frequency)  Incentivized(_incentive)  RateLimitedMinter((_initialMintAmount + _incentive) / _frequency, (_initialMintAmount + _incentive), true)  _initTimed();  _setTarget(_target);  _setMintAmount(_initialMintAmount);  code/contracts/token/FeiTimedMinter.sol:L82-L87  function setFrequency(uint256 newFrequency) external override onlyGovernorOrAdmin {  require(newFrequency >= MIN_MINT_FREQUENCY, \"FeiTimedMinter: frequency low\");  require(newFrequency <= MAX_MINT_FREQUENCY, \"FeiTimedMinter: frequency high\");  _setDuration(newFrequency);  Recommendation  Perform the same checks on frequency in the constructor as in the setFrequency method.  This contract is also inherited by a range of contracts that might specify different boundaries to what is hardcoded in the FeiTimedMinter. A way to enforce bounds-checks could be to allow overriding the setter method and using the setter in the constructor as well ensuring that bounds are also checked on deployment.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.15 CollateralizationOracle - swapDeposit should call internal functions to remove/add deposits ", "body": "  Description  Examples  code/contracts/oracle/CollateralizationOracle.sol:L191-L198  /// @notice Swap a PCVDeposit with a new one, for instance when a new version  ///         of a deposit (holding the same token) is deployed.  /// @param _oldDeposit : the PCVDeposit to remove from the list.  /// @param _newDeposit : the PCVDeposit to add to the list.  function swapDeposit(address _oldDeposit, address _newDeposit) external onlyGovernor {  removeDeposit(_oldDeposit);  addDeposit(_newDeposit);  Recommendation  Call the internal functions instead. addDeposit s and removeDeposit s visibility can then be changed from public to external.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.16 CollateralizationOracle - misleading comments ", "body": "  Description  According to an inline comment in isOvercollateralized, the validity status of pcvStats is ignored, while it is actually being checked.  Similarly, a comment in pcvStats mentions that the returned protocolEquity is 0 if there is less PCV than circulating FEI, while in reality, pcvStats always returns the difference between the former and the latter, even if it is negative.  Examples  code/contracts/oracle/CollateralizationOracle.sol:L332-L339  ///         Controlled Value) than the circulating (user-owned) FEI, i.e.  ///         a positive Protocol Equity.  ///         Note: the validity status is ignored in this function.  function isOvercollateralized() external override view whenNotPaused returns (bool) {  (,, int256 _protocolEquity, bool _valid) = pcvStats();  require(_valid, \"CollateralizationOracle: reading is invalid\");  return _protocolEquity > 0;  code/contracts/oracle/CollateralizationOracle.sol:L283-L284  /// @return protocolEquity : the difference between PCV and user circulating FEI.  ///         If there are more circulating FEI than $ in the PCV, equity is 0.  code/contracts/oracle/CollateralizationOracle.sol:L327  protocolEquity = int256(protocolControlledValue) - int256(userCirculatingFei);  Recommendation  Revise the comments.  5 Recommendations  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "5.1 Update Natspec", "body": "  Examples  token is not in natspec  code/contracts/pcv/utils/ERC20Splitter.sol:L6-L28  /// @notice a contract to split token held to multiple locations  contract ERC20Splitter is PCVSplitter {  /// @notice token to split  IERC20 public token;  /**  @notice constructor for ERC20Splitter  @param _core the Core address to reference  @param _pcvDeposits the locations to send tokens  @param _ratios the relative ratios of how much tokens to send each location, in basis points  /  constructor(  address _core,  IERC20 _token,  address[] memory _pcvDeposits,  uint256[] memory _ratios  CoreRef(_core)  PCVSplitter(_pcvDeposits, _ratios)  token = _token;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "5.2 TribeReserveStabilizer - different minting procedures", "body": "  Description  The TRIBE token doesn t have a burn functionality. TRIBE that is supposed to be taken out of circulation is sent to the TribeReserveStabilizer contract, and when that contract has to mint new TRIBE in exchange for FEI, it will first use up the currently held TRIBE balance before actually minting new tokens.  code/contracts/stabilizer/TribeReserveStabilizer.sol:L117-L133  // Transfer held TRIBE first, then mint to cover remainder  function _transfer(address to, uint256 amount) internal override {  _depleteBuffer(amount);  uint256 _tribeBalance = balance();  uint256 mintAmount = amount;  if(_tribeBalance != 0) {  uint256 transferAmount = Math.min(_tribeBalance, amount);  _withdrawERC20(address(token), to, transferAmount);  mintAmount = mintAmount - transferAmount;  assert(mintAmount + transferAmount == amount);  if (mintAmount != 0) {  _mint(to, mintAmount);  The contract also has a mint function that allows the Governor to mint new TRIBE. Unlike the exchangeFei function described above, this function does not first utilize TRIBE held in the contract but directly instructs the token contract to mint the entire amount.  code/contracts/stabilizer/TribeReserveStabilizer.sol:L102-L107  /// @notice mints TRIBE to the target address  /// @param to the address to send TRIBE to  /// @param amount the amount of TRIBE to send  function mint(address to, uint256 amount) external override onlyGovernor {  _mint(to, amount);  code/contracts/stabilizer/TribeReserveStabilizer.sol:L135-L138  function _mint(address to, uint256 amount) internal {  ITribe _tribe = ITribe(address(token));  _tribe.mint(to, amount);  Recommendation  It would make sense and be more consistent with exchangeFei if the mint function first used TRIBE held in the contract before actually minting new tokens.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/09/fei-protocol-v2-phase-1/"}, {"title": "4.1 Potential Reentrancy Into Strategies ", "body": "  Resolution  EigenLabs Quick Summary: The StrategyBase contract may be vulnerable to a token contract that employs some sort of callback to a function like sharesToUnderlyingView, before the balance change is reflected in the contract. The shares have been decremented, which would lead to an incorrect return value from sharesToUnderlyingView.  EigenLabs Response: As noted in the report, this is not an issue if the token contract being used does not allow for reentrancy. For now, we will make it clear both in the contracts as well as the docs that our implementation of StrategyBase.sol does not support tokens with reentrancy. Because of the way our system is designed, anyone can choose to design a strategy with this in mind!  Description  Nevertheless, other functions could be reentered, for example, sharesToUnderlyingView and underlyingToSharesView, as well as their (supposedly) non-view counterparts.  Let s look at the withdraw function in StrategyBase. First, the amountShares shares are burnt, and at the end of the function, the equivalent amount of token is transferred to the depositor:  src/contracts/strategies/StrategyBase.sol:L108-L143  function withdraw(address depositor, IERC20 token, uint256 amountShares)  external  virtual  override  onlyWhenNotPaused(PAUSED_WITHDRAWALS)  onlyStrategyManager  require(token == underlyingToken, \"StrategyBase.withdraw: Can only withdraw the strategy token\");  // copy `totalShares` value to memory, prior to any decrease  uint256 priorTotalShares = totalShares;  require(  amountShares <= priorTotalShares,  \"StrategyBase.withdraw: amountShares must be less than or equal to totalShares\"  );  // Calculate the value that `totalShares` will decrease to as a result of the withdrawal  uint256 updatedTotalShares = priorTotalShares - amountShares;  // check to avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES || updatedTotalShares == 0,  \"StrategyBase.withdraw: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  // Actually decrease the `totalShares` value  totalShares = updatedTotalShares;  /**  @notice calculation of amountToSend *mirrors* `sharesToUnderlying(amountShares)`, but is different since the `totalShares` has already  been decremented. Specifically, notice how we use `priorTotalShares` here instead of `totalShares`.  /  uint256 amountToSend;  if (priorTotalShares == amountShares) {  amountToSend = _tokenBalance();  } else {  amountToSend = (_tokenBalance() * amountShares) / priorTotalShares;  underlyingToken.safeTransfer(depositor, amountToSend);  If we assume that the token contract has a callback to the recipient of the transfer before the actual balance changes take place, then the recipient could reenter the strategy contract, for example, in sharesToUnderlyingView:  src/contracts/strategies/StrategyBase.sol:L159-L165  function sharesToUnderlyingView(uint256 amountShares) public view virtual override returns (uint256) {  if (totalShares == 0) {  return amountShares;  } else {  return (_tokenBalance() * amountShares) / totalShares;  The crucial point is: If the callback is executed before the actual balance change, then sharesToUnderlyingView will report a bad result because the shares have already been burnt but the token balance has not been updated yet.  For deposits, the token transfer to the strategy happens first, and the shares are minted after that:  src/contracts/core/StrategyManager.sol:L643-L652  function _depositIntoStrategy(address depositor, IStrategy strategy, IERC20 token, uint256 amount)  internal  onlyStrategiesWhitelistedForDeposit(strategy)  returns (uint256 shares)  // transfer tokens from the sender to the strategy  token.safeTransferFrom(msg.sender, address(strategy), amount);  // deposit the assets into the specified strategy and get the equivalent amount of shares in that strategy  shares = strategy.deposit(token, amount);  src/contracts/strategies/StrategyBase.sol:L69-L99  function deposit(IERC20 token, uint256 amount)  external  virtual  override  onlyWhenNotPaused(PAUSED_DEPOSITS)  onlyStrategyManager  returns (uint256 newShares)  require(token == underlyingToken, \"StrategyBase.deposit: Can only deposit underlyingToken\");  /**  @notice calculation of newShares *mirrors* `underlyingToShares(amount)`, but is different since the balance of `underlyingToken`  has already been increased due to the `strategyManager` transferring tokens to this strategy prior to calling this function  /  uint256 priorTokenBalance = _tokenBalance() - amount;  if (priorTokenBalance == 0 || totalShares == 0) {  newShares = amount;  } else {  newShares = (amount * totalShares) / priorTokenBalance;  // checks to ensure correctness / avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(newShares != 0, \"StrategyBase.deposit: newShares cannot be zero\");  uint256 updatedTotalShares = totalShares + newShares;  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES,  \"StrategyBase.deposit: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  // update total share amount  totalShares = updatedTotalShares;  return newShares;  That means if there is a callback in the token s transferFrom function and it is executed after the balance change, a reentering call to sharesToUnderlyingView (for example) will again return a wrong result because shares and token balances are not  in sync.   In addition to the reversed order of token transfer and shares update, there s another vital difference between withdraw and deposit: For withdrawals, the call to the token contract originates in the strategy, while for deposits, it is the strategy manager that initiates the call to the token contract (before calling into the strategy). That s a technicality that has consequences for reentrancy protection: Note that for withdrawals, it is the strategy contract that is reentered, while for deposits, there is not a single contract that is reentered; instead, it is the contract system that is in an inconsistent state when the reentrancy happens. Hence, reentrancy protection on the level of individual contracts is not sufficient.  src/contracts/core/StrategyManager.sol:L244-L286  function depositIntoStrategyWithSignature(  IStrategy strategy,  IERC20 token,  uint256 amount,  address staker,  uint256 expiry,  bytes memory signature  external  onlyWhenNotPaused(PAUSED_DEPOSITS)  onlyNotFrozen(staker)  nonReentrant  returns (uint256 shares)  require(  expiry >= block.timestamp,  \"StrategyManager.depositIntoStrategyWithSignature: signature expired\"  );  // calculate struct hash, then increment `staker`'s nonce  uint256 nonce = nonces[staker];  bytes32 structHash = keccak256(abi.encode(DEPOSIT_TYPEHASH, strategy, token, amount, nonce, expiry));  unchecked {  nonces[staker] = nonce + 1;  bytes32 digestHash = keccak256(abi.encodePacked(\"\\x19\\x01\", DOMAIN_SEPARATOR, structHash));  /**  check validity of signature:  1) if `staker` is an EOA, then `signature` must be a valid ECSDA signature from `staker`,  indicating their intention for this action  2) if `staker` is a contract, then `signature` must will be checked according to EIP-1271  /  if (Address.isContract(staker)) {  require(IERC1271(staker).isValidSignature(digestHash, signature) == ERC1271_MAGICVALUE,  \"StrategyManager.depositIntoStrategyWithSignature: ERC1271 signature verification failed\");  } else {  require(ECDSA.recover(digestHash, signature) == staker,  \"StrategyManager.depositIntoStrategyWithSignature: signature not from staker\");  shares = _depositIntoStrategy(staker, strategy, token, amount);  Hence, querying the staker s nonce in reentrancy would still give a result based on an  incomplete state change.  It is, for example, conceivable that the staker still has zero shares, and yet their nonce is already 1. This particular situation is most likely not an issue, but the example shows that reentrancy can be subtle.  Recommendation  This is fine if the token doesn t allow reentrancy in the first place. As discussed above, among the tokens that do allow reentrancy, some variants of when reentrancy can happen in relation to state changes in the token seem more dangerous than others, but we have also argued that this kind of reasoning can be dangerous and error-prone. Hence, we recommend employing comprehensive and defensive reentrancy protection based on reentrancy guards such as OpenZeppelin s ReentrancyGuardUpgradeable, which is already used in the StrategyManager.  Unfortunately, securing a multi-contract system against reentrancy can be challenging, but we hope the preceding discussion and the following pointers will prove helpful:  External functions in strategies that should only be callable by the strategy manager (such as deposit and withdraw) should have the onlyStrategyManager modifier. This is already the case in the current codebase and is listed here only for completeness.  External functions in strategies for which item 1 doesn t apply (such as sharesToUnderlying and underlyingToShares) should query the strategy manager s reentrancy lock and revert if it is set.  In principle, the restrictions above also apply to public functions, but if a public function is also used internally, checks against reentrancy can cause problems (if used in an internal context) or at least be redundant. In the context of reentrancy protection, it is often easier to split public functions into an internal and an external one.  If view functions are supposed to give reliable results (either internally   which is typically the case   or for other contracts), they have to be protected too.  The previous item also applies to the StrategyManager: view functions that have to provide correct results should query the reentrancy lock and revert if it is set.  Solidity automatically generates getters for public state variables. Again, if these (external view) functions must deliver correct results, the same measures must be taken as for explicit view functions. In practice, the state variable has to become internal or private, and the getter function must be hand-written.  The StrategyBase contract provides some basic functionality. Concrete strategy implementations can inherit from this contract, meaning that some functions may be overridden (and might or might not call the overridden version via super), and new functions might be added. While the guidelines above should be helpful, derived contracts must be reviewed and assessed separately on a case-by-case basis. As mentioned before, reentrancy protection can be challenging, especially in a multi-contract system.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.2 StrategyBase   Inflation Attack Prevention Can Lead to Stuck Funds ", "body": "  Resolution  EigenLabs Quick Summary: The StrategyBase contract sets a minimum initial deposit amount of 1e9. This is to mitigate ERC-4626 related inflation attacks, where an attacker can front-run a deposit, inflating the exchange rate between tokens and shares. A consequence of that protection is that any amount less than 1e9 is not withdrawable.  EigenLabs Response: We recognize that this may be notable for tokens such as USDC, where the smallest unit of which is 1e-6. For now, we will make it clear both in the contracts as well as the docs that our implementation of StrategyBase.sol makes this assumption about the tokens being used in the strategy.  Description  As a defense against what has come to be known as inflation or donation attack in the context of ERC-4626, the StrategyBase contract   from which concrete strategy implementations are supposed to inherit   enforces that the amount of shares in existence for a particular strategy is always either 0 or at least a certain minimum amount that is set to 10^9. This mitigates inflation attacks, which require a small total supply of shares to be effective.  src/contracts/strategies/StrategyBase.sol:L92-L95  uint256 updatedTotalShares = totalShares + newShares;  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES,  \"StrategyBase.deposit: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  src/contracts/strategies/StrategyBase.sol:L123-L127  // Calculate the value that `totalShares` will decrease to as a result of the withdrawal  uint256 updatedTotalShares = priorTotalShares - amountShares;  // check to avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES || updatedTotalShares == 0,  \"StrategyBase.withdraw: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  This particular approach has the downside that, in the worst case, a user may be unable to withdraw the underlying asset for up to 10^9 - 1 shares. While the extreme circumstances under which this can happen might be unlikely to occur in a realistic setting and, in many cases, the value of 10^9 - 1 shares may be negligible, this is not ideal.  Recommendation  It isn t easy to give a good general recommendation. None of the suggested mitigations are without a downside, and what s the best choice may also depend on the specific situation. We do, however, feel that alternative approaches that can t lead to stuck funds might be worth considering, especially for a default implementation.  One option is internal accounting, i.e., the strategy keeps track of the number of underlying tokens it owns. It uses this number for conversion rate calculation instead of its balance in the token contract. This avoids the donation attack because sending tokens directly to the strategy will not affect the conversion rate. Moreover, this technique helps prevent reentrancy issues when the EigenLayer state is out of sync with the token contract s state. The downside is higher gas costs and that donating by just sending tokens to the contract is impossible; more specifically, if it happens accidentally, the funds are lost unless there s some special mechanism to recover them.  An alternative approach with virtual shares and assets is presented here, and the document lists pointers to more discussions and proposed solutions.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.3 StrategyWrapper   Functions Shouldn t Be virtual (Out of Scope) ", "body": "  Resolution  EigenLabs Quick Summary: The documentation assumes that StrategyWrapper.sol shouldn t be inheritable, and yet its functions are marked  virtual . It should be noted that this contract was not audited beyond this issue which was noticed accidentally.  EigenLabs Response: This fix was acknowledged and fixed in the following commit: 053ee9d.  Description  The StrategyWrapper contract is a straightforward strategy implementation and   as its NatSpec documentation explicitly states   is not designed to be inherited from:  src/contracts/strategies/StrategyWrapper.sol:L8-L17  /**  @title Extremely simple implementation of `IStrategy` interface.  @author Layr Labs, Inc.  @notice Simple, basic, \"do-nothing\" Strategy that holds a single underlying token and returns it on withdrawals.  Assumes shares are always 1-to-1 with the underlyingToken.  @dev Unlike `StrategyBase`, this contract is *not* designed to be inherited from.  @dev This contract is expressly *not* intended for use with 'fee-on-transfer'-type tokens.  Setting the `underlyingToken` to be a fee-on-transfer token may result in improper accounting.  /  contract StrategyWrapper is IStrategy {  However, all functions in this contract are virtual, which only makes sense if inheriting from StrategyWrapper is possible.  Recommendation  Assuming the NatSpec documentation is correct, and no contract should inherit from StrategyWrapper, remove the virtual keyword from all function definitions. Otherwise, fix the documentation.  Remark  This contract is out of scope, and this finding is only included because we noticed it accidentally. This does not mean we have reviewed the contract or other out-of-scope files.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.4 StrategyBase   Inheritance-Related Issues ", "body": "  Resolution  60141d8.  Description  src/contracts/interfaces/IStrategy.sol:L39-L45  /**  @notice Used to convert an amount of underlying tokens to the equivalent amount of shares in this strategy.  @notice In contrast to `underlyingToSharesView`, this function **may** make state modifications  @param amountUnderlying is the amount of `underlyingToken` to calculate its conversion into strategy shares  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function underlyingToShares(uint256 amountUnderlying) external view returns (uint256);  src/contracts/strategies/StrategyBase.sol:L192-L200  /**  @notice Used to convert an amount of underlying tokens to the equivalent amount of shares in this strategy.  @notice In contrast to `underlyingToSharesView`, this function **may** make state modifications  @param amountUnderlying is the amount of `underlyingToken` to calculate its conversion into strategy shares  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function underlyingToShares(uint256 amountUnderlying) external view virtual returns (uint256) {  return underlyingToSharesView(amountUnderlying);  src/contracts/strategies/StrategyBase.sol:L167-L175  /**  @notice Used to convert a number of shares to the equivalent amount of underlying tokens for this strategy.  @notice In contrast to `sharesToUnderlyingView`, this function **may** make state modifications  @param amountShares is the amount of shares to calculate its conversion into the underlying token  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function sharesToUnderlying(uint256 amountShares) public view virtual override returns (uint256) {  return sharesToUnderlyingView(amountShares);  B. The initialize function in the StrategyBase contract is not virtual, which means the name will not be available in derived contracts (unless with different parameter types). It also has the initializer modifier, which is unavailable in concrete strategies inherited from StrategyBase.  Recommendation  A. If state-changing versions of the conversion functions are needed, the view modifier has to be removed from IStrategy.underlyingToShares, StrategyBase.underlyingToShares, and StrategyBase.sharesToUnderlying. They should be removed entirely from the interface and base contract if they re not needed.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.5 StrategyManager - Cross-Chain Replay Attacks After Chain Split Due to Hard-Coded DOMAIN_SEPARATOR ", "body": "  Resolution  EigenLabs Quick Summary: A. For the implementation of EIP-712 signatures, the domain separator is set in the initialization of the contract, which includes the chain ID. In the case of a chain split, this ID is subject to change. Thus the domain separator must be recomputed. B. The domain separator is calculated using bytes(\"EigenLayer\")   the EIP-712 spec requires a keccak256 hash, i.e. keccak256(bytes(\"EigenLayer\")). C. The EIP712Domain does not include a version string.  EigenLabs Response: A. We have modified our implementation to dynamically check for the chain ID. If we detect a change since initialization, we recompute the domain separator. If not, we use the precomputed value. B. We changed our computation to use keccak256(bytes(\"EigenLayer\")). C. We decided that we would forgo this change for the time being. Changes in A. and B. implemented in this commit: 714dbb6.  Description  A. The StrategyManager contract allows stakers to deposit into and withdraw from strategies. A staker can either deposit themself or have someone else do it on their behalf, where the latter requires an EIP-712-compliant signature. The EIP-712 domain separator is computed in the initialize function and stored in a state variable for later retrieval:  src/contracts/core/StrategyManagerStorage.sol:L23-L24  /// @notice EIP-712 Domain separator  bytes32 public DOMAIN_SEPARATOR;  src/contracts/core/StrategyManager.sol:L149-L153  function initialize(address initialOwner, address initialStrategyWhitelister, IPauserRegistry _pauserRegistry, uint256 initialPausedStatus, uint256 _withdrawalDelayBlocks)  external  initializer  DOMAIN_SEPARATOR = keccak256(abi.encode(DOMAIN_TYPEHASH, bytes(\"EigenLayer\"), block.chainid, address(this)));  Once set in the initialize function, the value can t be changed anymore. In particular, the chain ID is  baked into  the DOMAIN_SEPARATOR during initialization. However, it is not necessarily constant: In the event of a chain split, only one of the resulting chains gets to keep the original chain ID, and the other should use a new one. With the current approach to compute the DOMAIN_SEPARATOR during initialization, store it, and then use the stored value for signature verification, a signature will be valid on both chains after a split   but it should not be valid on the chain with the new ID. Hence, the domain separator should be computed dynamically.  B. The name in the EIP712Domain is of type string:  src/contracts/core/StrategyManagerStorage.sol:L18-L19  bytes32 public constant DOMAIN_TYPEHASH =  keccak256(\"EIP712Domain(string name,uint256 chainId,address verifyingContract)\");  What s encoded when the domain separator is computed is bytes(\"EigenLayer\"):  src/contracts/core/StrategyManager.sol:L153  DOMAIN_SEPARATOR = keccak256(abi.encode(DOMAIN_TYPEHASH, bytes(\"EigenLayer\"), block.chainid, address(this)));  According to EIP-712,  The dynamic values bytes and string are encoded as a keccak256 hash of their contents.  Hence, bytes(\"EigenLayer\") should be replaced with keccak256(bytes(\"EigenLayer\")).  C. The EIP712Domain does not include a version string:  src/contracts/core/StrategyManagerStorage.sol:L18-L19  bytes32 public constant DOMAIN_TYPEHASH =  keccak256(\"EIP712Domain(string name,uint256 chainId,address verifyingContract)\");  That is allowed according to the specification. However, given that most, if not all, projects, as well as OpenZeppelin s EIP-712 implementation, do include a version string in their EIP712Domain, it might be a pragmatic choice to do the same, perhaps to avoid potential incompatibilities.  Recommendation  Individual recommendations have been given above. Alternatively, you might want to utilize OpenZeppelin s EIP712Upgradeable library, which will take care of these issues. Note that some of these changes will break existing signatures.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.6 StrategyManagerStorage   Miscalculated Gap Size ", "body": "  Resolution  EigenLabs Quick Summary: Gap size in StrategyManagerStorage is set to 41 even though 10 slots are utilized. General convention is to have 50 total slots available for storage.  EigenLabs Response: Storage gap size fixed, changed from 41 to 40. This is possible as we aren t storing anything after the gap. Commit hash: d249641.  Description  Upgradeable contracts should have a  gap  of unused storage slots at the end to allow for adding state variables when the contract is upgraded. The convention is to have a gap whose size adds up to 50 with the used slots at the beginning of the contract s storage.  In StrategyManagerStorage, the number of consecutively used storage slots is 10:  DOMAIN_SEPARATOR  nonces  strategyWhitelister  withdrawalDelayBlocks  stakerStrategyShares  stakerStrategyList  withdrawalRootPending  numWithdrawalsQueued  strategyIsWhitelistedForDeposit  beaconChainETHSharesToDecrementOnWithdrawal  However, the gap size in the storage contract is 41:  src/contracts/core/StrategyManagerStorage.sol:L84  uint256[41] private __gap;  Recommendation  If you don t have to maintain compatibility with an existing deployment, we recommend reducing the storage gap size to 40. Otherwise, we recommend adding a comment explaining that, in this particular case, the gap size and the used storage slots should add up to 51 instead of 50 and that this invariant has to be maintained in future versions of this contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.7 Unnecessary Usage of BytesLib ", "body": "  Resolution  EigenLabs Quick Summary: Several contracts import BytesLib.sol and yet do not use it.  6164a12.  Description  Various contracts throughout the codebase import BytesLib.sol without actually using it. It is only utilized once in EigenPod to convert a bytes array of length 32 to a bytes32.  src/contracts/pods/EigenPod.sol:L189-L190  require(validatorFields[BeaconChainProofs.VALIDATOR_WITHDRAWAL_CREDENTIALS_INDEX] == _podWithdrawalCredentials().toBytes32(0),  \"EigenPod.verifyCorrectWithdrawalCredentials: Proof is not for this EigenPod\");  However, this can also be achieved with an explicit conversion to bytes32, and means provided by the language itself should usually be preferred over external libraries.  Recommendation  Remove the import of BytesLib.sol and the accompanying using BytesLib for bytes; when the library is unused. Consider replacing the single usage in EigenPod with an explicit conversion to bytes32, and consequentially removing the import and using statements.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.8 DelayedWithdrawalRouter   Anyone Can Claim Earlier Than Intended on Behalf of Someone Else", "body": "  Resolution  EigenLabs Quick Summary: DelayedWithdrawalRouter implements a claiming function that allows for a provided recipient address. This function is not permissioned and thus can be called at any time (potentially a griefing attack vector with something like taxes).  EigenLabs Response: For now we have decided to leave this function as is and have added a note/warning about the noted behavior.  Description  The DelayedWithdrawalRouter has two functions to claim delayed withdrawals: one the recipient can call themself (i.e., the recipient is msg.sender) and one to claim on behalf of someone else (i.e., the recipient is given as a parameter):  src/contracts/pods/DelayedWithdrawalRouter.sol:L84-L94  /**  @notice Called in order to withdraw delayed withdrawals made to the caller that have passed the `withdrawalDelayBlocks` period.  @param maxNumberOfDelayedWithdrawalsToClaim Used to limit the maximum number of delayedWithdrawals to loop through claiming.  /  function claimDelayedWithdrawals(uint256 maxNumberOfDelayedWithdrawalsToClaim)  external  nonReentrant  onlyWhenNotPaused(PAUSED_DELAYED_WITHDRAWAL_CLAIMS)  _claimDelayedWithdrawals(msg.sender, maxNumberOfDelayedWithdrawalsToClaim);  src/contracts/pods/DelayedWithdrawalRouter.sol:L71-L82  /**  @notice Called in order to withdraw delayed withdrawals made to the `recipient` that have passed the `withdrawalDelayBlocks` period.  @param recipient The address to claim delayedWithdrawals for.  @param maxNumberOfDelayedWithdrawalsToClaim Used to limit the maximum number of delayedWithdrawals to loop through claiming.  /  function claimDelayedWithdrawals(address recipient, uint256 maxNumberOfDelayedWithdrawalsToClaim)  external  nonReentrant  onlyWhenNotPaused(PAUSED_DELAYED_WITHDRAWAL_CLAIMS)  _claimDelayedWithdrawals(recipient, maxNumberOfDelayedWithdrawalsToClaim);  An attacker can not control where the funds are sent, but they can control when the funds are sent once the withdrawal becomes claimable. It is unclear whether this can become a problem. It is, for example, conceivable that there are negative tax implications if funds arrive earlier than intended at a particular address: For instance, disadvantages for the recipient can arise if funds arrive before the new year starts or before some other funds were sent to the same address (e.g., in case the FIFO principle is applied for taxes).  The downside of allowing only the recipient to claim their withdrawals is that contract recipients must be equipped to make the claim.  Recommendation  If these points haven t been considered yet, we recommend doing so.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.9 Consider Using Custom Errors", "body": "  Resolution  EigenLabs Quick Summary: Suggestion to use custom errors in Solidity.  EigenLabs Response: For now we will continue to use standard error messages for the upcoming deployment.  Description and Recommendation  Custom errors were introduced in Solidity version 0.8.4 and have some advantages over the traditional string-based errors: They are usually more gas-efficient, especially regarding deployment costs, and it is easier to include dynamic information in error messages.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.10 StrategyManager - Immediate Settings Changes Can Have Unintended Side Effects", "body": "  Resolution  EigenLabs Quick Summary: StrategyManager contract allows for setWithdrawalDelayBlocks() to be called such that a user attempting to call completeQueuedWithdrawal() may be prevented from withdrawing.  EigenLabs Response: We have decided to leave this concern unaddressed.  Description  Withdrawal delay blocks can be changed immediately:  src/contracts/core/StrategyManager.sol:L570-L572  function setWithdrawalDelayBlocks(uint256 _withdrawalDelayBlocks) external onlyOwner {  _setWithdrawalDelayBlocks(_withdrawalDelayBlocks);  Allows owner to sandwich e.g. completeQueuedWithdrawal function calls to prevent users from withdrawing their stake due to the following check:  src/contracts/core/StrategyManager.sol:L749-L753  require(queuedWithdrawal.withdrawalStartBlock + withdrawalDelayBlocks <= block.number  || queuedWithdrawal.strategies[0] == beaconChainETHStrategy,  \"StrategyManager.completeQueuedWithdrawal: withdrawalDelayBlocks period has not yet passed\"  );  Recommendation  We recommend introducing a simple delay for settings changes to prevent sandwiching attack vectors. It is worth noting that this finding has been explicitly raised because it allows authorized personnel to target individual transactions and users by extension.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.11 StrategyManager - Unused Modifier", "body": "  Resolution  EigenLabs Quick Summary: StrategyManager contract defines an onlyEigenPod modifier which is not used.  EigenLabs Response: We have removed that modifier. Commit hash: 50bb6a8.  Description  The StrategyManager contract defines an onlyEigenPod modifier, but it is never used.  src/contracts/core/StrategyManager.sol:L113-L116  modifier onlyEigenPod(address podOwner, address pod) {  require(address(eigenPodManager.getPod(podOwner)) == pod, \"StrategyManager.onlyEigenPod: not a pod\");  _;  Recommendation  The modifier can be removed.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.12 Inconsistent Data Types for Block Numbers", "body": "  Resolution  EigenLabs Quick Summary: Across our contracts we use both uint32 and uint64 to represent blockNumber values.  Description  The block number attribute is declared using uint32 and uint64. This usage should be more consistent.  Examples  uint32  src/contracts/interfaces/IEigenPod.sol:L30-L35  struct PartialWithdrawalClaim {  PARTIAL_WITHDRAWAL_CLAIM_STATUS status;  // block at which the PartialWithdrawalClaim was created  uint32 creationBlockNumber;  // last block (inclusive) in which the PartialWithdrawalClaim can be fraudproofed  uint32 fraudproofPeriodEndBlockNumber;  src/contracts/core/StrategyManager.sol:L393  withdrawalStartBlock: uint32(block.number),  src/contracts/pods/DelayedWithdrawalRouter.sol:L62-L65  DelayedWithdrawal memory delayedWithdrawal = DelayedWithdrawal({  amount: withdrawalAmount,  blockCreated: uint32(block.number)  });  uint64  src/contracts/pods/EigenPod.sol:L175-L176  function verifyWithdrawalCredentialsAndBalance(  uint64 oracleBlockNumber,  src/contracts/pods/EigenPodManager.sol:L216  function getBeaconChainStateRoot(uint64 blockNumber) external view returns(bytes32) {  Recommendation  Use one data type consistently to minimize the risk of conversion errors or truncation during casts. This is a measure aimed at future-proofing the code base.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.13 EigenPod   Stray nonReentrant Modifier", "body": "  Resolution  EigenLabs Quick Summary: There is a stray nonReentrant modifier in EigenPod.sol.  EigenLabs Response: Modifier has been removed. Commit hash: 9f45837.  Description  There is a stray nonReentrant modifier in EigenPod:  src/contracts/pods/EigenPod.sol:L432-L453  /**  @notice Transfers `amountWei` in ether from this contract to the specified `recipient` address  @notice Called by EigenPodManager to withdrawBeaconChainETH that has been added to the EigenPod's balance due to a withdrawal from the beacon chain.  @dev Called during withdrawal or slashing.  @dev Note that this function is marked as non-reentrant to prevent the recipient calling back into it  /  function withdrawRestakedBeaconChainETH(  address recipient,  uint256 amountWei  external  onlyEigenPodManager  nonReentrant  // reduce the restakedExecutionLayerGwei  restakedExecutionLayerGwei -= uint64(amountWei / GWEI_TO_WEI);  emit RestakedBeaconChainETHWithdrawn(recipient, amountWei);  // transfer ETH from pod to `recipient`  _sendETH(recipient, amountWei);  src/contracts/pods/EigenPod.sol:L466-L468  function _sendETH(address recipient, uint256 amountWei) internal {  delayedWithdrawalRouter.createDelayedWithdrawal{value: amountWei}(podOwner, recipient);  This modifier is likely a leftover from an earlier version of the contract in which Ether was sent directly.  Recommendation  Remove the ineffective modifier for more readable code and reduced gas usage. The import of ReentrancyGuardUpgradeable.sol and the inheritance from ReentrancyGuardUpgradeable can also be removed. Also, consider giving the _sendEth function a more appropriate name.  Remark  The two nonReentrant modifiers in DelayedWithdrawalRouter are neither strictly needed. Still, they look considerably less  random  than the one in EigenPod, and could be warranted by a defense-in-depth approach.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.1 RPC starkNet_sendTransaction - The User Displayed Message Generated With getSigningTxnText() Is Prone to Markdown/Control Chars Injection From contractCallData    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by rendering untrusted user input with the copyable UI component, preventing markdown injection. Additionally, the client provided the following statement:  restructure dialog ui by using MM copyable field, it can ignore any markdown or tag block  validate send transaction calldata has to be able to convert to bigInt  Description  In the code snippet below, contractCallData is potentially untrusted and may contain Markdown renderable strings or strings containing Control Characters that break the context of the message displayed to the user. This can lead to misrepresenting the transaction data to be signed, which should be avoided.  packages/starknet-snap/src/utils/snapUtils.ts:L163-L195  export function getSigningTxnText(  state: SnapState,  contractAddress: string,  contractFuncName: string,  contractCallData: string[],  senderAddress: string,  maxFee: number.BigNumberish,  network: Network,  ): string {  // Retrieve the ERC-20 token from snap state for confirmation display purpose  const token = getErc20Token(state, contractAddress, network.chainId);  let tokenTransferStr = '';  if (token && contractFuncName === 'transfer') {  try {  let amount = '';  if ([3, 6, 9, 12, 15, 18].includes(token.decimals)) {  amount = convert(contractCallData[1], -1 * token.decimals, 'ether');  } else {  amount = (Number(contractCallData[1]) * Math.pow(10, -1 * token.decimals)).toFixed(token.decimals);  tokenTransferStr = `\\n\\nSender Address: ${senderAddress}\\n\\nRecipient Address: ${contractCallData[0]}\\n\\nAmount(${token.symbol}): ${amount}`;  } catch (err) {  console.error(`getSigningTxnText: error found in amount conversion: ${err}`);  return (  `Contract: ${contractAddress}\\n\\nCall Data: [${contractCallData.join(', ')}]\\n\\nEstimated Gas Fee(ETH): ${convert(  maxFee,  'wei',  'ether',  )}\\n\\nNetwork: ${network.name}` + tokenTransferStr  );  packages/starknet-snap/src/sendTransaction.ts:L60-L80  const signingTxnText = getSigningTxnText(  state,  contractAddress,  contractFuncName,  contractCallData,  senderAddress,  maxFee,  network,  );  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([  heading('Do you want to sign this transaction ?'),  text(`It will be signed with address: ${senderAddress}`),  text(signingTxnText),  ]),  },  });  Please note that we have also reported to the MM Snaps team, that dialogues do not by default hint the origin of the action. We hope this will be addressed in a common way for all snaps in the future,  Recommendation  Validate inputs. Encode data in a safe way to be displayed to the user. Show the original data provided within a pre-text or code-block. Show derived or decoded information (token recipient) as additional information to the user.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.2 Lax Validation Using@starknet::validateAndParseAddress Allows Short Addresses and Does Not Verify Checksums    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by wrapping validateAndParseAddress() with an implicit length check. Additionally, the client provided the following statement:  Add validation on the snap side for address length  Checksum will not implement as some users are going to call the Snap directly without going through the dApp  As per the client s decision, checksummed addresses are not enforced.  Description  Address inputs in RPC calls are validated using @starknet::validateAndParseAddress().  packages/starknet-snap/src/getErc20TokenBalance.ts:L19-L28  try {  validateAndParseAddress(requestParamsObj.tokenAddress);  } catch (err) {  throw new Error(`The given token address is invalid: ${requestParamsObj.tokenAddress}`);  try {  validateAndParseAddress(requestParamsObj.userAddress);  } catch (err) {  throw new Error(`The given user address is invalid: ${requestParamsObj.userAddress}`);  While the message validates the general structure for valid addresses, it does not strictly enforce address length and may silently add padding to the inputs before validation. This can be problematic as it may hide user input errors when a user provides an address that is too short and silently gets left-padded with zeroes. This may unintentionally cause a user to request action on the wrong address without them recognizing it.  ../src/utils/address.ts:L14-L24  export function validateAndParseAddress(address: BigNumberish): string {  assertInRange(address, ZERO, MASK_251, 'Starknet Address');  const result = addAddressPadding(address);  if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {  throw new Error('Invalid Address Format');  return result;  export function validateAndParseAddress(address: BigNumberish): string {  assertInRange(address, ZERO, MASK_251, 'Starknet Address');  const result = addAddressPadding(address);  if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {  throw new Error('Invalid Address Format');  return result;  Recommendation  The exposed Snap API should strictly validate inputs. User input must be provided in a safe canonical form (exact address length, checksum) by the dapp.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.3 RPC starkNet_signMessage - Fails to Display the User Account That Is Used for Signing the Message    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by displaying the signing accounts address with the dialog. All user-provided fields are copyable, preventing any markdown injection. Additionally, the client provided the following statement:  add signer address add bottom of the dialog  We want to note that the origin of the RPC call is not visible in the dialog. However, we recommend addressing this with the MM Snap SDK by generically showing the origin of MM popups with the dialog.  Description  The signing request dialogue does not display the user account that is being used to sign the message. A malicious dapp may pretend to sign a message with one account while issuing an RPC call for a different account.  Note that StarkNet signing requests should implement similar security measures to how MetaMask signing requests work. Being fully transparent on  who signs what , also displaying the origin of the request. This is especially important on multi-dapp snaps to avoid users being tricked into signing transactions they did not intend to sign (wrong signer).  packages/starknet-snap/src/signMessage.ts:L34-L42  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([heading('Do you want to sign this message ?'), text(JSON.stringify(typedDataMessage))]),  },  });  if (!response) return false;  Examples  UI does not show the signing accounts address. Hence, the user cannot be sure what account is used to sign the message.  Recommendation  Show what account is requested to sign a message. Display the origin of the RPC call.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.4 RPC starkNet_signMessage - Inconsistency When Previewing the Signed Message (Markdown Injection)    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by rendering user-provided information with the copyable UI component. Additionally, the client provided the following statement:  restructure dialog ui by using MM copyable field, it can ignore any markdown or tag block  Description  The snap displays an dialogue to the user requesting them to confirm that they want to sign a message when a dapp performs a request to starkNet_signMessage. However, the MetaMask Snaps UI text() component will render Markdown. This means that the message-to-be-signed displayed to the user for approval will be inaccurate if it contains Markdown renderable text.  packages/starknet-snap/src/signMessage.ts:L35-L41  const response = await wallet.request({  method: 'snap_dialog',  params: {  type: DialogType.Confirmation,  content: panel([heading('Do you want to sign this message ?'), text(JSON.stringify(typedDataMessage))]),  },  });  Examples  {\"a **mykey**\":\"this should not render **markdown** <pre>test</pre><b>bbb</b><strong>strongstrong</strong>[visit oststrom](https://oststrom.com) _ital_\"}  Recommendation  Render signed message contents in a code block or preformatted text blocks.  Note: we ve also reported this to the MetaMask Snaps team to provide further guidance.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.5 UI/AlertView - Unnecessary Use of dangerouslySetInnerHTML    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by not using dangerouslySetInnerHTML. Additionally, the client provided the following statement:  Remove dangerouslySetInnerHTML from UI  Description  AlertView is populated by setting innerHTML instead of the component s value, which would be auto-escaped. This only makes sense if the component is supposed to render HTML. However, the component is never used with HTML as input, and the attribute name text is misleading.  packages/wallet-ui/src/components/ui/atom/Alert/Alert.view.tsx:L11-L36  export function AlertView({ text, variant, ...otherProps }: Props) {  const paragraph = useRef<HTMLParagraphElement | null>(null);  const [isMultiline, setIsMultiline] = useState(false);  useEffect(() => {  if (paragraph.current) {  const height = paragraph.current.offsetHeight;  setIsMultiline(height > 20);  }, []);  return (  <Wrapper isMultiline={isMultiline} variant={variant} {...otherProps}>  <>  {variant === VariantOptions.SUCCESS && <LeftIcon icon={['fas', 'check-circle']} />}  {variant === VariantOptions.INFO && <LeftIcon icon={['fas', 'info-circle']} color={theme.palette.info.dark} />}  {variant === VariantOptions.ERROR && (  <LeftIcon icon={['fas', 'exclamation-circle']} color={theme.palette.error.main} />  )}  {variant === VariantOptions.WARNING && (  <LeftIcon icon={['fas', 'exclamation-triangle']} color={theme.palette.warning.main} />  )}  <Parag ref={paragraph} color={variant} dangerouslySetInnerHTML={{ __html: text }} />  </>  </Wrapper>  );  packages/wallet-ui/src/components/ui/organism/NoFlaskModal/NoFlaskModal.view.tsx:L4-L25  export const NoFlaskModalView = () => {  return (  <Wrapper>  <StarknetLogo />  <Title>You don't have the MetaMask Flask extension</Title>  <DescriptionCentered>  You need to install MetaMask Flask extension in order to use the StarkNet Snap.  <br />  <br />  <AlertView  text=\"Please make sure that the regular MetaMask extension is disabled or use a different browser profile\"  variant=\"warning\"  />  </DescriptionCentered>  <a href=\"https://metamask.io/flask\" target=\"_blank\" rel=\"noreferrer noopener\">  <ConnectButton customIconLeft={<FlaskIcon />} onClick={() => {}}>  Download MetaMask Flask  </ConnectButton>  </a>  </Wrapper>  );  };  Setting HTML from code is risky because it s easy to inadvertently expose users to a cross-site scripting (XSS) attack.  Recommendation  Do not use dangerouslySetInnerHTML unless there is a specific requirement that passed in HTML be rendered. If so, rename the attribute name to html instead of text to set clear expectations regarding how the input is treated. Nevertheless, since the component is not used with HTML input, we recommend removing dangerouslySetInnerHTML altogether.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.6 RPC starkNet_addErc20Token - Should Ask for User Confirmation    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by requesting user confirmation for adding new ERC20 Tokens. Additionally, the client provided the following statement:  Adding confirm dialog with MM copyable field, it can ignore any markdown or tag block  Disable loading frame when user reject the add ec220 token request on UI  Description  The RPC method upserts ERC20 tokens received via RPC without asking the user for confirmation. This would allow a connected dapp to insert/change ERC20 token information anytime. This can even be more problematic when multiple dapps are connected to the StarkNet-Snap (race conditions).  packages/starknet-snap/src/addErc20Token.ts:L30-L47  validateAddErc20TokenParams(requestParamsObj, network);  const erc20Token: Erc20Token = {  address: tokenAddress,  name: tokenName,  symbol: tokenSymbol,  decimals: tokenDecimals,  chainId: network.chainId,  };  await upsertErc20Token(erc20Token, wallet, saveMutex);  console.log(`addErc20Token:\\nerc20Token: ${JSON.stringify(erc20Token)}`);  return erc20Token;  } catch (err) {  console.error(`Problem found: ${err}`);  throw err;  Recommendation  Ask the user for confirmation when changing the snaps state.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.7 getKeysFromAddress - Possible Unchecked Null Dereference When Looking Up Private Key    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by throwing an exception on error. Additionally, the client provided the following statement:  instead of return null, raise err in getKeysFromAddress, caller will catch the exception  Description  getKeysFromAddress() may return null if an invalid address was provided but most callers of the function do not check for the null condition and blindly dereference or unpack the return value causing an exception.  packages/starknet-snap/src/utils/starknetUtils.ts:L453-L455  return null;  };  Examples  packages/starknet-snap/src/signMessage.ts:L44-L46  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);  const signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);  const typedDataSignature = getTypedDataMessageSignature(signerKeyPair, typedDataMessage, signerAddress);  packages/starknet-snap/src/extractPrivateKey.ts:L37  const { privateKey: userPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);  packages/starknet-snap/src/extractPublicKey.ts:L31-L32  const { publicKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);  userPublicKey = publicKey;  packages/starknet-snap/src/sendTransaction.ts:L48-L52  const {  privateKey: senderPrivateKey,  publicKey,  addressIndex,  } = await getKeysFromAddress(keyDeriver, network, state, senderAddress);  packages/starknet-snap/src/signMessage.ts:L44-L45  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);  const signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);  packages/starknet-snap/src/verifySignedMessage.ts:L38  const { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, verifySignerAddress);  packages/starknet-snap/src/estimateFee.ts:L48-L53  const { privateKey: senderPrivateKey, publicKey } = await getKeysFromAddress(  keyDeriver,  network,  state,  senderAddress,  );  Recommendation  Explicitly check for the null or {} case. Consider returning {} to not allow unpacking followed by an explicit null check.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.8 RPC starkNet_getStoredTransactions - Lax or Missing Input Validation   ", "body": "  Resolution  Won t fix. The client provided the following statement:  not fix, minor impact  We want to note that strict input validation should be performed on all untrusted inputs for read/write and read-only methods. Just because the method is read-only now does not necessarily mean it will stay that way. Leaving untrusted inputs unchecked may lead to more severe security vulnerabilities with a growing codebase in the future.  Description  Potentially untrusted inputs, e.g. addresses received via RPC calls, are not always checked to conform to the StarkNet address format. For example, requestParamsObj.senderAddress is never checked to be a valid StarkNet address.  packages/starknet-snap/src/getStoredTransactions.ts:L18-L26  const transactions = getTransactions(  state,  network.chainId,  requestParamsObj.senderAddress,  requestParamsObj.contractAddress,  requestParamsObj.txnType,  undefined,  minTimeStamp,  );  Recommendation  This method is read-only, and therefore, severity is estimated as Minor. However, it is always suggested to perform strict input validation on all user-provided inputs for read-only and read-write methods.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.9 Disable Debug Log for Production Build    ", "body": "  Resolution  Addressed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a by introducing a configurable logger. Additionally, the client provided the following statement:  add custom logger to replace console.log, and log message base on debug level, when debug level is off, it will not log anything  update production CICD pipeline to build project with debug level = off/disabled  There re still some instances of console.log(). However, internal state or full requests are not logged anymore. We would still recommend replacing the remaining console.log calls (e.g. the one in addERC20Token).  Description  Throughout the codebase, there are various places where debug log output is being printed to the console. This should be avoided for production builds.  Examples  packages/starknet-snap/src/index.ts:L45-L46  // Switch statement for methods not requiring state to speed things up a bit  console.log(origin, request);  packages/starknet-snap/src/index.ts:L91-L92  console.log(`${request.method}:\\nrequestParams: ${JSON.stringify(requestParams)}`);  packages/starknet-snap/src/index.ts:L103  console.log(`Snap State:\\n${JSON.stringify(state, null, 2)}`);  Recommendation  Remove the debug output or create a custom log method that allows to enable/disable logging to console.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.10 package.json - Dependecy Mixup    ", "body": "  Resolution  Fixed with Consensys/starknet-snap@7231bb7fa4671283b2e7b4cbf5a519d56a57697a as per recommendation. Additionally, the client provided the following statement:  Move development dependencies to package.json::devDependencies  Description  The following dependencies are only used for testing or development purposes and should therefore be listed as devDependencies in package.json, otherwise they may be installed for production builds, too.  https://sinonjs.org/  https://www.chaijs.com/  packages/starknet-snap/package.json:L50  \"chai\": \"^4.3.6\",  packages/starknet-snap/package.json:L53-L54  \"sinon\": \"^13.0.2\",  \"sinon-chai\": \"^3.7.0\",  Recommendation  Move development dependencies to package.json::devDependencies.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.11 package.json - Invalid License    Invalid", "body": "  Resolution  Invalid. Legal clarified that it is perfectly fine to allow MIT+Apache2. Additionally, client provided the following statement:  not fix, choose to stick with dual license  Description  The license field in package.json is invalid.  packages/starknet-snap/package.json:L4  \"license\": \"(Apache-2.0 OR MIT)\",  Recommendation  Update the license field.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.12 RPC starkNet_extractPrivateKey - Should Be Renamed to starkNet_displayPrivateKey  ", "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, the extractPrivateKey is not for display purpose  We want to note that we still encourage changing the method name and return value to explicitly return null in the RPC handler for the sake of good secure coding practices discouraging future devs to return implementing key extraction RPC endpoints that may expose wallet credentials to a linked dapp.  Description  It is recommended to rename starkNet_extractPrivateKey  to starkNet_displayPrivateKey as this more accurately describes what the RPC method is doing.  Also, the way the method handler is implemented makes it appear as if it returns the private key to the RPC origin while the submethod returns null. Consider changing this to an explicit empty return to clearly mark in the outer call that no private key is exposed to the caller. Not to confuse this with how starkNet_extractPublicKey  works which actually returns the pubkey to the RPC caller.  packages/starknet-snap/src/index.ts:L123-L127  case 'starkNet_extractPrivateKey':  apiParams.keyDeriver = await getAddressKeyDeriver(snap);  return extractPrivateKey(apiParams);  ", "labels": ["Consensys", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.13 UI/hooks - detectEthereumProvider() Should Require mustBeMetaMask  ", "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, minor impact  Description  MetaMask Snaps require a Metamask provider. However, detectEthereumProvider() does not explicitly require a MetaMask provider and would continue if the alternative provider contains the substring flask in their signature.  packages/wallet-ui/src/hooks/useHasMetamaskFlask.ts:L7-L16  const detectMetamaskFlask = async () => {  try {  const provider = (await detectEthereumProvider({  mustBeMetaMask: false,  silent: true,  })) as any | undefined;  const isFlask = (await provider?.request({ method: 'web3_clientVersion' }))?.includes('flask');  if (provider && isFlask) {  return true;  Consider requiring mustBeMetaMask = true to enforce that the injected provider is indeed MetaMask. This will also work with MetaMask Flask as shown here:  \u21d2 window.ethereum.isMetaMask  true  \u21d2 await window.ethereum.request({ method: 'web3_clientVersion' })  'MetaMask/v10.32.0-flask.0'  ", "labels": ["Consensys", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.14 RPC starkNet_addNetwork - Not Implemented, No User Confirmation  ", "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, minor impact  Description  It was observed that the RPC method starkNet_addNetwork is not implemented.  In case this method is to be exposed to dapps, we recommended to follow the advise given in issue 4.6 to ask for user confirmation when adjusting the snaps configuration state.  ", "labels": ["Consensys", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.1 Insufficient tests ", "body": "  Resolution  Comment from NUTS Finance team:  We have added more mainnet fork test coverage for single plus assets. We will continue to add more test cases on edge cases. For the test coverage command, it s strange that if we run all test cases with  truffle test , the LiquidityGauge test case fails. However, it passes if we run it by ourselves. We have done some debugging but cannot figure it out yet. We believe that our test cases are all valid.  Description  It is crucial to write tests with possibly 100% coverage for smart contract systems. Given that BTCPlus has inner complexity and also integrates many DeFi projects, using unit testing and fuzzing in all code paths is essential to a secure system.  Currently there are only 63 unit tests (with 1 failing) for the main components (Plus/Composite token, Governance, Liquidity Gauge, etc) which are only testing the predetermined code execution paths. There are also DeFi protocol specific tests that are not well organized to be able to find the coverage on the system.  Recommendation  Write proper tests for all possible code flows and specially edge cases (Price volatility, token transfer failure, 0 amounts, etc). It is useful to have one command to run all tests and have a code coverage report at the end. Also using libraries like eth-gas-reporter it s possible to know the gas usage of different functionalities in order to optimize and prevent lock ups in the future.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.2 Simplify the harvest method in each SinglePlus    ", "body": "  Resolution  Comment from NUTS Finance team:  We have replaced all safeApprove() usage with approve() and used block.timestamp as the expiration date.  Description  The BadgerSBTCCrvPlus single plus contract implements a custom harvest method.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L52-L56  /**  @dev Harvest additional yield from the investment.  Only governance or strategist can call this function.  /  function harvest(address[] calldata _tokens, uint256[] calldata _cumulativeAmounts, uint256 _index, uint256 _cycle,  This method can only be called by the strategist because of the onlyStrategist modifier.  This method has a few steps which take one asset and transform it into another asset a few times.  It first claims the Badger tokens:  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L58-L59  // 1. Harvest from Badger Tree  IBadgerTree(BADGER_TREE).claim(_tokens, _cumulativeAmounts, _index, _cycle, _merkleProof, _amountsToClaim);  Then it transforms the Badger tokens into WBTC using Uniswap.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L61-L72  // 2. Sushi: Badger --> WBTC  uint256 _badger = IERC20Upgradeable(BADGER).balanceOf(address(this));  if (_badger > 0) {  IERC20Upgradeable(BADGER).safeApprove(SUSHISWAP, 0);  IERC20Upgradeable(BADGER).safeApprove(SUSHISWAP, _badger);  address[] memory _path = new address[](2);  _path[0] = BADGER;  _path[1] = WBTC;  IUniswapRouter(SUSHISWAP).swapExactTokensForTokens(_badger, uint256(0), _path, address(this), block.timestamp.add(1800));  This step can be simplified in two ways.  First, the safeApprove method isn t useful because its usage is not recommended anymore.  The OpenZeppelin version 4 implementation states the method is deprecated and its usage is discouraged.  contracts/token/ERC20/utils/SafeERC20Upgradeable.sol:L29-L30  * @dev Deprecated. This function has issues similar to the ones found in      * {IERC20-approve}, and its usage is discouraged.  @dev Deprecated. This function has issues similar to the ones found in  {IERC20-approve}, and its usage is discouraged.  Thus, the SafeERC20Upgradeable.sol is not needed anymore and the import can be removed.  @dev Deprecated. This function has issues similar to the ones found in  {IERC20-approve}, and its usage is discouraged.  Thus, the SafeERC20Upgradeable.sol is not needed anymore and the import can be removed.  Another step is swapping the tokens on Uniswap.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L71  IUniswapRouter(SUSHISWAP).swapExactTokensForTokens(_badger, uint256(0), _path, address(this), block.timestamp.add(1800));  In this case, the last argument block.timestamp.add(1800) is the deadline. This is useful when the transaction is sent to the network and a deadline is needed to expire the transaction. However, the execution is right now and there s no need for a future expiration date.  Removing the safe math addition will have the same end effect, the tokens will be swapped and the call is not at risk to expire.  Recommendation  Remove safeApprove and favor using approve. This also removes the need of having SafeERC20Upgradeable.sol included.  Do not use safe math when sending the expiration date. Use block.timestamp for the same effect and a reduced gas cost.  Apply the same principles for other Single Plus Tokens.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.3 Reduce complexity in modifiers related to governance and strategist    ", "body": "  Resolution  Comment from NUTS Finance team:  The code size seems to be an issue for us. For example, the code size of the CompositePlus contract is more than 21k. If you could provide more suggestions on how to reduce the contract code size, we d appreciate it.  Description  The modifier onlyGovernance:  code/BTC-Plus/contracts/Plus.sol:L101-L104  modifier onlyGovernance() {  _checkGovernance();  _;  Calls the internal function _checkGovernance:  code/BTC-Plus/contracts/Plus.sol:L97-L99  function _checkGovernance() internal view {  require(msg.sender == governance, \"not governance\");  There is no other case where the internal method _checkGovernance is called directly.  One can reduce complexity by removing the internal function and moving its code directly in the modifier. This will increase code size but reduce gas used and code complexity.  There are multiple similar instances:  code/BTC-Plus/contracts/Plus.sol:L106-L113  function _checkStrategist() internal view {  require(msg.sender == governance || strategists[msg.sender], \"not strategist\");  modifier onlyStrategist {  _checkStrategist();  _;  code/BTC-Plus/contracts/governance/GaugeController.sol:L298-L305  function _checkGovernance() internal view {  require(msg.sender == governance, \"not governance\");  modifier onlyGovernance() {  _checkGovernance();  _;  code/BTC-Plus/contracts/governance/LiquidityGauge.sol:L450-L457  function _checkGovernance() internal view {  require(msg.sender == IGaugeController(controller).governance(), \"not governance\");  modifier onlyGovernance() {  _checkGovernance();  _;  Recommendation  Consider removing the internal function and including its body in the modifier directly if the code size is not an issue.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.4 safeMath is integrated in Solidity 0.8.0^    ", "body": "  Resolution  Comment from NUTS Finance team:  We ve replaced all SafeMath usage with native math.  Description  The code base is using Solidity 0.8.0 which has safeMath integrated in the compiler. In addition, the codebase also utilizes OpenZepplin SafeMath library for arithmetic operations.  Recommendation  Removing safeMath from the code base results in gas usage optimization and also clearer code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.5 Lack of up to date documentation", "body": "  Resolution  Comment from NUTS Finance team:  We are continuing to enhance our docs about the latest design.  Description  This is a complicated system with many design decisions that are resulted from integration with other DeFi projects. In the code base there are many hard coded values or anti-patterns that are not documented and creates an unhealthy and hard to maintain code base.  Examples  TOKENLESS_PRODUCTION = 40; is not documented in the LiquidityGauge.sol.  uint256 _balance = balanceOf(_account);  uint256 _supply = totalSupply();  uint256 _limit = _balance.mul(TOKENLESS_PRODUCTION).div(100);  if (_votingTotal > 0) {  uint256 _boosting = _supply.mul(_votingBalance).mul(100 - TOKENLESS_PRODUCTION).div(_votingTotal).div(100);  _limit = _limit.add(_boosting);  Based on the conversation with the NUTS finance developer, this is due to fact that this is a fork of the way Curve s DAO contract works.  Recommendation  We recommend to have dedicated up to date documents on the system overview of the system and each module. In addition, in-line documentation in the code base helps to understand the code base and increases the readability of the code.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "6.1 ERC20Lockable - inconsistent locking status    ", "body": "  Resolution  Issue was fixed by completely removing the unlock date mechanism.  Description  Vega_Token.is_tradable() will incorrectly return false if the token is never manually unlocked by the owner but unlock_time has passed, which will automatically unlock trading.  Examples  code/ERC20Lockable.sol:L48-L67  /**  @dev locked status, only applicable before unlock_date  /  bool public _is_locked = true;  /**  @dev Modifier that only allows function to run if either token is unlocked or time has expired.  Throws if called while token is locked.  /  modifier onlyUnlocked() {  require(!_is_locked || now > unlock_date);  _;  /**  @dev Internal function that unlocks token. Can only be ran before expiration (give that it's irrelevant after)  /  function _unlock() internal {  require(now <= unlock_date);  _is_locked = false;  Recommendation  declare _is_locked as private instead of public  create a getter method that correctly returns the locking status  function _isLocked() internal view {  return !_is_locked || now > unlock_date;  make modifier onlyUnlocked() use the newly created getter (_isLocked())  make Vega_Token.is_tradeable() use the newly created getter (_isLocked())  _unlock() should raise an errorcondition when called on an already unlocked contract  it could make sense to emit a  contract hast been unlocked  event for auditing purposes  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of a MythX Full Mode analysis was reviewed by the audit team and no relevant issues were raised as part of the process.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  Solium version 1.2.5  contracts/Address.sol  22:8    warning    Line contains trailing whitespace    no-trailing-whitespace  29:8    error      Avoid using Inline Assembly.         security/no-inline-assembly  contracts/ERC20Lockable.sol  58:8     warning    Provide an error message for require()             error-reason  58:31    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  66:8     warning    Provide an error message for require()             error-reason  66:16    warning    Avoid using 'now' (alias to 'block.timestamp').    security/no-block-members  contracts/ERC20StaticSupply.sol  15:4    warning    Line exceeds the limit of 145 characters    max-len  contracts/SafeERC20.sol  33:16    error      Only use indent of 12 spaces.             indentation  67:65    warning    Avoid using low-level function 'call'.    security/no-low-level-calls  contracts/Vega_Token.sol  9:1    warning    Line contains trailing whitespace    no-trailing-whitespace  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"}, {"title": "7.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  S\u016brya s Description Report  Files Description Table  contracts/Vega_Token.sol  b92b3c54b2f47a88fa9e84534046b462dbaee9aa  contracts/Address.sol  1213b0f150dd5e3f694c3721c44cb5cc3202b743  contracts/ERC20Detailed.sol  7e4d00c462120565201f28361b29201d1bfe0a34  contracts/IERC20.sol  72c15b6a16b7dc92e69ff97ccfe1958d9948e200  contracts/ERC20Lockable.sol  377447995444beee2b3c5342bfa9b1bbc1d08356  contracts/SafeMath.sol  c8bda5eb19c16d34bc48bf115229a9b967feb6ef  contracts/ERC20StaticSupply.sol  bf3e66af74470eed08d0e0f82b9a98705a745c7c  contracts/Ownable.sol  12ec51ec8a3b4eed6326434fd0f5926b40602778  contracts/Roles.sol  2c85acf184ae36f96ebafd8f6e26232ea459a711  contracts/SafeERC20.sol  ebd65ea9a0cdcb29bbbbf651a1076d51be031443  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  Vega_Token  Implementation  Ownable, ERC20StaticSupply  Public    ERC20StaticSupply  unlock_token  Public    onlyOwner  is_tradable  Public    NO   Address  Library  isContract  Internal \ud83d\udd12  toPayable  Internal \ud83d\udd12  ERC20Detailed  Implementation  IERC20  Public    NO   name  Public    NO   symbol  Public    NO   decimals  Public    NO   IERC20  Interface  totalSupply  External    NO   balanceOf  External    NO   transfer  External    NO   allowance  External    NO   approve  External    NO   transferFrom  External    NO   ERC20Lockable  Implementation  IERC20  _unlock  Internal \ud83d\udd12  totalSupply  Public    NO   balanceOf  Public    NO   transfer  Public    onlyUnlocked  allowance  Public    NO   approve  Public    NO   transferFrom  Public    onlyUnlocked  increaseAllowance  Public    NO   decreaseAllowance  Public    NO   _transfer  Internal \ud83d\udd12  _mint  Internal \ud83d\udd12  _burn  Internal \ud83d\udd12  _approve  Internal \ud83d\udd12  _burnFrom  Internal \ud83d\udd12  SafeMath  Library  add  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  div  Internal \ud83d\udd12  mod  Internal \ud83d\udd12  mod  Internal \ud83d\udd12  ERC20StaticSupply  Implementation  ERC20Detailed, Ownable, ERC20Lockable  Public    ERC20Detailed  issue  Public    onlyOwner  Ownable  Implementation  Internal \ud83d\udd12  owner  Public    NO   isOwner  Public    NO   renounceOwnership  Public    onlyOwner  transferOwnership  Public    onlyOwner  _transferOwnership  Internal \ud83d\udd12  Roles  Library  add  Internal \ud83d\udd12  remove  Internal \ud83d\udd12  has  Internal \ud83d\udd12  SafeERC20  Library  safeTransfer  Internal \ud83d\udd12  safeTransferFrom  Internal \ud83d\udd12  safeApprove  Internal \ud83d\udd12  safeIncreaseAllowance  Internal \ud83d\udd12  safeDecreaseAllowance  Internal \ud83d\udd12  callOptionalReturn  Private \ud83d\udd10  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/vega-vegatoken/"}, {"title": "6.1 Re-Entrancy Risks Associated With External Calls With Other Liquid Staking Systems.    ", "body": "  Resolution   Fixed in   commit f43b7cd5135872143cc35f40cae95870446d0413 by introducing reentrancy guards.  Description  As part of the strategy to integrate with Liquid Staking tokens for Ethereum staking, the Lybra Protocol vaults are required to make external calls to Liquid Staking systems.  For example, the depositEtherToMint function in the vaults makes external calls to deposit Ether and receive the LSD tokens back. While external calls to untrusted third-party contracts may be dangerous, in this case, the Lybra Protocol already extends trust assumptions to these third parties simply through the act of accepting their tokens as collateral. Indeed, in some cases the contract addresses are even hardcoded into the contract and called directly instead of relying on some registry:  contracts/lybra/pools/LybraWstETHVault.sol:L21-L40  contract LybraWstETHVault is LybraPeUSDVaultBase {  Ilido immutable lido;  //WstETH = 0x7f39C581F595B53c5cb19bD0b3f8dA6c935E2Ca0;  //Lido = 0xae7ab96520DE3A18E5e111B5EaAb095312D7fE84;  constructor(address _lido, address _asset, address _oracle, address _config) LybraPeUSDVaultBase(_asset, _oracle, _config) {  lido = Ilido(_lido);  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 sharesAmount = lido.submit{value: msg.value}(address(configurator));  require(sharesAmount != 0, \"ZERO_DEPOSIT\");  lido.approve(address(collateralAsset), msg.value);  uint256 wstETHAmount = IWstETH(address(collateralAsset)).wrap(msg.value);  depositedAsset[msg.sender] += wstETHAmount;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,wstETHAmount, block.timestamp);  In that case, depending on the contract, it may be known what contract is being called, and the risk may be assessed as far as what logic may be executed.  However, in the cases of BETH and rETH, the calls are being made into a proxy and a contract registry of a DAO (RocketPool s DAO) respectively.  contracts/lybra/pools/LybraWbETHVault.sol:L15-L32  contract LybraWBETHVault is LybraPeUSDVaultBase {  //WBETH = 0xa2e3356610840701bdf5611a53974510ae27e2e1  constructor(address _asset, address _oracle, address _config)  LybraPeUSDVaultBase(_asset, _oracle, _config) {}  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 preBalance = collateralAsset.balanceOf(address(this));  IWBETH(address(collateralAsset)).deposit{value: msg.value}(address(configurator));  uint256 balance = collateralAsset.balanceOf(address(this));  depositedAsset[msg.sender] += balance - preBalance;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);  contracts/lybra/pools/LybraRETHVault.sol:L25-L42  constructor(address _rocketStorageAddress, address _rETH, address _oracle, address _config)  LybraPeUSDVaultBase(_rETH, _oracle, _config) {  rocketStorage = IRocketStorageInterface(_rocketStorageAddress);  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 preBalance = collateralAsset.balanceOf(address(this));  IRocketDepositPool(rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", \"rocketDepositPool\")))).deposit{value: msg.value}();  uint256 balance = collateralAsset.balanceOf(address(this));  depositedAsset[msg.sender] += balance - preBalance;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);  As a result, it is impossible to make any guarantees for what logic will be executed during the external calls. Namely, reentrancy risks can t be ruled out, and the damage could be critical to the system. While the trust in these parties isn t in question, it would be best practice to avoid any additional reentrancy risks by placing reentrancy guards. Indeed, in the LybraRETHVault and LybraWbETHVault contracts, one can see the possible damage as the calls are surrounded in a preBalance <-> balance pattern.  The whole of third party Liquid Staking systems  operations need not be compromised, only these particular parts would be enough to cause critical damage to the Lybra Protocol.  Recommendation  After conversations with the Lybra Finance team, it has been assessed that reentrancy guards are appropriate in this scenario to avoid any potential reentrancy risk, which is exactly the recommendation this audit team would provide.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.2 The Deployer of GovernanceTimelock Gets Privileged Access to the System.    ", "body": "  Resolution   As per discussions with the Lybra Finance team, this has been acknowledged as a temporary measure to configure anything before the launch of V2. Following the discussions, the Lybra Finance team has revoked the deployer s permissions in   transaction 0x12c95eec095f7e24abc6a127f378f9f0fb3a0021aeac82b487c11afa01b793af and updated the  commit 77e8bc3664fb1b195fd718c2ce1d49af8530f981 to instead introduce a multisig address that will have the  Description  The GovernanceTimelock contract is responsible for Roles Based Access Control management and checks in the Lybra Protocol. It offers two functions specifically that check if an address has the required role - checkRole and checkOnlyRole:  contracts/lybra/governance/GovernanceTimelock.sol:L24-L30  function checkRole(bytes32 role, address _sender) public view  returns(bool){  return hasRole(role, _sender) || hasRole(DAO, _sender);  function checkOnlyRole(bytes32 role, address _sender) public view  returns(bool){  return hasRole(role, _sender);  In checkRole, the contract also lets an address with the role DAO bypass the check altogether, making it a powerful role.  For initial role management, when the GovernanceTimelock contract gets deployed, its constructor logic initializes a few roles, assigns relevant admin roles, and, notably, assigns the DAO role to the contract, and the DAO and the GOV role to the deployer.  contracts/lybra/governance/GovernanceTimelock.sol:L14-L23  constructor(uint256 minDelay, address[] memory proposers, address[] memory executors, address admin) TimelockController(minDelay, proposers, executors, admin) {  _setRoleAdmin(DAO, GOV);  _setRoleAdmin(TIMELOCK, GOV);  _setRoleAdmin(ADMIN, GOV);  _grantRole(DAO, address(this));  _grantRole(DAO, msg.sender);  _grantRole(GOV, msg.sender);  The assignment of such powerful roles to a single private key with the deployer has inherent risks. Specifically in our case, the DAO role alone as we saw may bypass many checks within the Lybra Protocol, and the GOV role even has role management privileges.  However, it does make sense to assign such roles at the beginning of the deployment to finish initialization and assign the rest of the roles. One could argue that having access to the DAO role in the early stages of the system s life could allow for quick disaster recovery in the event of incidents as well. Though, it is still dangerous to hold privileges for such a system in a single address as we have seen over the last years in security incidents that have to do with compromised keys.  Recommendation  While redesigning the deployment process to account for a lesser-privileged deployer would be ideal, the Lybra Finance team should at least transfer ownership as soon as the deployment is complete to minimize compromised private key risk.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.3 The configurator.getEUSDMaxLocked() Condition Can Be Bypassed During a Flashloan    ", "body": "  Resolution   Fixed in   f6c3afb5e48355c180417b192bd24ba294f77797 by checking  Description  When converting EUSD tokens to peUSD, there is a check that limits the total amount of EUSD that can be converted:  contracts/lybra/token/PeUSDMainnet.sol:L74-L77  function convertToPeUSD(address user, uint256 eusdAmount) public {  require(_msgSender() == user || _msgSender() == address(this), \"MDM\");  require(eusdAmount != 0, \"ZA\");  require(EUSD.balanceOf(address(this)) + eusdAmount <= configurator.getEUSDMaxLocked(),\"ESL\");  The issue is that there is a way to bypass this restriction. An attacker can get a flash loan (in EUSD) from this contract, essentially reducing the visible amount of locked tokens (EUSD.balanceOf(address(this))).  Recommendation  Multiple approaches can solve this issue. One would be adding reentrancy protection. Another one could be keeping track of the borrowed amount for a flashloan.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.4 Liquidation Keepers Automatically Become eUSD Debt Providers for Other Liquidations.    ", "body": "  Resolution   Fixed in   commit bbcf1867ef66cfdcd4b4fd26df39518048fbde1f by adding an alternative check to the allowance flag to see if  Description  One of the most important mechanisms in the Lybra Protocol is the liquidation of poorly collateralized vaults. For example, if a vault is found to have a collateralization ratio that is too small, a liquidator may provide debt tokens to the protocol and retrieve the vault collateral at a discount:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L148-L170  function liquidation(address provider, address onBehalfOf, uint256 assetAmount) external virtual {  uint256 assetPrice = getAssetPrice();  uint256 onBehalfOfCollateralRatio = (depositedAsset[onBehalfOf] * assetPrice * 100) / borrowed[onBehalfOf];  require(onBehalfOfCollateralRatio < badCollateralRatio, \"Borrowers collateral ratio should below badCollateralRatio\");  require(assetAmount * 2 <= depositedAsset[onBehalfOf], \"a max of 50% collateral can be liquidated\");  require(EUSD.allowance(provider, address(this)) != 0, \"provider should authorize to provide liquidation EUSD\");  uint256 eusdAmount = (assetAmount * assetPrice) / 1e18;  _repay(provider, onBehalfOf, eusdAmount);  uint256 reducedAsset = assetAmount * 11 / 10;  totalDepositedAsset -= reducedAsset;  depositedAsset[onBehalfOf] -= reducedAsset;  uint256 reward2keeper;  if (provider == msg.sender) {  collateralAsset.safeTransfer(msg.sender, reducedAsset);  } else {  reward2keeper = (reducedAsset * configurator.vaultKeeperRatio(address(this))) / 110;  collateralAsset.safeTransfer(provider, reducedAsset - reward2keeper);  collateralAsset.safeTransfer(msg.sender, reward2keeper);  emit LiquidationRecord(provider, msg.sender, onBehalfOf, eusdAmount, reducedAsset, reward2keeper, false, block.timestamp);  To liquidate the vault, the liquidator needs to transfer debt tokens from the provider address, which in turn needs to have had approved allowance of the token for the vault:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L154  require(EUSD.allowance(provider, address(this)) != 0, \"provider should authorize to provide liquidation EUSD\");  The allowance doesn t need to be large, it only needs to be non-zero. While it is true that in the superLiquidation function the allowance check is for eusdAmount, which is the amount associated with assetAmount (the requested amount of collateral to be liquidated), the liquidator could simply call the maximum of the allowance the provider has given to the vault and then repeat the liquidation process. The allowance does not actually decrease throughout the liquidation process.  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L191  require(EUSD.allowance(provider, address(this)) >= eusdAmount, \"provider should authorize to provide liquidation EUSD\");  Notably, this address doesn t have to be the same one as the liquidator. In fact, there are no checks on whether the liquidator has an agreement or allowance from the provider to use their tokens in this particular vault s liquidation. The contract only checks to see if the provider has EUSD allowance for the vault, and how to split the rewards if the provider is different from the liquidator:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L162-L168  if (provider == msg.sender) {  collateralAsset.safeTransfer(msg.sender, reducedAsset);  } else {  reward2keeper = (reducedAsset * configurator.vaultKeeperRatio(address(this))) / 110;  collateralAsset.safeTransfer(provider, reducedAsset - reward2keeper);  collateralAsset.safeTransfer(msg.sender, reward2keeper);  In fact, this is a design choice of the system to treat the allowance to the vault as an agreement to become a public provider of debt tokens for the liquidation process. It is important to note that there are incentives associated with being a provider as they get the collateral asset at a discount.  However, it is not obvious from documentation at the time of the audit nor the code that an address having a non-zero EUSD allowance for the vault automatically allows other users to use that address as a provider. Indeed, many general-purpose liquidator bots use their tokens during liquidations, using the same address for both the liquidator and the provider. As a result, this would put that address at the behest of any other user who would want to utilize these tokens in liquidations. The user might not be comfortable doing this trade in any case, even at a discount.  In fact, due to this mechanism, even during consciously initiated liquidations MEV bots could spot this opportunity and front-run the liquidator s transaction. A frontrunner could put themselves as the keeper and the original user as the provider, grabbing the reward2keeper fee and leaving the original address with fewer rewards and failed gas after the liquidation.  Recommendation  While the mechanism is understood to be done for convenience and access to liquidity as a design decision, this could put unaware users in unfortunate situations of having performed a trade without explicit consent. Specifically, the MEV attack vector could be executed and repeated without fail by a capable actor monitoring the mempool. Consider having a separate, explicit flag for allowing others to use a user s tokens during liquidation, thus also accommodating solo liquidators by removing the MEV attack vector. Consider explicitly mentioning these mechanisms in the documentation as well.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.5 Use the Same Solidity Version Across Contracts.    ", "body": "  Resolution   Fixed in   commit 33af5c92044cd84c7f69eb8a55316d1e8535ea84 and  commit b1c6ac26b262ec6011c14297583d67d9e3e94326.  Description  Most contracts use the same Solidity version with pragma solidity ^0.8.17. The only exception is the StakingRewardsV2 contract which has pragma solidity ^0.8.  contracts/lybra/miner/stakerewardV2pool.sol:L2  pragma solidity ^0.8;  Recommendation  If all contracts will be tested and utilized together, it would be best to utilize and document the same version within all contract code to avoid any issues and inconsistencies that may arise across Solidity versions.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.6 Duplication of Bad Collateral Ratio   ", "body": "  Resolution  The Lybra Finance team has acknowledged this as a choice by design and provided the following note:  The liquidation ratio for each eUSD vault is fixed, and this has been stated in our docs. Therefore, we will keep it unchanged.  Description  It is possible to set a bad collateral ratio in the LybraConfigurator contract for any vault:  contracts/lybra/configuration/LybraConfigurator.sol:L137-L141  function setBadCollateralRatio(address pool, uint256 newRatio) external onlyRole(DAO) {  require(newRatio >= 130 * 1e18 && newRatio <= 150 * 1e18 && newRatio <= vaultSafeCollateralRatio[pool] + 1e19, \"LNA\");  vaultBadCollateralRatio[pool] = newRatio;  emit SafeCollateralRatioChanged(pool, newRatio);  But in the LybraEUSDVaultBase contract, this value is fixed and cannot be changed:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L19  uint256 public immutable badCollateralRatio = 150 * 1e18;  This duplication of values can be misleading at some point. It s better to make sure you cannot change the bad collateral ratio in the LybraConfigurator contract for some types of vaults.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.7 Missing Events.    ", "body": "  Resolution   Fixed in   commit 518ef434c6f89c7747373b6ae178d9665d3637f2.  Description  In a few cases in the Lybra Protocol system, there are contracts that are missing events in significant scenarios, such as important configuration changes like a price oracle change. Consider implementing more events in the below examples.  Examples  No events in the contract:  contracts/lybra/miner/esLBRBoost.sol:L10-L30  contract esLBRBoost is Ownable {  esLBRLockSetting[] public esLBRLockSettings;  mapping(address => LockStatus) public userLockStatus;  IMiningIncentives public miningIncentives;  // Define a struct for the lock settings  struct esLBRLockSetting {  uint256 duration;  uint256 miningBoost;  // Define a struct for the user's lock status  struct LockStatus {  uint256 lockAmount;  uint256 unlockTime;  uint256 duration;  uint256 miningBoost;  // Constructor to initialize the default lock settings  constructor(address _miningIncentives) {  Missing an event during a premature unlock:  contracts/lybra/miner/ProtocolRewardsPool.sol:L125-L135  function unlockPrematurely() external {  require(block.timestamp + exitCycle - 3 days > time2fullRedemption[msg.sender], \"ENW\");  uint256 burnAmount = getReservedLBRForVesting(msg.sender) - getPreUnlockableAmount(msg.sender);  uint256 amount = getPreUnlockableAmount(msg.sender) + getClaimAbleLBR(msg.sender);  if (amount > 0) {  LBR.mint(msg.sender, amount);  unstakeRatio[msg.sender] = 0;  time2fullRedemption[msg.sender] = 0;  grabableAmount += burnAmount;  Missing events for setting important configurations such as setToken, setLBROracle, and setPools:  contracts/lybra/miner/EUSDMiningIncentives.sol:L87-L102  function setToken(address _lbr, address _eslbr) external onlyOwner {  LBR = _lbr;  esLBR = _eslbr;  function setLBROracle(address _lbrOracle) external onlyOwner {  lbrPriceFeed = AggregatorV3Interface(_lbrOracle);  function setPools(address[] memory _vaults) external onlyOwner {  require(_vaults.length <= 10, \"EL\");  for (uint i = 0; i < _vaults.length; i++) {  require(configurator.mintVault(_vaults[i]), \"NOT_VAULT\");  vaults = _vaults;  Missing events for setting important configurations such as setRewardsDuration and setBoost:  contracts/lybra/miner/stakerewardV2pool.sol:L121-L130  // Allows the owner to set the rewards duration  function setRewardsDuration(uint256 _duration) external onlyOwner {  require(finishAt < block.timestamp, \"reward duration not finished\");  duration = _duration;  // Allows the owner to set the boost contract address  function setBoost(address _boost) external onlyOwner {  esLBRBoost = IesLBRBoost(_boost);  Missing event during what is essentially staking LBR into esLBR (such as in ProtocolRewardsPool.stake()). Consider an appropriate event here such as StakeLBR:  contracts/lybra/miner/esLBRBoost.sol:L55-L58  if(useLBR) {  IesLBR(miningIncentives.LBR()).burn(msg.sender, lbrAmount);  IesLBR(miningIncentives.esLBR()).mint(msg.sender, lbrAmount);  Recommendation  Implement additional events as appropriate.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.8 Incorrect Interfaces    ", "body": "  Resolution   Fixed in   commit 90285107de8a6754954c303cd69d97b5fdb4e248,  commit 0ac9cd732b601d0baef2690ef9f9f02cda989331, and  commit 518ef434c6f89c7747373b6ae178d9665d3637f2.  Description  In a few cases, incorrect interfaces are used on top of contracts. Though the effect is the same as the contracts are just tokens and follow the same interfaces, it is best practice to implement correct interfaces.  IPeUSD is used instead of IEUSD  contracts/lybra/configuration/LybraConfigurator.sol:L60  IPeUSD public EUSD;  IPeUSD is used instead of IEUSD  contracts/lybra/configuration/LybraConfigurator.sol:L109  if (address(EUSD) == address(0)) EUSD = IPeUSD(_eusd);  IesLBR instead of ILBR  contracts/lybra/miner/ProtocolRewardsPool.sol:L29  IesLBR public LBR;  IesLBR instead of ILBR  contracts/lybra/miner/ProtocolRewardsPool.sol:L57  LBR = IesLBR(_lbr);  Recommendation  Implement correct interfaces for consistency.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.9 The ETH Staking Rewards Distribution Tradeoff", "body": "  Description  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.1 Potentially dangerous use of a cached exchange rate from Compound ", "body": "  Description  GPortfolioReserveManager.adjustReserve performs reserve adjustment calculations based on Compound s cached exchange rate values (using CompoundLendingMarketAbstraction.getExchangeRate()) then triggers operations on managed tokens based on up-to-date values (using CompoundLendingMarketAbstraction.fetchExchangeRate()) . Significant deviation between the cached and up-to-date values may make it difficult to predict the outcome of reserve adjustments.  Recommendation  Use getExchangeRate() consistently, or ensure fetchExchangeRate() is used first, and getExchangeRate() afterward.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/12/growth-defi-v1/"}, {"title": "6.2 Potential resource exhaustion by external calls performed within an unbounded loop ", "body": "  Description  DydxFlashLoanAbstraction._requestFlashLoan performs external calls in a potentially-unbounded loop. Depending on changes made to DyDx s SoloMargin, this may render this flash loan provider prohibitively expensive. In the worst case, changes to SoloMargin could make it impossible to execute this code due to the block gas limit.  code/contracts/modules/DydxFlashLoanAbstraction.sol:L62-L69  uint256 _numMarkets = SoloMargin(_solo).getNumMarkets();  for (uint256 _i = 0; _i < _numMarkets; _i++) {  address _address = SoloMargin(_solo).getMarketTokenAddress(_i);  if (_address == _token) {  _marketId = _i;  break;  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/12/growth-defi-v1/"}, {"title": "6.1 Funds Refunded From Celer Bridge Might Be Stolen ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#144 by adding checks to see if the refund is received and equal to the expected amount.  Description  The function refundCelerUser from CelerImpl.sol allows a user that deposited into the Celer pool on the source chain, to be refunded for tokens that were not bridged to the destination chain. The tokens are reimbursed to the user by calling the withdraw method on the Celer pool. This is what the refundCelerUser function is doing.  src/bridges/cbridge/CelerImpl.sol:L413-L415  if (!router.withdraws(transferId)) {  router.withdraw(_request, _sigs, _signers, _powers);  From the point of view of the Celer bridge, the initial depositor of the tokens is the SocketGateway. As a consequence, the Celer contract transfers the tokens to be refunded to the gateway. The gateway is then in charge of forwarding the tokens to the initial depositor. To achieve this, it keeps a mapping of unique transfer IDs to depositor addresses. Once a refund is processed, the corresponding address in the mapping is reset to the zero address.  Looking at the withdraw function of the Celer pool, we see that for some tokens, it is possible that the reimbursement will not be processed directly, but only after some delay. From the gateway point of view, the reimbursement will be marked as successful, and the address of the original sender corresponding to this transfer ID will be reset to address(0).  It is then the responsibility of the user, once the locking delay has passed, to call another function to claim the tokens. Unfortunately, in our case, this means that the funds will be sent back to the gateway contract and not to the original sender. Because the gateway implements rescueEther, and rescueFunds functions, the admin might be able to send the funds back to the user. However, this requires manual intervention and breaks the trustlessness assumptions of the system. Also, in that case, there is no easy way to trace back the original address of the sender, that corresponds to this refund.  src/bridges/cbridge/CelerImpl.sol:L120-L127  function bridgeAfterSwap(  uint256 amount,  bytes calldata bridgeData  ) external payable override {  CelerBridgeData memory celerBridgeData = abi.decode(  bridgeData,  (CelerBridgeData)  );  src/bridges/stargate/l2/Stargate.sol:L183-L186  function swapAndBridge(  uint32 swapId,  bytes calldata swapData,  StargateBridgeDataNoToken calldata stargateBridgeData  Note that this violates the security assumption:  The contracts are not supposed to hold any funds post-tx execution.   Recommendation  Make sure that CelerImpl supports also the delayed withdrawals functionality and that withdrawal requests are deleted only if the receiver has received the withdrawal in a single transaction.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.2 Calls Made to Non-Existent/Removed Routes or Controllers Will Not Result in Failure ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#145 by adding a  Description  This issue was found in commit hash a8d0ad1c280a699d88dc280d9648eacaf215fb41.  In the Ethereum Virtual Machine (EVM), delegatecall will succeed for calls to externally owned accounts and more specifically to the zero address, which presents a potential security risk. We have identified multiple instances of delegatecall being used to invoke smart contract functions.  This, combined with the fact that routes can be removed from the system by the owner of the SocketGateway contract using the disableRoute function, makes it possible for the user s funds to be lost in case of an executeRoute transaction (for instance) that s waiting in the mempool is eventually being front-ran by a call to disableRoute.  Examples  src/SocketGateway.sol:L95  (bool success, bytes memory result) = addressAt(routeId).delegatecall(  src/bridges/cbridge/CelerImpl.sol:L208  .delegatecall(swapData);  src/bridges/stargate/l1/Stargate.sol:L187  .delegatecall(swapData);  src/bridges/stargate/l2/Stargate.sol:L190  .delegatecall(swapData);  src/controllers/BaseController.sol:L50  .delegatecall(data);  Even after the upgrade to commit hash d0841a3e96b54a9d837d2dba471aa0946c3c8e7b, the following bug is still present:  src/SocketGateway.sol:L411-L428  function addressAt(uint32 routeId) public view returns (address) {  if (routeId < 513) {  if (routeId < 257) {  if (routeId < 129) {  if (routeId < 65) {  if (routeId < 33) {  if (routeId < 17) {  if (routeId < 9) {  if (routeId < 5) {  if (routeId < 3) {  if (routeId == 1) {  return  0x822D4B4e63499a576Ab1cc152B86D1CFFf794F4f;  } else {  return  0x822D4B4e63499a576Ab1cc152B86D1CFFf794F4f;  } else {  src/SocketGateway.sol:L2971-L2972  if (routes[routeId] == address(0)) revert ZeroAddressNotAllowed();  return routes[routeId];  Recommendation  Consider adding a check to validate that the callee of a delegatecall is indeed a contract, you may refer to the Address library by OZ.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.3 Owner Can Add Arbitrary Code to Be Executed From the SocketGateway Contract ", "body": "  Resolution  The client team has responded with the following note:  Noted, we will setup tests and rigorous processes around adding new routes.  Description  Since these routes are called via delegatecall(), they don t hold any storage variables that would be used in the Socket systems. However, as Socket aggregates more solutions, unexpected complexities may arise that could require storing and accessing variables through additional contracts. Those contracts would be access control protected to only have the SocketGateway contract have the privileges to modify its variables.  This together with the Owner of the SocketGateway being able to add routes with arbitrary code creates an attack vector where a compromised address with Owner privileges may add a route that would contain code that exploits the special privileges assigned to the SocketGateway contract for their benefit.  For example, the Celer bridge needs extra logic to account for its refund mechanism, so there is an additional CelerStorageWrapper contract that maintains a mapping between individual bridge transfer transactions and their associated msg.sender:  src/bridges/cbridge/CelerImpl.sol:L145  celerStorageWrapper.setAddressForTransferId(transferId, msg.sender);  src/bridges/cbridge/CelerStorageWrapper.sol:L6-L12  /**  @title CelerStorageWrapper  @notice handle storageMappings used while bridging ERC20 and native on CelerBridge  @dev all functions ehich mutate the storage are restricted to Owner of SocketGateway  @author Socket dot tech.  /  contract CelerStorageWrapper {  Consequently, this contract has access-protected functions that may only be called by the SocketGateway to set and delete the transfer IDs:  src/bridges/cbridge/CelerStorageWrapper.sol:L32  function setAddressForTransferId(  src/bridges/cbridge/CelerStorageWrapper.sol:L52  function deleteTransferId(bytes32 transferId) external {  A compromised Owner of SocketGateway could then create a route that calls into the CelerStorageWrapper contract and updates the transfer IDs associated addresses to be under their control via deleteTransferId() and setAddressForTransferId() functions. This could create a significant drain of user funds, though, it depends on a compromised privileged Owner address.  Recommendation  Although it may indeed be unlikely, for aggregating solutions it is especially important to try and minimize compromised access issues. As future solutions require more complexity, consider architecting their integrations in such a way that they require as few administrative and SocketGateway-initiated transactions as possible. Through conversations with the Socket team, it appears that solutions such as timelocks on adding new routes are being considered as well, which would help catch the problem before it appears as well.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.4 Dependency on Third-Party APIs to Create the Right Payload ", "body": "  Resolution  The client team has responded with the following note:  We offset this risk by following 2 approaches - verifying oneinch calldata on our api before making full calldata for SocketGateway and making verifier contracts/libs that integrators can use to verify our calldata on their side before making actual transaction.  Description  The Socket system of routes and controllers integrates swaps, bridges, and potentially other solutions that are vastly different from each other. The function arguments that are required to execute them may often seem like a black box of a payload for a typical end user. In fact, even when users explicitly provide a destination token with an associated amount for a swap, these arguments themselves might not even be fully (or at all) used in the route itself. Instead, often the routes and controllers accept a bytes payload that contains all the necessary data for its action. These data payloads are generated off-chain, often via centralized APIs provided by the integrated systems themselves, which is understandable in isolation as they have to be generated somewhere at some point. However, the provided bytes do not get checked for their correctness or matching with the other arguments that the user explicitly provided. Even the events that get emitted refer to the individual arguments of functions as opposed to what actually was being used to execute the logic.  src/swap/oneinch/OneInchImpl.sol:L59-L63  // additional data is generated in off-chain using the OneInch API which takes in  // fromTokenAddress, toTokenAddress, amount, fromAddress, slippage, destReceiver, disableEstimate  (bool success, bytes memory result) = ONEINCH_AGGREGATOR.call(  swapExtraData  );  Even the event at the end of the transaction partially refers to the explicitly provided arguments instead of those that actually facilitated the execution of logic  src/swap/oneinch/OneInchImpl.sol:L84-L91  emit SocketSwapTokens(  fromToken,  toToken,  returnAmount,  amount,  OneInchIdentifier,  receiverAddress  );  As Socket aggregates other solutions, it naturally incurs the trust assumptions and risks associated with its integrations. In some ways, they even stack on top of each other, especially in those Socket functions that batch several routes together   all of them and their associated API calls need to return the correct payloads. So, there is an opportunity to minimize these risks by introducing additional checks into the contracts that would verify the correctness of the payloads that are passed over to the routes and controllers. In fact, creating these payloads within the contracts would allow other systems to integrate Socket more simpler as they could just call the functions with primary logical arguments such as the source token, destination token, and amount.  Recommendation  Consider allocating additional checks within the route implementations that ensure that the explicitly passed arguments match what is being sent for execution to the integrated solutions, like in the above example with the 1inch implementation.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.5 NativeOptimismImpl - Events Will Not Be Emitted in Case of Non-Native Tokens Bridging ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#146 by moving the event above the bridging code, making sure events are emitted for all cases, and adding the fix to other functions that had a similar issue.  Description  In the case of the usage of non-native tokens by users, the SocketBridge event will not be emitted since the code will return early.  Examples  src/bridges/optimism/l1/NativeOptimism.sol:L110  function bridgeAfterSwap(  src/bridges/optimism/l1/NativeOptimism.sol:L187  function swapAndBridge(  src/bridges/optimism/l1/NativeOptimism.sol:L283  function bridgeERC20To(  Recommendation  Make sure that the SocketBridge event is emitted for non-native tokens as well.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.6 Inconsistent Comments ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#147.  Description  Some of the contracts in the code have incorrect developer comments annotated for them. This could create confusion for future readers of this code that may be trying to maintain, audit, update, fork, integrate it, and so on.  Examples  src/bridges/stargate/l2/Stargate.sol:L174-L183  /**  @notice function to bridge tokens after swap. This is used after swap function call  @notice This method is payable because the caller is doing token transfer and briding operation  @dev for usage, refer to controller implementations  encodedData for bridge should follow the sequence of properties in Stargate-BridgeData struct  @param swapId routeId for the swapImpl  @param swapData encoded data for swap  @param stargateBridgeData encoded data for StargateBridgeData  /  function swapAndBridge(  This is the same comment as bridgeAfterSwap, whereas it instead does swapping and bridging together  src/bridges/cbridge/CelerStorageWrapper.sol:L24-L32  /**  @notice function to store the transferId and message-sender of a bridging activity  @notice This method is payable because the caller is doing token transfer and briding operation  @dev for usage, refer to controller implementations  encodedData for bridge should follow the sequence of properties in CelerBridgeData struct  @param transferId transferId generated during the bridging of ERC20 or native on CelerBridge  @param transferIdAddress message sender who is making the bridging on CelerBridge  /  function setAddressForTransferId(  This comment refers to a payable property of this function when it isn t.  src/bridges/cbridge/CelerStorageWrapper.sol:L45-L52  /**  @notice function to store the transferId and message-sender of a bridging activity  @notice This method is payable because the caller is doing token transfer and briding operation  @dev for usage, refer to controller implementations  encodedData for bridge should follow the sequence of properties in CelerBridgeData struct  @param transferId transferId generated during the bridging of ERC20 or native on CelerBridge  /  function deleteTransferId(bytes32 transferId) external {  This comment is copied from the above function when it does the opposite of storing - it deletes the transferId  Recommendation  Adjust comments so they reflect what the functions are actually doing.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.7 Ether Might Be Sent to Routes by Mistake, and Can Be Stolen ", "body": "  Resolution  The client team has responded with the following note:  This can happen only if there is an error in API or integration. There are test cases to verify value on API side and we also run an automated testing suite using small amounts after each upgrade to the API before releasing to public. We also work with integrators to test out the flow covering all edge cases before they release. Overall we are fine with taking this risk and relying on rescue function to recover funds while testing.  Description  Most functions of SocketGateway are payable, and can receive ether, which is processed in different ways, depending on the routes. A user might send ether to a payable function of SocketGateway with a wrong payload, either by mistake or because of an API bug. Let s illustrate the issue with the performAction of the 1inch route. However, this can be generalized to other routes.  src/SocketGateway.sol:L90-L97  function executeRoute(  uint32 routeId,  bytes calldata routeData,  bytes calldata eventData  ) external payable returns (bytes memory) {  (bool success, bytes memory result) = addressAt(routeId).delegatecall(  routeData  );  function performAction(  address fromToken,  address toToken,  uint256 amount,  address receiverAddress,  bytes calldata swapExtraData  ) external payable override returns (uint256) {  uint256 returnAmount;  if (fromToken != NATIVE_TOKEN_ADDRESS) {  ...  ...  (bool success, bytes memory result) = ONEINCH_AGGREGATOR.call(  swapExtraData //<-- here we do not use the value  );  ...  } else {  ....  (bool success, bytes memory result) = ONEINCH_AGGREGATOR.call{  value: amount  //<-- here we use the value  }(swapExtraData);  ...  ...  Assume the user sent some ETH, but sent a payload with fromToken != NATIVE_TOKEN_ADDRESS (and the user has already approved the gateway for fromToken). Then, the ether is not used in the transaction and remains stuck in the SocketGateway contract. This is because the function only executes the part of the code that transfers and swaps ERC-20 tokens, but not the part that handles ether.  Now, suppose another user calls the performAction function with fromToken == NATIVE_TOKEN_ADDRESS and provides enough gas to execute the function. Since there is ether stuck in the contract, this user can force the contract to use the stuck ether to execute the swap by sending the exact amount of ether stuck in the contract as the value of the transaction, effectively stealing the funds.  This is why it s important to ensure that ether is only accepted when it is needed and not left stuck in the contract, as it can be vulnerable to theft in future transactions.  One could be tempted to fix the issue by requiring that the gateway balance always equals 0 at the end of the transaction. However, this is not a good idea, as anyone could cause a Denial of Service in the gateway by sending a tiny amount of ETH.  One might also be tempted to fix this issue by requiring that msg.value == 0 iff fromToken != NATIVE_TOKEN_ADDRESS. However, this also poses a problem, as the gateway might execute multiple routes in a  for  loop. This could lead to reverting valid transactions (when both native and non-native tokens are involved).  The best way to solve this issue might be to compare the balance of the gateway before and after the transaction in all relevant functions. The balance should stay the same otherwise, something wrong happened, and we should revert the transaction. This could be implemented by adding a modifier in SocketGateway, that compares the balance of the gateway before and after the function call. Below is an example to illustrate the idea.  modifier checkGatewayBalance() {  uint256 initialBalance = address(this).balance;  _;  uint256 finalBalance = address(this).balance;  require(initialBalance == finalBalance, \"Gateway balance changed during execution\");  issue 6.1)  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.8 No Event Is Emitted When Invoking a Route Through the socketGateway Fallback Function ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#152. Further discussion about the scope of events in these cases is still ongoing.  Description  When a route is invoked through executeRoute, or executeRoutes functions, a SocketRouteExecuted event is emitted. However, a route can also be executed by invoking the fallback function of the socketGateway. And in that case, no event is emitted. This might impact off-chain systems that rely on those events.  Recommendation  Consider also emitting a SocketRouteExecuted event in case the route is invoked through the fallback function  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.9 Unused Error Codes. ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#148.  Description  SocketErrors.sol has errors that are defined but are not used:  error RouteAlreadyExist();  error ContractContainsNoCode();  error ControllerAlreadyExist();  error ControllerAddressIsZero();  It seems that they were created as errors that may have been expected to occur during the early stages of development, but the resulting architecture doesn t seem to have a place for them currently.  Examples  src/errors/SocketErrors.sol:L12-L19  error RouteAlreadyExist();  error SwapFailed();  error UnsupportedInterfaceId();  error ContractContainsNoCode();  error InvalidCelerRefund();  error CelerAlreadyRefunded();  error ControllerAlreadyExist();  error ControllerAddressIsZero();  Recommendation  Consider revisiting these errors and identifying whether they need to remain or can be removed.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.10 Inaccurate Interface. ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#149.  Description  ISocketGateway implies a bridge(uint32 routeId, bytes memory data) function, but there is no socket contract with a function like that, including the SocketGateway contract.  Examples  src/interfaces/ISocketGateway.sol:L32-L35  function bridge(  uint32 routeId,  bytes memory data  ) external payable returns (bytes memory);  Recommendation  Adjust the interface.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.11 Validate Array Length Matching Before Execution to Avoid Reverts ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#150 by adding the necessary array length checks.  Description  The Socket system not only aggregates different solutions via its routes and controllers but also allows to batch calls between them into one transaction. For example, a user may call swaps between several DEXs and then perform a bridge transfer.  As a result, the SocketGateway contract has many functions that accept multiple arrays that contain the necessary data for execution in their respective routes. However, these arrays need to be of the same length because individual elements in the arrays are intended to be matched at the same indices:  src/SocketGateway.sol:L196-L218  function executeRoutes(  uint32[] calldata routeIds,  bytes[] calldata dataItems,  bytes[] calldata eventDataItems  ) external payable {  uint256 routeIdslength = routeIds.length;  for (uint256 index = 0; index < routeIdslength; ) {  (bool success, bytes memory result) = addressAt(routeIds[index])  .delegatecall(dataItems[index]);  if (!success) {  assembly {  revert(add(result, 32), mload(result))  emit SocketRouteExecuted(routeIds[index], eventDataItems[index]);  unchecked {  ++index;  Note that in the above example function, all 3 different calldata arrays routeIds, dataItems, and eventDataItems were utilizing the same index to retrieve the correct element. A common practice in such cases is to confirm that the sizes of the arrays match before continuing with the execution of the rest of the transaction to avoid costly reverts that could happen due to  Index out of bounds  error.  Due to the aggregating and batching nature of the Socket system that may have its users rely on 3rd party offchain APIs to construct these array payloads, such as from APIs of the systems that Socket is integrating, a mishap in just any one of them could cause this issue.  Recommendation  Implement a check on the array lengths so they match.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.12 Destroyed Routes Eth Balances Will Be Left Locked in SocketDeployFactory ", "body": "  Resolution   Remediated as per the client team in   SocketDotTech/socket-ll-contracts#151 by adding rescue functions.  Description  SocketDeployFactory.destroy calls the killme function which in turn self-destructs the route and sends back any eth to the factory contract. However, these funds can not be claimed from the SocketDeployFactory contract.  Examples  src/deployFactory/SocketDeployFactory.sol:L170  function destroy(uint256 routeId) external onlyDisabler {  Recommendation  Make sure that these funds can be claimed.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "6.13 Possible Double Spends of msg.value in Code Paths That Include More Than One Delegatecall ", "body": "  Resolution  The client team has responded with the following note:  Adding the recommended CI/CD task to verify that future routes are delegate safe.  Description  The usage of msg.value multiple times in the context of a single transaction is dangerous and may lead to loss of funds as previously seen (in a different variation) in the Opyn hack. We were not able to find any concrete instance of the described issue, however, we do see how this pitfall may become an issue in future delegatee contracts.  Examples  Every code path that includes multiple delegatecalls, including:  SocketGateway.swapAndMultiBridge  the swapAndBridge function in all the different route contracts.  Recommendation  Consider implementing this recommendation.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/02/socket/"}, {"title": "5.1 VotingMachine - tryToMoveToValidating can lock up proposals    ", "body": "  Resolution  Fixed per our recommendation.  Description  After a vote was received, the proposal can move to a validating state if any of the votes pass the proposal s precReq value, referred to as the minimum threshold.  code/contracts/governance/VotingMachine.sol:L391  tryToMoveToValidating(_proposalId);  Inside the method tryToMoveToValidating each of the vote options are checked to see if they pass precReq. In case that happens, the proposal goes into the next stage, specifically Validating.  code/contracts/governance/VotingMachine.sol:L394-L407  /// @notice Function to move to Validating the proposal in the case the last vote action  ///  was done before the required votingBlocksDuration passed  /// @param _proposalId The id of the proposal  function tryToMoveToValidating(uint256 _proposalId) public {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING_STATUS_REQUIRED\");  if (_proposal.currentStatusInitBlock.add(_proposal.votingBlocksDuration) <= block.number) {  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  The method internalMoveToValidating checks the proposal s status to be Voting and proceeds to moving the proposal into Validating state.  code/contracts/governance/VotingMachine.sol:L270-L278  /// @notice Internal function to change proposalStatus from Voting to Validating  /// @param _proposalId The id of the proposal  function internalMoveToValidating(uint256 _proposalId) internal {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  _proposal.currentStatusInitBlock = block.number;  emit StatusChangeToValidating(_proposalId);  The problem appears if multiple vote options go past the minimum threshold. This is because the loop does not stop after the first found option and the loop will fail when the method internalMoveToValidating is called a second time.  code/contracts/governance/VotingMachine.sol:L401-L405  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  The method internalMoveToValidating fails the second time because the first time it is called, the proposal goes into the Validating state and the second time it is called, the require check fails.  code/contracts/governance/VotingMachine.sol:L274-L275  require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  This can lead to proposal lock-ups if there are enough votes to at least one option that pass the minimum threshold.  Recommendation  After moving to the Validating state return successfully.  function tryToMoveToValidating(uint256 _proposalId) public {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING_STATUS_REQUIRED\");  if (_proposal.currentStatusInitBlock.add(_proposal.votingBlocksDuration) <= block.number) {  for (uint256 i = 0; i <= COUNT_CHOICES; i++) {  if (_proposal.votes[i] > _proposal.precReq) {  internalMoveToValidating(_proposalId);  return; // <- this was added  An additional change can be done to internalMoveToValidating because it is called only in tryToMoveToValidating and the parent method already does the check.  /// @notice Internal function to change proposalStatus from Voting to Validating  /// @param _proposalId The id of the proposal  function internalMoveToValidating(uint256 _proposalId) internal {  Proposal storage _proposal = proposals[_proposalId];  // The line below can be removed  // require(_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY_ON_VOTING_STATUS\");  _proposal.proposalStatus = ProposalStatus.Validating;  _proposal.currentStatusInitBlock = block.number;  emit StatusChangeToValidating(_proposalId);  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "5.2 VotingMachine - verifyNonce should only allow the next nonce    ", "body": "  Resolution  Fixed per our recommendation.  Description  When a relayer calls submitVoteByRelayer they also need to provide a nonce. This nonce is cryptographicly checked against the provided signature. It is also checked again to be higher than the previous nonce saved for that voter.  code/contracts/governance/VotingMachine.sol:L232-L239  /// @notice Verifies the nonce of a voter on a proposal  /// @param _proposalId The id of the proposal  /// @param _voter The address of the voter  /// @param _relayerNonce The nonce submitted by the relayer  function verifyNonce(uint256 _proposalId, address _voter, uint256 _relayerNonce) public view {  Proposal storage _proposal = proposals[_proposalId];  require(_proposal.voters[_voter].nonce < _relayerNonce, \"INVALID_NONCE\");  When the vote is saved, the previous nonce is incremented.  code/contracts/governance/VotingMachine.sol:L387  voter.nonce = voter.nonce.add(1);  This leaves the opportunity to use the same signature to vote multiple times, as long as the provided nonce is higher than the incremented nonce.  Recommendation  The check should be more restrictive and make sure the consecutive nonce was provided.  require(_proposal.voters[_voter].nonce + 1 == _relayerNonce, \"INVALID_NONCE\");  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "5.3 VoteMachine - Cancelling vote does not increase the nonce    ", "body": "  Resolution  Fixed per our recommendation.  Description  A vote can be cancelled by calling cancelVoteByRelayer with the proposal ID, nonce, voter s address, signature and a hash of the sent params.  The parameters are hashed and checked against the signature correctly.  The nonce is part of these parameters and it is checked to be valid.  code/contracts/governance/VotingMachine.sol:L238  require(_proposal.voters[_voter].nonce < _relayerNonce, \"INVALID_NONCE\");  Once the vote is cancelled, the data is cleared but the nonce is not increased.  code/contracts/governance/VotingMachine.sol:L418-L434  if (_cachedVoter.balance > 0) {  _proposal.votes[_cachedVoter.vote] = _proposal.votes[_cachedVoter.vote].sub(_cachedVoter.balance.mul(_cachedVoter.weight));  _proposal.totalVotes = _proposal.totalVotes.sub(1);  voter.weight = 0;  voter.balance = 0;  voter.vote = 0;  voter.asset = address(0);  emit VoteCancelled(  _proposalId,  _voter,  _cachedVoter.vote,  _cachedVoter.asset,  _cachedVoter.weight,  _cachedVoter.balance,  uint256(_proposal.proposalStatus)  );  This means that in the future, the same signature can be used as long as the nonce is still higher than the current one.  Recommendation  Considering the recommendation from issue https://github.com/ConsenSys/aave-governance-dao-audit-2020-01/issues/4 is implemented, the nonce should also increase when the vote is cancelled. Otherwise the same signature can be replayed again.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "5.4 Possible lock ups with SafeMath multiplication   ", "body": "  Resolution  The situation described is unlikely to occur, and does not justify mitigations which might introduce other risks.  Description  In some cases using SafeMath can lead to a situation where a contract is locked up due to an unavoidable overflow.  It is theoretically possible that both the internalSubmitVote() and internalCancelVote() functions could become unusable by voters with a high enough balance, if the asset weighting is set extremely high.  Examples  This line in internalSubmitVote() could overflow if the voter s balance and the asset weight were sufficiently high:  code/contracts/governance/VotingMachine.sol:L379  uint256 _votingPower = _voterAssetBalance.mul(_assetWeight);  A similar situation occurs in internalCancelVote():  code/contracts/governance/VotingMachine.sol:L419-L420  _proposal.votes[_cachedVoter.vote] = _proposal.votes[_cachedVoter.vote].sub(_cachedVoter.balance.mul(_cachedVoter.weight));  _proposal.totalVotes = _proposal.totalVotes.sub(1);  Recommendation  This could be protected against by setting a maximum value for asset weights. In practice it is very unlikely to occur in this situation, but it could be introduced at some point in the future.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-governance-dao/"}, {"title": "6.1 Frontrunning attacks by the owner ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  There are few possible attack vectors by the owner:  All strategies have fees from rewards. In addition to that, the PancakeSwap strategy has deposit fees. The default deposit fees equal zero; the maximum is limited to 5%: wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L29-L33 uint256 constant MAXIMUM_DEPOSIT_FEE = 5e16; // 5% uint256 constant DEFAULT_DEPOSIT_FEE = 0e16; // 0%  uint256 constant MAXIMUM_PERFORMANCE_FEE = 50e16; // 50% uint256 constant DEFAULT_PERFORMANCE_FEE = 10e16; // 10% When a user deposits tokens, expecting to have zero deposit fees, the owner can frontrun the deposit and increase fees to 5%. If the deposit size is big enough, that may be a significant amount of money.  In the gulp function, the reward tokens are exchanged for the reserve tokens on the exchange: wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L218-L244 function gulp(uint256 _minRewardAmount) external onlyEOAorWhitelist nonReentrant { \tuint256 _pendingReward = _getPendingReward(); \tif (_pendingReward > 0) { \t\t_withdraw(0); \t} \t{ \t\tuint256 _totalReward = Transfers._getBalance(rewardToken); \t\tuint256 _feeReward = _totalReward.mul(performanceFee) / 1e18; \t\tTransfers._pushFunds(rewardToken, collector, _feeReward); \t} \tif (rewardToken != routingToken) { \t\trequire(exchange != address(0), \"exchange not set\"); \t\tuint256 _totalReward = Transfers._getBalance(rewardToken); \t\tTransfers._approveFunds(rewardToken, exchange, _totalReward); \t\tIExchange(exchange).convertFundsFromInput(rewardToken, routingToken, _totalReward, 1); \t} \tif (routingToken != reserveToken) { \t\trequire(exchange != address(0), \"exchange not set\"); \t\tuint256 _totalRouting = Transfers._getBalance(routingToken); \t\tTransfers._approveFunds(routingToken, exchange, _totalRouting); \t\tIExchange(exchange).joinPoolFromInput(reserveToken, routingToken, _totalRouting, 1); \t} \tuint256 _totalBalance = Transfers._getBalance(reserveToken); \trequire(_totalBalance >= _minRewardAmount, \"high slippage\"); \t_deposit(_totalBalance); } The owner can change the exchange parameter to the malicious address that steals tokens. The owner then calls gulp with _minRewardAmount==0, and all the rewards will be stolen. The same attack can be implemented in fee collectors and the buyback contract.  Recommendation  Use a timelock to avoid instant changes of the parameters.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.2 New deposits are instantly getting a share of undistributed rewards ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  When a new deposit is happening, the current pending rewards are not withdrawn and re-invested yet. And they are not taken into account when calculating the number of shares that the depositor receives. The number of shares is calculated as if there were no pending rewards. The other side of this issue is that all the withdrawals are also happening without considering the pending rewards. So currently, it makes more sense to withdraw right after gulp to gather the rewards. In addition to the general  unfairness  of the reward distribution during the deposit/withdrawal, there is also an attack vector created by this issue.  The Attack  If the deposit is made right before the gulp function is called, the rewards from the gulp are distributed evenly across all the current deposits, including the ones that were just recently made. So if the deposit-gulp-withdraw sequence is executed, the caller receives guaranteed profit. If the attacker also can execute these functions briefly (in one block or transaction) and take a huge loan to deposit a lot of tokens, almost all the rewards from the gulp will be stolen by the attacker. The easy 1-transaction attack with a flashloan can be done by the owner, miner, whitelisted contracts, or any contract if the onlyEOAorWhitelist modifier is disabled or stops working (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/3). Even if onlyEOAorWhitelist is working properly, anyone can take a regular loan to make the attack. The risk is not that big because no price manipulation is required. The price will likely remain the same during the attack (few blocks maximum).  Recommendation  If issue issue 6.3 is fixed while allowing anyone call the gulp contract, the best solution would be to include the gulp call at the beginning of the deposit and withdraw. In case of withdrawing, there should also be an option to avoid calling gulp as the emergency case.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.3 Proactive sandwiching of the gulp calls ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  Each strategy token contract provides a gulp method to fetch pending rewards, convert them into the reserve token and split up the balances. One share is sent to the fee collector as a performance fee, while the rest is deposited into the respective MasterChef contract to accumulate more rewards. Suboptimal trades are prevented by passing a minimum slippage value with the function call, which results in revert if the expected reserve token amount cannot be provided by the trade(s).  The slippage parameter and the trades performed in gulp open the function up to proactive sandwich attacks. The slippage parameter can be freely set by the attacker, resulting in the system performing arbitrarily bad trades based on how much the attacker can manipulate the liquidity of involved assets around the gulp function call.  This attack vector is significant under the following assumptions:  The exchange the trade is performed on allows significant changes in liquidity pools in a single transaction (e.g., not limiting transactions to X% of the pool amount),  The attacker can frontrun legitimate gulp calls with reasonable slippage values,  Trades are performed, i.e. when rewardToken != routingToken and/or routingToken != reserveToken hold true.  Examples  This affects the gulp functions in all the strategies:  PancakeSwapCompoundingStrategyToken  AutoFarmCompoundingStrategyToken  PantherSwapCompoundingStrategyToken  and also fees collectors and the buyback adapters:  PantherSwapBuybackAdapter  AutoFarmFeeCollectorAdapter  PancakeSwapFeeCollector  UniversalBuyback  Recommendation  There are different possible solutions to this issue and all have some tradeoffs. Initially, we came up with the following suggestion:  The onlyOwner modifier should be added to the gulp function to ensure only authorized parties with reasonable slippages can execute trades on behalf of the strategy contracts. Furthermore, additional slippage checks can be added to avoid unwanted behavior of authorized addresses, e.g., to avoid a bot setting unreasonable slippage values due to a software bug.  But in order to fix another issue (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/8), we came up with the alternative solution:  Use oracles to restrict users from calling the gulp function with unreasonable slippage (more than 5% from the oracle s moving average price). The side effect of that solution is that sometimes the outdated price will be used. That means that when the price crashes, nobody will be able to call the gulp.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.4 Expected amounts of tokens in the withdraw function ", "body": "  Resolution  Client s statement :  This issue did not really need fixing. The mitigation was already in place by depositing a tiny amount of the reserve into the contract, if necessary   Description  Every withdraw function in the strategy contracts is calculating the expected amount of the returned tokens before withdrawing them:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L200-L208  function withdraw(uint256 _shares, uint256 _minAmount) external onlyEOAorWhitelist nonReentrant  address _from = msg.sender;  (uint256 _amount, uint256 _withdrawalAmount, uint256 _netAmount) = _calcAmountFromShares(_shares);  require(_netAmount >= _minAmount, \"high slippage\");  _burn(_from, _shares);  _withdraw(_amount);  Transfers._pushFunds(reserveToken, _from, _withdrawalAmount);  After that, the contract is trying to transfer this pre-calculated amount to the msg.sender. It is never checked whether the intended amount was actually transferred to the strategy contract. If the amount is lower, that may result in reverting the withdraw function all the time and locking up tokens.  Even though we did not find any specific case of returning a different amount of tokens, it is still a good idea to handle this situation to minimize relying on the security of the external contracts.  Recommendation  There are a few options how to mitigate the issue:  Double-check the balance difference before and after the MasterChef s withdraw function is called.  Handle this situation in the emergency mode (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/11).  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.5 Emergency mode of the MasterChef contracts is not supported ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  All the underlying MasterChef contracts have the emergency withdrawal mode, which allows simpler withdrawal (excluding the rewards):  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public nonReentrant {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  uint256 amount = user.amount;  user.amount = 0;  user.rewardDebt = 0;  user.rewardLockedUp = 0;  user.nextHarvestUntil = 0;  pool.lpToken.safeTransfer(address(msg.sender), amount);  emit EmergencyWithdraw(msg.sender, _pid, amount);  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  pool.lpToken.safeTransfer(address(msg.sender), user.amount);  emit EmergencyWithdraw(msg.sender, _pid, user.amount);  user.amount = 0;  user.rewardDebt = 0;  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public nonReentrant {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  uint256 wantLockedTotal =  IStrategy(poolInfo[_pid].strat).wantLockedTotal();  uint256 sharesTotal = IStrategy(poolInfo[_pid].strat).sharesTotal();  uint256 amount = user.shares.mul(wantLockedTotal).div(sharesTotal);  IStrategy(poolInfo[_pid].strat).withdraw(msg.sender, amount);  pool.want.safeTransfer(address(msg.sender), amount);  emit EmergencyWithdraw(msg.sender, _pid, amount);  user.shares = 0;  user.rewardDebt = 0;  While it s hard to predict how and why the emergency mode can be enabled in the underlying MasterChef contracts, these functions are there for a reason, and it s safer to be able to use them. If some emergency happens and this is the only way to withdraw funds, the funds in the strategy contracts will be locked forever.  Recommendation  Add the emergency mode implementation.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.6 The capping mechanism for Panther token leads to increased fees ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  Panther token has a cap in transfer sizes, so any transfer in the contract is limited beforehand:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L218-L245  function gulp(uint256 _minRewardAmount) external onlyEOAorWhitelist nonReentrant  uint256 _pendingReward = _getPendingReward();  if (_pendingReward > 0) {  _withdraw(0);  uint256 __totalReward = Transfers._getBalance(rewardToken);  (uint256 _feeReward, uint256 _retainedReward) = _capFeeAmount(__totalReward.mul(performanceFee) / 1e18);  Transfers._pushFunds(rewardToken, buyback, _feeReward);  if (rewardToken != routingToken) {  require(exchange != address(0), \"exchange not set\");  uint256 _totalReward = Transfers._getBalance(rewardToken);  _totalReward = _capTransferAmount(rewardToken, _totalReward, _retainedReward);  Transfers._approveFunds(rewardToken, exchange, _totalReward);  IExchange(exchange).convertFundsFromInput(rewardToken, routingToken, _totalReward, 1);  if (routingToken != reserveToken) {  require(exchange != address(0), \"exchange not set\");  uint256 _totalRouting = Transfers._getBalance(routingToken);  _totalRouting = _capTransferAmount(routingToken, _totalRouting, _retainedReward);  Transfers._approveFunds(routingToken, exchange, _totalRouting);  IExchange(exchange).joinPoolFromInput(reserveToken, routingToken, _totalRouting, 1);  uint256 _totalBalance = Transfers._getBalance(reserveToken);  _totalBalance = _capTransferAmount(reserveToken, _totalBalance, _retainedReward);  require(_totalBalance >= _minRewardAmount, \"high slippage\");  _deposit(_totalBalance);  Fees here are calculated from the full amount of rewards (__totalReward ):  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L225  (uint256 _feeReward, uint256 _retainedReward) = _capFeeAmount(__totalReward.mul(performanceFee) / 1e18);  But in fact, if the amount of the rewards is too big, it will be capped, and the residuals will be  taxed  again during the next call of the gulp function. That behavior leads to multiple taxations of the same tokens, which means increased fees.  Recommendation  The best solution would be to cap __totalReward  first and then calculate fees from the capped value.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.7 The _capFeeAmount function is not working as intended ", "body": "  Resolution  Client s statement :  With the fix of 6.6 this code was removed and therefore no changes were required. \"  Description  Panther token has a limit on the transfer size. Because of that, all the Panther transfer values in the PantherSwapCompoundingStrategyToken are also capped beforehand. The following function is called to cap the size of fees:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L357-L366  function _capFeeAmount(uint256 _amount) internal view returns (uint256 _capped, uint256 _retained)  _retained = 0;  uint256 _limit = _calcMaxRewardTransferAmount();  if (_amount > _limit) {  _amount = _limit;  _retained = _amount.sub(_limit);  return (_amount, _retained);  This function should return the capped amount and the amount of retained tokens. But because the _amount is changed before calculating the _retained, the retained amount will always be 0.  Recommendation  Calculate the retained value before changing the amount.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.8 Stale split ratios in UniversalBuyback ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The gulp and pendingBurning functions of the UniversalBuyback contract use the hardcoded, constant values of DEFAULT_REWARD_BUYBACK1_SHARE and DEFAULT_REWARD_BUYBACK2_SHARE to determine the ratio the trade value is split with.  Consequently, any call to setRewardSplit to set a new ratio will be ineffective but still result in a ChangeRewardSplit event being emitted. This event can deceive system operators and users as it does not reflect the correct values of the contract.  Examples  wheat-v1-core-audit/contracts/UniversalBuyback.sol:L80-L81  uint256 _amount1 = _balance.mul(DEFAULT_REWARD_BUYBACK1_SHARE) / 1e18;  uint256 _amount2 = _balance.mul(DEFAULT_REWARD_BUYBACK2_SHARE) / 1e18;  wheat-v1-core-audit/contracts/UniversalBuyback.sol:L97-L98  uint256 _amount1 = _balance.mul(DEFAULT_REWARD_BUYBACK1_SHARE) / 1e18;  uint256 _amount2 = _balance.mul(DEFAULT_REWARD_BUYBACK2_SHARE) / 1e18;  Recommendation  Instead of the default values, rewardBuyback1Share and rewardBuyback2Share should be used.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.9 Future-proofness of the onlyEOAorWhitelist modifier ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The onlyEOAorWhitelist modifier is used in various locations throughout the code. It performs a check that asserts the message sender being equal to the transaction origin to assert the calling party is not a smart contract.  This approach may stop working if EIP-3074 and its AUTH and AUTHCALL opcodes get deployed.  While the OpenZeppelin reentrancy guard does not depend on tx.origin, the EOA check does. Its evasion can result in additional attack vectors such as flash loans opening up. It is noteworthy that preventing smart contract interaction with the protocol may limit its opportunities as smart contracts cannot integrate with it in the same way that GrowthDeFi integrates with its third-party service providers.  The onlyEOAorWhitelist modifier may give a false sense of security because it won t allow making a flash loan attack by most of the users. But the same attack can still be made by some people or with more risk:  The owner and the whitelisted contracts are not affected by the modifier.  The modifier can be disabled: **wheat-v1-core-audit/contracts/WhitelistGuard.sol:L21-L28** ```solidity modifier onlyEOAorWhitelist() { \tif (enabled) { \t\taddress _from = _msgSender(); \t\trequire(tx.origin == _from || whitelist.contains(_from), \"access denied\"); \t} \t_; } ```  And in the deployment script, this modifier is disabled for testing purposes, and it s important not to forget to turn it in on the production: wheat-v1-core-audit/migrations/02_deploy_contracts.js:L50 await pancakeSwapFeeCollector.setWhitelistEnabled(false); // allows testing  The attack can usually be split into multiple transactions. Miners can put these transactions closely together and don t take any additional risk. Regular users can take a risk, take the loan, and execute the attack in multiple transactions or even blocks.  Recommendation  It is strongly recommended to monitor the progress of this EIP and its potential implementation on the Binance Smart Chain. If this functionality gets enabled, the development team should update the contract system to use the new opcodes. We also strongly recommend relying less on the fact that only EOA will call the functions. It is better to write the code that can be called by the external smart contracts without compromising its security.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.10 Exchange owner might steal users  funds using reentrancy ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The practice of pulling funds from a user (by using safeTransferFrom) and then later pushing (some) of the funds back to the user occurs in various places in the Exchange contract. In case one of the used token contracts (or one of its dependent calls) externally calls the Exchange owner, the owner may utilize that to call back Exchange.recoverLostFunds and drain (some) user funds.  Examples  wheat-v1-core-audit/contracts/Exchange.sol:L80-L89  function convertFundsFromInput(address _from, address _to, uint256 _inputAmount, uint256 _minOutputAmount) external override returns (uint256 _outputAmount)  address _sender = msg.sender;  Transfers._pullFunds(_from, _sender, _inputAmount);  _inputAmount = Math._min(_inputAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  _outputAmount = UniswapV2ExchangeAbstraction._convertFundsFromInput(router, _from, _to, _inputAmount, _minOutputAmount);  _outputAmount = Math._min(_outputAmount, Transfers._getBalance(_to)); // deals with potential transfer tax  Transfers._pushFunds(_to, _sender, _outputAmount);  return _outputAmount;  wheat-v1-core-audit/contracts/Exchange.sol:L121-L130  function joinPoolFromInput(address _pool, address _token, uint256 _inputAmount, uint256 _minOutputShares) external override returns (uint256 _outputShares)  address _sender = msg.sender;  Transfers._pullFunds(_token, _sender, _inputAmount);  _inputAmount = Math._min(_inputAmount, Transfers._getBalance(_token)); // deals with potential transfer tax  _outputShares = UniswapV2LiquidityPoolAbstraction._joinPoolFromInput(router, _pool, _token, _inputAmount, _minOutputShares);  _outputShares = Math._min(_outputShares, Transfers._getBalance(_pool)); // deals with potential transfer tax  Transfers._pushFunds(_pool, _sender, _outputShares);  return _outputShares;  wheat-v1-core-audit/contracts/Exchange.sol:L99-L111  function convertFundsFromOutput(address _from, address _to, uint256 _outputAmount, uint256 _maxInputAmount) external override returns (uint256 _inputAmount)  address _sender = msg.sender;  Transfers._pullFunds(_from, _sender, _maxInputAmount);  _maxInputAmount = Math._min(_maxInputAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  _inputAmount = UniswapV2ExchangeAbstraction._convertFundsFromOutput(router, _from, _to, _outputAmount, _maxInputAmount);  uint256 _refundAmount = _maxInputAmount - _inputAmount;  _refundAmount = Math._min(_refundAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  Transfers._pushFunds(_from, _sender, _refundAmount);  _outputAmount = Math._min(_outputAmount, Transfers._getBalance(_to)); // deals with potential transfer tax  Transfers._pushFunds(_to, _sender, _outputAmount);  return _inputAmount;  wheat-v1-core-audit/contracts/Exchange.sol:L139-L143  function recoverLostFunds(address _token) external onlyOwner  uint256 _balance = Transfers._getBalance(_token);  Transfers._pushFunds(_token, treasury, _balance);  Recommendation  Reentrancy guard protection should be added to Exchange.convertFundsFromInput, Exchange.convertFundsFromOutput, Exchange.joinPoolFromInput, Exchange.recoverLostFunds at least, and in general to all public/external functions since gas price considerations are less relevant for contracts deployed on BSC.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "5.1 VaultConfig.setVaultConfig doesn t check all critical arguments ", "body": "  Resolution  Remediated per Notional s team notes in commit by adding the following checks:  Checks to ensure borrow currency and secondary currencies cannot change once set  Check to ensure liquidationRate does not exceed minCollateralRatioBPS  Check for maxBorrowMarketIndex was not added. The Notional team will review this parameter on a case-by-case basis as for some vaults borrowing idiosyncratic fCash may not be an issue  Description  The Notional Strategy Vaults need to get whitelisted and have specific Notional parameters set in order to interact with the rest of the Notional system. This is done through VaultAction.updateVault() where the owner address can provide a VaultConfigStorage calldata vaultConfig argument to either whitelist a new vault or change an existing one. While this is to be performed by a trusted privileged actor (the owner), and it could be assumed they are careful with their updates, the contracts themselves don t perform enough checks on the validity of the parameters, either in isolation or when compared against the existing vault state. Below are examples of arguments that should be better checked.  borrowCurrencyId  The borrowCurrencyId parameter gets provided to TokenHandler.getAssetToken() and TokenHandler.getUnderlyingToken() to retrieve its associated TokenStorage object and verify that the currency doesn t have transfer fees.  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L162-L164  Token memory assetToken = TokenHandler.getAssetToken(vaultConfig.borrowCurrencyId);  Token memory underlyingToken = TokenHandler.getUnderlyingToken(vaultConfig.borrowCurrencyId);  require(!assetToken.hasTransferFee && !underlyingToken.hasTransferFee);  However, these calls retrieve data from the mapping from storage which returns an empty struct for an unassigned currency ID. This would pass the check in the last require statement regarding the transfer fees and would successfully allow to set the currency even if isn t actually registered in Notional. The recommendation would be to check that the returned TokenStorage object has data inside of it, perhaps by checking the decimals on the token.  In the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the borrow currency without checking that the existing borrow and lending accounting has been cleared. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended.  liquidationRate and minCollateralRatioBPS  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L283  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  vaultAccount.vaultShares = vaultAccount.vaultShares.sub(vaultSharesToLiquidator);  maxBorrowMarketIndex  The current Strategy Vault implementation does not allow for idiosyncratic cash because it causes issues during exits as there are no active markets for the account s maturity. Therefore, the configuration shouldn t be set with maxBorrowMarketIndex >=3 as that would open up the 1 Year maturity for vault accounts that could cause idiosyncratic fCash. The recommendation would be to add that check.  secondaryBorrowCurrencies  Similarly to the borrowCurrencyId, there are few checks that actually determine that the secondaryBorrowCurrencies[] given are actually registered in Notional. This is, however, more inline with how some vaults are supposed to work as they may have no secondary currencies at all, such as when the secondaryBorrowCurrencies[] id is given as 0. In the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the secondary borrow currency without checking that the existing borrow and lending accounting has been cleared. For example, the VaultAction.updateSecondaryBorrowCapacity() function could be invoked on the new set of secondary currencies and simply increase the borrow there. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.2 Handle division by 0 ", "body": "  Resolution  Remediated per Notional s team notes in commit by adding the following checks:  Check to account for div by zero in settle vault account  Short circuit to ensure debtSharesToRepay is never zero. Divide by zero may still occur but this would signal a critical accounting issue  The Notional team also acknowledged that the contract will revert when vaultShareValue = 0. The team decided to not make any changes related to that since liquidation will not accomplish anything for an account with no vault share value.  Description  There are a few places in the code where division by zero may occur but isn t handled.  Examples  If the vault settles at exactly 0 value with 0 remaining strategy token value, there may be an unhandled division by zero trying to divide claims on the settled assets:  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L424-L436  int256 settledVaultValue = settlementRate.convertToUnderlying(residualAssetCashBalance)  .add(totalStrategyTokenValueAtSettlement);  // If the vault is insolvent (meaning residualAssetCashBalance < 0), it is necessarily  // true that totalStrategyTokens == 0 (meaning all tokens were sold in an attempt to  // repay the debt). That means settledVaultValue == residualAssetCashBalance, strategyTokenClaim == 0  // and assetCashClaim == totalAccountValue. Accounts that are still solvent will be paid from the  // reserve, accounts that are insolvent will have a totalAccountValue == 0.  strategyTokenClaim = totalAccountValue.mul(vaultState.totalStrategyTokens.toInt())  .div(settledVaultValue).toUint();  assetCashClaim = totalAccountValue.mul(residualAssetCashBalance)  .div(settledVaultValue);  If a vault account is entirely insolvent and its vaultShareValue is zero, there will be an unhandled division by zero during liquidation:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L281  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  If a vault account s secondary debt is being repaid when there is none, there will be an unhandled division by zero:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L661-L666  VaultSecondaryBorrowStorage storage balance =  LibStorage.getVaultSecondaryBorrow()[vaultConfig.vault][maturity][currencyId];  uint256 totalfCashBorrowed = balance.totalfCashBorrowed;  uint256 totalAccountDebtShares = balance.totalAccountDebtShares;  fCashToLend = debtSharesToRepay.mul(totalfCashBorrowed).div(totalAccountDebtShares).toInt();  While these cases may be unlikely today, this code could be reutilized in other circumstances later that could cause reverts and even disrupt operations more frequently.  Recommendation  Handle the cases where the denominator could be zero appropriately.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.3 Increasing a leveraged position in a vault with secondary borrow currency will revert ", "body": "  Resolution   Per Notional team s notes, they have rearranged if statement to ensure that increasing an existing position will work. The proposed solution was skipped as it creates issues with the   Commit  Description  From the client s specifications for the strategy vaults, we know that accounts should be able to increase their leveraged positions before maturity. This property will not hold for the vaults that require borrowing a secondary currency to enter a position. When an account opens its position in such vault for the first time, the VaultAccountSecondaryDebtShareStorage.maturity is set to the maturity an account has entered. When the account is trying to increase the debt position, an accounts current maturity will be checked, and since it is not set to 0, as in the case where an account enters the vault for the first time, nor it is smaller than the new maturity passed by an account as in case of a rollover, the code will revert.  Examples  contracts-v2/contracts/external/actions/VaultAction.sol:L226-L228  if (accountMaturity != 0) {  // Cannot roll to a shorter term maturity  require(accountMaturity < maturity);  Recommendation  In order to fix this issue, we recommend that < is replaced with <= so that account can enter the vault maturity the account is already in as well as the future once.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.4 Secondary Currency debt is not managed by the Notional Controller ", "body": "  Resolution   Remediated per Notional s team notes in   commit by adding valuation for secondary borrow within the vault.  Description  Some of the Notional Strategy Vaults may allow for secondary currencies to be borrowed as part of the same strategy. For example, a strategy may allow for USDC to be its primary borrow currency as well as have ETH as its secondary borrow currency.  In order to enter the vault, a user would have to deposit depositAmountExternal of the primary borrow currency when calling VaultAccountAction.enterVault(). This would allow the user to borrow with leverage, as long as the vaultConfig.checkCollateralRatio() check on that account succeeds, which is based on the initial deposit and borrow currency amounts. This collateral ratio check is then performed throughout that user account s lifecycle in that vault, such as when they try to roll their maturity, or when liquidators try to perform collateral checks to ensure there is no bad debt.  However, in the event that the vault has a secondary borrow currency as well, that additional secondary debt is not calculated as part of the checkCollateralRatio() check. The only debt that is being considered is the vaultAccount.fCash that corresponds to the primary borrow currency debt:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L313-L319  function checkCollateralRatio(  VaultConfig memory vaultConfig,  VaultState memory vaultState,  VaultAccount memory vaultAccount  ) internal view {  (int256 collateralRatio, /* */) = calculateCollateralRatio(  vaultConfig, vaultState, vaultAccount.account, vaultAccount.vaultShares, vaultAccount.fCash  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L278-L292  function calculateCollateralRatio(  VaultConfig memory vaultConfig,  VaultState memory vaultState,  address account,  uint256 vaultShares,  int256 fCash  ) internal view returns (int256 collateralRatio, int256 vaultShareValue) {  vaultShareValue = vaultState.getCashValueOfShare(vaultConfig, account, vaultShares);  // We do not discount fCash to present value so that we do not introduce interest  // rate risk in this calculation. The economic benefit of discounting will be very  // minor relative to the added complexity of accounting for interest rate risk.  // Convert fCash to a positive amount of asset cash  int256 debtOutstanding = vaultConfig.assetRate.convertFromUnderlying(fCash.neg());  Whereas the value of strategy tokens that belong to that user account are being calculated by calling IStrategyVault(vault).convertStrategyToUnderlying() on the associated strategy vault:  contracts-v2/contracts/internal/vaults/VaultState.sol:L314-L324  function getCashValueOfShare(  VaultState memory vaultState,  VaultConfig memory vaultConfig,  address account,  uint256 vaultShares  ) internal view returns (int256 assetCashValue) {  if (vaultShares == 0) return 0;  (uint256 assetCash, uint256 strategyTokens) = getPoolShare(vaultState, vaultShares);  int256 underlyingInternalStrategyTokenValue = _getStrategyTokenValueUnderlyingInternal(  vaultConfig.borrowCurrencyId, vaultConfig.vault, account, strategyTokens, vaultState.maturity  );  contracts-v2/contracts/internal/vaults/VaultState.sol:L296-L311  function _getStrategyTokenValueUnderlyingInternal(  uint16 currencyId,  address vault,  address account,  uint256 strategyTokens,  uint256 maturity  ) private view returns (int256) {  Token memory token = TokenHandler.getUnderlyingToken(currencyId);  // This will be true if the the token is \"NonMintable\" meaning that it does not have  // an underlying token, only an asset token  if (token.decimals == 0) token = TokenHandler.getAssetToken(currencyId);  return token.convertToInternal(  IStrategyVault(vault).convertStrategyToUnderlying(account, strategyTokens, maturity)  );  From conversations with the Notional team, it is assumed that this call returns the strategy token value subtracted against the secondary currencies debt, as is the case in the Balancer2TokenVault for example. In other words, when collateral ratio checks are performed, those strategy vaults that utilize secondary currency borrows would need to calculate the value of strategy tokens already accounting for any secondary debt. However, this is a dependency for a critical piece of the Notional controller s strategy vaults collateral checks.  Therefore, even though the strategy vaults  code and logic would be vetted before their whitelisting into the Notional system, they would still remain an external dependency with relatively arbitrary code responsible for the liquidation infrastructure that could lead to bad debt or incorrect liquidations if the vaults give inaccurate information, and thus potential loss of funds.  Recommendation  Specific strategy vault implementations using secondary borrows were not in scope of this audit. However, since the core Notional Vault system was, and it includes secondary borrow currency functionality, from the point of view of the larger Notional system it is recommended to include secondary debt checks within the Notional controller contract to reduce external dependency on the strategy vaults  logic.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.5 Vaults are unable to borrow single secondary currency ", "body": "  Resolution  Remediated per Notional s team notes.  Description  As was previously mentioned some strategies require borrowing one or two secondary currencies. All secondary currencies have to be whitelisted in the VaultConfig.secondaryBorrowCurrencies. Borrow operation on secondary currencies is performed in the borrowSecondaryCurrencyToVault(...) function. Due to a require statement in that function, vaults will only be able to borrow secondary currencies if both of the currencies are whitelisted in VaultConfig.secondaryBorrowCurrencies. Considering that many strategies will have just one secondary currency, this will prevent those strategies from borrowing any secondary assets.  Examples  contracts-v2/contracts/external/actions/VaultAction.sol:L214  require(currencies[0] != 0 && currencies[1] != 0);  Recommendation  contracts-v2/contracts/external/actions/VaultAction.sol:L202-L208  function borrowSecondaryCurrencyToVault(  address account,  uint256 maturity,  uint256[2] calldata fCashToBorrow,  uint32[2] calldata maxBorrowRate,  uint32[2] calldata minRollLendRate  ) external override returns (uint256[2] memory underlyingTokensTransferred) {  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.6 An account roll may be impossible if the vault is already at the maximum borrow capacity. ", "body": "  Resolution   Remediated per Notional s team notes in   commit by adding the ability for accounts to deposit during a roll vault position call to offset any additional cost that would put them over the maximum borrow capacity.  Description  One of the actions allowed in Notional Strategy Vaults is to roll an account s maturity to a later one by borrowing from a later maturity and repaying that into the debt of the earlier maturity.  However, this could cause an issue if the vault is at maximum capacity at the time of the roll. When an account performs this type of roll, the new borrow would have to be more than the existing debt simply because it has to at least cover the existing debt and pay for the borrow fees that get added on every new borrow. Since the whole vault was already at max borrow capacity before with the old, smaller borrow, this process would revert at the end after the new borrow as well once the process gets to VaultAccount.updateAccountfCash and VaultConfiguration.updateUsedBorrowCapacity:  contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L243-L257  function updateUsedBorrowCapacity(  address vault,  uint16 currencyId,  int256 netfCash  ) internal returns (int256 totalUsedBorrowCapacity) {  VaultBorrowCapacityStorage storage cap = LibStorage.getVaultBorrowCapacity()[vault][currencyId];  // Update the total used borrow capacity, when borrowing this number will increase (netfCash < 0),  // when lending this number will decrease (netfCash > 0).  totalUsedBorrowCapacity = int256(uint256(cap.totalUsedBorrowCapacity)).sub(netfCash);  if (netfCash < 0) {  // Always allow lending to reduce the total used borrow capacity to satisfy the case when the max borrow  // capacity has been reduced by governance below the totalUsedBorrowCapacity. When borrowing, it cannot  // go past the limit.  require(totalUsedBorrowCapacity <= int256(uint256(cap.maxBorrowCapacity)), \"Max Capacity\");  The result is that users won t able to roll while the vault is at max capacity. However, users may exit some part of their position to reduce their borrow, thereby reducing the overall vault borrow capacity, and then could execute the roll. A bigger problem would occur if the vault configuration got updated to massively reduce the borrow capacity, which would force users to exit their position more significantly with likely a much smaller chance at being able to roll.  Recommendation  Document this case so that users can realise that rolling may not always be an option. Perhaps consider adding ways where users can pay a small deposit, like on enterVault, to offset the additional difference in borrows and pay for fees so they can remain with essentially the same size position within Notional.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.7 Rollover might introduce economically impractical deposits of dust into a strategy ", "body": "  Resolution  Acknowledged with a note from the Notional s team:   This is true, however, vaults with secondary borrows may need to execute logic in order to roll positions forward. We will opt to not do any handling for dust amounts on the vault controller side and allow each vault to set its own dust thresholds.   Description  During the rollover of the strategy position into a longer maturity, several things happen:  Funds are borrowed from the longer maturity to pay off the debt and fees of the current maturity.  Strategy tokens that are associated with the current maturity are moved to the new maturity.  Any additional funds provided by the account are deposited into the strategy into a new longer maturity.  In reality, due to the AMM nature of the protocol, the funds borrowed from the new maturity could exceed the debt the account has in the current maturity, resulting in a non-zero vaultAccount.tempCashBalance. In that case, those funds will be deposited into the strategy. That would happen even if there are no external funds supplied by the account for the deposit.  It is possible that the dust in the temporary account balance will not cover the gas cost of triggering a full deposit call of the strategy.  Examples  contracts-v2/contracts/internal/vaults/VaultState.sol:L244-L246  uint256 strategyTokensMinted = vaultConfig.deposit(  vaultAccount.account, vaultAccount.tempCashBalance, vaultState.maturity, additionalUnderlyingExternal, vaultData  );  Recommendation  We suggest that additional checks are introduced that would check that on rollover vaultAccount.tempCashBalance + additionalUnderlyingExternal > 0 or larger than a certain threshold like minAccountBorrowSize for example.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "5.8 Significantly undercollateralized accounts will revert on liquidation ", "body": "  Resolution   Remediated per Notional s team notes in   commit by updating the calculations within  Description  The Notional Strategy Vaults utilise collateral to allow leveraged borrowing as long as the account passes the checkCollateralRatio check that ensures the overall account value is at least minCollateralRatio greater than its debts. If the account doesn t have sufficient collateral, it goes through a liquidation process where some of the collateral is sold to liquidators for the account s borrowed currency in attempt to improve the collateral ratio. However, if the account is severely undercollateralised, the entire account position is liquidated and given over to the liquidator:  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L282-L289  int256 depositRatio = maxLiquidatorDepositAssetCash.mul(vaultConfig.liquidationRate).div(vaultShareValue);  // Use equal to so we catch potential off by one issues, the deposit amount calculated inside the if statement  // below will round the maxLiquidatorDepositAssetCash down  if (depositRatio >= Constants.RATE_PRECISION) {  maxLiquidatorDepositAssetCash = vaultShareValue.divInRatePrecision(vaultConfig.liquidationRate);  // Set this to true to ensure that the account gets fully liquidated  mustLiquidateFullAmount = true;  Here, the liquidator will need to deposit exactly maxLiquidatorDepositAssetCash=vaultShareValue/liquidationRate in order to get all of account s assets, i.e. all of vaultShareValue in the form of vaultAccount.vaultShares. In fact, later this deposit will be set in vaultAccount.tempCashBalance:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L361-L380  int256 maxLiquidatorDepositExternal = assetToken.convertToExternal(maxLiquidatorDepositAssetCash);  // NOTE: deposit amount external is always positive in this method  if (depositAmountExternal < maxLiquidatorDepositExternal) {  // If this flag is set, the liquidator must deposit more cash in order to liquidate the account  // down to a zero fCash balance because it will fall under the minimum borrowing limit.  require(!mustLiquidateFull, \"Must Liquidate All Debt\");  } else {  // In the other case, limit the deposited amount to the maximum  depositAmountExternal = maxLiquidatorDepositExternal;  // Transfers the amount of asset tokens into Notional and credit it to the account's temp cash balance  int256 assetAmountExternalTransferred = assetToken.transfer(  liquidator, vaultConfig.borrowCurrencyId, depositAmountExternal  );  vaultAccount.tempCashBalance = vaultAccount.tempCashBalance.add(  assetToken.convertToInternal(assetAmountExternalTransferred)  );  Then the liquidator will get:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L281  uint256 vaultSharesToLiquidator;  vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()  .mul(vaultConfig.liquidationRate.toUint())  .mul(vaultAccount.vaultShares)  .div(vaultShareValue.toUint())  .div(uint256(Constants.RATE_PRECISION));  And if (except for precision and conversions) vaultAccount.tempCashBalance=maxLiquidatorDepositAssetCash=vaultShareValue/liquidationRate, then vaultSharesToLiquidator = (vaultAccount.tempCashBalance * liquidationRate * vaultAccount.vaultShares) / (vaultShareValue) becomes vaultSharesToLiquidator = ((vaultShareValue/liquidationRate)* liquidationRate * vaultAccount.vaultShares) / (vaultShareValue) = vaultAccount.vaultShares  In other words, the liquidator needed to deposit exactly vaultShareValue/liquidationRate to get all vaultAccount.vaultShares. However, the liquidator deposit (what would be in vaultAccount.tempCashBalance) needs to cover all of that account s debt, i.e. vaultAccount.fCash. At the end of the liquidation process, the vault account has its fCash and tempCash balances updated:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L289-L290  int256 fCashToReduce = vaultConfig.assetRate.convertToUnderlying(vaultAccount.tempCashBalance);  vaultAccount.updateAccountfCash(vaultConfig, vaultState, fCashToReduce, vaultAccount.tempCashBalance.neg());  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L77-L88  function updateAccountfCash(  VaultAccount memory vaultAccount,  VaultConfig memory vaultConfig,  VaultState memory vaultState,  int256 netfCash,  int256 netAssetCash  ) internal {  vaultAccount.tempCashBalance = vaultAccount.tempCashBalance.add(netAssetCash);  // Update fCash state on the account and the vault  vaultAccount.fCash = vaultAccount.fCash.add(netfCash);  require(vaultAccount.fCash <= 0);  While the vaultAccount.tempCashBalance gets cleared to 0, the vaultAccount.fCash amount only gets to vaultAccount.fCash = vaultAccount.fCash.add(netfCash), and netfCash=fCashToReduce = vaultConfig.assetRate.convertToUnderlying(vaultAccount.tempCashBalance), which, based on the constraints above essentially becomes:  vaultAccount.fCash=vaultAccount.fCash+vaultConfig.assetRate.convertToUnderlying(assetToken.convertToExternal(vaultShareValue/vaultConfig.liquidationRate))  However, later this account is set on storage, and, considering it is going through 100% liquidation, the account will necessarily be below minimum borrow size and will need to be at vaultAccount.fCash==0.  contracts-v2/contracts/internal/vaults/VaultAccount.sol:L52-L62  function setVaultAccount(VaultAccount memory vaultAccount, VaultConfig memory vaultConfig) internal {  mapping(address => mapping(address => VaultAccountStorage)) storage store = LibStorage  .getVaultAccount();  VaultAccountStorage storage s = store[vaultAccount.account][vaultConfig.vault];  // The temporary cash balance must be cleared to zero by the end of the transaction  require(vaultAccount.tempCashBalance == 0); // dev: cash balance not cleared  // An account must maintain a minimum borrow size in order to enter the vault. If the account  // wants to exit under the minimum borrow size it must fully exit so that we do not have dust  // accounts that become insolvent.  require(vaultAccount.fCash == 0 || vaultConfig.minAccountBorrowSize <= vaultAccount.fCash.neg(), \"Min Borrow\");  The case where vaultAccount.fCash>0 is taken care of by taking any extra repaid value and assigning it to the protocol, zeroing out the account s balances:  contracts-v2/contracts/external/actions/VaultAccountAction.sol:L293  if (vaultAccount.fCash > 0) vaultAccount.fCash = 0;  The case where vaultAccount.fCash < 0 is however not addressed, and instead the process will revert. This will occur whenever the vaultShareValue discounted with the liquidation rate is less than the fCash debt after all the conversions between external and underlying accounting. So, whenever the below is true, the account will not be liquidate-able. fCash>vaultShareValue/liquidationRate  This is an issue because the account is still technically solvent even though it is undercollateralized, but the current implementation would simply revert until the account is entirely insolvent (still without liquidation options) or its balances are restored enough to be liquidated fully.  Consider implementing a dynamic liquidation rate that becomes smaller the closer the account is to insolvency, thereby encouraging liquidators to promptly liquidate the accounts.  6 Strategy Vaults  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "6.1 Strategy vault swaps can be frontrun ", "body": "  Resolution  Acknowledged with a note from the Notional s team:  This is a large part of the diligence process for writing strategies   Description  Some strategy vaults utilize borrowing one currency, swapping it for another, and then using the new currency somewhere to generate yield. For example, the CrossCurrencyfCash strategy vault could borrow USDC, swap it for DAI, and then deposit that DAI back into Notional if the DAI lending interest rates are greater than USDC borrowing interest rates. However, during vault settlement the assets would need to be swapped back into the original borrow currency.  Since these vaults control the borrowed assets that go only into white-listed strategies, the Notional system allows users to borrow multiples of their posted collateral and claim the yield from a much larger position. As a result, these strategy vaults would likely have significant funds being borrowed and managed into these strategies.  However, as mentioned above, these strategies usually utilize a trading mechanism to swap borrowed currencies into whatever is required by the strategy, and these trades may be quite large. In fact, the BaseStrategyVault implementation contains functions that interact with Notional s trading module to assist with those swaps:  strategy-vaults/contracts/vaults/BaseStrategyVault.sol:L100-L127  /// @notice Can be used to delegate call to the TradingModule's implementation in order to execute  /// a trade.  function _executeTrade(  uint16 dexId,  Trade memory trade  ) internal returns (uint256 amountSold, uint256 amountBought) {  (bool success, bytes memory result) = nProxy(payable(address(TRADING_MODULE))).getImplementation()  .delegatecall(abi.encodeWithSelector(ITradingModule.executeTrade.selector, dexId, trade));  require(success);  (amountSold, amountBought) = abi.decode(result, (uint256, uint256));  /// @notice Can be used to delegate call to the TradingModule's implementation in order to execute  /// a trade.  function _executeTradeWithDynamicSlippage(  uint16 dexId,  Trade memory trade,  uint32 dynamicSlippageLimit  ) internal returns (uint256 amountSold, uint256 amountBought) {  (bool success, bytes memory result) = nProxy(payable(address(TRADING_MODULE))).getImplementation()  .delegatecall(abi.encodeWithSelector(  ITradingModule.executeTradeWithDynamicSlippage.selector,  dexId, trade, dynamicSlippageLimit  );  require(success);  (amountSold, amountBought) = abi.decode(result, (uint256, uint256));  Although some strategies may manage stablecoin <-> stablecoin swaps that typically would incur low slippage, large size trades could still suffer from low on-chain liquidity and end up getting frontrun and  sandwiched  by MEV bots or other actors, thereby extracting maximum amount from the strategy vault swaps as slippage permits. This could be especially significant during vaults  settlements, that can be initiated by anyone, as lending currencies may be swapped in large batches and not do it on a per-account basis. For example with the CrossCurrencyfCash vault, it can only enter settlement if all strategy tokens (lending currency in this case) are gone and swapped back into the borrow currency:  strategy-vaults/contracts/vaults/CrossCurrencyfCashVault.sol:L141-L143  if (vaultState.totalStrategyTokens == 0) {  NOTIONAL.settleVault(address(this), maturity);  As a result, in addition to the risk of stablecoins  getting off-peg, unfavorable market liquidity conditions and arbitrage-seeking actors could eat into the profits generated by this strategy as per the maximum allowed slippage. However, during settlement the strategy vaults don t have the luxury of waiting for the right conditions to perform the trade as the borrows need to repaid at their maturities.  So, the profitability of the vaults, and therefore users, could suffer due to potential low market liquidity allowing high slippage and risks of being frontrun with the chosen strategy vaults  currencies.  Recommendation  Ensure that the currencies chosen to generate yield in the strategy vaults have sufficient market liquidity on exchanges allowing for low slippage swaps.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "6.2 Cross currency strategy should not have same lend and borrow currencies ", "body": "  Description  Cross currency strategy currently takes lend and borrow currencies as the initialization arguments. Due to the way strategy and TradingModule are implemented, the strategy will not operate correctly if lend and borrow currencies are the same. Despite those arguments being passed exclusively by the Notional team, there is still a possibility of incorrect arguments being used.  Examples  strategy-vaults/contracts/vaults/CrossCurrencyfCashVault.sol:L77-L82  function initialize(  string memory name_,  uint16 borrowCurrencyId_,  uint16 lendCurrencyId_,  uint64 settlementSlippageLimit_  ) external initializer {  Recommendation  We suggest adding a require check in the initialization function of the CrossCurrencyfCashVault.sol that will ensure that lend and borrow currencies are different.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/07/notional-finance/"}, {"title": "3.1 TribalChief - A wrong user.rewardDebt value is calculated during the withdrawFromDeposit function call ", "body": "  Description  When withdrawing a single deposit, the reward debt is updated:  contracts/staking/TribalChief.sol:L468-L474  uint128 virtualAmountDelta = uint128( ( amount * poolDeposit.multiplier ) / SCALE_FACTOR );  // Effects  poolDeposit.amount -= amount;  user.rewardDebt = user.rewardDebt - toSigned128(user.virtualAmount * pool.accTribePerShare) / toSigned128(ACC_TRIBE_PRECISION);  user.virtualAmount -= virtualAmountDelta;  pool.virtualTotalSupply -= virtualAmountDelta;  Instead of the user.virtualAmount in reward debt calculation, the virtualAmountDelta  should be used. Because of that bug, the reward debt is much lower than it would be, which means that the reward itself will be much larger during the harvest. By making multiple deposit-withdraw actions, any user can steal all the Tribe tokens from the contract.  Recommendation  Use the virtualAmountDelta instead of the user.virtualAmount.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.2 TribalChief - Setting the totalAllocPoint to zero shouldn t be allowed ", "body": "  Description  TribalChief.updatePool will revert in the case totalAllocPoint = 0, which will essentially cause users  funds and rewards to be locked.  Recommendation  TribalChief.add and TribalChief.set should assert that totalAllocPoint > 0. A similar validation check should be added to TribalChief.updatePool as well.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.3 TribalChief - Unlocking users  funds in a pool where a multiplier has been increased is missing ", "body": "  Description  When a user deposits funds to a pool, the current multiplier in use for this pool is being stored locally for this deposit. The value that is used later in a withdrawal operation is the local one, and not the one that is changing when a governor calls governorAddPoolMultiplier. It means that a decrease in the multiplier value for a given pool does not affect users that already deposited, but an increase does. Users that had already deposited should have the right to withdraw their funds when the multiplier for their pool increases by the governor.  Examples  code/contracts/staking/TribalChief.sol:L143-L158  function governorAddPoolMultiplier(  uint256 _pid,  uint64 lockLength,  uint64 newRewardsMultiplier  ) external onlyGovernor {  PoolInfo storage pool = poolInfo[_pid];  uint256 currentMultiplier = rewardMultipliers[_pid][lockLength];  // if the new multplier is less than the current multiplier,  // then, you need to unlock the pool to allow users to withdraw  if (newRewardsMultiplier < currentMultiplier) {  pool.unlocked = true;  rewardMultipliers[_pid][lockLength] = newRewardsMultiplier;  emit LogPoolMultiplier(_pid, lockLength, newRewardsMultiplier);  Recommendation  Replace the < operator with > in TribalChief line 152.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.4 TribalChief - Unsafe down-castings ", "body": "  Description  TribalChief consists of multiple unsafe down-casting operations. While the usage of types that can be packed into a single storage slot is more gas efficient, it may introduce hidden risks in some cases that can lead to loss of funds.  Examples  Various instances in TribalChief, including (but not necessarily only) :  code/contracts/staking/TribalChief.sol:L429  user.rewardDebt = int128(user.virtualAmount * pool.accTribePerShare) / toSigned128(ACC_TRIBE_PRECISION);  code/contracts/staking/TribalChief.sol:L326  pool.accTribePerShare = uint128(pool.accTribePerShare + ((tribeReward * ACC_TRIBE_PRECISION) / virtualSupply));  code/contracts/staking/TribalChief.sol:L358  userPoolData.rewardDebt += int128(virtualAmountDelta * pool.accTribePerShare) / toSigned128(ACC_TRIBE_PRECISION);  Recommendation  Given the time constraints of this audit engagement, we could not verify the implications and provide mitigation actions for each of the unsafe down-castings operations. However, we do recommend to either use numeric types that use 256 bits, or to add proper validation checks and handle these scenarios to avoid silent over/under-flow errors. Keep in mind that reverting these scenarios can sometimes lead to a denial of service, which might be harmful in some cases.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.5 EthCompoundPCVDeposit - should provide means to recover ETH ", "body": "  Description  The CToken to be used is configured on EthCompoundPCVDeposit deployment. It is not checked, whether the provided CToken address is actually a valid CToken.  If the configured CToken ceases to work correctly (e.g. CToken.mint|redeem* disabled or the configured CToken address is invalid), ETH held by the contract may be locked up.  Recommendation  In CompoundPCVDepositBase consider verifying, that the CToken constructor argument is actually a valid CToken by checking require(ctoken.isCToken(), \"not a valid CToken\").  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.6 TribalChief - Governor decrease of pool s allocation point should unlock depositors  funds ", "body": "  Description  When the TribalChief governor decreases the ratio between the allocation point (PoolInfo.allocPoint) and the total allocation point (totalAllocPoint) for a specific pool (either be directly decreasing PoolInfo.allocPoint of a given pool, or by increasing this value for other pools), the total reward for this pool is decreased as well. Depositors should be able to withdraw their funds immediately after this kind of change.  Examples  code/contracts/staking/TribalChief.sol:L252-L261  function set(uint256 _pid, uint128 _allocPoint, IRewarder _rewarder, bool overwrite) public onlyGovernor {  totalAllocPoint = (totalAllocPoint - poolInfo[_pid].allocPoint) + _allocPoint;  poolInfo[_pid].allocPoint = _allocPoint.toUint64();  if (overwrite) {  rewarder[_pid] = _rewarder;  emit LogSetPool(_pid, _allocPoint, overwrite ? _rewarder : rewarder[_pid], overwrite);  Recommendation  Make sure that depositors  funds are unlocked for pools that affected negatively by calling TribalChief.set.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.7 TribalChief - new block reward retrospectively takes effect on pools that have not been updated recently ", "body": "  Description  When the governor updates the block reward tribalChiefTribePerBlock the new reward is applied for the outstanding duration of blocks in updatePool. This means, if a pool hasn t updated in a while (unlikely) the new block reward is retrospectively applied to the pending duration instead of starting from when the block reward changed.  Examples  rewards calculation  code/contracts/staking/TribalChief.sol:L323-L327  if (virtualSupply > 0) {  uint256 blocks = block.number - pool.lastRewardBlock;  uint256 tribeReward = (blocks * tribePerBlock() * pool.allocPoint) / totalAllocPoint;  pool.accTribePerShare = uint128(pool.accTribePerShare + ((tribeReward * ACC_TRIBE_PRECISION) / virtualSupply));  updating the block reward  code/contracts/staking/TribalChief.sol:L111-L116  /// @notice Allows governor to change the amount of tribe per block  /// @param newBlockReward The new amount of tribe per block to distribute  function updateBlockReward(uint256 newBlockReward) external onlyGovernor {  tribalChiefTribePerBlock = newBlockReward;  emit NewTribePerBlock(newBlockReward);  Recommendation  It is recommended to update pools before changing the block reward. Document and make users aware that the new reward is applied to the outstanding duration when calling updatePool.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.8 TribalChief - duplicate import SafeERC20 ", "body": "  Description  Duplicate import for SafeERC20.  Examples  code/contracts/staking/TribalChief.sol:L7-L8  import \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";  import \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";  Recommendation  Remove duplicate import line.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.9 TribalChief - resetRewards should emit an event ", "body": "  Description  The method resetRewards silently resets a pools tribe allocation.  Examples  code/contracts/staking/TribalChief.sol:L263-L275  /// @notice Reset the given pool's TRIBE allocation to 0 and unlock the pool. Can only be called by the governor or guardian.  /// @param _pid The index of the pool. See `poolInfo`.  function resetRewards(uint256 _pid) public onlyGuardianOrGovernor {  // set the pool's allocation points to zero  totalAllocPoint = (totalAllocPoint - poolInfo[_pid].allocPoint);  poolInfo[_pid].allocPoint = 0;  // unlock all staked tokens in the pool  poolInfo[_pid].unlocked = true;  // erase any IRewarder mapping  rewarder[_pid] = IRewarder(address(0));  Recommendation  For transparency and to create an easily accessible audit trail of events consider emitting an event when resetting a pools allocation.  4 Recommendations  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "4.1 EthCompoundPCVDeposit - stick to upstream interface contract names", "body": "  Recommendation  Stick to the original upstream interface names to make clear with which external system the contract interacts with. Rename CEth to CEther. See original upstream interface name.  code/contracts/pcv/compound/EthCompoundPCVDeposit.sol:L6-L8  interface CEth {  function mint() external payable;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "4.2 CompoundPCVDepositBase - verify provided CToken address is actually a CToken", "body": "  Recommendation  The ctoken address provided when deploying a new *CompoundPCVDeposit is never validated. Consider adding the following check: require(_cToken.isCToken, \"not a valid CToken\").  code/contracts/pcv/compound/CompoundPCVDepositBase.sol:L25-L30  constructor(  address _core,  address _cToken  ) CoreRef(_core) {  cToken = CToken(_cToken);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "4.3 CompoundPCV - documentation & testing", "body": "  Recommendation  Currently, the PCV flavor is only unit-tested using a mocked CToken. Consider providing integration tests that actually integrate and operate it in a compound test environment.  Provide a specification. & documentation describing the roles and functionality of the contract. Who deployes the PCVDeposit contract? Who Deploys the CToken and therefore may be in control of certain adminOnly functions of the CToken? What are the requirements for a CToken to be usable with CompoundPCVDeposit (listed/unlisted, \u2026)? Who has the potential power to borrow assets on behalf of the collateral provided?  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "4.4 TribalChief - immutable vs constant", "body": "  Recommendation  Constant state variables that are not initialized with the constructor can be constant instead of immutable.  code/contracts/staking/TribalChief.sol:L88-L90  uint256 private immutable ACC_TRIBE_PRECISION = 1e12;  /// exponent for rewards multiplier  uint256 public immutable SCALE_FACTOR = 1e18;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "4.5 TribalChief - governorAddPoolMultiplier should emit a PoolLocked event", "body": "  Description  Users should be notified if the pool gets unlocked during a call to governorAddPoolMultiplier. Consider emitting a PoolLocked(false) event.  code/contracts/staking/TribalChief.sol:L143-L158  function governorAddPoolMultiplier(  uint256 _pid,  uint64 lockLength,  uint64 newRewardsMultiplier  ) external onlyGovernor {  PoolInfo storage pool = poolInfo[_pid];  uint256 currentMultiplier = rewardMultipliers[_pid][lockLength];  // if the new multplier is less than the current multiplier,  // then, you need to unlock the pool to allow users to withdraw  if (newRewardsMultiplier < currentMultiplier) {  pool.unlocked = true;  rewardMultipliers[_pid][lockLength] = newRewardsMultiplier;  emit LogPoolMultiplier(_pid, lockLength, newRewardsMultiplier);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "4.6 TribalChief - updatePool invocation inside _harvest should be moved to harvest instead", "body": "  Description  When TribalChief.withdrawAllAndHarvest is executed, there s a redundant invocation of TribalChief.updatePool that caused by TribalChief._harvest, that can be moved to TribalChief.harvest instead.  Examples  code/contracts/staking/TribalChief.sol:L485-L515  function _harvest(uint256 pid, address to) private {  updatePool(pid);  PoolInfo storage pool = poolInfo[pid];  UserInfo storage user = userInfo[pid][msg.sender];  // assumption here is that we will never go over 2^128 -1  int256 accumulatedTribe = int256( uint256(user.virtualAmount) * uint256(pool.accTribePerShare) ) / int256(ACC_TRIBE_PRECISION);  // this should never happen  require(accumulatedTribe >= 0 || (accumulatedTribe - user.rewardDebt) < 0, \"negative accumulated tribe\");  uint256 pendingTribe = uint256(accumulatedTribe - user.rewardDebt);  // if pending tribe is ever negative, revert as this can cause an underflow when we turn this number to a uint  require(pendingTribe.toInt256() >= 0, \"pendingTribe is less than 0\");  // Effects  user.rewardDebt = int128(accumulatedTribe);  // Interactions  if (pendingTribe != 0) {  TRIBE.safeTransfer(to, pendingTribe);  IRewarder _rewarder = rewarder[pid];  if (address(_rewarder) != address(0)) {  _rewarder.onSushiReward( pid, msg.sender, to, pendingTribe, user.virtualAmount);  emit Harvest(msg.sender, pid, pendingTribe);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/fei-tribechief/"}, {"title": "3.1 GenesisGroup.commit overwrites previously-committed values ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#16.  Description  commit allows anyone to commit purchased FGEN to a swap that will occur once the genesis group is launched. This commitment may be performed on behalf of other users, as long as the calling account has sufficient allowance:  code/contracts/genesis/GenesisGroup.sol:L87-L94  function commit(address from, address to, uint amount) external override onlyGenesisPeriod {  burnFrom(from, amount);  committedFGEN[to] = amount;  totalCommittedFGEN += amount;  emit Commit(from, to, amount);  The amount stored in the recipient s committedFGEN balance overwrites any previously-committed value. Additionally, this also allows anyone to commit an amount of  0  to any account, deleting their commitment entirely.  Recommendation  Ensure the committed amount is added to the existing commitment.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.2 Purchasing and committing still possible after launch ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#11.  Description  Even after GenesisGroup.launch has successfully been executed, it is still possible to invoke GenesisGroup.purchase and GenesisGroup.commit.  Recommendation  Consider adding validation in GenesisGroup.purchase and GenesisGroup.commit to make sure that these functions cannot be called after the launch.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.3 UniswapIncentive overflow on pre-transfer hooks ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#15.  Description  Before a token transfer is performed, Fei performs some combination of mint/burn operations via UniswapIncentive.incentivize:  code/contracts/token/UniswapIncentive.sol:L49-L65  function incentivize(  address sender,  address receiver,  address operator,  uint amountIn  ) external override onlyFei {  updateOracle();  if (isPair(sender)) {  incentivizeBuy(receiver, amountIn);  if (isPair(receiver)) {  require(isSellAllowlisted(sender) || isSellAllowlisted(operator), \"UniswapIncentive: Blocked Fei sender or operator\");  incentivizeSell(sender, amountIn);  Both incentivizeBuy and incentivizeSell calculate buy/sell incentives using overflow-prone math, then mint / burn from the target according to the results. This may have unintended consequences, like allowing a caller to mint tokens before transferring them, or burn tokens from their recipient.  Examples  incentivizeBuy calls getBuyIncentive to calculate the final minted value:  code/contracts/token/UniswapIncentive.sol:L173-L186  function incentivizeBuy(address target, uint amountIn) internal ifMinterSelf {  if (isExemptAddress(target)) {  return;  (uint incentive, uint32 weight,  Decimal.D256 memory initialDeviation,  Decimal.D256 memory finalDeviation) = getBuyIncentive(amountIn);  updateTimeWeight(initialDeviation, finalDeviation, weight);  if (incentive != 0) {  fei().mint(target, incentive);  getBuyIncentive calculates price deviations after casting amount to an int256, which may overflow:  code/contracts/token/UniswapIncentive.sol:L128-L134  function getBuyIncentive(uint amount) public view override returns(  uint incentive,  uint32 weight,  Decimal.D256 memory initialDeviation,  Decimal.D256 memory finalDeviation  ) {  (initialDeviation, finalDeviation) = getPriceDeviations(-1 * int256(amount));  Recommendation  Ensure casts in getBuyIncentive and getSellPenalty do not overflow.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.4 BondingCurve allows users to acquire FEI before launch ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#59  Description  BondingCurve.allocate allocates the protocol s held PCV, then calls _incentivize, which rewards the caller with FEI if a certain amount of time has passed:  code-update/contracts/bondingcurve/BondingCurve.sol:L180-L186  /// @notice if window has passed, reward caller and reset window  function _incentivize() internal virtual {  if (isTimeEnded()) {  _initTimed(); // reset window  fei().mint(msg.sender, incentiveAmount);  allocate can be called before genesis launch, as long as the contract holds some nonzero PCV. By force-sending the contract 1 wei, anyone can bypass the majority of checks and actions in allocate, and mint themselves FEI each time the timer expires.  Recommendation  Prevent allocate from being called before genesis launch.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.5 Timed.isTimeEnded returns true if the timer has not been initialized ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#62  Description  Timed initialization is a 2-step process:  Timed.duration is set in the constructor: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/utils/Timed.sol#L15-L20  Timed.startTime is set when the method _initTimed is called: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/utils/Timed.sol#L43-L46  Before this second method is called, isTimeEnded() calculates remaining time using a startTime of 0, resulting in the method returning true for most values, even though the timer has not technically been started.  Recommendation  If Timed has not been initialized, isTimeEnded() should return false, or revert  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.6 Overflow/underflow protection ", "body": "  Resolution   This was partially addressed in   fei-protocol/fei-protocol-core#17 by using  Description  Having overflow/underflow vulnerabilities is very common for smart contracts. It is usually mitigated by using SafeMath or using solidity version ^0.8 (after solidity 0.8 arithmetical operations already have default overflow/underflow protection).  In this code, many arithmetical operations are used without the  safe  version. The reasoning behind it is that all the values are derived from the actual ETH values, so they can t overflow.  On the other hand, some operations can t be checked for overflow/underflow without going much deeper into the codebase that is out of scope:  code/contracts/genesis/GenesisGroup.sol:L131  uint totalGenesisTribe = tribeBalance() - totalCommittedTribe;  Recommendation  In our opinion, it is still safer to have these operations in a safe mode. So we recommend using SafeMath or solidity version ^0.8 compiler.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.7 Unchecked return value for IWETH.transfer call ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#12.  Description  In EthUniswapPCVController, there is a call to IWETH.transfer that does not check the return value:  code/contracts/pcv/EthUniswapPCVController.sol:L122  weth.transfer(address(pair), amount);  It is usually good to add a require-statement that checks the return value or to use something like safeTransfer; unless one is sure the given token reverts in case of a failure.  Recommendation  Consider adding a require-statement or using safeTransfer.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.8 GenesisGroup.emergencyExit remains functional after launch ", "body": "  Resolution   This was partially addressed in   fei-protocol/fei-protocol-core#14 and  fei-protocol/fei-protocol-core#13 by addressing the last two recommendations.  Description  emergencyExit is intended as an escape mechanism for users in the event the genesis launch method fails or is frozen. emergencyExit becomes callable 3 days after launch is callable. These two methods are intended to be mutually-exclusive, but are not: either method remains callable after a successful call to the other.  This may result in accounting edge cases. In particular, emergencyExit fails to decrease totalCommittedFGEN by the exiting user s commitment:  code/contracts/genesis/GenesisGroup.sol:L185-L188  burnFrom(from, amountFGEN);  committedFGEN[from] = 0;  payable(to).transfer(total);  As a result, calling launch after a user performs an exit will incorrectly calculate the amount of FEI to swap:  code/contracts/genesis/GenesisGroup.sol:L165-L168  uint amountFei = feiBalance() * totalCommittedFGEN / (totalSupply() + totalCommittedFGEN);  if (amountFei != 0) {  totalCommittedTribe = ido.swapFei(amountFei);  Recommendation  Ensure launch cannot be called if emergencyExit has been called  Ensure emergencyExit cannot be called if launch has been called  In emergencyExit, reduce totalCommittedFGEN by the exiting user s committed amount  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.9 Unchecked return value for transferFrom calls ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#12.  Description  There are two transferFrom calls that do not check the return value (some tokens signal failure by returning false):  code/contracts/pool/Pool.sol:L121  stakedToken.transferFrom(from, address(this), amount);  code/contracts/genesis/IDO.sol:L58  fei().transferFrom(msg.sender, address(pair), amountFei);  It is usually good to add a require-statement that checks the return value or to use something like safeTransferFrom; unless one is sure the given token reverts in case of a failure.  Recommendation  Consider adding a require-statement or using safeTransferFrom.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.10 GovernorAlpha proposals may be canceled by the proposer, even after they have been accepted and queued ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#61  Description  GovernorAlpha allows proposals to be canceled via cancel. To cancel a proposal, two conditions must be met by the proposer:  The proposal should not already have been executed: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/dao/GovernorAlpha.sol#L206-L208  The proposer must have under proposalThreshold() TRIBE balance: https://github.com/ConsenSys/fei-protocol-audit-2021-01/blob/d31114d834e62b4f3d4fa7b1c0b0c70fbff623a4/code-update/contracts/dao/GovernorAlpha.sol#L210-L211  Recommendation  Prevent proposals from being canceled unless they are in the Pending or Active states.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.11 Pool: claiming to the pool itself causes accounting issues ", "body": "  Resolution   This was addressed in   fei-protocol/fei-protocol-core#57  Description  In Pool.sol, claim(address from, address to) is used to claim staking rewards and send them to a destination address to:  code-update/contracts/pool/Pool.sol:L229-L238  function _claim(address from, address to) internal returns (uint256) {  (uint256 amountReward, uint256 amountPool) = redeemableReward(from);  require(amountPool != 0, \"Pool: User has no redeemable pool tokens\");  _burnFrom(from, amountPool);  _incrementClaimed(amountReward);  rewardToken.transfer(to, amountReward);  return amountReward;  If the destination address to is the pool itself, the pool will burn tokens and increment the amount of tokens claimed, then transfer the reward tokens to itself.  Recommendation  Prevent claims from specifying the pool as a destination.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.12 Assertions that can fail ", "body": "  Description  In UniswapSingleEthRouter there are two assert-statements that may fail:  code/contracts/router/UniswapSingleEthRouter.sol:L21  assert(msg.sender == address(WETH)); // only accept ETH via fallback from the WETH contract  code/contracts/router/UniswapSingleEthRouter.sol:L48  assert(IWETH(WETH).transfer(address(PAIR), amountIn));  Since they do some sort of input validation it might be good to replace them with require-statements. I would only use asserts for checks that should never fail and failure would constitute a bug in the code.  Recommendation  Consider replacing the assert-statements with require-statements. An additional benefit is that this will not result in consuming all the gas in case of a violation.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "3.13 Simplify API of GenesisGroup.purchase ", "body": "  Description  The API of GenesisGroup.purchase could be simplified by not including the value parameter that is required to be equivalent to msg.value:  code/contracts/genesis/GenesisGroup.sol:L79  require(msg.value == value, \"GenesisGroup: value mismatch\");  Using msg.value might make the API more explicit and avoid requiring msg.value == value. It can also save some gas due to fewer inputs and fewer checks.  Recommendation  Consider dropping the value parameter and changing the code to use msg.value instead.  4 Infrastructure Security Assessment  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.1 Clickjacking and Missing Content Security Policy    ", "body": "  Resolution  After multiple iterations, the following Content Security Policy has been put into effect:  The CSP is transmitted through the following headers:  Content-Security-Policy  X-Content-Security-Policy  X-WebKit-CSP  as well as through corresponding meta HTML tags. Additionally, the following frame-busting JavaScript code has been added to prevent Clickjacking attacks in the unlikely event that existing CSP measures fail or are bypassed:  Description  A content security policy (CSP) provides an added layer of protection against cross-site scripting (XSS), clickjacking, and other client-side attacks that rely on executing malicious content in the context of the website.  Specifically, the lack of a content security policy allows an adversary to perform a clickjacking attack by including the target URL (such as app.fei.money) in an iframe element on their site. The attacker then uses one or more transparent layers on top of the embedded site to trick a user into performing a click action on a different element.  This technique can be used to spawn malicious Metamask dialogues, tricking users into thinking that they are signing a legitimate transaction.  Affected Assets  All S3-hosted web sites.  Recommendation  It is recommended to add content security policy headers to the served responses to prevent browsers from embedding Fei-owned sites into malicious parent sites. Furthermore, CSP can be used to limit the permissions of JavaScript and CSS on the page, which can be used to further harden the deployment against a potential compromise of script dependencies.  It should be noted that security headers should not only be served from Cloudfront but any public-facing endpoint. Otherwise, it will be trivial for an attacker to circumvent the security headers added by Cloudfront, e.g. by embedding the index.html file directly from the public-facing S3 bucket URL.  Besides CSP headers, clickjacking can also be mitigated by directly including frame-busting JavaScript code into the served page.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.2 S3 Buckets Cleartext Communication    ", "body": "  Resolution   Direct access to S3 buckets through   Description  The system s S3 buckets are configured to allow unencrypted traffic:  Affected Assets  arn:aws:s3:::ropsten-app.fei.money/*  arn:aws:s3:::www.fei.money/*  arn:aws:s3:::feiprotocol.com/*  arn:aws:s3:::www.app.fei.money/*  arn:aws:s3:::www.ropsten-app.fei.money/*  arn:aws:s3:::app.fei.money/*  arn:aws:s3:::fei.money/*  Recommendation  It is recommended to enforce encryption of data in transit using TLS certificates. To accomplish this, the aws:SecureTransport can be set in the S3 bucket s policies.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.3 Missing Log Aggregation    ", "body": "  Resolution  CloudFrond and CloudTrail have been enabled. These components send endpoint-related and organizational log messages into S3 buckets where they can be queried using AWS Athena. The security review process section of this report contains sample queries for Athena.  Description  There is no centralized system that gathers operational events of AWS stack components. This includes S3 server access logs, configuration changes, as well as Cloudfront-related logging.  Recommendation  It is recommended to enable CloudTrail for internal log aggregation as it integrates seamlessly with S3, Cloudfront, and IAM. Furthermore, regular reviews should be set up where system activity is checked to detect suspicious activity as soon as possible.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.4 Enforce Strict Transport Security    ", "body": "  Resolution  All domains in scope now ship with the following header:  Description  The HTTP Strict-Transport-Security response header (often abbreviated as HSTS) lets a web site tell browsers that it should only be accessed using HTTPS, instead of using HTTP. This prevents attackers from stripping TLS certificates from connections and removing encryption.  Recommendation  It is recommended to deliver all responses with the Strict-Transport-Security header. In an S3-Cloudfront setup, this can be achieved using Lambda@Edge lambda functions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.5 Server Information Leak ", "body": "  Description  Responses from the fei.money domain and related assets leak server information in their response headers. This information can be used by an adversary to prepare more sophisticated attacks tailored to the deployed infrastructure.  Note: At the time of reporting, this issue was deemed not possible to fix due to technical limitations on AWS-hosted static sites using S3 and CloudFront.  Examples  Recommendation  It is recommended to remove any headers that hint at server technologies and are not directly required by the frontend.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.6 Missing Route53 Domain Lock    ", "body": "  Resolution   A transfer lock on both the   Description  Domain registrars often give customers the option to lock a domain. This prevents unauthorized parties from transferring it to another registrar, either through malicious interaction with the registrar itself, or compromised domain owner credentials. No domain currently has a lock enabled.  Affected Assets  fei.money  feiprotocol.com  Recommendation  It is recommended to set a lock for the affected domains, assuming that the registrar allows domain locks:  Sign in to the AWS Management Console and open the Route 53 console at https://console.aws.amazon.com/route53/.  In the navigation pane, choose Registered Domains.  Choose the name of the domain that you want to update.  Choose Enable (to lock the domain) or Disable (to unlock the domain).  Choose Save.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.7 Weak IAM Password Policy    ", "body": "  Resolution  This has been fixed by the client with the following notes:  Enforced 14 character password length  Enabled 90 day password expiration  Prevent password reuse  Require one uppercase, one lowercase, one number, one non-alphanumeric character  Require 2FA on all users via this doc and this post (Create new Force_MFA policy, attach it to the new Engineers group, and then assign all users (including Dominik) to this group  Also requiring 2FA on command line access. Using src/infra/aws-token.sh for generating the credentials and putting them in ~/.aws/config  Description  The password policy for IAM users currently does not enforce the use of strong passwords, multi-factor authentication, and regular password rotation.  Currently, only a minimum password length of 8 is enforced.  Recommendation  Require a minimum password length of 14  Set a password expiration policy of at most 90 days  Disallow the reuse of passwords  Enable mandatory multi-factor authentication with a virtual app  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.8 Review Access Key Expiration    ", "body": "  Resolution  This issue is considered resolved with the implementation of regular security review meetings.  Description  It is recommended to only create access keys when absolutely necessary. There should be no access keys given out to root users. Instead, temporary security credentials (IAM Roles) should be created.  Recommendation  It is recommended to read the Best practices for managing AWS access keys and incorporate the security practices where reasonable.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "4.9 Dependency Security    ", "body": "  Resolution  This issue has been resolved by implementing Snyk for continuous dependency security scanning. This allows the developers to review potential risks of included packages and receiving automated pull requests with fixes if necessary.  Furthermore, a manual review of select dependencies has been conducted by the penetration tester without significant, actionable results. The following dependencies have been checked:  bignumber  numeral  validator  web-vitals  Description  The Yarn audit feature currently finds two low-severity dependency issues:  Prototype pollution in ini - a dependency of react-scripts  Insecure Credential Storage in web3  Recommendations  It is recommended to apply the ini patch, which is already available. For web3, it is recommended to monitor the repository s Github issue https://github.com/ConsenSys/fei-protocol-audit-2021-01/issues/2739 and upgrade as soon as a fix is available.  For additional dependency security, it is recommended to integrate a security monitoring service. Snyk has a free plan which allows unlimited tests on public repositories, and 200 tests per month for private ones. A bot will automatically add a pull request to bump vulnerable dependency versions.  It should be noted that the quality and reliability of such automated contributions are highly dependent on the quality of the test suite. It is recommended to build strict tests around core functionality and expected dependency behaviour to detect breaking changes as soon as possible.  5 Security Review Process  In an additional effort to achieve security-in-depth, it is recommended to implement a schedule or recurring security review meetings. The goal of these meetings is to complete a checklist to enforce security best-practices, as well as find anomalies in the system as soon as possible to commence mitigation and investigations.  This section outlines recommendations for the contents of such a checklist. It should be noted that security requirements are likely to change, and thus, this list should be treated as a working document as the project s infrastructure and attack surface change.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.1 CloudTrail Anomalies", "body": "  Event filter query template:  SELECT useridentity.username,  sourceipaddress,  eventtime,  additionaleventdata  FROM cloudtrail_logs  WHERE {{ event filter }}  AND eventtime >= '<yyyy-mm-dd>'  AND eventtime < '<yyyy-mm-dd>';  eventname = 'ConsoleLogin'  Sign-in activity  eventname = 'AddUserToGroup'  User added to group  eventname = 'ChangePassword'  User password change  eventname LIKE '%AccessKey%'  Key management events  eventname LIKE '%MFADevice'  MFA deactivation/deletion/resync  eventname = 'StopLogging'  Logging stopped  eventname LIKE '%BucketPolicy%'  Bucket policy activity  eventname LIKE '%GroupPolicy%'  Group policy activity  eventname LIKE '%UserPolicy%'  User policy activity  eventname LIKE '%RolePolicy%'  Role policy activity  Aggregate statistics about failed authentication and user authorization attempts can be gathered with the following query:  SELECT count (*) AS totalEvents,  useridentity.arn,  eventsource,  eventname,  errorCode,  errorMessage  FROM cloudtrail_logs  WHERE (errorcode LIKE '%Denied%'  OR errorcode LIKE '%Unauthorized%')  AND eventtime >= '2021-02-17'  AND eventtime < '2021-02-17'  GROUP BY  eventsource, eventname, errorCode, errorMessage, useridentity.arn  ORDER BY  eventsource, eventname  For investigative purposes or the goal of covering new infrastructure components, it might be necessary to add more event names to the review process. AWS does not provide a comprehensive list of event names per stack component. An external list of CloudTrail event names is available on the GorillaStack blog.  Note: In case of issues with Athena or ingestion into the database, CloudTrail allows users to view the unfiltered event history for a user-specified time range as well. Particularly notable is the ability to filter by resource types, of which the following are relevant to the Fei AWS infrastructure:  AWS::S3::Bucket  AWS::CloudTrail::Trail  AWS::IAM::AccessKey  AWS::IAM::MfaDevice  AWS::IAM::Group  AWS::IAM::Policy  AWS::IAM::Role  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.2 Cloudfront Endpoint Anomalies", "body": "  Top 10 endpoints hit in a given time frame:  SELECT uri,  status,  count(*) AS ct  FROM cloudfront_logs_fei_landing  WHERE date >= DATE('2021-02-01')  AND date <= DATE('2021-02-28')  GROUP BY  uri, status  ORDER BY  ct DESC limit 10  This query can be filtered further by adding AND status = 500 or a similar condition to find suspicious response codes.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.3 Route53 CNAME Review", "body": "  Subdomain takeover vulnerabilities occur when a subdomain is pointing to a service, e.g. a previously deleted CloudFront endpoint or S3 bucket. This allows an attacker to set up a page on the service that was being used and point their page to that subdomain. Especially with wildcard certificates on the system, e.g. *.fei.money, this can lead to an exploitation of user trust and enables attacks that can result in reputational and financial loss.  It is recommended that DNS records in Route53 are reviewed regularly and removed as soon as the underlying resource is decommissioned.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.4 External Monitoring and Notifications", "body": "  Beyond manual checks, it is recommended that a service such as Assertible is used. This will allow the development team to detect unavailable endpoints and enforce regularly-checked assertions, such as proper return codes or page content. Furthermore, such a service should integrate other means of communication such as Slack notifications, SMS messages, or arbitrary webhook calls to notify an on-duty developer as quickly as possible.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/01/fei-protocol/"}, {"title": "5.1 Merkle.checkMembership allows existence proofs for the same leaf in multiple locations in the tree    Addressed", "body": "  Resolution   This was addressed in   omisego/plasma-contracts#533 by including a check in  omisego/plasma-contracts#547 ensured the passed-in index satisfied the recommended criterion.  Description  checkMembership is used by several contracts to prove that transactions exist in the child chain. The function uses a leaf, an index, and a proof to construct a hypothetical root hash. This constructed hash is compared to the passed in rootHash parameter. If the two are equivalent, the proof is considered valid.  The proof is performed iteratively, and uses a pseudo-index (j) to determine whether the next proof element represents a  left branch  or  right branch :  code/plasma_framework/contracts/src/utils/Merkle.sol:L28-L41  uint256 j = index;  // Note: We're skipping the first 32 bytes of `proof`, which holds the size of the dynamically sized `bytes`  for (uint256 i = 32; i <= proof.length; i += 32) {  // solhint-disable-next-line no-inline-assembly  assembly {  proofElement := mload(add(proof, i))  if (j % 2 == 0) {  computedHash = keccak256(abi.encodePacked(NODE_SALT, computedHash, proofElement));  } else {  computedHash = keccak256(abi.encodePacked(NODE_SALT, proofElement, computedHash));  j = j / 2;  If j is even, the computed hash is placed before the next proof element. If j is odd, the computed hash is placed after the next proof element. After each iteration, j is decremented by j = j / 2.  Because checkMembership makes no requirements on the height of the tree or the size of the proof relative to the provided index, it is possible to pass in invalid values for index that prove a leaf s existence in multiple locations in the tree.  Examples  By modifying existing tests, we showed that for a tree with 3 leaves, leaf 2 can be proven to exist at indices 2, 6, and 10 using the same proof each time. The modified test can be found here: https://gist.github.com/wadeAlexC/01b60099282a026f8dc1ac85d83489fd#file-merkle-test-js-L40-L67  Conclusion  Exit processing is meant to bypass exits processed more than once. This is implemented using an  output id  system, where each exited output should correspond to a unique id that gets flagged in the ExitGameController contract as it s exited. Before an exit is processed, its output id is calculated and checked against ExitGameController. If the output has already been exited, the exit being processed is deleted and skipped. Crucially, output id is calculated differently for standard transactions and deposit transactions: deposit output ids factor in the transaction index.  By using the behavior described in this issue in conjunction with methods discussed in issue 5.8 and issue 5.10, we showed that deposit transactions can be exited twice using indices 0 and 2**16. Because of the distinct output id calculation, these exits have different output ids and can be processed twice, allowing users to exit double their deposited amount.  A modified StandardExit.load.test.js shows that exits are successfully enqueued with a transaction index of 65536: https://gist.github.com/wadeAlexC/4ad459b7510e512bc9556e7c919e0965#file-standardexit-load-test-js-L55  Recommendation  Use the length of the proof to determine the maximum allowed index. The passed-in index should satisfy the following criterion: index < 2**(proof.length/32). Additionally, ensure range checks on transaction position decoding are sufficiently restrictive (see https://github.com/ConsenSys/omisego-morevp-audit-2019-10/issues/20).  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/546  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.2 Improper initialization of spending condition abstraction allows  v2 transactions  to exit using PaymentExitGame    Addressed", "body": "  Resolution   This was addressed in   omisego/plasma-contracts#478 by requiring that  Description  PaymentOutputToPaymentTxCondition is an abstraction around the transaction signature check needed for many components of the exit games. Its only function, verify, returns true if one transaction (inputTxBytes) is spent by another transaction (spendingTxBytes):  code/plasma_framework/contracts/src/exits/payment/spendingConditions/PaymentOutputToPaymentTxCondition.sol:L40-L69  function verify(  bytes calldata inputTxBytes,  uint16 outputIndex,  uint256 inputTxPos,  bytes calldata spendingTxBytes,  uint16 inputIndex,  bytes calldata signature,  bytes calldata /*optionalArgs*/  external  view  returns (bool)  PaymentTransactionModel.Transaction memory inputTx = PaymentTransactionModel.decode(inputTxBytes);  require(inputTx.txType == supportInputTxType, \"Input tx is an unsupported payment tx type\");  PaymentTransactionModel.Transaction memory spendingTx = PaymentTransactionModel.decode(spendingTxBytes);  require(spendingTx.txType == supportSpendingTxType, \"The spending tx is an unsupported payment tx type\");  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.build(TxPosLib.TxPos(inputTxPos), outputIndex);  require(  spendingTx.inputs[inputIndex] == bytes32(utxoPos.value),  \"Spending tx points to the incorrect output UTXO position\"  );  address payable owner = inputTx.outputs[outputIndex].owner();  require(owner == ECDSA.recover(eip712.hashTx(spendingTx), signature), \"Tx in not signed correctly\");  return true;  Verification process  The verification process is relatively straightforward. The contract performs some basic input validation, checking that the input transaction s txType matches supportInputTxType, and that the spending transaction s txType matches supportSpendingTxType. These values are set during construction.  Next, verify checks that the spending transaction contains an input that matches the position of one of the input transaction s outputs.  Finally, verify performs an EIP-712 hash on the spending transaction, and ensures it is signed by the owner of the output in question.  Implications of the abstraction  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78  /**  @notice Registers an exit game within the PlasmaFramework. Only the maintainer can call the function.  @dev Emits ExitGameRegistered event to notify clients  @param _txType The tx type where the exit game wants to register  @param _contract Address of the exit game contract  @param _protocol The transaction protocol, either 1 for MVP or 2 for MoreVP  /  function registerExitGame(uint256 _txType, address _contract, uint8 _protocol) public onlyFrom(getMaintainer()) {  require(_txType != 0, \"Should not register with tx type 0\");  require(_contract != address(0), \"Should not register with an empty exit game address\");  require(_exitGames[_txType] == address(0), \"The tx type is already registered\");  require(_exitGameToTxType[_contract] == 0, \"The exit game contract is already registered\");  require(Protocol.isValidProtocol(_protocol), \"Invalid protocol value\");  _exitGames[_txType] = _contract;  _exitGameToTxType[_contract] = _txType;  _protocols[_txType] = _protocol;  _exitGameQuarantine.quarantine(_contract);  emit ExitGameRegistered(_txType, _contract, _protocol);  Migration and initialization  The migration script seems to corroborate this interpretation:  code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L109-L124  // handle spending condition  await deployer.deploy(  PaymentOutputToPaymentTxCondition,  plasmaFramework.address,  PAYMENT_OUTPUT_TYPE,  PAYMENT_TX_TYPE,  );  const paymentToPaymentCondition = await PaymentOutputToPaymentTxCondition.deployed();  await deployer.deploy(  PaymentOutputToPaymentTxCondition,  plasmaFramework.address,  PAYMENT_OUTPUT_TYPE,  PAYMENT_V2_TX_TYPE,  );  const paymentToPaymentV2Condition = await PaymentOutputToPaymentTxCondition.deployed();  The migration script then registers both of these contracts in SpendingConditionRegistry, and then calls renounceOwnership, freezing the spending conditions registered permanently:  code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L126-L135  console.log(`Registering paymentToPaymentCondition (${paymentToPaymentCondition.address}) to spendingConditionRegistry`);  await spendingConditionRegistry.registerSpendingCondition(  PAYMENT_OUTPUT_TYPE, PAYMENT_TX_TYPE, paymentToPaymentCondition.address,  );  console.log(`Registering paymentToPaymentV2Condition (${paymentToPaymentV2Condition.address}) to spendingConditionRegistry`);  await spendingConditionRegistry.registerSpendingCondition(  PAYMENT_OUTPUT_TYPE, PAYMENT_V2_TX_TYPE, paymentToPaymentV2Condition.address,  );  await spendingConditionRegistry.renounceOwnership();  Finally, the migration script registers a single exit game contract in PlasmaFramework:  code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L137-L143  // register the exit game to framework  await plasmaFramework.registerExitGame(  PAYMENT_TX_TYPE,  paymentExitGame.address,  config.frameworks.protocols.moreVp,  { from: maintainerAddress },  );  Note that the associated _txType is permanently associated with the deployed exit game contract:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78  /**  @notice Registers an exit game within the PlasmaFramework. Only the maintainer can call the function.  @dev Emits ExitGameRegistered event to notify clients  @param _txType The tx type where the exit game wants to register  @param _contract Address of the exit game contract  @param _protocol The transaction protocol, either 1 for MVP or 2 for MoreVP  /  function registerExitGame(uint256 _txType, address _contract, uint8 _protocol) public onlyFrom(getMaintainer()) {  require(_txType != 0, \"Should not register with tx type 0\");  require(_contract != address(0), \"Should not register with an empty exit game address\");  require(_exitGames[_txType] == address(0), \"The tx type is already registered\");  require(_exitGameToTxType[_contract] == 0, \"The exit game contract is already registered\");  require(Protocol.isValidProtocol(_protocol), \"Invalid protocol value\");  _exitGames[_txType] = _contract;  _exitGameToTxType[_contract] = _txType;  _protocols[_txType] = _protocol;  _exitGameQuarantine.quarantine(_contract);  emit ExitGameRegistered(_txType, _contract, _protocol);  Conclusion  Recommendation  Remove PaymentOutputToPaymentTxCondition and SpendingConditionRegistry  Implement checks for specific spending conditions directly in exit game controllers. Emphasize clarity of function: ensure it is clear when called from the top level that a signature verification check and spending condition check are being performed.  If the inferred relationship between txType and PaymentExitGame is correct, ensure that each PaymentExitGame router checks for its supported txType. Alternatively, the check could be made in PaymentExitGame itself.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/472  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.3 RLPReader - Leading zeroes allow multiple valid encodings and exit / output ids for the same transaction    Addressed", "body": "  Resolution   This was addressed in   omisego/plasma-contracts#507 with the addition of checks to ensure primitive decoding functions in  omisego/plasma-contracts#476 rejects leading zeroes in  Description  The current implementation of RLP decoding can take 2 different txBytes and decode them to the same structure. Specifically, the RLPReader.toUint method can decode 2 different types of bytes to the same number. For example:  0x821234 is decoded to uint(0x1234)  0x83001234 is decoded to uint(0x1234)  0xc101 can decode to uint(1), even though the tag specifies a short list  0x01 can decode to uint(1), even though the tag specifies a single byte  As explanation for this encoding:  0x821234 is broken down into 2 parts:  0x82 - represents 0x80 (the string tag) + 0x02 bytes encoded  0x1234 - are the encoded bytes  The same for 0x83001234:  0x83 - represents 0x80 (the string tag) + 0x03 bytes encoded  0x001234 - are the encoded bytes  The current implementation casts the encoded bytes into a uint256, so these different encodings are interpreted by the contracts as the same number:  uint(0x1234) = uint(0x001234)  code/plasma_framework/contracts/src/utils/RLPReader.sol:L112  result := mload(memPtr)  Having different valid encodings for the same data is a problem because the encodings are used to create hashes that are used as unique ids. This means that multiple ids can be created for the same data. The data should only have one possible id.  The encoding is used to create ids in these parts of the code:  Outputid.sol  code/plasma_framework/contracts/src/exits/utils/OutputId.sol:L18  return keccak256(abi.encodePacked(_txBytes, _outputIndex, _utxoPosValue));  code/plasma_framework/contracts/src/exits/utils/OutputId.sol:L32  return keccak256(abi.encodePacked(_txBytes, _outputIndex));  ExitId.sol  code/plasma_framework/contracts/src/exits/utils/ExitId.sol:L41  bytes32 hashData = keccak256(abi.encodePacked(_txBytes, _utxoPos.value));  code/plasma_framework/contracts/src/exits/utils/ExitId.sol:L54  return uint160((uint256(keccak256(_txBytes)) >> 105).setBit(151));  TxFinalizationVerifier.sol  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L55  bytes32 leafData = keccak256(data.txBytes);  Other methods that are affected because they rely on the return values of these methods:  ExitId.sol  getStandardExitId getInFlightExitId  OutputId.sol  computeDepositOutputId computeNormalOutputId  PaymentChallengeIFENotCanonical.sol  verifyAndDeterminePositionOfTransactionIncludedInBlock verifyCompetingTxFinalized  PaymentChallengeStandardExit.sol  verifyChallengeTxProtocolFinalized  PaymentStartInFlightExit.sol  verifyInputTransactionIsStandardFinalized  PaymentExitGame.sol  getStandardExitId getInFlightExitId  PaymentOutputToPaymentTxCondition.sol  verify  Recommendation  Enforce strict-length decoding for txBytes, and specify that uint is decoded from a 32-byte short string.  Enforcing a 32-byte length for uint means that 0x1234 should always be encoded as:  0xa00000000000000000000000000000000000000000000000000000000000001234  0xa0 represents the tag + the length: 0x80 + 32  0000000000000000000000000000000000000000000000000000000000001234 is the number 32 bytes long with leading zeroes  Unfortunately, using leading zeroes is against the RLP spec:  https://github.com/ethereum/wiki/wiki/RLP  positive RLP integers must be represented in big endian binary form with no leading zeroes  This means that libraries interacting with OMG contracts which are going to correctly and fully implement the spec will generate  incorrect  encodings for uints; encodings that are not going to be recognized by the OMG contracts.  Fully correct spec encoding: 0x821234. Proposed encoding in this solution: 0xa00000000000000000000000000000000000000000000000000000000000001234.  Similarly enforce restrictions where they can be added; this is possible because of the strict structure format that needs to be encoded.  Some other potential solutions are included below. Note that these solutions are not recommended for reasons included below:  Normalize the encoding that gets passed to methods that hash the transaction for use as an id:  This can be implemented in the methods that call keccak256 on txBytes and should decode and re-encode the passed txBytes in order to normalize the passed encoding.  a txBytes is passed  the txBytes are decoded into structure: tmpDecodedStruct = decode(txBytes)  the tmpDecodedStruct is re-encoded in order to normalize it: normalizedTxBytes = encode(txBytes)  This method is not recommended because it needs a Solidity encoder to be implemented and a lot of gas will be used to decode and re-encode the initial txBytes.  Correctly and fully implement RLP decoding  This is another solution that adds a lot of code and is prone to errors.  The solution would be to enforce all of the restrictions when decoding and not accept any encoding that doesn t fully follow the spec. This for example means that is should not accept uints with leading zeroes.  This is a problem because it needs a lot of code that is not easy to write in Solidity (or EVM).  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.4 Recommendation: Remove TxFinalizationModel and TxFinalizationVerifier. Implement stronger checks in Merkle ", "body": "  Resolution   This was partially addressed in   omisego/plasma-contracts#503, with the removal of several unneeded branches of logic in  omisego/plasma-contracts#533 added a non-zero proof length check in  Description  TxFinalizationVerifier is an abstraction around the block inclusion check needed for many of the features of plasma exit games. It uses a struct defined in TxFinalizationModel as inputs to its two functions: isStandardFinalized and isProtocolFinalized.  isStandardFinalized returns the result of an inclusion proof. Although there are several branches, only the first is used:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L19-L32  /**  @notice Checks whether a transaction is \"standard finalized\"  @dev MVP: requires that both inclusion proof and confirm signature is checked  @dev MoreVp: checks inclusion proof only  /  function isStandardFinalized(Model.Data memory data) public view returns (bool) {  if (data.protocol == Protocol.MORE_VP()) {  return checkInclusionProof(data);  } else if (data.protocol == Protocol.MVP()) {  revert(\"MVP is not yet supported\");  } else {  revert(\"Invalid protocol value\");  isProtocolFinalized is unused:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L34-L47  /**  @notice Checks whether a transaction is \"protocol finalized\"  @dev MVP: must be standard finalized  @dev MoreVp: allows in-flight tx, so only checks for the existence of the transaction  /  function isProtocolFinalized(Model.Data memory data) public view returns (bool) {  if (data.protocol == Protocol.MORE_VP()) {  return data.txBytes.length > 0;  } else if (data.protocol == Protocol.MVP()) {  revert(\"MVP is not yet supported\");  } else {  revert(\"Invalid protocol value\");  Finally, the abstraction may have ramifications on the safety of Merkle.sol. As it stands now, Merkle.checkMembership should never be called directly by the exit game controllers, as it lacks an important check made in TxFinalizationVerifier.checkInclusionProof:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L49-L59  function checkInclusionProof(Model.Data memory data) private view returns (bool) {  if (data.inclusionProof.length == 0) {  return false;  (bytes32 root,) = data.framework.blocks(data.txPos.blockNum());  bytes32 leafData = keccak256(data.txBytes);  return Merkle.checkMembership(  leafData, data.txPos.txIndex(), root, data.inclusionProof  );  By introducing the abstraction of TxFinalizationVerifier, the input validation performed by Merkle is split across multiple files, and the reasonable-seeming decision of calling Merkle.checkMembership directly becomes unsafe. In fact, this occurs in one location in the contracts:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L187-L204  function verifyAndDeterminePositionOfTransactionIncludedInBlock(  bytes memory txbytes,  UtxoPosLib.UtxoPos memory utxoPos,  bytes32 root,  bytes memory inclusionProof  private  pure  returns(uint256)  bytes32 leaf = keccak256(txbytes);  require(  Merkle.checkMembership(leaf, utxoPos.txIndex(), root, inclusionProof),  \"Transaction is not included in block of Plasma chain\"  );  return utxoPos.value;  Recommendation  Remove TxFinalizationVerifier and TxFinalizationModel  Implement a proof length check in Merkle.sol  Call Merkle.checkMembership directly from exit controller contracts:  PaymentChallengeIFEOutputSpent.verifyInFlightTransactionStandardFinalized:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L91  require(controller.txFinalizationVerifier.isStandardFinalized(finalizationData), \"In-flight transaction not finalized\");  PaymentChallengeIFENotCanonical.verifyCompetingTxFinalized:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L244  require(self.txFinalizationVerifier.isStandardFinalized(finalizationData), \"Failed to verify the position of competing tx\");  PaymentStartInFlightExit.verifyInputTransactionIsStandardFinalized:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L307-L308  require(exitData.controller.txFinalizationVerifier.isStandardFinalized(finalizationData),  \"Input transaction is not standard finalized\");  If none of the above recommendations are implemented, ensure that PaymentChallengeIFENotCanonical uses the abstraction TxFinalizationVerifier so that a length check is performed on the inclusion proof.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/471  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.5 Merkle - The implementation does not enforce inclusion of leaf nodes.    Addressed", "body": "  Resolution   This was addressed in   omisego/plasma-contracts#452 with the addition of leaf and node salts to the  Description  A observation with the current Merkle tree implementation is that it may be possible to validate nodes other than leaves. This is done by providing checkMembership with a reference to a hash within the tree, rather than a leaf.  code/plasma_framework/contracts/src/utils/Merkle.sol:L9-L42  /**  @notice Checks that a leaf hash is contained in a root hash  @param leaf Leaf hash to verify  @param index Position of the leaf hash in the Merkle tree  @param rootHash Root of the Merkle tree  @param proof A Merkle proof demonstrating membership of the leaf hash  @return True, if the leaf hash is in the Merkle tree; otherwise, False  /  function checkMembership(bytes32 leaf, uint256 index, bytes32 rootHash, bytes memory proof)  internal  pure  returns (bool)  require(proof.length % 32 == 0, \"Length of Merkle proof must be a multiple of 32\");  bytes32 proofElement;  bytes32 computedHash = leaf;  uint256 j = index;  // Note: We're skipping the first 32 bytes of `proof`, which holds the size of the dynamically sized `bytes`  for (uint256 i = 32; i <= proof.length; i += 32) {  // solhint-disable-next-line no-inline-assembly  assembly {  proofElement := mload(add(proof, i))  if (j % 2 == 0) {  computedHash = keccak256(abi.encodePacked(computedHash, proofElement));  } else {  computedHash = keccak256(abi.encodePacked(proofElement, computedHash));  j = j / 2;  return computedHash == rootHash;  The current implementation will validate the provided  leaf  and return true. This is a known problem of Merkle trees https://en.wikipedia.org/wiki/Merkle_tree#Second_preimage_attack.  Examples  Provide a hash from within the Merkle tree as the leaf argument. The index has to match the index of that node in regards to its current level in the tree. The rootHash has to be the correct Merkle tree rootHash. The proof has to skip the necessary number of levels because the nodes  underneath  the provided  leaf  will not be processed.  Recommendation  A remediation needs a fixed Merkle tree size as well as the addition of a byte prepended to each node in the tree. Another way would be to create a structure for the Merkle node and mark it as leaf or no leaf.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/425  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.6 Maintainer can bypass exit game quarantine by registering not-yet-deployed contracts    Addressed", "body": "  Resolution   This was addressed in   commit 7669076be1dff47473ee877dcebef5989d7617ac by adding a check that registered contracts had nonzero  Description  The plasma framework uses an ExitGameRegistry to allow the maintainer to add new exit games after deployment. An exit game is any arbitrary contract. In order to prevent the maintainer from adding malicious exit games that steal user funds, the framework uses a  quarantine  system whereby newly-registered exit games have restricted permissions until their quarantine period has expired. The quarantine period is by default 3 * minExitPeriod, and is intended to facilitate auditing of the new exit game s functionality by the plasma users.  However, by registering an exit game at a contract which has not yet been deployed, the maintainer can prevent plasma users from auditing the game until the quarantine period has expired. After the quarantine period has expired, the maintainer can deploy the malicious exit game and immediately steal funds.  Explanation  Exit games are registered in the following function, callable only by the plasma contract maintainer:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78  /**  @notice Registers an exit game within the PlasmaFramework. Only the maintainer can call the function.  @dev Emits ExitGameRegistered event to notify clients  @param _txType The tx type where the exit game wants to register  @param _contract Address of the exit game contract  @param _protocol The transaction protocol, either 1 for MVP or 2 for MoreVP  /  function registerExitGame(uint256 _txType, address _contract, uint8 _protocol) public onlyFrom(getMaintainer()) {  require(_txType != 0, \"Should not register with tx type 0\");  require(_contract != address(0), \"Should not register with an empty exit game address\");  require(_exitGames[_txType] == address(0), \"The tx type is already registered\");  require(_exitGameToTxType[_contract] == 0, \"The exit game contract is already registered\");  require(Protocol.isValidProtocol(_protocol), \"Invalid protocol value\");  _exitGames[_txType] = _contract;  _exitGameToTxType[_contract] = _txType;  _protocols[_txType] = _protocol;  _exitGameQuarantine.quarantine(_contract);  emit ExitGameRegistered(_txType, _contract, _protocol);  Notably, the function does not check the extcodesize of the submitted contract. As such, the maintainer can submit the address of a contract which does not yet exist and is not auditable.  After at least 3 * minExitPeriod seconds pass, the submitted contract now has full permissions as a registered exit game and can pass all checks using the onlyFromNonQuarantinedExitGame modifier:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L33-L40  /**  @notice A modifier to verify that the call is from a non-quarantined exit game  /  modifier onlyFromNonQuarantinedExitGame() {  require(_exitGameToTxType[msg.sender] != 0, \"The call is not from a registered exit game contract\");  require(!_exitGameQuarantine.isQuarantined(msg.sender), \"ExitGame is quarantined\");  _;  Additionally, the submitted contract passes checks made by external contracts using the isExitGameSafeToUse function:  code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L48-L56  /**  @notice Checks whether the contract is safe to use and is not under quarantine  @dev Exposes information about exit games quarantine  @param _contract Address of the exit game contract  @return boolean Whether the contract is safe to use and is not under quarantine  /  function isExitGameSafeToUse(address _contract) public view returns (bool) {  return _exitGameToTxType[_contract] != 0 && !_exitGameQuarantine.isQuarantined(_contract);  These permissions allow a registered quarantine to:  Withdraw any users  tokens from ERC20Vault:  code/plasma_framework/contracts/src/vaults/Erc20Vault.sol:L52-L55  function withdraw(address payable receiver, address token, uint256 amount) external onlyFromNonQuarantinedExitGame {  IERC20(token).safeTransfer(receiver, amount);  emit Erc20Withdrawn(receiver, token, amount);  Withdraw any users  ETH from EthVault:  code/plasma_framework/contracts/src/vaults/EthVault.sol:L46-L54  function withdraw(address payable receiver, uint256 amount) external onlyFromNonQuarantinedExitGame {  // we do not want to block exit queue if transfer is unucessful  // solhint-disable-next-line avoid-call-value  (bool success, ) = receiver.call.value(amount)(\"\");  if (success) {  emit EthWithdrawn(receiver, amount);  } else {  emit WithdrawFailed(receiver, amount);  Activate and deactivate the ExitGameController reentrancy mutex:  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L63-L66  function activateNonReentrant() external onlyFromNonQuarantinedExitGame() {  require(!mutex, \"Reentrant call\");  mutex = true;  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L72-L75  function deactivateNonReentrant() external onlyFromNonQuarantinedExitGame() {  require(mutex, \"Not locked\");  mutex = false;  enqueue arbitrary exits:  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L115-L138  function enqueue(  uint256 vaultId,  address token,  uint64 exitableAt,  TxPosLib.TxPos calldata txPos,  uint160 exitId,  IExitProcessor exitProcessor  external  onlyFromNonQuarantinedExitGame  returns (uint256)  bytes32 key = exitQueueKey(vaultId, token);  require(hasExitQueue(key), \"The queue for the (vaultId, token) pair is not yet added to the Plasma framework\");  PriorityQueue queue = exitsQueues[key];  uint256 priority = ExitPriority.computePriority(exitableAt, txPos, exitId);  queue.insert(priority);  delegations[priority] = exitProcessor;  emit ExitQueued(exitId, priority);  return priority;  Flag outputs as  spent :  code/plasma_framework/contracts/src/framework/ExitGameController.sol:L210-L213  function flagOutputSpent(bytes32 _outputId) external onlyFromNonQuarantinedExitGame {  require(_outputId != bytes32(\"\"), \"Should not flag with empty outputId\");  isOutputSpent[_outputId] = true;  Recommendation  registerExitGame should check that extcodesize of the submitted contract is non-zero.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/410  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.7 EthVault - Unused state variable    Addressed", "body": "  Resolution   This was addressed in   commit ea36f5ff46ab72ec5c281fa0a3dffe3bcc83178b.  Description  The state variable withdrawEntryCounter is not used in the code.  code/plasma_framework/contracts/src/vaults/EthVault.sol:L8  uint256 private withdrawEntryCounter = 0;  Recommendation  Remove it from the contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.8 Recommendation: Add a tree height limit check to Merkle.sol ", "body": "  Description  Each plasma block has a maximum of 2 ** 16 transactions, which corresponds to a maximum Merkle tree height of 16. The Merkle library currently checks that the proof is comprised of 32-byte segments, but neglects to check the maximum height:  code/plasma_framework/contracts/src/utils/Merkle.sol:L17-L23  function checkMembership(bytes32 leaf, uint256 index, bytes32 rootHash, bytes memory proof)  internal  pure  returns (bool)  require(proof.length % 32 == 0, \"Length of Merkle proof must be a multiple of 32\");  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/467  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.9 Recommendation: remove IsDeposit and add a similar getter to BlockController    Addressed", "body": "  Resolution   This was addressed in   commit 0fee13f7f084983139eb47636ff785ebea8a1c36 by removing the  Description  The IsDeposit library is used to check whether a block number is a deposit or not. The logic is simple - if blockNum % childBlockInterval is nonzero, the block number is a deposit.  By including this check in BlockController instead, the contract can perform an existence check as well. The function in BlockController would return the same result as the IsDeposit library, but would additionally revert if the block in question does not exist:  Note that this check is made at the cost of an external call. If the check needs to be made multiple times in a transaction, the result should be cached.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/466  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.10 Recommendation: Merge TxPosLib into UtxoPosLib and implement a decode function with range checks. ", "body": "  Resolution   This was partially addressed in   omisego/plasma-contracts#515 with the merging of  omisego/plasma-contracts#533 implemented stricter range checks for block number and transaction index. Note that the maximum output index in  Description  TxPosLib and UtxoPosLib serve very similar functions. They both provide utility functions to access the block number and tx index of a packed utxo position variable. UtxoPosLib, additionally, provides a function to retrieve the output index of a packed utxo position variable.  What they both lack, though, is sanity checks on the values packed inside a utxo position variable. By implementing a function UtxoPosLib.decode(uint _utxoPos) returns (UtxoPos), each exit controller contract can ensure that the values it is using make logical sense. The decode function should check that:  txIndex is between 0 and 2**16  outputIndex is between 0 and 3  Currently, neither of these restrictions is explicitly enforced. As for blockNum, the best check is that it exists in the PlasmaFramework contract with a nonzero root. Since UtxoPosLib is a pure library, that check is better performed elsewhere (See https://github.com/ConsenSys/omisego-morevp-audit-2019-10/issues/21).  Once implemented, all contracts should avoid casting values directly to the UtxoPos struct, in favor of using the decode function. Merging the two files will help with this.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/465  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.11 Recommendation: Implement additional existence and range checks on inputs and storage reads ", "body": "  Resolution   This was partially addressed in   omisego/plasma-contracts#524 and  omisego/plasma-contracts#483. Not all recommended checks were included.  Description  Many input validation and storage read checks are made implicitly, rather than explicitly. The following compilation notes each line of code in the exit controller contracts where an additional check should be added.  Examples  1. PaymentChallengeIFEInputSpent:  Check that inFlightTx has a nonzero input at the provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L96  require(ife.isInputPiggybacked(args.inFlightTxInputIndex), \"The indexed input has not been piggybacked\");  Check that each transaction is nonzero and is correctly formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L98-L101  require(  keccak256(args.inFlightTx) != keccak256(args.challengingTx),  \"The challenging transaction is the same as the in-flight transaction\"  );  Check that resulting outputId is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L123  bytes32 ifeInputOutputId = data.ife.inputs[data.args.inFlightTxInputIndex].outputId;  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L125  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(data.args.inputUtxoPos);  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L126  bytes32 challengingTxInputOutputId = data.controller.isDeposit.test(utxoPos.blockNum())  Check that inputTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L127-L128  ? OutputId.computeDepositOutputId(data.args.inputTx, utxoPos.outputIndex(), utxoPos.value)  : OutputId.computeNormalOutputId(data.args.inputTx, utxoPos.outputIndex());  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L149  WireTransaction.Output memory output = WireTransaction.getOutput(data.args.challengingTx, data.args.challengingTxInputIndex);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L156  UtxoPosLib.UtxoPos memory inputUtxoPos = UtxoPosLib.UtxoPos(data.args.inputUtxoPos);  Check that challengingTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEInputSpent.sol:L163  data.args.challengingTxInputIndex,  2. PaymentChallengeIFENotCanonical:  Check that each transaction is nonzero and is correctly formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L98-L101  require(  keccak256(args.inFlightTx) != keccak256(args.competingTx),  \"The competitor transaction is the same as transaction in-flight\"  );  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L104  UtxoPosLib.UtxoPos memory inputUtxoPos = UtxoPosLib.UtxoPos(args.inputUtxoPos);  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L107  if (self.isDeposit.test(inputUtxoPos.blockNum())) {  Check that inputTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L108-L110  outputId = OutputId.computeDepositOutputId(args.inputTx, inputUtxoPos.outputIndex(), inputUtxoPos.value);  } else {  outputId = OutputId.computeNormalOutputId(args.inputTx, inputUtxoPos.outputIndex());  Check that inFlightTx has a nonzero input at the provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L112-L113  require(outputId == ife.inputs[args.inFlightTxInputIndex].outputId,  \"Provided inputs data does not point to the same outputId from the in-flight exit\");  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L115  WireTransaction.Output memory output = WireTransaction.getOutput(args.inputTx, args.inFlightTxInputIndex);  Check that competingTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L126  args.competingTxInputIndex,  Check that resulting position is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L133  uint256 competitorPosition = verifyCompetingTxFinalized(self, args, output);  Check that inFlightTxPos is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L171-L173  require(  ife.oldestCompetitorPosition > inFlightTxPos,  \"In-flight transaction must be younger than competitors to respond to non-canonical challenge\");  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L175  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(inFlightTxPos);  Check that block root is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L176  (bytes32 root, ) = self.framework.blocks(utxoPos.blockNum());  Check that inFlightTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L178  inFlightTx, utxoPos, root, inFlightTxInclusionProof  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L218  UtxoPosLib.UtxoPos memory competingTxUtxoPos = UtxoPosLib.UtxoPos(args.competingTxPos);  3. PaymentChallengeIFEOutputSpent:  Check that inFlightTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L54  uint160 exitId = ExitId.getInFlightExitId(args.inFlightTx);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L58  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.outputUtxoPos);  Check that inFlightTx has a nonzero output at the provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L60-L63  require(  ife.isOutputPiggybacked(outputIndex),  \"Output is not piggybacked\"  );  Check that bond size is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L70  uint256 piggybackBondSize = ife.outputs[outputIndex].piggybackBondSize;  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L83  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.outputUtxoPos);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L101  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.outputUtxoPos);  Check that challengingTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L102  uint256 challengingTxType = WireTransaction.getTransactionType(args.challengingTx);  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L103  WireTransaction.Output memory output = WireTransaction.getOutput(args.challengingTx, utxoPos.outputIndex());  Check that challengingTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFEOutputSpent.sol:L116  args.challengingTxInputIndex,  4. PaymentChallengeStandardExit:  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L110  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(data.exitData.utxoPos);  Check that exitingTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L112  .decode(data.args.exitingTx)  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L111-L113  PaymentOutputModel.Output memory output = PaymentTransactionModel  .decode(data.args.exitingTx)  .outputs[utxoPos.outputIndex()];  Check that challengeTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L128  uint256 challengeTxType = WireTransaction.getTransactionType(data.args.challengeTx);  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L134  txPos: TxPosLib.TxPos(data.args.challengeTxPos),  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L157  bytes32 outputId = data.controller.isDeposit.test(utxoPos.blockNum())  Check that challengeTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeStandardExit.sol:L166  args.inputIndex,  5. PaymentPiggybackInFlightExit:  Check that inFlightTx is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L93  uint160 exitId = ExitId.getInFlightExitId(args.inFlightTx);  Check that inFlightTx has a nonzero input at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L99  require(!exit.isInputPiggybacked(args.inputIndex), \"Indexed input already piggybacked\");  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L108  enqueue(self, withdrawData.token, UtxoPosLib.UtxoPos(exit.position), exitId);  Check that inFlightTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L130  uint160 exitId = ExitId.getInFlightExitId(args.inFlightTx);  Check that inFlightTx has a nonzero output at provided index:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L136  require(!exit.isOutputPiggybacked(args.outputIndex), \"Indexed output already piggybacked\");  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L147  enqueue(self, withdrawData.token, UtxoPosLib.UtxoPos(exit.position), exitId);  6. PaymentStartInFlightExit:  Check that inFlightTx is nonzero and is well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L146  exitData.exitId = ExitId.getInFlightExitId(args.inFlightTx);  Check that the length of inputTxs is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L150  exitData.inputTxs = args.inputTxs;  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L167  utxosPos[i] = UtxoPosLib.UtxoPos(inputUtxosPos[i]);  See issue 5.9  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L180  bool isDepositTx = controller.isDeposit.test(utxoPos[i].blockNum());  Check that each inputTxs is nonzero and well-formed:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L181-L183  outputIds[i] = isDepositTx  ? OutputId.computeDepositOutputId(inputTxs[i], utxoPos[i].outputIndex(), utxoPos[i].value)  : OutputId.computeNormalOutputId(inputTxs[i], utxoPos[i].outputIndex());  Check that each output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L200  WireTransaction.Output memory output = WireTransaction.getOutput(inputTxs[i], outputIndex);  Check that inFlightTx has nonzero inputs for all i:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L327-L328  exitData.inFlightTxRaw,  i,  Check that each output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L407  PaymentOutputModel.Output memory output = exitData.inFlightTx.outputs[i];  7. PaymentStartStandardExit:  See issue 5.10  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L119  UtxoPosLib.UtxoPos memory utxoPos = UtxoPosLib.UtxoPos(args.utxoPos);  Check that output is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L121  PaymentOutputModel.Output memory output = outputTx.outputs[utxoPos.outputIndex()];  Check that timestamp is nonzero:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L124  (, uint256 blockTimestamp) = controller.framework.blocks(utxoPos.blockNum());  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/463  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.12 Recommendation: Remove optional arguments and clean unused code    Addressed", "body": "  Resolution   This was addressed in   omisego/plasma-contracts#496 and  omisego/plasma-contracts#503 with the removal of the output guard handler pattern, the simplification of the tx finalization check via  Description  Several locations in the codebase feature unused arguments, functions, return values, and more. There are two primary reasons to remove these artifacts from the codebase:  Mass exits are the primary safeguard against a byzantine operator. The biggest bottleneck of a mass exit is transaction throughput, so plasma rootchain implementations should strive to be as efficient as possible. Many unused features require external calls, memory allocation, unneeded calculation, and more.  The contracts are set up to be extensible by way of the addition of new exit games to the system.  Optional  or unimplemented features in current exit games should be removed for simplicity s sake, as they currently make up a large portion of the codebase.  Examples  Output guard handlers  These offer very little utility in the current contracts. The main contract, PaymentOutputGuardHandler, has three functions:  isValid enforces that some  preimage  value passed in via calldata has a length of zero. This could be removed along with the unused  preimage  parameter. getExitTarget converts a bytes20 to address payable (with the help of AddressPayable.sol). This could be removed in favor of using AddressPayable directly where needed. getConfirmSigAddress simply returns an empty address. This should be removed wherever used - empty fields should be a rare exception or an error, rather than being injected as unused values into critical functions.   The minimal utility offered comes at the price of using an external call to the OutputGuardHandlerRegistry, as well as an external call for each of the functions mentioned above. Overall, the existence of output guard handlers adds thousands of gas to the exit process. Referenced contracts: IOutputGuardHandler, OutputGuardModel, PaymentOutputGuardHandler, OutputGuardHandlerRegistry  Payment router arguments  Several fields in the exit router structs are marked  optional,  and are not used in the contracts. While this is not particularly impactful, it does clutter and confuse the contracts. Many  optional  fields are referenced and passed into functions which do not use them. Of note is the crucially-important signature verification function, PaymentOutputToPaymentTxCondition.verify, where StartExitData.inputSpendingConditionOptionalArgs resolves to an unnamed parameter:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartInFlightExit.sol:L323-L332  bool isSpentByInFlightTx = condition.verify(  exitData.inputTxs[i],  exitData.inputUtxosPos[i].outputIndex(),  exitData.inputUtxosPos[i].txPos().value,  exitData.inFlightTxRaw,  i,  exitData.inFlightTxWitnesses[i],  exitData.inputSpendingConditionOptionalArgs[i]  );  require(isSpentByInFlightTx, \"Spending condition failed\");  code/plasma_framework/contracts/src/exits/payment/spendingConditions/PaymentOutputToPaymentTxCondition.sol:L40-L47  function verify(  bytes calldata inputTxBytes,  uint16 outputIndex,  uint256 inputTxPos,  bytes calldata spendingTxBytes,  uint16 inputIndex,  bytes calldata signature,  bytes calldata /*optionalArgs*/  The additional fields clutter the namespace of each struct, confusing the purpose of the other fields. For example, PaymentInFlightExitRouterArgs.StartExitArgs features two fields, inputTxsConfirmSigs and inFlightTxsWitnesses, the former of which is marked  optional . In fact, the inFlightTxsWitnesses field ends up containing the signatures passed to the spending condition verifier and ECDSA library:  code/plasma_framework/contracts/src/exits/payment/routers/PaymentInFlightExitRouterArgs.sol:L4-L24  /**  @notice Wraps arguments for startInFlightExit.  @param inFlightTx RLP encoded in-flight transaction.  @param inputTxs Transactions that created the inputs to the in-flight transaction. In the same order as in-flight transaction inputs.  @param inputUtxosPos Utxos that represent in-flight transaction inputs. In the same order as input transactions.  @param outputGuardPreimagesForInputs (Optional) Output guard pre-images for in-flight transaction inputs. Length must always match that of the inputTxs  @param inputTxsInclusionProofs Merkle proofs that show the input-creating transactions are valid. In the same order as input transactions.  @param inputTxsConfirmSigs (Optional) Confirm signatures for the input txs. Should be empty bytes if the input tx is MoreVP. Length must always match that of the inputTxs  @param inFlightTxWitnesses Witnesses for in-flight transaction. In the same order as input transactions.  @param inputSpendingConditionOptionalArgs (Optional) Additional args for the spending condition for checking inputs. Should provide empty bytes if nothing is required. Length must always match that of the inputTxs  /  struct StartExitArgs {  bytes inFlightTx;  bytes[] inputTxs;  uint256[] inputUtxosPos;  bytes[] outputGuardPreimagesForInputs;  bytes[] inputTxsInclusionProofs;  bytes[] inputTxsConfirmSigs;  bytes[] inFlightTxWitnesses;  bytes[] inputSpendingConditionOptionalArgs;  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/457  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.13 Recommendation: Remove WireTransaction and PaymentOutputModel. Fold functionality into an extended PaymentTransactionModel ", "body": "  Description  RLP decoding is performed on transaction bytes in each of WireTransaction, PaymentOutputModel, and PaymentTransactionModel. The latter is the primary decoding function for transactions, while the former two contracts deal with outputs specifically.  Both WireTransaction and PaymentOutputModel make use of RLPReader to decode transaction objects, and both implement very similar features. Rather than having a codebase with two separate definitions for struct Output, PaymentTransactionModel should be extended to implement all required functionality.  Examples  PaymentTransactionModel should include three distinct decoding functions:  decodeDepositTx decodes a deposit transaction, which has no inputs and exactly 1 output. decodeSpendTx decodes a spend transaction, which has exactly 4 inputs and 4 outputs. decodeOutput decodes an output, which is a long list with 4 fields (uint, address, address, uint)  A mock implementation including decodeSpendTx and decodeOutput is shown here: https://gist.github.com/wadeAlexC/7820c0cd82fd5fdc11a0ad58a84165ae  OmiseGo may want to consider enforcing restrictions on the ordering of empty and nonempty fields here as well.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/456  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.14 ECDSA error value is not handled    Addressed", "body": "  Resolution   This was addressed in   commit 32288ccff5b867a7477b4eaf3beb0587a4684d7a by adding a check that the returned value is nonzero.  Description  The OpenZeppelin ECDSA library returns address(0x00) for many cases with malformed signatures:  contracts/cryptography/ECDSA.sol:L57-L63  if (uint256(s) > 0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0) {  return address(0);  if (v != 27 && v != 28) {  return address(0);  The PaymentOutputToPaymentTxCondition contract does not explicitly handle this case:  code/plasma_framework/contracts/src/exits/payment/spendingConditions/PaymentOutputToPaymentTxCondition.sol:L65-L68  address payable owner = inputTx.outputs[outputIndex].owner();  require(owner == ECDSA.recover(eip712.hashTx(spendingTx), signature), \"Tx in not signed correctly\");  return true;  Recommendation  Adding a check to handle this case will make it easier to reason about the code.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/454  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.15 No existence checks on framework block and timestamp reads    Addressed", "body": "  Resolution   This was addressed in   commit c5e5a460a2082b809a2c45b2d6a69b738b34937a by adding checks that block root and timestamp reads return nonzero values.  Description  The exit game libraries make several queries to the main PlasmaFramework contract where plasma block hashes and timestamps are stored. In multiple locations, the return values of these queries are not checked for existence.  Examples  PaymentStartStandardExit.setupStartStandardExitData:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentStartStandardExit.sol:L124  (, uint256 blockTimestamp) = controller.framework.blocks(utxoPos.blockNum());  PaymentChallengeIFENotCanonical.respond:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L176  (bytes32 root, ) = self.framework.blocks(utxoPos.blockNum());  PaymentPiggybackInFlightExit.enqueue:  code/plasma_framework/contracts/src/exits/payment/controllers/PaymentPiggybackInFlightExit.sol:L167  (, uint256 blockTimestamp) = controller.framework.blocks(utxoPos.blockNum());  TxFinalizationVerifier.checkInclusionProof:  code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L54  (bytes32 root,) = data.framework.blocks(data.txPos.blockNum());  Recommendation  Although none of these examples seem exploitable, adding existence checks makes it easier to reason about the code. Each query to PlasmaFramework.blocks should be followed with a check that the returned value is nonzero.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/463  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.16 BondSize - effectiveUpdateTime should be uint64 ", "body": "  Description  In BondSize, the mechanism to update the size of the bond has a grace period after which the new bond size becomes active.  When updating the bond size, the time is casted as a uint64 and saved in a uint128 variable.  code/plasma_framework/contracts/src/exits/utils/BondSize.sol:L24  uint128 effectiveUpdateTime;  code/plasma_framework/contracts/src/exits/utils/BondSize.sol:L11  uint64 constant public WAITING_PERIOD = 2 days;  code/plasma_framework/contracts/src/exits/utils/BondSize.sol:L57  self.effectiveUpdateTime = uint64(now) + WAITING_PERIOD;  There s no need to use a uint128 to save the time if it never will take up that much space.  Recommendation  Change the type of the effectiveUpdateTime to uint64.  uint128 effectiveUpdateTime;  + uint64 effectiveUpdateTime;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.17 PaymentExitGame contains several redundant plasmaFramework declarations ", "body": "  Description  PaymentExitGame inherits from both PaymentInFlightExitRouter and PaymentStandardExitRouter. All three contracts declare and initialize their own PlasmaFramework variable. This pattern can be misleading, and may lead to subtle issues in future versions of the code.  Examples  PaymentExitGame declaration:  code/plasma_framework/contracts/src/exits/payment/PaymentExitGame.sol:L18  PlasmaFramework private plasmaFramework;  PaymentInFlightExitRouter declaration:  code/plasma_framework/contracts/src/exits/payment/routers/PaymentInFlightExitRouter.sol:L53  PlasmaFramework private framework;  PaymentStandardExitRouter declaration:  code/plasma_framework/contracts/src/exits/payment/routers/PaymentStandardExitRouter.sol:L45  PlasmaFramework private framework;  Each variable is initialized in the corresponding file s constructor.  Recommendation  Introduce an inherited contract common to PaymentStandardExitRouter and PaymentInFlightExitRouter with the PlasmaFramework variable. Make the variable internal so it is visible to inheriting contracts.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.18 BlockController - inaccurate description of childBlockInterval for submitDepositBlock ", "body": "  Description  code/plasma_framework/contracts/src/framework/BlockController.sol:L96-L114  /**  @notice Submits a block for deposit  @dev Block number adds 1 per submission; it's possible to have at most 'childBlockInterval' deposit blocks between two child chain blocks  @param _blockRoot Merkle root of the Plasma block  @return The deposit block number  /  function submitDepositBlock(bytes32 _blockRoot) public onlyFromNonQuarantinedVault returns (uint256) {  require(isChildChainActivated == true, \"Child chain has not been activated by authority address yet\");  require(nextDeposit < childBlockInterval, \"Exceeded limit of deposits per child block interval\");  uint256 blknum = nextDepositBlock();  blocks[blknum] = BlockModel.Block({  root : _blockRoot,  timestamp : block.timestamp  });  nextDeposit++;  return blknum;  However, the comment at line 98 mentions the following:  [..] it s possible to have at most  childBlockInterval  deposit blocks between two child chain blocks [..]  This comment is inaccurate, as a childBlockInterval of 1 would not allow deposits at all (Note how nextDeposit is always >=1).  Remediation  The comment should read: [..] it s possible to have at most  childBlockInterval -1  deposit blocks between two child chain blocks [..]. Make sure to properly validate inputs for these values when deploying the contract to avoid obvious misconfiguration.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.19 PlasmaFramework - Can omit inheritance of VaultRegistry ", "body": "  Description  The contract PlasmaFramework inherits VaultRegistry even though it does not use any of the methods directly. Also BlockController inherits VaultRegistry effectively adding all of the needed functionality in there.  Remediation  PlasmaFramework does not need to inherit VaultRegistry, thus the import and the inheritance can be removed from PlasmaFramework.sol.  import \"./BlockController.sol\";  import \"./ExitGameController.sol\";  import \"./registries/VaultRegistry.sol\";  import \"./registries/ExitGameRegistry.sol\";  contract PlasmaFramework is VaultRegistry, ExitGameRegistry, ExitGameController, BlockController {  +contract PlasmaFramework is ExitGameRegistry, ExitGameController, BlockController {  uint256 public constant CHILD_BLOCK_INTERVAL = 1000;  /**  All tests still pass after removing the inheritance.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "5.20 BlockController - maintainer should be the only entity to set new authority    Addressed", "body": "  Resolution   This was addressed in   commit 25c2560e3b2e40ce9a10c40da97c3f79afc2c641 with the removal of the  Description  code/plasma_framework/contracts/src/framework/BlockController.sol:L69-L72  function setAuthority(address newAuthority) external onlyFrom(authority) {  require(newAuthority != address(0), \"Authority address cannot be zero\");  authority = newAuthority;  security specification notes that the  Authority: EOA used exclusively to submit plasma block hashes to the root chain. The child chain assumes at deployment that the authority account has nonce zero and no transactions have been sent from it.  However, no transactions might not be possible as authority is the only one to activateChildChain. Once activated, the child chain cannot be de-activated but the authority can change.  elixir-omg#managing-the-operator-address notes the following for operator aka authority:  As a consequence, the operator address must never send any other transactions, if it intends to continue submitting blocks. (Workarounds to this limitation are available, if there s such requirement.)  Additionally, setAuthority should emit an event to allow participants to react to this change in the system and have an audit trial.  Remediation  Remove the setAuthority function, or clarify its intended purpose and add an event so it can be detected by users.  Corresponding issue in plasma-contracts repo: https://github.com/omisego/plasma-contracts/issues/403  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/omisego-morevp/"}, {"title": "4.1 Node Operators Can Stake Validators That Were Not Proposed by Them. ", "body": "  In GeodeFi system node operators are meant to add the new validators in two steps:  Proposal step where 1 ETH of the pre-stake deposit is committed.  Stake step, where the 1 ETH pre-stake is reimbursed to the node operator, and the 32ETH user stake is sent to a validator.  The issue itself stems from the fact that node operators are allowed to stake the validators of the other node operators. In the stake() function there is no check of the validator s operatorId against the operator performing the stake. Meaning that node operator A can stake validators of node operator B.  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L1478-L1558  function stake(  PooledStaking storage self,  DSML.IsolatedStorage storage DATASTORE,  uint256 operatorId,  bytes[] calldata pubkeys  ) external {  _authenticate(DATASTORE, operatorId, false, true, [true, false]);  require(  (pubkeys.length > 0) && (pubkeys.length <= DCL.MAX_DEPOSITS_PER_CALL),  \"SML:1 - 50 validators\"  );  uint256 _verificationIndex = self.VERIFICATION_INDEX;  for (uint256 j = 0; j < pubkeys.length; ) {  require(  _canStake(self, pubkeys[j], _verificationIndex),  \"SML:NOT all pubkeys are stakeable\"  );  unchecked {  j += 1;  bytes32 activeValKey = DSML.getKey(operatorId, rks.activeValidators);  bytes32 proposedValKey = DSML.getKey(operatorId, rks.proposedValidators);  uint256 poolId = self.validators[pubkeys[0]].poolId;  bytes memory withdrawalCredential = DATASTORE.readBytes(poolId, rks.withdrawalCredential);  uint256 lastIdChange = 0;  for (uint256 i = 0; i < pubkeys.length; ) {  uint256 newPoolId = self.validators[pubkeys[i]].poolId;  if (poolId != newPoolId) {  uint256 sinceLastIdChange;  unchecked {  sinceLastIdChange = i - lastIdChange;  DATASTORE.subUint(poolId, rks.secured, (DCL.DEPOSIT_AMOUNT * (sinceLastIdChange)));  DATASTORE.subUint(poolId, proposedValKey, (sinceLastIdChange));  DATASTORE.addUint(poolId, activeValKey, (sinceLastIdChange));  lastIdChange = i;  poolId = newPoolId;  withdrawalCredential = DATASTORE.readBytes(poolId, rks.withdrawalCredential);  DCL.depositValidator(  pubkeys[i],  withdrawalCredential,  self.validators[pubkeys[i]].signature31,  (DCL.DEPOSIT_AMOUNT - DCL.DEPOSIT_AMOUNT_PRESTAKE)  );  self.validators[pubkeys[i]].state = VALIDATOR_STATE.ACTIVE;  unchecked {  i += 1;  uint256 sinceLastIdChange;  unchecked {  sinceLastIdChange = pubkeys.length - lastIdChange;  if (sinceLastIdChange > 0) {  DATASTORE.subUint(poolId, rks.secured, DCL.DEPOSIT_AMOUNT * (sinceLastIdChange));  DATASTORE.subUint(poolId, proposedValKey, (sinceLastIdChange));  DATASTORE.addUint(poolId, activeValKey, (sinceLastIdChange));  _increaseWalletBalance(DATASTORE, operatorId, DCL.DEPOSIT_AMOUNT_PRESTAKE * pubkeys.length);  emit Stake(pubkeys);  This issue can later be escalated to a point where funds can be stolen. Consider the following case:  The attacker creates 10 validators directly through the ETH2 deposit contract using himself as the withdrawal address.  Attacker node operator proposes to add 10 validators and adds the 10ETH as pre-stake deposit. Since validators already exist withdrawal credentials will remain those of the Attacker. As a result of those actions, we have inflated the number of proposed validators the attacker has inside the Geode system.  Attacker then takes the validator keys proposed by someone else and stakes them. Since there is no check described above that is allowed. His proposed validators count will also decrease without a revert due to steps above.  As a result of that step, attacker will receive the pre-stake of the operator that actually proposed those validators. The attacker will immediately proceed to call decreaseWallet() to withdraw the funds.  The attacker will then withdraw the pre-stake he deposited in the initial validators with faulty withdrawal credential.  This way an attacker could profit 10ETH.  This can be prevented by making sure that validator s operatorId is checked on the stake() function call.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.2 Cannot Blame Operator for Proposed Validator ", "body": "  In the current code, anyone can blame an operator who does not withdraw in time:  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L931-L946  function blameOperator(  PooledStaking storage self,  DSML.IsolatedStorage storage DATASTORE,  bytes calldata pk  ) external {  require(  self.validators[pk].state == VALIDATOR_STATE.ACTIVE,  \"SML:validator is never activated\"  );  require(  block.timestamp > self.validators[pk].createdAt + self.validators[pk].period,  \"SML:validator is active\"  );  _imprison(DATASTORE, self.validators[pk].operatorId, pk);  There is one more scenario where the operator should be blamed. When a validator is in the PROPOSED state, only the operator can call the stake function to actually stake the rest of the funds. Before that, the funds of the pool will be locked under the rks.secured variable. So the malicious operator can lock up 31 ETH of the pool indefinitely by locking up only 1 ETH of the attacker. There is currently no way to release these 31 ETH.  We recommend introducing a mechanism that allows one to blame the operator for not staking for a long time after it was approved.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.3 Validators Array Length Has to Be Updated When the Validator Is Alienated. ", "body": "  In GeodeFi when the node operator creates a validator with incorrect withdrawal credentials or signatures the Oracle has the ability to alienate this validator. In the process of alienation, the validator status is updated.  contracts/Portal/modules/StakeModule/libs/OracleExtensionLib.sol:L111-L136  function _alienateValidator(  SML.PooledStaking storage STAKE,  DSML.IsolatedStorage storage DATASTORE,  uint256 verificationIndex,  bytes calldata _pk  ) internal {  require(STAKE.validators[_pk].index <= verificationIndex, \"OEL:unexpected index\");  require(  STAKE.validators[_pk].state == VALIDATOR_STATE.PROPOSED,  \"OEL:NOT all pubkeys are pending\"  );  uint256 operatorId = STAKE.validators[_pk].operatorId;  SML._imprison(DATASTORE, operatorId, _pk);  uint256 poolId = STAKE.validators[_pk].poolId;  DATASTORE.subUint(poolId, rks.secured, DCL.DEPOSIT_AMOUNT);  DATASTORE.addUint(poolId, rks.surplus, DCL.DEPOSIT_AMOUNT);  DATASTORE.subUint(poolId, DSML.getKey(operatorId, rks.proposedValidators), 1);  DATASTORE.addUint(poolId, DSML.getKey(operatorId, rks.alienValidators), 1);  STAKE.validators[_pk].state = VALIDATOR_STATE.ALIENATED;  emit Alienated(_pk);  An additional thing that has to be done during the alienation process is that the validator s count should be decreased in order for the monopoly threshold to be calculated correctly. That is because the length of the validators array is used twice in the OpeartorAllowance function:  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L975  uint256 numOperatorValidators = DATASTORE.readUint(operatorId, rks.validators);  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L988  uint256 numPoolValidators = DATASTORE.readUint(poolId, rks.validators);  Without the update of the array length, the monopoly threshold as well as the time when the fallback operator will be able to participate is going to be computed incorrectly.  It could be beneficial to not refer to rks.validators in the operator allowance function and instead use the rks.proposedValidators + rks.alienatedValidators + rks.activeValidators. This way allowance function can always rely on the most up to date data.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.4 A Potential Controller Update Issue. ", "body": "  We identified a potential issue in the code that is out of our current scope. In the GeodeModuleLib, there is a function that allows a controller of any ID to update the controller address:  contracts/Portal/modules/GeodeModule/libs/GeodeModuleLib.sol:L299-L309  function changeIdCONTROLLER(  DSML.IsolatedStorage storage DATASTORE,  uint256 id,  address newCONTROLLER  ) external onlyController(DATASTORE, id) {  require(newCONTROLLER != address(0), \"GML:CONTROLLER can not be zero\");  DATASTORE.writeAddress(id, rks.CONTROLLER, newCONTROLLER);  emit ControllerChanged(id, newCONTROLLER);  It s becoming tricky with the upgradability mechanism. The current version of any package is stored in the following format: DATASTORE.readAddress(versionId, rks. CONTROLLER). So the address of the current implementation of any package is stored as rks.CONTROLLER. That means if someone can hack the implementation address and make a transaction on its behalf to change the controller, this attacker can change the current implementation to a malicious one.  While this issue may not be exploitable now, many new packages are still to be implemented. So you need to ensure that nobody can get any control over the implementation contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.5 The Price Change Limit Could Prevent the Setting of the Correct Price. ", "body": "  In the share price update logic of OracleExtensionLib, there is a function called   ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.6 Potential for a Cross-Site-Scripting When Creating a Pool. ", "body": "  When creating a new staking pool, the creator has the ability to name it. While it does not present many issues on the chain, if this name is ever displayed on the UI it has to be handled carefully. An attacker could include a malicious script in the name and that could potentially be executed in the victim s browser.  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L358  DATASTORE.writeBytes(poolId, rks.NAME, name);  We suggest that proper escaping is used when displaying the names of the pool on the UI. We do not recommend adding string validation on the chain.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.1 TransactionManager - User might steal router s locked funds    ", "body": "  Resolution  This issue has been fixed.  Description  Recommendation  Consider using a data structure different than issuedShares for storing user deposits. This way, withdrawals by users will only be allowed when calling TransactionManager.cancel.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.2 TransactionManager - Receiver-side check also on sending side    Unverified Fix", "body": "  Resolution   The Connext team claims to have fixed this in commit   4adbfd52703441ee5de655130fc2e0252eae4661. We have not reviewed this commit or, generally, the codebase at this point.  Description  The functions prepare, cancel, and fulfill in the TransactionManager all have a  common part  that is executed on both the sending and the receiving chain and side-specific parts that are only executed either on the sending or on the receiving side. The following lines occur in fulfill s common part, but this should only be checked on the receiving chain. In fact, on the sending chain, we might even compare amounts of different assets.  code2/packages/contracts/contracts/TransactionManager.sol:L476-L478  // Sanity check: fee <= amount. Allow `=` in case of only wanting to execute  // 0-value crosschain tx, so only providing the fee amount  require(relayerFee <= txData.amount, \"#F:023\");  This could prevent a legitimate fulfill on the sending chain, causing a loss of funds for the router.  Recommendation  Move these lines to the receiving-side part.  Remark  The callData supplied to fulfill is not used at all on the sending chain, but the check whether its hash matches txData.callDataHash happens in the common part.  code2/packages/contracts/contracts/TransactionManager.sol:L480-L481  // Check provided callData matches stored hash  require(keccak256(callData) == txData.callDataHash, \"#F:024\");  In principle, this check could also be moved to the receiving-chain part, allowing the router to save some gas by calling sending-side fulfill with empty callData and skip the check. Note, however, that the TransactionFulfilled event will then also emit the  wrong  callData on the sending chain, so the off-chain code has to be able to deal with that if you want to employ this optimization.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.3 TransactionManager - Flawed shares arithmetic    ", "body": "  Resolution  Comment from Connext:  We removed the shares logic completely, and are instead only focusing on standard tokens (i.e. rebasing, inflationary, and deflationary tokens are not supported directly). If users want to transfer non-standard tokens, they do so at their own risk.  Description  To support a wide variety of tokens, the TransactionManager uses a per-asset shares system to represent fractional ownership of the contract s balance in a token. There are several flaws in the shares-related arithmetic, such as:  addLiquidity and sender-side prepare convert asset amounts 1:1 to shares, instead of taking the current value of a share into account. A 1:1 conversion is only appropriate if the number of shares is 0, for example, for the first deposit.  The WadRayMath library is not used correctly (and maybe not the ideal tool for the task in the first place): rayMul and rayDiv each operate on two rays (decimal numbers with 27 digits) but are not used according to that specification in getAmountFromIssuedShares and getIssuedSharesFromAmount. The scaling errors cancel each other out, though.  The WadRayMath library rounds to the nearest representable number. That might not be desirable for NXTP; for example, converting a token amount to shares and back to tokens might lead to a higher amount than we started with.  The WadRayMath library reverts on overflows, which might not be acceptable behavior. For instance, a receiver-side fulfill might fail due to an overflow in the conversion from shares to a token amount. The corresponding fulfill on the sending chain might very well succeed, though, and it is possible that, at a later point, the receiver-side transaction can be canceled. (Note that canceling does not involve actually converting shares into a token amount, but the calculation is done anyway for the event.)  The amount emitted in the TransactionPrepared event on the receiving chain can, depending on the granularity of the shares, differ considerably from what a user can expect to receive when the shares are converted back into tokens. The reason for this is the double conversion from the initial token amount \u2014 which is emitted \u2014 to shares and, later, back to tokens.  Special cases might have to be taken into account. As a more subtle example, converting a non-zero token amount to shares is not possible (or at least not with the usual semantics) if the contract s balance is zero, but the number of already existing shares is strictly greater than zero, as any number of shares will give you back less than the original amount. Whether this situation is possible depends on the token contract.  Recommendation  The shares logic was added late to the contract and is still in a pretty rough shape. While providing a full-fledged solution is beyond the scope of this review, we hope that the points raised above provide pointers and guidelines to inform a major overhaul.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.4 Router - handleMetaTxRequest - gas griefing / race conditions / missing validations / free meta transactions ", "body": "  Description  There s a comment in handleMetaTxRequest that asks whether data needs to be validated before interacting with the contract and the answer is yes, always, or else this opens up a gas griefing vector on the router side.  For example, someone might flood broadcast masses of metaTx requests (*.*.metatx) and all online routers will race to call TransactionManager.fulfill(). Even if only one transaction should be able to successfully go through all the others will loose on gas (until up to the first require failing).  Given that there is no rate limiting and it is a broadcast that is very cheap to perform on the client-side (I can just spawn a lot of nodes spamming messages) this can be very severe, keeping the router busy sending transactions that are deemed to fail until the routers balance falls below the min gas limit configured.  Even if the router would check the contracts current state first (performing read-only calls that can be done offline) to check if the transaction has a chance to succeed, it will still compete in a race for the current block (mempool).  Examples  code/packages/router/src/handler.ts:L459-L477  const fulfillData: MetaTxFulfillPayload = data.data;  // Validate that metatx request matches with known data about fulfill  // Is this needed? Can we just submit to chain without validating?  // Technically this is ok, but perhaps we want to validate only for our own  // logging purposes.  // Would also be bad if router had no gas here  // Next, prepare the tx object  // - Get chainId from data  // - Get fulfill fee from data and validate it covers gas  // - etc.  // Send to txService  // Update metrics  // TODO: make sure fee is something we want to accept  this.logger.info({ method, methodId, requestContext, chainId, data }, \"Submitting tx\");  const res = await this.txManager  .fulfill(  chainId,  Recommendation  For state-changing transactions that actually cost gas there is no way around implementing strict validation whenever possible and avoid performing the transaction in case validation fails. Contract state should always be validated before issuing new online transactions but this might not fix the problem that routers still compete for their transaction to be included in the next block (mempool not monitored). The question therefore is, whether it would be better to change the metaTX flow to have a router confirm that they will send the tx via the messaging service first so others know they do not even have to try to send it. However, even this scenario may allow to DoS the system by maliciously responding with such a method.  In general, there re a lot of ways to craft a message that forces the router to issue an on-chain transaction that may fail with no consequences for the sender of the metaTx message.  Additionally, the relayerFee is currently unchecked which may lead to the router loosing funds because they effectively accept zero-fee relays.  As noted in issue 4.6, the missing return after detecting that the metatx is destined for a TransactionManager that is not supported allows for explicit gas griefing attacks (deploy a fake TransactionManager.fulfill that mines all the gas for a beneficiary).  The contract methods should additionally validate that the sender balance can cover for the gas required to send the transaction.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.5 Router - subgraphLoop may process transactions the router was not configured for (code fragility) ", "body": "  Description  subgraphLoop gets all sending transactions for the router, chain, status triplet.  code/packages/router/src/subgraph.ts:L155-L159  allSenderPrepared = await sdk.GetSenderTransactions({  routerId: this.routerAddress.toLowerCase(),  sendingChainId: chainId,  status: TransactionStatus.Prepared,  });  and then sorts the results by receiving chain id. Note that this keeps track of chainID s the router was not configured for.  code/packages/router/src/subgraph.ts:L168-L176  // create list of txIds for each receiving chain  const receivingChains: Record<string, string[]> = {};  allSenderPrepared.router?.transactions.forEach(({ transactionId, receivingChainId }) => {  if (receivingChains[receivingChainId]) {  receivingChains[receivingChainId].push(transactionId);  } else {  receivingChains[receivingChainId] = [transactionId];  });  In a next step, transactions are resolved from the various chains. This filters out chainID s the router was not configured for (and just returns an empty array), however, the GetTransactions query assumes that transactionID s are unique across the subgraph which might not be true!  code/packages/router/src/subgraph.ts:L179-L193  let correspondingReceiverTxs: any[];  try {  const queries = await Promise.all(  Object.entries(receivingChains).map(async ([cId, txIds]) => {  const _sdk = this.sdks[Number(cId)];  if (!_sdk) {  this.logger.error({ chainId: cId, method, methodId }, \"No config for chain, this should not happen\");  return [];  const query = await _sdk.GetTransactions({ transactionIds: txIds.map((t) => t.toLowerCase()) });  return query.transactions;  }),  );  correspondingReceiverTxs = queries.flat();  } catch (err) {  In the last step, all chainID s (even the one s the router was not configured for) are iterated again (which might be unnecessary). TransactionID s are loosely matched from the previously flattened results from all the various chains. Since transactionID s don t necessarily need to be unique across chains or within the chain, it is likely that the subsequent matching of transactionID s (correspondingReceiverTxs.find) returns more than 1 entry. However, find() just returns the first item and covers up the fact that there might be multiple matches. Also, since the code returned an empty array for chains it was not configured for, the find will return undef satisfying the !corresponding branch and fire an SenderTransactionPrepared triggering the handler to perform an on-chain action that will most definitely fail at some point.  Recommendation  The code in this module is generally very fragile. It is based on assumptions that can likely be exploited by a third party re-using transactionID s (or other values). It is highly recommended to rework the code making it more resilient to potential corner cases.  Filter receivingChains for chainID s that are not supported by the router  Avoid having to integrating the allSenderPrepared array twice and use a filtered list instead  Change the very broad query in _sdk.GetTransactions() that assumes transactionID s are unique across all chains to a specific query that selects transactions specific to the chain and this router. The more specific the better!  When matching the transactions also match the source/receiver chains instead of only matching the transactionID. Additionally, check if more than one entry matches the condition instead of silently taking the first result (this is what array.find() does)  Also see issue 5.2  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.6 Router - handler reports an error condition but continues execution instead of aborting it ", "body": "  Description  There are some code paths that detect and log an error but then continue the execution flow instead of returning the error condition to the caller. This may allow for a variety of griefing vectors (e.g. gas griefing).  Examples  reports an error because the received address does not match our configured transaction manager, but then proceeds. This means the router would accept a transaction manager it was not configured for.  code/packages/router/src/handler.ts:L448-L458  if (utils.getAddress(data.to) !== utils.getAddress(chainConfig.transactionManagerAddress)) {  const err = new HandlerError(HandlerError.reasons.ConfigError, {  requestContext,  calling: \"chainConfig.transactionManagerAddress\",  methodId,  method,  configError: `Provided transactionManagerAddress does not map to our configured transactionManagerAddress`,  });  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");  If chainConfig is undef this should return or else it will bail later  code/packages/router/src/handler.ts:L436-L445  if (!chainConfig) {  const err = new HandlerError(HandlerError.reasons.ConfigError, {  requestContext,  calling: \"getConfig\",  methodId,  method,  configError: `No chainConfig for ${chainId}`,  });  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");  if data is not fulfill this silently returns, while it should probably raise an error instead (unexpected message)  code/packages/router/src/handler.ts:L447-L447  if (data.type === \"Fulfill\") {  Recommendation  Implement strict validation of untrusted data.  Be explicit and raise error conditions on unexpected messages (e.g. type is not fulfill) instead of silently skipping the message.  Add the missing returns after reporting an error instead of continuing the execution flow on errors.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.7 Router - spawns unauthenticated admin API endpoint listening on all interfaces ", "body": "  Description  unauthenticated  listening on allips  pot. allows any local or remote unpriv user with access to the endpoint to steal the routers liquidity /remove-liquidity -> req.body.recipientAddress  Examples  code/packages/router/src/index.ts:L123-L130  server.listen(8080, \"0.0.0.0\", (err, address) => {  if (err) {  console.error(err);  process.exit(1);  console.log(`Server listening at ${address}`);  });  Recommendation  require authentication  should only bind to localhost by default  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.8 TODO comments should be resolved ", "body": "  Description  As part of the process of bringing the application to production readiness, dev comments (especially TODOs) should be resolved. In many cases, these comments indicate a missing functionality that should be implemented, or some missing necessary validation checks.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.9 TransactionManager - Missing nonReentrant modifier on removeLiquidity    ", "body": "  Resolution  This issue has been fixed.  Description  The removeLiquidity function does not have a nonReentrant modifier.  code/packages/contracts/contracts/TransactionManager.sol:L274-L329  /**  @notice This is used by any router to decrease their available  liquidity for a given asset.  @param shares The amount of liquidity to remove for the router in shares  @param assetId The address (or `address(0)` if native asset) of the  asset you're removing liquidity for  @param recipient The address that will receive the liquidity being removed  /  function removeLiquidity(  uint256 shares,  address assetId,  address payable recipient  ) external override {  // Sanity check: recipient is sensible  require(recipient != address(0), \"#RL:007\");  // Sanity check: nonzero shares  require(shares > 0, \"#RL:035\");  // Get stored router shares  uint256 routerShares = issuedShares[msg.sender][assetId];  // Get stored outstanding shares  uint256 outstanding = outstandingShares[assetId];  // Sanity check: owns enough shares  require(routerShares >= shares, \"#RL:018\");  // Convert shares to amount  uint256 amount = getAmountFromIssuedShares(  shares,  outstanding,  Asset.getOwnBalance(assetId)  );  // Update router issued shares  // NOTE: unchecked due to require above  unchecked {  issuedShares[msg.sender][assetId] = routerShares - shares;  // Update the total shares for asset  outstandingShares[assetId] = outstanding - shares;  // Transfer from contract to specified recipient  Asset.transferAsset(assetId, recipient, amount);  // Emit event  emit LiquidityRemoved(  msg.sender,  assetId,  shares,  amount,  recipient  );  Assuming we re dealing with a token contract that allows execution of third-party-supplied code, that means it is possible to leave the TransactionManager contract in one of the functions that call into the token contract and then reenter via removeLiquidity. Alternatively, we can leave the contract in removeLiquidity and reenter through an arbitrary external function, even if it has a nonReentrant modifier.  Example  Recommendation  While tokens that behave as described in the example might be rare or not exist at all, caution is advised when integrating with unknown tokens or calling untrusted code in general. We strongly recommend adding a nonReentrant modifier to removeLiquidity.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.10 TransactionManager - Relayer may use user s cancel after expiry signature to steal user s funds by colluding with a router   ", "body": "  Resolution   This has been acknowledged by the Connext team. As discussed below in the  Recommendation , it is not a flaw in the contracts   Description  Users that are willing to have a lower trust dependency on a relayer should have the ability to opt-in only for the service that allows the relayer to withdraw back users  funds from the sending chain after expiry. However, in practice, a user is forced to opt-in for the service that refunds the router before the expiry, since the same signature is used for both services (lines 795,817 use the same signature).  Let s consider the case of a user willing to call fulfill on his own, but to use the relayer only to withdraw back his funds from the sending chain after expiry. In this case, the relayer can collude with the router and use the user s cancel signature (meant for withdrawing his only after expiry) as a front-running transaction for a user call to fulfill. This way the router will be able to withdraw both his funds and the user s funds since the user s fulfill signature is now public data residing in the mem-pool.  Examples  code/packages/contracts/contracts/TransactionManager.sol:L795-L817  require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");  Asset.transferAsset(txData.sendingAssetId, payable(msg.sender), relayerFee);  // Get the amount to refund the user  uint256 toRefund;  unchecked {  toRefund = amount - relayerFee;  // Return locked funds to sending chain fallback  if (toRefund > 0) {  Asset.transferAsset(txData.sendingAssetId, payable(txData.sendingChainFallback), toRefund);  } else {  // Receiver side, router liquidity is returned  if (txData.expiry >= block.timestamp) {  // Timeout has not expired and tx may only be cancelled by user  // Validate signature  require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");  Recommendation  The crucial point here is that the user must never sign a  cancel  that could be used on the receiving chain while fulfillment on the sending chain is still a possibility. Or, to put it differently: A user may only sign a  cancel  that is valid on the receiving chain after sending-chain expiry or if they never have and won t ever sign a  fulfill  (or at least won t sign until sending-chain expiry \u2014 but it is pointless to sign a  fulfill  after that, so  never  is a reasonable simplification). Or, finally, a more symmetric perspective on this requirement: If a user has signed  fulfill , they must not sign a receiving-chain-valid  cancel  until sending-chain expiry, and if they have signed a receiving-chain-valid  cancel , they must not sign a  fulfill  (until sending-chain expiry).  In this sense,  cancel  signatures that are valid on the receiving chain are dangerous, while sending-side cancellations are not. So the principle stated in the previous paragraph might be easier to follow with different signatures for sending- and receiving-chain cancellations.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.11 Router - handleSenderPrepare - missing validation, unchecked bidExpiry, unchecked expiry, unchecked chainids/swaps, race conidtions ", "body": "  Description  This finding highlights a collection of issues with the handleSenderPrepare method. The code and coding style appears fragile. Validation should be strictly enforced and protective measures against potential race conditions should be implemented.  The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:  unchecked bidExpiry might allow using bids even after expiration.  code/packages/router/src/handler.ts:L612-L626  .andThen(() => {  // TODO: anything else? seems unnecessary to validate everything  if (!BigNumber.from(bid.amount).eq(amount) || bid.transactionId !== txData.transactionId) {  return err(  new HandlerError(HandlerError.reasons.PrepareValidationError, {  method,  methodId,  calling: \"\",  requestContext,  prepareError: \"Bid params not equal to tx data\",  }),  );  return ok(undefined);  });  unchecked txdata.expiry might lead to router preparing for an already expired prepare. However, this is rather unlikely easily exploitable as the data source is a subgraph.  a bid might not be fulfillable anymore due to changes to the router (e.g. removing a chainconfig or assets) but the router would still attempt it. Make sure to always verify chainid/assets/the configured system parameters.  potential race condition. make sure to lock the txID in the beginning.  code/packages/router/src/handler.ts:L663-L669  // encode the data for contract call  // Send to txService  this.receiverPreparing.set(txData.transactionId, true);  this.logger.info(  { method, methodId, requestContext, transactionId: txData.transactionId },  \"Sending receiver prepare tx\",  );  Note that transactionID s as they are used in the system must be unique across chains.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.12 Router - handleNewAuction - fragile code ", "body": "  Description  This finding highlights a collection of issues with the handleNewAuction. The code and coding style appears fragile. Validation should be strictly enforced, debugging code should be removed or disabled in production and protective measures should be taken from abusive clients.  The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:  router bids on zero-amount requests (this will fail later when calling the contract, thus a potential gas griefing attack vector)  code/packages/router/src/handler.ts:L197-L201  // validate that assets/chains are supported and there is enough liquidity  // and gas on both sender and receiver side.  // TODO: will need to track this offchain  const amountReceived = mutateAmount(amount);  duplicate constant var assignment (subfunction const shadowing and unchecked initial config!)  code/packages/router/src/handler.ts:L202-L204  const config = getConfig();  const sendingConfig = config.chainConfig[sendingChainId];  const receivingConfig = config.chainConfig[receivingChainId];  code/packages/router/src/handler.ts:L231-L240  // validate config  const config = getConfig();  const sendingConfig = config.chainConfig[sendingChainId];  const receivingConfig = config.chainConfig[receivingChainId];  if (  !sendingConfig.providers ||  sendingConfig.providers.length === 0 ||  !receivingConfig.providers ||  receivingConfig.providers.length === 0  ) {  actual estimated gas required to fuel transaction is never checked. current balance might be outdated, especially in race condition scenarios.  code/packages/router/src/handler.ts:L315-L318  .andThen((balances) => {  const [senderBalance, receiverBalance] = balances as BigNumber[];  if (senderBalance.lt(sendingConfig.minGas) || receiverBalance.lt(receivingConfig.minGas)) {  return errAsync(  remove debug code from production build (dry-run)  code/packages/router/src/handler.ts:L194-L194  dryRun,  code/packages/router/src/handler.ts:L385-L385  this.messagingService.publishAuctionResponse(inbox, { bid, bidSignature: dryRun ? undefined : bidSignature }),  signer address might be different for different chains  code/packages/router/src/handler.ts:L290-L312  return combine([  ResultAsync.fromPromise(  this.txService.getBalance(sendingChainId, this.signer.address),  (err) =>  new HandlerError(HandlerError.reasons.TxServiceError, {  calling: \"txService.getBalance => sending\",  method,  methodId,  requestContext,  txServiceError: jsonifyError(err as NxtpError),  }),  ),  ResultAsync.fromPromise(  this.txService.getBalance(receivingChainId, this.signer.address),  (err) =>  new HandlerError(HandlerError.reasons.TxServiceError, {  calling: \"txService.getBalance => receiving\",  method,  methodId,  requestContext,  txServiceError: jsonifyError(err as NxtpError),  }),  ),  no rate limiting. potential DoS vector when someone floods the node with auction requests (significant work to be done, handler is async, will trigger a reply message). user might force the router to sign the same message multiple times.  missing validation of bid parameters (expiriy within valid range, \u2026)  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.13 Router - Cancel is not implemented ", "body": "  Description  Canceling of failed/expired swaps does not seem to be implemented in the router. This may allow a user to trick the router into preparing all its funds which will not automatically be reclaimed after expiration (router DoS).  Examples  cancelExpired is never called  code/packages/sdk/src/sdk.ts:L873-L885  // TODO: this just cancels a transaction, it is misnamed, has nothing to do with expiries  public async cancelExpired(cancelParams: CancelParams, chainId: number): Promise<providers.TransactionResponse> {  const method = this.cancelExpired.name;  const methodId = getRandomBytes32();  this.logger.info({ method, methodId, cancelParams, chainId }, \"Method started\");  const cancelRes = await this.transactionManager.cancel(chainId, cancelParams);  if (cancelRes.isOk()) {  this.logger.info({ method, methodId }, \"Method complete\");  return cancelRes.value;  } else {  throw cancelRes.error;  disabled code  code/packages/router/src/handler.ts:L719-L733  \"Do not cancel ATM, figure out why we are in this case first\",  );  // const cancelRes = await this.txManager.cancel(txData.sendingChainId, {  //   txData,  //   signature: \"0x\",  //   relayerFee: \"0\",  // });  // if (cancelRes.isOk()) {  //   this.logger.warn(  //     { method, methodId, transactionHash: cancelRes.value.transactionHash },  //     \"Cancelled transaction\",  //   );  // } else {  //   this.logger.error({ method, methodId }, \"Could not cancel transaction after error!\");  // }  Recommendation  Implement the cancel flow.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.14 TransactionManager.prepare - Possible griefing/denial of service by front-running    Unverified Fix", "body": "  Resolution  Comment from Connext:  We see this as a highly unlikely attack vector and have chosen not to mitigate it, but it is possible. Users can always and easily generate a new key prepare from a new account, and performing this attack will always cost gas and some dust amount. Further, adding in the suggested require(msg.sender == invariantData.user) will lock out many contract-based use cases and requiring an additional signature/user interaction (auth, approve, prepare, fulfill) is not desirable.  The Connext team claims to have implemented this solution in commit 6811bb2681f44f34ce28906cb842db49fb73d797. We have not reviewed this commit or, generally, the codebase at this point.  Description  A call to TransactionManager.prepare might be front-run with a transaction using the same invariantData but with a different amount and/or expiry values. By choosing a tiny amount of assets, the attacker may prevent the user from locking his original desired amount. The attacker can repeat this process for any new transactionId presented by the user, thus effectively denying the service for him.  Recommendation  Consider adding a require(msg.sender == invariantData.user) restriction to TransactionManager.prepare.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.15 Router - Provide and enforce safe defaults (config) ", "body": "  Description  Chain confirmations default to 1 which is not safe. In case of a re-org the router might (temporarily) get out of sync with the chain and perform actions it should not perform. This may put funds at risk.  Examples  the schema requires an unsafe minimum of 1 confirmation  code/packages/router/src/config.ts:L33-L36  export const TChainConfig = Type.Object({  providers: Type.Array(Type.String()),  confirmations: Type.Number({ minimum: 1 }),  subgraph: Type.String(),  the default configuration uses 1 confirmation  code/packages/router/config.json.example:L1-L17  \"adminToken\": \"blahblah\",  \"chainConfig\": {  \"4\": {  \"providers\": [\"https://rinkeby.infura.io/v3/\"],  \"confirmations\": 1,  \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-rinkeby\"  },  \"5\": {  \"providers\": [\"https://goerli.infura.io/v3/\"],  \"confirmations\": 1,  \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-goerli\"  },  \"logLevel\": \"info\",  \"mnemonic\": \"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\"  Recommendation  Give guidance, provide and enforce safe defaults.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.16 ProposedOwnable - two-step ownership transfer should be confirmed by the new owner    ", "body": "  Resolution  All recommendations given below have been implemented. In addition to that, the privilege to manage assets and the privilege to manage routers can now be renounced separately.  Description  In order to avoid losing control of the contract, the two-step ownership transfer should be confirmed by the new owner s address instead of the current owner.  Examples  acceptProposedOwner is restricted to onlyOwner while ownership should be accepted by the newOwner  code/packages/contracts/contracts/ProposedOwnable.sol:L89-L96  /**  @notice Transfers ownership of the contract to a new account (`newOwner`).  Can only be called by the current owner.  /  function acceptProposedOwner() public virtual onlyOwner {  require((block.timestamp - _proposedTimestamp) > _delay, \"#APO:030\");  _setOwner(_proposed);  move renounced() to ProposedOwnable as this is where it logically belongs to  code/packages/contracts/contracts/TransactionManager.sol:L160-L162  function renounced() public view override returns (bool) {  return owner() == address(0);  onlyOwner can directly access state-var _owner instead of spending more gas on calling owner()  code/packages/contracts/contracts/ProposedOwnable.sol:L76-L79  modifier onlyOwner() {  require(owner() == msg.sender, \"#OO:029\");  _;  Recommendation  onlyOwner can directly access _owner (gas optimization)  add a method to explicitly renounce ownership of the contract  move TransactionManager.renounced() to ProposedOwnable as this is where it logically belongs to  change the access control for acceptProposedOwner from onlyOwner to require(msg.sender == _proposed) (new owner).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.17 FulfillInterpreter - Wrong order of actions in fallback handling ", "body": "  Description  code2/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L68-L90  bool isNative = LibAsset.isNativeAsset(assetId);  if (!isNative) {  LibAsset.increaseERC20Allowance(assetId, callTo, amount);  // Check if the callTo is a contract  bool success;  bytes memory returnData;  if (Address.isContract(callTo)) {  // Try to execute the callData  // the low level call will return `false` if its execution reverts  (success, returnData) = callTo.call{value: isNative ? amount : 0}(callData);  // Handle failure cases  if (!success) {  // If it fails, transfer to fallback  LibAsset.transferAsset(assetId, fallbackAddress, amount);  // Decrease allowance  if (!isNative) {  LibAsset.decreaseERC20Allowance(assetId, callTo, amount);  For the fallback scenario, i.e., the call isn t executed or fails, the funds are first transferred to fallbackAddress, and the previously increased allowance is decreased after that. If the token supports it, the recipient of the direct transfer could try to exploit that the approval hasn t been revoked yet, so the logically correct order is to decrease the allowance first and transfer the funds later. However, it should be noted that the FulfillInterpreter should, at any point in time, only hold the funds that are supposed to be transferred as part of the current transaction; if there are any excess funds, these are leftovers from a previous failure to withdraw everything that could have been withdrawn, so these can be considered up for grabs. Hence, this is only a minor issue.  Recommendation  We recommend reversing the order of actions for the fallback case: Decrease the allowance first, and transfer later. Moreover, it would be better to increase the allowance only in case a call will actually be made, i.e., if Address.isContract(callTo) is true.  Remark  This issue was already present in the original version of the code but was missed initially and only found during the re-audit.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.18 FulfillInterpreter - Executed event can t be linked to TransactionFulfilled event    ", "body": "  Resolution  This issue has been fixed. We d like to point out, though:  Based on the data emitted by the TransactionFulfilled event, it is currently not possible to distinguish between: (A) No call to callTo has been made because the address didn t contain code. (B) Address callTo did contain code, a call was made, and it failed with empty return data. If this distinction seems relevant, an additional bool should be returned from FulfillInterpreter.execute and emitted in TransactionFulfilled, indicating which of the two scenarios were encountered.  The Executed event isn t needed anymore and could be removed.  Description  While it is in the user s best interest not to reuse a transactionId they have used before, unique transaction IDs are not enforced, and a user seeking to wreak havoc might choose to reuse an ID if it helps them accomplish their goal. In this case, event-monitoring software might get confused by several Executed events with the same transactionId and not be able to match the event with its TransactionFulfilled counterpart.  Recommendation  Generally, the following rules apply to transaction IDs:  A user must, in their own best interest, never reuse a transactionId they have used before \u2014 not even across different chains and no matter whether the transaction was successful or not.  This per-user uniqueness of transaction IDs is not enforced, though \u2014 not even per TransactionManager deployment. Hence, the code may not rely on this assumption, and no harm must come from a reused transaction ID for the system or anyone else than the user who reused the ID.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.19 Sdk.finishTransfer - missing validation ", "body": "  Description  Sdk.finishTransfer should validate that the router that locks liquidity in the receiving chain, should be the same router the user had committed to in the sending chain.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.20 FulfillInterpreter - Missing check whether callTo address contains code    ", "body": "  Resolution  This issue has been fixed.  Description  The receiver-side prepare checks whether the callTo address is either zero or a contract:  code/packages/contracts/contracts/TransactionManager.sol:L466-L470  // Check that the callTo is a contract  // NOTE: This cannot happen on the sending chain (different chain  // contexts), so a user could mistakenly create a transfer that must be  // cancelled if this is incorrect  require(invariantData.callTo == address(0) || Address.isContract(invariantData.callTo), \"#P:031\");  However, as a contract may selfdestruct and the check is not repeated later, there is no guarantee that callTo still contains code when the call to this address (assuming it is non-zero) is actually executed in FulfillInterpreter.execute:  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L71-L82  // Try to execute the callData  // the low level call will return `false` if its execution reverts  (bool success, bytes memory returnData) = callTo.call{value: isEther ? amount : 0}(callData);  if (!success) {  // If it fails, transfer to fallback  Asset.transferAsset(assetId, fallbackAddress, amount);  // Decrease allowance  if (!isEther) {  Asset.decreaseERC20Allowance(assetId, callTo, amount);  As a result, if the contract at callTo self-destructs between prepare and fulfill (both on the receiving chain), success will be true, and the funds will probably be lost to the user.  A user could currently try to avoid this by checking that the contract still exists before calling fulfill on the receiving chain, but even then, they might get front-run by selfdestruct, and the situation is even worse with a relayer, so this provides no reliable protection.  Recommendation  Repeat the Address.isContract check on callTo before making the external call in FulfillInterpreter.execute and send the funds to the fallbackAddress if the result is false.  Remark  It should be noted that an unsuccessful call, i.e., a revert, is the only behavior that is recognized by FulfillInterpreter.execute as failure. While it is prevalent to indicate failure by reverting, this doesn t have to be the case; a well-known example is an ERC20 token that indicates a failing transfer by returning false. A user who wants to utilize this feature has to make sure that the called contract behaves accordingly; if that is not the case, an intermediary contract may be employed, which, for example, reverts for return value false.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.21 TransactionManager - Adherence to EIP-712   ", "body": "  Resolution  Comment from Connext:  We did not fully adopt EIP712 because hardware wallet support is still not universal. Additionally, we chose not to address this issue in the recommended fashion (using address(this), block.chainId) because the fulfill signature must be usable across both the sending and receiving chain. Instead, we made sure the transactionManagerReceivingAddress, receivingChainId was signed.  We advise users of the system not to use their key and address for other systems that operate with signed messages unless they can rule out the possibility of replay attacks. Regarding the signed receivingChainId and receivingChainTxManagerAddress, we d like to mention that even for receiver-side fulfillment, these are not verified against the current chain ID and address of the contract.  Description  fulfill function requires the user signature on a transactionId. While currently, the user SDK code is using a cryptographically secured pseudo-random function to generate the transactionId, it should not be counted upon and measures should be placed on the smart-contract level to ensure replay-attack protection.  Examples  code/packages/contracts/contracts/TransactionManager.sol:L918-L933  function recoverSignature(  bytes32 transactionId,  uint256 relayerFee,  string memory functionIdentifier,  bytes calldata signature  ) internal pure returns (address) {  // Create the signed payload  SignedData memory payload = SignedData({  transactionId: transactionId,  relayerFee: relayerFee,  functionIdentifier: functionIdentifier  });  // Recover  return ECDSA.recover(ECDSA.toEthSignedMessageHash(keccak256(abi.encode(payload))), signature);  Recommendation  Consider adhering to EIP-712, or at least including address(this), block.chainId as part of the data signed by the user.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.22 TransactionManager - Hard-coded chain ID might lead to problems after a chain split   Pending", "body": "  Resolution  The recommendation below has been implemented, but the current codebase doesn t handle chain splits correctly. On the chain that gets a new chain ID, funds may be lost or frozen.  More specifically, after a chain split, we may find ourselves in the situation that the current chain ID is neither the sendingChainId nor the receivingChainId stored in the invariant transaction data. If that is the case, we re on the chain that got a new chain ID. fulfill should always revert in this situation, but cancellation should be possible to release locked funds. We don t know, however, whether we should send the funds back to the user (that is, we re on a fork of the sending chain) or whether they should be given back to the router (that is, we re on a fork of the receiving chain). Our recommendation to solve this is to store in the variant transaction data explicitly whether this is the sending chain or the receiving chain; with this information, we can disambiguate the situation and implement cancel correctly.  Description  The ID of the chain on which the contract is deployed is supplied as a constructor argument and stored as an immutable state variable:  code/packages/contracts/contracts/TransactionManager.sol:L104-L107  /**  @dev The chain id of the contract, is passed in to avoid any evm issues  /  uint256 public immutable chainId;  code/packages/contracts/contracts/TransactionManager.sol:L125-L128  constructor(uint256 _chainId) {  chainId = _chainId;  interpreter = new FulfillInterpreter(address(this));  Hence, chainId can never change, and even after a chain split, both contracts would continue to use the same chain ID. That can have undesirable consequences. For example, a transaction that was prepared before the split could be fulfilled on both chains.  Recommendation  It would be better to query the chain ID directly from the chain via block.chainId. However, the development team informed us that they had encountered problems with this approach as some chains apparently are not implementing this correctly. They resorted to the method described above, a constructor-supplied, hard-coded value. For chains that do indeed not inform correctly about their chain ID, this is a reasonable solution. However, for the reasons outlined above, we still recommend querying the chain ID via block.chainId for chains that do support that \u2014 which should be the vast majority \u2014 and using the fallback mechanism only when necessary.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.23 Router - handling of native assetID (0x000..00, e.g. ETH) not implemented ", "body": "  Description  Additionally, handleSenderPrepare does not manage approvals for ERC20 transfers.  Examples  harcoded zero amount  code/packages/router/src/contract.ts:L137-L147  return ResultAsync.fromPromise(  this.txService.sendTx(  to: this.config.chainConfig[chainId].transactionManagerAddress,  data: encodedData,  value: constants.Zero,  chainId,  from: this.signerAddress,  },  requestContext,  ),  code/packages/router/src/contract.ts:L206-L215  this.txService.sendTx(  chainId,  data: fulfillData,  to: nxtpContractAddress,  value: 0,  from: this.signerAddress,  },  requestContext,  ),  approveTokensIfNeeded will fail when using native assets  code/packages/sdk/src/transactionManager.ts:L329-L333  ).andThen((signerAddress) => {  const erc20 = new Contract(  assetId,  ERC20.abi,  this.signer.provider ? this.signer : this.signer.connect(config.provider),  Recommendation  Remove complexity by requiring ERC20 compliant wrapped native assets (e.g.WETH instead of native ETH).  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.24 Router - config file is missing the swapPools attribute and credentials are leaked to console in case of invalid config ", "body": "  Description  Node startup fails due to missing swapPools configuration in config.json.example. Confidential secrets are leaked to console in the event that the config file is invalid.  Examples  yarn workspace @connext/nxtp-router dev  [app] [nodemon] 2.0.12  [app] [nodemon] to restart at any time, enter `rs`  [app] [nodemon] watching path(s): .env dist/**/* ../@connext/nxtp-txservice/dist ../@connext/nxtp-contracts/dist ../@connext/nxtp-utils/dist  [app] [nodemon] watching extensions: js,json  [app] [nodemon] starting `node --enable-source-maps ./dist/index.js | pino-pretty`  [tsc]  [tsc] 13:52:29 - Starting compilation in watch mode...  [tsc]  [tsc]  [tsc] 13:52:29 - Found 0 errors. Watching for file changes.  [app] Found configFile  [app] Invalid config: {  [app]   \"mnemonic\": \"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\",  [app]   \"authUrl\": \"https://auth.connext.network\",  [app]   \"natsUrl\": \"nats://nats1.connext.provide.network:4222,nats://nats2.connext.provide.network:4222,nats://nats3.connext.provide.network:4222\",  [app]   \"adminToken\": \"blahblah\",  [app]   \"chainConfig\": {  [app]     \"4\": {  [app]       \"providers\": [  [app]         \"https://rinkeby.infura.io/v3/\"  [app]       ],  [app]       \"confirmations\": 1,  [app]       \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-rinkeby\",  [app]       \"transactionManagerAddress\": \"0x29E81453AAe28A63aE12c7ED7b3F8BC16629A4Fd\",  [app]       \"minGas\": \"100000000000000000\"  [app]     },  [app]     \"5\": {  [app]       \"providers\": [  [app]         \"https://goerli.infura.io/v3/\"  [app]       ],  [app]       \"confirmations\": 1,  [app]       \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-goerli\",  [app]       \"transactionManagerAddress\": \"0xbF0F4f639cDd010F38CeBEd546783BD71c9e5Ea0\",  [app]       \"minGas\": \"100000000000000000\"  [app]     }  [app]   },  [app]   \"logLevel\": \"info\"  [app] }  [app] Error: must have required property 'swapPools'  [app]     at Object.getEnvConfig (code/packages/router/dist/config.js:135:15)  [app]         -> code/packages/router/src/config.ts:145:11  [app]     at Object.getConfig (code/packages/router/dist/config.js:149:30)  [app]         -> code/packages/router/src/config.ts:161:18  [app]     at Object.<anonymous> (code/packages/router/dist/index.js:19:25)  [app]         -> code/packages/router/src/index.ts:23:16  [app]     at Module._compile (internal/modules/cjs/loader.js:1063:30)  [app]     at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)  [app]     at Module.load (internal/modules/cjs/loader.js:928:32)  [app]     at Function.Module._load (internal/modules/cjs/loader.js:763:16)  [app]     at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:72:12)  [app]     at internal/main/run_main_module.js:17:47  Confidential information is only cleared in case the config file is valid but not in the event of an error  code/packages/router/src/config.ts:L143-L149  if (!valid) {  console.error(`Invalid config: ${JSON.stringify(nxtpConfig, null, 2)}`);  throw new Error(validate.errors?.map((err) => err.message).join(\",\"));  console.log(JSON.stringify({ ...nxtpConfig, mnemonic: \"********\" }, null, 2));  return nxtpConfig;  Recommendation  Provide a valid default example config. Fix integration tests.  Always remove confidential information before logging on screen.  Avoid providing default credentials as it is very likely that someone might end up using them. Consider asking the user to provide missing credentials on first run or autogenerate it for them.  Note that the adminToken is not cleared before it is being printed to screen. If this is a credential it should be blanked out before being printed. Consider separating application-specific configuration from credentials/secrets.  5 Recommendations  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.1 Router - Logging Consistency", "body": "  Description  Avoid using console.*() in favor of the logger.*() family to provide a consistent timestamped log trail. Note that console.* might have different buffering behavior than logger.log which may mix up output lines.  Examples  code/packages/router/src/index.ts:L124-L131  server.listen(8080, \"0.0.0.0\", (err, address) => {  if (err) {  console.error(err);  process.exit(1);  console.log(`Server listening at ${address}`);  });  code/packages/router/src/contract.ts:L415-L416  const decoded = this.txManagerInterface.decodeFunctionResult(\"getRouterBalance\", encodedData);  console.log(\"decoded: \", decoded);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.2 Router - Always perform strict validation of data received from third-parties or untrusted sources", "body": "  Description  For example, in subgraph.ts an external resource is queried to return transactions that match the router s ID, sending Chain, and status. An honest external party will only return items that match this filter. However, in case of the third-party misbehaving (or being breached), it might happen that entries that do not belong to this node or chain configuration are returned.  Examples  code/packages/router/src/subgraph.ts:L153-L175  let allSenderPrepared: GetSenderTransactionsQuery;  try {  allSenderPrepared = await sdk.GetSenderTransactions({  routerId: this.routerAddress.toLowerCase(),  sendingChainId: chainId,  status: TransactionStatus.Prepared,  });  } catch (err) {  this.logger.error(  { method, methodId, error: jsonifyError(err) },  \"Error in sdk.GetSenderTransactions, aborting loop interval\",  );  return;  // create list of txIds for each receiving chain  const receivingChains: Record<string, string[]> = {};  allSenderPrepared.router?.transactions.forEach(({ transactionId, receivingChainId }) => {  if (receivingChains[receivingChainId]) {  receivingChains[receivingChainId].push(transactionId);  } else {  receivingChains[receivingChainId] = [transactionId];  Recommendation  It is recommended to implement a defense-in-depth approach always validating inputs that come from third-parties or untrusted sources. Especially because the resources spent on performing the checks are negligible and significantly reduce the risk posed by third-party data providers.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.3 FulfillInterpreter - ReentrancyGuard can be removed  Pending", "body": "  Resolution   The   Description and Recommendation  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L22-L28  /**  @notice Errors if the sender is not the transaction manager  /  modifier onlyTransactionManager {  require(msg.sender == _transactionManager, \"#OTM:027\");  _;  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L54-L61  function execute(  bytes32 transactionId,  address payable callTo,  address assetId,  address payable fallbackAddress,  uint256 amount,  bytes calldata callData  ) override external payable nonReentrant onlyTransactionManager {  Consequently, if the TransactionManager contract can t be reentered, the FulfillInterpreter is automatically protected against reentrancy. Hence, if issue 4.9 is fixed, the reentrancy guard can be removed from FulfillInterpreter.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.4 FulfillInterpreter - _transactionManager state variable can be immutable   ", "body": "  Resolution  This recommendation has been implemented.  Description and Recommendation  The _transactionManager state variable in the FulfillInterpreter is set in the constructor and never changed afterward. Hence, it can be immutable.  code/packages/contracts/contracts/interpreters/FulfillInterpreter.sol:L16-L20  address private _transactionManager;  constructor(address transactionManager) {  _transactionManager = transactionManager;  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "5.5 TransactionManager - Risk mitigation for addLiquidity   ", "body": "  Resolution  This recommendation has been implemented.  Description and Recommendation  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/07/connext-nxtp-noncustodial-xchain-transfer-protocol/"}, {"title": "4.1 _WETH private constant address in UnoswapRouter makes for tricky deployments to other chains ", "body": "  Resolution  The 1inch team acknowledged but is unable to implement changes due to stack-too-deep errors that would require a large refactor of this already well-used and tested library.  To interact with WETH, the UnoswapRouter uses a hardcoded _WETH private constant in the contract. Therefore, this currently needs changing every time the contract is deployed to a different chain, as noted by the comment within the contract:  1inch-contract/contracts/routers/UnoswapRouter.sol:L24-L26  /// @dev WETH address is network-specific and needs to be changed before deployment.  /// It can not be moved to immutable as immutables are not supported in assembly  address private constant _WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  As the comment also points out, the choice to not make it an immutable variable is not possible since they are not supported in assembly, and the UnoswapRouter contract is highly efficient and almost entirely written in assembly. However, the other contracts within the scope of this audit, do utilize setting a private immutable variable for WETH in the constructor, and some of them then initialize a new address variable derived from this private immutable variable, thereby allowing the address variable to be used in the assembly blocks instead:  UnoswapV3Router  1inch-contract/contracts/routers/UnoswapV3Router.sol:L33-L37  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  constructor(IWETH weth) {  _WETH = weth;  ClipperRouter  1inch-contract/contracts/routers/ClipperRouter.sol:L18-L24  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  IClipperExchangeInterface private immutable _clipperExchange;  constructor(IWETH weth, IClipperExchangeInterface clipperExchange) {  _clipperExchange = clipperExchange;  _WETH = weth;  1inch-contract/contracts/routers/ClipperRouter.sol:L101  address weth = address(_WETH);  1inch-contract/contracts/routers/ClipperRouter.sol:L112  if iszero(call(gas(), weth, 0, ptr, 0x64, 0, 0)) {  OrderMixin  limit-order-protocol/contracts/OrderMixin.sol:L63-L70  IWETH private immutable _WETH;  // solhint-disable-line var-name-mixedcase  /// @notice Stores unfilled amounts for each order plus one.  /// Therefore 0 means order doesn't exist and 1 means order was filled  mapping(bytes32 => uint256) private _remaining;  constructor(IWETH weth) {  _WETH = weth;  Normalizing this process across all smart contracts in the 1inch system could help avoid accidental mistakes when the deployer could forget to first edit the unoswap contract to have the correct address.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "4.2 Selfdestruct may be removed as an opcode in future ", "body": "  Resolution  The 1inch team acknowledged and noted.  The AggregationRouterV5 contract implements a function called destroy that calls a selfdestuct on the contract with the msg.sender as the argument, that is checked by the onlyOwner modifier on the function.  1inch-contract/contracts/AggregationRouterV5.sol:L35-L37  function destroy() external onlyOwner {  selfdestruct(payable(msg.sender));  However, there are discussions currently around removing the selfdestruct functionality from the EVM altogether with various motivations and rationale provided, such as this being not possible with Verkle trees and it being a requirement for stateleness. Link to the EIP is below: https://eips.ethereum.org/EIPS/eip-4758  It appears that the suggested remediation of this functionality per the EIP-4758 will not significantly change the results, for example all of the funds will still be sent to the specified address, but the destruction of the actual contract will not occur. So this is just an advisory note for the 1inch team to notify of this potential change in the future.  5 Limit Order Protocol  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.1 Malicious maker can take more takers funds than taker expected ", "body": "  Resolution   Remediated as per the 1inch team in   1inch/limit-order-protocol@9ddc086 by adding a check that reverts when  OrderMixin contract allows users to match makers(sellers) and takers(buyers) in an orderbook-like manner. One additional feature this contract has is that both makers and takers are allowed to integrate hooks into their orders to better react to market conditions and manage funds on the fly. Two out of many of these hooks are called: _getMakingAmount and _getTakingAmount. Those particular hooks allow the maker to dynamically respond to the making or taking amounts supplied by the taker. Essentially they allow overriding the rate that was initially set by the maker when creating an order up to a certain extent. To make sure that the newly suggested maker rate is reasonable taker also provides a threshold value or in other words the minimum amount of assets the taker is going to be fine receiving.  Generally speaking, the maker can override the taking amount offered to the taker if the buyer passed a specific making amount in the fill transaction and vice versa. But there is one special case where the maker will be able to override both, which when done right will force the taker to spend an amount larger than the one intended. Specifically, this happens when the taker passed the desired taking amount and the maker returns a suggested making amount that is larger than the remaining order size. In this case, the making amount is being set to the remaining order amount and the taking is being recomputed.  limit-order-protocol/contracts/OrderMixin.sol:L214-L217  actualMakingAmount = _getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);  if (actualMakingAmount > remainingMakingAmount) {  actualMakingAmount = remainingMakingAmount;  actualTakingAmount = _getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);  Essentially this allows the maker to override the taker amount and as long as the maker keeps the price intact or changed within a certain threshold like described in this issue, they can take all taking tokens of the buyer up to an amount of the token balance or approval limit whatever comes first.  Consider the following example scenario:  The maker has a large order to sell 100 ETH on the order book for 100 DAI each.  The taker then wants to partially fill the order and buy as much ETH as 100 DAI will buy. At the same time taker has 100,000 DAI in the wallet.  When taker tries to fill this order taker passes the takingAmount to be 100. Since OrderMixin received the taking amount we go this route:  limit-order-protocol/contracts/OrderMixin.sol:L214  actualMakingAmount = _getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);  However, note that when executing the _getMakingAmount() function, it first evaluates the order.getMakingAmount() argument which is evaluated as bytes calldata _getter within the function.  limit-order-protocol/contracts/OrderMixin.sol:L324-L337  function _getMakingAmount(  bytes calldata getter,  uint256 orderTakingAmount,  uint256 requestedTakingAmount,  uint256 orderMakingAmount,  uint256 remainingMakingAmount,  bytes32 orderHash  ) private view returns(uint256) {  if (getter.length == 0) {  // Linear proportion  return getMakingAmount(orderMakingAmount, orderTakingAmount, requestedTakingAmount);  return _callGetter(getter, orderTakingAmount, requestedTakingAmount, orderMakingAmount, remainingMakingAmount, orderHash);  That is because the Order struct that is made and signed by the maker actually contains the necessary bytes within it that can be decoded to construct a target and calldata for static calls, which in this case are supposed to be used to return the making asset amounts that the maker determines to be appropriate, as seen in the comments under the uint256 offsets part of the struct.  limit-order-protocol/contracts/OrderLib.sol:L7-L27  library OrderLib {  struct Order {  uint256 salt;  address makerAsset;  address takerAsset;  address maker;  address receiver;  address allowedSender;  // equals to Zero address on public orders  uint256 makingAmount;  uint256 takingAmount;  uint256 offsets;  // bytes makerAssetData;  // bytes takerAssetData;  // bytes getMakingAmount; // this.staticcall(abi.encodePacked(bytes, swapTakerAmount)) => (swapMakerAmount)  // bytes getTakingAmount; // this.staticcall(abi.encodePacked(bytes, swapMakerAmount)) => (swapTakerAmount)  // bytes predicate;       // this.staticcall(bytes) => (bool)  // bytes permit;          // On first fill: permit.1.call(abi.encodePacked(permit.selector, permit.2))  // bytes preInteraction;  // bytes postInteraction;  bytes interactions; // concat(makerAssetData, takerAssetData, getMakingAmount, getTakingAmount, predicate, permit, preIntercation, postInteraction)  Finally, if these bytes indeed contain data (i.e. length>0), they are passed to the _callGetter() function that asks the previously mentioned target for the data.  limit-order-protocol/contracts/OrderMixin.sol:L354-L377  function _callGetter(  bytes calldata getter,  uint256 orderExpectedAmount,  uint256 requestedAmount,  uint256 orderResultAmount,  uint256 remainingMakingAmount,  bytes32 orderHash  ) private view returns(uint256) {  if (getter.length == 1) {  if (OrderLib.getterIsFrozen(getter)) {  // On \"x\" getter calldata only exact amount is allowed  if (requestedAmount != orderExpectedAmount) revert WrongAmount();  return orderResultAmount;  } else {  revert WrongGetter();  } else {  (address target, bytes calldata data) = getter.decodeTargetAndCalldata();  (bool success, bytes memory result) = target.staticcall(abi.encodePacked(data, requestedAmount, remainingMakingAmount, orderHash));  if (!success || result.length != 32) revert GetAmountCallFailed();  return abi.decode(result, (uint256));  However, since the getter is set in the Order struct, and the Order is set by the maker, the getter itself is entirely under the maker s control and can return whatever the maker wants, with no regard for the taker s passed actualTakingAmount or any arguments at all for that matter. So, in our example, the return value could be 100.1 ETH, i.e. just above the total order size. That will get us on the route of recomputing the taking amount since 100.1 is over the 100ETH remaining in the order.  limit-order-protocol/contracts/OrderMixin.sol:L217  actualTakingAmount = _getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);  This branch will set the actualMakingAmount to 100ETH and then the malicious maker will say the actualTakingAmount is 10000 DAI, this can be done via the _getTakingAmount static call in the same exact way as the making amount was manipulated.  Then the threshold check would look like this as defined by its formula:  limit-order-protocol/contracts/OrderMixin.sol:L222  if (actualMakingAmount * takingAmount < thresholdAmount * actualTakingAmount) revert MakingAmountTooLow();  ActualMakingAmount - 100ETH  ActualTakingAmount - 10000DAI  threshold - 1 ETH  takingAmount - 100 DAI  then: 100ETH * 100DAI < 1ETH*10000DAI This condition will be false so we will pass this check.  Then we proceed to taker interaction. Assuming the taker did not pass any interaction, the actualTakingAmount will not change.  Then we proceed to exchange tokens between maker and taker in the amount of actualTakingAmount and actualMakingAmount.  The scenario allows the maker to take the taker s funds up to an amount of taker s approval or balance. Essentially while taker wanted to only spend 100 DAI, potentially they ended up spending much more. This paired with infinite approvals that are currently enabled on the 1inch UI could lead to funds being lost.  While this does not introduce a price discrepancy this attack can be profitable to the malicious actor. The attacker could put an order to sell a large amount of new not trustworthy tokens for sale who s supply the attacker controls. Then after a short marketing campaign when people will cautiously try to buy a small amount of those tokens for let s say a small amount of USDC due to this bug attacker could drain all of their USDC.  We advise that 1inch team treats this issue with extra care since a similar issue is present in a currently deployed production version of 1inch OrderMixin. One potential solution to this bug is introducing a global threshold that would represent by how much the actual taking amount can differ from the taker provided taking amount.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.2 Invalidating users orders ", "body": "  1inch team has implemented a more streamlined version of the order book that is called OrderRFQMixin. This version has no hooks and is meant to be more straightforward than the main order book contract.  One significant difference between those contracts is that the RFQ version invalidates the orders even after they have been only partially filled.  limit-order-protocol/contracts/OrderRFQMixin.sol:L197-L203  {  // Stack too deep  uint256 info = order.info;  // Check time expiration  uint256 expiration = uint128(info) >> 64;  if (expiration != 0 && block.timestamp > expiration) revert OrderExpired(); // solhint-disable-line not-rely-on-time  _invalidateOrder(maker, info, 0);  Since makers have to sign the orders, only makers can place the remainder of the original order as a new one. Given that information, an attacker could take all the orders and fill them with 1 wei of taking assets. While this will cost an attacker gas, on some chains it would be possible to make the operations of the protocol unreliable and impractical for makers.  One way to fix that without making significant changes to the logic is to introduce a threshold that will determine the smallest taking amount for each order. That could be a percent of the taking amount specified in the order. This change will make the attack more expensive and less likely to happen.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.3 Reentrancy potential issue for contracts building on top RFQOrderMixin ", "body": "  Resolution   Remediated as per the 1inch team in   1inch/limit-order-protocol@d3957fe by forwarding a limited amount of gas to guard against complex execution at the target but still allow for smart contract receipt that may require a bit more gas than usual EOA receipts.  The RFQOrderMixin contract is used to facilitate transfer of assets in RFQ orders between makers and takers. Naturally, one of such possible assets could be the native coin of the chain, such as ETH. In order to perform these transfers, the contract currently utilizes the target.call(){value:X} method to transfer X ETH to the target address. However, this also calls into the target address and opens up arbitrary code execution that could lead to significant problems, that often times result in a reentrancy attack.  limit-order-protocol/contracts/OrderRFQMixin.sol:L233  (bool success, ) = target.call{value: makingAmount}(\"\");  // solhint-disable-line avoid-low-level-calls  While 1inch s RFQOrderMixin contract doesn t have a clear reentrancy attack vector, other smart contract systems that might utilize 1inch RFQ orders will have to handle a potential reentrancy due to this problem. The impact for downstream systems could be critical.  This could be changed to .transfer() or .send() methods of transferring ETH, or at least heavily noted in documentation for any and all developers who may fork/utilize this code so reentrancy risks are made aware of.  This does not seem to be a general-purpose use library for other systems, so likelihood of this issue happening isn t as high as in issue 6.4, so the severity is lower.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.4 Order cancellation event spam in orderMixin ", "body": "  The 1inch Limit Order protocol s contracts utilize mechanisms to allow creation of orders without posting the orders on chain. Indeed, the orders are created by signing Order struct hashes off-chain by the maker, and then having takers pass the signatures associated with those hashes to fill in those orders. However, a maker needs to be able to cancel their order if they change their mind, which would require them to execute an on-chain transaction marking that order hash as invalid:  limit-order-protocol/contracts/OrderMixin.sol:L113-L121  function cancelOrder(OrderLib.Order calldata order) external returns(uint256 orderRemaining, bytes32 orderHash) {  if (order.maker != msg.sender) revert AccessDenied();  orderHash = hashOrder(order);  orderRemaining = _remaining[orderHash];  if (orderRemaining == _ORDER_FILLED) revert AlreadyFilled();  emit OrderCanceled(msg.sender, orderHash, orderRemaining);  _remaining[orderHash] = _ORDER_FILLED;  Unfortunately, since the OrderMixin contract is not aware of order hashes before interacting with them for the first time, it can not verify that the order was actually ever seriously present or intended to be executed. As a result, this would allow users to cancel non-existent orders and create event spam. While this would be costly to the spammer, it would nonetheless be possible.  The impact of this would need systems that rely on the OrderCanceled event log to be aware of potential spam attacks with fake order cancellation and not use them, for example, for analytics, potential volume forecasting, tracking order created -> order cancelled metrics and so on.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "5.5 Order book slippage ", "body": "  OrderMixin contract allows users to match makers and takers in an orderbook-like manner. One additional feature this contract has is that both makers and takers are allowed to integrate hooks into their orders to better react to market conditions and manage funds on the fly. Two of these hooks are called: _getMakingAmount and _getTakingAmount.  When trying to fill an order taker is required to provide either the making amount or taking amount as well as the threshold or in other words the minimum amount of assets the taker is going to be fine receiving. During the fill transaction, an order maker is given the opportunity to update the offer by the means of the _getMakingAmount and _getTakingAmount. A threshold checks are then used in order to make sure that the updated values are within taker s acceptable bounds:  limit-order-protocol/contracts/OrderMixin.sol:L222  if (actualMakingAmount * takingAmount < thresholdAmount * actualTakingAmount) revert MakingAmountTooLow();  limit-order-protocol/contracts/OrderMixin.sol:L212  if (actualTakingAmount * makingAmount > thresholdAmount * actualMakingAmount) revert TakingAmountTooHigh();  It is reasonable to assume that if the maker knows the threshold the taker selected, the maker will attempt to update the making or taking amount to maximize profits. While _getMakingAmount and _getTakingAmount do not pass the threshold selected by the taker directly, it is still possible for the maker to obtain this information and act accordingly.  A malicious maker could listen to the mempool and wait for a transaction that is meant to fill his order obtaining the threshold value.  Maker would then update the state of the contract that responds to the static call of the _getMakingAmount and _getTakingAmount hooks.  If the maker is using FlashBots or a similar service, the maker can ensure that the above actions are performed before the transaction that would fill the order.  While there is no good way to alleviate this issue given the current design we believe it is important to be aware of this issue and allow the 1inch users to know that some analogy of slippage is still possible even on the orderbook-like system. This will allow them to choose tighter and more secure threshold values.  6 Solidity Utils  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.1 ECDSA library has a vulnerability for signature malleability of EIP-2098 compact signatures ", "body": "  Resolution   Remediated as per the 1inch team in   1inch/solidity-utils@166353b by adding a warning note in the comments of the library code.  The 1inch ECDSA library supports several types of signatures and forms in which they could be provided. However, for compact signatures there is a recently found malleability attack vector. Specifically, the issue arises when contracts use transaction replay protection through signature uniqueness (i.e. by marking it as used). While this may not be the case in the scope of other contracts of this audit, this ECDSA library is meant to be a general use library so it should be fixed so as to not mislead others who might use this.  For more details and context, find below the advisory notice and fix in the OpenZeppelin s ECDSA library: https://github.com/OpenZeppelin/openzeppelin-contracts/security/advisories/GHSA-4h98-2769-gh6h OpenZeppelin/openzeppelin-contracts@d693d89  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.2 Ethereum reimbursements sent to an incorrect address ", "body": "  Resolution   Remediated as per the 1inch team as of   1inch/solidity-utils@6b1a3df by adding the correct recipient of the refund.  1inch team has written a library called UniERC20 that extends the traditional ERC20 standard to also support eth transfers seamlessly. In the case of the uniTransferFrom function call, the library checks that the msg.value of the transaction is bigger or equal to the amount passed in the function argument. If the msg.value is larger than the amount required, the difference, or extra funds, should be sent to the sender. In the actual implementation Instead of returning the funds to the sender, extra funds are actually sent to the destination.  solidity-utils/contracts/libraries/UniERC20.sol:L59-L65  if (msg.value > amount) {  // Return remainder if exist  unchecked {  (bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls  if (!success) revert ETHSendFailed();  Given that this code is packed as a library and allows for easy reusability by the 1inch team and outside developers it is crucial that this logic is written well and well tested.  We recommend reconsidering reimbursing the sender when an incorrect amount is being sent because it introduces an easy-to-oversee reentrancy backdoor with call() that is mentioned in issue 6.4. Reverting was a default behavior in similar cases across the rest of the 1inch contracts.  If this functionality is required, a fix we could recommend is replacing the to with from. We can also suggest running a fuzzing campaign against this library.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.3 ECDSA incorrect size provided for calldata in the static call ", "body": "  Resolution   Remediated as per the 1inch team in   1inch/solidity-utils@cfdc889 by passing the correct data size.  The ECDSA library implements support for IERC1271 interfaces that verify provided signature for the data through the different isValidSignature functions that depend on the type of signature used.  However, the library passes an incorrect size for the calldata in the static call for signatures that are of the form (bytes32 r, bytes32 vs). It should be 0xa4 (164 bytes) instead of 0xa5 (165 bytes).  solidity-utils/contracts/libraries/ECDSA.sol:L178  if staticcall(gas(), signer, ptr, 0xa5, 0, 0x20) {  The impact could vary and depends on the signature verifier. For example, it could be significant if the signature verifier performs a check on the calldatasize for this specific type of signature and reverts on incorrect sizes, thereby having valid signatures return false when passed to isValidSignature.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.4 Re-entrancy risk in UniERC20 ", "body": "  Resolution   Remediated as per the 1inch team in   1inch/solidity-utils@6b1a3df by forwarding a limited amount of gas to guard against complex execution at the target but still allow for smart contract receipt that may require a bit more gas than usual EOA receipts.  UniERC20 is a general library for facilitating transfers of any ERC20 or native coin assets. It features gas-efficient code and could be easily integrated into large systems of contract, such as those that are used in this audit   1inch routers and limit order protocol.  However, it also utilizes .call(){value:X} method of transferring chain native assets, such as ETH. This introduces a large risk in the form of re-entrancy attacks, so any system implementing this library would have to handle them. While 1inch s projects in the scope of this audit do not seem to have re-entrancy attack vectors, other projects that could be utilizing this library might. Since this is an especially efficient and convenient library, the likelihood that some other project using this suffers and then sufferring a re-entrancy attack is significant.  solidity-utils/contracts/libraries/UniERC20.sol:L45  (bool success, ) = to.call{value: amount}(\"\");  // solhint-disable-line avoid-low-level-calls  solidity-utils/contracts/libraries/UniERC20.sol:L62  (bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls  Consider instead implementing transfer() or send() methods for transferring chain native assets, such as ETH, instead of performing a .call()  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/08/1inch-exchange-aggregationrouter-v5/"}, {"title": "6.1 in3-server - amplified DDoS on incubed requests on proof with signature    ", "body": "  Resolution   Mitigated by adding   merge_requests/101. The full extent of this fix is outside the scope of this audit.  Description  It is possible for a client to send a request to each node of the network to request a signature with proof for every other node in the network. This can result in DDoSing the network as there are no costs for the client to request this and client can send the same request to all the nodes in the network, resulting in n^2 requests.  Examples  Client asks each node for in3_nodeList to get all the signer addresses, this could also be done using NodeRegistry contract  Client asks each node for a proof with signature, e.g.:  \"jsonrpc\": \"2.0\",  \"id\": 2,  \"method\": \"eth_getTransactionByHash\",  \"params\": [\"0xf84cfb78971ebd940d7e4375b077244e93db2c3f88443bb93c561812cfed055c\"],  \"in3\": {  \"chainId\": \"0x1\",  \"verification\": \"proofWithSignature\",  \"signatures\":[\"0x784bfa9eb182C3a02DbeB5285e3dBa92d717E07a\", ALL OTHER SIGNERS HERE]  All the nodes are now sending requests to each other with signature required which is an expensive computation. This can go on for more transactions (or blocks, or other Eth_ requests) and can result in DDoS of the network.  Recommendation  Limit the number of signers in proof with signature requests. Also exclude self.signer from the list. This combined with the remediation of issue 6.6 can partially mitigate the attack vector.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.2 BlockProof - Node conviction race condition may trick all but one node into losing funds    ", "body": "  Resolution  Mitigated by:  80bb6ecf by checking if blockhash exists and prevent an overwrite, saving gas  Client will blacklist the server if the signature is missing, has a wrong signer or is invalid.  6cc0dbc0 Removing nodes from local available nodes list when the server detects wrong responses  Other commits to mitigate the mentioned vulnerable scenarios  With the new handling, the client will not call convict immediately (as this could be exploited again). Instead, the client will do the calculation whether it s worth convicting the server before even calling convict.  It should be noted that the changes are scattered and modified in the final source code, and this behaviour of IN3-server code is outside the scope of this audit.  Description  TLDR; One node can force all other nodes to convict a specific malicious signer controlled by the attacker and spend gas on something they are not going to be rewarded for. The attacker loses deposit but all other nodes that try to convict and recreate in the same block will lose the fees less or equal to deposit/2. Another variant forces the same node to recreate the blockheaders multiple times within the same block as the node does not check if it is already convicting/recreating blockheaders.  Nodes can request various types of proofs from other nodes. For example, if a node requests a proof when calling one of the eth_getBlock* methods, the in3-server s method handleBlock will be called. The request should contain a list of addresses registered to the NodeRegistry that are requested to sign the blockhash.  code/in3-server/src/modules/eth/EthHandler.ts:L105-L112  // handle special jspn-rpc  if (request.in3.verification.startsWith('proof'))  switch (request.method) {  case 'eth_getBlockByNumber':  case 'eth_getBlockByHash':  case 'eth_getBlockTransactionCountByHash':  case 'eth_getBlockTransactionCountByNumber':  return handleBlock(this, request)  in3-server will subsequently reach out to it s connected blockchain node execute the eth_getBlock* call to get the block data. If the block data is available the in3-server, it will try to collect signatures from the nodes that signature was requested from (request.in3.signatures, collectSignatures())  code/in3-server/src/modules/eth/proof.ts:L237-L243  // create the proof  response.in3 = {  proof: {  type: 'blockProof',  signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)  If the node does not find the address it will throw an exception. Note that if this exception is not caught it will actually allow someone to boot nodes off the network - which is critical.  code/in3-server/src/chains/signatures.ts:L58-L60  const config = nodes.nodes.find(_ => _.address.toLowerCase() === adr.toLowerCase())  if (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')  If the address is valid and existent in the NodeRegistry the in3-node will ask the node to sign the blockhash of the requested blocknumber:  code/in3-server/src/chains/signatures.ts:L69-L84  // send the sign-request  let response: RPCResponse  try {  response = (blocksToRequest.length  ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3_sign', params: blocksToRequest })  : { result: [] }) as RPCResponse  if (response.error) {  //throw new Error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(_ => _.blockNumber).join() + ':' + response.error)  logger.error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(_ => _.blockNumber).join() + ':' + response.error)  return null  } catch (error) {  logger.error(error.toString())  return null  For all the signed blockhashes that have been returned the in3-server will subsequently check if one of the nodes provided a wrong blockhash.  We note that nodes might:  decided to not follow the in_3sign request and just not provide a signed response  a node might sign with a different key  a node might sign a different blockheader  a node might sign a previous blocknumber  In all these cases, the node will not be convicted, even though it was able to request other nodes to perform work.  If another node signed a wrong blockhash the in3-server will automatically try to convict it. If the block is within the most recent 255 it will directly call convict() on the NodeRegistry (takes less gas). if it is an older block, it will try to recreate the blockchain in the RlockhashRegistry (takes more gas).  code/in3-server/src/chains/signatures.ts:L128-L152  const convictSignature: Buffer = keccak(Buffer.concat([bytes32(s.blockHash), address(singingNode.address), toBuffer(s.v, 1), bytes32(s.r), bytes32(s.s)]))  if (diffBlocks < 255) {  await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {  privateKey: handler.config.privateKey,  gas: 500000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  handler.watcher.futureConvicts.push({  convictBlockNumber: latestBlockNumber,  signer: singingNode.address,  wrongBlockHash: s.blockHash,  wrongBlockNumber: s.block,  v: s.v,  r: s.r,  s: s.s,  recreationDone: true  })  else {  await handleRecreation(handler, nodes, singingNode, s, diffBlocks)  The recreation and convict is only done if it is profitable for the node. (Note the issue mentioned in issue 6.13)  code/in3-server/src/chains/signatures.ts:L209-L213  const costPerBlock = 86412400000000  const blocksMissing = latestSS - s.block  const costs = blocksMissing * costPerBlock * 1.25  if (costs > (deposit / 2)) {  A malicious node can exploit the hardcoded profit economics and the fact that in3-server implementation will try to auto-convict nodes in the following scenario:  malicious node requests a blockproof with an eth_getBlock* call from the victim node (in3-server) for a block that is not in the most recent 256 blocks (to maximize effort for the node). This equals to spending more gas in order to convict the node (costs <= (deposit / 2)).  the malicious node prepares the BlockhashRegistry to contain a blockhash that would maximize the gas needed to convict the malicious node (can be calculated offline; must fulfill costs <= (deposit /2).  with the blockproof request the malicious node asks the in3-server to get the signature from a specific signer. The signer will also be malicious and is going to sign a wrong blockhash with a valid signature.  the malicious signer is going to lose it s deposit but the deposit also incentivizes other nodes to spend gas on the conviction process. The higher the deposit, the more an in3-server is willing to spend on the conviction.  In this scenario one malicious node tries to trick another node into convicting a malicious signer while having to spend the maximum amount of gas to make it profitable for the node.  The problem is, that the malicious node can ask multiple (or even all other nodes in the registry) to provide a blockproof and ask the malicious signer for a signed blockhash. All nodes will come to the conclusion that the signer returned an invalid hash and will try to convict the node. They will try to recreate the blockchain in the BlockhashRegistry for a barely profitable scenario. Since in3-nodes do not monitor the tx-pool they will not know that other nodes are already trying to convict the node. All nodes are going to spend gas on recreating the same blockchain in the BlockhashRegistry leading to all but the first transaction in the block to lose funds (up to deposit/2 based on the hardcoded costPerBlock)  Another variant of the same issue is that nodes do not check if they already convicted another node (or recreated blockheaders). An attacker can therefore force a specific node to convict a malicious node multiple times before the nodes transactions are actually in a block as the nodes does not check if it is already convicting that node. The node might lose gas on the recreation/conviction process multiple times.  Recommendation  To reduce the impact of multiple nodes trying to update the blockhashRegistry at the same time and avoid nodes losing gas by recreating the same blocks over and over again, the BlockhashRegistry should require that the target blockhash for the blocknumber does not yet exist in the registry (similar to the issue mentioned in https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/24).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.3 NodeRegistry Front-running attack on convict()    ", "body": "  Resolution  Blocknumber is removed from convict function, which removes any signal for an attacker in the scenario provided. However, the order of the transactions to convict a wrong signed hash is necessary to prevent any front-running attacks:  Convict(_Blockhash)  recreate Blockheaders  RevealConvict (minimum 2 blocks after convict but as soon as recreateBlockheaders is confirmed)  The fixes were introduced in ecf2c6a6 and f4250c9a, although later on NodeRegistry contract was split in two other contracts NodeRegistryLogic and NodeRegistryData and further changes were done in the conviction flow in different commits.  Description  convict(uint _blockNumber, bytes32 _hash) and revealConvict() are designed to prevent front-running and they do so for the purpose they are designed for. However, if the malicious node, is still sending out the wrong blockhash for the convicted block, anyone seeing the initial convict transaction, can check the convicted blocknumber with the nodes and send his own revealConvict before the original sender.  The original sender will be the one updating the block headers recreateBlockheaders(_blockNumber, _blockheaders), and the attacker can just watch for the update headers to perform this attack.  Recommendation  For the first attack vector, remove the blocknumber from the convict(uint _blockNumber, bytes32 _hash) inputs and just use the hash.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.4 NodeRegistry - URL can be arbitrary dns resolvable names, IP s and even localhost or private subnets    ", "body": "  Resolution  This issue has been addressed with the following commits:  4c93a10f adding 48 hours delay in the server code before they communicate with the newly registered nodes.  merge_requests/111 adding a whole new smart contract to the IN3 system, IN3WhiteList.sol, and supporting code in the server.  issues/94 To prevent attacker to use nodes as a DoS network, a DNS record verification is discussed to be implemented.  It is a design decision to base the Node registry on URLs (DNS resolvable names). This has the implications outlined in this issue and they cannot easily be mitigated. Adding a delay until nodes can be used after registration only delays the problem. Assuming that an entity curates the registry or a whitelist is in place centralizes the system. Adding DNS record verification still allows an owner of a DNS entry to point its name to any IP address they would like it to point to. It certainly makes it harder to add RPC URLs with DNS names that are not in control of the attacker but it also adds a whole lot more complexity to the system (including manual steps performed by the node operator). In the end, the system allows IP based URLs in the registry which cannot be used for DNS validation.  Note that the server code changes, and the new smart contract IN3WhiteList.sol are outside the scope of the original audit. We strongly recommend to reduce complexity and audit the final codebase before mainnet deployment.  Description  As outlined in issue 6.9 the NodeRegistry allows anyone to register nodes with arbitrary URLs. The url is then used by in3-server or clients to connect to other nodes in the system. Signers can only be convicted if they sign wrong blockhashes. However, if they never provide any signatures they can stay in the registry for as long as they want and sabotage the network. The Registry implements an admin functionality that is available for the first year to remove misbehaving nodes (or spam entries) from the Registry. However, this is insufficient as an attacker might just re-register nodes after the minimum timeout they specify or spend some more finneys on registering more nodes. Depending on the eth-price this will be more or less profitable.  From an attackers perspective the NodeRegistry is a good source of information for reconnaissance, allows to de-anonymize and profile nodes based on dns entries or netblocks or responses to in3_stats (https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/49), makes a good list of target for DoS attacks on the system or makes it easy to exploit nodes for certain yet unknown security vulnerabilities.  Since nodes and potentially clients (not in scope) do not validate the rpc URL received from the NodeRegistry they will try to connect to whatever is stored in a nodes url entry.  code/in3-server/src/chains/signatures.ts:L58-L75  const config = nodes.nodes.find(_ => _.address.toLowerCase() === adr.toLowerCase())  if (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')  // get cache signatures and remaining blocks that have no signatures  const cachedSignatures: Signature[] = []  const blocksToRequest = blocks.filter(b => {  const s = signatureCaches.get(b.hash) && false  return s ? cachedSignatures.push(s) * 0 : true  })  // send the sign-request  let response: RPCResponse  try {  response = (blocksToRequest.length  ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3_sign', params: blocksToRequest })  : { result: [] }) as RPCResponse  if (response.error) {  This allows for a wide range of attacks not limited to:  An attacker might register a node with an empty or invalid URL. The in3-server does not validate the URL and therefore will attempt to connect to the invalid URL, spending resources (cpu, file-descriptors, ..) to find out that it is invalid.  An attacker might register a node with a URL that is pointing to another node s rpc endpoint and specify weights that suggest that it is capable of service a lot of requests to draw more traffic towards that node in an attempt to cause a DoS situation.  An attacker might register a node for a http/https website at any port in an extortion attempt directed to website owners. The incubed network nodes will have to learn themselves that the URL is invalid and they will at least attempt to connect the website once.  An attacker might update the node information in the NodeRegistry for a specific node every block, providing a new url (or a slightly different URLs issue 6.9) to avoid client/node URL blacklists.  An attacker might provide IP addresses instead of DNS resolvable names with the url in an attempt to draw traffic to targets, avoiding canonicalization and blacklisting features.  An attacker might provide a URL that points to private IP netblocks for IPv4 or IPv6 in various formats. Combined with the ability to ask another node to connect to an attacker defined url (via blockproof, signatures[] -> signer_address -> signer.url) this might allow an attacker to enumerate services in the LAN of node operators.  An attacker might provide the loopback IPv4, IPv6 or resolvable name as the URL in an attempt to make the node connect to local loopback services (service discovery, bypassing authentication for some local running services - however this is very limited to the requests nodes may execute).  URLs may be provided in various formats: resolvable dns names, IPv4, IPv6 and depending on the http handler implementation even in Decimal, Hex or Octal form (i.e. http://2130706433/)  A valid DNS resolvable name might point to a localhost or private IP netblock.  Since none of the rpc endpoints provide signatures they cannot be convicted or removed (unless the unregisterKey does it within the first year. However, that will not solve the problem that someone can re-register the same URLs over and over again)  Recommendation  It is a fundamental design decision of the system architecture to allow rpc urls in the Node Registry, therefore this issue can only be partially mitigated unless the system design is reworked. It is therefore suggested to add checks to both the registry contract (coarse validation to avoid adding invalid urls) and node implementations (rigorous validation of URL s and resolved IP addresses) and filter out any potentially harmful destinations.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.5 Malicious clients can use forks or reorgs to convict honest nodes   ", "body": "  Resolution  Default value for past signed blocks is changed to 10 blocks. Slockit plans to use their off-chain channels to notify clients for planned forks. They also looking into using fork oracles in the future releases to detect planned hardforks to mitigate risks.  Description  In case of reorgs it is possible to have more than 6 blocks in a node that gets replaced by a new longer chain. Also for forks, such as upcoming Istanbul fork, it s common to have some nodes taking some time to update and they will be in the wrong chain for the time being. In both cases, in3-nodes are prone to sign blocks that are considered invalid in the main chain. Malicious nodes can catch these instances and convict the honest users in the main chain to get 50% of their deposits.  Recommendation  No perfect solution comes to mind at this time. One possible mitigation method for forks could be to disable the network on the time of the fork but this is most certainly going to be a threat to the system itself.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.6 in3-server - should protect itself from abusive clients    ", "body": "  Resolution  Slockit implemented their own DOS protection for incubed server in merge_requests/99. The variant of this implementation adds more complexity to the code base. The benchmark and testing of the new DOS protection is not in scope for this audit.  The incubed server has now an additional DOS-Protection build in. Here we first estimate a Weight of such a request and add them together for all incoming requests per IP of the client per Minute. Since we estimate the execution, we can prevent a client running DOS-Attacks from the same IP with heavy requests (such as eth_getLogs)  Description  The in3-node implementation should provide features for client request throttling to avoid that a client can consume most of the nodes resources by causing a lot of resource intensive requests.  This is a general problem to the system which is designed to make sure that low resource clients can verify blockchain properties. What this means is that almost all of the client requests are very lightweight. Clients can request nodes to sign data for them. A sign request involves cryptographic operations and a http-rpc request to a back-end blockchain node. The imbalance is clearly visible in the case of blockProofs where a client may request another node to interact with a smart contract (NodeRegistry) and ask other nodes to sign blockhashes. All other nodes will have to get the requested block data from their local blockchain nodes and the incubed node requesting the signatures will have to wait for all responses. The client instead only has to send out that request once and may just leave that tcp connection open. It might even consume more resources from a specific node by requesting the same signatures again and again not even waiting for a response but causing a lot of work on the node that has to collect all the signatures. This combined with unbound requests for signatures or other properties can easily be exploited by a powerful client implementation with a mission to stall the whole incubed network.  Recommendation  According to the threat model outlines a general DDoS scenario specific to rpcUrls. It discusses that the nodes are themselves responsible for DDoS protection. However, DDoS protection is a multi-layer approach and it is highly unlikely that every node-operator will hide their nodes behind a DDoS CDN like cloudflare. We therefore suggest to also build in strict limitations for clients that can be checked in code. Similar to checkPerformanceLimits which is just checking for some specific it is suggested to implement a multi-layer throttling mechanism that prevents nodes from being abused by single clients. Methods must be designed with (D)DoS scenarios in mind to avoid that third parties are abusing the network for DDoS campaigns or trying to DoS the incubed network.  code/in3-server/src/modules/eth/EthHandler.ts:L74-L91  private checkPerformanceLimits(request: RPCRequest) {  if (request.method === 'eth_call') {  if (!request.params || request.params.length < 2) throw new Error('eth_call must have a transaction and a block as parameters')  const tx = request.params as TxRequest  if (!tx || (tx.gas && toNumber(tx.gas) > 10000000)) throw new Error('eth_call with a gaslimit > 10M are not allowed')  else if (request.method === 'eth_getLogs') {  if (!request.params || request.params.length < 1) throw new Error('eth_getLogs must have a filter as parameter')  const filter: LogFilter = request.params[0]  let toB = filter && filter.toBlock  if (toB === 'latest' || toB === 'pending' || !toB) toB = this.watcher && this.watcher.block && this.watcher.block.number  let fromB = toB && filter && filter.fromBlock  if (fromB === 'earliest') fromB = 1;  const range = fromB && (toNumber(toB) - toNumber(fromB))  if (range > (request.in3.verification.startsWith('proof') ? 1000 : 10000))  throw new Error('eth_getLogs for a range of ' + range + ' blocks is not allowed. limits: with proof: 1000, without 10000 ')  implement request throttling per client  implement caching mechanism for similar requests if it is expected that the same response is to be delivered multiple times  implement general performance limits and reject further requests if the node is close to exhausting its resources (soft DoS)  make sure the node does not exhaust the systems resources  implement throttling per request method  design methods to prevent (D)DoS in the first place. Methods that allow a client to send one request that causes a node to perform multiple client controlled requests must be avoided or at least bound and throttled (issue 6.7, https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50).  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.7 in3-server - DoS on in3.sign and other requests    ", "body": "  Resolution  Similar to issue 6.1, Mitigated by adding maxBlocksSigned and maxSignatures for requests of any client.  The Numbers of signatures a client can ask to fetch is now limited to maxSignatures which defaults to 5  in merge_requests/101. The full extent of this fix is outside the scope of this audit.  We have limited the number of block you can ask to sign in the in3_sign-request. The default is 10, because this function is also used for eth_getLogs to provide proof for all events. This limit will also limit the result of logs returned to include only max 10 different blocks.  Description  It is free for the client to ask the nodes to sign block hashes (and also other requests). in3.sign([{\"blockNumber\": 123}]) Takes an array of objects that will result in multiple requests in the node. This sample request has (at least) two internal requests, one eth_getBlockByNumber and signing the block hash.  These requests can be continuously sent out to clients and result in using computation power of the nodes without any expense from the client.  Examples  Request to get and sign the first 200 blocks:  web3.manager.request_blocking(\"in3_sign\", [{'blockNumber':i} for i in range(200)])  Recommendation  Limit the number of blocks (input), or do not accept arrays for input.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.8 in3-server - key management   Pending", "body": "  Resolution  The breakdown of the fixes addressed with git.slock.it/PR/13 are as follows:  Keys should never be stored or accepted in plaintext format Keys should only be accepted in an encrypted and protected format  The private key in code/in3-server/config.json has been removed. The repository still contains private keys at least in the following locations:  package.json  vscode/launch.json  example_docker-compose.yml  Note that private keys indexed by a git repository can be restored from the repository history.  The following statement has been provided to address this issue:  We have removed all examples and usage of plain private keys and replaced them with json-keystore files. Also in the documentation we added warnings on how to deal with keys, especially with hints to the bash history or enviroment  A single key should be used for only one purpose. Keys should not be shared.  The following statement has been provided to address this issue:  This is why we seperated the owner and signer-key. This way you can use a multisig to securly protect the owner-key. The signer-key is used to sign blocks (and convict) and is not able to do anything else (not even changing its own url)  The application should support developers in understanding where cryptographic keys are stored within the application as well as in which memory regions they might be accessible for other applications  Addressed by wrapping the private key in an object that stores the key in encrypted form and only decrypts it when signing. The key is cleared after usage. The IN3-server still allows raw private keys to be configured. A warning is printed if that is the case. The loaded raw private key is temporarily assigned to a local variable and not explicitly cleared by the method.  While we used to keep the unlocked key as part of the config, we have now removed the key from the config and store them in a special signer-function. https://git.slock.it/in3/ts/in3-server/merge_requests/113  Keys should be protected in memory and only decrypted for the duration of time they are actively used. Keys should not be stored with the applications source-code repository  see previous remediation note.  After unlocking the signer key, we encrypt it again and keep it encrypted only decrypting it when signing. This way the raw private key only exist for a very short time in memory and will be filled with 0 right after. ( https://git.slock.it/in3/ts/in3-server/merge_requests/113/diffs#653b04fa41e35b55181776b9f14620b661cff64c_54_73 )  Use standard libraries for cryptographic operations  The following statement has been provided to address this issue  We are using ethereumjs-libs.  Use the system keystore and API to sign and avoid to store key material at all  The following statement has been provided to address this issue  We are looking into using different signer-apis, even supporting hardware-modules like HSMs. But this may happen in future releases.  The application should store the keys eth-address (util.getAddress()) instead of re-calculating it multiple times from the private key.  Fixed by generating the address for a private key once and storing it in a private key wrapper object.  Do not leak credentials and key material in debug-mode, to local log-output or external log aggregators.  txArgs still contains a field privateKey as outlined in the issue description. However, this privateKey now represents the wrapper object noted in a previous comment which only provides access to the ETH address generated from the raw private key.  The following statement has been provided to address this issue:  since the private key and the passphrase are actually deleted from the config, logoutputs or even debug will not be able to leak this information.  Description  Secure and efficient key management is a challenge for any cryptographic system. Incubed nodes for example require an account on the ethereum blockchain to actively participate in the incubed network. The account and therefore a private-key is used to sign transactions on the ethereum blockchain and to provide signed proofs to other in3-nodes.  This means that an attacker that is able to discover the keys used by an in3-server by any mechanism may be able to impersonate that node, steal the nodes funds or sign wrong data on behalf of the node which might also lead to a loss of funds.  The private key for the in3-server can be specified in a configuration file called config.json residing in the program working dir. Settings from the config.json can be overridden via command-line options. The application keeps configuration parameters available internally in an IN3RPCConfig object and passes this object as an initialization parameter to other objects.  The key can either be provided in plaintext as a hex-string starting with 0x or within an ethereum keystore format compatible protected keystore file. Either way it is provided it will be held in plaintext in the object.  The application accepts plaintext private keys and the keys are stored unprotected in the applications memory in JavaScript objects. The in3-server might even re-use the nodes private key which may weaken the security provided by the node. The repository leaks a series of presumably  test private keys  and the default config file already comes with a private key set that might be shared across unvary users that fail to override it.  code/in3-server/config.json:L1-L4  \"privateKey\": \"0xc858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3\",  \"rpcUrl\": \"http://rpc-kovan.slock.it\"  code/in3-server/package.json:L20-L31  \"docker-run\": \"docker run -p 8500:8500 docker.slock.it/slockit/in3-server:latest --privateKey=0x3858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3 --chain=0x2a --rpcUrl=https://kovan.infura.io/HVtVmCIHVgqHGUgihfhX --minBlockHeight=6 --registry=0x013b82355a066A31427df3140C5326cdE9c64e3A --persistentFile=false --logging-host=logs7.papertrailapp.com --logging-name=Papertrail --logging-port=30571 --logging-type=winston-papertrail\",  \"docker-setup\": \"docker run -p 8500:8500 slockit/in3-server:latest --privateKey=0x3858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3 --chain=0x2a --rpcUrl=https://kovan.infura.io/HVtVmCIHVgqHGUgihfhX --minBlockHeight=6 --registry=0x013b82355a066A31427df3140C5326cdE9c64e3A --persistentFile=false --autoRegistry-url=https://in3.slock.it/kovan1 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=1\",  \"local\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xD231FCF9349A296F555A060A619235F88650BBA795E5907CFD7F5442876250E4 --chain=0x2a --rpcUrl=https://rpc.slock.it/kovan --minBlockHeight=6 --registry=0x27a37a1210df14f7e058393d026e2fb53b7cf8c1 --persistentFile=false\",  \"ipfs\": \"docker run -d -p 5001:5001 jbenet/go-ipfs  daemon --offline\",  \"linkIn3\": \"cd node_modules; rm -rf in3; ln -s ../../in3 in3; cd ..\",  \"lint:solium\": \"node node_modules/ethlint/bin/solium.js -d contracts/\",  \"lint:solium:fix\": \"node node_modules/ethlint/bin/solium.js -d contracts/ --fix\",  \"lint:solhint\": \"node node_modules/solhint/solhint.js \\\"contracts/**/*.sol\\\" -w 0\",  \"local-env\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0x9e53e6933d69a28a737943e227ad013c7489e366f33281d350c77f089d8411a6 --chain=0x111 --rpcUrl=http://localhost:8545 --minBlockHeight=6 --registry=0x31636f91297C14A8f1E7Ac271f17947D6A5cE098 --persistentFile=false --autoRegistry-url=http://127.0.0.1:8500 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=0\",  \"local-env2\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x111 --rpcUrl=http://localhost:8545 --minBlockHeight=6 --registry=0x31636f91297C14A8f1E7Ac271f17947D6A5cE098 --persistentFile=false --autoRegistry-url=http://127.0.0.1:8501 --autoRegistry-capabilities-proof=true --autoRegistry-capabilities-multiChain=true --autoRegistry-deposit=0\",  \"local-env3\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x5 --rpcUrl=https://rpc.slock.it/goerli --minBlockHeight=6 --registry=0x85613723dB1Bc29f332A37EeF10b61F8a4225c7e --persistentFile=false\",  \"local-env4\": \"export NODE_ENV=0 && npm run build && node ./js/src/server/server.js --privateKey=0xf7db260e6edcdfe396d75f8283aad5aed835815f7d1db4458896310553a8a1a9 --chain=0x2a --rpcUrl=https://rpc.slock.it/kovan --minBlockHeight=6 --registry=0x27a37a1210df14f7e058393d026e2fb53b7cf8c1 --persistentFile=false\"  The private key is also passed as arguments to other functions. In error cases these may leak the private key to log interfaces or remote log aggregation instances (sentry). See txargs.privateKey in the example below:  code/in3-server/src/util/tx.ts:L100-L100  const key = toBuffer(txargs.privateKey)  code/in3-server/src/util/tx.ts:L134-L140  const txHash = await transport.handle(url, {  jsonrpc: '2.0',  id: idCount++,  method: 'eth_sendRawTransaction',  params: [toHex(tx.serialize())]  }).then((_: RPCResponse) => _.error ? Promise.reject(new SentryError('Error sending tx', 'tx_error', 'Error sending the tx ' + JSON.stringify(txargs) + ':' + JSON.stringify(_.error))) as any : _.result + '')  Recommendation  Keys should never be stored or accepted in plaintext format.  Keys should not be stored in plaintext on the file-system as they might easily be exposed to other users. Credentials on the file-system must be tightly restricted by access control. Keys should not be provided as plaintext via environment variables as this might make them available to other processes sharing the same environment (child-processes, e.g. same shell session) Keys should not be provided as plaintext via command-line arguments as they might persist in the shell s command history or might be available to privileged system accounts that can query other processes startup parameters.  Keys should only be accepted in an encrypted and protected format.  A single key should be used for only one purpose. Keys should not be shared.  The use of the same key for two different cryptographic processes may weaken the security provided by one or both of the processes. The use of the same key for two different applications may weaken the security provided by one or both of the applications. Limiting the use of a key limits the damage that could be done if the key is compromised. Node owners keys should not be re-used as signer keys.  The application should support developers in understanding where cryptographic keys are stored within the application as well as in which memory regions they might be accessible for other applications.  Keys should be protected in memory and only decrypted for the duration of time they are actively used.  Keys should not be stored with the applications source-code repository.  Use standard libraries for cryptographic operations.  Use the system keystore and API to sign and avoid to store key material at all.  The application should store the keys eth-address (util.getAddress()) instead of re-calculating it multiple times from the private key.  Do not leak credentials and key material in debug-mode, to local log-output or external log aggregators.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.9 NodeRegistry - Multiple nodes can share slightly different RPC URL    ", "body": "  Resolution   Same mitigation as   issue 6.4.  Description  One of the requirements for Node registration is to have a unique URL which is not already used by a different owner. The uniqueness check is done by hashing the provided _url and checking if someone already registered with that hash of _url.  However, byte-equality checks (via hashing in this case) to enforce uniqueness will not work for URLs. For example, while the following URLs are not equal and will result in different urlHashes they can logically be the same end-point:  https://some-server.com/in3-rpc  https://some-server.com:443/in3-rpc  https://some-server.com/in3-rpc/  https://some-server.com/in3-rpc///  https://some-server.com/in3-rpc?something  https://some-server.com/in3-rpc?something&something  https://www.some-server.com/in3-rpc?something (if www resolves to the same ip)  code/in3-contracts/contracts/NodeRegistry.sol:L547-L553  bytes32 urlHash = keccak256(bytes(_url));  // make sure this url and also this owner was not registered before.  // solium-disable-next-line  require(!urlIndex[urlHash].used && signerIndex[_signer].stage == Stages.NotInUse,  \"a node with the same url or signer is already registered\");  This leads to the following attack vectors:  A user signs up multiple nodes that resolve to the same end-point (URL). A minimum deposit of 0.01 ether is required for each registration. Registering multiple nodes for the same end-point might allow an attacker to increase their chance of being picked to provide proofs. Registering multiple nodes requires unique signer addresses per node.  Also one node can have multiple accounts, hence one node can have slightly different URL and different accounts as the signers.  DoS - A user might register nodes for URLs that do not serve in3-clients in an attempt to DDoS e.g. in an attempt to extort web-site operators. This is kind of a reflection attack where nodes will request other nodes from the contract and try to contact them over RPC. Since it is http-rpc it will consume resources on the receiving end.  DoS - A user might register Nodes with RPC URLs of other nodes, manipulating weights to cause more traffic than the node can actually handle. Nodes will try to communicate with that node. If no proof is requested the node will not even know that someone else signed up other nodes with their RPC URL to cause problems. If they request proof the original signer will return a signed proof and the node will fail due to a signature mismatch. However, the node cannot be convicted and therefore forced to lose the deposit as conviction is bound the signer and the block was not signed by the rogue node entry. There will be no way to remove the node from the registry other than the admin functionality.  Recommendation  Canonicalize URLs, but that will not completely prevent someone from registering nodes for other end-points or websites. Nodes can be removed by an admin in the first year but not after that. Rogue owners cannot be prevented from registering random nodes with high weights and minimum deposit. They cannot be convicted as they do not serve proofs. Rogue owners can still unregister to receive their deposit after messing with the system.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.10 in3-server - should enforce safe settings for minBlockHeight   ", "body": "  Resolution  The default block is changed to 10 and minBlockHeight is added to the registry (as part of the properties) in 8c72633e, but allow the user to define a minBlockHeight lower than this number. The client is responsible to review the settings depending on how secure they want their nodes to be.  Client response:  We have discussed this, but decided to keep it flexible. This means:  We have put the minBlockHeight into the registry (as part of the properties). Because these properties indicate the limit and capabilities of the node and give the client a chance to filter out nodes if they don t match the requirements. So each client is able to filter out node who are not willing to take the risk and sign for example latest-6. Of course these nodes will most likely only store a low deposit ( you can not have a signature of a young block and a high deposit), but if you need a high security the nodes with a deposit will propably wait at least 10 or more blocks. In order to protect the owner of a node of using insecure settings, we will use our wizard to check the deposit and minBlockHeights and warn or educate the user. The reason why this flexibility is important, is because there use cases where dapps will not accept the let user wait 10 blocks before confirming a transaction. If the dapp developer needs a signature of a younger block, he will need to live with the fact, that he won t be able to find a high deposit to secure it.  We also changed the default to 10 blocks, but allow the user to define a minBlockHeight lower than this number. In this case the node would write a warning in the logfile, but still accepts the user configuration. This allows to use incubed also on different chains other than the mainnet.  The safeMinBlockHeight is now dependend on different chains, which is one single function, so we don t have hardcoded values in different places anymore.  Description  A node that is signing wrong blockhashes might get their deposit slashed from the registry. The entity that is convicting a node that signs a wrong blockhash is awarded half of the deposit.  A threat to this kind of system is that blocks might constantly be reorganized in the chain, especially with the latest block. Allowing a node to sign the latest block will definitely put the node s deposit at stake with every signature they provide.  A node can configure the minBlockHeight it is about to sign with a configurative option. The option defaults to a minBlockHeight of 6 in the default config:  code/in3-server/src/server/config.ts:L32-L32  minBlockHeight: 6,  And again in the signing function for blockheaders:  code/in3-server/src/chains/signatures.ts:L189-L189  const blockHeight = handler.config.minBlockHeight === undefined ? 6 : handler.config.minBlockHeight  handleSign will refuse to sign any block that is within the last 5 blocks. The 6th block will be signed.  code/in3-server/src/chains/signatures.ts:L190-L193  const tooYoungBlock = blockData.find(block => toNumber(blockNumber) - toNumber(block.number) < blockHeight)  if (tooYoungBlock)  throw new Error(' cannot sign for block ' + tooYoungBlock.number + ', because the blockHeight must be at least ' + blockHeight)  However, a user is not prevented from configuring an insecure minBlockHeight (e.g. 0) which will very likely lead to the loss of funds because the node will be signing the latest block.  Kraken requires at least 30 confirmation (abt. 6 minutes) until a transaction is confirmed. For Bitcoin it is said to be safe to wait more than 6 blocks (abt. 1 hr) for a transaction to be confirmed. ETC even underwent a  deep chain reorg that could have caused many nodes to lose their deposits. The  ethereum whitepaper defines an uncle that can be referenced in a block to have the following property:  Bitfinex requires a minimum of 10 confirmations. Some blockchain explorers and analytics tools also require a minimum of 10 confirmations. Scraped data from  https://etherscan.io/blocks_forked?ps=100 shows 3 forks of depth 3 since they started keeping records 115 days ago, and no forks deeper than 3. So some applications might legitimately pick a number somewhere between 5 and 20, trading some security for better UX. However, it should be re-evaluated whether the current default provides enough security to protect the nodes funds with a trade-off of lag to the network.  Given these values it is suggested to revalidate the default of a minBlockHeight of 6 in favor of a more secure depth to make sure that - with a default setting - nodes will not lose funds in case of re-orgs.  Recommendation  config.minBlockHeight should always be set to a sane value when loading the configuration. There should be no need to reset it to a hardcoded default value of 6 in handleSign. Do not hardcode the values in various places in the config.  normalize and sanitize the settings to make sure that after loading they are always valid and within reasonable bounds. the application should refuse to run with a minBlockHeader set to 0 as this is a guarantee for losing funds. Other nodes can enumerate nodes that are misconfigured (e.g. with minBlockHeight being 0) to request signatures just to convict them on micro-forks.  assume a secure default setting for every chain (note that this might be different for every chain). allow to override the value by the user. warn the user of less secure settings and do not allow to set settings that are obviously leading to the loss of funds.  re-evaluate the minBlockHeight of 6 for the ethereum blockchain and choose a koservative secure default.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.11 in3-server - rpc proof handler specification inconsistency    ", "body": "  Resolution   Addressed with   https://git.slock.it/in3/ts/in3-server/issues/100. Checks for  Description  According to the specification incubed requests must specify whether they want to have a proof or not. There are three variants of proofs that can be requested:  never - no proof appended  proof - proof but no signed blockhashes  proofWithSignature- proof and a request to sign blockhashes from the list of addresses provided in signatures.  Note that the name signatures for the array of signers a blockhash signature is requested from is misleading. It is actually signer addresses as listed in the NodeRegistry and not signatures.  Following the in3-server we found at least one inconsistency (and suspect more) with the proof requested by a client. The graceful check for the existence of something starting with proof will pass proof and proofWithSignature but also any other proofXYZ to the blockproof handler.  code/in3-server/src/modules/eth/EthHandler.ts:L106-L112  if (request.in3.verification.startsWith('proof'))  switch (request.method) {  case 'eth_getBlockByNumber':  case 'eth_getBlockByHash':  case 'eth_getBlockTransactionCountByHash':  case 'eth_getBlockTransactionCountByNumber':  return handleBlock(this, request)  Following through handleBlock we cannot find any check for proofWithSignature. The string is not found in the whole codebase which also suggests it is not tested. However, the code assumes that because request.in3.signatures is not empty, signatures were requested. This is inconsistent with the specification and a protocol violation.  code/in3-server/src/modules/eth/proof.ts:L237-L244  // create the proof  response.in3 = {  proof: {  type: 'blockProof',  signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)  The same is valid for all other types of proofs. proofWithSignature is never checked and it is assumed that proofWithSignature was requested just because request.in3.signatures is present non-empty.  The same is true for  never  which is actually never handled in code.  Recommendation  The protocol should be strictly enforced without allowing any ambiguities and unsharpness. Ambiguities and gracefulness in the protocol can lead to severe inconsistencies and encourage client authors to not strictly adhere to the protocol. This makes it hard to update and maintain the protocol in the future and may allow potential attackers enough freedom to exploit the protocol. Furthermore the specification must be kept up-to-date at all times. The specification is to lead development and code must always be verified against the specification.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.12 in3-server - hardcoded gas limit could result in failed transactions/requests    ", "body": "  Resolution   Fixed by using web3   merge_requests/109 to dynamically price the gas according to the network state.  Description  There are many instances of hardcoded gas limit in in3-server that depending on the complexity of the transaction or gas cost changes in Ethereum could result in failed transactions.  Examples  convict():  code/in3-server/src/chains/signatures.ts:L132-L137  await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {  privateKey: handler.config.privateKey,  gas: 500000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  recreateBlockheaders():  code/in3-server/src/chains/signatures.ts:L275-L280  await callContract(handler.config.rpcUrl, blockHashRegistry, 'recreateBlockheaders(uint,bytes[])', [latestSS - diffBlock, txArray], {  privateKey: handler.config.privateKey,  gas: 8000000,  value: 0,  confirm: true                       //  we are not waiting for confirmation, since we want to deliver the answer to the client.  })  Other instances of hard coded gasLimit or gasPrice:  code/in3-server/src/modules/eth/EthHandler.ts:L78-L79  if (!tx || (tx.gas && toNumber(tx.gas) > 10000000)) throw new Error('eth_call with a gaslimit > 10M are not allowed')  Recommendation  Use web3 gas estimate instead. To be sure, there can be an additional gas added to the estimated value or max(HARDCODED_GAS, estimated_amount)  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.13 in3-server - handleRecreation tries to recreate blockchain if no block is available to recreate it from    ", "body": "  Resolution  Mitigated by 502b5528 by falling back to using the current block in case searchForAvailableBlock returns 0. Costs can be zero, but cannot be negative anymore.  The behaviour of the IN3-server code is outside the scope of this audit. However, while verifying the fixes for this specific issue it was observed that the watch.ts:handleConvict() relies on a static hardcoded cost calculation. We further note that the cost calculation formula has an error and is missing parentheses to avoid that costs can be zero. We did not see a reason for the costs not to be allowed to be zero. Furthermore, costs are calculated based on the difference of the conviction block to the latest block. Actual recreation costs can be less if there is an available block in blockhashRegistry to recreate it from that is other than the latest block.  Description  A node that wants to convict another node for false proof must update the BlockhashRegistry for signatures provided in blocks older than the most recent 256 blocks. Only when the smart contract is able to verify that the signed blockhash is wrong the convicting node will be able to receive half of its deposit.  The in3-server implements an automated mechanism to recreate blockhashes. It first searches for an existing blockhash within a range of blocks. If one is found and it is profitable (gas spend vs. amount awarded) the node will try to recreate the blockchain updating the registry.  The call to searchForAvailableBlock might return 0 (default) because no block is actually found within the range, this will cause costs to be negative and the code will proceed trying to convict the node even though it cannot work.  The call to searchForAvailableBlock might also return the convict block number (latestSS==s.block) in which case costs will be 0 and the code will still proceed trying to recreate the blockheaders and convict the node.  code/in3-server/src/chains/signatures.ts:L207-L231  const [, deposit, , , , , , ,] = await callContract(handler.config.rpcUrl, nodes.contract, 'nodes(uint):(string,uint,uint64,uint64,uint128,uint64,address,bytes32)', [toNumber(singingNode.index)])  const latestSS = toNumber((await callContract(handler.config.rpcUrl, blockHashRegistry, 'searchForAvailableBlock(uint,uint):(uint)', [s.block, diffBlocks]))[0])  const costPerBlock = 86412400000000  const blocksMissing = latestSS - s.block  const costs = blocksMissing * costPerBlock * 1.25  if (costs > (deposit / 2)) {  console.log(\"not worth it\")  //it's not worth it  return  else {  // it's worth convicting the server  const blockrequest = []  for (let i = 0; i < blocksMissing; i++) {  blockrequest.push({  jsonrpc: '2.0',  id: i + 1,  method: 'eth_getBlockByNumber', params: [  toHex(latestSS - i), false  })  Please note that certain parts of the code rely on hardcoded gas values. Gas economics might change with future versions of the evm and have to be re-validated with every version. It is also good practice to provide inline comments about how and on what base certain values were selected.  Recommendation  Verify that the call succeeds and returns valid values. Check if the block already exists in the BlockhashRegistry and avoid recreation. Also note that searchForAvailableBlock can wrap with values close to uint_max even though that is unlikely to happen. In general, return values for external calls should be validated more rigorously.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.14 Impossible to remove malicious nodes after the initial period    ", "body": "  Resolution  This issue has been addressed with a large change-set that splits the NodeRegistry into two contracts, which results in a code flow that mitigates this issue by making the logic contract upgradable (after 47 days of notice). The resolution adds more complexity to the system, and this complexity is not covered by the original audit. Splitting up the contracts has the side-effect of events being emitted by two different contracts, requiring nodes to subscribe to both contracts  events.  The need for removing malicious nodes from the registry, arises from the design decision to allow anyone to register any URL. These URLs might not actually belong to the registrar of the URL and might not be IN3 nodes. This is partially mitigated by a centralization feature introduced in the mitigation phase that implements whitelist functionality for adding nodes.  We generally advocate against adding complexity, centralization and upgrading mechanisms that can allow one party to misuse functionalities of the contract system for their benefit (e.g. adminSetNodeDeposit is only used to reset the deposit but allows the Logic contract to set any deposit; the logic contract is set by the owner and there is a 47 day timelock).  We believe the solution to this issue, should have not been this complex. The trust model of the system is changed with this solution, now the logic contract can allow the admin a wide range of control over the system state and data.  The following statement has been provided with the change-set:  During the 1st year, we will keep the current mechanic even though it s a centralized approach. However, we changed the structure of the smart contracts and separated the NodeRegistry into two different smart contracts: NodeRegistryLogic and NodeRegistryData. After a successful deployment only the NodeRegistryLogic-contract is able to write data into the NodeRegistryData-contract. This way, we can keep the stored data (e.g. the nodeList) in the NodeRegistryData-contract while changing the way the data gets added/updated/removed is handled in the NodeRegistryLogic-contract. We also provided a function to update the NodeRegistryLogic-contract, so that we are able to change to a better solution for removing nodes in an updated contract.  Description  The system has centralized power structure for the first year after deployment. An unregisterKey (creator of the contract) is allowed to remove Nodes that are in state Stages.Active from the registry, only in 1st year.  However, there is no possibility to remove malicious nodes from the registry after that.  code/in3-contracts/contracts/NodeRegistry.sol:L249-L264  /// @dev only callable in the 1st year after deployment  function removeNodeFromRegistry(address _signer)  external  onlyActiveState(_signer)  // solium-disable-next-line security/no-block-members  require(block.timestamp < (blockTimeStampDeployment + YEAR_DEFINITION), \"only in 1st year\");// solhint-disable-line not-rely-on-time  require(msg.sender == unregisterKey, \"only unregisterKey is allowed to remove nodes\");  SignerInformation storage si = signerIndex[_signer];  In3Node memory n = nodes[si.index];  unregisterNodeInternal(si, n);  Recommendation  Provide a solution for the network to remove fraudulent node entries. This could be done by voting mechanism (with staking, etc).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.15 NodeRegistry.registerNodeFor() no replay protection and expiration   ", "body": "  Resolution  This issue was addressed with the following statement:  In our understanding of the relationship between node-owner and signer the owner both are controlled by the very same entity, thus the owner should always know the privateKey of the signer. With this in mind a replay-protection would be useless, as the owner could always sign the necessary message. The reason why we separated the signer from the owner was to enable the possibility of owning an in3-node as with a multisig-account, as due to the nature of the exposal of the signer-key the possibility of it being leaked somehow is given (e.g. someone  hacks  the server), making the signer-key more unsecure. In addition, even though it s possible to replay the register as an owner it would unfeasable, as the owner would have to pay for the deposit anyway thus rendering the attack useless as there would be no benefit for an owner to do it.  Description  An owner can register a node with the signer not being the owner by calling registerNodeFor. The owner submits a message signed for the owner including the properties of the node including the url.  The signed data does not include the registryID nor the NodeRegistry s address and can therefore be used by the owner to submit the same node to multiple registries or chains without the signers consent.  The signed data does not expire and can be re-used by the owner indefinitely to submit the same node again to future contracts or the same contract after the node has been removed.  Arguments are not validated in the external function (also see issue 6.17)  code/in3-contracts/contracts/NodeRegistry.sol:L215-L223  bytes32 tempHash = keccak256(  abi.encodePacked(  _url,  _props,  _timeout,  _weight,  msg.sender  );  Recommendation  Include registryID and an expiration timestamp that is checked in the contract with the signed data. Validate function arguments.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.16 BlockhashRegistry - Structure of provided blockheaders should be validated    ", "body": "  Resolution  Mitigated by:  99f35fce - validating the block number in the provided RLP encoded input data  79e5a302 - fixes the potential out of bounds access for parentHash by requiring the the input to contain at least data up until including the parentHash in the user provided RLP blob. However, this check does not enforce that the minimum amount of data is available to extract the blockNumber  Additionally we would like to note the following:  While the code decodes the RLPLongList structure that contains the blockheader fields it does not decode the RLPLongString parentHash and just assumes one length-byte for it.  The length of the RLPLongString parentHash is never used but skipped instead.  The decoding is incomplete and fragile. The method does not attempt to decode other fields in the struct to verify that they are indeed valid RLP data. For the blockNumber extraction a fixed offset of 444 is assumed to access the difficulty RLP field (this might through as the minimum input length up to this field is not enforced). difficulty is then skipped and the blockNumber is accessed.  The minimum input data length enforced is shorter than a typical blockheader.  The code relies on implicit exceptions for out of bounds array access instead of verifying early on that enough input bytes are available to extract the required data.  We would also like to note that the commit referenced as mitigation does not appear to be based on the audit code.  Description  getParentAndBlockhash takes an rlp-encoded blockheader blob, extracts the parent parent hash and returns both the parent hash and the calculated blockhash of the provided data. The method is used to add blockhashes to the registry that are older than 256 blocks as they are not available to the evm directly. This is done by establishing a trust-chain from a blockhash that is already in the registry up to an older block  The method assumes that valid rlp encoded data is provided but the structure is not verified (rlp decodes completely; block number is correct; timestamp is younger than prevs, \u2026), giving a wide range of freedom to an attacker with enough hashing power (or exploiting potential future issues with keccak) to forge blocks that would never be accepted by clients, but may be accepted by this smart contract. (threat: mining pool forging arbitrary non-conformant blocks to exploit the BlockhashRegistry)  It is not checked that input was actually provided. However, accessing an array at an invalid index will raise an exception in the EVM. Providing a single byte > 0xf7 will yield a result and succeed even though it would have never been accepted by a real node.  It is assumed that the first byte is the rlp encoded length byte and an offset into the provided _blockheader bytes-array is calculated. Memory is subsequently accessed via a low-level mload at this calculated offset. However, it is never validated that the offset actually lies within the provided range of bytes _blockheader leading to an out-of-bounds memory read access.  The rlp encoded data is only partially decoded. For the first rlp list the number of length bytes is extracted. For the rlp encoded long string a length byte of 1 is assumed. The inline comment appears to be inaccurate or might be misleading. // we also have to add \"2\" = 1 byte to it to skip the length-information  Invalid intermediary blocks (e.g. with parent hash 0x00) will be accepted potentially allowing an attacker to optimize the effort needed to forge invalid blocks skipping to the desired blocknumber overwriting a certain blockhash (see issue 6.18)  With one collisions (very unlikely) an attacker can add arbitrary or even random values to the BlockchainRegistry. The parent-hash of the starting blockheader cannot be verified by the contract ([target_block_random]<--parent_hash--[rnd]<--parent_hash--[rnd]<--parent_hash--...<--parent_hash--[collision]<--parent_hash_collission--[anchor_block]). While nodes can verify block structure and bail on invalid structure and check the first blocks hash and make sure the chain is in-tact the contract can t. Therefore one cannot assume the same trust in the blockchain registry when recreating blocks compared to running a full node.  code/in3-contracts/contracts/BlockhashRegistry.sol:L98-L126  function getParentAndBlockhash(bytes memory _blockheader) public pure returns (bytes32 parentHash, bytes32 bhash) {  /// we need the 1st byte of the blockheader to calculate the position of the parentHash  uint8 first = uint8(_blockheader[0]);  /// calculates the offset  /// by using the 1st byte (usually f9) and substracting f7 to get the start point of the parentHash information  /// we also have to add \"2\" = 1 byte to it to skip the length-information  require(first > 0xf7, \"invalid offset\");  uint8 offset = first - 0xf7 + 2;  /// we are using assembly because it's the most efficent way to access the parent blockhash within the rlp-encoded blockheader  // solium-disable-next-line security/no-inline-assembly  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  mstore(0x20, _blockheader)  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  mload(0x20), 0x20  ), offset)  bhash = keccak256(_blockheader);  Recommendation  Validate that the provided data is within a sane range of bytes that is expected (min/max blockheader sizes).  Validate that the provided data is actually an rlp encoded blockheader.  Validate that the offset for the parent Hash is within the provided data.  Validate that the parent Hash is non zero.  Validate that blockhashes do not repeat.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.17 Registries - Incomplete input validation and inconsistent order of validations   Pending", "body": "  Resolution  This issue describes general inconsistencies of the smart contract code base. The inconsistencies have been addressed with multiple change-sets:  Issues that have been addressed by the development team:  BlockhashRegistry.reCalculateBlockheaders - bhash can be zero; blockheaders can be empty  Fixed in 8d2bfa40 by adding the missing checks.  BlockhashRegistry.recreateBlockheaders - blockheaders can be empty; Arguments should be validated before calculating values that depend on them.  Fixed in 8d2bfa40 by adding the missing checks.  NodeRegistry.removeNode - should check require(_nodeIndex < nodes.length) first before any other action.  Fixed in 47255587 by adding the missing checks.  NodeRegistry.registerNodeFor - Signature version v should be checked to be either 27 || 28 before verifying it.  The fix in 47255587 introduced a serious typo (v != _v) that has been fixed with 4a0377c5 .  NodeRegistry.revealConvict - unchecked signer  Addressed with the comment that signer gets checked by ecrecover (slock.it/issue/10).  NodeRegistry.revealConvict - signer status can be checked earlier. Addressed with the following comment (slock.it/issue/10):  Due to the seperation of the contracts we will now check check the signatures and whether the blockhash is right. Only after this steps we will call into the NodeRegistryData contracts, thus potentially saving gas  NodeRegistry.updateNode - the check if the newURL is registered can be done earlier  Fixed in 4786a966.  BlockhashRegistry.getParentAndBlockhash- blockheader structure can be random as long as parenthash can be extracted  This issue has been reviewed as part of issue 6.16 (99f35fce).  Issues that have not been addressed by the development team and still persist:  BlockhashRegistry.searchForAvailableBlock - _startNumber + _numBlocks can be > block.number; _startNumber + _numBlocks can overflow. This issue has not been addressed.  General Notes:  Ideally commits directly reference issues that were raised during the audit. During the review of the mitigations provided with the change-sets for the listed issues we observed that change-sets contain changes that are not directly related to the issues. (e.g. 79e5a302)  Description  Methods and Functions usually live in one of two worlds:  public API - methods declared with visibility public or external exposed for interaction by other parties  internal API - methods declared with visibility internal, private that are not exposed for interaction by other parties  While it is good practice to visually distinguish internal from public API by following commonly accepted naming convention e.g. by prefixing internal functions with an underscore (_doSomething vs. doSomething) or adding the keyword unsafe to unsafe functions that are not performing checks and may have a dramatic effect to the system (_unsafePayout vs. RequestPayout), it is important to properly verify that inputs to methods are within expected ranges for the implementation.  Input validation checks should be explicit and well documented as part of the code s documentation. This is to make sure that smart-contracts are robust against erroneous inputs and reduce the potential attack surface for exploitation.  It is good practice to verify the methods input as early as possible and only perform further actions if the validation succeeds. Methods can be split into an external or public API that performs initial checks and subsequently calls an internal method that performs the action.  The following lists some public API methods that are not properly checking the provided data:  BlockhashRegistry.reCalculateBlockheaders - bhash can be zero; blockheaders can be empty  BlockhashRegistry.getParentAndBlockhash- blockheader structure can be random as long as parenthash can be extracted  BlockhashRegistry.recreateBlockheaders - blockheaders can be empty; Arguments should be validated before calculating values that depend on them:  code/in3-contracts/contracts/BlockhashRegistry.sol:L70-L70  assert(_blockNumber > _blockheaders.length);  BlockhashRegistry.searchForAvailableBlock - _startNumber + _numBlocks can be > block.number; _startNumber + _numBlocks can overflow.  NodeRegistry.removeNode - should check require(_nodeIndex < nodes.length) first before any other action.  code/in3-contracts/contracts/NodeRegistry.sol:L602-L609  function removeNode(uint _nodeIndex) internal {  // trigger event  emit LogNodeRemoved(nodes[_nodeIndex].url, nodes[_nodeIndex].signer);  // deleting the old entry  delete urlIndex[keccak256(bytes(nodes[_nodeIndex].url))];  uint length = nodes.length;  assert(length > 0);  NodeRegistry.registerNodeFor - Signature version v should be checked to be either 27 || 28 before verifying it.  code/in3-contracts/contracts/NodeRegistry.sol:L200-L212  function registerNodeFor(  string calldata _url,  uint64 _props,  uint64 _timeout,  address _signer,  uint64 _weight,  uint8 _v,  bytes32 _r,  bytes32 _s  external  payable  NodeRegistry.revealConvict - unchecked signer  code/in3-contracts/contracts/NodeRegistry.sol:L321-L321  SignerInformation storage si = signerIndex[_signer];  NodeRegistry.revealConvict - signer status can be checked earlier.  code/in3-contracts/contracts/NodeRegistry.sol:L344-L344  require(si.stage != Stages.Convicted, \"node already convicted\");  NodeRegistry.updateNode - the check if the newURL is registered can be done earlier  code/in3-contracts/contracts/NodeRegistry.sol:L444-L444  require(!urlIndex[newURl].used, \"url is already in use\");  Recommendation  Use Checks-Effects-Interactions pattern for all functions.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.18 BlockhashRegistry - recreateBlockheaders allows invalid parent hashes for intermediary blocks    ", "body": "  Resolution  Fixed by requiring valid parent hashes for blockheaders.  Description  It is assumed that a blockhash of 0x00 is invalid, but the method accepts intermediary parent hashes extracted from blockheaders that are zero when establishing the trust chain.  This may allow an attacker with enough hashing power to store a blockheader hash that is actually invalid on the real chain but accepted within this smart contract. This may even only be done temporarily to overwrite an existing hash for a short period of time (see https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/24).  code/in3-contracts/contracts/BlockhashRegistry.sol:L141-L147  for (uint i = 0; i < _blockheaders.length; i++) {  (calcParent, calcBlockhash) = getParentAndBlockhash(_blockheaders[i]);  if (calcBlockhash != currentBlockhash) {  return 0x0;  currentBlockhash = calcParent;  Recommendation  Stop processing the array of _blockheaders immediately if a blockheader is invalid.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.19 BlockhashRegistry - recreateBlockheaders succeeds and emits an event even though no blockheaders have been provided    ", "body": "  Resolution  Fixed the vulnerable scenarios by adding proper checks to:  Prevent passing empty _blockheaders in 8d2bfa40  Prevent storing the same blockhash twice in 80bb6ecf  Description  The method is used to re-create blockhashes from a list of rlp-encoded _blockheaders. However, the method never checks if _blockheaders actually contains items. The result is, that the method will unnecessarily store the same value that is already in the blockhashMapping at the same location and wrongly log LogBlockhashAdded even though nothing has been added nor changed.  assume _blockheaders is empty and the registry already knows the blockhash of _blockNumber  code/in3-contracts/contracts/BlockhashRegistry.sol:L61-L67  function recreateBlockheaders(uint _blockNumber, bytes[] memory _blockheaders) public {  bytes32 currentBlockhash = blockhashMapping[_blockNumber];  require(currentBlockhash != 0x0, \"parentBlock is not available\");  bytes32 calculatedHash = reCalculateBlockheaders(_blockheaders, currentBlockhash);  require(calculatedHash != 0x0, \"invalid headers\");  An attempt is made to re-calculate the hash of an empty _blockheaders array (also passing the currentBlockhash from the registry)  code/in3-contracts/contracts/BlockhashRegistry.sol:L66-L66  bytes32 calculatedHash = reCalculateBlockheaders(_blockheaders, currentBlockhash);  The following loop in reCalculateBlockheaders is skipped and the currentBlockhash is returned.  code/in3-contracts/contracts/BlockhashRegistry.sol:L134-L149  function reCalculateBlockheaders(bytes[] memory _blockheaders, bytes32 _bHash) public pure returns (bytes32 bhash) {  bytes32 currentBlockhash = _bHash;  bytes32 calcParent = 0x0;  bytes32 calcBlockhash = 0x0;  /// save to use for up to 200 blocks, exponential increase of gas-usage afterwards  for (uint i = 0; i < _blockheaders.length; i++) {  (calcParent, calcBlockhash) = getParentAndBlockhash(_blockheaders[i]);  if (calcBlockhash != currentBlockhash) {  return 0x0;  currentBlockhash = calcParent;  return currentBlockhash;  The assertion does not fire, the bnr to store the calculatedHash is the same as the one initially provided to the method as an argument.. Nothing has changed but an event is emitted.  code/in3-contracts/contracts/BlockhashRegistry.sol:L69-L74  /// we should never fail this assert, as this would mean that we were able to recreate a invalid blockchain  assert(_blockNumber > _blockheaders.length);  uint bnr = _blockNumber - _blockheaders.length;  blockhashMapping[bnr] = calculatedHash;  emit LogBlockhashAdded(bnr, calculatedHash);  Recommendation  The method is crucial for the system to work correctly and must be tightly controlled by input validation. It should not be allowed to overwrite an existing value in the contract (issue 6.29) or emit an event even though nothing has happened. Therefore validate that user provided input is within safe bounds. In this case, that at least one _blockheader has been provided. Validate that _blockNumber is less than block.number and do not expect that parts of the code will throw and safe the contract from exploitation.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.20 NodeRegistry.updateNode replaces signer with owner and emits inconsistent events    ", "body": "  Resolution  Reviewed merged changes at in3-contracts/5cb54165.  The method now emits a distinct event twice when node properties are updated.  The event correctly emits the signer.  When updating a node URL, the new URLInformation now correctly sets the signer.  However, there is a discrepancy between the process of registering a node and updating node s properties. When registering a node the owner has to provide a signed message containing the registration properties from the signer. Once the node is registered it can be unilaterally updated by the owner without requiring the signers permission to do so. According to slock.it it is assumed that the node owner and the signer are in control of the same entity and therefore this is not a concern.  Description  https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/36).  code/in3-contracts/contracts/NodeRegistry.sol:L438-L452  if (newURl != keccak256(bytes(node.url))) {  // deleting the old entry  delete urlIndex[keccak256(bytes(node.url))];  // make sure the new url is not already in use  require(!urlIndex[newURl].used, \"url is already in use\");  UrlInformation memory ui;  ui.used = true;  ui.signer = msg.sender;  urlIndex[newURl] = ui;  node.url = _url;  Furthermore, the method emits a LogNodeRegistered event when the node structure is updated. However, the event will always emit msg.sender as the signer even though that might not be true. For example, if the url does not change, the signer can still be another account that was previously registered with registerNodeFor and is not necessarily the owner.  code/in3-contracts/contracts/NodeRegistry.sol:L473-L478  emit LogNodeRegistered(  node.url,  _props,  msg.sender,  node.deposit  );  code/in3-contracts/contracts/NodeRegistry.sol:L30-L30  event LogNodeRegistered(string url, uint props, address signer, uint deposit);  Recommendation  The updateNode() function gets the signer as an input used to reference the node structure and this signer should be set for the UrlInformation.  function updateNode(  address _signer,  string calldata _url,  uint64 _props,  uint64 _timeout,  uint64 _weight  The method should actually only allow to change node properties when owner==signer otherwise updateNode is bypassing the strict requirements enforced with registerNodeFor where e.g. the url needs to be signed by the signer in order to register it.  The emitted event should always emit node.signer instead of msg.signer which can be wrong.  The method should emit its own distinct event LogNodeUpdated for audit purposes and to be able to distinguish new node registrations from node structure updates. This might also require software changes to client/node implementations to listen for node updates.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.21 NodeRegistry - In3Node memory n is never used    ", "body": "  Resolution   Fixed by removing the modifier and move the node-signer check to functions in   f1fd7943  Description  NodeRegistry In3Node memory n is never used inside the modifier onlyActiveState.  code/in3-contracts/contracts/NodeRegistry.sol:L125-L133  modifier onlyActiveState(address _signer) {  SignerInformation memory si = signerIndex[_signer];  require(si.stage == Stages.Active, \"address is not an in3-signer\");  In3Node memory n = nodes[si.index];  assert(nodes[si.index].signer == _signer);  _;  Recommendation  Use n in the assertion to access the node signer assert(n.signer == _signer);  or directly access it from storage and avoid copying the struct.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.22 NodeRegistry - returnDeposit and transferOwnership should emit an event    ", "body": "  Resolution   Fixed in   f1fd7943 by adding new events (  Description  Important state changing functions should emit an event for the purpose of having an audit trail and being able to monitor the smart contract usage and performance.  Recommendation  Emit events for returnDeposit and transferOwnership.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.23 in3-server - in3_stats leaks information   ", "body": "  Resolution   There is a config-option   Description  in3_stat shows information from node activities in the currentMonth, currentDay, currentHour which can result in leaking information about the functionality that node is being used for. This information might be valuable when an attacker wants to find out how utilized a node is and if any reflection attacks are successful (https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50).  Examples  'profile': AttributeDict({  'name': 'Slockit2',  'icon': 'https://slock.it/assets/slock_logo.png',  'url': 'https://slock.it'  }),  'stats': Attr  ibuteDict({  'upSince': 1568400626355,  'currentMonth': AttributeDict({  'requests': 47618,  'lastRequest': 1569422025347,  'methods': AttributeDict({  'nd-2': 2,  'eth_call': 940,  'eth_blockNumber': 25,  'eth_getBlockByNumber': 45395,  'web3_clientVersion': 386,  'admin_datadir': 7,  'admin_peers': 11,  'shh_version': 7,  'shh_info': 14,  'admin_nodeInfo ': 9, '  txpool_status ': 9, '  personal_listAccounts ': 3, '  eth_chainId ': 12, '  eth_protocolVersion ': 6, '  net_listening ': 6, '  net_peerCount ': 6, '  eth_syncing ': 6,  'eth_mining': 6,  'eth_hashrate': 6,  'eth_gasPrice': 18,  'eth_coinbase': 44,  'eth_accounts': 54,  'eth_getBalance': 321,  'personal_unlockAccount': 61,  'personal_  importRawKey ': 5, '  personal_newAccount ': 8, '  eth_estimateGas ': 16, '  eth_sendRawTransaction ': 9, '  eth_getTransactionReceipt ': 49, '  in3_sign ': 59, '  eth_getCode ': 33,  'eth_getTransactionCount': 15,  'eth_getLogs': 8,  'in3_stats': 16,  'in3_validatorlist': 15,  'in3_nodeList': 15,  'in3_call': 15,  'proof_in3_sign': 1  })  }),  'currentDay': AttributeDict({  'requests': 144,  'lastRequest': 1569422025347,  'methods': AttributeDict({  'eth_getBlockByNumber': 135,  'web3_clientVersion': 6,  'eth_coinbase': 1,  'eth_accounts': 1,  'in3_stats': 1  })  }),  'currentHour': AttributeDict({  'requests': 144,  'lastRequest': 1569422025346,  'methods': AttributeDict({  'eth_getBlockByNumber': 135,  'web3_clientVersion': 6,  'eth_coinbase': 1,  'eth_accounts': 1,  'in3_stats': 1  })  })  })  Recommendation  Make sure if this information is needed, if not enable it just for debugging purposes.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.24 NodeRegistry - removeNode unnecessarily casts the nodeIndex to uint64 potentially truncating its value    ", "body": "  Resolution   Fixed as per recommendation   https://git.slock.it/in3/in3-contracts/commit/6c35dd422e27eec1b1d2f70e328268014cadb515.  Description  removeNode removes a node from the Nodes array. This is done by copying the last node of the array to the _nodeIndex of the node that is to be removed. Finally the node array size is decreased.  A Node s index is also referenced in the SignerInformation struct. This index needs to be adjusted when removing a node from the array as the last node is copied to the index of the node that is to be removed.  code/in3-contracts/contracts/NodeRegistry.sol:L60-L69  struct SignerInformation {  uint64 lockedTime;                  /// timestamp until the deposit of an in3-node can not be withdrawn after the node was removed  address owner;                      /// the owner of the node  Stages stage;                       /// state of the address  uint depositAmount;                 /// amount of deposit to be locked, used only after a node had been removed  uint index;                         /// current index-position of the node in the node-array  code/in3-contracts/contracts/NodeRegistry.sol:L614-L620  // move the last entry to the removed one.  In3Node memory m = nodes[length - 1];  nodes[_nodeIndex] = m;  SignerInformation storage si = signerIndex[m.signer];  si.index = uint64(_nodeIndex);  nodes.length--;  Recommendation  Do not cast and therefore truncate the index.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.25 Registries - general inconsistencies   Pending", "body": "  Resolution  The breakdown of the fixes are as follows:  NodeRegistry - check for addr(0) being passed. This is anyway only done in the constructor and will not require a lot of gas.  The proper checks for registry addresses are added in 4786a966.  NodeRregistry - unnecessary payable  Removed payable modifier everywhere, as ERC20 support is added to the system. ERC20 support is not part of this audit.  NodeRegistry - deposit checks can be combined into one function to make the code more readable. The min deposit amount could be exposed as public const to allow other entities to query the contracts minimum deposit similar to the max ether amount. MAX_ETHER_LIMIT should make clear that this limit is only applicable in the first year (e.g. MAX_ETHER_LIMIT_FIRST_YEAR) .  Fixed and variables renamed.  NodeRegistry - require(si.owner == msg.sender) can be checked before accessing the nodes array  Added proper checks in c9e75b35.  NodeRegistry - removeNode resets index to a valid node array index of 0. Even though the code will access the index it is good practice to set this to an invalid value to make sure it raises an error condition if it is wrongly accessed in a future revision of the code. This is mainly a safeguard. Another option is to invalidate the 0 index.  Although the index is not set to 0, this issue is not yet fixed (Follow up here).  NodeRegistry - implicitly set defaults are hard to maintain. This should be a constant state variable that can be queried to be transparent about minimum and maximum values. Prefer throwing an exception instead of automatically setting the value to a minimum as this might be unexpected by a client and can cover error conditions.  timeout has been removed, so this is obsolete as it is not in the new code anymore.  NodeRegistry - one year startup period: instead of storing the deployment timestamp the contract should store the end-of-admin-timestamp.  Fixed as recommended timestampAdminKeyActive = block.timestamp + YEAR_DEFINITION;  NodeRegistry - inefficient re-calculation of hash  Fixed (issues/16).  NodeRegistry - weight should be part of proofHash  Added in 9fa5548d.  NodeRegistry - updateNode if the new timeout is smaller than the current timeout it will silently be ignored. This may be unexpected by the caller and cover error conditions where a client provides wrong inputs. Raising an exception should be preferred in such cases instead of gracefully assuming values.  Fixed by removing timeout variable (issues/16).  NodeRegistry - admin functionality should be clearly named as such for transparency reasons (e.g. adminRemovenodeFromRegistry) .  Renamed all admin function in both contracts with prefix admin.  Description  NodeRegistry - check for addr(0) being passed. This is anyway only done in the constructor and will not require a lot of gas.  code/in3-contracts/contracts/NodeRegistry.sol:L138-L139  constructor(BlockhashRegistry _blockRegistry) public {  blockRegistry = _blockRegistry;  NodeRregistry - unnecessary payable  code/in3-contracts/contracts/NodeRegistry.sol:L535-L535  address payable _owner,  NodeRegistry - deposit checks can be combined into one function to make the code more readable. The min deposit amount could be exposed as public const to allow other entities to query the contracts minimum deposit similar to the max ether amount. MAX_ETHER_LIMIT should make clear that this limit is only applicable in the first year (e.g. MAX_ETHER_LIMIT_FIRST_YEAR) .  code/in3-contracts/contracts/NodeRegistry.sol:L543-L545  require(_deposit >= 10 finney, \"not enough deposit\");  checkNodeProperties(_deposit, _timeout);  code/in3-contracts/contracts/NodeRegistry.sol:L120-L120  uint constant internal MAX_ETHER_LIMIT = 50 ether;  NodeRegistry - require(si.owner == msg.sender) can be checked before accessing the nodes array  code/in3-contracts/contracts/NodeRegistry.sol:L402-L404  SignerInformation storage si = signerIndex[_signer];  In3Node memory n = nodes[si.index];  require(si.owner == msg.sender, \"only for the in3-node owner\");  NodeRegistry - removeNode resets index to a valid node array index of 0. Even though the code will access the index it is good practice to set this to an invalid value to make sure it raises an error condition if it is wrongly accessed in a future revision of the code. This is mainly a safeguard. Another option is to invalidate the 0 index.  code/in3-contracts/contracts/NodeRegistry.sol:L612-L612  signerIndex[nodes[_nodeIndex].signer].index = 0;  NodeRegistry - implicitly set defaults are hard to maintain. This should be a constant state variable that can be queried to be transparent about minimum and maximum values. Prefer throwing an exception instead of automatically setting the value to a minimum as this might be unexpected by a client and can cover error conditions.  code/in3-contracts/contracts/NodeRegistry.sol:L565-L565  m.timeout = _timeout > 1 hours ? _timeout : 1 hours;  NodeRegistry - one year startup period: instead of storing the deployment timestamp the contract should store the end-of-admin-timestamp.  code/in3-contracts/contracts/NodeRegistry.sol:L256-L256  require(block.timestamp < (blockTimeStampDeployment + YEAR_DEFINITION), \"only in 1st year\");// solhint-disable-line not-rely-on-time  NodeRegistry - inefficient re-calculation of hash  code/in3-contracts/contracts/NodeRegistry.sol:L438-L441  if (newURl != keccak256(bytes(node.url))) {  // deleting the old entry  delete urlIndex[keccak256(bytes(node.url))];  NodeRegistry - weight should be part of proofHash  code/in3-contracts/contracts/NodeRegistry.sol:L490-L502  function calcProofHash(In3Node memory _node) internal pure returns (bytes32) {  return keccak256(  abi.encodePacked(  _node.deposit,  _node.timeout,  _node.registerTime,  _node.props,  _node.signer,  _node.url  );  NodeRegistry - updateNode if the new timeout is smaller than the current timeout it will silently be ignored. This may be unexpected by the caller and cover error conditions where a client provides wrong inputs. Raising an exception should be preferred in such cases instead of gracefully assuming values.  code/in3-contracts/contracts/NodeRegistry.sol:L463-L465  if (_timeout > node.timeout) {  node.timeout = _timeout;  NodeRegistry - admin functionality should be clearly named as such for transparency reasons (e.g. adminRemovenodeFromRegistry) .  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.26 BlockhashRegistry- assembly code can be optimized    ", "body": "  Resolution   Fixed as per recommendation with   https://git.slock.it/in3/in3-contracts/commit/87f02a7c4f5c30d2b4be42f331c1306e85d42ca6.  Description  The following code can be optimized by removing mload and mstore:  code/in3-contracts/contracts/BlockhashRegistry.sol:L106-L125  require(first > 0xf7, \"invalid offset\");  uint8 offset = first - 0xf7 + 2;  /// we are using assembly because it's the most efficent way to access the parent blockhash within the rlp-encoded blockheader  // solium-disable-next-line security/no-inline-assembly  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  mstore(0x20, _blockheader)  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  mload(0x20), 0x20  ), offset)  Recommendation  assembly { // solhint-disable-line no-inline-assembly  // mstore to get the memory pointer of the blockheader to 0x20  //mstore(0x20, _blockheader)  //@audit should assign 0x20ptr to variable first and use it.  // we load the pointer we just stored  // then we add 0x20 (32 bytes) to get to the start of the blockheader  // then we add the offset we calculated  // and load it to the parentHash variable  parentHash :=mload(  add(  add(  _blockheader, 0x20  ), offset)  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.27 Experimental Compiler features are enabled - ABIEncoderV2   ", "body": "  Resolution  This issue has been addressed with the following statement:  In order to pass structs between contracts we need that new ABIEncoder. [..] The old NodeRegistry did not require the ABIEncoderV2. [..] But due to the separation of the contracts in Logic and Data we are passing certain data-structures between contracts.  Description  The smart contracts enable experimental compiler features. Please note that these features are experimental for a reason and should be avoided unless explicitly required.  code/in3-contracts/contracts/BlockhashRegistry.sol:L21-L21  pragma experimental ABIEncoderV2;  Seems that NodeRegistry does not require any ABIEncoderV2 specific functionality.  code/in3-contracts/contracts/NodeRegistry.sol:L21-L21  pragma experimental ABIEncoderV2;  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.28 BlockhashRegistry - recreateBlockheaders() should use the evm provided blockhash when applicable   Pending", "body": "  Resolution  The provided code-change at 79fa3ef1 is not addressing the raised concerns. As noted in the recommendation it is suggested to completely skip the recreation routine if the target blockhash (_blockNumber.sub(_blockheaders.length)) is available to the evm. The method should call saveBlockNumber(_blockNumber) instead.  The commit attempts to add a verification for extracted blockhashes from the user provided RLP data if the blockhash for the block is available. However, the variable name currentBlock is misleading making it hard to follow the authors intent.  Description  There are different levels of trust attached to blockhashes stored in the BlockhashRegistry. On one side there are blockhashes which data-source is the evm ( blockhash(blocknumber)) and on the other side there are blockhashes that have been fed into the system by recalculating block-headers and establishing a trust chain to an already existing blockhash in the contract. While the contract can trust the result of blockhash(blocknumber) for the most recent 256 blocks because the information is coming directly from the evm, blockhashes that are re-created by calling recreateBlockheaders are manually verified and trust relies on the proper validation of the chain of block-headers provided.  Side-effect: Also saves gas by avoiding unnecessary calculations within the recreateBlockheaders() codepath as blockhash is already available via evm.  Recommendation  recreateBlockheaders() should prefer to use blockhash(number) by calling saveBlockNumber() instead of re-calculating the blockhash from the user provided chain of blockheaders, if the blockhash can easily be accessed by the evm (most recent 256 blocks, except current block). Check if _blockheaders.length > 0 && _blockNumber.sub(_blockheaders.length) < block.number-256.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "6.29 BlockhashRegistry - Existing blockhashes can be overwritten    ", "body": "  Resolution   Addressed with   80bb6ecf and  17d450cf by checking if blockhash exists and changing the  Description  Last 256 blocks, that are available in the EVM environment, are stored in BlockhashRegistry by calling snapshot() or saveBlockNumber(uint _blockNumber) functions. Older blocks are recreated by calling recreateBlockheaders.  The methods will overwrite existing blockhashes.  code/in3-contracts/contracts/BlockhashRegistry.sol:L79-L87  function saveBlockNumber(uint _blockNumber) public {  bytes32 bHash = blockhash(_blockNumber);  require(bHash != 0x0, \"block not available\");  blockhashMapping[_blockNumber] = bHash;  emit LogBlockhashAdded(_blockNumber, bHash);  code/in3-contracts/contracts/BlockhashRegistry.sol:L72  blockhashMapping[bnr] = calculatedHash;  Recommendation  If a block is already saved in the smart contract, it can be checked and a SSTORE can be prevented to save gas. Require that blocknumber hash is not stored.  require(blockhashMapping[_blockNumber] == 0x0, \"block already saved\");  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  Below is the raw output of the MythX vulnerability scan:  \"issues\": [  \"swcID\": \"SWC-127\",  \"swcTitle\": \"\",  \"description\": {  \"head\": \"jump to arbitrary destination\",  \"tail\": \"A caller can trigger a jump to an arbitrary destination. Make sure this does not enable unintended control flow.\"  },  \"severity\": \"High\",  \"locations\": [  \"sourceMap\": \"20901:1:1\",  \"sourceType\": \"raw-bytecode\",  \"sourceFormat\": \"evm-byzantium-bytecode\",  \"sourceList\": [  \"0x63ad5e3d2c8551cf64f6d0425940efdeb79801907fcad157d1c82922919c13cb\",  \"0xd8d70c0998b3293c364b1cde922c80081d70d02726e74091c8aae6fa6a10b892\"  },  \"sourceMap\": \"23263:248:-1\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 5593089348,  \"testCases\": [  \"initialState\": {  \"accounts\": {  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\": {  \"nonce\": 0,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa1\": {  \"nonce\": 1,  \"balance\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa2\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa3\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x00\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa4\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0xfd\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa5\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x608060405260005600a165627a7a72305820466f8a1bdae15c60b8e998fe04836ef505803cfbd8edd29bd4679531357576530029\",  \"storage\": {}  },  \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa6\": {  \"nonce\": 1,  \"balance\": \"0x00000000000000000000000000000000000000ffffffffffffffffffffffffff\",  \"code\": \"0x608060405273aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa63081146038578073ffffffffffffffffffffffffffffffffffffffff16ff5b5000fea165627a7a723058205e8b906b72ad42c69b05acf4542283b6080ae82562bc74baac467daac2fb0e0e0029\",  \"storage\": {}  },  \"0xaffeaffeaffeaffeaffeaffeaffeaffeaffeaffe\": {  \"nonce\": 0,  \"balance\": \"0x0000000000000000000000000000000000ffffffffffffffffffffffffffffff\",  \"code\": \"\",  \"storage\": {}  },  \"steps\": [  \"address\": \"\",  \"gasLimit\": \"0xffffff\",  \"gasPrice\": \"0x773594000\",  \"input\": REMOVED,  \"origin\": \"0xaffeaffeaffeaffeaffeaffeaffeaffeaffeaffe\",  \"value\": \"0x0\",  \"blockCoinbase\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"blockDifficulty\": \"0xa7d7343662e26\",  \"blockGasLimit\": \"0xffffff\",  \"blockNumber\": \"0x661a55\",  \"blockTime\": \"0x5be99aa8\"  },  \"address\": \"0x0901d12ebe1b195e5aa8748e62bd7734ae19b51f\",  \"gasLimit\": \"0x7d00\",  \"gasPrice\": \"0x773594000\",  \"input\": \"0xac48987300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\",  \"origin\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"value\": \"0x9\",  \"blockCoinbase\": \"0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0\",  \"blockDifficulty\": \"0x52c054bfb494c\",  \"blockGasLimit\": \"0x7d0000\",  \"blockNumber\": \"0x661a55\",  \"blockTime\": \"0x5be99aa8\"  ],  \"toolName\": \"harvey\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"blockhash\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"blockhash\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"6746:25:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666071716,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"blockhash\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"blockhash\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"13158:23:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666159516,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"6756:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1666984722,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"7532:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1667012822,  \"toolName\": \"maru\"  },  \"swcID\": \"SWC-120\",  \"swcTitle\": \"Weak Sources of Randomness from Chain Attributes\",  \"description\": {  \"head\": \"Potential use of a weak source of randomness \\\"block.number\\\".\",  \"tail\": \"Using past or the current block hashes through \\\"block.number\\\" as a source of randomness is predictable. The issue can be ignored if this is unrelated to randomness or if the usage is related to a secure implementation of a commit/reveal scheme.\"  },  \"severity\": \"Medium\",  \"locations\": [  \"sourceMap\": \"13711:12:0\",  \"sourceType\": \"solidity-file\",  \"sourceFormat\": \"text\",  \"sourceList\": [  \"NodeRegistry.sol\"  ],  \"extra\": {  \"discoveryTime\": 1667019122,  \"toolName\": \"maru\"  ],  \"sourceType\": \"raw-bytecode\",  \"sourceFormat\": \"evm-byzantium-bytecode\",  \"sourceList\": [  \"0x63ad5e3d2c8551cf64f6d0425940efdeb79801907fcad157d1c82922919c13cb\",  \"0xd8d70c0998b3293c364b1cde922c80081d70d02726e74091c8aae6fa6a10b892\"  ],  \"meta\": {  \"selectedCompiler\": \"Unknown\",  \"logs\": [],  \"toolName\": \"maru\",  \"coveredPaths\": 91,  \"coveredInstructions\": 7058  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/BlockhashRegistry.sol  21:0     warning    Avoid using experimental features in production code    no-experimental  46:12    warning    Line exceeds the limit of 145 characters                max-len  61:1     error      Line contains trailing whitespace                       no-trailing-whitespace  79:4     warning    Line exceeds the limit of 145 characters                max-len  81:1     error      Line contains trailing whitespace                       no-trailing-whitespace  98:4     warning    Line exceeds the limit of 145 characters                max-len  134:4    warning    Line exceeds the limit of 145 characters                max-len  142:1    error      Line contains trailing whitespace                       no-trailing-whitespace  contracts/NodeRegistry.sol  21:0     warning    Avoid using experimental features in production code    no-experimental  117:1    error      Line contains trailing whitespace                       no-trailing-whitespace  123:1    error      Line contains trailing whitespace                       no-trailing-whitespace  128:8    warning    Line exceeds the limit of 145 characters                max-len  143:1    error      Line contains trailing whitespace                       no-trailing-whitespace  143:8    warning    Line exceeds the limit of 145 characters                max-len  152:1    error      Line contains trailing whitespace                       no-trailing-whitespace  152:4    warning    Line exceeds the limit of 145 characters                max-len  197:1    error      Line contains trailing whitespace                       no-trailing-whitespace  200:1    error      Line contains trailing whitespace                       no-trailing-whitespace  215:1    error      Line contains trailing whitespace                       no-trailing-whitespace  224:1    error      Line contains trailing whitespace                       no-trailing-whitespace  324:1    error      Line contains trailing whitespace                       no-trailing-whitespace  342:1    error      Line contains trailing whitespace                       no-trailing-whitespace  448:1    error      Line contains trailing whitespace                       no-trailing-whitespace  555:2    error      Line contains trailing whitespace                       no-trailing-whitespace  555:8    warning    Line exceeds the limit of 145 characters                max-len  568:1    error      Line contains trailing whitespace                       no-trailing-whitespace  571:1    error      Line contains trailing whitespace                       no-trailing-whitespace  602:1    error      Line contains trailing whitespace                       no-trailing-whitespace  615:1    error      Line contains trailing whitespace                       no-trailing-whitespace  \u2716 19 errors, 10 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Function Name  Visibility  Mutability  Modifiers  NodeRegistry  Implementation  <Constructor>  Public    convict  External    NO   registerNode  External    NO   registerNodeFor  External    NO   removeNodeFromRegistry  External    onlyActiveState  returnDeposit  External    NO   revealConvict  External    NO   transferOwnership  External    onlyActiveState  unregisteringNode  External    onlyActiveState  updateNode  External    onlyActiveState  totalNodes  External    NO   calcProofHash  Internal \ud83d\udd12  checkNodeProperties  Internal \ud83d\udd12  registerNodeInternal  Internal \ud83d\udd12  unregisterNodeInternal  Internal \ud83d\udd12  removeNode  Internal \ud83d\udd12  BlockhashRegistry  Implementation  <Constructor>  Public    searchForAvailableBlock  External    NO   recreateBlockheaders  Public    NO   saveBlockNumber  Public    NO   snapshot  Public    NO   getParentAndBlockhash  Public    NO   reCalculateBlockheaders  Public    NO   Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.4 Other Tools", "body": "  Other security tools such as Slither was also used to identify problems in the smart contract.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "7.5 Test Coverage", "body": "  Code coverage metrics indicate the amount of lines/statements/branches that are covered by the test-suite. It s important to note that  100% test coverage  does not indicate the code has no vulnerabilities. Be aware that code coverage does not provide information about the individual test-cases quality.  A fork of the Solidity-Coverage tool was used to measure the portion of the code base exercised by the test suite, and identify areas with little or no coverage. Specific sections of the code where necessary test coverage is missing are included in the Issue Details section.  The project is using the automated testing framework provided by Truffle. The test-suite is evaluating 62 individual tests and the test-suite passed without errors. The corresponding console output can be found here.  A code coverage report was generated and is provided along other tool output. The test coverage results for NodeRegistry.sol can be viewed here. The test coverage results for BlockhashRegistry.sol can be viewed here. Please find a summary of the coverage results below.  BlockhashRegistry.sol  100%  30/30  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "92.86%", "body": "  13/14  100%  7/7  100%  31/31  NodeRegistry.sol  100%  123/123  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "95.45%", "body": "  63/66  100%  17/17  100%  129/129  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/09/slock.it-incubed3/"}, {"title": "2.1 Accounts that claim incentives immediately before the migration will be stuck ", "body": "  Description  For accounts that existed before the migration to the new incentive calculation, the following happens when they claim incentives for the first time after the migration: First, the incentives that are still owed from before the migration are computed according to the old formula; the incentives since the migration are calculated according to the new logic, and the two values are added together. The first part   calculating the pre-migration incentives according to the old formula   happens in function MigrateIncentives.migrateAccountFromPreviousCalculation; the following lines are of particular interest in the current context:  code-582dc37/contracts/external/MigrateIncentives.sol:L39-L50  uint256 timeSinceMigration = finalMigrationTime - lastClaimTime;  // (timeSinceMigration * INTERNAL_TOKEN_PRECISION * finalEmissionRatePerYear) / YEAR  uint256 incentiveRate =  timeSinceMigration  .mul(uint256(Constants.INTERNAL_TOKEN_PRECISION))  // Migration emission rate is stored as is, denominated in whole tokens  .mul(finalEmissionRatePerYear).mul(uint256(Constants.INTERNAL_TOKEN_PRECISION))  .div(Constants.YEAR);  // Returns the average supply using the integral of the total supply.  uint256 avgTotalSupply = finalTotalIntegralSupply.sub(lastClaimIntegralSupply).div(timeSinceMigration);  The division in the last line will throw if finalMigrationTime and lastClaimTime are equal. This will happen if an account claims incentives immediately before the migration happens   where  immediately  means in the same block. In such a case, the account will be stuck as any attempt to claim incentives will revert.  Recommendation  The function should return 0 if finalMigrationTime and lastClaimTime are equal. Moreover, the variable name timeSinceMigration is misleading, as the variable doesn t store the time since the migration but the time between the last incentive claim and the migration.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/notional-protocol-v2.1/"}, {"title": "2.2 type(T).max is inclusive ", "body": "  Description  Throughout the codebase, there are checks whether a number can be represented by a certain type.  Examples  code-582dc37/contracts/internal/nToken/nTokenSupply.sol:L71  require(accumulatedNOTEPerNToken < type(uint128).max); // dev: accumulated NOTE overflow  code-582dc37/contracts/internal/nToken/nTokenSupply.sol:L134  require(blockTime < type(uint32).max); // dev: block time overflow  code-582dc37/contracts/external/patchfix/MigrateIncentivesFix.sol:L86-L87  require(totalSupply <= type(uint96).max);  require(blockTime <= type(uint32).max);  Sometimes these checks use <=, sometimes they use <.  Recommendation  type(T).max is inclusive, i.e., it is the greatest number that can be represented with type T. Strictly speaking, it can and should therefore be used consistently with <= instead of <.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/03/notional-protocol-v2.1/"}, {"title": "2.3  mathematical mistake in comment ", "body": "  Description  In nTokenSupply.sol, there is a comment explaining why 18 decimal places for the accumulation precision is a good choice. There is a minor mistake in the calculation. It does not invalidate the reasoning, but as it is confusing for a reader, we recommend correcting it.  code-582dc37/contracts/internal/nToken/nTokenSupply.sol:L85-L88  // If we use 18 decimal places as the accumulation precision then we will overflow uint128 when  // a single nToken has accumulated 3.4 x 10^20 NOTE tokens. This isn't possible since the max  // NOTE that can accumulate is 10^17 (100 million NOTE in 1e8 precision) so we should be safe  // using 18 decimal places and uint128 storage slot  100 million NOTE in 1e8 precision is 10^16.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/03/notional-protocol-v2.1/"}, {"title": "6.1 Unexpected response in an assimilator s external call can lock-up the whole system    ", "body": "  Resolution  Comment from the development team:  When this was brought to our attention, it made the most sense to look at it from a bird s eye view. In the event that an assimilator does seize up either due to smart contract malfunctioning or to some type of governance decision in one of our dependencies, then depending on the severity of the event, it could either make it so that that particular dependency is unable to be transacted with or it could brick the pool altogether.  In the case of the latter severity where the pool is bricked altogether for an extended period of time, then this means the end of that particular pool s life. In this case, we find it prudent to allow for the withdrawal of any asset still functional from the pool. Should such an event transpire, we have instituted functionality to allow users to withdraw individually from the pool s assets according to their Shell balances without being exposed to the inertia of the incapacitated assets.  In such an event, the owner of the pool can now trigger a partitioned state which is an end of life state for the pool in which users send Shells as normal until they decide to redeem any portion of them, after which they will only be able to redeem the portion of individual asset balances their Shell balance held claims on.  Description  The assimilators, being the  middleware  between a shell and all the external DeFi systems it interacts with, perform several external calls within their methods, as would be expected.  An example of such a contract is mainnetSUsdToASUsdAssimilator.sol (the contract can be found here).  The problem outlined in the title arises from the fact that Solidity automatically checks for the successful execution of the underlying message call (i.e., it bubbles up assertions and reverts) and, therefore, if any of these external systems changes in unexpected ways the call to the shell will revert itself.  This problem is immensely magnified by the fact that all the external methods in Loihi dealing with deposits, withdraws, and swaps rebalance the pool and, as a consequence, all of the assimilators for the reserve tokens get called at some point.  In summary, if any of the reserve tokens start, for some reason, refusing to complete a call to some of their methods, the whole protocol stops working, and the tokens are locked in forever (this is assuming the development team removes the safeApprove function from Loihi, v. https://github.com/ConsenSys/shell-protocol-audit-2020-06/issues/10).  Recommendation  There is no easy solution to this problem since calls to these external systems cannot simply be ignored. Shell needs successful responses from the reserve assimilators to be able to function properly.  One possible mitigation is to create a trustless mechanism based on repeated misbehavior by an external system to be able to remove a reserve asset from the pool.  Such a design could consist of an external function accessible to all actors that needs X confirmations over a period of Y blocks (or days, for that matter) with even spacing between them to be able to remove a reserve asset.  This means that no trust to the owners is implied (since this would require the extreme power to take user s tokens) and still maintains the healthy option of being able to remove faulty tokens from the pool.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.2 Certain functions lack input validation routines    ", "body": "  Resolution  Comment from the development team:  Now all functions in the Orchestrator revert on incorrect arguments.  All functions in Loihi in general revert on incorrect arguments.  Description  The functions should first check if the passed arguments are valid first. The checks-effects-interactions pattern should be implemented throughout the code.  These checks should include, but not be limited to:  uint should be larger than 0 when 0 is considered invalid  uint should be within constraints  int should be positive in some cases  length of arrays should match if more arrays are sent as arguments  addresses should not be 0x0  Examples  The function includeAsset does not do any checks before changing the contract state.  src/Loihi.sol:L59-L61  function includeAsset (address _numeraire, address _nAssim, address _reserve, address _rAssim, uint256 _weight) public onlyOwner {  shell.includeAsset(_numeraire, _nAssim, _reserve, _rAssim, _weight);  The internal function called by the public method includeAsset again doesn t check any of the data.  src/Controller.sol:L77-L97  function includeAsset (Shells.Shell storage shell, address _numeraire, address _numeraireAssim, address _reserve, address _reserveAssim, uint256 _weight) internal {  Assimilators.Assimilator storage _numeraireAssimilator = shell.assimilators[_numeraire];  _numeraireAssimilator.addr = _numeraireAssim;  _numeraireAssimilator.ix = uint8(shell.numeraires.length);  shell.numeraires.push(_numeraireAssimilator);  Assimilators.Assimilator storage _reserveAssimilator = shell.assimilators[_reserve];  _reserveAssimilator.addr = _reserveAssim;  _reserveAssimilator.ix = uint8(shell.reserves.length);  shell.reserves.push(_reserveAssimilator);  shell.weights.push(_weight.divu(1e18).add(uint256(1).divu(1e18)));  Similar with includeAssimilator.  src/Loihi.sol:L63-L65  function includeAssimilator (address _numeraire, address _derivative, address _assimilator) public onlyOwner {  shell.includeAssimilator(_numeraire, _derivative, _assimilator);  Again no checks are done in any function.  src/Controller.sol:L99-L106  function includeAssimilator (Shells.Shell storage shell, address _numeraire, address _derivative, address _assimilator) internal {  Assimilators.Assimilator storage _numeraireAssim = shell.assimilators[_numeraire];  shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix);  // shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix, 0, 0);  Not only does the administrator functions not have any checks, but also user facing functions do not check the arguments.  For example swapByOrigin does not check any of the arguments if you consider it calls MainnetDaiToDaiAssimilator.  src/Loihi.sol:L85-L89  function swapByOrigin (address _o, address _t, uint256 _oAmt, uint256 _mTAmt, uint256 _dline) public notFrozen returns (uint256 tAmt_) {  return transferByOrigin(_o, _t, _dline, _mTAmt, _oAmt, msg.sender);  It calls transferByOrigin and we simplify this example and consider we have _o.ix == _t.ix  src/Loihi.sol:L181-L187  function transferByOrigin (address _origin, address _target, uint256 _dline, uint256 _mTAmt, uint256 _oAmt, address _rcpnt) public notFrozen nonReentrant returns (uint256 tAmt_) {  Assimilators.Assimilator memory _o = shell.assimilators[_origin];  Assimilators.Assimilator memory _t = shell.assimilators[_target];  // TODO: how to include min target amount  if (_o.ix == _t.ix) return _t.addr.outputNumeraire(_rcpnt, _o.addr.intakeRaw(_oAmt));  In which case it can call 2 functions on an assimilatior such as MainnetDaiToDaiAssimilator.  The first called function is intakeRaw.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L42-L49  // transfers raw amonut of dai in, wraps it in cDai, returns numeraire amount  function intakeRaw (uint256 _amount) public returns (int128 amount_, int128 balance_) {  dai.transferFrom(msg.sender, address(this), _amount);  amount_ = _amount.divu(1e18);  And its result is used in outputNumeraire that again does not have any checks.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L83-L92  // takes numeraire amount of dai, unwraps corresponding amount of cDai, transfers that out, returns numeraire amount  function outputNumeraire (address _dst, int128 _amount) public returns (uint256 amount_) {  amount_ = _amount.mulu(1e18);  dai.transfer(_dst, amount_);  return amount_;  Recommendation  Implement the checks-effects-interactions as a pattern to write code. Add tests that check if all of the arguments have been validated.  Consider checking arguments as an important part of writing code and developing the system.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.3 Remove Loihi methods that can be used as backdoors by the administrator    ", "body": "  Resolution  Issue was partly addressed by the development team. However, the feature to add new assimilators is still present and that ultimately means that the administrators have power to run arbitrary bytecode.  Updated remediation response Since the development team still hadn t fully settled on a strategy for a mainnet launch, this was left as a residue even after the audit mitigation phase. However, at launch time, this issue was no longer present and all the assimilators are now defined at deploy-time, it is fully resolved.  Description  There are several functions in Loihi that give extreme powers to the shell administrator. The most dangerous set of those is the ones granting the capability to add assimilators.  Since assimilators are essentially a proxy architecture to delegate code to several different implementations of the same interface, the administrator could, intentionally or unintentionally, deploy malicious or faulty code in the implementation of an assimilator. This means that the administrator is essentially totally trusted to not run code that, for example, drains the whole pool or locks up the users  and LPs  tokens.  In addition to these, the function safeApprove allows the administrator to move any of the tokens the contract holds to any address regardless of the balances any of the users have.  This can also be used by the owner as a backdoor to completely drain the contract.  src/Loihi.sol:L643-L649  function safeApprove(address _token, address _spender, uint256 _value) public onlyOwner {  (bool success, bytes memory returndata) = _token.call(abi.encodeWithSignature(\"approve(address,uint256)\", _spender, _value));  require(success, \"SafeERC20: low-level call failed\");  Recommendation  Remove the safeApprove function and, instead, use a trustless escape-hatch mechanism like the one suggested in issue 6.1.  For the assimilator addition functions, our recommendation is that they are made completely internal, only callable in the constructor, at deploy time.  Even though this is not a big structural change (in fact, it reduces the attack surface), it is, indeed, a feature loss. However, this is the only way to make each shell a time-invariant system.  This would not only increase Shell s security but also would greatly improve the trust the users have in the protocol since, after deployment, the code is now static and auditable.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.4 Assimilators should implement an interface    ", "body": "  Resolution  Comment from the development team:  They now implement the interface in  src/interfaces/IAssimilator.sol .  Description  The Assimilators are one of the core components within the application. They are used to move the tokens and can be thought of as a  middleware  between the Shell Protocol application and any other supported tokens.  The methods attached to the assimilators are called throughout the application and they are a critical component of the whole system. Because of this fact, it is extremely important that they behave correctly.  A suggestion to restrict the possibility of errors when implementing them and when using them is to make all of the assimilators implement a unique specific interface. This way, any deviation would be immediately observed, right when the compilation happens.  Examples  Consider this example. The user calls swapByOrigin.  src/Loihi.sol:L85-L89  function swapByOrigin (address _o, address _t, uint256 _oAmt, uint256 _mTAmt, uint256 _dline) public notFrozen returns (uint256 tAmt_) {  return transferByOrigin(_o, _t, _dline, _mTAmt, _oAmt, msg.sender);  Which calls transferByOrigin. In transferByOrigin, if the origin index matches the target index, a different execution branch is activated.  src/Loihi.sol:L187  if (_o.ix == _t.ix) return _t.addr.outputNumeraire(_rcpnt, _o.addr.intakeRaw(_oAmt));  In this case we need the output of _o.addr.intakeRaw(_oAmt).  If we pick a random assimilator and check the implementation, we see the function intakeRaw needs to return the transferred amount.  src/assimilators/mainnet/daiReserves/mainnetCDaiToDaiAssimilator.sol:L52-L67  // takes raw cdai amount, transfers it in, calculates corresponding numeraire amount and returns it  function intakeRaw (uint256 _amount) public returns (int128 amount_) {  bool success = cdai.transferFrom(msg.sender, address(this), _amount);  if (!success) revert(\"CDai/transferFrom-failed\");  uint256 _rate = cdai.exchangeRateStored();  _amount = ( _amount * _rate ) / 1e18;  cdai.redeemUnderlying(_amount);  amount_ = _amount.divu(1e18);  However, with other implementations, the returns do not match. In the case of MainnetDaiToDaiAssimilator, it returns 2 values, which will make the Loihi contract work in this case but can misbehave in other cases, or even fail.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L42-L49  // transfers raw amonut of dai in, wraps it in cDai, returns numeraire amount  function intakeRaw (uint256 _amount) public returns (int128 amount_, int128 balance_) {  dai.transferFrom(msg.sender, address(this), _amount);  amount_ = _amount.divu(1e18);  Making all the assimilators implement one unique interface will enforce the functions to look the same from the outside.  Recommendation  Create a unique interface for the assimilators and make all the contracts implement that interface.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.5 Assimilators do not conform to the ERC20 specification    ", "body": "  Resolution  Comment from the development team:  All calls to compliant ERC20s now check for return booleans. Non compliant ERC20s are called with a function that checks for the success of the call.  Description  The assimilators in the codebase make heavy usage of both the transfer and transferFrom methods in the ERC20 standard.  Quoting the relevant parts of the specification of the standard:  Transfers _value amount of tokens to address _to, and MUST fire the Transfer event. The function SHOULD throw if the message caller s account balance does not have enough tokens to spend.  The transferFrom method is used for a withdraw workflow, allowing contracts to transfer tokens on your behalf. This can be used for example to allow a contract to transfer tokens on your behalf and/or to charge fees in sub-currencies. The function SHOULD throw unless the _from account has deliberately authorized the sender of the message via some mechanism.  We can see that, even though it is suggested that ERC20-compliant tokens do throw on the lack of authorization from the sender or lack of funds to complete the transfer, the standard does not enforce it.  This means that, in order to make the system both more resilient and future-proof, code in each implementation of current and future assimilators should check for the return value of both transfer and transferFrom call instead of just relying on the external contract to revert execution.  The extent of this issue is only mitigated by the fact that new assets are only added by the shell administrator and could, therefore, be audited prior to their addition.  Non-exhaustive Examples  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L45  dai.transferFrom(msg.sender, address(this), _amount);  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L64  dai.transfer(_dst, _amount);  Recommendation  Add a check for the return boolean of the function.  Example:  require(dai.transferFrom(msg.sender, address(this), _amount) == true);  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.6 Access to assimilators does not check for existence and allows delegation to the zeroth address    ", "body": "  Resolution  Comment from the development team:  All retrieval of assimilators now check that the assimilators address is not the zeroth address.  Description  For every method that allows to selectively withdraw, deposit, or swap tokens in Loihi, the user is allowed to specify addresses for the assimilators of said tokens (by inputting the addresses of the tokens themselves).  The shell then performs a lookup on a mapping called assimilators inside its main structure and uses the result of that lookup to delegate call the assimilator deployed by the shell administrator.  However, there are no checks for prior instantiation of a specific, supported token, effectively meaning that we can do a lookup on an all-zeroed-out member of that mapping and delegate call execution to the zeroth address.  The only thing preventing execution from going forward in this unwanted fashion is the fact that the ABI decoder expects a certain return data size from the interface implemented in Assimilator.sol.  For example, the 32 bytes expected as a result of this call:  src/Assimilators.sol:L58-L66  function viewNumeraireAmount (address _assim, uint256 _amt) internal returns (int128 amt_) {  // amount_ = IAssimilator(_assim).viewNumeraireAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  amt_ = abi.decode(_assim.delegate(data), (int128)); // for development  This is definitely an insufficient check since the interface for the assimilators might change in the future to include functions that have no return values.  Recommendation  Check for the prior instantiation of assimilators by including the following requirement:  require(shell.assimilators[<TOKEN_ADDRESS>].ix != 0);  In all the functions that access the assimilators mapping and change the indexes to be 1-based instead pf 0-based.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.7 Math library s fork has problematic changes    ", "body": "  Description  The math library ABDK Libraries for Solidity was forked and modified to add a few unsafe_* functions.  unsafe_add  unsafe_sub  unsafe_mul  unsafe_div  unsafe_abs  The problem which was introduced is that unsafe_add ironically is not really unsafe, it is as safe as the original add function. It is, in fact, identical to the safe add function.  src/ABDKMath64x64.sol:L102-L113  /**  Calculate x + y.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @param y signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function add (int128 x, int128 y) internal pure returns (int128) {  int256 result = int256(x) + y;  require (result >= MIN_64x64 && result <= MAX_64x64);  return int128 (result);  src/ABDKMath64x64.sol:L115-L126  /**  Calculate x + y.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @param y signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function unsafe_add (int128 x, int128 y) internal pure returns (int128) {  int256 result = int256(x) + y;  require (result >= MIN_64x64 && result <= MAX_64x64);  return int128 (result);  Fortunately, unsafe_add is not used anywhere in the code.  However, unsafe_abs was changed from this:  src/ABDKMath64x64.sol:L322-L331  /**  Calculate |x|.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function abs (int128 x) internal pure returns (int128) {  require (x != MIN_64x64);  return x < 0 ? -x : x;  To this:  src/ABDKMath64x64.sol:L333-L341  /**  Calculate |x|.  Revert on overflow.  @param x signed 64.64-bit fixed point number  @return signed 64.64-bit fixed point number  /  function unsafe_abs (int128 x) internal pure returns (int128) {  return x < 0 ? -x : x;  The check that was removed, is actually an important check:  require (x != MIN_64x64);  src/ABDKMath64x64.sol:L19  int128 private constant MIN_64x64 = -0x80000000000000000000000000000000;  The problem is that for an int128 variable that is equal to -0x80000000000000000000000000000000, there is no absolute value within the constraints of int128.  Recommendation  Remove unused unsafe_* functions and try to find other ways of doing unsafe math (if it is fundamentally important) without changing existing, trusted, already audited code.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.8 Use one file for each contract or library    ", "body": "  Resolution  Issue fixed by the development team.  Description  The repository contains a lot of contracts and libraries that are added in the same file as another contract or library.  Organizing the code in this manner makes it hard to navigate, develop and audit. It is a best practice to have each contract or library in its own file. The file also needs to bear the name of the hosted contract or library.  Examples  src/Shells.sol:L20  library SafeERC20Arithmetic {  src/Shells.sol:L32  library Shells {  src/Loihi.sol:L26-L28  contract ERC20Approve {  function approve (address spender, uint256 amount) public returns (bool);  src/Loihi.sol:L30  contract Loihi is LoihiRoot {  src/Assimilators.sol:L19  library Delegate {  src/Assimilators.sol:L33  library Assimilators {  Recommendation  Split up contracts and libraries in single files.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.9 Remove debugging code from the repository    ", "body": "  Resolution  Issue fixed but he development team.  Description  Throughout the repository, there is source code from the development stage that was used for debugging the functionality and was not removed.  This should not be present in the source code and even if they are used while functionality is developed, they should be removed after the functionality was implemented.  Examples  src/Shells.sol:L63-L67  event log(bytes32);  event log_int(bytes32, int256);  event log_ints(bytes32, int256[]);  event log_uint(bytes32, uint256);  event log_uints(bytes32, uint256[]);  src/Assimilators.sol:L44-L46  event log(bytes32);  event log_uint(bytes32, uint256);  event log_int(bytes32, int256);  src/Controller.sol:L33-L37  event log(bytes32);  event log_int(bytes32, int128);  event log_int(bytes32, int);  event log_uint(bytes32, uint);  event log_addr(bytes32, address);  src/LoihiRoot.sol:L53  event log(bytes32);  src/Shells.sol:L63-L67  event log(bytes32);  event log_int(bytes32, int256);  event log_ints(bytes32, int256[]);  event log_uint(bytes32, uint256);  event log_uints(bytes32, uint256[]);  src/Loihi.sol:L470-L474  event log_int(bytes32, int);  event log_ints(bytes32, int128[]);  event log_uint(bytes32, uint);  event log_uints(bytes32, uint[]);  event log_addrs(bytes32, address[]);  src/assimilators/mainnet/cdaiReserves/mainnetDaiToCDaiAssimilator.sol:L35-L36  event log_uint(bytes32, uint256);  event log_int(bytes32, int256);  src/assimilators/mainnet/cusdcReserves/mainnetUsdcToCUsdcAssimilator.sol:L38  event log_uint(bytes32, uint256);  src/Loihi.sol:L51  shell.testHalts = true;  src/LoihiRoot.sol:L79-L83  function setTestHalts (bool _testOrNotToTest) public {  shell.testHalts = _testOrNotToTest;  src/Shells.sol:L60  bool testHalts;  Recommendation  Remove the debug functionality at the end of the development cycle of each functionality.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.10 Tests should not fail    ", "body": "  Resolution  Comment from the development team:  The failing tests are because we made minute changes to our present model (changes in applying the base fee -  epsilon ), so in a sense, rather than failing they just need updating. Many of them are also an artifact of architecting the tests in such a way that they can be run against arbitrary parameter sets - or in different  suites .  Description  The role of the tests should be to make sure the application behaves properly. This should include positive tests (functionality that should be implemented) and negative tests (behavior stopped or limited by the application).  The test suite should pass 100% of the tests. After spending time with the development team, we managed to ask for the changes that allowed us to run the tests suite. This revealed that out of the 555 tests, 206 are failing. This staggering number does not allow us to check what the problem is and makes anybody running tests ignore them completely.  Tests should be an integral part of the codebase, and they should be considered as important (or even more important) than the code itself. One should be able to recreate the whole codebase by just having the tests.  Recommendation  Update tests in order for the whole of the test suite to pass.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.11 Remove commented out code from the repository    ", "body": "  Description  Having commented out code increases the cognitive load on an already complex system. Also, it hides the important parts of the system that should get the proper attention, but that attention gets to be diluted.  There is no code that is important enough to be left commented out in a repository. Git branching should take care of having different code versions or diffs should show what was before.  If there is commented out code, this also has to be maintained; it will be out of date if other parts of the system are changed, and the tests will not pick that up.  The main problem is that commented code adds confusion with no real benefit. Code should be code, and comments should be comments.  Examples  Commented out code should be removed or dealt with in a separate branch that is later included in the master branch.  src/Assimilators.sol:L48-L56  function viewRawAmount (address _assim, int128 _amt) internal returns (uint256 amount_) {  // amount_ = IAssimilator(_assim).viewRawAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewRawAmount.selector, _amt.abs()); // for development  amount_ = abi.decode(_assim.delegate(data), (uint256)); // for development  src/Assimilators.sol:L58-L66  function viewNumeraireAmount (address _assim, uint256 _amt) internal returns (int128 amt_) {  // amount_ = IAssimilator(_assim).viewNumeraireAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  amt_ = abi.decode(_assim.delegate(data), (int128)); // for development  src/Assimilators.sol:L58-L66  function viewNumeraireAmount (address _assim, uint256 _amt) internal returns (int128 amt_) {  // amount_ = IAssimilator(_assim).viewNumeraireAmount(_amt); // for production  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  amt_ = abi.decode(_assim.delegate(data), (int128)); // for development  src/Controller.sol:L99-L106  function includeAssimilator (Shells.Shell storage shell, address _numeraire, address _derivative, address _assimilator) internal {  Assimilators.Assimilator storage _numeraireAssim = shell.assimilators[_numeraire];  shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix);  // shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix, 0, 0);  src/Loihi.sol:L596-L618  function transfer (address _recipient, uint256 _amount) public nonReentrant returns (bool) {  // return shell.transfer(_recipient, _amount);  function transferFrom (address _sender, address _recipient, uint256 _amount) public nonReentrant returns (bool) {  // return shell.transferFrom(_sender, _recipient, _amount);  function approve (address _spender, uint256 _amount) public nonReentrant returns (bool success_) {  // return shell.approve(_spender, _amount);  function increaseAllowance(address _spender, uint256 _addedValue) public returns (bool success_) {  // return shell.increaseAllowance(_spender, _addedValue);  function decreaseAllowance(address _spender, uint256 _subtractedValue) public returns (bool success_) {  // return shell.decreaseAllowance(_spender, _subtractedValue);  function balanceOf (address _account) public view returns (uint256) {  // return shell.balances[_account];  src/test/deposits/suiteOne.t.sol:L15-L29  // function test_s1_selectiveDeposit_noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_NO_HACK () public logs_gas {  //     uint256 newShells = super.noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD();  //     assertEq(newShells, 32499999216641686631);  // }  // function test_s1_selectiveDeposit_noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_HACK () public logs_gas {  //     uint256 newShells = super.noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_HACK();  //     assertEq(newShells, 32499999216641686631);  // }  src/test/deposits/depositsTemplate.sol:L40-L56  // function noSlippage_balanced_10DAI_10USDC_10USDT_2p5SUSD_HACK () public returns (uint256 shellsMinted_) {  //     uint256 startingShells = l.proportionalDeposit(300e18);  //     uint256 gas = gasleft();  //     shellsMinted_ = l.depositHack(  //         address(dai), 10e18,  //         address(usdc), 10e6,  //         address(usdt), 10e6,  //         address(susd), 2.5e18  //     );  //     emit log_uint(\"gas for deposit\", gas - gasleft());  // }  Recommendation  Remove all the commented out code or transform it into comments.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.12 Should check if the asset already exists when adding a new asset    ", "body": "  Resolution  Comment from the development team:  We have decided not to have dynamic adding/removing of assets in this release.  Description  The public function includeAsset  src/Loihi.sol:L128-L130  function includeAsset (address _numeraire, address _nAssim, address _reserve, address _rAssim, uint256 _weight) public onlyOwner {  shell.includeAsset(_numeraire, _nAssim, _reserve, _rAssim, _weight);  Calls the internal includeAsset implementation  src/Controller.sol:L72  function includeAsset (Shells.Shell storage shell, address _numeraire, address _numeraireAssim, address _reserve, address _reserveAssim, uint256 _weight) internal {  But there is no check to see if the asset already exists in the list. Because the check was not done, shell.numeraires can contain multiple identical instances.  src/Controller.sol:L80  shell.numeraires.push(_numeraireAssimilator);  Recommendation  Check if the _numeraire already exists before invoking includeAsset.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.13 Check return values for both internal and external calls    ", "body": "  Resolution  Comment from the development team:  This doesn t seem feasible. Checking how much was transferred to/from the contract would pose unacceptable gas costs. Because of these constraints, the value returned by the assimilator methods never touches the outside world. They are just converted into numeraire format and returned, so checking these values would not provide any previously unknown information.  Description  There are some cases where functions which return values are called throughout the source code but the return values are not processed, nor checked.  The returns should in principle be handled and checked for validity to provide more robustness to the code.  Examples  The function intakeNumeraire receives a number of tokens and returns how many tokens were transferred to the contract.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L51-L59  // transfers numeraire amount of dai in, wraps it in cDai, returns raw amount  function intakeNumeraire (int128 _amount) public returns (uint256 amount_) {  // truncate stray decimals caused by conversion  amount_ = _amount.mulu(1e18) / 1e3 * 1e3;  dai.transferFrom(msg.sender, address(this), amount_);  Similarly, the function outputNumeraire receives a destination address and an amount of token for withdrawal and returns a number of transferred tokens to the specified address.  src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L83-L92  // takes numeraire amount of dai, unwraps corresponding amount of cDai, transfers that out, returns numeraire amount  function outputNumeraire (address _dst, int128 _amount) public returns (uint256 amount_) {  amount_ = _amount.mulu(1e18);  dai.transfer(_dst, amount_);  return amount_;  However, the results are not handled in the main contract.  src/Loihi.sol:L497  shell.numeraires[i].addr.intakeNumeraire(_shells.mul(shell.weights[i]));  src/Loihi.sol:L509  shell.numeraires[i].addr.intakeNumeraire(_oBals[i].mul(_multiplier));  src/Loihi.sol:L586  shell.reserves[i].addr.outputNumeraire(msg.sender, _oBals[i].mul(_multiplier));  A sanity check can be done to make sure that more than 0 tokens were transferred to the contract.  unit intakeAmount = shell.numeraires[i].addr.intakeNumeraire(_shells.mul(shell.weights[i]));  require(intakeAmount > 0, \"Must intake a positive number of tokens\");  Recommendation  Handle all return values everywhere returns exist and add checks to make sure an expected value was returned.  If the return values are never used, consider not returning them at all.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.14 Interfaces do not need to be implemented for the compiler to access their selectors.    ", "body": "  Resolution  Comment from the development team:  This is the case for the version we used, solc 0.5.15. Versions 0.5.17 and 0.6.* do not require it.  Description  In Assimilators.sol the interface for the assimilators is implemented in a state variable constant as an interface to the zeroth address in order to make use of it s selectors.  src/Assimilators.sol:L37  IAssimilator constant iAsmltr = IAssimilator(address(0));  This pattern is unneeded since you can reference selectors by using the imported interface directly without any implementation. It hinders both gas costs and readability of the code.  Examples  Recommendation  Delete line 37 in Assimilators.sol and instead of getting selectors through:  src/Assimilators.sol:L62  bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, _amt); // for development  use the expression:  IAssimilator.viewRawAmount.selector  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.15 Use consistent interfaces for functions in the same group    ", "body": "  Description  In the file Shells.sol, there also is a library that is being used internally for safe adds and subtractions.  This library has 2 functions.  add which receives 2 arguments, x and y.  src/Shells.sol:L22-L24  function add(uint x, uint y) internal pure returns (uint z) {  require((z = x + y) >= x, \"add-overflow\");  sub which receives 3 arguments x, y and _errorMessage.  src/Shells.sol:L26-L28  function sub(uint x, uint y, string memory _errorMessage) internal pure returns (uint z) {  require((z = x - y) <= x, _errorMessage);  In order to reduce the cognitive load on the auditors and developers alike, somehow-related functions should have coherent logic and interfaces. Both of the functions either need to have 2 arguments, with an implied error message passed to require, or both functions need to have 3 arguments, with an error message that can be specified.  Recommendation  Update the functions to be coherent with other related functions.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.16 Code coverage should be close to 100%    ", "body": "  Resolution  Comment from the development team:  This is true for all aspects of the bonding curve.  Things that have been tested on Kovan with the frontend dapp but could use a unit test are things relevant to sending shell tokens - issuing approvals, transfers and transferfroms.  The adding of assets and assimilators are tested by proxy because they are dependencies for the entire behavior of the bonding surface.  For this release, we plan on having the assets and the assimilators frozen at launch, so there is not much to test regarding continuous updating/changing of assets and assimilators.  We have, however, considered allowing for the dynamic adjustment of weights in addition to parameters.  Description  Code coverage is a measure used to describe how much of the source code is executed during the automated test suite. A system with high code coverage, measured as lines of code executed, has a lower chance to contain undiscovered bugs.  The codebase does not have any information about the code coverage.  Recommendation  Make the test suite output code coverage and add more tests to handle the lines of code that are not touched by any tests.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.17 Consider emitting an event when changing the frozen state of the contract    ", "body": "  Description  The function freeze allows the owner to freeze and unfreeze the contract.  src/Loihi.sol:L144-L146  function freeze (bool _freeze) public onlyOwner {  frozen = _freeze;  The common pattern when doing actions important for the outside of the blockchain is to emit an event when the action is successful.  It s probably a good idea to emit an event stating the contract was frozen or unfrozen.  Recommendation  Create an event that displays the current state of the contract.  event Frozen(bool frozen);  And emit the event when frozen is called.  function freeze (bool _freeze) public onlyOwner {  frozen = _freeze;  emit Frozen(_freeze);  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.18 Function supportsInterface can be restricted to pure    ", "body": "  Description  The function supportsInterface returns a bool stating that the contract supports one of the defined interfaces.  src/Loihi.sol:L140-L142  function supportsInterface (bytes4 interfaceID) public returns (bool) {  return interfaceID == ERC20ID || interfaceID == ERC165ID;  The function does not access or change the state of the contract, this is why it can be restricted to pure.  Recommendation  Restrict the function definition to pure.  function supportsInterface (bytes4 interfaceID) public pure returns (bool) {  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.19 Use more consistent function naming (includeAssimilator / excludeAdapter)    ", "body": "  Description  The function includeAssimilator adds a new assimilator to the list  src/Controller.sol:L98  shell.assimilators[_derivative] = Assimilators.Assimilator(_assimilator, _numeraireAssim.ix);  The function excludeAdapter removes the specified assimilator from the list  src/Loihi.sol:L137  delete shell.assimilators[_assimilator];  Recommendation  Consider renaming the function excludeAdapter to removeAssimilator and moving the logic of adding and removing in the same source file.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/shell-protocol/"}, {"title": "6.1 zDAO Token - Specification violation - Snapshots are never taken   Partially Addressed", "body": "  Resolution  Addressed with zer0-os/zDAO-Token@81946d4 by exposing the _snapshot() method to a dedicated snapshot role (likely to be a DAO) and the owner of the contract.  We would like to note that we informed the client that depending on how the snapshot method is used and how predictably snapshots are consumed this might open up a frontrunning vector where someone observing that a _snapshot() is about to be taken might sandwich the snapshot call, accumulate a lot of stake (via 2nd markets, lending platforms), and returning it right after it s been taken. The risk of losing funds may be rather low (especially if performed by a miner) and the benefit from a DAO proposal using this snapshot might outweigh it. It is still recommended to increase the number of snapshots taken or take them on a regular basis (e.g. with every first transaction to the contract in a block) to make it harder to sandwich the snapshot taking.  Description  According to the zDAO Token specification the DAO token should implement a snapshot functionality to allow it being used for DAO governance votings.  Any transfer, mint, or burn operation should result in a snapshot of the token balances of involved users being taken.  While the corresponding functionality is implemented and appears to update balances for snapshots, _snapshot() is never called, therefore, the snapshot is never taken. e.g. attempting to call balanceOfAt always results in an error as no snapshot is available.  zDAO-Token/contracts/ZeroDAOToken.sol:L12-L17  contract ZeroDAOToken is  OwnableUpgradeable,  ERC20Upgradeable,  ERC20PausableUpgradeable,  ERC20SnapshotUpgradeable  zDAO-Token/contracts/ZeroDAOToken.sol:L83-L83  _updateAccountSnapshot(sender);  Note that this is an explicit requirement as per specification but unit tests do not seem to attempt calls to balanceOfAt at all.  Recommendation  Actually, take a snapshot by calling _snapshot() once per block when executing the first transaction in a new block. Follow the openzeppeling documentation for ERC20Snapshot.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"}, {"title": "6.2 zDAO-Token - Revoking vesting tokens right before cliff period expiration might be delayed/front-runned ", "body": "  Description  The owner of TokenVesting contract has the right to revoke the vesting of tokens for any beneficiary. By doing so, the amount of tokens that are already vested and weren t released yet are being transferred to the beneficiary, and the rest are being transferred to the owner. The beneficiary is expected to receive zero tokens in case the revocation transaction was executed before the cliff period is over. Although unlikely, the beneficiary may front run this revocation transaction by delaying the revocation (and) or inserting a release transaction right before that, thus withdrawing the vested amount.  zDAO-Token/contracts/TokenVesting.sol:L69-L109  function release(address beneficiary) public {  uint256 unreleased = getReleasableAmount(beneficiary);  require(unreleased > 0, \"Nothing to release\");  TokenAward storage award = getTokenAwardStorage(beneficiary);  award.released += unreleased;  targetToken.safeTransfer(beneficiary, unreleased);  emit Released(beneficiary, unreleased);  /**  @notice Allows the owner to revoke the vesting. Tokens already vested  are transfered to the beneficiary, the rest are returned to the owner.  @param beneficiary Who the tokens are being released to  /  function revoke(address beneficiary) public onlyOwner {  TokenAward storage award = getTokenAwardStorage(beneficiary);  require(award.revocable, \"Cannot be revoked\");  require(!award.revoked, \"Already revoked\");  // Figure out how many tokens were owed up until revocation  uint256 unreleased = getReleasableAmount(beneficiary);  award.released += unreleased;  uint256 refund = award.amount - award.released;  // Mark award as revoked  award.revoked = true;  award.amount = award.released;  // Transfer owed vested tokens to beneficiary  targetToken.safeTransfer(beneficiary, unreleased);  // Transfer unvested tokens to owner (revoked amount)  targetToken.safeTransfer(owner(), refund);  emit Released(beneficiary, unreleased);  emit Revoked(beneficiary, refund);  Recommendation  The issue described above is possible, but very unlikely. However, the TokenVesting owner should be aware of that, and make sure not to revoke vested tokens closely to cliff period ending.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"}, {"title": "6.3 zDAO-Token - Vested tokens revocation depends on claiming state ", "body": "  Description  Examples  zDAO-Token/contracts/TokenVesting.sol:L86-L109  function revoke(address beneficiary) public onlyOwner {  TokenAward storage award = getTokenAwardStorage(beneficiary);  require(award.revocable, \"Cannot be revoked\");  require(!award.revoked, \"Already revoked\");  // Figure out how many tokens were owed up until revocation  uint256 unreleased = getReleasableAmount(beneficiary);  award.released += unreleased;  uint256 refund = award.amount - award.released;  // Mark award as revoked  award.revoked = true;  award.amount = award.released;  // Transfer owed vested tokens to beneficiary  targetToken.safeTransfer(beneficiary, unreleased);  // Transfer unvested tokens to owner (revoked amount)  targetToken.safeTransfer(owner(), refund);  emit Released(beneficiary, unreleased);  emit Revoked(beneficiary, refund);  Recommendation  Make sure that the potential owner of a TokenVesting contract is aware of this potential issue, and has the required processes in place to handle it.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"}, {"title": "6.4 zDAO-Token - Total amount of claimable tokens is not verifiable    ", "body": "  Description  Since both MerkleTokenVesting and MerkleTokenAirdrop use an off-chain Merkle tree to store the accounts that can claim tokens from the underlying contract, there is no way for a user to verify whether the contract token balance is sufficient for all claimers.  Recommendation  Make sure that users are aware of this trust assumption.  7 Document Change Log  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"}, {"title": "1.0", "body": "  2021-05-20  Initial report  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"}, {"title": "1.1", "body": "  2021-08-23  Update: added section 3 - WILD Token  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zdao-token/"}, {"title": "6.1 Staking node can be inappropriately removed from the tree    ", "body": "  Resolution   This is fixed in   OrchidProtocol/orchid@8c586f2.  Description  The following code in OrchidDirectory.pull() is responsible for reattaching a child from a removed tree node:  code/dir-ethereum/directory.sol:L275-L281  if (name(stake.left_) == key) {  current.right_ = stake.right_;  current.after_ = stake.after_;  } else {  current.left_ = stake.left_;  current.before_ = stake.before_;  The condition name(stake.left_) == key can never hold because key is the key for stake itself.  The result of this bug is somewhat catastrophic. The child is not reattached, but it still has a link to the rest of the tree via its  parent_  pointer. This means reducing the stake of that node can underflow the ancestors  before/after amounts, leading to improper random selection or failing altogether.  The node replacing the removed node also ends up with itself as a child, which violates the basic tree structure and is again likely to produce integer underflows and other failures.  Recommendation  As a simple fix, use if(name(stake.left_) == name(last)) as already suggested by the development team when this bug was first shared.  Two suggestions for better long-term fixes:  Use a strict interface for tree operations. It should be impossible to update a node s parent without simultaneously updating that parent s child pointer.  As suggested in (https://github.com/ConsenSys/orchid-audit-2019-10/issues/7), simplify the logic in pull() to avoid this logic altogether.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.2 Verifiers need to be pure, but it s very difficult to validate pureness    ", "body": "  Resolution  This is addressed in OrchidProtocol/orchid@1b405fb. With this change, the contract checks that the verifier s code doesn t change (via extcodehash). If the code does change, the contract  fails open  by skipping the verifier and allowing all payments.  Because the code can no longer change, the server can use the (relatively) simple method of executing the contract locally and only allowing a whitelist of opcodes that don t depend on or modify state.  The server already has mitigations for denial of service attacks, including limiting the amount of computing resources that can be used for validating code.  Description  After the initial audit, a  verifier  was introduced to the OrchidLottery code. Each Pot can have an associated OrchidVerifier. This is a contract with a good() function that accepts three parameters:  code/lot-ethereum/lottery.sol:L28  function good(bytes calldata shared, address target, bytes calldata receipt) external pure returns (bool);  The verifier returns a boolean indicating whether a given micropayment should be allowed or not. An example use case is a verifier that only allows certain target addresses to be paid. In this case, shared (a single value for a given Pot) is a merkle root, target is (as always) the address being paid, and receipt (specified by the payment recipient) is a merkle proof that the target address is within the merkle tree with the given root.  Unfortunately, this simple scheme is insufficient. As a simple example, a verifier contract could be created with the CREATE2 opcode. It could be demonstrated that it reads no state when good() is called. Then the contract could be destroyed by calling a function that performs a SELFDESTRUCT, and it could be replaced via another CREATE2 call with different code.  This could be mitigated by rejecting any verifier contract that contains the SELFDESTRUCT opcode, but this would also catch harmless occurrences of that particular byte. https://gist.github.com/Arachnid/e8f0638dc9f5687ff8170a95c47eac1e attempts to find SELFDESTRUCT opcodes but fails to account for tricks where the SELFDESTRUCT appears to be data but can actually be executed. (See Recmo s comment.) In general, this approach is difficult to get right and probably requires full data flow analysis to be correct.  Another possible mitigation is to use a factory contract to deploy the verifiers, guaranteeing that they re not created with CREATE2. This should render SELFDESTRUCT harmless, but there s no guarantee that future forks won t introduce new vectors here.  Finally, requiring servers to implement potentially complex contract validation opens up potential for denial-of-service attacks. A server will have to implement mitigations to prevent repeatedly checking the same verifier or spending inordinate resources checking a maliciously crafted contract (e.g. one with high branching factors).  Recommendation  The verifiers add quite a bit of complexity and risk. We recommend looking for an alternative approach, such as including a small number of vetted verifiers (e.g. a merkle proof verifier) or having servers use their own  allow list  for verifiers that they trust.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.3 Simplify the logic in OrchidDirectory.pull()    ", "body": "  Resolution  This was addressed in the following commits:  OrchidProtocol/orchid@0ad2484  OrchidProtocol/orchid@8b3e821  OrchidProtocol/orchid@affbf93  OrchidProtocol/orchid@e506c0f  OrchidProtocol/orchid@f864e60  Description  pull() is the most complex function in OrchidDirectory, due to its need to handle removing a node altogether when its stake amount reaches 0.  The current logic for removing an interior node is roughly this:  Given a node to be remove called old, walk down the tree, always stepping towards the  heavier  (in terms of total stake) subtree, until you reach a leaf node (called target).  If target is a direct child of old:  Set target to be a child of old.parent. Move the remaining child of old to be under target.  If target is not a direct child of old:  Swap target and old in the tree. Walk up the tree from old (now a leaf node) to target to subtract target s staked amount from the nodes in between. Detach old from the tree.  The code for this is fairly complex, and one serious bug (issue 6.1) was identified in this code.  This logic can be simplified by combining the two cases (direct child and not) and thinking of it as roughly a two-step operation of  detach leaf node  and  replace interior node with leaf node .  Given a node to be removed called old, walk the tree to find target as before.  Walk back up to old, subtracting target s staked amount from the nodes in between.  Detach target from the tree.  Replace old with target.  (Note that in the code,  old  above is called stake and  target  is calledcurrent.)  Recommendation  Replace this code:  code/dir-ethereum/directory.sol:L266-L297  bytes32 direct = current.parent_;  copy(pivot, last);  current.parent_ = stake.parent_;  if (direct == key) {  Primary storage other = stake.before_ > stake.after_ ? stake.right_ : stake.left_;  if (!nope(other))  stakes_[name(other)].parent_ = name(last);  if (name(stake.left_) == key) {  current.right_ = stake.right_;  current.after_ = stake.after_;  } else {  current.left_ = stake.left_;  current.before_ = stake.before_;  } else {  if (!nope(stake.left_))  stakes_[name(stake.left_)].parent_ = name(last);  if (!nope(stake.right_))  stakes_[name(stake.right_)].parent_ = name(last);  current.right_ = stake.right_;  current.after_ = stake.after_;  current.left_ = stake.left_;  current.before_ = stake.before_;  stake.parent_ = direct;  copy(last, staker, stakee);  step(key, stake, -current.amount_, current.parent_);  kill(last);  with something like this code:  // Remember this key so we can update `pivot` later  bytes32 currentKey = name(last);  // Remove `current` from the subtree rooted at `stake`  step(currentKey, current, -current.amount_, stake.parent_);  kill(last);  // Replace `stake` with `current`  current.left_ = stake.left_;  if (!nope(current.left_))  stakes_[name(current.left_)].parent_ = currentKey;  current.right_ = stake.right_;  if (!nope(current.right_))  stakes_[name(current.right_)].parent_ = currentKey;  current.before_ = stake.before_;  current.after_ = stake.after_;  current.parent_ = stake.parent_;  pivot.value_ = currentKey; // `pivot` was parent's pointer to `stake`  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.4 Remove unnecessary address payable   ", "body": "  Resolution   The development team decided to leave this as-is.   Description  The address payable type is only needed for transferring ether to an address. The OrchidDirectory and OrchidLottery contracts work with tokens, not ether, so there s no need for any parameters to be of type address payable.  Recommendation  Use simply address instead of address payable everywhere.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.5 Use consistent staker, stakee ordering in OrchidDirectory    ", "body": "  Resolution   This is fixed in   OrchidProtocol/orchid@1cfef88.  Description  code/dir-ethereum/directory.sol:L156  function lift(bytes32 key, Stake storage stake, uint128 amount, address stakee, address staker) private {  OrchidDirectory.lift() has a parameter stakee that precedes staker, while the rest of the code always places staker first. Because Solidity doesn t have named parameters, it s a good idea to use a consistent ordering to avoid mistakes.  Recommendation  Switch lift() to follow the  staker then stakee  ordering convention of the rest of the contract.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.6 Use more descriptive function and variable names   ", "body": "  Resolution  This issue is about readability. Even though the audit team firmly believes that improved readability would increase trust in Orchid from its clients, this is not a correctness issue.  The Orchid team believes that making this change, particularly this late in their development cycle, would be too risky. The development team is very familiar with the current terminology, and bugs may accidentally be introduced with the change.  Description  Throughout OrchidDirectory and OrchidLottery, function and variable names are quite obscure. This makes it harder for a reader to understand the code.  Examples  OrchidDirectory:  heft() returns the total staked for a given stakee (perhaps totalForStakee()) Primary is a pointer to a tree node (perhaps NodePointer), and its member value_ could be named key name() gives the key for a given (staker, stakee) pair or a Primary (perhaps getKey()) copy() writes a key to a node pointer (probably better to remove this and just do pointer.key = ...) kill() sets a node pointer to zero (probably better to just remove this and use delete pointer) nope() checks whether a node pointer exists (probably better to just do pointer.key == 0) have() returns the total number of staked tokens (perhaps totalStaked) scan() finds a node, given a random 128-bit number (perhaps selectNode(uint128 random)) turn() is only used in one place and is likely better just inlined step() walks up a subtree, adjusting before/after amounts along the way (perhaps propagate() or bubbleUp()) lift() updates the stake for a given node and then calls step() (perhaps updateNodeStake()) more() is really just the body for push(), so it should probably be moved inside push() instead push() is the external method for staking (perhaps increaseStake() or just stake()) wait() increases the withdrawal delay for the sender s stake for a given stakee (increaseDelay()) Pending could be called PendingWithdrawal take() could be called completeWithdrawal() stop() could be called cancelWithdrawal() delay_ could be withdrawalDelay pull() decreases stake and establishes a pending withdrawal (perhaps decreaseStake(), unstake() or startWithdrawal()) Within pull():  pivot could be pointerToStake last could be pointerToLeaf current could be leaf direct could be leafParent other could be sibling  OrchidLottery:  Pot could perhaps be Fund send() just emits an Update event (perhaps log() or logUpdate()) Track is a struct that keeps track of a ticket that has already been redeemed to prevent replay (perhaps RedeemedTicket) kill() is overloaded to delete funds and used tickets (perhaps deleteFund() and forgetTicket()) take() could be called transferTokens() grab() redeems a winning ticket (perhaps redeem() or redeemTicket()) give() and pull() both transfer tokens from a given Pot, but one is used by the signer and one by the funder. Perhaps better would be a single transferFromPot(address funder, address signer, address target, uint128 amount) with require(msg.sender == funder || msg.sender == signer). warn() could be startWithdrawal() lock() could be cancelWithdrawal() pull() could be completeWithdrawal()  Recommendation  Consider using longer, more descriptive names to make it easier to understand the code. Where there s no particularly good name, add comments explaining the meaning.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.7 In OrchidDirectory.step() and OrchidDirectory.lift(), use a signed amount   ", "body": "  Resolution   The variables in question are now   Description  step() and lift() both accept a uint128 parameter called amount. This amount is added to various struct fields, which are also of type uint128.  The contract intentionally underflows this amount to represent negative numbers. This is roughly equivalent to using a signed integer, except that:  Unsigned integers aren t sign extended when they re cast to a larger integer type, so care must be taken to avoid this.  Tools that look for integer overflow/underflow will detect this possibility as a bug. It s then hard to determine which overflows are intentional and which are not.  Examples  code/dir-ethereum/directory.sol:L247  lift(key, stake, -amount, stakee, staker);  code/dir-ethereum/directory.sol:L296  step(key, stake, -current.amount_, current.parent_);  Recommendation  Use int128 instead, and ensure that amounts can never exceed the maximum int128 value. (This is trivially achieved by limiting the total number of tokens that can exist.)  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.8 Document that math in OrchidDirectory assumes a maximum number of tokens    ", "body": "  Resolution   This is fixed in   OrchidProtocol/orchid@f2efe42 by using  Description  OrchidDirectory relies on mathematical operations being unable to overflow due to the particular ERC20 token being used being capped at less than 2**128.  Examples  The following code in step() assumes that no before/after amount can reach 2**128:  code/dir-ethereum/directory.sol:L145-L148  if (name(stake.left_) == key)  stake.before_ += amount;  else  stake.after_ += amount;  The following code in lift() assumes that no staked amount (or total amount for a given stakee) can reach 2**128:  code/dir-ethereum/directory.sol:L157-L164  uint128 local = stake.amount_;  local += amount;  stake.amount_ = local;  emit Update(staker, stakee, local);  uint128 global = stakees_[stakee].amount_;  global += amount;  stakees_[stakee].amount_ = global;  The following code in have() assumes that the total amount staked cannot reach 2**128:  code/dir-ethereum/directory.sol:L103  return stake.before_ + stake.after_ + stake.amount_;  Recommendation  Document this assumption in the form of code comments where potential overflows exist.  Consider also asserting the ERC20 token s total supply in the constructor to attempt to block using a token that violates this constraint and/or checking in push() that the total amount staked will remain less than 2**128. This recommendation is in line with the mitigation proposed for issue 6.7.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.9 Unneeded named return parameter    ", "body": "  Resolution   Fixed in   OrchidProtocol/orchid@21d56d5  Description  In the heft function in the OrchidDirectory contract, there is an unused and unneeded named return parameter (that actually instantiates a new variable in memory which is not used).  Remediation  Change returns (uint128 amount) to returns (uint128).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "6.10 Improve function visibility    ", "body": "  Resolution   Fixed in   OrchidProtocol/orchid@68fb26a  Description  The following methods are not called internally in the token contract and visibility can, therefore, be restricted to external rather than public. This is more gas efficient because less code is emitted and data does not need to be copied into memory. It also makes functions a bit simpler to reason about because there s no need to worry about the possibility of internal calls.  OrchidDirectory.heft()  OrchidDirectory.scan()  OrchidDirectory.push()  OrchidDirectory.wait()  OrchidDirectory.take()  OrchidDirectory.stop()  OrchidDirectory.pull()  OrchidLocation.move()  OrchidLocation.look()  OrchidLottery.size()  OrchidLottery.keys()  OrchidLottery.seek()  OrchidLottery.look()  OrchidLottery.push()  OrchidLottery.move()  OrchidLottery.kill()  OrchidLottery.grab()  OrchidLottery.pull()  OrchidLottery.warn()  OrchidLottery.lock()  OrchidLottery.pull()  OrchidCurator.list()  OrchidCurator.good()  OrchidUntrusted.good()  Recommendation  Change visibility of these methods to external.  7 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "7.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of the MythX Pro vulnerability scan was reviewed by the audit team and no vulnerabilities were identified as part of the process.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "7.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/curator.sol  35:8    warning    Provide an error message for require().    error-reason  contracts/directory.sol  107:8     warning    Provide an error message for require().            error-reason  141:4     error      \"step\": Avoid assigning to function parameters.    security/no-assign-params  141:4     error      \"step\": Avoid assigning to function parameters.    security/no-assign-params  176:8     warning    Provide an error message for require().            error-reason  180:12    warning    Provide an error message for require().            error-reason  202:8     warning    Provide an error message for require().            error-reason  209:8     warning    Provide an error message for require().            error-reason  211:8     warning    Provide an error message for require().            error-reason  226:8     warning    Provide an error message for require().            error-reason  226:35    warning    Avoid using 'block.timestamp'.                     security/no-block-members  228:8     warning    Provide an error message for require().            error-reason  233:8     warning    Provide an error message for require().            error-reason  233:35    warning    Avoid using 'block.timestamp'.                     security/no-block-members  244:8     warning    Provide an error message for require().            error-reason  245:8     warning    Provide an error message for require().            error-reason  305:8     warning    Provide an error message for require().            error-reason  306:26    warning    Avoid using 'block.timestamp'.                     security/no-block-members  contracts/location.sol  38:24    warning    Avoid using 'block.timestamp'.    security/no-block-members  contracts/lottery.sol  66:8      warning    Provide an error message for require().            error-reason  104:8     warning    Provide an error message for require().            error-reason  111:8     warning    Provide an error message for require().            error-reason  117:8     warning    Provide an error message for require().            error-reason  131:8     warning    Provide an error message for require().            error-reason  131:32    warning    Avoid using 'block.timestamp'.                     security/no-block-members  140:4     error      \"take\": Avoid assigning to function parameters.    security/no-assign-params  153:12    warning    Provide an error message for require().            error-reason  156:4     error      \"grab\": Avoid assigning to function parameters.    security/no-assign-params  156:4     warning    Line exceeds the limit of 145 characters           max-len  157:8     warning    Provide an error message for require().            error-reason  158:8     warning    Provide an error message for require().            error-reason  163:12    error      Only use indent of 8 spaces.                       indentation  165:12    error      Only use indent of 8 spaces.                       indentation  166:12    error      Only use indent of 8 spaces.                       indentation  167:12    error      Only use indent of 8 spaces.                       indentation  167:12    warning    Provide an error message for require().            error-reason  167:28    warning    Avoid using 'block.timestamp'.                     security/no-block-members  168:12    error      Only use indent of 8 spaces.                       indentation  168:12    warning    Provide an error message for require().            error-reason  169:12    error      Only use indent of 8 spaces.                       indentation  171:12    error      Only use indent of 8 spaces.                       indentation  172:0     error      Only use indent of 8 spaces.                       indentation  175:20    warning    Avoid using 'block.timestamp'.                     security/no-block-members  176:64    warning    Avoid using 'block.timestamp'.                     security/no-block-members  182:8     warning    Provide an error message for require().            error-reason  200:22    warning    Avoid using 'block.timestamp'.                     security/no-block-members  214:8     warning    Provide an error message for require().            error-reason  215:8     warning    Provide an error message for require().            error-reason  215:31    warning    Avoid using 'block.timestamp'.                     security/no-block-members  219:8     warning    Provide an error message for require().            error-reason  \u2716 12 errors, 38 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "7.3 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  8 S\u016brya s Description Report  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "8.1 Files Description Table", "body": "  contracts/curator.sol  5ea6c8374cec289bcf4dfe1adb3bb157ca9bab74  contracts/directory.sol  811ab3b049d570c4236ee965d76ed1a9f5cb929e  contracts/location.sol  1ea56960f41ca3a299c4fd35fab9ef1fdd494d5b  contracts/lottery.sol  e63f3c86b3abba57d0a7e3ca36436bfee4d9ac1b  contracts/token.sol  faf15f117ac160641adfe56c2a01ad14bff931f3  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "8.2 Contracts Description Table", "body": "  Function Name  Visibility  Mutability  Modifiers  OrchidCurator  Implementation  <Constructor>  Public    list  Public    NO   good  Public    NO   OrchidUntrusted  Implementation  good  Public    NO   IOrchidDirectory  Interface  have  External    NO   OrchidDirectory  Implementation  IOrchidDirectory  <Constructor>  Public    heft  Public    NO   name  Public    NO   name  Private \ud83d\udd10  copy  Private \ud83d\udd10  copy  Private \ud83d\udd10  kill  Private \ud83d\udd10  nope  Private \ud83d\udd10  have  Public    NO   scan  Public    NO   turn  Private \ud83d\udd10  step  Private \ud83d\udd10  lift  Private \ud83d\udd10  more  Private \ud83d\udd10  push  Public    NO   wait  Public    NO   take  Public    NO   stop  Public    NO   pull  Public    NO   OrchidLocation  Implementation  move  Public    NO   look  Public    NO   OrchidLottery  Implementation  <Constructor>  Public    send  Private \ud83d\udd10  find  Private \ud83d\udd10  kill  Private \ud83d\udd10  size  Public    NO   keys  Public    NO   seek  Public    NO   page  Public    NO   look  Public    NO   push  Public    NO   move  Public    NO   kill  Private \ud83d\udd10  kill  Public    NO   take  Private \ud83d\udd10  grab  Public    NO   give  Public    NO   pull  Public    NO   warn  Public    NO   lock  Public    NO   pull  Public    NO   OrchidToken  Implementation  ERC20, ERC20Detailed  <Constructor>  Public    ERC20Detailed  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "8.3 Legend", "body": "  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/orchid-network-protocol/"}, {"title": "5.1 zBanc - DynamicLiquidTokenConverter ineffective reentrancy protection    ", "body": "  Resolution   Fixed with   zer0-os/zBanc@ff3d913 by following the recommendation.  Description  reduceWeight calls _protected() in an attempt to protect from reentrant calls but this check is insufficient as it will only check for the locked statevar but never set it. A potential for direct reentrancy might be present when an erc-777 token is used as reserve.  It is assumed that the developer actually wanted to use the protected modifier that sets the lock before continuing with the method.  Examples  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L123-L128  function reduceWeight(IERC20Token _reserveToken)  public  validReserve(_reserveToken)  ownerOnly  _protected();  contract ReentrancyGuard {  // true while protected code is being executed, false otherwise  bool private locked = false;  /**  @dev ensures instantiation only by sub-contracts  /  constructor() internal {}  // protects a function against reentrancy attacks  modifier protected() {  _protected();  locked = true;  _;  locked = false;  // error message binary size optimization  function _protected() internal view {  require(!locked, \"ERR_REENTRANCY\");  Recommendation  To mitigate potential attack vectors from reentrant calls remove the call to _protected() and decorate the function with protected instead. This will properly set the lock before executing the function body rejecting reentrant calls.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.2 zBanc - DynamicLiquidTokenConverter input validation    ", "body": "  Resolution   fixed with   zer0-os/zBanc@ff3d913 by checking that the provided values are at least 0% < p <= 100%.  Description  Check that the value in PPM is within expected bounds before updating system settings that may lead to functionality not working correctly. For example, setting out-of-bounds values for stepWeight or setMinimumWeight may make calls to reduceWeight fail. These values are usually set in the beginning of the lifecycle of the contract and misconfiguration may stay unnoticed until trying to reduce the weights. The settings can be fixed, however, by setting the contract inactive and updating it with valid settings. Setting the contract to inactive may temporarily interrupt the normal operation of the contract which may be unfavorable.  Examples  Both functions allow the full uint32 range to be used, which, interpreted as PPM would range from 0% to 4.294,967295%  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L75-L84  function setMinimumWeight(uint32 _minimumWeight)  public  ownerOnly  inactive  //require(_minimumWeight > 0, \"Min weight 0\");  //_validReserveWeight(_minimumWeight);  minimumWeight = _minimumWeight;  emit MinimumWeightUpdated(_minimumWeight);  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L92-L101  function setStepWeight(uint32 _stepWeight)  public  ownerOnly  inactive  //require(_stepWeight > 0, \"Step weight 0\");  //_validReserveWeight(_stepWeight);  stepWeight = _stepWeight;  emit StepWeightUpdated(_stepWeight);  Recommendation  Reintroduce the checks for _validReserveWeight to check that a percent value denoted in PPM is within valid bounds _weight > 0 && _weight <= PPM_RESOLUTION. There is no need to separately check for the value to be >0 as this is already ensured by _validReserveWeight.  Note that there is still room for misconfiguration (step size too high, min-step too high), however, this would at least allow to catch obviously wrong and often erroneously passed parameters early.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.3 zBanc - DynamicLiquidTokenConverter introduces breaking changes to the underlying bancorprotocol base    ", "body": "  Resolution  Addressed with zer0-os/zBanc@ff3d913 by removing the modifications in favor of surgical and more simple changes, keeping the factory and upgrade components as close as possible to the forked bancor contracts.  Additionally, the client provided the following statement:  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.14 Removed excess functionality from factory and restored the bancor factory pattern.", "body": "  Description  Introducing major changes to the complex underlying smart contract system that zBanc was forked from(bancorprotocol) may result in unnecessary complexity to be added. Complexity usually increases the attack surface and potentially introduces software misbehavior. Therefore, it is recommended to focus on reducing the changes to the base system as much as possible and comply with the interfaces and processes of the system instead of introducing diverging behavior.  For example, DynamicLiquidTokenConverterFactory does not implement the ITypedConverterFactory while other converters do. Furthermore, this interface and the behavior may be expected to only perform certain tasks e.g. when called during an upgrade process. Not adhering to the base systems expectations may result in parts of the system failing to function for the new convertertype. Changes introduced to accommodate the custom behavior/interfaces may result in parts of the system failing to operate with existing converters. This risk is best to be avoided.  In the case of DynamicLiquidTokenConverterFactory the interface is imported but not implemented at all (unused import). The reason for this is likely because the function createConverter in DynamicLiquidTokenConverterFactory does not adhere to the bancor-provided interface anymore as it is doing way more than  just  creating and returning a new converter. This can create problems when trying to upgrade the converter as the upgraded expected the shared interface to be exposed unless the update mechanisms are modified as well.  In general, the factories createConverter method appears to perform more tasks than comparable type factories. It is questionable if this is needed but may be required by the design of the system. We would, however, highly recommend to not diverge from how other converters are instantiated unless it is required to provide additional security guarantees (i.e. the token was instantiated by the factory and is therefore trusted).  The ConverterUpgrader changed in a way that it now can only work with the DynamicLiquidTokenconverter instead of the more generalized IConverter interface. This probably breaks the update for all other converter types in the system.  The severity is estimated to be medium based on the fact that the development team seems to be aware of the breaking changes but the direction of the design of the system was not yet decided.  Examples  unused import  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L6-L6  import \"../../interfaces/ITypedConverterFactory.sol\";  converterType should be external as it is not called from within the same or inherited contracts  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L144-L146  function converterType() public pure returns (uint16) {  return 3;  createToken can be external and is actually creating a token and converter that is using that token (the converter is not returned)(consider renaming to createTokenAndConverter)  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L54-L74  DSToken token = new DSToken(_name, _symbol, _decimals);  token.issue(msg.sender, _initialSupply);  emit NewToken(token);  createConverter(  token,  _reserveToken,  _reserveWeight,  _reserveBalance,  _registry,  _maxConversionFee,  _minimumWeight,  _stepWeight,  _marketCapThreshold  );  return token;  the upgrade interface changed and now requires the converter to be a DynamicLiquidTokenConverter. Other converters may potentially fail to upgrade unless they implement the called interfaces.  zBanc/solidity/contracts/converter/ConverterUpgrader.sol:L96-L122  function upgradeOld(DynamicLiquidTokenConverter _converter, bytes32 _version) public {  _version;  DynamicLiquidTokenConverter converter = DynamicLiquidTokenConverter(_converter);  address prevOwner = converter.owner();  acceptConverterOwnership(converter);  DynamicLiquidTokenConverter newConverter = createConverter(converter);  copyReserves(converter, newConverter);  copyConversionFee(converter, newConverter);  transferReserveBalances(converter, newConverter);  IConverterAnchor anchor = converter.token();  // get the activation status before it's being invalidated  bool activate = isV28OrHigherConverter(converter) && converter.isActive();  if (anchor.owner() == address(converter)) {  converter.transferTokenOwnership(address(newConverter));  newConverter.acceptAnchorOwnership();  handleTypeSpecificData(converter, newConverter, activate);  converter.transferOwnership(prevOwner);  newConverter.transferOwnership(prevOwner);  emit ConverterUpgrade(address(converter), address(newConverter));  solidity/contracts/converter/ConverterUpgrader.sol:L95-L101  function upgradeOld(  IConverter _converter,  bytes32 /* _version */  ) public {  // the upgrader doesn't require the version for older converters  upgrade(_converter, 0);  Recommendation  It is a fundamental design decision to either follow the bancorsystems converter API or diverge into a more customized system with a different design, functionality, or even security assumptions. From the current documentation, it is unclear which way the development team wants to go.  However, we highly recommend re-evaluating whether the newly introduced type and components should comply with the bancor API (recommended; avoid unnecessary changes to the underlying system,) instead of changing the API for the new components. Decide if the new factory should adhere to the usually commonly shared ITypedConverterFactory (recommended) and if not, remove the import and provide a new custom shared interface. It is highly recommended to comply and use the bancor systems extensibility mechanisms as intended, keeping the previously audited bancor code in-tact and voiding unnecessary re-assessments of the security impact of changes.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.4 zBanc - DynamicLiquidTokenConverter isActive should only be returned if converter is fully configured and converter parameters should only be updateable while converter is inactive    ", "body": "  Resolution  Addressed with zer0-os/zBanc@ff3d913 by removing the custom ACL modifier falling back to checking whether the contract is configured (isActive, inactive modifiers). When a new contract is deployed it will be inactive until the main vars are set by the owner (upgrade contract). The upgrade path is now aligned with how the LiquidityPoolV2Converter performs upgrades.  Additionally, the client provided the following statement:  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.13 - upgrade path resolved - inactive modifier back on the setters, and upgrade path now mirrors lpv2 path. An important note here is that lastWeightAdjustmentMarketCap setting isn t included in the inActive() override, since it has a valid state of 0. So it must be set before the others settings, or it will revert as inactive", "body": "  Description  By default, a converter is active once the anchor ownership was transferred. This is true for converters that do not require to be properly set up with additional parameters before they can be used.  zBanc/solidity/contracts/converter/ConverterBase.sol:L272-L279  /**  @dev returns true if the converter is active, false otherwise  @return true if the converter is active, false otherwise  /  function isActive() public view virtual override returns (bool) {  return anchor.owner() == address(this);  For a simple converter, this might be sufficient. If a converter requires additional setup steps (e.g. setting certain internal variables, an oracle, limits, etc.) it should return inactive until the setup completes. This is to avoid that users are interacting with (or even pot. frontrunning) a partially configured converter as this may have unexpected outcomes.  For example, the LiquidityPoolV2Converter overrides the isActive method to require additional variables be set (oracle) to actually be in active state.  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L79-L85  Additionally, settings can only be updated while the contract is inactive which will be the case during an upgrade. This ensures that the owner cannot adjust settings at will for an active contract.  @dev returns true if the converter is active, false otherwise  @return true if the converter is active, false otherwise  /  function isActive() public view override returns (bool) {  return super.isActive() && address(priceOracle) != address(0);  Additionally, settings can only be updated while the contract is inactive which will be the case during an upgrade. This ensures that the owner cannot adjust settings at will for an active contract.  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L97-L109  function activate(  IERC20Token _primaryReserveToken,  IChainlinkPriceOracle _primaryReserveOracle,  IChainlinkPriceOracle _secondaryReserveOracle)  public  inactive  ownerOnly  validReserve(_primaryReserveToken)  notThis(address(_primaryReserveOracle))  notThis(address(_secondaryReserveOracle))  validAddress(address(_primaryReserveOracle))  validAddress(address(_secondaryReserveOracle))  The DynamicLiquidTokenConverter is following a different approach. It inherits the default isActive which sets the contract active right after anchor ownership is transferred. This kind of breaks the upgrade process for DynamicLiquidTokenConverter as settings cannot be updated while the contract is active (as anchor ownership might be transferred before updating values). To unbreak this behavior a new authentication modifier was added, that allows updates for the upgrade contradict while the contract is active. Now this is a behavior that should be avoided as settings should be predictable while a contract is active. Instead it would make more sense initially set all the custom settings of the converter to zero (uninitialized) and require them to be set and only the return the contract as active. The behavior basically mirrors the upgrade process of LiquidityPoolV2Converter.  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L44-L50  modifier ifActiveOnlyUpgrader(){  if(isActive()){  require(owner == addressOf(CONVERTER_UPGRADER), \"ERR_ACTIVE_NOTUPGRADER\");  _;  Pre initialized variables should be avoided. The marketcap threshold can only be set by the calling entity as it may be very different depending on the type of reserve (eth, token).  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L17-L20  uint32 public minimumWeight = 30000;  uint32 public stepWeight = 10000;  uint256 public marketCapThreshold = 10000 ether;  uint256 public lastWeightAdjustmentMarketCap = 0;  Here s one of the setter functions that can be called while the contract is active (only by the upgrader contract but changing the ACL commonly followed with other converters).  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L67-L74  function setMarketCapThreshold(uint256 _marketCapThreshold)  public  ownerOnly  ifActiveOnlyUpgrader  marketCapThreshold = _marketCapThreshold;  emit MarketCapThresholdUpdated(_marketCapThreshold);  Recommendation  Align the upgrade process as much as possible to how LiquidityPoolV2Converter performs it. Comply with the bancor API.  override isActive and require the contracts main variables to be set.  do not pre initialize the contracts settings to  some  values. Require them to be set by the caller (and perform input validation)  mirror the upgrade process of LiquidityPoolV2Converter and instead of activate call the setter functions that set the variables. After setting the last var and anchor ownership been transferred, the contract should return active.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.5 zBanc - DynamicLiquidTokenConverter frontrunner can grief owner when calling reduceWeight   ", "body": "  Resolution  The client acknowledged this issue by providing the following statement:  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.12 - admin by a DAO will mitigate the owner risks here", "body": "  Description  The owner of the converter is allowed to reduce the converters weights once the marketcap surpasses a configured threshhold. The thresshold is configured on first deployment. The marketcap at the beginning of the call is calculated as reserveBalance / reserve.weight and stored as lastWeightAdjustmentMarketCap after reducing the weight.  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L130-L138  function reduceWeight(IERC20Token _reserveToken)  public  validReserve(_reserveToken)  ownerOnly  _protected();  uint256 currentMarketCap = getMarketCap(_reserveToken);  require(currentMarketCap > (lastWeightAdjustmentMarketCap.add(marketCapThreshold)), \"ERR_MARKET_CAP_BELOW_THRESHOLD\");  The reserveBalance can be manipulated by buying (adding reserve token) or selling liquidity tokens (removing reserve token). The success of a call to reduceWeight is highly dependant on the marketcap. A malicious actor may, therefore, attempt to grief calls made by the owner by sandwiching them with buy and sell calls in an attempt to (a) raise the barrier for the next valid payout marketcap or (b) temporarily lower the marketcap if they are a major token holder in an attempt to fail the reduceWeights call.  In both cases the griefer may incur some losses due to conversion errors, bancor fees if they are set, and gas spent. It is, therefore, unlikely that a third party may spend funds on these kinds of activities. However, the owner as a potential major liquid token holder may use this to their own benefit by artificially lowering the marketcap to the absolute minimum (old+threshold) by selling liquidity and buying it back right after reducing weights.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.6 zBanc - outdated fork   ", "body": "  Description  According to the client the system was forked off bancor v0.6.18 (Oct 2020). The current version 0.6.x is v0.6.36 (Apr 2021).  Recommendation  It is recommended to check if relevant security fixes were released after v0.6.18 and it should be considered to rebase with the current stable release.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.7 zBanc - inconsistent DynamicContractRegistry, admin risks    ", "body": "  Resolution  The client acknowledged the admin risk and addressed the itemCount concerns by exposing another method that only returns the overridden entries. The following statement was provided:  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.10 - keeping this pattern which matches the bancor pattern, and noting the DCR should be owned by a DAO, which is our plan. solved itemCount issue - Added dcrItemCount and made itemCount call the bancor registry s itemCount, so unpredictable behavior due to the count should be eliminated.", "body": "  Description  DynamicContractRegistry is a wrapper registry that allows the zBanc to use the custom upgrader contract while still providing access to the normal bancor registry.  For this to work, the registry owner can add or override any registry setting. Settings that don t exist in this contract are attempted to be retrieved from an underlying registry (contractRegistry).  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L66-L70  function registerAddress(bytes32 _contractName, address _contractAddress)  public  ownerOnly  validAddress(_contractAddress)  If the item does not exist in the registry, the request is forwarded to the underlying registry.  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L52-L58  function addressOf(bytes32 _contractName) public view override returns (address) {  if(items[_contractName].contractAddress != address(0)){  return items[_contractName].contractAddress;  }else{  return contractRegistry.addressOf(_contractName);  According to the documentation this registry is owned by zer0 admins and this means users have to trust zer0 admins to play fair.  To handle this, we deploy our own ConverterUpgrader and ContractRegistry owned by zer0 admins who can register new addresses  The owner of the registry (zer0 admins) can change the underlying registry contract at will. The owner can also add new or override any settings that already exist in the underlying registry. This may for example allow a malicious owner to change the upgrader contract in an attempt to potentially steal funds from a token converter or upgrade to a new malicious contract. The owner can also front-run registry calls changing registry settings and thus influencing the outcome. Such an event will not go unnoticed as events are emitted.  It should also be noted that itemCount will return only the number of items in the wrapper registry but not the number of items in the underlying registry. This may have an unpredictable effect on components consuming this information.  zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L36-L43  /**  @dev returns the number of items in the registry  @return number of items  /  function itemCount() public view returns (uint256) {  return contractNames.length;  Recommendation  Require the owner/zer0 admins to be a DAO or multisig and enforce 2-step (notify->wait->upgrade) registry updates (e.g. by requiring voting or timelocks in the admin contract). Provide transparency about who is the owner of the registry as this may not be clear for everyone. Evaluate the impact of itemCount only returning the number of settings in the wrapper not taking into account entries in the subcontract (including pot. overlaps).  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.8 zBanc - DynamicLiquidTokenConverter consider using PPM_RESOLUTION instead of hardcoding integer literals    ", "body": "  Resolution   This issue was present in the initial commit under review (  zer0-os/zBanc@48da0ac) but has since been addressed with  zer0-os/zBanc@3d6943e.  Description  getMarketCap calculates the reserve s market capitalization as reserveBalance * 1e6 / weight where 1e6 should be expressed as the constant PPM_RESOLUTION.  Examples  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L157-L164  function getMarketCap(IERC20Token _reserveToken)  public  view  returns(uint256)  Reserve storage reserve = reserves[_reserveToken];  return reserveBalance(_reserveToken).mul(1e6).div(reserve.weight);  Recommendation  Avoid hardcoding integer literals directly into source code when there is a better expression available. In this case 1e6 is used because weights are denoted in percent to base PPM_RESOLUTION (=100%).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.9 zBanc - DynamicLiquidTokenConverter avoid potential converter type overlap with bancor   ", "body": "  Resolution  Acknowledged by providing the following statement:  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.24 the converterType relates to an array selector in the test helpers, so would be inconvenient to make a higher value. we will have to maintain the value when rebasing in DynamicLiquidTokenConverter & Factory, ConverterUpgrader, and the ConverterUpgrader.js test file and Converter.js test helper file.", "body": "  Description  The system is forked frombancorprotocol/contracts-solidity. As such, it is very likely that security vulnerabilities reported to bancorprotocol upstream need to be merged into the zer0/zBanc fork if they also affect this codebase. There is also a chance that security fixes will only be available with feature releases or that the zer0 development team wants to merge upstream features into the zBanc codebase.  Note that the current master of the bancorprotocol already appears to defined converterType 3 and 4: https://github.com/bancorprotocol/contracts-solidity/blob/5f4c53ebda784751c3a90b06aa2c85e9fdb36295/solidity/test/helpers/Converter.js#L51-L54  Examples  The new custom converter  zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L50-L52  function converterType() public pure override returns (uint16) {  return 3;  ConverterTypes from the bancor base system  zBanc/solidity/contracts/converter/types/liquidity-pool-v1/LiquidityPoolV1Converter.sol:L71-L73  function converterType() public pure override returns (uint16) {  return 1;  zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L73-L76  Recommendation  /  function converterType() public pure override returns (uint16) {  return 2;  Recommendation  Choose a converterType id for this custom implementation that does not overlap with the codebase the system was forked from. e.g. uint16(-1) or 1001 instead of 3 which might already be used upstream.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "5.10 zBanc - unnecessary contract duplication    ", "body": "  Resolution   fixed with   zer0-os/zBanc@ff3d913 by removing the duplicate contract.  Description  DynamicContractRegistryClient is an exact copy of ContractRegistryClient. Avoid unnecessary code duplication.  < contract DynamicContractRegistryClient is Owned, Utils {  ---  > contract ContractRegistryClient is Owned, Utils {  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zbanc/"}, {"title": "3.1 A reverting fallback function will lock up all payouts    ", "body": "  Resolution  Replace the push method to pull pattern.  Remove transfer of ETH in the process of execution, and store ETH amount to mapping(address => uint256) ethBalance (contracts/Inheritance/ETHExchange.sol, L21)  Add function withdrawETH to send ethBalance[msg.sender] (contracts/Inheritance/ETHExchange.sol, L158-L162)  Description  In BoxExchange.sol, the internal function _transferEth() reverts if the transfer does not succeed:  code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958-L963  function _transferETH(address _recipient, uint256 _amount) private {  (bool success, ) = _recipient.call{value: _amount}(  abi.encodeWithSignature(\"\")  );  require(success, \"Transfer Failed\");  The _payment() function processes a list of transfers to settle the transactions in an ExchangeBox. If any of the recipients of an Eth transfer is a smart contract that reverts, then the entire payout will fail and will be unrecoverable.  Recommendation  Implement a queuing mechanism to allow buyers/sellers to initiate the withdrawal on their own using a  pull-over-push pattern.   Ignore a failed transfer and leave the responsibility up to users to receive them properly.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.2 Force traders to mint gas token    ", "body": "  Resolution  Replace push funds with Pull Pattern.  Remove transfer of ETH in the process of execution, and store ETH amount to mapping(address => uint256) ethBalance (contracts/Inheritance/ETHExchange.sol, L21)  Add function withdrawETH to send ethBalance[msg.sender] (contracts/Inheritance/ETHExchange.sol, L158-L162)  Description  Attack scenario:  Alice makes a large trade via the Fairswap_iDOLvsEth exchange. This will tie up her iDOL until the box is executed.  Mallory makes a small trades to buy ETH immediately afterwards, the trades are routed through an attack contract.  Alice needs to execute the box to get her iDOL out.  Because the gas amount is unlimited, when you Mallory s ETH is paid out to her attack contract, mint a lot of GasToken.  If Alice has $100 worth of ETH tied up in the exchange, you can basically ransom her for $99 of gas token or else she ll never see her funds again.  Examples  code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958  function _transferETH(address _recipient, uint256 _amount) private {  Recommendation  When sending ETH, a pull-payment model is generally preferable.  This would require setting up a queue, allowing users to call a function to initiate a withdrawal.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.3 Missing Proper Access Control    ", "body": "  Resolution  Comment from Lien Protocol:  remove comment out lines to activate time control functions at Auction.sol and AuctionBoard.sol (a contract divided from Auction.sol)  Description  Some functions do not have proper access control and are public, meaning that anyone can call them. This will result in system take over depending on how critical those functionalities are.  Examples  Anyone can set IDOLContract in MainContracts.Auction.sol, which is a critical aspect of the auction contract, and it cannot be changed after it is set:  code/MainContracts/contracts/Auction.sol:L144-L148  Recommendation  /  function setIDOLContract(address contractAddress) public {  require(address(_IDOLContract) == address(0), \"IDOL contract is already registered\");  _setStableCoinContract(contractAddress);  Recommendation  Make the setIDOLContract() function internal and call it from the constructor, or only allow the deployer to set the value.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.4 Code is not production-ready    ", "body": "  Resolution  Comment from Lien Protocol:  remove comment out lines and update code to activate time control functions at AuctionTimeControl.sol  Description  Similar to other discussed issues, several areas of the code suggest that the system is not production-ready. This results in narrow test scenarios that do not cover production code flow.  Examples  In MainContracts/contracts/AuctionTimeControl.sol the following functions are commented out and replaced with same name functions that simply return True for testing purposes:  isNotStartedAuction  inAcceptingBidsPeriod  inRevealingValuationPeriod  inReceivingBidsPeriod  code/MainContracts/contracts/AuctionTimeControl.sol:L30-L39  /*  // Indicates any auction has never held for a specified BondID  function isNotStartedAuction(bytes32 auctionID) public virtual override returns (bool) {  uint256 closingTime = _auctionClosingTime[auctionID];  return closingTime == 0;  // Indicates if the auctionID is in bid acceptance status  function inAcceptingBidsPeriod(bytes32 auctionID) public virtual override returns (bool) {  uint256 closingTime = _auctionClosingTime[auctionID];  code/MainContracts/contracts/AuctionTimeControl.sol:L67-L78  // TEST  function isNotStartedAuction(bytes32 auctionID)  public  virtual  override  returns (bool)  return true;  // TEST  function inAcceptingBidsPeriod(bytes32 auctionID)  These commented-out functions contain essential functionality for the Auction contract. For example, inRevealingValuationPeriod is used to allow revealing of the bid price publicly:  code/MainContracts/contracts/Auction.sol:L403-L406  require(  inRevealingValuationPeriod(auctionID),  \"it is not the time to reveal the value of bids\"  );  Recommendation  Remove the test functions and use the production code for testing. The tests must have full coverage of the production code to be considered complete.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.5 Unable to compile contracts    ", "body": "  Resolution   The related code was updated on the 7th day of the audit, and fixed this issue for   Description  In the Fairswap_iDOLvsImmortalOptionsrepository:  Compilation with truffle fails due to a missing file: contracts/testTokens/TestBondMaker.sol. Compilation with solc fails due to an undefined interface function:  In the Fairswap_iDOLvsLien repository:  Compilation with truffle fails due to a missing file: ./ERC20RegularlyRecord.sol. The correct filename is ./TestERC20RegularlyRecord.sol.  Recommendation  Ensure all contracts are easily compilable by following simple instructions in the README.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.6 Unreachable code due to checked conditions    ", "body": "  Resolution  Comment from Lien Protocol:  Fix Unreachable codes in Auction.sol which were made for the tests.  Description  The code flow in MainContracts.Auction.sol revealBid() is that it first checks if the function has been called during the reveal period, which means  after closing  and  before the end of the reveal period.   code/MainContracts/contracts/Auction.sol:L508-L517  function revealBid(  bytes32 auctionID,  uint256 price,  uint256 targetSBTAmount,  uint256 random  ) public override {  require(  inRevealingValuationPeriod(auctionID),  \"it is not the time to reveal the value of bids\"  );  However, later in the same function, code exists to introduce  Penalties for revealing too early.  This checks to see if the function was called before closing, which should not be possible given the previous check.  code/MainContracts/contracts/Auction.sol:L523-L537  /**  @dev Penalties for revealing too early.  Some participants may not follow the rule and publicate their bid information before the reveal process.  In such a case, the bid price is overwritten by the bid with the strike price (slightly unfavored price).  /  uint256 bidPrice = price;  /**  @dev FOR TEST CODE RUNNING: The following if statement in L533 should be replaced by the comment out  /  if (inAcceptingBidsPeriod(auctionID)) {  // if (false) {  (, , uint256 solidStrikePriceE4, ) = _getBondFromAuctionID(auctionID);  bidPrice = _exchangeSBT2IDOL(solidStrikePriceE4.mul(10**18));  Recommendation  Double-check the logic in these functions. If revealing should be allowed (but penalized in the earlier stage), the first check should be changed. However, based on our understanding, the first check is correct, and the second check for early reveal is redundant and should be removed.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.7 TODO tags present in the code    ", "body": "  Resolution  Comment from Lien Protocol:  All TODOs in repositories were solved and removed.  Description  There are a few instances of TODO tags in the codebase that must be addressed before production as they correspond to commented-out code that makes up essential parts of the system.  Examples  code/MainContracts/contracts/Auction.sol:L310-L311  // require(strikePriceIDOLAmount > 10**10, 'at least 100 iDOL is required for the bid Amount'); // require $100 for spam protection // TODO  require(  code/MainContracts/contracts/BondMaker.sol:L392-L394  bytes32[] storage bondIDs = bondGroup.bondIDs;  // require(msg.value.mul(998).div(1000) > amount, 'fail to transfer Ether'); // TODO  code/MainContracts/contracts/BondMaker.sol:L402-L404  _issueNewBond(bondID, msg.sender, amount);  // transferETH(bondTokenAddress, msg.value - amount); // TODO  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.8 Documented function getERC20TokenDividend() does not exist    ", "body": "  Resolution  Comment from Lien Protocol:  Fairswap Fix ReadMe (README.md)  Description  In the README of Fairswap_iDOLvsLien, a function is listed which is not implemented in the codebase:  getERC20TokenDividend() function withdraws ETH and baseToken dividends for the Lien token stored in the exchange.(the dividends are stored in the contract at this moment)  Recommendation  Implement the function, or update the documentation  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.9 Fairswap interfaces are inconsistent    ", "body": "  Resolution  Comment from Lien Protocol:  Implement interface(contracts/Inheritance/TokenExchange.sol L11, contracts/Inheritance/ETHExchange.sol L12)  Description  There are unexpected inconsistencies between the three Fairswap contract interfaces, which may cause issues for composability with external contracts.  Examples  The function used to submit orders between the base and settlement currency has a different name across the three exchanges:  In Fairswap_iDOLvsETH it is called: orderEThToToken().  In Fairswap_iDOLvsLien it is called: OrderBaseToSettlement() (capitalized).  In Fairswap_iDOLvsImmmortalOptions it is called: orderBaseToSettlement().  Recommendation  Implement the desired interface in a separate file, and inherit it on the exchange contracts to ensure they are implemented as intended.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.10 Fairswap: inconsistent checks on _executionOrder()    ", "body": "  Resolution  Comment from Lien Protocol:  Fairswap- Integrate if statements about executeUnexecutedBox() (contracts/Inheritance/TokenExchange.sol L161, contracts/Inheritance/ETHExchange.sol L143, contracts/Inheritance/BoxExchange.sol L401-L405)  Description  The _executionOrder() function should only be called under specific conditions. However, these conditions are not always consistently defined.  Examples  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L218  if (nextBoxNumber > 1 && nextBoxNumber > nextExecuteBoxNumber) {  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L312  if (nextBoxNumber > 1 && nextBoxNumber > nextExecuteBoxNumber) {  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L647  if (nextBoxNumber > 1 && nextBoxNumber >= nextExecuteBoxNumber) {  Recommendation  Reduce duplicate code by defining an internal function to perform this check. A clear, descriptive name will help to clarify the intention.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.11 Inconsistency in DecimalSafeMath implementations    ", "body": "  Resolution  Comment from Lien Protocol:  Integrate and rename DecimalSafeMath to RateMath (contracts/Inheritance/RateMath.sol)  Description  There are two different implementations of DecimalSafeMath in the 3 FairSwap repositories.  Examples  FairSwap_iDOLvsLien/contracts/util/DecimalSafeMath.sol#L4-L11  library DecimalSafeMath {  function decimalDiv(uint256 a, uint256 b)internal pure returns (uint256) {  // assert(b > 0); // Solidity automatically throws when dividing by 0  uint256 a_ = a * 1000000000000000000;  uint256 c = a_ / b;  // assert(a == b * c + a % b); // There is no case in which this doesn't hold  return c;  Fairswap_iDOLvsETH/contracts/util/DecimalSafeMath.sol#L3-L11  library DecimalSafeMath {  function decimalDiv(uint256 a, uint256 b)internal pure returns (uint256) {  // assert(b > 0); // Solidity automatically throws when dividing by 0  uint256 c = (a * 1000000000000000000) / b;  // assert(a == b * c + a % b); // There is no case in which this doesn't hold  return c;  Recommendation  Try removing duplicate code/libraries and using a better inheritance model to include one file in all FairSwaps.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "4.1 ESMS use of sanitized user_amount & user_id values    ", "body": "  Resolution   Fixed in   https://github.com/nopslip/gtc-request-signer/pull/4/ , by using the sanitized integer value in the code flow.  Description  In the Signer service, values are properly checked, however the checked values are not preserved and the user input is passed down in the function.  The values are sanitized here:  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L98-L108  try:  int(user_id)  except ValueError:  gtc_sig_app.logger.error('Invalid user_id received!')  return Response('{\"message\":\"ESMS error\"}', status=400, mimetype='application/json')  # make sure it's an int  try:  int(user_amount)  except ValueError:  gtc_sig_app.logger.error('Invalid user_amount received!')  return Response('{\"message\":\"ESMS error\"}', status=400, mimetype='application/json')  But the original user inputs are being used here:  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L110-L113  try:  leaf = proofs[str(user_id)]['leaf']  proof = proofs[str(user_id)]['proof']  leaf_bytes = Web3.toBytes(hexstr=leaf)  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L128-L131  # this is a bit of hack to avoid bug in old web3 on frontend  # this means that user_amount is not converted back to wei before tx is broadcast!  user_amount_in_eth = Web3.fromWei(user_amount, 'ether')  Examples  if a float amount is passed for user_amount, all checks will pass, however the final amount will be slightly different that what it is intended:  >>> print(str(Web3.fromWei(123456789012345, 'ether')))  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "0.000123456789012345", "body": "  >>> print(str(Web3.fromWei(123456789012345.123, 'ether')))  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "0.000123456789012345125", "body": "  Recommendation  After the sanity check, use the sanitized value for the rest of the code flow.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.2 Prefer using abi.encode in TokenDistributor    ", "body": "  Resolution   Fixed in   gitcoinco/governance#7  Description  The method _hashLeaf is called when a user claims their airdrop.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L128-L129  // can we repoduce leaf hash included in the claim?  require(_hashLeaf(user_id, user_amount, leaf), 'TokenDistributor: Leaf Hash Mismatch.');  This method receives the user_id and the user_amount as arguments.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L253-L257  /**  @notice hash user_id + claim amount together & compare results to leaf hash  @return boolean true on match  /  function _hashLeaf(uint32 user_id, uint256 user_amount, bytes32 leaf) private returns (bool) {  These arguments are abi encoded and hashed together to produce a unique hash.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L258  bytes32 leaf_hash = keccak256(abi.encodePacked(keccak256(abi.encodePacked(user_id, user_amount))));  This hash is checked against the third argument for equality.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L259  return leaf == leaf_hash;  If the hash matches the third argument, it returns true and considers the provided user_id and user_amount are correct.  However, packing differently sized arguments may produce collisions.  The Solidity documentation states that packing dynamic types will produce collisions, but this is also the case if packing uint32 and uint256.  Examples  Below there s an example showing that packing uint32 and uint256 in both orders can produce collisions with carefully picked values.  library Encode {  function encode32Plus256(uint32 _a, uint256 _b) public pure returns (bytes memory) {  return abi.encodePacked(_a, _b);  function encode256Plus32(uint256 _a, uint32 _b) public pure returns (bytes memory) {  return abi.encodePacked(_a, _b);  contract Hash {  function checkEqual() public pure returns (bytes32, bytes32) {  // Pack 1  uint32  a1 = 0x12345678;  uint256 b1 = 0x99999999999999999999999999999999999999999999999999999999FFFFFFFF;  // Pack 2  uint256 a2 = 0x1234567899999999999999999999999999999999999999999999999999999999;  uint32  b2 = 0xFFFFFFFF;  // Encode these 2 different values  bytes memory packed1 = Encode.encode32Plus256(a1, b1);  bytes memory packed2 = Encode.encode256Plus32(a2, b2);  // Check if the packed encodings match  require(keccak256(packed1) == keccak256(packed2), \"Hash of representation should match\");  // The hashes are the same  // 0x9e46e582607c5c6e05587dacf66d311c4ced0819378a41d4b4c5adf99d72408e  return (  keccak256(packed1),  keccak256(packed2)  );  Changing abi.encodePacked to abi.encode in the library will make the transaction fail with error message Hash of representation should match.  Recommendation  Unless there s a specific use case to use abi.encodePacked, you should always use abi.encode. You might need a few more bytes in the transaction data, but it prevents collisions. Similar fix can be achieved by using unit256 for both values to be packed to prevent any possible collisions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.3 Simplify claim tokens for a gas discount and less code    ", "body": "  Resolution  Fixed in gitcoinco/governance#4  Structure Claim can still be removed for further optimization.  Description  The method claimTokens in TokenDistributor needs to do a few checks before it can distribute the tokens.  A few of these checks can be simplified and optimized.  The method hashMatch can be removed because it s only used once and the contents can be moved directly into the parent method.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L125-L126  // can we reproduce the same hash from the raw claim metadata?  require(hashMatch(user_id, user_address, user_amount, delegate_address, leaf, eth_signed_message_hash_hex), 'TokenDistributor: Hash Mismatch.');  Because this method also uses a few other internal calls, they also need to be moved into the parent method.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L211  return getDigest(claim) == eth_signed_message_hash_hex;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L184  hashClaim(claim)  Moving the code directly in the parent method and removing them will improve gas costs for users.  The structure Claim can also be removed because it s not used anywhere else in the code.  Recommendation  Consider simplifying claimTokens and remove unused methods.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.4 ESMS use of environment variable for chain info [Optimization]    ", "body": "  Resolution   Fixed in   nopslip/gtc-request-signer#5 by moving the variables to the environment variable.  Description  Variables to create domain separator are hardcoded in the code, and it requires the modify code on different deployments (e.g. testnet, mainnet, etc).  Examples  code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L203-L208  domain = make_domain(  name='GTA',  version='1.0.0',  chainId=4,  verifyingContract='0xBD2525B5F0B2a663439a78A99A06605549D25cE5')  Recommendation  Use environment variable for these values. This way there is no need to change the source code on different deployments and it can be scripted to prevent any possible errors on the code base.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.5 Rename method _hashLeaf to something that represents the validity of the leaf    ", "body": "  Resolution   Closed because the method was removed in   gitcoinco/governance#4  Description  The method _hashLeaf accepts 3 arguments.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L257  function _hashLeaf(uint32 user_id, uint256 user_amount, bytes32 leaf) private returns (bool) {  The arguments user_id and user_amount are used to create a keccak256 hash.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L258  bytes32 leaf_hash = keccak256(abi.encodePacked(keccak256(abi.encodePacked(user_id, user_amount))));  This hash is then checked if it matches the third argument.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L259  return leaf == leaf_hash;  The result of the equality is returned by the method.  The name of the method is confusing because it should say that it returns true if the leaf is considered valid.  Recommendation  Consider renaming the method to something like isValidLeafHash.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.6 Method returns bool but result is never used in TokenDistributor.claimTokens    ", "body": "  Resolution   Removed in   gitcoinco/governance#4  Description  The method _delegateTokens is called when a user claims their tokens to automatically delegate the claimed tokens to their own address or to a different one.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L135  _delegateTokens(user_address, delegate_address);  The method accepts the addresses of the delegator and the delegate and returns a boolean.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L262-L270  /**  @notice execute call on token contract to delegate tokens  @return boolean true on success  /  function _delegateTokens(address delegator, address delegatee) private returns (bool) {  GTCErc20  GTCToken = GTCErc20(token);  GTCToken.delegateOnDist(delegator, delegatee);  return true;  But this boolean is never used.  Recommendation  Remove the returned boolean because it s always returned as true anyway and the transaction will be a bit cheaper.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.7 Use a unified compiler version for all contracts    ", "body": "  Resolution   Compiler versions updated to   gitcoinco/governance#2  Description  Currently the smart contracts for the Gitcoin token and governance use different versions of Solidity compiler (^0.5.16, 0.6.12 , 0.5.17).  Recommendation  It is suggested to use a unified compiler version for all contracts (e.g. 0.6.12).  Note that it is recommended to use the latest version of Solidity compiler with security patches (currently 0.8.3), although given that these contracts are forks of the battle tested Uniswap governance contracts, the Gitcoin team prefer to keep the modifications to the code at minimum.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "4.8 Improve efficiency by using immutable in TreasuryVester    ", "body": "  Resolution   Fixed in   gitcoinco/governance#5  Description  The TreasuryVester contract when deployed has a few fixed storage variables.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L30  gtc = gtc_;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L33-L36  vestingAmount = vestingAmount_;  vestingBegin = vestingBegin_;  vestingCliff = vestingCliff_;  vestingEnd = vestingEnd_;  These storage variables are defined in the contract.  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L8  address public gtc;  code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TreasuryVester.sol:L11-L14  uint public vestingAmount;  uint public vestingBegin;  uint public vestingCliff;  uint public vestingEnd;  But they are never changed.  Recommendation  Consider setting storage variables as immutable type for a considerable gas improvement.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/gitcoin-token-distribution/"}, {"title": "5.1 safeRagequit makes you lose funds     in Pull Pattern", "body": "  Resolution  Description  safeRagequit and ragequit functions are used for withdrawing funds from the LAO. The difference between them is that ragequit function tries to withdraw all the allowed tokens and safeRagequit function withdraws only some subset of these tokens, defined by the user. It s needed in case the user or GuildBank is blacklisted in some of the tokens and the transfer reverts. The problem is that even though you can quit in that case, you ll lose the tokens that you exclude from the list.  To be precise, the tokens are not completely lost, they will belong to the LAO and can still potentially be transferred to the user who quit. But that requires a lot of trust, coordination, time and anyone can steal some part of these tokens.  Recommendation  Implementing pull pattern for token withdrawals should solve the issue. Users will be able to quit the LAO and burn their shares but still keep their tokens in the LAO s contract for some time if they can t withdraw them right now.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.2 Creating proposal is not trustless     in Pull Pattern", "body": "  Resolution  this issue no longer exists in the Pull Pattern update, due to the fact that emergency processing and in function ERC20 transfers are removed.  Description  Usually, if someone submits a proposal and transfers some amount of tribute tokens, these tokens are transferred back if the proposal is rejected. But if the proposal is not processed before the emergency processing, these tokens will not be transferred back to the proposer. This might happen if a tribute token or a deposit token transfers are blocked.  code/contracts/Moloch.sol:L407-L411  if (!emergencyProcessing) {  require(  proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),  \"failing vote token transfer failed\"  );  Tokens are not completely lost in that case, they now belong to the LAO shareholders and they might try to return that money back. But that requires a lot of coordination and time and everyone who ragequits during that time will take a part of that tokens with them.  Recommendation  Pull pattern for token transfers would solve the issue.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.3 Emergency processing can be blocked     in Pull Pattern", "body": "  Resolution  Emergency Processing no longer exists in the Pull Pattern update.  Description  The main reason for the emergency processing mechanism is that there is a chance that some token transfers might be blocked. For example, a sender or a receiver is in the USDC blacklist. Emergency processing saves from this problem by not transferring tribute token back to the user (if there is some) and rejecting the proposal.  code/contracts/Moloch.sol:L407-L411  if (!emergencyProcessing) {  require(  proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),  \"failing vote token transfer failed\"  );  The problem is that there is still a deposit transfer back to the sponsor and it could be potentially blocked too. If that happens, proposal can t be processed and the LAO is blocked.  Recommendation  Implementing pull pattern for all token withdrawals would solve the problem. The alternative solution would be to also keep the deposit tokens in the LAO, but that makes sponsoring the proposal more risky for the sponsor.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.4 Token Overflow might result in system halt or loss of funds    ", "body": "  Resolution   Fixed in   fd2da6, and  32ad9b by allowing overflows in most balance calculations (e.g.  Description  If a token overflows, some functionality such as processProposal, cancelProposal will break due to safeMath reverts. The overflow could happen because the supply of the token was artificially inflated to oblivion.  This issue was pointed out by Heiko Fisch in Telegram chat.  Examples  Any function using internalTransfer() can result in an overflow:  contracts/Moloch.sol:L631-L634  function max(uint256 x, uint256 y) internal pure returns (uint256) {  return x >= y ? x : y;  Recommendation  We recommend to allow overflow for broken or malicious tokens. This is to prevent system halt or loss of funds. It should be noted that in case an overflow occurs, the balance of the token will be incorrect for all token holders in the system.  rageKick, rageQuit were fixed by not using safeMath within the function code, however this fix is risky and not recommended, as there are other overflows in other functions that might still result in system halt or loss of funds.  One suggestion is having a function named unsafeInternalTransfer() which does not use safeMath for the cases that overflow should be allowed. This mainly adds better readability to the code.  It is still a risky fix and a better solution should be planned.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.5 Whitelisted tokens limit    ", "body": "  Resolution  mitigated by having separate limits for number of whitelisted tokens (for non-zero balance and for zero balance) in 486f1b3 and follow up commits. That s helpful because it s much cheaper to process tokens with zero balance in the guild bank and you can have much more whitelisted tokens overall.  uint256 constant MAX_TOKEN_WHITELIST_COUNT = 400; // maximum number of whitelisted tokens  uint256 constant MAX_TOKEN_GUILDBANK_COUNT = 200; // maximum number of tokens with non-zero balance in guildbank  uint256 public totalGuildBankTokens = 0; // total tokens with non-zero balance in guild bank  It should be noted that this is an estimated limit based on the manual calculations and current OP code gas costs. DAO members should consider splitting the DAO into two if more than 100 tokens with non-zero balance are used in the DAO to be safe.  Description  _ragequit function is iterating over all whitelisted tokens:  contracts/Moloch.sol:L507-L513  for (uint256 i = 0; i < tokens.length; i++) {  uint256 amountToRagequit = fairShare(userTokenBalances[GUILD][tokens[i]], sharesAndLootToBurn, initialTotalSharesAndLoot);  // deliberately not using safemath here to keep overflows from preventing the function execution (which would break ragekicks)  // if a token overflows, it is because the supply was artificially inflated to oblivion, so we probably don't care about it anyways  userTokenBalances[GUILD][tokens[i]] -= amountToRagequit;  userTokenBalances[memberAddress][tokens[i]] += amountToRagequit;  If the number of tokens is too big, a transaction can run out of gas and all funds will be blocked forever. Ballpark estimation of this number is around 300 tokens based on the current OpCode gas costs and the block gas limit.  Recommendation  A simple solution would be just limiting the number of whitelisted tokens.  If the intention is to invest in many new tokens over time, and it s not an option to limit the number of whitelisted tokens, it s possible to add a function that removes tokens from the whitelist. For example, it s possible to add a new type of proposals, that is used to vote on token removal if the balance of this token is zero. Before voting for that, shareholders should sell all the balance of that token.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.6 Summoner can steal funds using bailout     in Pull Pattern", "body": "  Resolution  Description  Currently, there are 2 major reasons for using the bailout function:  Kick someone out of the LAO. If the shareholders vote for kicking somebody, the kicked user goes to jail at first. If the LAO kicks someone, it s important not to steal user s funds, but remove them from profit-sharing as soon as possible. Currently, because the user can potentially block some token transfers, funds can t be transferred and the user is still having loot and is participation in a profit-sharing. In order to avoid that, bailout function was introduced. It allows anyone to transfer kicked user s funds to the summoner if the user does not call safeRagequit (which forces the user to lose some funds). The intention is for the summoner to transfer these funds to the kicked member afterwards. The issue here is that it requires a lot of trust to the summoner on the one hand, and requires more time to kick the member out of the LAO.   lost private key  problem. If someone s private key was lost, shareholders can allow summoner to transfer funds from any user whose keys were lost. The problem is that any member s funds can be stolen by the LAO members and the summoner like that. So every member should keep track of that kind of proposal and is forced to do the ragequit if that proposal passes. That decreases trustlessness because if a user is not tracking the system for some time, the user s money can possibly be stolen.  Recommendation  To solve these issues, these 2 intentions should be split into 2 different mechanisms. By implementing pull pattern for token transfers, kicked member won t be able to block the ragekick and the LAO members would be able to kick anyone much quicker. There is no need to keep the bailout for this intention.  If  lost private key  problem should be addressed in the LAO, the time period for the funds recovery should be big because there is no need to do the recovery asap. Recovery can be done without a preliminary kick and can even cover not only the shares and loot, but also tokens that should be withdrawn (if pull pattern is implemented)  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.7 Sponsorship front-running     in Pull Pattern", "body": "  Resolution  this issue no longer exists in the Pull Pattern update with Major severity, as mentioned in the recommendation, the front-running vector is still open but no rationale exist for such a behaviour.  Description  If proposal submission and sponsorship are done in 2 different transactions, it s possible to front-run the sponsorProposal function by any member. The incentive to do that is to be able to block the proposal afterwards. It s sometimes possible to block the proposal by getting blacklisted at depositToken. In that case, the proposal won t be accepted and the emergency processing is going to happen next. Currently, if the attacker can become whitelisted again, he might even not lose the deposit tokens. If not, it will block the whole system forever and everyone would have to ragequit (but that s the part of another issue).  Recommendation  Pull pattern for token transfers will solve the issue. Front-running will still be possible but it doesn t affect anything.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.8 Delegate assignment front-running   ", "body": "  Description  Any member can front-run another member s delegateKey assignment.  if you try to submit an address as your delegateKey, someone else can try to assign your delegate address tp themselves. While incentive of this action is unclear, it s possible to block some address from being a delegate forever. ragekick and ragequit do not free the delegate address and the delegate itself also cannot change the address.  The possible attack could be that a well-known hard-to-replace multisig address is assigned as a delegateKey and someone else take this address to block it. Also, if the malicious member is about to ragequit or be kicked, it s possible to do this attack without losing anything.  The only way to free the delegate is to make it a member, but then it can never be a delegate after.  Recommendation  Make it possible for a delegateKey to approve delegateKey assignment or cancel the current delegation. And additionally, it may be valuable to clear the delegate address in the _ragequit function.  Commit-reveal methods can also be used to mitigate this attack.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.9 No votes are still valid after the ragequit/ragekick   ", "body": "  Description  Shareholders can vote for the upcoming proposals 2 weeks before they can be executed. If they ragequit or get ragekicked, their votes are still considered valid. And while the LAO does not allow anyone to ragequit before the last proposal with Yes vote is processed, it s still possible to quit the LAO and having active No votes on some proposals.  It s not naturally expected behaviour because by that time a user ragequits, they are not part of the LAO and do not have any voting power. Moreover, there is no incentive not to vote No just to fail all the possible proposals, because the user won t be sharing any consequences of the result of these proposals. And even incentivized to vote No for every proposal just as the act of revenge for the ragekick.  Recommendation  The problem is mitigated by the fact that all rejected proposals can be submitted again and be processed a few weeks after.  It s possible to remove all the No votes from the proposals after user s ragekick/ragequit.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.10 Dilution bound should be a fixed-point number   ", "body": "  Resolution  a per-proposal dilution bound was considered for the v1, but kept it global in the interest of code simplicity.  Description  The dilution bound is designed to mitigate an issue where a proposal is passed, then many users ragequit from the DAO and the remaining members have to pay more than they initially intended to. Because of that, the proposal will be automatically rejected if the total amount of shares becomes dilutionBound times less than it was before. The problem is that dilutionBound is an integer value and it s impossible to configure it to decimal values such as 1.2, for example.  Recommendation  Make dilutionBound a fixed-point number.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.11 Whitelist proposal duplicate   ", "body": "  Description  Every time when a whitelist proposal is sponsored, it s checked that there is no other sponsored whitelist proposal with the same token. This is done in order to avoid proposal duplicates.  code/contracts/Moloch.sol:L277-L281  // whitelist proposal  if (proposal.flags[4]) {  require(!tokenWhitelist[address(proposal.tributeToken)], \"cannot already have whitelisted the token\");  require(!proposedToWhitelist[address(proposal.tributeToken)], 'already proposed to whitelist');  proposedToWhitelist[address(proposal.tributeToken)] = true;  The issue is that even though you can t sponsor a duplicate proposal, you can still submit a new proposal with the same token.  Recommendation  Check that there is currently no sponsored proposal with the same token on proposal submission.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.12 Moloch - bool[6] flags can be changed to a dedicated structure   ", "body": "  Resolution   The Moloch team decided to leave the   Description  The Moloch contract uses a structure that includes an array of bools to store a few flags about the proposal:  code/contracts/Moloch.sol:L88  bool[6] flags; // [sponsored, processed, didPass, cancelled, whitelist, guildkick]  This makes reasoning about the correctness of the code a bit complicated because one needs to remember what each item in the flag list stands for. The make the reader s life simpler a dedicated structure can be created that incorporates all of the required flags.  Examples  bool[6] memory flags; // [sponsored, processed, didPass, cancelled, whitelist, guildkick]  Recommendation  Based on the provided examples change the bool[6] flags to the proposed examples.  Flags as bool array with enum (proposed)  This second contract implements the flags as a defined structure with each named element representing a specific flag. This method makes clear which flag is accessed because they are referred to by the name, not by the index.  This third contract has the least amount of changes to the code and uses an enum structure to handle the index.  pragma solidity 0.5.15;  contract FlagsEnum {  struct Proposal {  address applicant;  uint value;  bool[3] flags; // [sponsored, processed, kicked]  enum ProposalFlags {  SPONSORED,  PROCESSED,  KICKED  uint proposalCount;  mapping(uint256 => Proposal) public proposals;  function addProposal(uint _value, bool _sponsored, bool _processed, bool _kicked) public returns (uint) {  Proposal memory proposal = Proposal({  applicant: msg.sender,  value: _value,  flags: [_sponsored, _processed, _kicked]  });  proposals[proposalCount] = proposal;  proposalCount += 1;  return (proposalCount);  function getProposal(uint _proposalId) public view returns (address, uint, bool, bool, bool) {  return (  proposals[_proposalId].applicant,  proposals[_proposalId].value,  proposals[_proposalId].flags[uint(ProposalFlags.SPONSORED)],  proposals[_proposalId].flags[uint(ProposalFlags.PROCESSED)],  proposals[_proposalId].flags[uint(ProposalFlags.KICKED)]  );  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "6.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "6.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/GuildBank.sol  13:8     warning    Provide an error message for require()                                                     error-reason  23:12    warning    Provide an error message for require()                                                     error-reason  34:8     warning    Provide an error message for require()                                                     error-reason  36:27    warning    There should be no whitespace or comments between the opening brace '{' and first item.    whitespace  36:37    warning    There should be no whitespace or comments between the last item and closing brace '}'.     whitespace  contracts/Moloch.sol  34:4      warning    Line exceeds the limit of 145 characters                                   max-len  41:4      warning    Line exceeds the limit of 145 characters                                   max-len  42:4      warning    Line exceeds the limit of 145 characters                                   max-len  76:8      warning    Line exceeds the limit of 145 characters                                   max-len  169:24    warning    Avoid using 'now' (alias to 'block.timestamp').                            security/no-block-members  262:8     warning    Line exceeds the limit of 145 characters                                   max-len  272:49    error      String literal must be quoted with double quotes.                          quotes  280:74    error      String literal must be quoted with double quotes.                          quotes  285:57    error      String literal must be quoted with double quotes.                          quotes  362:8     warning    Line exceeds the limit of 145 characters                                   max-len  517:8     warning    Line exceeds the limit of 145 characters                                   max-len  540:13    warning    Assignment operator must have exactly single space on both sides of it.    operator-whitespace  559:8     warning    Error message exceeds max length of 76 characters                          error-reason  583:8     warning    Error message exceeds max length of 76 characters                          error-reason  641:15    warning    Avoid using 'now' (alias to 'block.timestamp').                            security/no-block-members  contracts/oz/ERC20.sol  128:8    warning    Provide an error message for require()    error-reason  143:8    warning    Provide an error message for require()    error-reason  157:8    warning    Provide an error message for require()    error-reason  171:8    warning    Provide an error message for require()    error-reason  172:8    warning    Provide an error message for require()    error-reason  contracts/oz/SafeMath.sol  10:8    warning    Provide an error message for require()    error-reason  17:8    warning    Provide an error message for require()    error-reason  24:8    warning    Provide an error message for require()    error-reason  32:8    warning    Provide an error message for require()    error-reason  contracts/test-helpers/Submitter.sol  9:2     error    Only use indent of 4 spaces.    indentation  11:2    error    Only use indent of 4 spaces.    indentation  13:2    error    Only use indent of 4 spaces.    indentation  15:0    error    Only use indent of 4 spaces.    indentation  17:2    error    Only use indent of 4 spaces.    indentation  39:0    error    Only use indent of 4 spaces.    indentation  41:2    error    Only use indent of 4 spaces.    indentation  51:0    error    Only use indent of 4 spaces.    indentation  53:2    error    Only use indent of 4 spaces.    indentation  63:0    error    Only use indent of 4 spaces.    indentation  contracts/tokens/ClaimsToken.sol  95:1     error      Only use indent of 4 spaces.              indentation  98:1     error      Only use indent of 4 spaces.              indentation  100:1    error      Only use indent of 4 spaces.              indentation  102:1    error      Only use indent of 4 spaces.              indentation  105:1    error      Only use indent of 4 spaces.              indentation  112:0    error      Only use indent of 4 spaces.              indentation  121:1    error      Only use indent of 4 spaces.              indentation  129:0    error      Only use indent of 4 spaces.              indentation  140:1    error      Only use indent of 4 spaces.              indentation  148:0    error      Only use indent of 4 spaces.              indentation  154:1    error      Only use indent of 4 spaces.              indentation  160:0    error      Only use indent of 4 spaces.              indentation  167:1    error      Only use indent of 4 spaces.              indentation  173:0    error      Only use indent of 4 spaces.              indentation  180:1    error      Only use indent of 4 spaces.              indentation  184:0    error      Only use indent of 4 spaces.              indentation  190:1    error      Only use indent of 4 spaces.              indentation  197:0    error      Only use indent of 4 spaces.              indentation  203:1    error      Only use indent of 4 spaces.              indentation  208:0    error      Only use indent of 4 spaces.              indentation  216:1    error      Only use indent of 4 spaces.              indentation  226:0    error      Only use indent of 4 spaces.              indentation  232:1    error      Only use indent of 4 spaces.              indentation  234:1    error      Only use indent of 4 spaces.              indentation  237:0    error      Only use indent of 4 spaces.              indentation  239:1    error      Only use indent of 4 spaces.              indentation  243:2    warning    Provide an error message for require()    error-reason  246:0    error      Only use indent of 4 spaces.              indentation  251:1    error      Only use indent of 4 spaces.              indentation  260:0    error      Only use indent of 4 spaces.              indentation  268:1    error      Only use indent of 4 spaces.              indentation  276:0    error      Only use indent of 4 spaces.              indentation  contracts/tokens/Token.sol  25:8    warning    Provide an error message for require()    error-reason  \u2716 44 errors, 28 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "6.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  contracts/GuildBank.sol  d4329bc7836a1800eb2376da05f76f1783700b9a  contracts/Moloch.sol  8f55cc17fcf0488acdc9dd2261dca1e08f42c4ac  contracts/oz/ERC20.sol  6db943e86683ce536b8e75e79d7fb80a02b855ae  contracts/oz/IERC20.sol  f249341b598ed60fdb987fc6dd05b6cd15da7b6b  contracts/oz/ReentrancyGuard.sol  115a19532af141450ea30ad141aecb76b79035b4  contracts/oz/SafeMath.sol  b86ab5a6679fd597c3a0412d31080893beeb653a  contracts/test-helpers/Submitter.sol  7b29e3178cb4c7848851a8c92661a0e12fee7489  contracts/tokens/ClaimsToken.sol  11bb8b648de195efbca13df15e10b3e6a75fcab6  contracts/tokens/Token.sol  7c193d22ad069e368aba4fa9bc3d4c28e8e1973b  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  GuildBank  Implementation  <Constructor>  Public    withdraw  Public    onlyOwner  withdrawToken  Public    onlyOwner  fairShare  Internal \ud83d\udd12  Moloch  Implementation  ReentrancyGuard  <Constructor>  Public    submitProposal  Public    nonReentrant  submitWhitelistProposal  Public    nonReentrant  submitGuildKickProposal  Public    nonReentrant  _submitProposal  Internal \ud83d\udd12  sponsorProposal  Public    nonReentrant onlyDelegate  submitVote  Public    nonReentrant onlyDelegate  processProposal  Public    nonReentrant  processWhitelistProposal  Public    nonReentrant  processGuildKickProposal  Public    nonReentrant  _didPass  Internal \ud83d\udd12  _validateProposalForProcessing  Internal \ud83d\udd12  _returnDeposit  Internal \ud83d\udd12  ragequit  Public    nonReentrant onlyMember  safeRagequit  Public    nonReentrant onlyMember  _ragequit  Internal \ud83d\udd12  ragekick  Public    nonReentrant  bailout  Public    nonReentrant  cancelProposal  Public    nonReentrant  updateDelegateKey  Public    nonReentrant onlyShareholder  max  Internal \ud83d\udd12  getCurrentPeriod  Public    NO   getProposalQueueLength  Public    NO   getProposalFlags  Public    NO   canRagequit  Public    NO   canBailout  Public    NO   hasVotingPeriodExpired  Public    NO   getMemberProposalVote  Public    NO   ERC20  Implementation  IERC20  totalSupply  Public    NO   balanceOf  Public    NO   allowance  Public    NO   transfer  Public    NO   approve  Public    NO   transferFrom  Public    NO   increaseAllowance  Public    NO   decreaseAllowance  Public    NO   _transfer  Internal \ud83d\udd12  _mint  Internal \ud83d\udd12  _burn  Internal \ud83d\udd12  _approve  Internal \ud83d\udd12  _burnFrom  Internal \ud83d\udd12  IERC20  Interface  transfer  External    NO   approve  External    NO   transferFrom  External    NO   totalSupply  External    NO   balanceOf  External    NO   allowance  External    NO   ReentrancyGuard  Implementation  <Constructor>  Internal \ud83d\udd12  SafeMath  Library  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  add  Internal \ud83d\udd12  Submitter  Implementation  <Constructor>  Public    submitProposal  Public    NO   submitWhitelistProposal  Public    NO   submitGuildKickProposal  Public    NO   ERC20Detailed  Implementation  IERC20  <Constructor>  Public    name  Public    NO   symbol  Public    NO   decimals  Public    NO   IClaimsToken  Interface  withdrawFunds  External    NO   availableFunds  External    NO   totalReceivedFunds  External    NO   ClaimsToken  Implementation  IClaimsToken, ERC20, ERC20Detailed  <Constructor>  Public    ERC20Detailed  transfer  Public    NO   transferFrom  Public    NO   totalReceivedFunds  External    NO   availableFunds  Public    NO   _registerFunds  Internal \ud83d\udd12  _calcUnprocessedFunds  Internal \ud83d\udd12  _claimFunds  Internal \ud83d\udd12  _prepareWithdraw  Internal \ud83d\udd12  ClaimsTokenERC20Extension  Implementation  IClaimsToken, ClaimsToken  <Constructor>  Public    ClaimsToken  withdrawFunds  External    NO   tokenFallback  Public    onlyFundsToken  Token  Implementation  ERC20  <Constructor>  Public    updateTransfersEnabled  External    NO   updateTransfersReturningFalse  External    NO   transfer  Public    NO   Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "4.1 Reentrancy vulnerability in MetaSwap.swap()    ", "body": "  Resolution   This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  MetaSwap.swap() should have a reentrancy guard.  The adapters use this general process:  Collect the from token (or ether) from the user.  Execute the trade.  Transfer the contract s balance of tokens (from and to) and ether to the user.  If an attacker is able to reenter swap() before step 3, they can execute their own trade using the same tokens and get all the tokens for themselves.  This is partially mitigated by the check against amountTo in CommonAdapter, but note that the amountTo typically allows for slippage, so it may still leave room for an attacker to siphon off some amount while still returning the required minimum to the user.  code/contracts/adapters/CommonAdapter.sol:L57-L62  // Transfer remaining balance of tokenTo to sender  if (address(tokenTo) != Constants.ETH) {  uint256 balance = tokenTo.balanceOf(address(this));  require(balance >= amountTo, \"INSUFFICIENT_AMOUNT\");  _transfer(tokenTo, balance, recipient);  } else {  Examples  As an example of how this could be exploited, 0x supports an  EIP1271Wallet  signature type, which invokes an external contract to check whether a trade is allowed. A malicious maker might front run the swap to reduce their inventory. This way, the taker is sending more of the taker asset than necessary to MetaSwap. The excess can be stolen by the maker during the EIP1271 call.  Recommendation  Use a simple reentrancy guard, such as OpenZeppelin s ReentrancyGuard to prevent reentrancy in MetaSwap.swap(). It might seem more obvious to put this check in Spender.swap(), but the Spender contract intentionally does not use any storage to avoid interference between different adapters.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.2 A new malicious adapter can access users  tokens    ", "body": "  Resolution   This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  The purpose of the MetaSwap contract is to save users gas costs when dealing with a number of different aggregators. They can just approve() their tokens to be spent by MetaSwap (or in a later architecture, the Spender contract). They can then perform trades with all supported aggregators without having to reapprove anything.  A downside to this design is that a malicious (or buggy) adapter has access to a large collection of valuable assets. Even a user who has diligently checked all existing adapter code before interacting with MetaSwap runs the risk of having their funds intercepted by a new malicious adapter that s added later.  Recommendation  There are a number of designs that could be used to mitigate this type of attack. After discussion and iteration with the client team, we settled on a pattern where the MetaSwap contract is the only contract that receives token approval. It then moves tokens to the Spender contract before that contract DELEGATECALLs to the appropriate adapter. In this model, newly added adapters shouldn t be able to access users  funds.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.3 Owner can front-run traders by updating adapters    ", "body": "  Resolution   This is fixed in   ConsenSys/metaswap-contracts@8de01f6.  Description  MetaSwap owners can front-run users to swap an adapter implementation. This could be used by a malicious or compromised owner to steal from users.  Because adapters are DELEGATECALLed, they can modify storage. This means any adapter can overwrite the logic of another adapter, regardless of what policies are put in place at the contract level. Users must fully trust every adapter because just one malicious adapter could change the logic of all other adapters.  Recommendation  At a minimum, disallow modification of existing adapters. Instead, simply add new adapters and disable the old ones. (They should be deleted, but the aggregator IDs of deleted adapters should never be reused.)  This is, however, insufficient. A new malicious adapter could still overwrite the adapter mapping to modify existing adapters. To fully address this issue, the adapter registry should be in a separate contract. Through discussion and iteration with the client team, we settled on the following pattern:  MetaSwap contains the adapter registry. It calls into a new Spender contract.  The Spender contract has no storage at all and is just used to DELEGATECALL to the adapter contracts.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.4 Simplify fee calculation in WethAdapter    ", "body": "  Resolution  ConsenSys/metaswap-contracts@93bf5c6.  Description  WethAdapter does some arithmetic to keep track of how much ether is being provided as a fee versus as funds that should be transferred into WETH:  code/contracts/adapters/WethAdapter.sol:L41-L59  // Some aggregators require ETH fees  uint256 fee = msg.value;  if (address(tokenFrom) == Constants.ETH) {  // If tokenFrom is ETH, msg.value = fee + amountFrom (total fee could be 0)  require(amountFrom <= fee, \"MSG_VAL_INSUFFICIENT\");  fee -= amountFrom;  // Can't deal with ETH, convert to WETH  IWETH weth = getWETH();  weth.deposit{value: amountFrom}();  _approveSpender(weth, spender, amountFrom);  } else {  // Otherwise capture tokens from sender  // tokenFrom.safeTransferFrom(recipient, address(this), amountFrom);  _approveSpender(tokenFrom, spender, amountFrom);  // Perform the swap  aggregator.functionCallWithValue(abi.encodePacked(method, data), fee);  This code can be simplified by using address(this).balance instead.  Recommendation  Consider something like the following code instead:  if (address(tokenFrom) == Constants.ETH) {  getWETH().deposit{value: amountFrom}(); // will revert if the contract has an insufficient balance  _approveSpender(weth, spender, amountFrom);  } else {  tokenFrom.safeTransferFrom(recipient, address(this), amountFrom);  _approveSpender(tokenFrom, spender, amountFrom);  // Send the remaining balance as the fee.  aggregator.functionCallWithValue(abi.encodePacked(method, data), address(this).balance);  Aside from being a little simpler, this way of writing the code makes it obvious that the full balance is being properly consumed. Part is traded, and the rest is sent as a fee.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "4.5 Consider checking adapter existence in MetaSwap    ", "body": "  Resolution   The MetaSwap team found that doing the check in   Description  MetaSwap doesn t check that an adapter exists before calling into Spender:  code/contracts/MetaSwap.sol:L87-L100  function swap(  string calldata aggregatorId,  IERC20 tokenFrom,  uint256 amount,  bytes calldata data  ) external payable whenNotPaused nonReentrant {  Adapter storage adapter = adapters[aggregatorId];  if (address(tokenFrom) != Constants.ETH) {  tokenFrom.safeTransferFrom(msg.sender, address(spender), amount);  spender.swap{value: msg.value}(  adapter.addr,  Then Spender performs the check and reverts if it receives address(0).  code/contracts/Spender.sol:L15-L16  function swap(address adapter, bytes calldata data) external payable {  require(adapter != address(0), \"ADAPTER_NOT_SUPPORTED\");  It can be difficult to decide where to put a check like this, especially when the operation spans multiple contracts. Arguments can be made for either choice (or even duplicating the check), but as a general rule it s a good idea to avoid passing invalid parameters internally. Checking for adapter existence in MetaSwap.swap() is a natural place to do input validation, and it means Spender can have a simpler model where it trusts its inputs (which always come from MetaSwap).  Recommendation  Drop the check from Spender.swap() and perform the check instead in MetaSwap.swap().  5 Second Assessment  We performed a second assessment between October 3rd and October 4th, 2020. The engagement was conducted primarily by Steve Marx. The total effort expended was 2 person-days.  This second assessment covered three new features added by the MetaSwap team:  Support for the CHI gas token   This allows users to offset their gas costs by burning gas tokens. These tokens can come from the user or from tokens that are owned by the MetaSwap contract itself.  Uniswap Adapter   This adapter allows swaps to be executed via the Uniswap v2 Router directly, rather than going through some other exchange first.  Fee collection   FeeCommonAdapter and FeeWethAdapter are fee-collecting versions of the original CommonAdapter and WethAdapter. They support an extra parameter fee, indicating the quantity of the from asset to be sent to a fee wallet.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "5.1 Scope for the Second Assessment", "body": "  The following files were in scope for the second assessment:  MetaSwap.sol  5d66ea56c131b3ad5246e9fc6c126a0b7ba497fa  adapters/FeeCommonAdapter.sol  1bb0e2b4f7fca8e0d98113cf152eeb6be4ff13c7  adapters/FeeWethAdapter.sol  f844d9e13bd2cbf52a81ae4637b35f214098f3b2  adapters/UniswapAdapter.sol  d0733f6f4567dc58d3caf4af8875e17824a97f2d  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "5.2 Security Specification", "body": "  The security specification hasn t change much from the original assessment, so please refer to that. There are two significant changes to the security model: fee collection and gas token ownership.  In the new code, fees are collected, but these fees can be seen as voluntary from the perspective of the smart contracts. Users are free to pass any value for the fee parameter, including 0 to avoid all fees. The assumption is that most users will not bother to change the fee suggested by the MetaSwap API.  The other significant change is the introduction of the CHI gas token. In particular, the ability to use gas tokens held by the MetaSwap contract opens a new potential attack surface. Indeed, we found that an attacker could use contract-held tokens for other purposes.  6 Second Assessment Issues  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "6.1 Attacker can abuse gas tokens stored in MetaSwap    ", "body": "  Resolution   This function was removed in   ConsenSys/metaswap-contracts@75c4454.  Description  MetaSwap.swapUsingGasToken() allows users to make use of gas tokens held by the MetaSwap contract itself to reduce the gas cost of trades.  This mechanism is unsafe because any tokens held by the contract can be used by an attacker for other purposes.  Examples  An attack could also be made by using an existing token that makes external calls (e.g. an ERC777 token) or a mechanism in an aggregated exchange that makes external calls (e.g. wallet signatures in 0x).  Recommendation  The simplest way to avoid this vulnerability is to never transfer CHI gas tokens to MetaSwap at all. An alternative would be to only allow gas tokens to be used by approved transactions from the MetaSwap API. A possible mechanism for that would be to require a signature from the MetaSwap API. If such a signature were only provided in known-good situations (which are admittedly hard to define), it wouldn t be possible for an attacker to misuse the tokens.  7 Third Assessment  We performed a third assessment between November 7th and November 10th, 2020. The engagement was conducted primarily by Steve Marx. The total expended effort was 4 person-hours.  This third assessment covered the new FeeDistributor contract, which divides assets among a number of recipients. It s used in the MetaSwap system to distribute fees. Each recipient has a number of  shares , and assets are divided according to each recipients portion of share ownership. Potential assets include ether and ERC20-compatible tokens.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "7.1 Scope for the third assessment", "body": "  The only contract in scope was the FeeDistributor:  FeeDistributor.sol  23749a338461db92a96ae87a2fd454d1aa0cbb92  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "7.2 Security Specification", "body": "  At setup, the FeeDistributor is initialized with a number of recipients, each with a corresponding number of shares.  Recipients should be able to withdraw their fair share (<recipient's shares> / <total shares>) of any stored asset at any time.  No recipient should receive more than their fair share of an asset.  8 Third Assessment Recommendations  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "8.1 Document assumptions about ERC20 tokens", "body": "  Most ERC20-compatible tokens can be used with the FeeDistributor contract, but it s wise to document some assumptions made by the contract:  Token balances will not be too big (relative to the number of shares). Specifically, the total number of token units received by the contract must be able to be multiplied by the largest share amount held by a recipient.  Token balances will not be too small (relative to share amounts). It s impossible to divide a balance of 1 among more than 1 recipient. To be safe, it would be good to make sure that no one cares about losing less than totalShares token units. For example, if there are 1,000,000 total shares, an asset like ether would not be a problem because 1,000,000 wei is a trivial amount.  Token balances will not decrease without an explicit transfer. The contract makes the assumption that it can always compute the total received tokens by adding tokenBalance(token) and _totalWithdrawn[token]. This is not the case if the token balance can be manipulated externally.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "8.2 Only allow full withdrawal", "body": "  The current code has both   ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "8.3 Drop the recipient parameter", "body": "  Everywhere in the code, the   9 Third Assessment Issues  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "9.1 Simplify accounting and better handle remainders    ", "body": "  Resolution   This was fixed in   ConsenSys/metaswap-contracts@f0a62e5. The accounting was reworked according to the recommendation here.  Description  The current code does some fairly complex and redundant calculations during withdrawal to keep track of various pieces of state. In particular, the pair of _available[recipient][token] and _totalOnLastUpdate[recipient][token] is difficult to describe and reason about.  Recommendation  For a given token and recipient, we recommend instead just tracking how much has already been withdrawn. The rest can be easily calculated:  function earned(IERC20 token, address recipient) public view returns (uint256) {  uint256 totalReceived = tokenBalance(token).add(_totalWithdrawn[token]);  return totalReceived.mul(shares[recipient]).div(totalShares);  function available(IERC20 token, address recipient) public view returns (uint256) {  return earned(token, recipient).sub(_withdrawn[token][recipient]);  function withdraw(IERC20[] calldata tokens) external {  for (uint256 i = 0; i < tokens.length; i++) {  IERC20 token = tokens[i];  uint256 amount = available(token, msg.sender);  _withdrawn[token][msg.sender] += amount;  _totalWithdrawn[token] += amount;  _transfer(token, msg.sender, amount);  emit Withdrawal(tokens, msg.sender);  This code is easier to reason about:  It s easy to see that withdrawn[token][msg.sender] is correct because it s only increased when there s a corresponding transfer.  It s easy to see that _totalWithdrawn[token] is correct for the same reason.  It s easy to see that earned() is correct under standard assumptions about ERC20 balances.  It s easy to see that available() is correct, as it s just the earned amount less the already-withdrawn amount.  Remainders are better handled. If 1 token unit is available and you own half the shares, nothing happens on withdrawal, and if there are later 2 token units available, you can withdraw 1. (Under the previous code, if you tried to withdraw when 1 token unit was available, you would be unable to withdraw when 2 were available.)  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/metaswap/"}, {"title": "5.1 Code readability - Rename priceDeviation to maxPriceDeviation   ", "body": "  Resolution  The variable was renamed.  Description  Improve code readability by renaming the state variable priceDeviation to maxPriceDeviation, distinguishing it from the local variable price_deviation and indicating that the variable is a limit as outlined in the specification (MAX_DEVIATION).  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L124-L129  if (  price_deviation > (BONE + priceDeviation) ||  price_deviation < (BONE - priceDeviation)  ) {  return true;  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L83-L95  if (  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  ) {  return true;  price_deviation = Math.bdiv(ethTotal_1, ethTotal_0);  if (  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  ) {  return true;  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "5.2 Improve Input Validation   ", "body": "  Resolution  the recommended checks have been added to the constructor.  Description  The constructor does not validate whether the provided price provider arguments actually make sense. In the worst-case someone might be able to deploy the contract that cannot be used. It is recommended to fail the contract creation early if invalid arguments are detected.  Consider implementing the following checks to detect whether a non-viable price provider is being deployed:  tokens.length > 1 and less than the maximum supported tokens (note that hasDeviation requires token.length**2 iterations if no deviation is detected)  _isPeggedToEth.length == tokens.length  _decimals.length == tokens.length  approximationMatrix.length && approximationMatrix[0][0].length == tokens.length +1  _priceDeviation is within bounds (less than 100%, i.e. less than 1 * BONE) otherwise the calculation might underflow.  _powerPrecision is within bounds  address(_priceOracle) != address(0)  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L38-L63  constructor(  BPool _pool,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation,  uint256 _K,  uint256 _powerPrecision,  uint256[][] memory _approximationMatrix  ) public {  pool = _pool;  //Get token list  tokens = pool.getFinalTokens(); //This already checks for pool finalized  //Get token normalized weights  uint256 length = tokens.length;  for (uint8 i = 0; i < length; i++) {  weights.push(pool.getNormalizedWeight(tokens[i]));  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  K = _K;  powerPrecision = _powerPrecision;  approximationMatrix = _approximationMatrix;  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L35-L50  constructor(  IUniswapV2Pair _pair,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation  ) public {  pair = _pair;  //Get tokens  tokens.push(pair.token0());  tokens.push(pair.token1());  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "5.3 Use SafeMath consistently   ", "body": "  Resolution  All arithmetic operations now use SafeMath.  Description  Even though the Uniswap price provider imports the SafeMath library, the SafeMath library functions aren t always used for integer arithmetic operations. Note that plain Solidity arithmetic operators do not check for integer underflows and overflows.  Examples  Example 1:  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L66  uint256 missingDecimals = 18 - decimals[index];  Example 2 (same in line 91-92):  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L84-L85  price_deviation > (Math.BONE + priceDeviation) ||  price_deviation < (Math.BONE - priceDeviation)  Example 3:  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L164-L165  uint256 liquidity = numerator / denominator;  totalSupply += liquidity;  Recommendation  In some cases, this issue is cosmetic because the values are assumed to be within certain ranges. Nevertheless, we recommend accepting the slightly higher gas cost for SafeMath functions for consistency and to prevent potential issues.  6 Issues  The issues are presented in approximate order of priority from highest to lowest.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "6.1 Unchecked Specification requirement - token limit Closed", "body": "  Description  According to the Balancer Shared Pool Price Provider that was provided with the audit code-base the price provide must fulfill the following requirements:  Pool token price cannot be manipulated  Chainlink will be used as the main oracle  It should use as less gas as possible  Limited to Balancer s shared pools where the weights cannot be changed  Limited to a pool containing 2 to 3 tokens  However, the constructor of the price provider does not enforce the limit of 2 to 3 tokens.  Examples  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L38-L63  constructor(  BPool _pool,  bool[] memory _isPeggedToEth,  uint8[] memory _decimals,  IPriceOracle _priceOracle,  uint256 _priceDeviation,  uint256 _K,  uint256 _powerPrecision,  uint256[][] memory _approximationMatrix  ) public {  pool = _pool;  //Get token list  tokens = pool.getFinalTokens(); //This already checks for pool finalized  //Get token normalized weights  uint256 length = tokens.length;  for (uint8 i = 0; i < length; i++) {  weights.push(pool.getNormalizedWeight(tokens[i]));  isPeggedToEth = _isPeggedToEth;  decimals = _decimals;  priceOracle = _priceOracle;  priceDeviation = _priceDeviation;  K = _K;  powerPrecision = _powerPrecision;  approximationMatrix = _approximationMatrix;  Recommendation  Require that the number of tokens returned by pool.getFinalTokens() is 2<= len <=3.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "6.2 Integer underflow if a token specifies more than 18 decimals Closed", "body": "  Description  Decimals are provided by the account deploying the price provider contract. In getEthBalanceByToken the assumption is made that decimals[index] is less or equal to 18 decimals, however, the deployer may provide decimals that are not within normal operating bounds. Contract creation succeeds, while the contract is not viable.  Examples  The value underflows if the contract is used with a token decimals > 18.  Balancer  code/aave-balancer-3e8367ab/contracts/proxies/BalancerSharedPoolPriceProvider.sol:L69-L78  function getEthBalanceByToken(uint256 index)  internal  view  returns (uint256)  uint256 pi = isPeggedToEth[index]  ? BONE  : uint256(priceOracle.getAssetPrice(tokens[index]));  require(pi > 0, \"ERR_NO_ORACLE_PRICE\");  uint256 missingDecimals = 18 - decimals[index];  Uniswapv2  code/aave-uniswapv2-e81cf872/contracts/proxies/UniswapV2PriceProvider.sol:L57-L66  function getEthBalanceByToken(uint256 index, uint112 reserve)  internal  view  returns (uint256)  uint256 pi = isPeggedToEth[index]  ? Math.BONE  : uint256(priceOracle.getAssetPrice(tokens[index]));  require(pi > 0, \"ERR_NO_ORACLE_PRICE\");  uint256 missingDecimals = 18 - decimals[index];  Recommendation  Add a check to the constructor to ensure that none of the provided decimals is greater than 18.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/08/aave-balancer-and-uniswap-v2-price-providers/"}, {"title": "5.1 Warning about ERC20 handling function", "body": "  Description  There is something worth bringing up for discussion in the ERC20 disbursement function.  code/BitwaveMultiSend.sol:L55  assert(token.transferFrom(msg.sender, _to[i], _value[i]) == true);  In the above presented line, the external call is being compared to a truthful boolean. And, even though, this is clearly part of the ERC20 specification there have historically been cases where tokens with sizeable market caps and liquidity have erroneously not implemented return values in any of the transfer functions.  The question presents itself as to whether these non-ERC20-conforming tokens are meant to be supported or not.  The audit team believes that the purpose of this smart contract is to disburse OXT tokens and therefore, since its development was under the umbrella of the Orchid team, absolutely no security concerns should arise from this issue.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.2 Discussion on the permissioning of send functions", "body": "  Description  Since the disbursement of funds is all made atomically (i.e., the Ether funds held by the smart contract are transient) there is no need to permission the function with the restrictedToOwner modifier.  Even in the case of ERC20 tokens, there is no need to permission the function since the smart contract can only spend allowance attributed to it by the caller (msg.sender).  This being said there is value in permissioning this contract, specifically if attribution of the deposited funds in readily available tools like Etherscan is important. Because turning this into a publicly available tool for batch sends of Ether and ERC20 tokens would mean that someone could wrongly attribute some disbursement to Orchid Labs should they be ignorant to this fact.  A possible solution to this problem would be the usage of events to properly attribute the disbursements but it is, indeed, an additional burden to carefully analyse these for proper attribution.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.3 Improve function visibility ", "body": "  Description  The following methods are not called internally in the token contract and visibility can, therefore, be restricted to external rather than public. This is more gas efficient because less code is emitted and data does not need to be copied into memory. It also makes functions a bit simpler to reason about because there s no need to worry about the possibility of internal calls.  BitwaveMultiSend.sendEth()  BitwaveMultiSend.sendErc20()  Recommendation  Change visibility of these methods to external.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.4 Ether send function remainder handling ", "body": "  Description  The Ether send function depicted below implements logic to reimburse the sender if an extraneous amount is left in the contract after the disbursement.  code/BitwaveMultiSend.sol:L22-L43  function sendEth(address payable [] memory _to, uint256[] memory _value) public restrictedToOwner payable returns (bool _success) {  // input validation  require(_to.length == _value.length);  require(_to.length <= 255);  // count values for refunding sender  uint256 beforeValue = msg.value;  uint256 afterValue = 0;  // loop through to addresses and send value  for (uint8 i = 0; i < _to.length; i++) {  afterValue = afterValue.add(_value[i]);  assert(_to[i].send(_value[i]));  // send back remaining value to sender  uint256 remainingValue = beforeValue.sub(afterValue);  if (remainingValue > 0) {  assert(msg.sender.send(remainingValue));  return true;  It is also the only place where the SafeMath dependency is being used. More specifically to check there was no underflow in the arithmetic adding up the disbursed amounts.  However, since the individual sends would revert themselves should more Ether than what was available in the balance be specified these protection measures seem unnecessary.  Not only the above is true but the current codebase does not allow to take funds locked within the contract out in the off chance someone forced funds into this smart contract (e.g., by self-destructing some other smart contract containing funds into this one).  Recommendation  The easiest way to handle both retiring SafeMath and returning locked funds would be to phase out all the intra-function arithmetic and just transferring address(this).balance to msg.sender at the end of the disbursement. Since all the funds in there are meant to be from the caller of the function this serves the purpose of returning extraneous funds to him well and, adding to that, it allows for some front-running fun if someone  self-destructed  funds to this smart contract by mistake.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.5 Unneeded type cast of contract type ", "body": "  Description  The typecast being done on the address parameter in the lien below is unneeded.  code/BitwaveMultiSend.sol:L51  ERC20 token = ERC20(_tokenAddress);  Recommendation  Assign the right type at the function parameter definition like so:  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.6 Inadequate use of assert ", "body": "  Description  The usage of require vs assert has always been a matter of discussion because of the fine lines distinguishing these transaction-terminating expressions.  However, the usage of the assert syntax in this case is not the most appropriate.  Borrowing the explanation from the latest solidity docs (v. https://solidity.readthedocs.io/en/latest/control-structures.html#id4) :  Since assert-style exceptions (using the 0xfe opcode) consume all gas available to the call and require-style ones (using the 0xfd opcode) do not since the Metropolis release when the REVERT instruction was added, the usage of require in the lines depicted in the examples section would only result in gas savings and the same security assumptions.  In this case, even though the calls are being made to external contracts the supposedly abide to a predefined specification, this is by no means an invariant of the presented system since the component is external to the built system and its integrity cannot be formally verified.  Examples  code/BitwaveMultiSend.sol:L34  assert(_to[i].send(_value[i]));  code/BitwaveMultiSend.sol:L40  assert(msg.sender.send(remainingValue));  code/BitwaveMultiSend.sol:L55  assert(token.transferFrom(msg.sender, _to[i], _value[i]) == true);  Recommendation  Exchange the assert statements for require ones.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "6.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The output of a MythX Full Mode analysis was reviewed by the audit team and no relevant issues were raised as part of the process.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "6.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  code/BitwaveMultiSend.sol  3:7      error      \"./ERC20.sol\": Import statements must use double quotes only.                              quotes  22:16    error      There should be no whitespace between \"address payable\" and the opening square bracket.    array-declarations  24:8     warning    Provide an error message for require()                                                     error-reason  25:8     warning    Provide an error message for require()                                                     error-reason  34:19    warning    Consider using 'transfer' in place of 'send'.                                              security/no-send  40:19    warning    Consider using 'transfer' in place of 'send'.                                              security/no-send  47:8     warning    Provide an error message for require()                                                     error-reason  48:8     warning    Provide an error message for require()                                                     error-reason  \u2716 2 errors, 6 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "6.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Function Name  Visibility  Mutability  Modifiers  BitwaveMultiSend  Implementation  Public    NO   sendEth  Public    restrictedToOwner  sendErc20  Public    restrictedToOwner  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/orchid-bitwavemultisend/"}, {"title": "5.1 Anyone can remove a maker s pending pool join status    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250 by removing the two-step handshake for a maker to join a pool.  Description  Using behavior described in issue 5.6, it is possible to delete the pending join status of any maker in any pool by passing in NIL_POOL_ID to removeMakerFromStakingPool. Note that the attacker in the following example must not be a confirmed member of any pool:  The attacker calls addMakerToStakingPool(NIL_POOL_ID, makerAddress). In this case, makerAddress can be almost any address, as long as it has not called joinStakingPoolAsMaker (an easy example is address(0)). The key goal of this call is to increment the number of makers in pool 0: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L262 _poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();  The attacker calls removeMakerFromStakingPool(NIL_POOL_ID, targetAddress). This function queries getStakingPoolIdOfMaker(targetAddress) and compares it to the passed-in pool id. Because the target is an unconfirmed maker, their staking pool id is NIL_POOL_ID: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L166-L173 bytes32 makerPoolId = getStakingPoolIdOfMaker(makerAddress); if (makerPoolId != poolId) {     LibRichErrors.rrevert(LibStakingRichErrors.MakerPoolAssignmentError(         LibStakingRichErrors.MakerPoolAssignmentErrorCodes.MakerAddressNotRegistered,         makerAddress,         makerPoolId     )); }  The check passes, and the target s _poolJoinedByMakerAddress struct is deleted. Additionally, the number of makers in pool 0 is decreased:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177  delete _poolJoinedByMakerAddress[makerAddress];  _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  This can be used to prevent any makers from being confirmed into a pool.  Recommendation  See issue 5.6.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.2 Delegated stake weight reduction can be bypassed by using an external contract   ", "body": "  Resolution  From the development team:  Although it is possible to bypass the weight reduction via external smart contracts, we believe there is some value to having a lower delegated stake weight as the default behavior. This can still approximate the intended behavior and should give a very slight edge to pool operators that own their stake.  Description  Staking pools allow ZRX holders to delegate their staked ZRX to a market maker in exchange for a configurable percentage of the stake reward (accrued over time through exchange fees). When staking as expected through the 0x contracts, the protocol favors ZRX staked directly by the operator of the pool, assigning a lower weight (90%) to ZRX staked by delegation. In return, delegated members receive a configurable portion of the operator s stake reward.  Using a smart contract, it is possible to represent ZRX owned by any number of parties as ZRX staked by a single party. This contract can serve as the operator of a pool with a single member\u2014itself. The advantages are clear for ZRX holders:  ZRX staked through this contract will be given full (100%) stake weight.  Because stake weight is a factor in reward allocation, the ZRX staked through this contract receives a higher proportion of the stake reward.  Recommendation  Remove stake weight reduction for delegated stake.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.3 MixinParams.setParams bypasses safety checks made by standard StakingProxy upgrade path.    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2279. Now the parameter validity is asserted in  Description  The staking contracts use a set of configurable parameters to determine the behavior of various parts of the system. The parameters dictate the duration of epochs, the ratio of delegated stake weight vs operator stake, the minimum pool stake, and the Cobb-Douglas numerator and denominator. These parameters can be configured in two ways:  An authorized address can deploy a new Staking contract (perhaps with altered parameters), and configure the StakingProxy to delegate to this new contract. This is done by calling   StakingProxy.detachStakingContract: code/contracts/staking/contracts/src/StakingProxy.sol:L82-L90 /// @dev Detach the current staking contract. /// Note that this is callable only by an authorized address. function detachStakingContract()     external     onlyAuthorized {     stakingContract = NIL_ADDRESS;     emit StakingContractDetachedFromProxy(); }   StakingProxy.attachStakingContract(newContract): code/contracts/staking/contracts/src/StakingProxy.sol:L72-L80 /// @dev Attach a staking contract; future calls will be delegated to the staking contract. /// Note that this is callable only by an authorized address. /// @param _stakingContract Address of staking contract. function attachStakingContract(address _stakingContract)     external     onlyAuthorized {     _attachStakingContract(_stakingContract); }   During the latter call, the StakingProxy performs a delegatecall to Staking.init, then checks the values of the parameters set during initialization: code/contracts/staking/contracts/src/StakingProxy.sol:L208-L219 // Call `init()` on the staking contract to initialize storage. (bool didInitSucceed, bytes memory initReturnData) = stakingContract.delegatecall(     abi.encodeWithSelector(IStorageInit(0).init.selector) ); if (!didInitSucceed) {     assembly {         revert(add(initReturnData, 0x20), mload(initReturnData))     } }  // Assert initialized storage values are valid _assertValidStorageParams();  An authorized address can call MixinParams.setParams at any time and set the contract s parameters to arbitrary values.  The latter method introduces the possibility of setting unsafe or nonsensical values for the contract parameters: epochDurationInSeconds can be set to 0, cobbDouglassAlphaNumerator can be larger than cobbDouglassAlphaDenominator, rewardDelegatedStakeWeight can be set to a value over 100% of the staking reward, and more.  Note, too, that by using MixinParams.setParams to set all parameters to 0, the Staking contract can be re-initialized by way of Staking.init. Additionally, it can be re-attached by way of StakingProxy.attachStakingContract, as the delegatecall to Staking.init will succeed.  Recommendation  Ensure that calls to setParams check that the provided values are within the same range currently enforced by the proxy.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.4 Authorized addresses can indefinitely stall ZrxVaultBackstop catastrophic failure mode    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2295 by removing the  Description  The ZrxVaultBackstop contract was added to allow anyone to activate the staking system s  catastrophic failure  mode if the StakingProxy is in  read-only  mode for at least 40 days. To enable this behavior, the StakingProxy contract was modified to track the last timestamp at which  read-only  mode was activated. This is done by way of StakingProxy.setReadOnlyMode:  code/contracts/staking/contracts/src/StakingProxy.sol:L92-L104  /// @dev Set read-only mode (state cannot be changed).  function setReadOnlyMode(bool shouldSetReadOnlyMode)  external  onlyAuthorized  // solhint-disable-next-line not-rely-on-time  uint96 timestamp = block.timestamp.downcastToUint96();  if (shouldSetReadOnlyMode) {  stakingContract = readOnlyProxy;  readOnlyState = IStructs.ReadOnlyState({  isReadOnlyModeSet: true,  lastSetTimestamp: timestamp  });  Because the timestamp is updated even if  read-only  mode is already active, any authorized address can prevent ZrxVaultBackstop from activating catastrophic failure mode by repeatedly calling setReadOnlyMode.  Recommendation  If  read-only  mode is already active, setReadOnlyMode(true) should result in a no-op.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.5 Pool 0 can be used to temporarily prevent makers from joining another pool    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1.  Description  removeMakerFromStakingPool reverts if the number of makers currently in the pool is 0, due to safeSub catching an underflow:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L177  _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  Because of this, edge behavior described in issue 5.6 can allow an attacker to temporarily prevent makers from joining a pool:  The attacker calls addMakerToStakingPool(NIL_POOL_ID, victimAddress). This sets the victim s MakerPoolJoinStatus.confirmed field to true and increases the number of makers in pool 0 to 1: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L257-L262 poolJoinStatus = IStructs.MakerPoolJoinStatus({     poolId: poolId,     confirmed: true }); _poolJoinedByMakerAddress[makerAddress] = poolJoinStatus; _poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();  The attacker calls removeMakerFromStakingPool(NIL_POOL_ID, randomAddress). The net effect of this call simply decreases the number of makers in pool 0 by 1, back to 0: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177 delete _poolJoinedByMakerAddress[makerAddress]; _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  Typically, the victim should be able to remove themselves from pool 0 by calling removeMakerFromStakingPool(NIL_POOL_ID, victimAddress), but because the attacker can set the pool s number of makers to 0, the aforementioned underflow causes this call to fail. The victim must first understand what is happening in MixinStakingPool before they are able to remedy the situation:  The victim must call addMakerToStakingPool(NIL_POOL_ID, randomAddress2) to increase pool 0 s number of makers back to 1.  The victim can now call removeMakerFromStakingPool(NIL_POOL_ID, victimAddress), and remove their confirmed status.  Additionally, if the victim in question currently has a pending join, the attacker can use issue 5.1 to first remove their pending status before locking them in pool 0.  Recommendation  See issue 5.1.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.6 Recommendation: Fix weak assertions in MixinStakingPool stemming from use of NIL_POOL_ID    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1.  Description  The modifier onlyStakingPoolOperatorOrMaker(poolId) is used to authorize actions taken on a given pool. The sender must be either the operator or a confirmed maker of the pool in question. However, the modifier queries getStakingPoolIdOfMaker(maker), which returns NIL_POOL_ID if the maker s MakerPoolJoinStatus struct is not confirmed. This implicitly makes anyone a maker of the nonexistent  pool 0 :  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L189-L200  function getStakingPoolIdOfMaker(address makerAddress)  public  view  returns (bytes32)  IStructs.MakerPoolJoinStatus memory poolJoinStatus = _poolJoinedByMakerAddress[makerAddress];  if (poolJoinStatus.confirmed) {  return poolJoinStatus.poolId;  } else {  return NIL_POOL_ID;  joinStakingPoolAsMaker(poolId) makes no existence checks on the provided pool id, and allows makers to become pending makers in nonexistent pools.  addMakerToStakingPool(poolId, maker) makes no existence checks on the provided pool id, allowing makers to be added to nonexistent pools (as long as the sender is an operator or maker in the pool).  Recommendation  Avoid use of 0x00...00 for NIL_POOL_ID. Instead, use 2**256 - 1.  Implement stronger checks for pool existence. Each time a pool id is supplied, it should be checked that the pool id is between 0 and nextPoolId.  onlyStakingPoolOperatorOrMaker should revert if poolId == NIL_POOL_ID or if poolId is not in the valid range: (0, nextPoolId).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.7 LibMath functions fail to catch a number of overflows    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2255 and  0xProject/0x-monorepo#2311.  Description  The __add(), __mul(), and __div() functions perform arithmetic on 256-bit signed integers, and they all miss some specific overflows.  Addition Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L359-L376  /// @dev Adds two numbers, reverting on overflow.  function _add(int256 a, int256 b) private pure returns (int256 c) {  c = a + b;  if (c > 0 && a < 0 && b < 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.SUBTRACTION_OVERFLOW,  a,  ));  if (c < 0 && a > 0 && b > 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.ADDITION_OVERFLOW,  a,  ));  The two overflow conditions it tests for are:  Adding two positive numbers shouldn t result in a negative number.  Adding two negative numbers shouldn t result in a positive number.  __add(-2**255, -2**255) returns 0 without reverting because the overflow didn t match either of the above conditions.  Multiplication Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L332-L345  /// @dev Returns the multiplication two numbers, reverting on overflow.  function _mul(int256 a, int256 b) private pure returns (int256 c) {  if (a == 0) {  return 0;  c = a * b;  if (c / a != b) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.MULTIPLICATION_OVERFLOW,  a,  ));  The function checks via division for most types of overflows, but it fails to catch one particular case. __mul(-2**255, -1) returns -2**255 without error.  Division Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L347-L357  /// @dev Returns the division of two numbers, reverting on division by zero.  function _div(int256 a, int256 b) private pure returns (int256 c) {  if (b == 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.DIVISION_BY_ZERO,  a,  ));  c = a / b;  It does not check for overflow. Due to this, __div(-2**255, -1) erroneously returns -2**255.  Recommendation  For addition, the specific case of __add(-2**255, -2**255) can be detected by using a >= 0 check instead of > 0, but the below seems like a clearer check for all cases:  // if b is negative, then the result should be less than a  if (b < 0 && c >= a) { /* subtraction overflow */ }  // if b is positive, then the result should be greater than a  if (b > 0 && c <= a) { /* addition overflow */ }  For multiplication and division, the specific values of -2**255 and -1 are the only missing cases, so that can be explicitly checked in the __mul() and __div() functions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.8 Recommendation: Remove MixinAbstract and fold MixinStakingPoolRewards into MixinFinalizer and MixinStake   ", "body": "  Resolution  The development team investigated this suggestion, but they were ultimately uncomfortable making such a large change in this cycle. This can be considered again in a future version of the code.  Description  issue 5.12,  issue 5.11,  issue 5.10, and  issue 5.9,  Move MixinStakingPoolRewards.withdrawDelegatorRewards into MixinStake. As per the comments above this function, its behavior is very similar to functions in MixinStake: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L35-L56 /// @dev Syncs rewards for a delegator. This includes transferring WETH ///      rewards to the delegator, and adding/removing ///      dependencies on cumulative rewards. ///      This is used by a delegator when they want to sync their rewards ///      without delegating/undelegating. It's effectively the same as ///      delegating zero stake. /// @param poolId Unique id of pool. function withdrawDelegatorRewards(bytes32 poolId)     external {     address member = msg.sender;      _withdrawAndSyncDelegatorRewards(         poolId,         member     );      // Update stored balance with synchronized version; this prevents     // redundant withdrawals.     _delegatedStakeToPoolByOwner[member][poolId] =         _loadSyncedBalance(_delegatedStakeToPoolByOwner[member][poolId]); }  Move the rest of the MixinStakingPoolRewards functions into MixinFinalizer. This change allows the MixinStakingPoolRewards and MixinAbstract files to be removed. MixinStakingPool can now inherit directly from MixinFinalizer.  After implementing all recommendations mentioned here, the inheritance graph of the staking contracts is much simpler. The previous graph is pictured here:  The new graph is pictured here:  Further improvements may consider:  Having MixinStorage inherit MixinConstants and IStakingEvents  Moving _loadCurrentBalance into MixinStorage. Currently MixinStakeBalances only inherits from MixinStakeStorage because of this function.  After implementing the above, MixinExchangeFees is no longer dependent on MixinStakingPool and can inherit directly from MixinExchangeManager  A sample inheritance graph including the above is pictured below:  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.9 Recommendation: remove confusing access to activePoolsThisEpoch    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2276. Along with other state cleanup, these functions and  Description  MixinFinalizer provides two functions to access activePoolsThisEpoch:  _getActivePoolsFromEpoch returns a storage pointer to the mapping: code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L211-L225 /// @dev Get a mapping of active pools from an epoch. ///      This uses the formula `epoch % 2` as the epoch index in order ///      to reuse state, because we only need to remember, at most, two ///      epochs at once. /// @return activePools The pools that were active in `epoch`. function _getActivePoolsFromEpoch(     uint256 epoch )     internal     view     returns (mapping (bytes32 => IStructs.ActivePool) storage activePools) {     activePools = _activePoolsByEpoch[epoch % 2];     return activePools; }  _getActivePoolFromEpoch invokes _getActivePoolsFromEpoch, then loads an ActivePool struct from a passed-in poolId: code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L195-L209 /// @dev Get an active pool from an epoch by its ID. /// @param epoch The epoch the pool was/will be active in. /// @param poolId The ID of the pool. /// @return pool The pool with ID `poolId` that was active in `epoch`. function _getActivePoolFromEpoch(     uint256 epoch,     bytes32 poolId )     internal     view     returns (IStructs.ActivePool memory pool) {     pool = _getActivePoolsFromEpoch(epoch)[poolId];     return pool; }  Ultimately, the two functions are syntax sugar for activePoolsThisEpoch[epoch % 2], with the latter also accessing a value within the mapping. Because of the naming similarity, and because one calls the other, this abstraction is more confusing that simply accessing the state variable directly.  Additionally, by removing these functions and adopting the long-form syntax, MixinExchangeFees no longer needs to inherit MixinFinalizer.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.10 Recommendation: remove MixinFinalizer._getUnfinalizedPoolRewardsFromState   ", "body": "  Resolution  The development team decided to keep this function for its optimization on storage loads. It s will still be used internally by getters that are important for client-side code.  Description  MixinFinalizer._getUnfinalizedPoolRewardsFromState is a simple wrapper around the library function LibCobbDouglas.cobbDouglas:  code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L250-L286  /// @dev Computes the reward owed to a pool during finalization.  /// @param pool The active pool.  /// @param state The current state of finalization.  /// @return rewards Unfinalized rewards for this pool.  function _getUnfinalizedPoolRewardsFromState(  IStructs.ActivePool memory pool,  IStructs.UnfinalizedState memory state  private  view  returns (uint256 rewards)  // There can't be any rewards if the pool was active or if it has  // no stake.  if (pool.feesCollected == 0) {  return rewards;  // Use the cobb-douglas function to compute the total reward.  rewards = LibCobbDouglas.cobbDouglas(  state.rewardsAvailable,  pool.feesCollected,  state.totalFeesCollected,  pool.weightedStake,  state.totalWeightedStake,  cobbDouglasAlphaNumerator,  cobbDouglasAlphaDenominator  );  // Clip the reward to always be under  // `rewardsAvailable - totalRewardsPaid`,  // in case cobb-douglas overflows, which should be unlikely.  uint256 rewardsRemaining = state.rewardsAvailable.safeSub(state.totalRewardsFinalized);  if (rewardsRemaining < rewards) {  rewards = rewardsRemaining;  After implementing issue 5.11, this function is only called a single time, in MixinFinalizer.finalizePool:  code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L119-L129  // Noop if the pool was not active or already finalized (has no fees).  if (pool.feesCollected == 0) {  return;  // Clear the pool state so we don't finalize it again, and to recoup  // some gas.  delete _getActivePoolsFromEpoch(prevEpoch)[poolId];  // Compute the rewards.  uint256 rewards = _getUnfinalizedPoolRewardsFromState(pool, state);  Because it is only used a single time, and because it obfuscates an essential library call during the finalization process, the function should be removed and folded into finalizePool. Additionally, the first check for pool.feesCollected == 0 can be removed, as this case is covered in finalizePool already (see above).  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.11 Recommendation: remove complicating getters from MixinStakingPoolRewards   ", "body": "  Resolution  These getters are useful for client-side code, such as the staking interface.  Description  MixinStakingPoolRewards has two external view functions that contribute complexity to essential functions, as well as the overall inheritance tree:  computeRewardBalanceOfOperator, used to compute the reward balance of a pool s operator on an unfinalized pool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L55-L69 /// @dev Computes the reward balance in ETH of the operator of a pool. /// @param poolId Unique id of pool. /// @return totalReward Balance in ETH. function computeRewardBalanceOfOperator(bytes32 poolId)     external     view     returns (uint256 reward) {     // Because operator rewards are immediately withdrawn as WETH     // on finalization, the only factor in this function are unfinalized     // rewards.     IStructs.Pool memory pool = _poolById[poolId];     // Get any unfinalized rewards.     (uint256 unfinalizedTotalRewards, uint256 unfinalizedMembersStake) =         _getUnfinalizedPoolRewards(poolId);  computeRewardBalanceOfDelegator, used to compute the reward balance of a delegator for an unfinalized pool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L80-L99 /// @dev Computes the reward balance in ETH of a specific member of a pool. /// @param poolId Unique id of pool. /// @param member The member of the pool. /// @return totalReward Balance in ETH. function computeRewardBalanceOfDelegator(bytes32 poolId, address member)     external     view     returns (uint256 reward) {     IStructs.Pool memory pool = _poolById[poolId];     // Get any unfinalized rewards.     (uint256 unfinalizedTotalRewards, uint256 unfinalizedMembersStake) =         _getUnfinalizedPoolRewards(poolId);      // Get the members' portion.     (, uint256 unfinalizedMembersReward) = _computePoolRewardsSplit(         pool.operatorShare,         unfinalizedTotalRewards,         unfinalizedMembersStake     );  These two functions are the sole reason for the existence of MixinFinalizer._getUnfinalizedPoolRewards, one of the two functions in MixinAbstract:  code/contracts/staking/contracts/src/sys/MixinAbstract.sol:L40-L52  /// @dev Computes the reward owed to a pool during finalization.  ///      Does nothing if the pool is already finalized.  /// @param poolId The pool's ID.  /// @return totalReward The total reward owed to a pool.  /// @return membersStake The total stake for all non-operator members in  ///         this pool.  function _getUnfinalizedPoolRewards(bytes32 poolId)  internal  view  returns (  uint256 totalReward,  uint256 membersStake  );  These functions also necessitate two additional parameters in MixinStakingPoolRewards._computeDelegatorReward, which are used a single time to call _computeUnfinalizedDelegatorReward:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L253-L259  // 1/3 Unfinalized rewards earned in `currentEpoch - 1`.  reward = _computeUnfinalizedDelegatorReward(  delegatedStake,  _currentEpoch,  unfinalizedMembersReward,  unfinalizedMembersStake  );  By removing the functions computeRewardBalanceOfOperator and computeRewardBalanceOfDelegator, the following simplifications can be made:  _getUnfinalizedPoolRewards can be removed from both MixinAbstract and MixinFinalizer  The parameters unfinalizedMembersReward and unfinalizedMembersStake can be removed from _computeDelegatorReward  The function _computeUnfinalizedDelegatorReward can be removed  A branch of now-unused logic in _computeDelegatorReward can be removed  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.12 Recommendation: remove unneeded dependency on MixinStakeBalances   ", "body": "  Resolution  From the development team:  We re going to keep this abstraction to future-proof balance queries.  Description  MixinStakeBalances has two functions used by inheriting contracts:  getStakeDelegatedToPoolByOwner, which provides shorthand to access _delegatedStakeToPoolByOwner: code/contracts/staking/contracts/src/stake/MixinStakeBalances.sol:L84-L95 /// @dev Returns the stake delegated to a specific staking pool, by a given staker. /// @param staker of stake. /// @param poolId Unique Id of pool. /// @return Stake delegated to pool by staker. function getStakeDelegatedToPoolByOwner(address staker, bytes32 poolId)     public     view     returns (IStructs.StoredBalance memory balance) {     balance = _loadCurrentBalance(_delegatedStakeToPoolByOwner[staker][poolId]);     return balance; }  getTotalStakeDelegatedToPool, which provides shorthand to access _delegatedStakeByPoolId: code/contracts/staking/contracts/src/stake/MixinStakeBalances.sol:L97-L108 /// @dev Returns the total stake delegated to a specific staking pool, ///      across all members. /// @param poolId Unique Id of pool. /// @return Total stake delegated to pool. function getTotalStakeDelegatedToPool(bytes32 poolId)     public     view     returns (IStructs.StoredBalance memory balance) {     balance = _loadCurrentBalance(_delegatedStakeByPoolId[poolId]);     return balance; }  Each of these functions is used only a single time:  MixinExchangeFees.payProtocolFee: code/contracts/staking/contracts/src/fees/MixinExchangeFees.sol:L78 uint256 poolStake = getTotalStakeDelegatedToPool(poolId).currentEpochBalance;  MixinExchangeFees._computeMembersAndWeightedStake: code/contracts/staking/contracts/src/fees/MixinExchangeFees.sol:L143-L146 uint256 operatorStake = getStakeDelegatedToPoolByOwner(     _poolById[poolId].operator,     poolId ).currentEpochBalance;  By replacing these function invocations in MixinExchangeFees with the long-form access to each state variable, MixinStakeBalances will no longer need to be included in the inheritance trees for several contracts.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.13 Misleading MoveStake event when moving stake from UNDELEGATED to UNDELEGATED    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2280. If  Description  Although moving stake between the same status (UNDELEGATED <=> UNDELEGATED) should be a no-op, calls to moveStake succeed even for invalid amount and nonsensical poolId. The resulting MoveStake event can log garbage, potentially confusing those observing events.  Examples  When moving between UNDELEGATED and UNDELEGATED, each check and function call results in a no-op, save the final event:  Neither from nor to are StakeStatus.DELEGATED, so these checks are passed: code/contracts/staking/contracts/src/stake/MixinStake.sol:L115-L129 if (from.status == IStructs.StakeStatus.DELEGATED) {     _undelegateStake(         from.poolId,         staker,         amount     ); }  if (to.status == IStructs.StakeStatus.DELEGATED) {     _delegateStake(         to.poolId,         staker,         amount     ); }  The primary state changing function, _moveStake, immediately returns because the from and to balance pointers are equivalent: code/contracts/staking/contracts/src/stake/MixinStakeStorage.sol:L47-L49 if (_arePointersEqual(fromPtr, toPtr)) {     return; }  Finally, the MoveStake event is invoked, which can log completely invalid values for amount, from.poolId, and to.poolId: code/contracts/staking/contracts/src/stake/MixinStake.sol:L141-L148 emit MoveStake(     staker,     amount,     uint8(from.status),     from.poolId,     uint8(to.status),     to.poolId );  Recommendation  If amount is 0 or if moving between UNDELEGATED and UNDELEGATED, this function should no-op or revert. An explicit check for this case should be made near the start of the function.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.14 The staking contracts contain several artifacts of a quickly-changing codebase    ", "body": "  Resolution   These issues were addressed in a variety of fixes, most notably   0xProject/0x-monorepo#2262.  Examples  address payable is used repeatedly, but payments use WETH:   MixinStakingPool.createStakingPool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L54 address payable operator = msg.sender;   ZrxVault.stakingProxyAddress: code/contracts/staking/contracts/src/ZrxVault.sol:L38 address payable public stakingProxyAddress;   ZrxVault.setStakingProxy: code/contracts/staking/contracts/src/ZrxVault.sol:L76 function setStakingProxy(address payable _stakingProxyAddress)   IZrxVault.setStakingProxy: code/contracts/staking/contracts/src/interfaces/IZrxVault.sol:L53 function setStakingProxy(address payable _stakingProxyAddress)   struct IStructs.Pool: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L114 address payable operator;   MixinStake.stake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L38 address payable staker = msg.sender;   MixinStake.unstake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L63 address payable staker = msg.sender;   MixinStake.moveStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L119 address payable staker = msg.sender;   MixinStake._delegateStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L181 address payable staker,   MixinStake._undelegateStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L210 address payable staker,  Some identifiers are used multiple times for different purposes:  currentEpoch is:   A state variable: code/contracts/staking/contracts/src/immutable/MixinStorage.sol:L86 uint256 public currentEpoch = INITIAL_EPOCH;   A function parameter: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L323 uint256 currentEpoch,   A struct field: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L62 uint32 currentEpoch;  Several comments are out of date:   Many struct comments reference fees and rewards denominated in ETH, while only WETH is used: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L36-L38 /// @param rewardsAvailable Rewards (ETH) available to the epoch ///        being finalized (the previous epoch). This is simply the balance ///        of the contract at the end of the epoch.   UnfinalizedState.totalFeesCollected should specify that it is tracking fees attributed to a pool. Fees not attributed to a pool are still collected, but are not recorded: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L41 /// @param totalFeesCollected The total fees collected for the epoch being finalized.   UnfinalizedState.totalWeightedStake is copy-pasted from totalFeesCollected: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L42 /// @param totalWeightedStake The total fees collected for the epoch being finalized.   Pool.initialized seems to be copy-pasted from an older version of the struct StoredBalance or StakeBalance: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L108 /// @param initialized True iff the balance struct is initialized.  The final contracts produce several compiler warnings:   Several functions are intentionally marked view to allow overriding implementations to read from state. These can be silenced by adding block.timestamp; or similar statements to the functions.   One function is erroneously marked view, and should be changed to pure: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L315-L330 /// @dev Computes the unfinalized rewards earned by a delegator in the last epoch. /// @param unsyncedStake Unsynced delegated stake to pool by staker /// @param currentEpoch The epoch in which this call is executing /// @param unfinalizedMembersReward Unfinalized total members reward (if any). /// @param unfinalizedMembersStake Unfinalized total members stake (if any). /// @return reward Balance in WETH. function _computeUnfinalizedDelegatorReward(     IStructs.StoredBalance memory unsyncedStake,     uint256 currentEpoch,     uint256 unfinalizedMembersReward,     uint256 unfinalizedMembersStake )     private     view     returns (uint256) {  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.15 Remove unneeded fields from StoredBalance and Pool structs    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2248. As part of a larger refactor, these fields were removed.  Description  Both structs have fields that are only written to, and never read:  StoredBalance.isInitialized: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L61 bool isInitialized;  Pool.initialized: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L113 bool initialized;  Recommendation  The unused fields should be removed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.16 Remove unnecessary fallback function in Staking contract    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2277.  Description  The Staking contract has a payable fallback function that is never used. Because it is used with a proxy contract, this pattern introduces silent failures when calls are made to the contract with no matching function selector.  Recommendation  Remove the fallback function from Staking.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.17 Pool IDs can just be incrementing integers    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1 and increment by 1 each time.  Description  Pool IDs are currently bytes32 values that increment by 2**128. After discussion with the development team, it seems that this was in preparation for a feature that was ultimately not used. Pool IDs should instead just be incrementing integers.  Examples  code/contracts/staking/contracts/src/immutable/MixinConstants.sol:L30-L34  // The upper 16 bytes represent the pool id, so this would be pool id 1. See MixinStakinPool for more information.  bytes32 constant internal INITIAL_POOL_ID = 0x0000000000000000000000000000000100000000000000000000000000000000;  // The upper 16 bytes represent the pool id, so this would be an increment of 1. See MixinStakinPool for more information.  uint256 constant internal POOL_ID_INCREMENT_AMOUNT = 0x0000000000000000000000000000000100000000000000000000000000000000;  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L271-L280  /// @dev Computes the unique id that comes after the input pool id.  /// @param poolId Unique id of pool.  /// @return Next pool id after input pool.  function _computeNextStakingPoolId(bytes32 poolId)  internal  pure  returns (bytes32)  return bytes32(uint256(poolId).safeAdd(POOL_ID_INCREMENT_AMOUNT));  Recommendation  Make pool IDs uint256 values and simply add 1 to generate the next ID.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.18 LibProxy.proxyCall() may overwrite important memory    ", "body": "  Resolution   This is fixed in   0xProject/0x-monorepo#2301. This function has been rewritten in Solidity and now avoids manual memory management.  Description  LibProxy.proxyCall() copies from call data to memory, starting at address 0:  code/contracts/staking/contracts/src/libs/LibProxy.sol:L52-L71  assembly {  // store selector of destination function  let freeMemPtr := 0  if gt(customEgressSelector, 0) {  mstore(0x0, customEgressSelector)  freeMemPtr := add(freeMemPtr, 4)  // adjust the calldata offset, if we should ignore the selector  let calldataOffset := 0  if gt(ignoreIngressSelector, 0) {  calldataOffset := 4  // copy calldata to memory  calldatacopy(  freeMemPtr,  calldataOffset,  calldatasize()  The first 64 bytes of memory are treated as  scratch space  by the Solidity compiler. Writing beyond that point is dangerous, as it will overwrite the free memory pointer and the  zero slot  which is where length-0 arrays point.  Although the current callers of proxyCall() don t appear to use any memory after calling proxyCall(), future changes to the code may introduce very serious and subtle bugs due to this unsafe handling of memory.  Recommendation  Use the actual free memory pointer to determine where it s safe to write to memory.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "6.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The full set of MythX results for both the exchange and staking contracts are available in a separate report.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "6.2 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  S\u016brya s Description Report  Files Description Table  ReadOnlyProxy.sol  6ec64526446ebff87ec5528ee3b2786338cc4fa0  Staking.sol  67ddcb9ab75e433882e28d9186815990b7084c61  StakingProxy.sol  248f562d014d0b1ca6de3212966af3e52a7deef1  ZrxVault.sol  6c3249314868a2f5d0984122e8ab1413a5b521c9  fees/MixinExchangeFees.sol  9ac3b696baa8ba09305cfc83d3c08f17d9d528e1  fees/MixinExchangeManager.sol  46f48136a49919cdb5588dc1b3d64c977c3367f2  immutable/MixinConstants.sol  97c2ac83ef97a09cfd485cb0d4b119ba0902cc79  immutable/MixinDeploymentConstants.sol  424f22c45df8e494c4a78f239ea07ff0400d694b  immutable/MixinStorage.sol  8ad475b0e424e7a3ff65eedf2e999cba98f414c8  interfaces/IStaking.sol  ec1d7f214e3fd40e14716de412deee9769359bc0  interfaces/IStakingEvents.sol  25f16b814c4df9d2002316831c3f727d858456c4  interfaces/IStakingProxy.sol  02e35c6b51e08235b2a01d30a8082d60d9d61bee  interfaces/IStorage.sol  eeaa798c262b46d1874e904cf7de0423d4132cee  interfaces/IStorageInit.sol  b9899b03e474ea5adc3b4818a4357f71b8d288d4  interfaces/IStructs.sol  fee17d036883d641afb1222b75eec8427f3cdb96  interfaces/IZrxVault.sol  9067154651675317e000cfa92de9741e50c1c809  libs/LibCobbDouglas.sol  242d62d71cf8bc09177d240c0db59b83f9bb4e96  libs/LibFixedMath.sol  36311e7be09a947fa4e6cd8c544cacd13d65833c  libs/LibFixedMathRichErrors.sol  39cb3e07bbce3272bbf090e87002d5834d288ec2  libs/LibProxy.sol  29abe52857a782c8da39b053cc54e02e295c1ae2  libs/LibSafeDowncast.sol  ae16ed2573d64802793320253b060b9507729c3d  libs/LibStakingRichErrors.sol  f5868ef6066a18277c932e59c0a516ec58920b00  stake/MixinStake.sol  ade59ed356fe72521ffd2ef12ff8896c852f11f8  stake/MixinStakeBalances.sol  cde6ca1a6200570ba18dd6d392ffabf68c2bb464  stake/MixinStakeStorage.sol  cadf34d9d341efd2a85dd13ec3cd4ce8383e0f73  staking_pools/MixinCumulativeRewards.sol  664ea3e35376c81492457dc17832a4d0d602c8ae  staking_pools/MixinStakingPool.sol  74ba9cb2db29b8dd6376d112e9452d117a391b18  staking_pools/MixinStakingPoolRewards.sol  a3b4e5c9b1c3568c94923e2dd9a93090ebdf8536  sys/MixinAbstract.sol  99fd4870c20d8fa03cfa30e8055d3dfb348ed5cd  sys/MixinFinalizer.sol  cc658ed07241c1804cec75b12203be3cd8657b9b  sys/MixinParams.sol  7b395f4da7ed787d7aa4eb915f15377725ff8168  sys/MixinScheduler.sol  2fab6b83a6f9e1d0dd1b1bdcea4b129d166aef1d  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  ReadOnlyProxy  Implementation  MixinStorage  <Fallback>  External    NO   revertDelegateCall  External    NO   Staking  Implementation  IStaking, MixinParams, MixinStake, MixinExchangeFees  <Fallback>  External    NO   init  Public    onlyAuthorized  StakingProxy  Implementation  IStakingProxy, MixinStorage  <Constructor>  Public    MixinStorage  <Fallback>  External    NO   attachStakingContract  External    onlyAuthorized  detachStakingContract  External    onlyAuthorized  setReadOnlyMode  External    onlyAuthorized  batchExecute  External    NO   _assertValidStorageParams  Internal \ud83d\udd12  _attachStakingContract  Internal \ud83d\udd12  ZrxVault  Implementation  Authorizable, IZrxVault  <Constructor>  Public    Authorizable  setStakingProxy  External    onlyAuthorized  enterCatastrophicFailure  External    onlyAuthorized  setZrxProxy  External    onlyAuthorized onlyNotInCatastrophicFailure  depositFrom  External    onlyStakingProxy onlyNotInCatastrophicFailure  withdrawFrom  External    onlyStakingProxy onlyNotInCatastrophicFailure  withdrawAllFrom  External    onlyInCatastrophicFailure  balanceOf  External    NO   _withdrawFrom  Internal \ud83d\udd12  _assertSenderIsStakingProxy  Private \ud83d\udd10  _assertInCatastrophicFailure  Private \ud83d\udd10  _assertNotInCatastrophicFailure  Private \ud83d\udd10  MixinExchangeFees  Implementation  MixinExchangeManager, MixinStakingPool, MixinFinalizer  payProtocolFee  External    onlyExchange  getActiveStakingPoolThisEpoch  External    NO   _computeMembersAndWeightedStake  Private \ud83d\udd10  _assertValidProtocolFee  Private \ud83d\udd10  MixinExchangeManager  Implementation  IStakingEvents, MixinStorage  addExchangeAddress  External    onlyAuthorized  removeExchangeAddress  External    onlyAuthorized  MixinConstants  Implementation  MixinDeploymentConstants  MixinDeploymentConstants  Implementation  getWethContract  Public    NO   getZrxVault  Public    NO   MixinStorage  Implementation  MixinConstants, Authorizable  IStaking  Interface  moveStake  External    NO   payProtocolFee  External    NO   stake  External    NO   IStakingEvents  Interface  IStakingProxy  Interface  <Fallback>  External    NO   attachStakingContract  External    NO   detachStakingContract  External    NO   IStorage  Interface  stakingContract  External    NO   readOnlyProxy  External    NO   readOnlyProxyCallee  External    NO   nextPoolId  External    NO   numMakersByPoolId  External    NO   currentEpoch  External    NO   currentEpochStartTimeInSeconds  External    NO   protocolFeesThisEpochByPool  External    NO   activePoolsThisEpoch  External    NO   validExchanges  External    NO   epochDurationInSeconds  External    NO   rewardDelegatedStakeWeight  External    NO   minimumPoolStake  External    NO   maximumMakersInPool  External    NO   cobbDouglasAlphaNumerator  External    NO   cobbDouglasAlphaDenominator  External    NO   IStorageInit  Interface  init  External    NO   IStructs  Interface  IZrxVault  Interface  setStakingProxy  External    NO   enterCatastrophicFailure  External    NO   setZrxProxy  External    NO   depositFrom  External    NO   withdrawFrom  External    NO   withdrawAllFrom  External    NO   balanceOf  External    NO   LibCobbDouglas  Library  cobbDouglas  Internal \ud83d\udd12  LibFixedMath  Library  one  Internal \ud83d\udd12  add  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  mulDiv  Internal \ud83d\udd12  uintMul  Internal \ud83d\udd12  abs  Internal \ud83d\udd12  invert  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toInteger  Internal \ud83d\udd12  ln  Internal \ud83d\udd12  exp  Internal \ud83d\udd12  _mul  Private \ud83d\udd10  _div  Private \ud83d\udd10  _add  Private \ud83d\udd10  LibFixedMathRichErrors  Library  SignedValueError  Internal \ud83d\udd12  UnsignedValueError  Internal \ud83d\udd12  BinOpError  Internal \ud83d\udd12  LibProxy  Library  proxyCall  Internal \ud83d\udd12  LibSafeDowncast  Library  downcastToUint96  Internal \ud83d\udd12  downcastToUint64  Internal \ud83d\udd12  downcastToUint32  Internal \ud83d\udd12  LibStakingRichErrors  Library  OnlyCallableByExchangeError  Internal \ud83d\udd12  ExchangeManagerError  Internal \ud83d\udd12  InsufficientBalanceError  Internal \ud83d\udd12  OnlyCallableByPoolOperatorOrMakerError  Internal \ud83d\udd12  MakerPoolAssignmentError  Internal \ud83d\udd12  BlockTimestampTooLowError  Internal \ud83d\udd12  OnlyCallableByStakingContractError  Internal \ud83d\udd12  OnlyCallableIfInCatastrophicFailureError  Internal \ud83d\udd12  OnlyCallableIfNotInCatastrophicFailureError  Internal \ud83d\udd12  OperatorShareError  Internal \ud83d\udd12  PoolExistenceError  Internal \ud83d\udd12  InvalidProtocolFeePaymentError  Internal \ud83d\udd12  InvalidStakeStatusError  Internal \ud83d\udd12  InitializationError  Internal \ud83d\udd12  InvalidParamValueError  Internal \ud83d\udd12  ProxyDestinationCannotBeNilError  Internal \ud83d\udd12  PreviousEpochNotFinalizedError  Internal \ud83d\udd12  MixinStake  Implementation  MixinStakingPool  stake  External    NO   unstake  External    NO   moveStake  External    NO   _delegateStake  Private \ud83d\udd10  _undelegateStake  Private \ud83d\udd10  _getBalancePtrFromStatus  Private \ud83d\udd10  MixinStakeBalances  Implementation  MixinStakeStorage  getGlobalActiveStake  External    NO   getGlobalInactiveStake  External    NO   getGlobalDelegatedStake  External    NO   getTotalStake  External    NO   getActiveStake  External    NO   getInactiveStake  External    NO   getStakeDelegatedByOwner  External    NO   getWithdrawableStake  Public    NO   getStakeDelegatedToPoolByOwner  Public    NO   getTotalStakeDelegatedToPool  Public    NO   _computeWithdrawableStake  Internal \ud83d\udd12  MixinStakeStorage  Implementation  MixinScheduler  _moveStake  Internal \ud83d\udd12  _loadSyncedBalance  Internal \ud83d\udd12  _loadUnsyncedBalance  Internal \ud83d\udd12  _increaseCurrentAndNextBalance  Internal \ud83d\udd12  _decreaseCurrentAndNextBalance  Internal \ud83d\udd12  _increaseNextBalance  Internal \ud83d\udd12  _decreaseNextBalance  Internal \ud83d\udd12  _storeBalance  Private \ud83d\udd10  _arePointersEqual  Private \ud83d\udd10  MixinCumulativeRewards  Implementation  MixinStakeBalances  _initializeCumulativeRewards  Internal \ud83d\udd12  _isCumulativeRewardSet  Internal \ud83d\udd12  _forceSetCumulativeReward  Internal \ud83d\udd12  _computeMemberRewardOverInterval  Internal \ud83d\udd12  _getMostRecentCumulativeReward  Internal \ud83d\udd12  _getCumulativeRewardAtEpoch  Internal \ud83d\udd12  MixinStakingPool  Implementation  MixinAbstract, MixinStakingPoolRewards  createStakingPool  External    NO   decreaseStakingPoolOperatorShare  External    onlyStakingPoolOperatorOrMaker  joinStakingPoolAsMaker  External    NO   addMakerToStakingPool  External    onlyStakingPoolOperatorOrMaker  removeMakerFromStakingPool  External    onlyStakingPoolOperatorOrMaker  getStakingPoolIdOfMaker  Public    NO   getStakingPool  Public    NO   _addMakerToStakingPool  Internal \ud83d\udd12  _computeNextStakingPoolId  Internal \ud83d\udd12  _assertStakingPoolExists  Internal \ud83d\udd12  _assertNewOperatorShare  Private \ud83d\udd10  _assertSenderIsPoolOperatorOrMaker  Private \ud83d\udd10  MixinStakingPoolRewards  Implementation  MixinAbstract, MixinCumulativeRewards  withdrawDelegatorRewards  External    NO   computeRewardBalanceOfOperator  External    NO   computeRewardBalanceOfDelegator  External    NO   _withdrawAndSyncDelegatorRewards  Internal \ud83d\udd12  _syncPoolRewards  Internal \ud83d\udd12  _computePoolRewardsSplit  Internal \ud83d\udd12  _computeDelegatorReward  Private \ud83d\udd10  _computeUnfinalizedDelegatorReward  Private \ud83d\udd10  _increasePoolRewards  Private \ud83d\udd10  _decreasePoolRewards  Private \ud83d\udd10  MixinAbstract  Implementation  finalizePool  Public    NO   _getUnfinalizedPoolRewards  Internal \ud83d\udd12  MixinFinalizer  Implementation  MixinStakingPoolRewards  endEpoch  External    NO   finalizePool  Public    NO   _getUnfinalizedPoolRewards  Internal \ud83d\udd12  _getActivePoolFromEpoch  Internal \ud83d\udd12  _getActivePoolsFromEpoch  Internal \ud83d\udd12  _wrapEth  Internal \ud83d\udd12  _getAvailableWethBalance  Internal \ud83d\udd12  _getUnfinalizedPoolRewardsFromState  Private \ud83d\udd10  _creditRewardsToPool  Private \ud83d\udd10  MixinParams  Implementation  IStakingEvents, MixinStorage  setParams  External    onlyAuthorized  getParams  External    NO   _initMixinParams  Internal \ud83d\udd12  _assertParamsNotInitialized  Internal \ud83d\udd12  _setParams  Private \ud83d\udd10  MixinScheduler  Implementation  IStakingEvents, MixinStorage  getCurrentEpochEarliestEndTimeInSeconds  Public    NO   _initMixinScheduler  Internal \ud83d\udd12  _goToNextEpoch  Internal \ud83d\udd12  _assertSchedulerNotInitialized  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.1 InfinityPool Contract Authorization Bypass Attack    ", "body": "  Resolution  Addressed by not allowing the  Description  An attacker could create their own credential and set the Agent ID to 0, which would bypass the subjectIsAgentCaller modifier. The attacker could use this attack to borrow funds from the pool, draining any available liquidity. For example, only an Agent should be able to borrow funds from the pool and call the borrow function:  src/Pool/InfinityPool.sol:L302-L325  function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {  // 1e18 => 1 FIL, can't borrow less than 1 FIL  if (vc.value < WAD) revert InvalidParams();  // can't borrow more than the pool has  if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();  Account memory account = _getAccount(vc.subject);  // fresh account, set start epoch and epochsPaid to beginning of current window  if (account.principal == 0) {  uint256 currentEpoch = block.number;  account.startEpoch = currentEpoch;  account.epochsPaid = currentEpoch;  GetRoute.agentPolice(router).addPoolToList(vc.subject, id);  account.principal += vc.value;  account.save(router, vc.subject, id);  totalBorrowed += vc.value;  emit Borrow(vc.subject, vc.value);  // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier  asset.transfer(msg.sender, vc.value);  The following modifier checks that the caller is an Agent:  src/Pool/InfinityPool.sol:L96-L101  modifier subjectIsAgentCaller(VerifiableCredential memory vc) {  if (  GetRoute.agentFactory(router).agents(msg.sender) != vc.subject  ) revert Unauthorized();  _;  Recommendation  Ensure only an Agent can call borrow and pass the subjectIsAgentCaller modifier.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.2 Agent Data Oracle Signed Credential Front-Running Attack    ", "body": "  Resolution  Mitigated by allowing only the  Description  Recommendation  Ensure an Agent can always have new credentials that are needed. One solution would be to allow only an Agent s owner to request the credentials. The problem is that the beneficiary is also supposed to do that, but the beneficiary may also be a contract.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.3 Wrong Accounting for totalBorrowed in the InfinityPool.writeOff Function    ", "body": "  Resolution  Fixed.  Description  Here is a part of the InfinityPool.writeOff function:  src/Pool/InfinityPool.sol:L271-L287  // transfer the assets into the pool  // whatever we couldn't pay back  uint256 lostAmt = principalOwed > recoveredFunds ? principalOwed - recoveredFunds : 0;  uint256 totalOwed = interestPaid + principalOwed;  asset.transferFrom(  msg.sender,  address(this),  totalOwed > recoveredFunds ? recoveredFunds : totalOwed  );  // write off only what we lost  totalBorrowed -= lostAmt;  // set the account with the funds the pool lost  account.principal = lostAmt;  account.save(router, agentID, id);  The totalBorrowed is decreased by the lostAmt value. Instead, it should be decreased by the original account.principal value to acknowledge the loss.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.4 Wrong Accounting for totalBorrowed in the InfinityPool.pay Function    ", "body": "  Resolution   Addressed as recommended in two pull rquests:   1,  2.  Description  If the Agent pays more than the current interest debt, the remaining payment will be accounted as repayment of the principal debt:  src/Pool/InfinityPool.sol:L382-L401  // pay interest and principal  principalPaid = vc.value - interestOwed;  // the fee basis only applies to the interest payment  feeBasis = interestOwed;  // protect against underflow  totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;  // fully paid off  if (principalPaid >= account.principal) {  // remove the account from the pool's list of accounts  GetRoute.agentPolice(router).removePoolFromList(vc.subject, id);  // return the amount of funds overpaid  refund = principalPaid - account.principal;  // reset the account  account.reset();  } else {  // interest and partial principal payment  account.principal -= principalPaid;  // move the `epochsPaid` cursor to mark the account as \"current\"  account.epochsPaid = block.number;  Let s focus on the totalBorrowed changes:  src/Pool/InfinityPool.sol:L387  totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;  This value is supposed to be decreased by the principal that is repaid. So there are 2 mistakes in the calculation:  Should be totalBorrowed instead of 0.  The principalPaid cannot be larger than the account.principal in that calculation.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.5 The beneficiaryWithdrawable Function Can Be Called by Anyone    ", "body": "  Resolution  Fixed by removing beneficiary logic completely.  Description  The beneficiaryWithdrawable function is supposed to be called by the Agent when a beneficiary is trying to withdraw funds:  src/Agent/AgentPolice.sol:L320-L341  function beneficiaryWithdrawable(  address recipient,  address sender,  uint256 agentID,  uint256 proposedAmount  ) external returns (  uint256 amount  ) {  AgentBeneficiary memory beneficiary = _agentBeneficiaries[agentID];  address benneficiaryAddress = beneficiary.active.beneficiary;  // If the sender is not the owner of the Agent or the beneficiary, revert  if(  !(benneficiaryAddress == sender || (IAuth(msg.sender).owner() == sender && recipient == benneficiaryAddress) )) {  revert Unauthorized();  beneficiary,  amount  ) = beneficiary.withdraw(proposedAmount);  // update the beneficiary in storage  _agentBeneficiaries[agentID] = beneficiary;  This function reduces the quota that is supposed to be transferred during the withdraw call:  src/Agent/Agent.sol:L343-L352  sendAmount = agentPolice.beneficiaryWithdrawable(receiver, msg.sender, id, sendAmount);  else if (msg.sender != owner()) {  revert Unauthorized();  // unwrap any wfil needed to withdraw  _poolFundsInFIL(sendAmount);  // transfer funds  payable(receiver).sendValue(sendAmount);  The issue is that anyone can call this function directly, and the quota will be reduced without funds being transferred.  Recommendation  Ensure only the Agent can call this function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.6 An Agent Can Borrow Even With Existing Debt in Interest Payments    ", "body": "  Resolution  Mitigated by adding a limit to the remaining interest debt when borrowing. So an agent should have an interest debt that is no larger than 1 day.  Description  To borrow funds, an Agent has to call the borrow function of the pool:  src/Pool/InfinityPool.sol:L302-L325  function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {  // 1e18 => 1 FIL, can't borrow less than 1 FIL  if (vc.value < WAD) revert InvalidParams();  // can't borrow more than the pool has  if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();  Account memory account = _getAccount(vc.subject);  // fresh account, set start epoch and epochsPaid to beginning of current window  if (account.principal == 0) {  uint256 currentEpoch = block.number;  account.startEpoch = currentEpoch;  account.epochsPaid = currentEpoch;  GetRoute.agentPolice(router).addPoolToList(vc.subject, id);  account.principal += vc.value;  account.save(router, vc.subject, id);  totalBorrowed += vc.value;  emit Borrow(vc.subject, vc.value);  // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier  asset.transfer(msg.sender, vc.value);  Let s assume that the Agent already had some funds borrowed. During this function execution, the current debt status is not checked. The principal debt increases after borrowing, but account.epochsPaid remains the same. So the pending debt will instantly increase as if the borrowing happened on account.epochsPaid.  Recommendation  Ensure the debt is paid when borrowing more funds.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.7 The AgentPolice.distributeLiquidatedFunds() Function Can Have Undistributed Residual Funds    ", "body": "  Resolution  Mitigated by returning the excess funds in  Description  When an Agent is liquidated, the liquidator (owner of the protocol) is supposed to try to redeem as many funds as possible and re-distribute them to the pools:  src/Agent/AgentPolice.sol:L185-L191  function distributeLiquidatedFunds(uint256 agentID, uint256 amount) external {  if (!liquidated[agentID]) revert Unauthorized();  // transfer the assets into the pool  GetRoute.wFIL(router).transferFrom(msg.sender, address(this), amount);  _writeOffPools(agentID, amount);  The problem is that in the pool, it s accounted that the amount of funds can be larger than the debt. In that case, the pool won t transfer more funds than the pool needs:  src/Pool/InfinityPool.sol:L275-L289  uint256 totalOwed = interestPaid + principalOwed;  asset.transferFrom(  msg.sender,  address(this),  totalOwed > recoveredFunds ? recoveredFunds : totalOwed  );  // write off only what we lost  totalBorrowed -= lostAmt;  // set the account with the funds the pool lost  account.principal = lostAmt;  account.save(router, agentID, id);  emit WriteOff(agentID, recoveredFunds, lostAmt, interestPaid);  If that happens, the remaining funds will be stuck in the AgentPolice contract.  Recommendation  Return the residual funds to the Agent s owner or process them in some way so they are not lost.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.8 An Agent Can Be Upgraded Even if There Is No New Implementation    ", "body": "  Resolution  Mitigated by introducing a new version control mechanism. This solution also adds centralized power. The owner can create a new deployer with an arbitrary (even lower) version number, while agents can only upgrade to a higher version. Also, agents are forced to upgrade to a new version in another  pull request.  Description  Agents can be upgraded to a new implementation, and only the Agent s owner can call the upgrade function:  src/Agent/AgentFactory.sol:L51-L72  function upgradeAgent(  address agent  ) external returns (address newAgent) {  IAgent oldAgent = IAgent(agent);  address owner = IAuth(address(oldAgent)).owner();  uint256 agentId = agents[agent];  // only the Agent's owner can upgrade, and only a registered agent can be upgraded  if (owner != msg.sender || agentId == 0) revert Unauthorized();  // deploy a new instance of Agent with the same ID and auth  newAgent = GetRoute.agentDeployer(router).deploy(  router,  agentId,  owner,  IAuth(address(oldAgent)).operator()  );  // Register the new agent and unregister the old agent  agents[newAgent] = agentId;  // transfer funds from old agent to new agent and mark old agent as decommissioning  oldAgent.decommissionAgent(newAgent);  // delete the old agent from the registry  agents[agent] = 0;  The issue is that the owner can trigger the upgrade even if no new implementation exists. Multiple possible problems derive from it.  Upgrading to the current implementation of the Agent will break the logic because the current version is not calling the migrateMiner function, so all the miners will stay with the old Agent, and their funds will be lost.  The owner can accidentally trigger multiple upgrades simultaneously, leading to a loss of funds (https://github.com/ConsenSysDiligence/glif-audit-2023-04/issues/2).  The owner also has no control over the new version of the Agent. To increase decentralization, it s better to pass the deployer s address as a parameter additionally.  Recommendation  Ensure the upgrades can only happen when there is a new version of an Agent, and the owner controls this version.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.9 Potential Re-Entrancy Issues When Upgrading the Contracts    ", "body": "  Resolution   The issue is   mitigated by removing the old agent before the potential re-entrancy.  Description  The protocol doesn t have any built-in re-entrancy protection mechanisms. That mainly explains by using the wFIL token, which is not supposed to give that opportunity. And also by carefully using FIL transfers.  However, there are some places in the code where things may go wrong in the future. For example, when upgrading an Agent:  src/Agent/AgentFactory.sol:L51-L72  function upgradeAgent(  address agent  ) external returns (address newAgent) {  IAgent oldAgent = IAgent(agent);  address owner = IAuth(address(oldAgent)).owner();  uint256 agentId = agents[agent];  // only the Agent's owner can upgrade, and only a registered agent can be upgraded  if (owner != msg.sender || agentId == 0) revert Unauthorized();  // deploy a new instance of Agent with the same ID and auth  newAgent = GetRoute.agentDeployer(router).deploy(  router,  agentId,  owner,  IAuth(address(oldAgent)).operator()  );  // Register the new agent and unregister the old agent  agents[newAgent] = agentId;  // transfer funds from old agent to new agent and mark old agent as decommissioning  oldAgent.decommissionAgent(newAgent);  // delete the old agent from the registry  agents[agent] = 0;  Here, we see the oldAgent.decommissionAgent(newAgent); call happens before the oldAgent is deleted. Inside this function, we see:  src/Agent/Agent.sol:L200-L212  function decommissionAgent(address _newAgent) external {  // only the agent factory can decommission an agent  AuthController.onlyAgentFactory(router, msg.sender);  // if the newAgent has a mismatching ID, revert  if(IAgent(_newAgent).id() != id) revert Unauthorized();  // set the newAgent in storage, which marks the upgrade process as starting  newAgent = _newAgent;  uint256 _liquidAssets = liquidAssets();  // Withdraw all liquid funds from the Agent to the newAgent  _poolFundsInFIL(_liquidAssets);  // transfer funds to new agent  payable(_newAgent).sendValue(_liquidAssets);  Here, the FIL is transferred to a new contract which is currently unimplemented and unknown. Potentially, the fallback function of this contract could trigger a re-entrancy attack. If that s the case, during the execution of this function, there will be two contracts that are active agents with the same ID, and the attacker can try to use that maliciously.  Recommendation  Be very cautious with further implementations of agents and pools. Also, consider using reentrancy protection in public functions.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.10 InfinityPool Is Subject to a Donation With Inflation Attack if Emtied.    ", "body": "  Resolution  this issue will not be fixed in the current version of the contracts since some of the shares were already minted. The next iteration of the pool will have a more generic fix to this issue.  Description  Since InfinityPool is an implementation of the ERC4626 vault, it is too susceptible to inflation attacks. An attacker could front-run the first deposit and inflate the share price to an extent where the following deposit will be less than the value of 1 wei of share resulting in 0 shares minted. The attacker could conduct the inflation by means of self-destructing of another contract. In the case of GLIF this attack is less likely on the first pool since GLIF team accepts predeposits so some amount of shares was already minted. We do suggest fixing this issue before the next pool is deployed and no pre-stake is generated.  Examples  src/Pool/InfinityPool.sol:L491-L516  /*//////////////////////////////////////////////////////////////  4626 LOGIC  //////////////////////////////////////////////////////////////*/  /**  @dev Converts `assets` to shares  @param assets The amount of assets to convert  @return shares - The amount of shares converted from assets  /  function convertToShares(uint256 assets) public view returns (uint256) {  uint256 supply = liquidStakingToken.totalSupply(); // Saves an extra SLOAD if totalSupply is non-zero.  return supply == 0 ? assets : assets * supply / totalAssets();  /**  @dev Converts `shares` to assets  @param shares The amount of shares to convert  @return assets - The amount of assets converted from shares  /  function convertToAssets(uint256 shares) public view returns (uint256) {  uint256 supply = liquidStakingToken.totalSupply(); // Saves an extra SLOAD if totalSupply is non-zero.  return supply == 0 ? shares : shares * totalAssets() / supply;  Recommendation  Since the pool does not need to accept donations, the easiest way to handle this case is to use virtual price, where the balance of the contract is duplicated in a separate variable.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.11 MaxWithdraw Should Potentially Account for the Funds Available in the Ramp.    ", "body": "  Resolution   Partially fixed in   https://github.com/glif-confidential/pools/issues/462 but the ramp balance is still not accounted for.  Description  Since InfinityPool is ERC4626 it should also support the MaxWithdraw method. According to the EIP it should include any withdrawal limitation that the participant could encounter. At the moment the MaxWithdraw function returns the maximum amount of IOU tokens rather than WFIL. Since IOU token is not the asset token of the vault, this behavior is not ideal.  Examples  src/Pool/InfinityPool.sol:L569-L571  function maxWithdraw(address owner) public view returns (uint256) {  return convertToAssets(liquidStakingToken.balanceOf(owner));  Recommendation  We suggest considering returning the maximum amount of WFIL withdrawal which should account for Ramp balance.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.12 The Upgradeability of MinerRegistry, AgentPolice, and Agent Is Overcomplicated and Has a Hight Chance of Errors.   ", "body": "  Description  During the engagement, we have identified a few places that signify that the Agent, MinerRegistry and AgentPolice can be upgraded, for example:  Ability to migrate the miner from one version of the Agent to another inside the migrateMiner.  Ability to refreshRoutes that would update the AgentPolice and MinerRegistry addresses for a given Agent.  Ability to decommission pool. We believe that this functionality is present it is not very well thought through. For example, both MinerRegistry and AgentPolice are not upgradable but have mappings inside of them.  src/Agent/AgentPolice.sol:L51-L60  mapping(uint256 => bool) public liquidated;  /// @notice `_poolIDs` maps agentID to the pools they have actively borrowed from  mapping(uint256 => uint256[]) private _poolIDs;  /// @notice `_credentialUseBlock` maps signature bytes to when a credential was used  mapping(bytes32 => uint256) private _credentialUseBlock;  /// @notice `_agentBeneficiaries` maps an Agent ID to its Beneficiary struct  mapping(uint256 => AgentBeneficiary) private _agentBeneficiaries;  src/Agent/MinerRegistry.sol:L18-L20  mapping(bytes32 => bool) private _minerRegistered;  mapping(uint256 => uint64[]) private _minersByAgent;  That means that any time these contracts would need to be upgraded, the contents of those mappings will need to be somehow recreated in the new contract. That is not trivial since it is not easy to obtain all values of a mapping. This will also require an additional protocol-controlled setter ala kickstart mapping functions that are not ideal.  In the case of Agent if the contract was upgradable there would be no need for a process of migrating miners that can be tedious and opens possibilities for errors. Since protocol has a lot of centralization and trust assumptions already, having upgradability will not contribute to it a lot.  We also believe that during the upgrade of the pool, the PoolToken will stay the same in the new pool. That means that the minting and burning permissions of the share tokens have to be carefully updated or checked in a manner that does not require the address of the pool to be constant. Since we did not have access to this file, we can not check if that is done correctly.  Recommendation  Consider using upgradable contracts or have a solid upgrade plan that is well-tested before an emergency situation occurs.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.13 Mint Function in the Infinity Pool Will Emit the Incorrect Value.    ", "body": "  Resolution  Fixed by emitting the right value.  Description  Examples  src/Pool/InfinityPool.sol:L449-L457  function mint(uint256 shares, address receiver) public isOpen returns (uint256 assets) {  if(shares == 0) revert InvalidParams();  // These transfers need to happen before the mint, and this is forcing a higher degree of coupling than is ideal  assets = previewMint(shares);  asset.transferFrom(msg.sender, address(this), assets);  liquidStakingToken.mint(receiver, shares);  assets = convertToAssets(shares);  emit Deposit(msg.sender, receiver, assets, shares);  Recommendation  Use the assets value computed by the previewMint when emitting the event.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.14 Incorrect Operator Used    ", "body": "  Resolution  Fixed.  Description  Minor typo in the InfinityPool where the -= should be replaced with -.  Examples  src/Pool/InfinityPool.sol:L200  return balance -= feesCollected;  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.15 Potential Overpayment Due to Rounding Imprecision   ", "body": "  Resolution  The issue is acknowledged and the potential loss is considered tolerable.  Description  Inside the InifintyPool the pay function might accept unaccounted files. Imagine a situation where an Agent is trying to repay only the fees portion of the debt. In that case, the following branch will be executed:  src/Pool/InfinityPool.sol:L373-L381  if (vc.value <= interestOwed) {  // compute the amount of epochs this payment covers  // vc.value is not WAD yet, so divWadDown cancels the extra WAD in interestPerEpoch  uint256 epochsForward = vc.value.divWadDown(interestPerEpoch);  // update the account's `epochsPaid` cursor  account.epochsPaid += epochsForward;  // since the entire payment is interest, the entire payment is used to compute the fee (principal payments are fee-free)  feeBasis = vc.value;  } else {  The issue is if the value does not divide by the interestPerEpoch exactly, any remainder will remain in the InfinityPool.  src/Pool/InfinityPool.sol:L376  uint256 epochsForward = vc.value.divWadDown(interestPerEpoch);  Recommendation  Since the remainder will most likely not be too large this is not critical, but ideally, those remaining funds would be included in the refund variable.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.16 jumpStartAccount Should Be Subject to the Same Approval Checks as Regular Borrow.    ", "body": "  Resolution  Will not be fixed due to the complexity of the fix which will require passing verified credentials to be executed.  Description  InfinityPool contract has the ability to kick start an account that will have a debt position in this pool.  Examples  src/Pool/InfinityPool.sol:L673-L689  function jumpStartAccount(address receiver, uint256 agentID, uint256 accountPrincipal) external onlyOwner {  Account memory account = _getAccount(agentID);  // if the account is already initialized, revert  if (account.principal != 0) revert InvalidState();  // create the account  account.principal = accountPrincipal;  account.startEpoch = block.number;  account.epochsPaid = block.number;  // save the account  account.save(router, agentID, id);  // add the pool to the agent's list of borrowed pools  GetRoute.agentPolice(router).addPoolToList(agentID, id);  // mint the iFIL to the receiver, using principal as the deposit amount  liquidStakingToken.mint(receiver, convertToShares(accountPrincipal));  // account for the new principal in the total borrowed of the pool  totalBorrowed += accountPrincipal;  Recommendation  We suggest that this action is subject to the same rules as the standard borrow action. Thus checks on DTE, LTV and DTI should be done if possible.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.17 No Miner Migration Is Happening in the Current Implementation of the Agent  ", "body": "  Description  All miners should be transferred from the old Agent to a new one when upgrading an Agent. To do so, the new Agent is supposed to call the migrateMiner function for every miner:  src/Agent/Agent.sol:L219-L235  function migrateMiner(uint64 miner) external {  if (newAgent != msg.sender) revert Unauthorized();  uint256 newId = IAgent(newAgent).id();  if (  // first check to make sure the agentFactory knows about this \"agent\"  GetRoute.agentFactory(router).agents(newAgent) != newId ||  // then make sure this is the same agent, just upgraded  newId != id ||  // check to ensure this miner was registered to the original agent  !minerRegistry.minerRegistered(id, miner)  ) revert Unauthorized();  // propose an ownership change (must be accepted in v2 agent)  miner.changeOwnerAddress(newAgent);  emit MigrateMiner(msg.sender, newAgent, miner);  The problem is that this function is not called in the current Agent implementation. Since it s just the first version of an Agent contract, it s not a big issue. There is only one edge case where this may be a vulnerability. That may happen if the owner of an Agent decides to upgrade the contract to the same version. It is possible to do, and in that case, the miners  funds will be lost.  Recommendation  It s important to remember to call migrateMiner in a new version and not allow upgrading to the same implementation.  ", "labels": ["Consensys", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "3.1 The Hypervisor.deposit function does not check the msg.sender    ", "body": "  Resolution   Partially fixed in   GammaStrategies/hypervisor@9a7a3dd, by allowing only  Description  Hypervisor.deposit pulls pre-approved ERC20 tokens from the from address to the contract. Later it mints shares to the to address. Attackers can determine both the from and to addresses as they wish, and thus steal shares (that can be redeemed to tokens immediately) from users that pre-approved the contract to spend ERC20 tokens on their behalf.  Recommendation  As described in issue 3.5, we recommend restricting access to this function only for UniProxy. Moreover, the UniProxy contract should validate that from == msg.sender.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.2 UniProxy.depositSwap - Tokens are not approved before calling Router.exactInput    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  the call to Router.exactInputrequires the sender to pre-approve the tokens. We could not find any reference for that, thus we assume that a call to UniProxy.depositSwap will always revert.  Examples  code/contracts/UniProxy.sol:L202-L234  router = ISwapRouter(_router);  uint256 amountOut;  uint256 swap;  if(swapAmount < 0) {  //swap token1 for token0  swap = uint256(swapAmount * -1);  IHypervisor(pos).token1().transferFrom(msg.sender, address(this), deposit1+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit0  );  else{  //swap token1 for token0  swap = uint256(swapAmount);  IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit1  );  Recommendation  Consider approving the exact amount of input tokens before the swap.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.3 Uniproxy.depositSwap - _router should not be determined by the caller    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  Uniproxy.depositSwap uses _router that is determined by the caller, which in turn might inject a  fake  contract, and thus may steal funds stuck in the UniProxy contract.  The UniProxy contract has certain trust assumptions regarding the router. The router is supposed to return not less than deposit1(or deposit0) amount of tokens but that fact is never checked.  Examples  code/contracts/UniProxy.sol:L168-L177  function depositSwap(  int256 swapAmount, // (-) token1, (+) token0 for token1; amount to swap  uint256 deposit0,  uint256 deposit1,  address to,  address from,  bytes memory path,  address pos,  address _router  ) external returns (uint256 shares) {  Recommendation  Consider removing the _router parameter from the function, and instead, use a storage variable that will be initialized in the constructor.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.4 Re-entrancy + flash loan attack can invalidate price check    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  The UniProxy contract has a price manipulation protection:  code/contracts/UniProxy.sol:L75-L82  if (twapCheck || positions[pos].twapOverride) {  // check twap  checkPriceChange(  pos,  (positions[pos].twapOverride ? positions[pos].twapInterval : twapInterval),  (positions[pos].twapOverride ? positions[pos].priceThreshold : priceThreshold)  );  But after that, the tokens are transferred from the user, if the token transfer allows an attacker to hijack the call-flow of the transaction inside, the attacker can manipulate the Uniswap price there, after the check happened. The Hypervisor s deposit function itself is vulnerable to the flash-loan attack.  Recommendation  Make sure the price does not change before the Hypervisor.deposit call. For example, the token transfers can be made at the beginning of the UniProxy.deposit function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.5 The deposit function of the Hypervisor contract should only be called from UniProxy    ", "body": "  Resolution   Partially fixed in   GammaStrategies/hypervisor@9a7a3dd, by allowing only  Description  The deposit function is designed to be called only from the UniProxy contract, but everyone can call it. This function does not have any protection against price manipulation in the Uniswap pair. A deposit can be frontrunned, and the depositor s funds may be  stolen .  Recommendation  Make sure only UniProxy can call the deposit function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.6 UniProxy.properDepositRatio - Proper ratio will not prevent liquidity imbalance for all possible scenarios    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  Examples  code/contracts/UniProxy.sol:L258-L275  function properDepositRatio(  address pos,  uint256 deposit0,  uint256 deposit1  ) public view returns (bool) {  (uint256 hype0, uint256 hype1) = IHypervisor(pos).getTotalAmounts();  if (IHypervisor(pos).totalSupply() != 0) {  uint256 depositRatio = deposit0 == 0 ? 10e18 : deposit1.mul(1e18).div(deposit0);  depositRatio = depositRatio > 10e18 ? 10e18 : depositRatio;  depositRatio = depositRatio < 10e16 ? 10e16 : depositRatio;  uint256 hypeRatio = hype0 == 0 ? 10e18 : hype1.mul(1e18).div(hype0);  hypeRatio = hypeRatio > 10e18 ? 10e18 : hypeRatio;  hypeRatio = hypeRatio < 10e16 ? 10e16 : hypeRatio;  return (FullMath.mulDiv(depositRatio, deltaScale, hypeRatio) < depositDelta &&  FullMath.mulDiv(hypeRatio, deltaScale, depositRatio) < depositDelta);  return true;  Recommendation  Consider removing the cap of [0.1,10] both for depositRatio and for hypeRatio.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.7 UniProxy - SafeERC20 is declared but safe functions are not used    ", "body": "  Resolution   fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  The UniProxy contract declares the usage of the SafeERC20 library for functions of the IERC20 type. However, unsafe functions are used instead of safe ones.  Examples  Usage of approve instead of safeApprove  Usage of transferFrom instead of safeTransferFrom.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.8 Missing/wrong implementation    ", "body": "  Resolution  Fixed in GammaStrategies/hypervisor@9a7a3dd by introducing two new functions: toggleDepositOverride, setPriceThresholdPos.  Fixed in GammaStrategies/hypervisor@9a7a3dd by keeping only the version of deposit function with 4 parameters.  Fixed in GammaStrategies/hypervisor@9a7a3dd by removing the unreachable code.  Examples  The UniProxy contract has different functions used for setting the properties of a position. However, Position.priceThreshold, and Position.depositOverride are never assigned to, even though they are being used.  UniProxy.deposit is calling IHypervisor.deposit multiple times with different function signatures (3 and 4 parameters), while the Hypervisor contract only implements the version with 4 parameters, and does not implement the IHypervisor interface.  Hypervisor.uniswapV3MintCallback | uniswapV3SwapCallback - both these functions contain unreachable code, namely the case where payer != address(this).  Recommendations  Consider adding functions to set these properties, or alternatively, a single function to set the properties of a position.  Consider supporting a single deposit function for IHypervisor, and make sure that the actual implementation adheres to this interface.  Consider deleting these lines.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.9 Hypervisor.withdraw - Possible reentrancy    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Description  Recommendation  Consider adding a ReentrancyGuard both to Hypervisor.withdraw and Hypervisor.deposit  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.10 UniProxy.depositSwap doesn t deposit all the users  funds    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by deleting the  Description  When executing the swap, the minimal amount out is passed to the router (deposit1 in this example), but the actual swap amount will be amountOut. But after the trade, instead of depositing amountOut, the contract tries to deposit deposit1, which is lower. This may result in some users  funds staying in the UniProxy contract.  code/contracts/UniProxy.sol:L220-L242  else{  //swap token1 for token0  swap = uint256(swapAmount);  IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);  amountOut = router.exactInput(  ISwapRouter.ExactInputParams(  path,  address(this),  block.timestamp + swapLife,  swap,  deposit1  );  require(amountOut > 0, \"Swap failed\");  if (positions[pos].version < 2) {  // requires lp token transfer from proxy to msg.sender  shares = IHypervisor(pos).deposit(deposit0, deposit1, address(this));  IHypervisor(pos).transfer(to, shares);  Recommendation  Deposit all the user s funds to the Hypervisor.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.11 Hypervisor - Multiple  sandwiching  front running vectors    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by removing the call to  Description  The amount of tokens received from UniswapV3Pool functions might be manipulated by front-runners due to the decentralized nature of AMMs, where the order of transactions can not be pre-determined. A potential  sandwicher  may insert a buying order before the user s call to Hypervisor.rebalance for instance, and a sell order after.  More specifically, calls to pool.swap, pool.mint, pool.burn are susceptible to  sandwiching  vectors.  Examples  Hypervisor.rebalance  code/contracts/Hypervisor.sol:L278-L286  if (swapQuantity != 0) {  pool.swap(  address(this),  swapQuantity > 0,  swapQuantity > 0 ? swapQuantity : -swapQuantity,  swapQuantity > 0 ? TickMath.MIN_SQRT_RATIO + 1 : TickMath.MAX_SQRT_RATIO - 1,  abi.encode(address(this))  );  code/contracts/Hypervisor.sol:L348-L363  function _mintLiquidity(  int24 tickLower,  int24 tickUpper,  uint128 liquidity,  address payer  ) internal returns (uint256 amount0, uint256 amount1) {  if (liquidity > 0) {  (amount0, amount1) = pool.mint(  address(this),  tickLower,  tickUpper,  liquidity,  abi.encode(payer)  );  code/contracts/Hypervisor.sol:L365-L383  function _burnLiquidity(  int24 tickLower,  int24 tickUpper,  uint128 liquidity,  address to,  bool collectAll  ) internal returns (uint256 amount0, uint256 amount1) {  if (liquidity > 0) {  // Burn liquidity  (uint256 owed0, uint256 owed1) = pool.burn(tickLower, tickUpper, liquidity);  // Collect amount owed  uint128 collect0 = collectAll ? type(uint128).max : _uint128Safe(owed0);  uint128 collect1 = collectAll ? type(uint128).max : _uint128Safe(owed1);  if (collect0 > 0 || collect1 > 0) {  (amount0, amount1) = pool.collect(to, tickLower, tickUpper, collect0, collect1);  Recommendation  Consider adding an amountMin parameter(s) to ensure that at least the amountMin of tokens was received.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.12 Full test suite is necessary ", "body": "  Description  The test suite at this stage is not complete. It is crucial to have a full test coverage that includes the edge cases and failure scenarios, especially for complex system like Gamma.  As we ve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.  Some issues such as issue 3.8, issue 3.2 could be caught by a full-coverage test suite.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.13 Uniswap v3 callbacks access control should be hardened    ", "body": "  Resolution   Fixed in   GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation for  Description  Examples  code/contracts/Hypervisor.sol:L407-L445  function uniswapV3MintCallback(  uint256 amount0,  uint256 amount1,  bytes calldata data  ) external override {  require(msg.sender == address(pool));  address payer = abi.decode(data, (address));  if (payer == address(this)) {  if (amount0 > 0) token0.safeTransfer(msg.sender, amount0);  if (amount1 > 0) token1.safeTransfer(msg.sender, amount1);  } else {  if (amount0 > 0) token0.safeTransferFrom(payer, msg.sender, amount0);  if (amount1 > 0) token1.safeTransferFrom(payer, msg.sender, amount1);  function uniswapV3SwapCallback(  int256 amount0Delta,  int256 amount1Delta,  bytes calldata data  ) external override {  require(msg.sender == address(pool));  address payer = abi.decode(data, (address));  if (amount0Delta > 0) {  if (payer == address(this)) {  token0.safeTransfer(msg.sender, uint256(amount0Delta));  } else {  token0.safeTransferFrom(payer, msg.sender, uint256(amount0Delta));  } else if (amount1Delta > 0) {  if (payer == address(this)) {  token1.safeTransfer(msg.sender, uint256(amount1Delta));  } else {  token1.safeTransferFrom(payer, msg.sender, uint256(amount1Delta));  Recommendation  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "3.14 Code quality comments    ", "body": "  Resolution  Fixed in GammaStrategies/hypervisor@9a7a3dd by removing the from parameter.  Fixed in GammaStrategies/hypervisor@9a7a3dd by implementing the auditor s recommendation.  Fixed in GammaStrategies/hypervisor@9a7a3dd by deleting depositSwap.  Examples  UniProxy.deposit - from parameter is never used.  UniProxy - MAX_INT should be changed to MAX_UINT.  Consider using compiler version >= 0.8.0, and make sure that the compiler version is specified explicitly for every .sol file in the repo.  UniProxy - Minimize code duplication in deposit and depositSwap.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/02/gamma/"}, {"title": "6.1 Ether temporarily held during transactions can be stolen via reentrancy    ", "body": "  Resolution  This is addressed in 0xProject/protocol@437a3b0 by transferring exactly msg.value in sellToLiquidityProvider(). This adequately protects against this specific vulnerability.  The client team decided to leave the accounting in MetaTransactionsFeature as-is due to the complexity/expense of tracking ether consumption more strictly.  Description  The exchange proxy typically holds no ether balance, but it can temporarily hold a balance during a transaction. This balance is vulnerable to theft if the following conditions are met:  No check at the end of the transaction reverts if ether goes missing,  reentrancy is possible during the transaction, and  a mechanism exists to spend ether held by the exchange proxy.  We found one example where these conditions are met, but it s possible that more exist.  Example  MetaTransactionsFeature.executeMetaTransaction() accepts ether, which is used to pay protocol fees. It s possible for less than the full amount in msg.value to be consumed, which is why the function uses the refundsAttachedEth modifier to return any remaining ether to the caller:  code/contracts/zero-ex/contracts/src/features/MetaTransactionsFeature.sol:L98-L106  /// @dev Refunds up to `msg.value` leftover ETH at the end of the call.  modifier refundsAttachedEth() {  _;  uint256 remainingBalance =  LibSafeMathV06.min256(msg.value, address(this).balance);  if (remainingBalance > 0) {  msg.sender.transfer(remainingBalance);  Notice that this modifier just returns the remaining ether balance (up to msg.value). It does not check for a specific amount of remaining ether. This meets condition (1) above.  It s impossible to reenter the system with a second metatransaction because executeMetaTransaction() uses the modifier nonReentrant, but there s nothing preventing reentrancy via a different feature. We can achieve reentrancy by trading a token that uses callbacks (e.g. ERC777 s hooks) during transfers. This meets condition (2).  LiquidityProviderFeature.sellToLiquidityProvider() provides such a mechanism. By passing  code/contracts/zero-ex/contracts/src/features/LiquidityProviderFeature.sol:L114-L115  if (inputToken == ETH_TOKEN_ADDRESS) {  provider.transfer(sellAmount);  This meets condition (3).  The full steps to exploit this vulnerability are as follows:  A maker/attacker signs a trade where one of the tokens will invoke a callback during the trade.  A taker signs a metatransaction to take this trade.  A relayer sends in the metatransaction, providing more ether than is necessary to pay the protocol fee. (It s unclear how likely this situation is.)  During the token callback, the attacker invokes LiquidityProviderFeature.sellToLiquidityProvider() to transfer the excess ether to their account.  The metatransaction feature returns the remaining ether balance, which is now zero.  Recommendation  In general, we recommend using strict accounting of ether throughout the system. If there s ever a temporary balance, it should be accurately resolved at the end of the transaction, after any potential reentrancy opportunities.  For the example we specifically found, we recommend doing strict accounting in the metatransactions feature. This means features called via a metatransaction would need to return how much ether was consumed. The metatransactions feature could then refund exactly msg.value - <consumed ether>. The transaction should be reverted if this fails because it means ether went missing during the transaction.  We also recommend limiting sellToLiquidityProvider() to only transfer up to msg.value. This is a form of defense in depth in case other vectors for a similar attack exist.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"}, {"title": "6.2 UniswapFeature: Non-static call to ERC20.allowance()    ", "body": "  Resolution   This is fixed in   0xProject/protocol@437a3b0.  Description  In the case where a token is possibly  greedy  (consumes all gas on failure), UniswapFeature makes a call to the token s allowance() function to check whether the user has provided a token allowance to the protocol proxy or to the AllowanceTarget. This call is made using call(), potentially allowing state-changing operations to take place before control of the execution returns to UniswapFeature.  code/contracts/zero-ex/contracts/src/features/UniswapFeature.sol:L373-L377  // `token.allowance()``  mstore(0xB00, ALLOWANCE_CALL_SELECTOR_32)  mstore(0xB04, caller())  mstore(0xB24, address())  let success := call(gas(), token, 0, 0xB00, 0x44, 0xC00, 0x20)  Recommendation  Replace the call() with a staticcall().  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"}, {"title": "6.3 UniswapFeature: Unchecked returndatasize in low-level external calls    ", "body": "  Resolution   This is fixed in   0xProject/protocol@437a3b0.  Description  UniswapFeature makes a number of external calls from low-level assembly code. Two of these calls rely on the CALL opcode to copy the returndata to memory without checking that the call returned the expected amount of data. Because the CALL opcode does not zero memory if the call returns less data than expected, this can lead to usage of dirty memory under the assumption that it is data returned from the most recent call.  Examples  Call to UniswapV2Pair.getReserves()  code/contracts/zero-ex/contracts/src/features/UniswapFeature.sol:L201-L205  // Call pair.getReserves(), store the results at `0xC00`  mstore(0xB00, UNISWAP_PAIR_RESERVES_CALL_SELECTOR_32)  if iszero(staticcall(gas(), pair, 0xB00, 0x4, 0xC00, 0x40)) {  bubbleRevert()  Call to ERC20.allowance()  code/contracts/zero-ex/contracts/src/features/UniswapFeature.sol:L372-L377  // Check if we have enough direct allowance by calling  // `token.allowance()``  mstore(0xB00, ALLOWANCE_CALL_SELECTOR_32)  mstore(0xB04, caller())  mstore(0xB24, address())  let success := call(gas(), token, 0, 0xB00, 0x44, 0xC00, 0x20)  Recommendation  Instead of providing a memory range for call() to write returndata to, explicitly check returndatasize() after the call is made and then copy the data into memory using returndatacopy().  if lt(returndatasize(), EXPECTED_SIZE) {  revert(0, 0)  returndatacopy(0xC00, 0x00, EXPECTED_SIZE)  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"}, {"title": "6.4 Rollback functionality can lead to untested combinations   ", "body": "  Resolution  From the client team:  Just like our migrations, we batch our rollbacks by release, which enforces rolling back to known good configurations.  The documentation now includes an emergency playbook that describes how rollbacks should be done.  Description  SimpleFunctionRegistry maps individual function selectors to implementation contracts. As features are newly deployed or upgraded, functions are registered in logical groups after a timelock enforced by the owning multisig wallet. This gives users time to evaluate upcoming changes and stop using the contract if they don t like the changes.  Once deployed, however, any function can individually be rolled back without a timelock to any previous version of that function. Users are given no warning, functions can be rolled back to any previous implementation (regardless of how old), and the per-function granularity means that the configuration after rollback may be a never-before-seen combination of functions.  The combinatorics makes it impossible for a user (or auditor) to be comfortable with all the possible outcomes of rollbacks. If there are n versions each of m functions, there are n^m combinations that could be in effect at any moment. Some functions depend on other onlySelf functions, so the behavior of those combinations is not at all obvious.  This presents a trust problem for users.  Recommendation  Rollback makes sense as a way to rapidly recover from a bad deployment, but we recommend limiting its scope. The following ideas are in preferred order (our favorite first):  Disallow rollback altogether except to an implementation of address(0). This way broken functionality can be immediately disabled, but no old version of a function can be reinstated.  Limit rollback by number of versions, e.g. only allowing rollback to the immediately previous version of a function.  Limit rollback by time, e.g. only allowing rollback to versions in the past n weeks.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/12/0x-exchange-v4/"}, {"title": "4.1 Reuse of CHAINID from contract deployment    ", "body": "  Resolution   This is addressed in   ScopeLift/umbra-protocol@7cfdc81.  Description  The internal function _validateWithdrawSignature() is used to check whether a sponsored token withdrawal is approved by the owner of the stealth address that received the tokens. Among other data, the chain ID is signed over to prevent replay of signatures on other EVM-compatible chains.  contracts/contracts/Umbra.sol:L307-L329  function _validateWithdrawSignature(  address _stealthAddr,  address _acceptor,  address _tokenAddr,  address _sponsor,  uint256 _sponsorFee,  IUmbraHookReceiver _hook,  bytes memory _data,  uint8 _v,  bytes32 _r,  bytes32 _s  ) internal view {  bytes32 _digest =  keccak256(  abi.encodePacked(  \"\\x19Ethereum Signed Message:\\n32\",  keccak256(abi.encode(chainId, version, _acceptor, _tokenAddr, _sponsor, _sponsorFee, address(_hook), _data))  );  address _recoveredAddress = ecrecover(_digest, _v, _r, _s);  require(_recoveredAddress != address(0) && _recoveredAddress == _stealthAddr, \"Umbra: Invalid Signature\");  However, this chain ID is set as an immutable value in the contract constructor. In the case of a future contentious hard fork of the Ethereum network, the same Umbra contract would exist on both of the resulting chains. One of these two chains would be expected to change the network s chain ID, but the Umbra contracts would not be aware of this change. As a result, signatures to the Umbra contract on either chain would be replayable on the other chain.  This is a common pattern in contracts that implement EIP-712 signatures. Presumably, the motivation in most cases for committing to the chain ID at deployment time is to avoid recomputing the EIP-712 domain separator for every signature verification. In this case, the chain ID is a direct input to the generation of the signed digest, so this should not be a concern.  Recommendation  Replace the use of the chainId immutable value with the CHAINID opcode in _validateWithdrawSignature(). Note that CHAINID is only available using Solidity s inline assembly, so this would need to be accessed in the same way as it is currently accessed in the contract s constructor:  contracts/contracts/Umbra.sol:L68-L72  uint256 _chainId;  assembly {  _chainId := chainid()  5 Recommendations  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.1 Use separate mappings for keys in StealthKeyResolver", "body": "  Description  The StealthKeyResolver currently stores keys in a mapping bytes32 => uint256 => uint256 that maps nodes => prefixes => keys. The prefixes are offset in the setStealthKeys() function to differentiate between viewing public keys and spending public keys, and these offsets are reversed in the stealthKeys() view function.  contracts/profiles/StealthKeyResolver.sol:L37-L56  function setStealthKeys(bytes32 node, uint256 spendingPubKeyPrefix, uint256 spendingPubKey, uint256 viewingPubKeyPrefix, uint256 viewingPubKey) external authorised(node) {  require(  (spendingPubKeyPrefix == 2 || spendingPubKeyPrefix == 3) &&  (viewingPubKeyPrefix == 2 || viewingPubKeyPrefix == 3),  \"StealthKeyResolver: Invalid Prefix\"  );  emit StealthKeyChanged(node, spendingPubKeyPrefix, spendingPubKey, viewingPubKeyPrefix, viewingPubKey);  // Shift the spending key prefix down by 2, making it the appropriate index of 0 or 1  spendingPubKeyPrefix -= 2;  // Ensure the opposite prefix indices are empty  delete _stealthKeys[node][1 - spendingPubKeyPrefix];  delete _stealthKeys[node][5 - viewingPubKeyPrefix];  // Set the appropriate indices to the new key values  _stealthKeys[node][spendingPubKeyPrefix] = spendingPubKey;  _stealthKeys[node][viewingPubKeyPrefix] = viewingPubKey;  This manual adjustment of prefixes adds complexity to an otherwise simple function. To avoid this, consider splitting this into two separate mappings   one for viewing keys and one for spending keys. For clarity, also specify the visibility of these mappings explicitly.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.2 Document potential edge cases for hook receiver contracts", "body": "  Description  The functions withdrawTokenAndCall() and withdrawTokenAndCallOnBehalf() make a call to a hook contract designated by the owner of the withdrawing stealth address.  contracts/contracts/Umbra.sol:L289-L291  if (address(_hook) != address(0)) {  _hook.tokensWithdrawn(_withdrawalAmount, _stealthAddr, _acceptor, _tokenAddr, _sponsor, _sponsorFee, _data);  There are very few constraints on the parameters to these calls in the Umbra contract itself. Anyone can force a call to a hook contract by transferring a small amount of tokens to an address that they control and withdrawing these tokens, passing the target address as the hook receiver. Developers of these UmbraHookReceiver contracts should be sure to validate both the caller of the tokensWithdrawn() function and the function parameters. There are a number of possible edge cases that should be handled when relevant. These include, but are not limited to, the following:  The _amount may not have been transferred to the hook receiver itself.  All four addresses passed to tokensWithdrawn() could be the same. Most of these address parameters could also be any arbitrary address. This includes the token contract address, the address of the hook receiver, or the address of the Umbra contract itself.  The token received may be valueless.  The token received may be malicious. The only requirements are that the token contract address contains code and accepts calls to the ERC20 methods transfer() and transferFrom().  While it is difficult to determine a feasible exploit without knowledge of what hook receiver contracts may do in the future, a slightly contrived example follows.  Suppose a user builds a hook receiver contract that accepts an arbitrary token, TOK, and immediately provides liquidity to the ETH-TOK Uniswap pair when tokensWithdrawn() is called by the Umbra contract. An attacker could create a malicious token that can not be transferred out of its own Uniswap Pair contract and force a call to the hook receiver contract from Umbra. The hook receiver would be able to provide liquidity to the pool but would be unable to remove it, losing any ETH that was provided.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.3 Document token behavior restrictions", "body": "  As with any protocol that interacts with arbitrary ERC20 tokens, it is important to clearly document which tokens are supported. Often this is best done by providing a specification for the behavior of the expected ERC20 tokens and only relaxing this specification after careful review of a particular class of tokens and their interactions with the protocol.  In the absence of this, the following is a necessarily incomplete list of some known deviations from  normal  ERC20 behavior that should be explicitly noted as NOT supported by the Umbra Protocol:  Deflationary or fee-on-transfer tokens: These are tokens in which the balance of the recipient of a transfer may not be increased by the amount of the transfer. There may also be some alternative mechanism by which balances are unexpectedly decreased. While these tokens can be successfully sent via the sendToken() function, the internal accounting of the Umbra contract will be out of sync with the balance as recorded in the token contract, resulting in loss of funds.  Inflationary tokens: The opposite of deflationary tokens. The Umbra contract provides no mechanism for claiming positive balance adjustments.  Rebasing tokens: A combination of the above cases, these are tokens in which an account s balance increases or decreases along with expansions or contractions in supply. The contract provides no mechanism to update its internal accounting in response to these unexpected balance adjustments, and funds may be lost as a result.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.4 Add an address parameter to withdrawal signatures   ", "body": "  Resolution   This is addressed in   ScopeLift/umbra-protocol@d6e4235, which replaces the  Description  As discussed above, the _validateWithdrawSignature() function checks the signer of a digest consisting of the keccak-256 hash of the following preimage:  abi.encodePacked(  \"\\x19Ethereum Signed Message:\\n32\",  keccak256(abi.encode(chainId, version, _acceptor, _tokenAddr, _sponsor, _sponsorFee, address(_hook), _data))  Consider adding the address of the contract itself to this signed message. Currently, it is possible to deploy any number of contracts with the same version to the same chain, and signatures would be replayable across all of these contracts. While users are likely to only have balances for the same stealth address in one of these contracts, adding an address parameter provides some additional replay protection. Because the contract can not be self-destructed, a given address can only ever contain a single version of the Umbra contract.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/03/umbra-smart-contracts/"}, {"title": "5.1 Reactivated gauges can t queue up rewards    ", "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by making it so all gauges are always included in cycles, thus keeping in sync their  Description  Active gauges as set in ERC20Gauges.addGauge() function by authorised users get their rewards queued up in the FlywheelGaugeRewards._queueRewards() function. As part of it, their associated struct QueuedRewards updates its storedCycle value to the cycle in which they get queued up:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L202-L206  gaugeQueuedRewards[gauge] = QueuedRewards({  priorCycleRewards: queuedRewards.priorCycleRewards + completedRewards,  cycleRewards: uint112(nextRewards),  storedCycle: currentCycle  });  Once reactivated later with at least 1 full cycle being done without it, it will produce issues. It will now be returned by gaugeToken.gauges() to be processed in either FlywheelGaugeRewards.queueRewardsForCycle()or FlywheelGaugeRewards.queueRewardsForCyclePaginated(), but, once the reactivated gauge is passed to _queueRewards(), it will fail an assert:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L196  assert(queuedRewards.storedCycle == 0 || queuedRewards.storedCycle >= lastCycle);  This is because it already has a set value from the cycle it was processed in previously (i.e. storedCycle>0), and, since that cycle is at least 1 full cycle behind the state contract, it will also not pass the second condition queuedRewards.storedCycle >= lastCycle.  The result is that this gauge is locked out of queuing up for rewards because queuedRewards.storedCycle is only synchronised with the contract s cycle later in _queueRewards() which will now always fail for this gauge.  Recommendation  Account for the reactivated gauges that previously went through the rewards queue process, such as introducing a separate flow for newly activated gauges. However, any changes such as removing the above mentioned assert() should be carefully validated for other downstream logic that may use the QueuedRewards.storedCycle value. Therefore, it is recommended to review the state transitions as opposed to only passing this specific check.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"}, {"title": "5.2 Reactivated gauges have incorrect accounting for the last cycle s rewards    ", "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by making it so all gauges are always included in cycles, thus keeping in sync their  Description  As described in issue 5.1, reactivated gauges that previously had queued up rewards have a mismatch between their storedCycle and contract s gaugeCycle state variable.  Due to this mismatch, there is also a resulting issue with the accounting logic for its completed rewards:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L198  uint112 completedRewards = queuedRewards.storedCycle == lastCycle ? queuedRewards.cycleRewards : 0;  Consequently, this then produces an incorrect value for QueuedRewards.priorCycleRewards:  code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L203  priorCycleRewards: queuedRewards.priorCycleRewards + completedRewards,  As now completedRewards will be equal to 0 instead of the previous cycle s rewards for that gauge. This may cause a loss of rewards accounted for this gauge as this value is later used in getAccruedRewards().  Recommendation  Consider changing the logic of the check so that storedCycle values further in the past than lastCycle may produce the right rewards return for this expression, such as using <= instead of == and adding an explicit check for storedCycle == 0 to account for the initial scenario.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"}, {"title": "5.3 Lack of input validation in delegateBySig    ", "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by reverting for  Description  ERC20MultiVotes.sol makes use of ecrecover() in delegateBySig to return the address of the message signer. ecrecover() typically returns address(0x0) to indicate an error; however, there s no zero address check in the function logic. This might not be exploitable though, as delegate(0x0, arbitraryAddress) might always return zero votes (in freeVotes). Additionally, ecrecover() can be forced to return a random address by messing with the parameters. Although this is extremely rare and will likely resolve to zero free votes most times, this might return a random address and delegate someone else s votes.  Examples  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L364-L387  function delegateBySig(  address delegatee,  uint256 nonce,  uint256 expiry,  uint8 v,  bytes32 r,  bytes32 s  ) public {  require(block.timestamp <= expiry, \"ERC20MultiVotes: signature expired\");  address signer = ecrecover(  keccak256(  abi.encodePacked(  \"\\x19\\x01\",  DOMAIN_SEPARATOR(),  keccak256(abi.encode(DELEGATION_TYPEHASH, delegatee, nonce, expiry))  ),  v,  r,  );  require(nonce == nonces[signer]++, \"ERC20MultiVotes: invalid nonce\");  _delegate(signer, delegatee);  Recommendation  Introduce a zero address check i.e require signer!=address(0) and check if the recovered signer is an expected address. Refer to ERC20 s permit for inspiration.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"}, {"title": "5.4 Decreasing maxGauges does not account for users  previous gauge list size.    ", "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by documenting.  Description  ERC20Gauges contract has a maxGauges state variable meant to represent the maximum amount of gauges a user can allocate to. As per the natspec, it is meant to protect against gas DOS attacks upon token transfer to allow complicated transactions to fit in a block. There is also a function setMaxGauges for authorised users to decrease or increase this state variable.  code-flywheel-v2/src/token/ERC20Gauges.sol:L499-L504  function setMaxGauges(uint256 newMax) external requiresAuth {  uint256 oldMax = maxGauges;  maxGauges = newMax;  emit MaxGaugesUpdate(oldMax, newMax);  Recommendation  Either document the potential discrepancy between the user gauges size and the maxGauges state variable, or limit maxGauges to be only called within the contract thereby forcing other contracts to retrieve user gauge list size through numUserGauges().  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"}, {"title": "5.5 Decrementing a gauge by 0 that is not in the user gauge list will fail an assert.    ", "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by implementing auditor s recommendation.  Description  ERC20Gauges._decrementGaugeWeight has an edge case scenario where a user can attempt to decrement a gauge that is not in the user gauge list by 0 weight, which would trigger a failure in an assert.  code-flywheel-v2/src/token/ERC20Gauges.sol:L333-L345  function _decrementGaugeWeight(  address user,  address gauge,  uint112 weight,  uint32 cycle  ) internal {  uint112 oldWeight = getUserGaugeWeight[user][gauge];  getUserGaugeWeight[user][gauge] = oldWeight - weight;  if (oldWeight == weight) {  // If removing all weight, remove gauge from user list.  assert(_userGauges[user].remove(gauge));  code-flywheel-v2/src/token/ERC20Gauges.sol:L339-L341  uint112 oldWeight = getUserGaugeWeight[user][gauge];  getUserGaugeWeight[user][gauge] = oldWeight - weight;  However, passing a weight=0 parameter with a gauge that doesn t belong to the user, would successfully process that line. This would then be followed by an evaluation if (oldWeight == weight), which would also succeed since both are 0, to finally reach an assert that will verify a remove of that gauge from the user gauge list. However, it will fail since it was never there in the first place.  code-flywheel-v2/src/token/ERC20Gauges.sol:L344  assert(_userGauges[user].remove(gauge));  Although an edge case with no effect on contract state s health, it may happen with front end bugs or incorrect user transactions, and it is best not to have asserts fail.  Recommendation  Replace assert() with a require() or verify that the gauge belongs to the user prior to performing any operations.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"}, {"title": "5.6 Undelegating 0 votes from an address who is not a delegate of a user will fail an assert.    ", "body": "  Resolution   Fixed in   fei-protocol/flywheel-v2@e765d24 by implementing auditor s recommendation.  Description  Similar scenario with issue 5.5. ERC20MultiVotes._undelegate has an edge case scenario where a user can attempt to undelegate from a delegatee that is not in the user delegates list by 0 amount, which would trigger a failure in an assert.  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L251-L260  function _undelegate(  address delegator,  address delegatee,  uint256 amount  ) internal virtual {  uint256 newDelegates = _delegatesVotesCount[delegator][delegatee] - amount;  if (newDelegates == 0) {  assert(_delegates[delegator].remove(delegatee)); // Should never fail.  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L256  uint256 newDelegates = _delegatesVotesCount[delegator][delegatee] - amount;  However, passing a amount=0 parameter with a delegatee that doesn t belong to the user, would successfully process that line. This would then be followed by an evaluation if (newDelegates == 0), which would succeed, to finally reach an assert that will verify a remove of that delegatee from the user delegates list. However, it will fail since it was never there in the first place.  code-flywheel-v2/src/token/ERC20MultiVotes.sol:L259  assert(_delegates[delegator].remove(delegatee)); // Should never fail.  Although an edge case with no effect on contract state s health, it may happen with front end bugs or incorrect user transactions, and it is best not to have asserts fail, as per the dev comment in that line  // Should never fail .  Recommendation  Replace assert() with a require() or verify that the delegatee belongs to the user prior to performing any operations.  6 Findings: xTRIBE  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"}, {"title": "6.1 xTRIBE.emitVotingBalances - DelegateVotesChanged event can be emitted by anyone    ", "body": "  Resolution   Fixed in   fei-protocol/xTRIBE@ea9705b by adding authentication.  Description  xTRIBE.emitVotingBalances is an external function without authentication constraints. It means anyone can call it and emit DelegateVotesChanged which may impact other layers of code that rely on these events.  Examples  code-xTRIBE/src/xTRIBE.sol:L89-L99  function emitVotingBalances(address[] calldata accounts) external {  uint256 size = accounts.length;  for (uint256 i = 0; i < size; ) {  emit DelegateVotesChanged(accounts[i], 0, getVotes(accounts[i]));  unchecked {  i++;  Recommendation  Consider restricting access to this function for allowed accounts only.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2022/04/tribe-dao-flywheel-v2-xtribe-xerc4626/"}, {"title": "6.1 IdleCDO._deposit() allows re-entrancy from hookable tokens. ", "body": "  Resolution   The development team has addressed this concern in commit   5fbdc0506c94a172abbd4122276ed2bd489d1964. This change has not been reviewed by the audit team.  Description  The function IdleCDO._deposit() updates the system s internal accounting and mints shares to the caller, then transfers the deposited funds from the user. Some token standards, such as ERC777, allow a callback to the source of the funds before the balances are updated in transferFrom(). This callback could be used to re-enter the protocol while already holding the minted tranche tokens and at a point where the system accounting reflects a receipt of funds that has not yet occurred.  While an attacker could not interact with IdleCDO.withdraw() within this callback because of the _checkSameTx() restriction, they would be able to interact with the rest of the protocol.  code/contracts/IdleCDO.sol:L230-L245  function _deposit(uint256 _amount, address _tranche) internal returns (uint256 _minted) {  // check that we are not depositing more than the contract available limit  _guarded(_amount);  // set _lastCallerBlock hash  _updateCallerBlock();  // check if strategyPrice decreased  _checkDefault();  // interest accrued since last depositXX/withdrawXX/harvest is splitted between AA and BB  // according to trancheAPRSplitRatio. NAVs of AA and BB are updated and tranche  // prices adjusted accordingly  _updateAccounting();  // mint tranche tokens according to the current tranche price  _minted = _mintShares(_amount, msg.sender, _tranche);  // get underlyings from sender  IERC20Detailed(token).safeTransferFrom(msg.sender, address(this), _amount);  Recommendation  Move the transferFrom() action in _deposit() to immediately after _updateCallerBlock().  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.2 IdleCDO.virtualPrice() and _updatePrices() yield different prices in a number of cases ", "body": "  Resolution  The development team implemented a new version of both functions using a third method, virtualPricesAux(), to perform the primary price calculation. Additionally, _updatePrices() was renamed to _updateAccounting().  This change was incorporated in commit ff0b69380828657f16df8683c35703b325a6b656.  Description  The function IdleCDO.virtualPrice() is used to determine the current price of a tranche. Similarly, IdleCDO._updatePrices() is used to store the latest price of a tranche, as well as update other parts of the system accounting. There are a number of cases where the prices yielded by these two functions differ. While these are primarily corner cases that are not obviously exploitable in practice, potential violations of key accounting invariants should always be considered serious.  Additionally, the use of two separate implementations of the same calculation suggest the potential for more undiscovered discrepancies, possibly of higher consequence.  As an example, in _updatePrices() the precision loss from splitting the strategy returns favors BB tranche holders. In virtualPrice() both branches of the price calculation incur precision loss, favoring the IdleCDO contract itself.  _updatePrices()  code/contracts/IdleCDO.sol:L331-L341  if (BBTotSupply == 0) {  // if there are no BB holders, all gain to AA  AAGain = gain;  } else if (AATotSupply == 0) {  // if there are no AA holders, all gain to BB  BBGain = gain;  } else {  // split the gain between AA and BB holders according to trancheAPRSplitRatio  AAGain = gain * trancheAPRSplitRatio / FULL_ALLOC;  BBGain = gain - AAGain;  virtualPrice()  code/contracts/IdleCDO.sol:L237-L245  if (_tranche == AATranche) {  // calculate gain for AA tranche  // trancheGain (AAGain) = gain * trancheAPRSplitRatio / FULL_ALLOC;  trancheNAV = lastNAVAA + (gain * _trancheAPRSplitRatio / FULL_ALLOC);  } else {  // calculate gain for BB tranche  // trancheGain (BBGain) = gain * (FULL_ALLOC - trancheAPRSplitRatio) / FULL_ALLOC;  trancheNAV = lastNAVBB + (gain * (FULL_ALLOC - _trancheAPRSplitRatio) / FULL_ALLOC);  Recommendation  Implement a single method that determines the current price for a tranche, and use this same implementation anywhere the price is needed.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.3 IdleCDO.harvest() allows price manipulation in certain circumstances ", "body": "  Resolution   The development team has addressed this concern in a pull request with a final commit hash of   5341a9391f9c42cadf26d72c9f804ca75a15f0fb. This change has not been reviewed by the audit team.  Description  The function IdleCDO.harvest() uses Uniswap to liquidate rewards earned by the contract s strategy, then updates the relevant positions and internal accounting. This function can only be called by the contract owner or the designated rebalancer address, and it accepts an array which indicates the minimum buy amounts for the liquidation of each reward token.  The purpose of permissioning this method and specifying minimum buy amounts is to prevent a sandwiching attack from manipulating the reserves of the Uniswap pools and forcing the IdleCDO contract to incur loss due to price slippage.  However, this does not effectively prevent price manipulation in all cases. Because the contract sells it s entire balance of redeemed rewards for the specified minimum buy amount, this approach does not enforce a minimum price for the executed trades. If the balance of IdleCDO or the amount of claimable rewards increases between the submission of the harvest() transaction and its execution, it may be possible to perform a profitable sandwiching attack while still satisfying the required minimum buy amounts.  The viability of this exploit depends on how effectively an attacker can increase the amount of rewards tokens to be sold without incurring an offsetting loss. The strategy contracts used by IdleCDO are expected to vary widely in their implementations, and this manipulation could potentially be done either through direct interaction with the protocol or as part of a flashbots bundle containing a large position adjustment from an honest user.  code/contracts/IdleCDO.sol:L564-L565  function harvest(bool _skipRedeem, bool _skipIncentivesUpdate, bool[] calldata _skipReward, uint256[] calldata _minAmount) external {  require(msg.sender == rebalancer || msg.sender == owner(), \"IDLE:!AUTH\");  code/contracts/IdleCDO.sol:L590-L599  // approve the uniswap router to spend our reward  IERC20Detailed(rewardToken).safeIncreaseAllowance(address(_uniRouter), _currentBalance);  // do the uniswap trade  _uniRouter.swapExactTokensForTokensSupportingFeeOnTransferTokens(  _currentBalance,  _minAmount[i],  _path,  address(this),  block.timestamp + 1  );  Recommendation  Update IdleCDO.harvest() to enforce a minimum price rather than a minimum buy amount. One method of doing so would be taking an additional array parameter indicating the amount of each token to sell in exchange for the respective buy amount.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.4 Prevent zero amount transfers/minting ", "body": "  Resolution   The development team has addressed this concern in commit   a72747da8c0ca71274f3a1506c6faf724cf82dd2. This change has not been reviewed by the audit team.  Description  Many of the functions in the system can be called with amount = 0. This is not a security issue, however a  defense in depth  approach in this and similar cases may prevent an undiscovered bug from being exploitable. Most of the functionalities that were reviewed in this audit won t create an exploitable state transition in these cases, however they will trigger a 0 token transfer or minting.  Examples  depositAA()  depositBB()  stake()  unstake()  Recommendation  Check and return early (or revert) on requests with zero amount.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.5 Missing Sanity checks ", "body": "  Resolution   The development team has addressed this concern in commit   a1d5dac0ad5f562d4c75bff99e770d92bcc2a72f. This change has not been reviewed by the audit team.  Description  The implementation of initialize() functions are missing some sanity checks. The proper checks are implemented in some of the setter functions but missing in some others.  Examples  Missing sanity check for != address(0)  code/contracts/IdleCDO.sol:L54-L57  token = _guardedToken;  strategy = _strategy;  strategyToken = IIdleCDOStrategy(_strategy).strategyToken();  rebalancer = _rebalancer;  code/contracts/IdleCDO.sol:L84-L84  guardian = _owner;  code/contracts/IdleCDO.sol:L672-L673  address _currAAStaking = AAStaking;  address _currBBStaking = BBStaking;  code/contracts/IdleCDOTrancheRewards.sol:L50-L53  idleCDO = _idleCDO;  tranche = _trancheToken;  rewards = _rewards;  governanceRecoveryFund = _governanceRecoveryFund;  Recommendation  Add sanity checks before assigning system variables.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "6.6 IdleCDO.virtualPrice() & _updatePrices() too complicated to verify ", "body": "  Resolution  These methods were revisited in the continuation of the original review and more time was allotted to them than was possible previously. Some refactoring also occurred during that time (see 6.2). However, the development team elected to maintain the general approach used in these functions.  The primary challenge in verifying their correctness remains, which is their heavy reliance on external interactions with contracts whose expected semantics are poorly defined.  Description  IdleCDO.virtualPrice() and _updatePrices() functions are used for many important functionality in the Idle system. They also have nested external calls to many other contracts (e.g. IdleTokenGovernance, IdleCDOStrategy and strategy token, IdleCDOTranche on both Tranche tokens, etc). This level of complexity for a vital function is not recommended and is considered dangerous implementation.  Examples  Recommendation  Consider refactoring the code to use less complicated logic and code flow.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/06/idle-finance/"}, {"title": "3.1 Owners can never be removed    ", "body": "  Resolution   This has been fixed in   paxosglobal/simple-multisig#5, and appropriate tests have been added.  Description  The intention of setOwners() is to replace the current set of owners with a new set of owners. However, the isOwner mapping is never updated, which means any address that was ever considered an owner is permanently considered an owner for purposes of signing transactions.  Recommendation  In setOwners_(), before adding new owners, loop through the current set of owners and clear their isOwner booleans, as in the following code:  for (uint256 i = 0; i < ownersArr.length; i++) {  isOwner[ownersArr[i]] = false;  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/paxos/"}, {"title": "6.1 Swap fees can be bypassed using redeemMasset    Addressed", "body": "  Resolution   This issue was reported independently via the bug bounty program and was   fixed early during the audit. The fix has already been deployed on mainnet using the upgrade mechanism  Description  Part of the value proposition for liquidity providers is earning fees incurred for swapping between assets. However, traders can perform fee-less swaps by providing liquidity in one bAsset, followed by calling redeemMasset() to convert the resulting mAssets back into a proportional amount of bAssets. Since removing liquidity via redeemMasset() does not incur a fee this is equivalent to doing a swap with zero fees.  As a very simple example, assuming a pool with 2 bAssets (say, DAI and USDT), it would be possible to swap 10 DAI to USDT as follows:  Add 20 DAI to the pool, receive 20 mUSD  call redeemMasset() to redeem 10 DAI and 10 USDT  Examples  The boolean argument applyFee is set to false in _redeemMasset:  code/contracts/masset/Masset.sol:L569  _settleRedemption(_recipient, _mAssetQuantity, props.bAssets, bAssetQuantities, props.indexes, props.integrators, false);  Recommendation  Charge a small redemption fee in redeemMasset().  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.2 Users can collect interest from SavingsContract by only staking mTokens momentarily    Addressed", "body": "  Resolution  The blocker on collecting interest more than once in 30 minute period. A new APY bounds check has been added to verify that supply isn t inflated by more than 0.1% within a 30 minutes window.  Description  The SAVE contract allows users to deposit mAssets in return for lending yield and swap fees. When depositing mAsset, users receive a  credit  tokens at the momentary credit/mAsset exchange rate which is updated at every deposit. However, the smart contract enforces a minimum timeframe of 30 minutes in which the interest rate will not be updated. A user who deposits shortly before the end of the timeframe will receive credits at the stale interest rate and can immediately trigger and update of the rate and withdraw at the updated (more favorable) rate after the 30 minutes window. As a result, it would be possible for users to benefit from interest payouts by only staking mAssets momentarily and using them for other purposes the rest of the time.  Examples  code/contracts/savings/SavingsManager.sol:L141-L143  // 1. Only collect interest if it has been 30 mins  uint256 timeSinceLastCollection = now.sub(previousCollection);  if(timeSinceLastCollection > THIRTY_MINUTES) {  Recommendation  Remove the 30 minutes window such that every deposit also updates the exchange rate between credits and tokens. Note that this issue was reported independently during the bug bounty program and a fix is currently being worked on.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.3 Internal accounting of vault balance may diverge from actual token balance in lending pool   ", "body": "  Resolution  After discussion with the team the risk of this invariant violation was considered negligible as the gas cost increase for querying constantly querying the lending pool would outweigh the size of the accounting error of only 1 base unit.  Description  It is possible that the vault balance for a given bAsset is greater than the corresponding balance in the lending pool. This violates one of the correctness properties stated in the audit brief. Our Harvey fuzzer was able to generate a transaction that mints a small amount (0xf500) of mAsset. Due to the way that the lending pool integration (Compound in this case) updates the vault balance it ends up greater than the available balance in the lending pool.  More specifically, the integration contract assumes that the amount deposited into the pool is equal to the amount received by the mAsset contract for the case where no transaction fees are charged for token transfers:  code/contracts/masset/platform-integrations/CompoundIntegration.sol:L45-L58  quantityDeposited = _amount;  if(_isTokenFeeCharged) {  // If we charge a fee, account for it  uint256 prevBal = _checkBalance(cToken);  require(cToken.mint(_amount) == 0, \"cToken mint failed\");  uint256 newBal = _checkBalance(cToken);  quantityDeposited = _min(quantityDeposited, newBal.sub(prevBal));  } else {  // Else just execute the mint  require(cToken.mint(_amount) == 0, \"cToken mint failed\");  emit Deposit(_bAsset, address(cToken), quantityDeposited);  For illustration, consider the following scenario: assume your current balance in a lending pool is 0. When you deposit some amount X into the lending pool your balance after the deposit may be less than X (even if the underlying token does not charge transfer fees). One reason for this is rounding, but, in theory, a lending pool could also charge fees, etc.  The vault balance is updated in function Masset._mintTo based on the amount returned by the integration.  code/contracts/masset/Masset.sol:L189  basketManager.increaseVaultBalance(bInfo.index, integrator, quantityDeposited);  code/contracts/masset/Masset.sol:L274  uint256 deposited = IPlatformIntegration(_integrator).deposit(_bAsset, quantityTransferred, _erc20TransferFeeCharged);  This violation of the correctness property is temporary since the vault balance is readjusted when interest is collected. However, the time frame of ca. 30 minutes between interest collections (may be longer if no continuous interest is distributed) means that it may be violated for substantial periods of time.  code/contracts/masset/BasketManager.sol:L243-L249  uint256 balance = IPlatformIntegration(integrations[i]).checkBalance(b.addr);  uint256 oldVaultBalance = b.vaultBalance;  // accumulate interest (ratioed bAsset)  if(balance > oldVaultBalance && b.status == BassetStatus.Normal) {  // Update balance  basket.bassets[i].vaultBalance = balance;  The regular updates due to interest collection should ensure that the difference stays relatively small. However, note that the following scenarios is feasible: assuming there is 0 DAI in the basket, a user mints X mUSD by depositing X DAI. While the interest collection hasn t been triggered yet, the user tries to redeem X mUSD for DAI. This may fail since the amount of DAI in the lending pool is smaller than X.  Recommendation  It seems like this issue could be fixed by using the balance increase from the lending pool to update the vault balance (much like for the scenario where transfer fees are charged) instead of using the amount received.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.4 Missing validation in Masset._redeemTo   ", "body": "  Resolution  An explicit check will be added with the next Masset proxy upgrade.  Description  In function _redeemTo the collateralisation ratio is not taken into account unlike in _redeemMasset:  code/contracts/masset/Masset.sol:L558-L561  uint256 colRatio = StableMath.min(props.colRatio, StableMath.getFullScale());  // Ensure payout is related to the collateralised mAsset quantity  uint256 collateralisedMassetQuantity = _mAssetQuantity.mulTruncate(colRatio);  It seems like _redeemTo should not be executed if the collateralisation ratio is below 100%. However, the contracts (that is, Masset and ForgeValidator) themselves don t seem to enforce this explicitly. Instead, the governor needs to ensure that the collateralisation ratio is only set to a value below 100% when the basket is not  healthy  (for instance, if it is considered  failed ). Failing to ensure this may allow an attacker to redeem a disproportionate amount of assets. Note that the functionality for setting the collateralisation ratio is not currently implemented in the audited code.  Recommendation  Consider enforcing the intended use of _redeemTo more explicitly. For instance, it might be possible to introduce additional input validation by requiring that the collateralisation ratio is not below 100%.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.5 Removing a bAsset might leave some tokens stuck in the vault   ", "body": "  Resolution  The issue was acknowledged and downgraded to  minor  risk as only very small token amounts can be affected. A fix will be triaged for a future update.  Description  In function _removeBasset there is existing validation to make sure only  empty  vaults are removed:  code/contracts/masset/BasketManager.sol:L464  require(bAsset.vaultBalance == 0, \"bAsset vault must be empty\");  However, this is not necessarily sufficient since the lending pool balance may be higher than the vault balance. The reason is that the vault balance is usually slightly out-of-date due to the 30 minutes time span between interest collections. Consider the scenario: (1) a user swaps out an asset 29 minutes after the last interest collection to reduce its vault balance from 100 USD to 0, and (2) the governor subsequently remove the asset. During those 29 minutes the asset was collecting interest (according to the lending pool the balance was higher than 100 USD at the time of the swap) that is now  stuck  in the vault.  Recommendation  Consider adding additional input validation (for instance, by requiring that the lending pool balance to be 0) or triggering a swap directly when removing an asset from the basket.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.6 Unused parameter in BasketManager._addBasset   ", "body": "  Resolution  While the parameter is not currently used it will be used in future mAssets such as mGOLD.  Description  It seems like the _measurementMultiple parameter is always StableMath.getRatioScale() (1e8). There is also some range validation code that seems unnecessary if the parameter is always 1e8.  code/contracts/masset/BasketManager.sol:L310  require(_measurementMultiple >= 1e6 && _measurementMultiple <= 1e10, \"MM out of range\");  Recommendation  Consider removing the parameter and the input validation to improve the readability of the code.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.7 Unused event BasketStatusChanged   ", "body": "  Resolution  This event will be used in future releases.  Description  It seems like the event BasketManager.BasketStatusChanged event is unused.  Recommendation  Consider removing the event declaration to improve the readability of the code.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.8 Assumptions are made about interest distribution   ", "body": "  Description  There is a mechanism that prevents interest collection if the extrapolated APY exceeds a threshold (MAX_APY).  code/contracts/savings/SavingsManager.sol:L174  require(extrapolatedAPY < MAX_APY, \"Interest protected from inflating past maxAPY\");  The extrapolation seems to assume that the interest is payed out frequently and continuously. It seems like a less frequent payout (for instance, once a month/year) could be rejected since the extrapolation considers the interest since the last time that collectAndDistributeInterest was called (potentially without interest being collected).  Recommendation  Consider revisiting or documenting this assumption. For instance, one could consider extrapolating between the current time and the last time that (non-zero) interest was actually collected.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.9 Assumptions are made about Aave and Compound integrations   ", "body": "  Resolution  it was acknowledged that unexpected changes in behaviour by the integrated lending pools could potentially cause issues; However, it was decided that the risk is minor since the current lending pool behaviour is known and the fact that lending pools might introduce severe changes is accounted for by keeping the integrations separate and upgradable such that governance can react these changes in time.  Description  The code makes several assumptions about the Aave and Compound integrations. A malicious or malfunctioning integration (or lending pool) might violate those assumptions. This might lead to unintended behavior in the system. Below are three such assumptions:  function checkBalance reverts if the token hasn t been added:  code/contracts/masset/BasketManager.sol:L317  IPlatformIntegration(_integration).checkBalance(_bAsset);  function withdraw is trusted to not fail when it shouldn t:  code/contracts/masset/Masset.sol:L611  IPlatformIntegration(_integrators[i]).withdraw(_recipient, bAsset, q, _bAssets[i].isTransferFeeCharged);  the mapping from mAssets to pTokens is fixed:  code/contracts/masset/platform-integrations/InitializableAbstractIntegration.sol:L119  require(bAssetToPToken[_bAsset] == address(0), \"pToken already set\");  The first assumption could be avoided by adding a designated function to check if the token was added.  The second assumption is more difficult to avoid, but should be considered when adding new integrations. The system needs to trust the lending pools to work properly; for instance, if the lending pool would blacklist the integration contract the system may behave in unintended ways.  The third assumption could be avoided, but it comes at a cost.  Recommendation  Consider revisiting or avoiding these assumptions. For any assumptions that are there by design it would be good to document them to facilitate future changes. One should also be careful to avoid coupling between external systems. For instance, if withdrawing from Aave fails this should not prevent withdrawing from Compound.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}, {"title": "6.10 Assumptions are made about bAssets   ", "body": "  Description  The code makes several assumptions about the bAssets that can be used. A malicious or malfunctioning asset contract might violate those assumptions. This might lead to unintended behavior in the system. Below there are several such assumptions:  Decimals of a bAsset are constant where the decimals are used to derive the asset s ratio:  code/contracts/masset/BasketManager.sol:L319  uint256 bAsset_decimals = CommonHelpers.getDecimals(_bAsset);  Decimals must be in a range from 4 to 18:  code/contracts/shared/CommonHelpers.sol:L23  require(decimals >= 4 && decimals <= 18, \"Token must have sufficient decimal places\");  The governor is able to foresee when transfer fees are charged (which needs to be called if anything changes); in theory, assets could be much more flexible in when transfer fees are charged (for instance, during certain periods or for certain users)  code/contracts/masset/BasketManager.sol:L425  function setTransferFeesFlag(address _bAsset, bool _flag)  It seems like some of these assumptions could be avoided, but there might be a cost. For instance, one could retrieve the decimals directly instead of  caching  them and one could always enable the setting where transfer fees may be charged.  Recommendation  Consider revisiting or avoiding these assumptions. For any assumptions that are there by design it would be good to document them to facilitate future changes.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/07/mstable-1.1/"}]