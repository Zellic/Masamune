[{"title": "4.13 UI/hooks - detectEthereumProvider() Should Require mustBeMetaMask  ", "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, minor impact  Description  MetaMask Snaps require a Metamask provider. However, detectEthereumProvider() does not explicitly require a MetaMask provider and would continue if the alternative provider contains the substring flask in their signature.  packages/wallet-ui/src/hooks/useHasMetamaskFlask.ts:L7-L16  const detectMetamaskFlask = async () => {  try {  const provider = (await detectEthereumProvider({  mustBeMetaMask: false,  silent: true,  })) as any | undefined;  const isFlask = (await provider?.request({ method: 'web3_clientVersion' }))?.includes('flask');  if (provider && isFlask) {  return true;  Consider requiring mustBeMetaMask = true to enforce that the injected provider is indeed MetaMask. This will also work with MetaMask Flask as shown here:  \u21d2 window.ethereum.isMetaMask  true  \u21d2 await window.ethereum.request({ method: 'web3_clientVersion' })  'MetaMask/v10.32.0-flask.0'  ", "labels": ["Consensys", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "4.14 RPC starkNet_addNetwork - Not Implemented, No User Confirmation  ", "body": "  Resolution  Won t Fix. The client provided the following statement:  not fix, minor impact  Description  It was observed that the RPC method starkNet_addNetwork is not implemented.  In case this method is to be exposed to dapps, we recommended to follow the advise given in issue 4.6 to ask for user confirmation when adjusting the snaps configuration state.  ", "labels": ["Consensys", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/metamask/partner-snaps-starknetsnap/"}, {"title": "5.1 RocketNodeDistributorDelegate - Reentrancy in distribute() allows node owner to drain distributor funds    ", "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by implementing a custom reentrancy guard via a new state variable lock that is appended to the end of the storage layout. The reentrancy guard is functionally equivalent to the OpenZeppelin implementation. The method was not refactored to give user funds priority over the node share. Additionally, the client provided the following statement:  We acknowledge this as a critical issue and have solved with a reentrancy guard.  We followed OpenZeppelin s design for a reentrancy guard. We were unable to use it directly as it is hardcoded to use storage slot 0 and because we already have deployment of this delegate in the wild already using storage slot 0 for another purpose, we had to append it to the end of the existing storage layout.  Description  The distribute() function distributes the contract s balance between the node operator and the user. The node operator is returned their initial collateral, including a fee. The rest is returned to the RETH token contract as user collateral.  After determining the node owner s share, the contract transfers ETH to the node withdrawal address, which can be the configured withdrawal address or the node address. Both addresses may potentially be a malicious contract that recursively calls back into the distribute() function to retrieve the node share multiple times until all funds are drained from the contract. The distribute() function is not protected against reentrancy:  code/contracts/contract/node/RocketNodeDistributorDelegate.sol:L59-L73  /// @notice Distributes the balance of this contract to its owners  function distribute() override external {  // Calculate node share  uint256 nodeShare = getNodeShare();  // Transfer node share  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  (bool success,) = withdrawalAddress.call{value : nodeShare}(\"\");  require(success);  // Transfer user share  uint256 userShare = address(this).balance;  address rocketTokenRETH = rocketStorage.getAddress(rocketTokenRETHKey);  payable(rocketTokenRETH).transfer(userShare);  // Emit event  emit FeesDistributed(nodeAddress, userShare, nodeShare, block.timestamp);  We also noticed that any address could set a withdrawal address as there is no check for the caller to be a registered node. In fact, the caller can be the withdrawal address or node operator.  code/contracts/contract/RocketStorage.sol:L118-L133  // Set a node's withdrawal address  function setWithdrawalAddress(address _nodeAddress, address _newWithdrawalAddress, bool _confirm) external override {  // Check new withdrawal address  require(_newWithdrawalAddress != address(0x0), \"Invalid withdrawal address\");  // Confirm the transaction is from the node's current withdrawal address  address withdrawalAddress = getNodeWithdrawalAddress(_nodeAddress);  require(withdrawalAddress == msg.sender, \"Only a tx from a node's withdrawal address can update it\");  // Update immediately if confirmed  if (_confirm) {  updateWithdrawalAddress(_nodeAddress, _newWithdrawalAddress);  // Set pending withdrawal address if not confirmed  else {  pendingWithdrawalAddresses[_nodeAddress] = _newWithdrawalAddress;  Recommendation  Add a reentrancy guard to functions that interact with untrusted contracts. Adhere to the checks-effects pattern and send user funds to the  trusted  RETH contract first. Only then send funds to the node s withdrawal address.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.2 RocketMinipoolDelegateOld - Node operator may reenter finalise() to manipulate accounting    ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this issue (it was reported via our Immunefi bug bounty). It is live but that code path is inaccessible. It requires the oDAO to mark a minipool as Withdrawable which we don t do and have removed from the withdrawal process moving forward.  In a later revision, the development team fixed the issue in the following commit: 73d5792a671db5d2f4dcbd35737e729f9e01aa11  Description  node.minipools.finalised.count<NodeAddress>: NodeAddress finalised count increased twice instead  minipools.finalised.count: global finalised count increased twice  eth.matched.node.amount<NodeAddress> - NodeAddress eth matched amount potentially reduced too many times; has an impact on getNodeETHCollateralisationRatio -> GetNodeShare, getNodeETHProvided -> getNodeEffectiveRPLStake and getNodeETHProvided->getNodeMaximumRPLStake->withdrawRPL and is the limiting factor when withdrawing RPL to ensure the pools stay collateralized.  Note: RocketMinipoolDelegateOld is assumed to be the currently deployed MiniPool implementation. Users may upgrade from this delegate to the new version and can roll back at any time and re-upgrade, even within the same transaction (see issue 5.3 ).  The following is an annotated call stack from a node operator calling minipool.finalise() reentering finalise() once more on their Minipool:  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L182-L191  // Called by node operator to finalise the pool and unlock their RPL stake  function finalise() external override onlyInitialised onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) {  // Can only call if withdrawable and can only be called once  require(status == MinipoolStatus.Withdrawable, \"Minipool must be withdrawable\");  // Node operator cannot finalise the pool unless distributeBalance has been called  require(withdrawalBlock > 0, \"Minipool balance must have been distributed at least once\");  // Finalise the pool  _finalise();  _refund() handing over control flow to nodeWithdrawalAddress  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L311-L341  // Perform any slashings, refunds, and unlock NO's stake  function _finalise() private {  // Get contracts  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  // Can only finalise the pool once  require(!finalised, \"Minipool has already been finalised\");  // If slash is required then perform it  if (nodeSlashBalance > 0) {  _slash();  // Refund node operator if required  if (nodeRefundBalance > 0) {  _refund();  // Send any left over ETH to rETH contract  if (address(this).balance > 0) {  // Send user amount to rETH contract  payable(rocketTokenRETH).transfer(address(this).balance);  // Trigger a deposit of excess collateral from rETH contract to deposit pool  RocketTokenRETHInterface(rocketTokenRETH).depositExcessCollateral();  // Unlock node operator's RPL  rocketMinipoolManager.incrementNodeFinalisedMinipoolCount(nodeAddress);  // Update unbonded validator count if minipool is unbonded  if (depositType == MinipoolDeposit.Empty) {  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  rocketDAONodeTrusted.decrementMemberUnbondedValidatorCount(nodeAddress);  // Set finalised flag  finalised = true;  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L517-L528  function _refund() private {  // Update refund balance  uint256 refundAmount = nodeRefundBalance;  nodeRefundBalance = 0;  // Get node withdrawal address  address nodeWithdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  // Transfer refund amount  (bool success,) = nodeWithdrawalAddress.call{value : refundAmount}(\"\");  require(success, \"ETH refund amount was not successfully transferred to node operator\");  // Emit ether withdrawn event  emit EtherWithdrawn(nodeWithdrawalAddress, refundAmount, block.timestamp);  Methods adjusting system settings called twice:  code/contracts/contract/old/minipool/RocketMinipoolManagerOld.sol:L265-L272  // Increments _nodeAddress' number of minipools that have been finalised  function incrementNodeFinalisedMinipoolCount(address _nodeAddress) override external onlyLatestContract(\"rocketMinipoolManager\", address(this)) onlyRegisteredMinipool(msg.sender) {  // Update the node specific count  addUint(keccak256(abi.encodePacked(\"node.minipools.finalised.count\", _nodeAddress)), 1);  // Update the total count  addUint(keccak256(bytes(\"minipools.finalised.count\")), 1);  code/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L139-L142  function decrementMemberUnbondedValidatorCount(address _nodeAddress) override external onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) onlyRegisteredMinipool(msg.sender) {  subUint(keccak256(abi.encodePacked(daoNameSpace, \"member.validator.unbonded.count\", _nodeAddress)), 1);  Recommendation  We recommend setting the finalised = true flag immediately after checking for it. Additionally, the function flow should adhere to the checks-effects-interactions pattern whenever possible. We recommend adding generic reentrancy protection whenever the control flow is handed to an untrusted entity.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.3 RocketMinipoolDelegate - Sandwiching of Minipool calls can have unintended side effects ", "body": "  Resolution  The client provided the following statement:  The slashed value is purely for NO informational purposes and not used in any logic in the contracts so this example is benign as you say. We have fixed this particular issue by moving the slashed boolean out of the delegate and into RocketMinipooLManager. It is now set on any call to rocketNodeStaking.slashRPL which covers both old delegate and new.  We appreciate that the finding was more a classification of potential issues with upgrades and rollbacks. At this stage, we cannot change this functionality as it is already deployed in a non-upgradable way to over 12,000 contracts.  As this is more of a guidance and there is no immediate threat, we don t believe this should be considered a  major  finding.  With https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 the slashed flag was moved to RocketNodeStaking.slashRPL() (minipool.rpl.slashed|<msg.sender> = true).  The audit team acknowledges that this issue does not provide a concrete exploit that puts funds at risk. However, due to the sensitive nature and potential for issues regarding future updates, we stand by the initial severity rating as it stands for security vulnerabilities that may not be directly exploitable or require certain conditions to be exploited.  Description  The RocketMinipoolBase contract exposes the functions delegateUpgrade and delegateRollback, allowing the minipool owner to switch between delegate implementations. While giving the minipool owner a chance to roll back potentially malfunctioning upgrades, the fact that upgrades and rollback are instantaneous also gives them a chance to alternate between executing old and new code (e.g. by utilizing callbacks) and sandwich user calls to the minipool.  Examples  Assuming the latest minipool delegate implementation, any user can call RocketMinipoolDelegate.slash, which slashes the node operator s RPL balance if a slashing has been recorded on their validator. To mark the minipool as having been slashed, the slashed contract variable is set to true. A minipool owner can avoid this flag from being set By sandwiching the user calls:  Minipool owner rolls back to the old implementation from RocketMinipoolDelegateOld.sol  User calls slash on the now old delegate implementation (where slashed is not set)  Minipool owner upgrades to the latest delegate implementation again  In detail, the new slash implementation:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L687-L696  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  // Record slashing  slashed = true;  Compared to the old slash implementation:  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L531-L539  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  While the bypass of slashed being set is a benign example, the effects of this issue, in general, could result in a significant disruption of minipool operations and potentially affect the system s funds. The impact highly depends on the changes introduced by future minipool upgrades.  Recommendation  We recommend limiting upgrades and rollbacks to prevent minipool owners from switching implementations with an immediate effect. A time lock can fulfill this purpose when a minipool owner announces an upgrade to be done at a specific block. A warning can precede user-made calls that an upgrade is pending, and their interaction can have unintended side effects.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.4 RocketDAONodeTrustedActions - No way to access ETH provided by non-member votes   ", "body": "  Resolution  According to the client, this is the intended behavior. The client provided the following statement:  This is by design.  Description  DAO members can challenge nodes to prove liveliness for free. Non-DAO members must provide members.challenge.cost = 1 eth to start a challenge. However, the provided challenge cost is locked within the contract instead of being returned or recycled as system collateral.  Examples  code/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L181-L192  // In the event that the majority/all of members go offline permanently and no more proposals could be passed, a current member or a regular node can 'challenge' a DAO members node to respond  // If it does not respond in the given window, it can be removed as a member. The one who removes the member after the challenge isn't met, must be another node other than the proposer to provide some oversight  // This should only be used in an emergency situation to recover the DAO. Members that need removing when consensus is still viable, should be done via the 'kick' method.  function actionChallengeMake(address _nodeAddress) override external onlyTrustedNode(_nodeAddress) onlyRegisteredNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrustedActions\", address(this)) payable {  // Load contracts  RocketDAONodeTrustedInterface rocketDAONode = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  RocketDAONodeTrustedSettingsMembersInterface rocketDAONodeTrustedSettingsMembers = RocketDAONodeTrustedSettingsMembersInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMembers\"));  // Members can challenge other members for free, but for a regular bonded node to challenge a DAO member, requires non-refundable payment to prevent spamming  if(rocketDAONode.getMemberIsValid(msg.sender) != true) require(msg.value == rocketDAONodeTrustedSettingsMembers.getChallengeCost(), \"Non DAO members must pay ETH to challenge a members node\");  // Can't challenge yourself duh  require(msg.sender != _nodeAddress, \"You cannot challenge yourself\");  // Is this member already being challenged?  Recommendation  We recommend locking the ETH inside the contract during the challenge process. If a challenge is refuted, we recommend feeding the locked value back into the system as protocol collateral. If the challenge succeeds and the node is kicked, it is assumed that the challenger will be repaid the amount they had to lock up to prove non-liveliness.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.5 Multiple checks-effects violations ", "body": "  Resolution  The client provided the following statement:  In many of the cited examples, the  external call  is a call to another network contract that has the same privileges as the caller. Preventing reentrancy against our own internal contracts provides no additional security. If a malicious contract is introduced via a malicious oDAO they already have full keys to the kingdom.  None of the examples provide an attack surface and so we don t believe this to be a  major  finding and should be downgraded.  This finding highlights our concerns about a dangerous pattern used throughout the codebase that may eventually lead to exploitable scenarios if continued to be followed, especially on codebases that do not employ protective measures against reentrant calls. This report also flagged one such exploitable instance, leading to a critical exploitable issue in one of the components.  This repeated occurrence led us to flag this as a major issue to highlight a general error and attack surface present in several places.  From our experience, there are predominantly positive side-effects of adhering to safe coding patterns, even for trusted contract interactions, as developers indirectly follow or pick up the coding style from existing code, reducing the likelihood of following a pattern that may be prone to be taken advantage of.  For example, to a developer, it might not always be directly evident that control flow is passed to potentially untrusted components/addresses from the code itself, especially when calling multiple  trusted  components in the system. Furthermore, individual components down the call stack may be updated at later times, introducing an untrusted external call (i.e., because funds are refunded) and exposing the initially calling contract to a reentrancy-type issue. Therefore, we highly recommend adhering to a safe checks-effects pattern even though the contracts mainly interact with other trusted components and build secure code based on defense-in-depth principles to contain potential damage in favor of assuming worst-case scenarios.  Description  Throughout the system, there are various violations of the checks-effects-interactions pattern where the contract state is updated after an external call. Since large parts of the Rocket Pool system s smart contracts are not guarded against reentrancy, the external call s recipient may reenter and potentially perform malicious actions that can impact the overall accounting and, thus, system funds.  Examples  distributeToOwner() sends the contract s balance to the node or the withdrawal address before clearing the internal accounting:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L564-L581  /// @notice Withdraw node balances from the minipool and close it. Only accepts calls from the owner  function close() override external onlyMinipoolOwner(msg.sender) onlyInitialised {  // Check current status  require(status == MinipoolStatus.Dissolved, \"The minipool can only be closed while dissolved\");  // Distribute funds to owner  distributeToOwner();  // Destroy minipool  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  require(rocketMinipoolManager.getMinipoolExists(address(this)), \"Minipool already closed\");  rocketMinipoolManager.destroyMinipool();  // Clear state  nodeDepositBalance = 0;  nodeRefundBalance = 0;  userDepositBalance = 0;  userDepositBalanceLegacy = 0;  userDepositAssignedTime = 0;  The withdrawal block should be set before any other contracts are called:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L498-L499  // Save block to prevent multiple withdrawals within a few blocks  withdrawalBlock = block.number;  The slashed state should be set before any external calls are made:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L686-L696  /// @dev Slash node operator's RPL balance based on nodeSlashBalance  function _slash() private {  // Get contracts  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  // Slash required amount and reset storage value  uint256 slashAmount = nodeSlashBalance;  nodeSlashBalance = 0;  rocketNodeStaking.slashRPL(nodeAddress, slashAmount);  // Record slashing  slashed = true;  In the bond reducer, the accounting values should be cleared before any external calls are made:  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L120-L134  // Get desired to amount  uint256 newBondAmount = getUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));  require(rocketNodeDeposit.isValidDepositAmount(newBondAmount), \"Invalid bond amount\");  // Calculate difference  uint256 existingBondAmount = minipool.getNodeDepositBalance();  uint256 delta = existingBondAmount.sub(newBondAmount);  // Get node address  address nodeAddress = minipool.getNodeAddress();  // Increase ETH matched or revert if exceeds limit based on current RPL stake  rocketNodeDeposit.increaseEthMatched(nodeAddress, delta);  // Increase node operator's deposit credit  rocketNodeDeposit.increaseDepositCreditBalance(nodeAddress, delta);  // Clean up state  deleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.time\", msg.sender)));  deleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));  The counter for reward snapshot execution should be incremented before RPL gets minted:  code/contracts/contract/rewards/RocketRewardsPool.sol:L210-L213  // Execute inflation if required  rplContract.inflationMintTokens();  // Increment the reward index and update the claim interval timestamp  incrementRewardIndex();  Recommendation  We recommend following the checks-effects-interactions pattern and adjusting any contract state variables before making external calls. With the upgradeable nature of the system, we also recommend strictly adhering to this practice when all external calls are being made to trusted network contracts.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.6 Minipool state machine design and pseudo-states   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement.  We agree that the state machine is complicated. This is a symptom of technical debt and backwards compatibility.  There is no actionable response to this finding as we cannot make changes to the existing 12,000 contracts already deployed.  We want to emphasize that this finding strongly suggests that there are design deficits in the minipool state machine that, sooner or later, may impact the overall system s security. We suggest refactoring a clean design with clear transitions and states for the current iteration removing technical debt from future versions. This may mean that it may be warranted to release a new major Rocketpool version as a standalone system with a clean migration path avoiding potential problems otherwise introduced by dealing with the current technical debt.  Description  Recommendation  We strongly discourage the use of pseudo-states in state machines as they make the state machine less intuitive and present challenges in mapping state transitions to the code base. Real states and transitions should be used where possible.  Generally, we recommend the following when designing state machines:  Using clear and descriptive transition names,  Avoiding having multiple transitions with the same trigger,  Modeling decisions in the form of state transitions rather than states themselves.  In any case, every Minipool should terminate in a clear end state.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.7 RocketMinipoolDelegate - Redundant refund() call on forced finalization    ", "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by refactoring refund() to avoid a double invocation of _refund() in the _finalise() codepath.  Fixed per the recommendation. Thanks.  Description  The RocketMinipoolDelegate.refund function will force finalization if a user previously distributed the pool. However, _finalise already calls _refund() if there is a node refund balance to transfer, making the additional call to _refund() in refund() obsolete.  Examples  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L200-L209  function refund() override external onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) onlyInitialised {  // Check refund balance  require(nodeRefundBalance > 0, \"No amount of the node deposit is available for refund\");  // If this minipool was distributed by a user, force finalisation on the node operator  if (!finalised && userDistributed) {  _finalise();  // Refund node  _refund();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L445-L459  function _finalise() private {  // Get contracts  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  // Can only finalise the pool once  require(!finalised, \"Minipool has already been finalised\");  // Set finalised flag  finalised = true;  // If slash is required then perform it  if (nodeSlashBalance > 0) {  _slash();  // Refund node operator if required  if (nodeRefundBalance > 0) {  _refund();  Recommendation  We recommend refactoring the if condition to contain _refund() in the else branch.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.8 Sparse documentation and accounting complexity   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  Acknowledged and agree.  Description  Throughout the project, inline documentation is either sparse or missing altogether. Furthermore, few technical documents about the system s design rationale are available. The recent releases  increased complexity makes it significantly harder to trace the flow of funds through the system as components change semantics, are split into separate contracts, etc.  It is essential that documentation not only outlines what is being done but also why and what a function s role in the system s  bigger picture  is. Many comments in the code base fail to fulfill this requirement and are thus redundant, e.g.  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L292-L293  // Sanity check that refund balance is zero  require(nodeRefundBalance == 0, \"Refund balance not zero\");  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L333-L334  // Remove from vacant set  rocketMinipoolManager.removeVacantMinipool();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L381-L383  if (ownerCalling) {  // Finalise the minipool if the owner is calling  _finalise();  The increased complexity and lack of documentation can increase the likelihood of developer error. Furthermore, the time spent maintaining the code and introducing new developers to the code base will drastically increase. This effect can be especially problematic in the system s accounting of funds as the various stages of a Minipool imply different flows of funds and interactions with external dependencies. Documentation should explain the rationale behind specific hardcoded values, such as the magic 8 ether boundary for withdrawal detection. An example of a lack of documentation and distribution across components is the calculation and influence of ethMatched as it plays a role in:  the minipool bond reducer,  the node deposit contract,  the node manager, and  the node staking contract.  Recommendation  As the Rocketpool system grows in complexity, we highly recommend significantly increasing the number of inline comments and general technical documentation and exploring ways to centralize the system s accounting further to provide a clear picture of which funds move where and at what point in time. Where the flow of funds is obscured because multiple components or multi-step processes are involved, we recommend adding extensive inline documentation to give context.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.9 RocketNodeDistributor - Missing extcodesize check in dynamic proxy   ", "body": "  Resolution  The client decided not to address the finding with the upcoming update. As per their assessment, the scenario outlined would require a series of misconfigurations/failures and hence is unlikely to happen. Following a defense-in-depth approach we, nevertheless, urge to implement safeguards on multiple layers as a condition like this can easily go undetected. However, after reviewing the feedback provided by the client we share the assessment that the finding should be downgraded from Major to Medium as funds are not at immediate risk and they can recover from this problem by fixing the delegate. For transparency, the client provided the following statement:  Agree that an extcodesize check here would add safety against a future mistake. But it does require a failure at many points for it to actually lead to an issue. Beacuse this contract is not getting upgraded in Atlas, we will leave it as is. We will make note to add a safety check on it in a future update of this contract.  We don t believe this consitutes a  major  finding given that it requires a future significant failure. If such a failure were to happen, the impact is also minimal as any calls to distribute() would simply do nothing. A contract upgrade would fix the problem and no funds would be at risk.  Description  Examples  code/contracts/contract/node/RocketNodeDistributor.sol:L23-L31  fallback() external payable {  address _target = rocketStorage.getAddress(distributorStorageKey);  assembly {  calldatacopy(0x0, 0x0, calldatasize())  let result := delegatecall(gas(), _target, 0x0, calldatasize(), 0x0, 0)  returndatacopy(0x0, 0x0, returndatasize())  switch result case 0 {revert(0, returndatasize())} default {return (0, returndatasize())}  code/contracts/contract/RocketStorage.sol:L153-L155  function getAddress(bytes32 _key) override external view returns (address r) {  return addressStorage[_key];  Recommendation  Before delegate-calling into the target contract, check if it exists.  assembly {  codeSize := extcodesize(_target)  require(codeSize > 0);  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.10 Kicked oDAO members  votes taken into account   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but the additional changes required to implement a fix outweigh the concern in our opinion.  Description  oDAO members can vote on proposals or submit external data to the system, acting as an oracle. Data submission is based on a vote by itself, and multiple oDAO members must submit the same data until a configurable threshold (51% by default) is reached for the data to be confirmed.  When a member gets kicked or leaves the oDAO after voting, their vote is still accounted for while the total number of oDAO members decreases.  A (group of) malicious oDAO actors may exploit this fact to artificially lower the consensus threshold by voting for a proposal and then leaving the oDAO. This will leave excess votes with the proposal while the total member count decreases.  For example, let s assume there are 17 oDAO members. 9 members must vote for the proposal for it to pass (52.9%). Let s assume 8 members voted for, and the rest abstained and is against the proposal (47%, threshold not met). The proposal is unlikely to pass unless two malicious oDAO members leave the DAO, lowering the member count to 15 in an attempt to manipulate the vote, suddenly inflating vote power from 8/17 (47%; rejected) to 8/15 (53.3%; passed).  The crux is that the votes of ex-oDAO members still count, while the quorum is based on the current oDAO member number.  Here are some examples, however, this is a general pattern used for oDAO votes in the system.  Example: RocketNetworkPrices  Members submit votes via submitPrices(). If the threshold is reached, the proposal is executed. Quorum is based on the current oDAO member count, votes of ex-oDAO members are still accounted for. If a proposal is a near miss, malicious actors can force execute it by leaving the oDAO, lowering the threshold, and then calling executeUpdatePrices() to execute it.  code/contracts/contract/network/RocketNetworkPrices.sol:L75-L79  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  // Update the price  updatePrices(_block, _rplPrice);  code/contracts/contract/network/RocketNetworkPrices.sol:L85-L86  function executeUpdatePrices(uint256 _block, uint256 _rplPrice) override external onlyLatestContract(\"rocketNetworkPrices\", address(this)) {  // Check settings  RocketMinipoolBondReducer  The RocketMinipoolBondReducer contract s voteCancelReduction function takes old votes of previously kicked oDAO members into account. This results in the vote being significantly higher and increases the potential for malicious actors, even after their removal, to sway the vote. Note that a canceled bond reduction cannot be undone.  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L94-L98  RocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));  uint256 quorum = rocketDAONode.getMemberCount().mul(rocketDAONodeTrustedSettingsMinipool.getCancelBondReductionQuorum()).div(calcBase);  bytes32 totalCancelVotesKey = keccak256(abi.encodePacked(\"minipool.bond.reduction.vote.count\", _minipoolAddress));  uint256 totalCancelVotes = getUint(totalCancelVotesKey).add(1);  if (totalCancelVotes > quorum) {  RocketNetworkPenalties  code/contracts/contract/network/RocketNetworkPenalties.sol:L47-L51  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodePenaltyThreshold()) {  setBool(executedKey, true);  incrementMinipoolPenaltyCount(_minipoolAddress);  code/contracts/contract/network/RocketNetworkPenalties.sol:L54-L58  // Executes incrementMinipoolPenaltyCount if consensus threshold is reached  function executeUpdatePenalty(address _minipoolAddress, uint256 _block) override external onlyLatestContract(\"rocketNetworkPenalties\", address(this)) {  // Get contracts  RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));  // Get submission keys  Recommendation  Track oDAO members  votes and remove them from the tally when the removal from the oDAO is executed.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.11 RocketDAOProtocolSettingsRewards - settings key collission   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but making this change now with an existing deployment outweighs the concern in our opinion.  Description  A malicious user may craft a DAO protocol proposal to set a rewards claimer for a specific contract, thus overwriting another contract s settings. This issue arises due to lax requirements when choosing safe settings keys.  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49  function setSettingRewardsClaimer(string memory _contractName, uint256 _perc) override public onlyDAOProtocolProposal {  // Get the total perc set, can't be more than 100  uint256 percTotal = getRewardsClaimersPercTotal();  // If this group already exists, it will update the perc  uint256 percTotalUpdate = percTotal.add(_perc).sub(getRewardsClaimerPerc(_contractName));  // Can't be more than a total claim amount of 100%  require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");  // Update the total  setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);  // Update/Add the claimer amount  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", _contractName)), _perc);  // Set the time it was updated at  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", _contractName)), block.timestamp);  The method updates the rewards claimer for a specific contract by writing to the following two setting keys:  settingNameSpace.rewards.claimsgroup.amount<_contractName>  settingNameSpace.rewards.claimsgroup.amount.updated.time<_contractName>  Due to the way the settings hierarchy was chosen in this case, a malicious proposal might define a <_contractName> = .updated.time<targetContract> that overwrites the settings of a different contract with an invalid value.  Note that the issue of delimiter consistency is also discussed in issue 5.12.  The severity rating is based on the fact that this should be detectable by DAO members. However, following a defense-in-depth approach means that such collisions should be avoided wherever possible.  Recommendation  We recommend enforcing a unique prefix and delimiter when concatenating user-provided input to setting keys. In this specific case, the settings could be renamed as follows:  settingNameSpace.rewards.claimsgroup.amount.value<_contractName>  settingNameSpace.rewards.claimsgroup.amount.updated.time<_contractName>  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.12 RocketDAOProtocolSettingsRewards - missing setting delimiters   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  We are aware of this limitation but making this change now with an existing deployment outweighs the concern in our opinion.  Description  Settings in the Rocket Pool system are hierarchical, and namespaces are prefixed using dot delimiters.  Calling abi.encodePacked(<string>, <string>) on strings performs a simple concatenation. According to the settings  naming scheme, it is suggested that the following example writes to a key named: <settingNameSpace>.rewards.claims.group.amount.<_contractName>. However, due to missing delimiters, the actual key written to is: <settingNameSpace>.rewards.claimsgroup.amount<_contractName>.  Note that there is no delimiter between claims|group and amount|<_contractName>.  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49  function setSettingRewardsClaimer(string memory _contractName, uint256 _perc) override public onlyDAOProtocolProposal {  // Get the total perc set, can't be more than 100  uint256 percTotal = getRewardsClaimersPercTotal();  // If this group already exists, it will update the perc  uint256 percTotalUpdate = percTotal.add(_perc).sub(getRewardsClaimerPerc(_contractName));  // Can't be more than a total claim amount of 100%  require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");  // Update the total  setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);  // Update/Add the claimer amount  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", _contractName)), _perc);  // Set the time it was updated at  setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", _contractName)), block.timestamp);  Recommendation  We recommend adding the missing intermediate delimiters. The system should enforce delimiters after the last setting key before user input is concatenated to reduce the risk of accidental namespace collisions.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.13 Use of address instead of specific contract types   ", "body": "  Resolution  The client acknowledges the finding, removed the unnecessary casts from canReduceBondAmount and voteCancelReduction with https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6, and provided the following statement:  Acknowledged. We will migrate to this pattern as we upgrade contracts.  Description  Rather than using a low-level address type and then casting to the safer contract type, it s better to use the best type available by default so the compiler can eventually check for type safety and contract existence and only downcast to less secure low-level types (address) when necessary.  Examples  RocketStorageInterface _rocketStorage should be declared in the arguments, removing the need to cast the address explicitly.  code/contracts/contract/minipool/RocketMinipoolBase.sol:L39-L47  /// @notice Sets up starting delegate contract and then delegates initialisation to it  function initialise(address _rocketStorage, address _nodeAddress) external override notSelf {  // Check input  require(_nodeAddress != address(0), \"Invalid node address\");  require(storageState == StorageState.Undefined, \"Already initialised\");  // Set storage state to uninitialised  storageState = StorageState.Uninitialised;  // Set rocketStorage  rocketStorage = RocketStorageInterface(_rocketStorage);  RocketMinipoolInterface _minipoolAddress should be declared in the arguments, removing the need to cast the address explicitly. Downcast to low-level address if needed. The event can be redeclared with the contract type.  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L33-L34  function beginReduceBondAmount(address _minipoolAddress, uint256 _newBondAmount) override external onlyLatestContract(\"rocketMinipoolBondReducer\", address(this)) {  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L69-L76  /// @notice Returns whether owner of given minipool can reduce bond amount given the waiting period constraint  /// @param _minipoolAddress Address of the minipool  function canReduceBondAmount(address _minipoolAddress) override public view returns (bool) {  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  RocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));  uint256 reduceBondTime = getUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.time\", _minipoolAddress)));  return rocketDAONodeTrustedSettingsMinipool.isWithinBondReductionWindow(block.timestamp.sub(reduceBondTime));  code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L80-L84  function voteCancelReduction(address _minipoolAddress) override external onlyTrustedNode(msg.sender) onlyLatestContract(\"rocketMinipoolBondReducer\", address(this)) {  // Prevent calling if consensus has already been reached  require(!getReduceBondCancelled(_minipoolAddress), \"Already cancelled\");  // Get contracts  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipoolAddress);  Note that abi.encode*(contractType) assumes address for contract types by default. An explicit downcast is not required.  More examples of address _minipool declarations:  code/contracts/contract/minipool/RocketMinipoolManager.sol:L449-L455  /// @dev Internal logic to set a minipool's pubkey  /// @param _pubkey The pubkey to set for the calling minipool  function _setMinipoolPubkey(address _minipool, bytes calldata _pubkey) private {  // Load contracts  AddressSetStorageInterface addressSetStorage = AddressSetStorageInterface(getContractAddress(\"addressSetStorage\"));  // Initialize minipool & get properties  RocketMinipoolInterface minipool = RocketMinipoolInterface(_minipool);  code/contracts/contract/minipool/RocketMinipoolManager.sol:L474-L478  function getMinipoolDetails(address _minipoolAddress) override external view returns (MinipoolDetails memory) {  // Get contracts  RocketMinipoolInterface minipoolInterface = RocketMinipoolInterface(_minipoolAddress);  RocketMinipoolBase minipool = RocketMinipoolBase(payable(_minipoolAddress));  RocketNetworkPenaltiesInterface rocketNetworkPenalties = RocketNetworkPenaltiesInterface(getContractAddress(\"rocketNetworkPenalties\"));  More examples of RocketStorageInterface _rocketStorage casts:  code/contracts/contract/node/RocketNodeDistributor.sol:L8-L13  contract RocketNodeDistributor is RocketNodeDistributorStorageLayout {  bytes32 immutable distributorStorageKey;  constructor(address _nodeAddress, address _rocketStorage) {  rocketStorage = RocketStorageInterface(_rocketStorage);  nodeAddress = _nodeAddress;  Recommendation  We recommend using more specific types instead of address where possible. Downcast if necessary. This goes for parameter types as well as state variable types.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.14 Redundant double casts   ", "body": "  Resolution  The client acknowledges the finding and provided the following statement:  Acknowledged. These contracts are non-upgradable.  Description  _rocketStorageAddress  is already of contract type RocketStorageInterface.  code/contracts/contract/RocketBase.sol:L78-L82  /// @dev Set the main Rocket Storage address  constructor(RocketStorageInterface _rocketStorageAddress) {  // Update the contract address  rocketStorage = RocketStorageInterface(_rocketStorageAddress);  _tokenAddress  is already of contract type ERC20Burnable.  code/contracts/contract/RocketVault.sol:L132-L138  function burnToken(ERC20Burnable _tokenAddress, uint256 _amount) override external onlyLatestNetworkContract {  // Get contract key  bytes32 contractKey = keccak256(abi.encodePacked(getContractName(msg.sender), _tokenAddress));  // Update balances  tokenBalances[contractKey] = tokenBalances[contractKey].sub(_amount);  // Get the token ERC20 instance  ERC20Burnable tokenContract = ERC20Burnable(_tokenAddress);  _rocketTokenRPLFixedSupplyAddress is already of contract type IERC20.  code/contracts/contract/token/RocketTokenRPL.sol:L47-L51  constructor(RocketStorageInterface _rocketStorageAddress, IERC20 _rocketTokenRPLFixedSupplyAddress) RocketBase(_rocketStorageAddress) ERC20(\"Rocket Pool Protocol\", \"RPL\") {  // Version  version = 1;  // Set the mainnet RPL fixed supply token address  rplFixedSupplyContract = IERC20(_rocketTokenRPLFixedSupplyAddress);  Recommendation  We recommend removing the unnecessary double casts and copies of local variables.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.15 RocketMinipoolDelegate - Missing event in prepareVacancy    ", "body": "  Resolution  Fixed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by emitting a new event MinipoolVacancyPrepared.  Agreed. Added event per recommendation. Thanks.  Description  The function prepareVacancy updates multiple contract state variables and should therefore emit an event.  Examples  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L286-L309  /// @dev Sets the bond value and vacancy flag on this minipool  /// @param _bondAmount The bond amount selected by the node operator  /// @param _currentBalance The current balance of the validator on the beaconchain (will be checked by oDAO and scrubbed if not correct)  function prepareVacancy(uint256 _bondAmount, uint256 _currentBalance) override external onlyLatestContract(\"rocketMinipoolManager\", msg.sender) onlyInitialised {  // Check status  require(status == MinipoolStatus.Initialised, \"Must be in initialised status\");  // Sanity check that refund balance is zero  require(nodeRefundBalance == 0, \"Refund balance not zero\");  // Check balance  RocketDAOProtocolSettingsMinipoolInterface rocketDAOProtocolSettingsMinipool = RocketDAOProtocolSettingsMinipoolInterface(getContractAddress(\"rocketDAOProtocolSettingsMinipool\"));  uint256 launchAmount = rocketDAOProtocolSettingsMinipool.getLaunchBalance();  require(_currentBalance >= launchAmount, \"Balance is too low\");  // Store bond amount  nodeDepositBalance = _bondAmount;  // Calculate user amount from launch amount  userDepositBalance = launchAmount.sub(nodeDepositBalance);  // Flag as vacant  vacant = true;  preMigrationBalance = _currentBalance;  // Refund the node whatever rewards they have accrued prior to becoming a RP validator  nodeRefundBalance = _currentBalance.sub(launchAmount);  // Set status to preLaunch  setStatus(MinipoolStatus.Prelaunch);  Recommendation  Emit the missing event.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.16 Compiler error due to missing RocketMinipoolBaseInterface    ", "body": "  Resolution                           Fixed in   https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by adding the missing interface file.  Description  The interface RocketMinipoolBaseInterface is missing from the code repository. Manually generating the interface and adding it to the repository fixes the error.  Recommendation  Add the missing source unit to the repository.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.17 Unused Imports   Partially Addressed", "body": "  Resolution  Addressed in https://github.com/rocket-pool/rocketpool/tree/77d7cca65b7c0557cfda078a4fc45f9ac0cc6cc6 by removing all but the following two mentioned unused imports:  RocketRewardsPoolInterface  RocketSmoothingPoolInterface  Description  The following source units are imported but not referenced in the importing source unit:  code/contracts/contract/rewards/RocketMerkleDistributorMainnet.sol:L11  import \"../../interface/rewards/RocketSmoothingPoolInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L12-L18  import \"../../interface/minipool/RocketMinipoolManagerInterface.sol\";  import \"../../interface/minipool/RocketMinipoolQueueInterface.sol\";  import \"../../interface/node/RocketNodeStakingInterface.sol\";  import \"../../interface/util/AddressSetStorageInterface.sol\";  import \"../../interface/node/RocketNodeManagerInterface.sol\";  import \"../../interface/network/RocketNetworkPricesInterface.sol\";  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsMinipoolInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L8-L10  import \"../../types/MinipoolStatus.sol\";  import \"../../types/MinipoolDeposit.sol\";  import \"../../interface/dao/node/RocketDAONodeTrustedInterface.sol\";  code/contracts/contract/minipool/RocketMinipoolBase.sol:L7-L8  import \"../../types/MinipoolDeposit.sol\";  import \"../../types/MinipoolStatus.sol\";  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L13-L14  import \"../../interface/network/RocketNetworkPricesInterface.sol\";  import \"../../interface/node/RocketNodeManagerInterface.sol\";  code/contracts/contract/node/RocketNodeManager.sol:L13  import \"../../interface/rewards/claims/RocketClaimNodeInterface.sol\";  code/contracts/contract/rewards/RocketClaimDAO.sol:L7  import \"../../interface/rewards/RocketRewardsPoolInterface.sol\";  Duplicate Import:  code/contracts/contract/minipool/RocketMinipoolFactory.sol:L19-L20  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsNodeInterface.sol\";  import \"../../interface/dao/protocol/settings/RocketDAOProtocolSettingsNodeInterface.sol\";  The above list is exemplary, and there are likely more occurrences across the code base.  Recommendation  We recommend checking all imports and removing unused/unreferenced and unnecessary imports.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.18 RocketMinipool - Inconsistent access control modifier declaration onlyMinipoolOwner   ", "body": "  Resolution  Acknowledged by the client. Not addressed within rocket-pool/rocketpool@77d7cca  Agreed. This would change a lot of contracts just for a minor improvement in readbility.  Description  The access control modifier onlyMinipoolOwner should be renamed to onlyMinipoolOwnerOrWithdrawalAddress to be consistent with the actual check permitting the owner or the withdrawal address to interact with the function. This would also be consistent with other declarations in the codebase.  Example  The onlyMinipoolOwner modifier in RocketMinipoolBase is the same as onlyMinipoolOwnerOrWithdrawalAddress in other modules.  code/contracts/contract/minipool/RocketMinipoolBase.sol:L31-L37  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  code/contracts/contract/old/minipool/RocketMinipoolOld.sol:L21-L27  // Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  Other declarations:  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L97-L107  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner(address _nodeAddress) {  require(_nodeAddress == nodeAddress, \"Invalid minipool owner\");  _;  /// @dev Only allow access from the owning node address or their withdrawal address  modifier onlyMinipoolOwnerOrWithdrawalAddress(address _nodeAddress) {  require(_nodeAddress == nodeAddress || _nodeAddress == rocketStorage.getNodeWithdrawalAddress(nodeAddress), \"Invalid minipool owner\");  _;  code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L82-L92  // Only allow access from the owning node address  modifier onlyMinipoolOwner(address _nodeAddress) {  require(_nodeAddress == nodeAddress, \"Invalid minipool owner\");  _;  // Only allow access from the owning node address or their withdrawal address  modifier onlyMinipoolOwnerOrWithdrawalAddress(address _nodeAddress) {  require(_nodeAddress == nodeAddress || _nodeAddress == rocketStorage.getNodeWithdrawalAddress(nodeAddress), \"Invalid minipool owner\");  _;  Recommendation  We recommend renaming RocketMinipoolBase.onlyMinipoolOwner to RocketMinipoolBase.onlyMinipoolOwnerOrWithdrawalAddress.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.19 RocketDAO*Settings - settingNameSpace should be immutable   ", "body": "  Resolution  Acknowledged by the client. Not addressed within rocket-pool/rocketpool@77d7cca  Acknowledged. We can fix this as we upgrade the related contracts.  Description  The settingNameSpace in the abstract contract RocketDAONodeTrustedSettings is only set on contract deployment. Hence, the fields should be declared immutable to make clear that the settings namespace cannot change after construction.  Examples  RocketDAONodeTrustedSettings  code/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L13-L16  // The namespace for a particular group of settings  bytes32 settingNameSpace;  code/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L25-L30  // Construct  constructor(RocketStorageInterface _rocketStorageAddress, string memory _settingNameSpace) RocketBase(_rocketStorageAddress) {  // Apply the setting namespace  settingNameSpace = keccak256(abi.encodePacked(\"dao.trustednodes.setting.\", _settingNameSpace));  RocketDAOProtocolSettings  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L13-L14  // The namespace for a particular group of settings  bytes32 settingNameSpace;  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L25-L29  // Construct  constructor(RocketStorageInterface _rocketStorageAddress, string memory _settingNameSpace) RocketBase(_rocketStorageAddress) {  // Apply the setting namespace  settingNameSpace = keccak256(abi.encodePacked(\"dao.protocol.setting.\", _settingNameSpace));  code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsAuction.sol:L13-L15  constructor(RocketStorageInterface _rocketStorageAddress) RocketDAOProtocolSettings(_rocketStorageAddress, \"auction\") {  // Set version  version = 1;  Recommendation  We recommend using the immutable annotation in Solidity (see Immutable).  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.20 Inefficiencies with the onlyMinipoolOwner modifier  ", "body": "  Resolution  Acknowledged by the client. No further actions.  Correct. This change would change every single contract we have and so the benefit does not outweigh the change.  Description  If a withdrawal address has not been set (or has been zeroed out), rocketStorage.getNodeWithdrawalAddress(nodeAddress) returns nodeAddress. This outcome leads to the modifier checking the same address twice (msg.sender == nodeAddress || msg.sender == nodeAddress):  code/contracts/contract/minipool/RocketMinipoolBase.sol:L31-L37  /// @dev Only allow access from the owning node address  modifier onlyMinipoolOwner() {  // Only the node operator can upgrade  address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);  require(msg.sender == nodeAddress || msg.sender == withdrawalAddress, \"Only the node operator can access this method\");  _;  code/contracts/contract/RocketStorage.sol:L103-L111  // Get a node's withdrawal address  function getNodeWithdrawalAddress(address _nodeAddress) public override view returns (address) {  // If no withdrawal address has been set, return the nodes address  address withdrawalAddress = withdrawalAddresses[_nodeAddress];  if (withdrawalAddress == address(0)) {  return _nodeAddress;  return withdrawalAddress;  ", "labels": ["Consensys", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.21 RocketNodeDeposit - Duplicate check to avoid revert   ", "body": "  Resolution  Fixed with rocket-pool/rocketpool@3ab7af1 by introducing a new method maybeAssignDeposits() that does not revert by default but returns a boolean instead. This way, RocketNodeDeposit directly call the maybeAssignDeposits() function, avoiding the duplicate check.  This finding does not present a security-related problem in the code base, which is why we downgrade its severity to informational. However, we opted to keep this recommendation present in the report since it underlines a form of technical debt where old functionality is wrapped by new functionality using a workaround.  Description  When receiving and subsequently assigning deposits, the RocketNodeDeposit contract s assignDeposits function calls RocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled and skips the assignment of funds. This is done because the RocketDepositPool.assignDeposits function reverts if the setting is disabled:  code/contracts/contract/deposit/RocketDepositPool.sol:L207-L212  function assignDeposits() override external onlyThisLatestContract {  // Load contracts  RocketDAOProtocolSettingsDepositInterface rocketDAOProtocolSettingsDeposit = RocketDAOProtocolSettingsDepositInterface(getContractAddress(\"rocketDAOProtocolSettingsDeposit\"));  // Revert if assigning is disabled  require(_assignDeposits(rocketDAOProtocolSettingsDeposit), \"Deposit assignments are currently disabled\");  However, the underlying _assignDeposits function already performs a check for the setting and returns prematurely to avoid assignment.  code/contracts/contract/deposit/RocketDepositPool.sol:L217-L219  if (!_rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled()) {  return false;  The rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled() setting is checked twice. The first occurrence is in RocketNodeDeposit.assignDeposits and the second one in the same flow is contained in RocketDepositPool._assignDeposits. The second check is performed in a reverting fashion, thus requiring the top-level check in the RocketNodeDeposit contract to preemptively fetch and check the setting before continuing.  Recommendation  Since Rocketpool v1.2 already aims to perform an upgrade on the RocketDepositPool contract, we do recommend adding a separate, non-reverting version of the RocketDepositPool.assignDeposits function to the code base and removing the redundant preemptive check in RocketNodeDeposit.assignDeposits. This will improve readability and maintainability of future versions of the code, and save gas cost on deposit assignment operations.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.22 Inconsistent Coding Style  ", "body": "  Resolution  The client provided the following statement:  Acknolwedge your recommendation but we are dealing with an existing deployed codebase and if we change codestyle on only the contracts we update we will end up with a codebase with different code styles which is worse than one that is internally consistent but not consistent with best practice.  Description  Deviations from the Solidity Style Guide were identified throughout the codebase. Considering how much value a consistent coding style adds to the project s readability, enforcing a standard coding style with the help of linter tools is recommended.  Inconsistent Function naming scheme for external and internal interfaces  Throughout the codebase, private/internal functions are generally prefixed with an underscore (_<name>). This allows for an easy way to see if an external party can interact with a function without having to scan the declaration line for the corresponding visibility keywords. However, this naming scheme is not enforced consistently. Many internal function names are indistinguishable from external function names. It is therefore highly recommended to implement a consistent naming scheme and prefix internal functions with an underscore (_<name>).  code/contracts/contract/node/RocketNodeDeposit.sol:L268-L283  /// @dev Reverts if vacant minipools are not enabled  function checkVacantMinipoolsEnabled() private view {  // Get contracts  RocketDAOProtocolSettingsNodeInterface rocketDAOProtocolSettingsNode = RocketDAOProtocolSettingsNodeInterface(getContractAddress(\"rocketDAOProtocolSettingsNode\"));  // Check node settings  require(rocketDAOProtocolSettingsNode.getVacantMinipoolsEnabled(), \"Vacant minipools are currently disabled\");  /// @dev Executes an assignDeposits call on the deposit pool  function assignDeposits() private {  RocketDAOProtocolSettingsDepositInterface rocketDAOProtocolSettingsDeposit = RocketDAOProtocolSettingsDepositInterface(getContractAddress(\"rocketDAOProtocolSettingsDeposit\"));  if (rocketDAOProtocolSettingsDeposit.getAssignDepositsEnabled()) {  RocketDepositPoolInterface rocketDepositPool = RocketDepositPoolInterface(getContractAddress(\"rocketDepositPool\"));  rocketDepositPool.assignDeposits();  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L339-L345  /// @dev Stakes the balance of this minipool into the deposit contract to set withdrawal credentials to this contract  /// @param _validatorSignature A signature over the deposit message object  /// @param _depositDataRoot The hash tree root of the deposit data object  function preStake(bytes calldata _validatorPubkey, bytes calldata _validatorSignature, bytes32 _depositDataRoot) internal {  // Load contracts  DepositInterface casperDeposit = DepositInterface(getContractAddress(\"casperDeposit\"));  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L651-L654  /// @dev Distributes the current contract balance based on capital ratio and node fee  function distributeSkimmedRewards() internal {  uint256 rewards = address(this).balance.sub(nodeRefundBalance);  uint256 nodeShare = calculateNodeRewards(nodeDepositBalance, getUserDepositBalance(), rewards);  code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L661-L663  /// @dev Set the minipool's current status  /// @param _status The new status  function setStatus(MinipoolStatus _status) private {  code/contracts/contract/node/RocketNodeDeposit.sol:L202-L206  /// @dev Adds a minipool to the queue  function enqueueMinipool(address _minipoolAddress) private {  // Add minipool to queue  RocketMinipoolQueueInterface(getContractAddress(\"rocketMinipoolQueue\")).enqueueMinipool(_minipoolAddress);  code/contracts/contract/node/RocketNodeDeposit.sol:L208-L213  /// @dev Reverts if node operator has not initialised their fee distributor  function checkDistributorInitialised() private view {  // Check node has initialised their fee distributor  RocketNodeManagerInterface rocketNodeManager = RocketNodeManagerInterface(getContractAddress(\"rocketNodeManager\"));  require(rocketNodeManager.getFeeDistributorInitialised(msg.sender), \"Fee distributor not initialised\");  code/contracts/contract/node/RocketNodeDeposit.sol:L215-L218  /// @dev Creates a minipool and returns an instance of it  /// @param _salt The salt used to determine the minipools address  /// @param _expectedMinipoolAddress The expected minipool address. Reverts if not correct  function createMinipool(uint256 _salt, address _expectedMinipoolAddress) private returns (RocketMinipoolInterface) {  code/contracts/contract/auction/RocketAuctionManager.sol:L58-L60  function setLotCount(uint256 _amount) private {  setUint(keccak256(\"auction.lots.count\"), _amount);  ", "labels": ["Consensys", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/01/rocket-pool-atlas-v1.2/"}, {"title": "5.1 Oracle s _sanityCheck for prices will not work with slashing ", "body": "  Description  The _sanityCheck is verifying that the new price didn t change significantly:  code/contracts/Portal/utils/OracleUtilsLib.sol:L405-L417  uint256 maxPrice = curPrice +  ((curPrice *  self.PERIOD_PRICE_INCREASE_LIMIT *  _periodsSinceUpdate) / PERCENTAGE_DENOMINATOR);  uint256 minPrice = curPrice -  ((curPrice *  self.PERIOD_PRICE_DECREASE_LIMIT *  _periodsSinceUpdate) / PERCENTAGE_DENOMINATOR);  require(  _newPrice >= minPrice && _newPrice <= maxPrice,  \"OracleUtils: price is insane\"  While the rewards of staking can be reasonably predicted, the balances may also be changed due to slashing. So any slashing event should reduce the price, and if enough ETH is slashed, the price will drop heavily. The oracle will not be updated because of a sanity check. After that, there will be an arbitrage opportunity, and everyone will be incentivized to withdraw as soon as possible. That process will inevitably devaluate gETH to zero. The severity of this issue is also amplified by the fact that operators have no skin in the game and won t lose anything from slashing.  Recommendation  Make sure that slashing can be adequately processed when updating the price.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.2 Multiple calculation mistakes in the _findPricesClearBuffer function ", "body": "  Description  The _findPricesClearBuffer function is designed to calculate the gETH/ETH prices. The first one (oracle price) is the price at the reference point, for ease of calculation let s assume it is midnight. The second price is the price at the time the reportOracle is called.  code/contracts/Portal/utils/OracleUtilsLib.sol:L388  return (unbufferedEther / unbufferedSupply, totalEther / supply);  To calculate the oracle price at midnight, the current ETH balance is reduced by all the minted gETH (converted to ETH with the old price) and increased by all the burnt gETH (converted to ETH with the old price) starting from midnight to the time transaction is being executed:  code/contracts/Portal/utils/OracleUtilsLib.sol:L368-L374  uint256 unbufferedEther = totalEther -  (DATASTORE.readUintForId(_poolId, _dailyBufferMintKey) * price) /  self.gETH.totalSupply(_poolId);  unbufferedEther +=  (DATASTORE.readUintForId(_poolId, _dailyBufferBurnKey) * price) /  self.gETH.denominator();  But in the first calculation, the self.gETH.totalSupply(_poolId) is mistakenly used instead of self.gETH.denominator(). This can lead to the unbufferedEther being much larger, and the eventual oracle price will be much larger too.  There is another serious calculation mistake. In the end, the function returns the following line:  code/contracts/Portal/utils/OracleUtilsLib.sol:L388  return (unbufferedEther / unbufferedSupply, totalEther / supply);  But none of these values are multiplied by self.gETH.denominator(); so they are in the same range. Both values will usually be around 1. While the actual price value should be multiplied by self.gETH.denominator();.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.3 New interfaces can add malicious code without any delay or check ", "body": "  Description  Geode Finance uses an interesting system of contracts for each individual staked ETH derivative. At the base of it all is an ERC1155 gETH contract where planet id acts as a token id. To make it more compatible with the rest of DeFi the Geode team pairs it up with an ERC20 contract that users would normally interact with and where all the allowances are stored. Naturally, since the balances are stored in the gETH contract, ERC20 interfaces need to ask gETH contract to update the balance. It is done in a way where the gETH contract will perform any transfer requested by the interface since the interface is expected to do all the checks and accountings. The issue comes with the fact that planet maintainers can whitelist new interfaces and that process does not require any approval. Planet maintainers could whitelist an interface that will send all the available tokens to the maintainer s wallet for example. This essentially allows Planet maintainers to steal all derivative tokens in circulation in one transaction.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L165-L173  function setInterface(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  address _interface  ) external {  DATASTORE.authenticate(id, true, [false, true, true]);  _setInterface(self, DATASTORE, id, _interface);  Recommendation  gETH.sol contract has a concept of avoiders. One of the ways to fix this issue is to have the avoidance be set on a per-interface basis and avoiding new interfaces by default. This way users will need to allow the new tokens to access the balances.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.4 MiniGovernance - fetchUpgradeProposal will always revert ", "body": "  Description  In the function fetchUpgradeProposal(), newProposal() is called with a hard coded duration of 4 weeks. This means the function will always revert since newProposal() checks that the proposal duration is not more than the constant MAX_PROPOSAL_DURATION of 2 weeks. Effectively, this leaves MiniGovernance non-upgradeable.  Examples  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L183  GEM.newProposal(proposal.CONTROLLER, 2, proposal.NAME, 4 weeks);  code/contracts/Portal/utils/GeodeUtilsLib.sol:L328-L331  require(  duration <= MAX_PROPOSAL_DURATION,  \"GeodeUtils: duration exceeds MAX_PROPOSAL_DURATION\"  );  Recommendation  Switch the hard coded proposal duration to 2 weeks.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.5 reportOracle can be sandwiched for profit. ", "body": "  Description  The fact that price update happens in an on-chain transaction gives the searches the ability to see the future price and then act accordingly.  Examples  MEV searcher can find the reportOracle transaction in the mem-pool and if the price is about to increase he could proceed to mint as much gETH as he can with a flash loan. They would then bundle the reportOracle transaction. Finally, they would redeem all the gETH for ETH at a higher price per share value as the last transaction in the bundle.  This paired with the fact that oracle might be updated less frequently than once per day, could lead to the fact that profits from this attack will outweigh the fees for performing it.  Fortunately, due to the nature of the protocol, the price fluctuations from day to day will most likely be smaller than the fees encountered during this arbitrage, but this is still something to be aware of when updating the values for DWP donations and fees. But it also makes it crucial to update the oracle every day not to increase the profit margins for this attack.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.6 Updating interfaces of derivatives is done in a dangerous and unpredictable manner. ", "body": "  Description  Geode Finance codebase provides planet maintainers with the ability to enable or disable different contracts to act as the main token contract. In fact, multiple separate contracts can be used at the same time if decided so by the planet maintainer. Those contracts will have shared balances but will not share the allowances as you can see below:  code/contracts/Portal/helpers/ERC1155SupplyMinterPauser.sol:L47  mapping(uint256 => mapping(address => uint256)) private _balances;  code/contracts/Portal/gETHInterfaces/ERC20InterfaceUpgradable.sol:L60  mapping(address => mapping(address => uint256)) private _allowances;  Unfortunately, this approach comes with some implications that are very hard to predict as they involve interactions with other systems, but is possible to say that the consequences of those implications will most always be negative. We will not be able to outline all the implications of this issue, but we can try and outline the pattern that they all would follow.  Examples  There are really two ways to update an interface: set the new one and immediately unset the old one, or have them both run in parallel for some time. Let s look at them one by one.  in the first case, the old interface is disabled immediately. Given that interfaces share balances that will lead to some very serious consequences. Imagine the following sequence:  Alice deposits her derivatives into the DWP contract for liquidity mining.  Planet maintainer updates the interface and immediately disables the old one.  DWP contract now has the old tokens and the new ones. But only the new ones are accounted for in the storage and thus can be withdrawn. Unfortunately, the old tokens are disabled meaning that now both old and new tokens are lost.  This can happen in pretty much any contract and not just the DWP token. Unless the holders had enough time to withdraw the derivatives back to their wallets all the funds deposited into contracts could be lost.  This leads us to the second case where the two interfaces are active in parallel. This would solve the issue above by allowing Alice to withdraw the old tokens from the DWP and make the new tokens follow. Unfortunately, there is an issue in that case as well.  Some DeFi contracts allow their owners to withdraw any tokens that are not accounted for by the internal accounting. DWP allows the withdrawal of admin fees if the contract has more tokens than balances[] store. Some contracts even allow to withdraw funds that were accidentally sent to the contract by people. Either to recover them or just as a part of dust collection. Let s call such contracts  dangerous contracts  for our purposes.  Alice deposits her derivatives into the dangerous contract.  Planet maintainer sets a new interface.  Owner of the dangerous contract sees that some odd and unaccounted tokens landed in the contract. He learns those are real and are part of Geode ecosystem. So he takes them.  Old tokens will follow the new tokens. That means Alice now has no claim to them and the contract that they just left has broken accounting since numbers there are not backed by tokens anymore.  One other issue we would like to highlight here is that despite the contracts being expected to have separate allowances, if the old contract has the allowance set, the initial 0 value of the new one will be ignored. Here is an example:  Alice approves Bob for 100 derivatives.  Planet maintainer sets a new interface. The new interface has no allowance from Alice to Bob.  Bob still can transfer new tokens from Alice to himself by transferring the old tokens for which he still has the allowance. New token balances will be updated accordingly.  Alice could also give Bob an allowance of 100 tokens in the new contract since that was her original intent, but this would mean that Bob now has 200 token allowance.  This is extremely convoluted and will most likely result in errors made by the planet maintainers when updating the interfaces.  Recommendation  The safest option is to only allow a list of whitelisted interfaces to be used that are well-documented and audited. Planet maintainers could then choose the once that they see fit.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.7 A sandwich attack on fetchUnstake ", "body": "  Description  Operators are incentivized to withdraw the stake when there is a debt in the system. Withdrawn ETH will be sold in the DWP, and a portion of the arbitrage profit will be sent to the operator. But the operators cannot unstake and earn the arbitrage boost instantly. Node operator will need to start the withdrawal process, signal unstake, and only then, after some time, potentially days, Oracle will trigger fetchUnstake and will take the arbitrage opportunity if it is still there.  code/contracts/Portal/utils/StakeUtilsLib.sol:L1276-L1288  function fetchUnstake(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  uint256 poolId,  uint256 operatorId,  bytes[] calldata pubkeys,  uint256[] calldata balances,  bool[] calldata isExit  ) external {  require(  msg.sender == self.TELESCOPE.ORACLE_POSITION,  \"StakeUtils: sender NOT ORACLE\"  );  In reality, the DWP contract s swap function is external and can be used by anyone, so anyone could try and take the arbitrage.  code/contracts/Portal/withdrawalPool/Swap.sol:L341-L358  function swap(  uint8 tokenIndexFrom,  uint8 tokenIndexTo,  uint256 dx,  uint256 minDy,  uint256 deadline  external  payable  virtual  override  nonReentrant  whenNotPaused  deadlineCheck(deadline)  returns (uint256)  return swapStorage.swap(tokenIndexFrom, tokenIndexTo, dx, minDy);  In fact, one could take this arbitrage with no risk or personal funds. This is due to the fact that fetchUnstake() could get sandwiched. Consider the following case:  There is a debt in the DWP and the node operator decides to withdraw the stake to take the arbitrage opportunity.  After some time the Oracle will actually finalize the withdrawal by calling fecthUnstake.  If debt is still there MEV searcher will see that transaction in the mem-pool and will take an ETH loan to buy cheap gETH.  fetchUnstake() will execute and since the debt was repaid in the previous step all of the withdrawn ETH will go into surplus.  Searcher will redeem gETH that they bought for the oracle price from surplus and will get all of the profit.  At the end of the day, the goal of regaining the peg will be accomplished, but node operators will not be interested in withdrawing early later. This will potentially create unhealthy situations when withdrawals are required in case of a serious de-peg.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.8 Only the GOVERNANCE can initialize the Portal ", "body": "  Description  In the Portal s initialize function, the _GOVERNANCE is passed as a parameter:  code/contracts/Portal/Portal.sol:L156-L196  function initialize(  address _GOVERNANCE,  address _gETH,  address _ORACLE_POSITION,  address _DEFAULT_gETH_INTERFACE,  address _DEFAULT_DWP,  address _DEFAULT_LP_TOKEN,  address _MINI_GOVERNANCE_POSITION,  uint256 _GOVERNANCE_TAX,  uint256 _COMET_TAX,  uint256 _MAX_MAINTAINER_FEE,  uint256 _BOOSTRAP_PERIOD  ) public virtual override initializer {  __ReentrancyGuard_init();  __Pausable_init();  __ERC1155Holder_init();  __UUPSUpgradeable_init();  GEODE.SENATE = _GOVERNANCE;  GEODE.GOVERNANCE = _GOVERNANCE;  GEODE.GOVERNANCE_TAX = _GOVERNANCE_TAX;  GEODE.MAX_GOVERNANCE_TAX = _GOVERNANCE_TAX;  GEODE.SENATE_EXPIRY = type(uint256).max;  STAKEPOOL.GOVERNANCE = _GOVERNANCE;  STAKEPOOL.gETH = IgETH(_gETH);  STAKEPOOL.TELESCOPE.gETH = IgETH(_gETH);  STAKEPOOL.TELESCOPE.ORACLE_POSITION = _ORACLE_POSITION;  STAKEPOOL.TELESCOPE.MONOPOLY_THRESHOLD = 20000;  updateStakingParams(  _DEFAULT_gETH_INTERFACE,  _DEFAULT_DWP,  _DEFAULT_LP_TOKEN,  _MAX_MAINTAINER_FEE,  _BOOSTRAP_PERIOD,  type(uint256).max,  type(uint256).max,  _COMET_TAX,  3 days  );  But then it calls the updateStakingParams function, which requires the msg.sender to be the governance:  code/contracts/Portal/Portal.sol:L651-L665  function updateStakingParams(  address _DEFAULT_gETH_INTERFACE,  address _DEFAULT_DWP,  address _DEFAULT_LP_TOKEN,  uint256 _MAX_MAINTAINER_FEE,  uint256 _BOOSTRAP_PERIOD,  uint256 _PERIOD_PRICE_INCREASE_LIMIT,  uint256 _PERIOD_PRICE_DECREASE_LIMIT,  uint256 _COMET_TAX,  uint256 _BOOST_SWITCH_LATENCY  ) public virtual override {  require(  msg.sender == GEODE.GOVERNANCE,  \"Portal: sender not GOVERNANCE\"  );  So only the future governance can initialize the Portal. In the case of the Geode protocol, the governance will be represented by a token contract, making it hard to initialize promptly.  Initialization should be done by an actor that is more flexible than governance.  Recommendation  Split the updateStakingParams function into public and private ones and use them accordingly.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.9 The maintainer of the MiniGovernance can block the changeMaintainer function ", "body": "  Description  Every entity with an ID has a controller and a maintainer. The controller tends to have more control, and the maintainer is mostly used for operational purposes. So the controller should be able to change the maintainer if that is required. Indeed we see that it is possible in the MiniGovernance too:  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L224-L246  function changeMaintainer(  bytes calldata password,  bytes32 newPasswordHash,  address newMaintainer  external  virtual  override  onlyPortal  whenNotPaused  returns (bool success)  require(  SELF.PASSWORD_HASH == bytes32(0) ||  SELF.PASSWORD_HASH ==  keccak256(abi.encodePacked(SELF.ID, password))  );  SELF.PASSWORD_HASH = newPasswordHash;  _refreshSenate(newMaintainer);  success = true;  Here the changeMaintainer function can only be called by the Portal, and only the controller can initiate that call. But the maintainer can pause the MiniGovernance, which will make this call revert because the _refreshSenate function has the whenNotPaused modifier. Thus maintainer could intentionally prevent the controller from replacing it by another maintainer.  Recommendation  Make sure that the controller can always change the malicious maintainer.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.10 Entities are not required to be initiated ", "body": "  Description  Every entity (Planet, Comet, Operator) has a 3-step creation process:  Creation of the proposal.  Approval of the proposal.  Initiation of the entity.  The last step is crucial, but it is never explicitly checked that the entity is initialized. The initiation always includes the initiator modifier that works with the \"initiated\" slot on DATASTORE:  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L46-L72  modifier initiator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 _TYPE,  uint256 _id,  address _maintainer  ) {  require(  msg.sender == DATASTORE.readAddressForId(_id, \"CONTROLLER\"),  \"MaintainerUtils: sender NOT CONTROLLER\"  );  require(  DATASTORE.readUintForId(_id, \"TYPE\") == _TYPE,  \"MaintainerUtils: id NOT correct TYPE\"  );  require(  DATASTORE.readUintForId(_id, \"initiated\") == 0,  \"MaintainerUtils: already initiated\"  );  DATASTORE.writeAddressForId(_id, \"maintainer\", _maintainer);  _;  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  emit IdInitiated(_id, _TYPE);  But this slot is never actually checked when the entities are used. While we did not find any profitable attack vector using uninitiated entities, the code will be upgraded, which may allow for possible attack vectors related to this issue.  Recommendation  Make sure the entities are initiated before they are used.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.11 Node operators are not risking anything when abandoning their activity or performing malicious actions ", "body": "  Description  During the staking process, the node operators need to provide 1 ETH as a deposit for every validator that they would like to initiate. After that is done, Oracle needs to ensure that validator creation has been done correctly and then deposit the remaining 31 ETH on chain as well as reimburse 1 ETH back to the node operator. The node operator can then proceed to withdraw the funds that were used as initial deposits. As the result, node operators operate nodes that have 32 ETH each and none of which originally belonged to the operator. They essentially have no skin in the game to continue managing the validators besides a potential share in staking rewards. Instead, node operators could stop operation, or try to get slashed on purpose to create turmoil around derivatives on the market and try to capitalize while shorting the assets elsewhere.  Recommendation  Senate will need to be extra careful when approving operator onboarding proposals or potentially only reimburse the node operators the initial deposit after the funds were withdrawn from the MiniGovernance.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.12 Planets should not act as operators ", "body": "  Description  The system stores every entity (e.g., planet, comet, and operator) separately in DATASTORE under different IDs. But there is one exception, every planet can also act as an operator by default. This exception bypasses the general rule and goes against some expectations readers might have about the code:  Every entity with ID has fees; they are stored in DATASTORE for each entity DATASTORE.readUintForId(id, \"fee\"). The fees for a planet and an operator should be able to be different. But if a planet acts like an operator, both fees are stored under the same variable.  The same problem arises with the maintainer address. Since there will probably be different scripts for maintaining a planet and an operator, having separate addresses for the maintainers would make sense.  Every operator should be initialized before usage, but it is impossible to initialize a planet as an operator. There are two reasons behind it. First, only the original  Operator type  can call initiateOperator, while the planet will have a  Planet type . Second, an entity cannot be initialized twice; even different initialization functions use the same  initiated  storage slot.  Recommendation  Do not allow planets to be operators in the code. If every planet should be able to act as an operator simultaneously, it is better to create separate operator entities for every planet.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.13 The blameOperator can be called for an alienated validator ", "body": "  Description  The blameOperator  function is designed to be called by anyone. If some operator did not signal to exit in time, anyone can blame and imprison this operator.  code/contracts/Portal/utils/StakeUtilsLib.sol:L1205-L1224  /**  @notice allows improsening an Operator if the validator have not been exited until expectedExit  @dev anyone can call this function  @dev if operator has given enough allowence, they can rotate the validators to avoid being prisoned  /  function blameOperator(  StakePool storage self,  DataStoreUtils.DataStore storage DATASTORE,  bytes calldata pk  ) external {  if (  block.timestamp > self.TELESCOPE._validators[pk].expectedExit &&  self.TELESCOPE._validators[pk].state != 3  ) {  OracleUtils.imprison(  DATASTORE,  self.TELESCOPE._validators[pk].operatorId  );  The problem is that it can be called for any state that is not 3 (self.TELESCOPE._validators[pk].state != 3). But it should only be called for active validators whose state equals 2. So the blameOperator can be called an infinite amount of time for alienated or not approved validators. These types of validators cannot switch to state 3.  The severity of the issue is mitigated by the fact that this function is currently unavailable for users to call. But it is intended to be external once the withdrawal process is in place.  Recommendation  Make sure that you can only blame the operator of an active validator.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.14 Latency timelocks on certain functions can be bypassed ", "body": "  Description  The functions switchMaintainerFee() and switchWithdrawalBoost() add a latency of typically three days to the current timestamp at which the new value is meant to be valid. However, they don t limit the number of times this value can be changed within the latency period. This allows a malicious maintainer to set their desired value twice and effectively make the change immediately. Let s take the first function as an example. The first call to it sets a value as the newFee, moving the old value to priorFee, which is effectively the fee in use until the time lock is up. A follow-up call to the function with the same value as a parameter would mean the  new  value overwrites the old priorFee while remaining in the queue for the switch.  Examples  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L311-L333  function switchMaintainerFee(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 newFee  ) external {  DATASTORE.writeUintForId(  id,  \"priorFee\",  DATASTORE.readUintForId(id, \"fee\")  );  DATASTORE.writeUintForId(  id,  \"feeSwitch\",  block.timestamp + FEE_SWITCH_LATENCY  );  DATASTORE.writeUintForId(id, \"fee\", newFee);  emit MaintainerFeeSwitched(  id,  newFee,  block.timestamp + FEE_SWITCH_LATENCY  );  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L296-L304  function getMaintainerFee(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id  ) internal view returns (uint256 fee) {  if (DATASTORE.readUintForId(id, \"feeSwitch\") > block.timestamp) {  return DATASTORE.readUintForId(id, \"priorFee\");  return DATASTORE.readUintForId(id, \"fee\");  Recommendation  Add a check to make sure only one value can be set between time lock periods.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.15 MiniGovernance s senate has almost unlimited validity ", "body": "  Description  A new senate for the MiniGovernance contract is set in the following line:  code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L201  GEM._setSenate(newSenate, block.timestamp + SENATE_VALIDITY);  The validity period argument should not include block.timestamp, because it is going to be added a bit later in the code:  code/contracts/Portal/utils/GeodeUtilsLib.sol:L496  self.SENATE_EXPIRY = block.timestamp + _senatePeriod;  So currently, every senate of MiniGovernance will have much longer validity than it is supposed to.  Recommendation  Pass onlySENATE_VALIDITY in the _refreshSenate function.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.16 Proposed validators not accounted for in the monopoly check. ", "body": "  Description  The Geode team introduced a check that makes sure that node operators do not initiate more validators than a threshold called MONOPOLY_THRESHOLD allows. It is used on call to proposeStake(...) which the operator would call in order to propose new validators.  It is worth mentioning that onboarding new validator nodes requires 2 steps: a proposal from the node operator and approval from the planet maintainer. After the first step validators get a status of proposed.  After the second step validators get the status of active and all eth accounting is done. The issue we found is that the proposed validators step performs the monopoly check but does not account for previously proposed but not active validators.  Examples  Assume that MONOPOLY_THRESHOLD is set to 5. The node operator could propose 4 new validators and pass the monopoly check and label those validators as proposed. The node operator could then suggest 4 more validators in a separate transaction and since the monopoly check does not check for the proposed validators, that would pass as well. Then in beaconStake or the step of maintainer approval, there is no monopoly check at all, so 8 validators could be activated at once.  code/contracts/Portal/utils/StakeUtilsLib.sol:L978-L982  require(  (DATASTORE.readUintForId(operatorId, \"totalActiveValidators\") +  pubkeys.length) <= self.TELESCOPE.MONOPOLY_THRESHOLD,  \"StakeUtils: IceBear does NOT like monopolies\"  );  Recommendation  Include the (DATASTORE.readUintForId(poolId,DataStoreUtils.getKey(operatorId, \"proposedValidators\")) into the require statement, just like in the check for the node operator allowance check.  code/contracts/Portal/utils/StakeUtilsLib.sol:L983-L995  require(  (DATASTORE.readUintForId(  poolId,  DataStoreUtils.getKey(operatorId, \"proposedValidators\")  ) +  DATASTORE.readUintForId(  poolId,  DataStoreUtils.getKey(operatorId, \"activeValidators\")  ) +  pubkeys.length) <=  operatorAllowance(DATASTORE, poolId, operatorId),  \"StakeUtils: NOT enough allowance\"  );  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.17 Comparison operator used instead of assignment operator ", "body": "  Description  A common typo is present twice in the OracleUtilsLib.sol where == is used instead of = resulting in incorrect storage updates.  Examples  code/contracts/Portal/utils/OracleUtilsLib.sol:L250  self._validators[_pk].state == 2;  code/contracts/Portal/utils/OracleUtilsLib.sol:L269  self._validators[_pk].state == 3;  Recommendation  Replace == with =.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.18 initiator modifier will not work in the context of one transaction ", "body": "  Description  Each planet, comet or operator must be initialized after the onboarding proposal is approved. In order to make sure that these entities are not initialized more than once initiateOperator, initiateComet and initiatePlanet have the initiator modifier.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L135-L147  function initiatePlanet(  DataStoreUtils.DataStore storage DATASTORE,  uint256[3] memory uintSpecs,  address[5] memory addressSpecs,  string[2] calldata interfaceSpecs  external  initiator(DATASTORE, 5, uintSpecs[0], addressSpecs[1])  returns (  address miniGovernance,  address gInterface,  address withdrawalPool  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L184-L189  function initiateComet(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 fee,  address maintainer  ) external initiator(DATASTORE, 6, id, maintainer) {  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L119-L124  function initiateOperator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 id,  uint256 fee,  address maintainer  ) external initiator(DATASTORE, 4, id, maintainer) {  Inside that modifier, we check that the initiated flag is 0 and if so we proceed to initialization. We later update it to the current timestamp.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L46-L72  modifier initiator(  DataStoreUtils.DataStore storage DATASTORE,  uint256 _TYPE,  uint256 _id,  address _maintainer  ) {  require(  msg.sender == DATASTORE.readAddressForId(_id, \"CONTROLLER\"),  \"MaintainerUtils: sender NOT CONTROLLER\"  );  require(  DATASTORE.readUintForId(_id, \"TYPE\") == _TYPE,  \"MaintainerUtils: id NOT correct TYPE\"  );  require(  DATASTORE.readUintForId(_id, \"initiated\") == 0,  \"MaintainerUtils: already initiated\"  );  DATASTORE.writeAddressForId(_id, \"maintainer\", _maintainer);  _;  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  emit IdInitiated(_id, _TYPE);  Unfortunately, this does not follow the checks-effects-interractions pattern. If one for example would call initiatePlanet again from the body of the modifier, this check will still pass making it susceptible to a reentrancy attack. While we could not find a way to exploit this in the current engagement, given that system is designed to be upgradable this could become a risk in the future. For example, if during the initialization of the planet the maintainer will be allowed to pass a custom interface that could potentially allow reentering.  Recommendation  Bring the line that updated the initiated flag to the current timestamp before the _;.  code/contracts/Portal/utils/MaintainerUtilsLib.sol:L69  DATASTORE.writeUintForId(_id, \"initiated\", block.timestamp);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.19 Incorrect accounting for the burned gEth ", "body": "  Description  Geode Portal records the amount of minted and burned gETH on any given day during the active period of the oracle. One case where some gETH is burned is when the users redeem gETH for ETH. In the burn function we burn the spentGeth - gEthDonation but in the accounting code we do not account for  gEthDonation so the code records more assets burned than was really burned.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L823-L832  DATASTORE.subUintForId(poolId, \"surplus\", spentSurplus);  self.gETH.burn(address(this), poolId, spentGeth - gEthDonation);  if (self.TELESCOPE._isOracleActive()) {  bytes32 dailyBufferKey = DataStoreUtils.getKey(  block.timestamp - (block.timestamp % OracleUtils.ORACLE_PERIOD),  \"burnBuffer\"  );  DATASTORE.addUintForId(poolId, dailyBufferKey, spentGeth);  Recommendation  Record the spentGeth - gEthDonation instead of just spentGeth in the burn buffer.  code/contracts/Portal/utils/StakeUtilsLib.sol:L831  DATASTORE.addUintForId(poolId, dailyBufferKey, spentGeth);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.20 Boost calculation on fetchUnstake should not be using the cumBalance when it is larger than debt. ", "body": "  Description  The Geode team implemented the 2-step withdrawal mechanism for the staked ETH. First, node operators signal their intent to withdraw the stake, and then the oracle will trigger all of the accounting of rewards, balances, and buybacks if necessary. Buybacks are what we are interested in at this time. Buybacks are performed by checking if the derivative asset is off peg in the Dynamic Withdrawal Pool contract. Once the debt is larger than some ignorable threshold an arbitrage buyback will be executed. A portion of the arbitrage profit will go to the node operator. The issue here is that when simulating the arbitrage swap in the calculateSwap call we use the cumulative un-stake balance rather than ETH debt preset in the DWP. In the case where the withdrawal cumulative balance is higher than the debt node operator will receive a higher reward than intended.  Examples  code/contracts/Portal/utils/StakeUtilsLib.sol:L1353-L1354  uint256 arb = withdrawalPoolById(DATASTORE, poolId)  .calculateSwap(0, 1, cumBal);  Recommendation  Use the debt amount of ETH in the boost reward calculation when the cumulative balance is larger than the debt.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "5.21 DataStore struct not having the _gap for upgrades. ", "body": "  Description  Geode Finance codebase follows a structure where most of the storage variables are stored in the structs. You can see an example of that in the Portal.sol.  code/contracts/Portal/Portal.sol:L152-L154  DataStoreUtils.DataStore private DATASTORE;  GeodeUtils.Universe private GEODE;  StakeUtils.StakePool private STAKEPOOL;  It is worth mentioning that Geode contracts are meant to support the upgradability pattern. Given that information, one should be careful not to overwrite the storage variables by reordering the old ones or adding the new once not at the end of the list of variables when upgrading. The issue comes with the fact that structs seem to give a false sense of security making it feel like they are an isolated set of storage variables that will not override anything else. In reality, struts are just tuples that are expanded in storage sequentially just like all the other storage variables. For that reason, if you have two struct storage variables listed back to back like in the code above, you either need to make sure not to change the order or the number of variables in the structs other than the last one between upgrades or you need to add a uint256[N] _gap array of fixed size to reserve some storage slots for the future at the end of each struct. The Geode Finance team is missing the gap in the DataStrore struct making it non-upgradable.  code/contracts/Portal/utils/DataStoreUtilsLib.sol:L34-L39  struct DataStore {  mapping(uint256 => uint256[]) allIdsByType;  mapping(bytes32 => uint256) uintData;  mapping(bytes32 => bytes) bytesData;  mapping(bytes32 => address) addressData;  Recommendation  We suggest that gap is used in DataStore as well. Since it was used for all the other structs we consider it just a typo.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/11/geodefi/"}, {"title": "6.1 Frontrunning attacks by the owner ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  There are few possible attack vectors by the owner:  All strategies have fees from rewards. In addition to that, the PancakeSwap strategy has deposit fees. The default deposit fees equal zero; the maximum is limited to 5%: wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L29-L33 uint256 constant MAXIMUM_DEPOSIT_FEE = 5e16; // 5% uint256 constant DEFAULT_DEPOSIT_FEE = 0e16; // 0%  uint256 constant MAXIMUM_PERFORMANCE_FEE = 50e16; // 50% uint256 constant DEFAULT_PERFORMANCE_FEE = 10e16; // 10% When a user deposits tokens, expecting to have zero deposit fees, the owner can frontrun the deposit and increase fees to 5%. If the deposit size is big enough, that may be a significant amount of money.  In the gulp function, the reward tokens are exchanged for the reserve tokens on the exchange: wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L218-L244 function gulp(uint256 _minRewardAmount) external onlyEOAorWhitelist nonReentrant { \tuint256 _pendingReward = _getPendingReward(); \tif (_pendingReward > 0) { \t\t_withdraw(0); \t} \t{ \t\tuint256 _totalReward = Transfers._getBalance(rewardToken); \t\tuint256 _feeReward = _totalReward.mul(performanceFee) / 1e18; \t\tTransfers._pushFunds(rewardToken, collector, _feeReward); \t} \tif (rewardToken != routingToken) { \t\trequire(exchange != address(0), \"exchange not set\"); \t\tuint256 _totalReward = Transfers._getBalance(rewardToken); \t\tTransfers._approveFunds(rewardToken, exchange, _totalReward); \t\tIExchange(exchange).convertFundsFromInput(rewardToken, routingToken, _totalReward, 1); \t} \tif (routingToken != reserveToken) { \t\trequire(exchange != address(0), \"exchange not set\"); \t\tuint256 _totalRouting = Transfers._getBalance(routingToken); \t\tTransfers._approveFunds(routingToken, exchange, _totalRouting); \t\tIExchange(exchange).joinPoolFromInput(reserveToken, routingToken, _totalRouting, 1); \t} \tuint256 _totalBalance = Transfers._getBalance(reserveToken); \trequire(_totalBalance >= _minRewardAmount, \"high slippage\"); \t_deposit(_totalBalance); } The owner can change the exchange parameter to the malicious address that steals tokens. The owner then calls gulp with _minRewardAmount==0, and all the rewards will be stolen. The same attack can be implemented in fee collectors and the buyback contract.  Recommendation  Use a timelock to avoid instant changes of the parameters.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.2 New deposits are instantly getting a share of undistributed rewards ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  When a new deposit is happening, the current pending rewards are not withdrawn and re-invested yet. And they are not taken into account when calculating the number of shares that the depositor receives. The number of shares is calculated as if there were no pending rewards. The other side of this issue is that all the withdrawals are also happening without considering the pending rewards.  So currently, it makes more sense to withdraw right after gulp to gather the rewards. In addition to the general  unfairness  of the reward distribution during the deposit/withdrawal, there is also an attack vector created by this issue.  The Attack  If the deposit is made right before the gulp function is called, the rewards from the gulp are distributed evenly across all the current deposits, including the ones that were just recently made. So if the deposit-gulp-withdraw sequence is executed, the caller receives guaranteed profit. If the attacker also can execute these functions briefly (in one block or transaction) and take a huge loan to deposit a lot of tokens, almost all the rewards from the gulp will be stolen by the attacker. The easy 1-transaction attack with a flashloan can be done by the owner, miner, whitelisted contracts, or any contract if the onlyEOAorWhitelist modifier is disabled or stops working (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/3). Even if onlyEOAorWhitelist  is working properly, anyone can take a regular loan to make the attack.  The risk is not that big because no price manipulation is required. The price will likely remain the same during the attack (few blocks maximum).  Recommendation  If issue issue 6.3 is fixed while allowing anyone call the gulp contract, the best solution would be to include the gulp call at the beginning of the deposit and withdraw. In case of withdrawing, there should also be an option to avoid calling gulp as the emergency case.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.3 Proactive sandwiching of the gulp calls ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  Each strategy token contract provides a gulp method to fetch pending rewards, convert them into the reserve token and split up the balances. One share is sent to the fee collector as a performance fee, while the rest is deposited into the respective MasterChef contract to accumulate more rewards. Suboptimal trades are prevented by passing a minimum slippage value with the function call, which results in revert if the expected reserve token amount cannot be provided by the trade(s).  The slippage parameter and the trades performed in gulp open the function up to proactive sandwich attacks. The slippage parameter can be freely set by the attacker, resulting in the system performing arbitrarily bad trades based on how much the attacker can manipulate the liquidity of involved assets around the gulp function call.  This attack vector is significant under the following assumptions:  The exchange the trade is performed on allows significant changes in liquidity pools in a single transaction (e.g., not limiting transactions to X% of the pool amount),  The attacker can frontrun legitimate gulp calls with reasonable slippage values,  Trades are performed, i.e. when rewardToken != routingToken and/or routingToken != reserveToken hold true.  Examples  This affects the gulp functions in all the strategies:  PancakeSwapCompoundingStrategyToken  AutoFarmCompoundingStrategyToken  PantherSwapCompoundingStrategyToken  and also fees collectors and the buyback adapters:  PantherSwapBuybackAdapter  AutoFarmFeeCollectorAdapter  PancakeSwapFeeCollector  UniversalBuyback  Recommendation  There are different possible solutions to this issue and all have some tradeoffs. Initially, we came up with the following suggestion:  The onlyOwner modifier should be added to the gulp function to ensure only authorized parties with reasonable slippages can execute trades on behalf of the strategy contracts. Furthermore, additional slippage checks can be added to avoid unwanted behavior of authorized addresses, e.g., to avoid a bot setting unreasonable slippage values due to a software bug.  But in order to fix another issue (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/8), we came up with the alternative solution:  Use oracles to restrict users from calling the gulp function with unreasonable slippage (more than 5% from the oracle s moving average price). The side effect of that solution is that sometimes the outdated price will be used. That means that when the price crashes, nobody will be able to call the gulp.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.4 Expected amounts of  tokens in the withdraw function ", "body": "  Resolution  Client s statement :  This issue did not really need fixing. The mitigation was already in place by depositing a tiny amount of the reserve into the contract, if necessary   Description  Every withdraw function in the strategy contracts is calculating the expected amount of the returned tokens before withdrawing them:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L200-L208  function withdraw(uint256 _shares, uint256 _minAmount) external onlyEOAorWhitelist nonReentrant  address _from = msg.sender;  (uint256 _amount, uint256 _withdrawalAmount, uint256 _netAmount) = _calcAmountFromShares(_shares);  require(_netAmount >= _minAmount, \"high slippage\");  _burn(_from, _shares);  _withdraw(_amount);  Transfers._pushFunds(reserveToken, _from, _withdrawalAmount);  After that, the contract is trying to transfer this pre-calculated amount to the msg.sender. It is never checked whether the intended amount was actually transferred to the strategy contract. If the amount is lower, that may result in reverting the withdraw function all the time and locking up tokens.  Even though we did not find any specific case of returning a different amount of tokens, it is still a good idea to handle this situation to minimize relying on the security of the external contracts.  Recommendation  There are a few options how to mitigate the issue:  Double-check the balance difference before and after the MasterChef s withdraw function is called.  Handle this situation in the emergency mode (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/11).  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.5 Emergency mode of the MasterChef contracts is not supported ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  All the underlying MasterChef contracts have the emergency withdrawal mode, which allows simpler withdrawal (excluding the rewards):  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public nonReentrant {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  uint256 amount = user.amount;  user.amount = 0;  user.rewardDebt = 0;  user.rewardLockedUp = 0;  user.nextHarvestUntil = 0;  pool.lpToken.safeTransfer(address(msg.sender), amount);  emit EmergencyWithdraw(msg.sender, _pid, amount);  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  pool.lpToken.safeTransfer(address(msg.sender), user.amount);  emit EmergencyWithdraw(msg.sender, _pid, user.amount);  user.amount = 0;  user.rewardDebt = 0;  // Withdraw without caring about rewards. EMERGENCY ONLY.  function emergencyWithdraw(uint256 _pid) public nonReentrant {  PoolInfo storage pool = poolInfo[_pid];  UserInfo storage user = userInfo[_pid][msg.sender];  uint256 wantLockedTotal =  IStrategy(poolInfo[_pid].strat).wantLockedTotal();  uint256 sharesTotal = IStrategy(poolInfo[_pid].strat).sharesTotal();  uint256 amount = user.shares.mul(wantLockedTotal).div(sharesTotal);  IStrategy(poolInfo[_pid].strat).withdraw(msg.sender, amount);  pool.want.safeTransfer(address(msg.sender), amount);  emit EmergencyWithdraw(msg.sender, _pid, amount);  user.shares = 0;  user.rewardDebt = 0;  While it s hard to predict how and why the emergency mode can be enabled in the underlying MasterChef contracts, these functions are there for a reason, and it s safer to be able to use them. If some emergency happens and this is the only way to withdraw funds, the funds in the strategy contracts will be locked forever.  Recommendation  Add the emergency mode implementation.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.6 The capping mechanism for Panther token leads to increased fees ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  Panther token has a cap in transfer sizes, so any transfer in the contract is limited beforehand:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L218-L245  function gulp(uint256 _minRewardAmount) external onlyEOAorWhitelist nonReentrant  uint256 _pendingReward = _getPendingReward();  if (_pendingReward > 0) {  _withdraw(0);  uint256 __totalReward = Transfers._getBalance(rewardToken);  (uint256 _feeReward, uint256 _retainedReward) = _capFeeAmount(__totalReward.mul(performanceFee) / 1e18);  Transfers._pushFunds(rewardToken, buyback, _feeReward);  if (rewardToken != routingToken) {  require(exchange != address(0), \"exchange not set\");  uint256 _totalReward = Transfers._getBalance(rewardToken);  _totalReward = _capTransferAmount(rewardToken, _totalReward, _retainedReward);  Transfers._approveFunds(rewardToken, exchange, _totalReward);  IExchange(exchange).convertFundsFromInput(rewardToken, routingToken, _totalReward, 1);  if (routingToken != reserveToken) {  require(exchange != address(0), \"exchange not set\");  uint256 _totalRouting = Transfers._getBalance(routingToken);  _totalRouting = _capTransferAmount(routingToken, _totalRouting, _retainedReward);  Transfers._approveFunds(routingToken, exchange, _totalRouting);  IExchange(exchange).joinPoolFromInput(reserveToken, routingToken, _totalRouting, 1);  uint256 _totalBalance = Transfers._getBalance(reserveToken);  _totalBalance = _capTransferAmount(reserveToken, _totalBalance, _retainedReward);  require(_totalBalance >= _minRewardAmount, \"high slippage\");  _deposit(_totalBalance);  Fees here are calculated from the full amount of rewards (__totalReward ):  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L225  (uint256 _feeReward, uint256 _retainedReward) = _capFeeAmount(__totalReward.mul(performanceFee) / 1e18);  But in fact, if the amount of the rewards is too big, it will be capped, and the residuals will be  taxed  again during the next call of the gulp function. That behavior leads to multiple taxations of the same tokens, which means increased fees.  Recommendation  The best solution would be to cap __totalReward  first and then calculate fees from the capped value.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.7 The _capFeeAmount function is not working as intended ", "body": "  Resolution  Client s statement :  With the fix of 6.6 this code was removed and therefore no changes were required. \"  Description  Panther token has a limit on the transfer size. Because of that, all the Panther transfer values in the PantherSwapCompoundingStrategyToken are also capped beforehand. The following function is called to cap the size of fees:  wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L357-L366  function _capFeeAmount(uint256 _amount) internal view returns (uint256 _capped, uint256 _retained)  _retained = 0;  uint256 _limit = _calcMaxRewardTransferAmount();  if (_amount > _limit) {  _amount = _limit;  _retained = _amount.sub(_limit);  return (_amount, _retained);  This function should return the capped amount and the amount of retained tokens. But because the _amount is changed before calculating the _retained, the retained amount will always be 0.  Recommendation  Calculate the retained value before changing the amount.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.8 Stale split ratios in UniversalBuyback ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The gulp and pendingBurning functions of the UniversalBuyback contract use the hardcoded, constant values of DEFAULT_REWARD_BUYBACK1_SHARE and DEFAULT_REWARD_BUYBACK2_SHARE to determine the ratio the trade value is split with.  Consequently, any call to setRewardSplit to set a new ratio will be ineffective but still result in a ChangeRewardSplit event being emitted. This event can deceive system operators and users as it does not reflect the correct values of the contract.  Examples  wheat-v1-core-audit/contracts/UniversalBuyback.sol:L80-L81  uint256 _amount1 = _balance.mul(DEFAULT_REWARD_BUYBACK1_SHARE) / 1e18;  uint256 _amount2 = _balance.mul(DEFAULT_REWARD_BUYBACK2_SHARE) / 1e18;  wheat-v1-core-audit/contracts/UniversalBuyback.sol:L97-L98  uint256 _amount1 = _balance.mul(DEFAULT_REWARD_BUYBACK1_SHARE) / 1e18;  uint256 _amount2 = _balance.mul(DEFAULT_REWARD_BUYBACK2_SHARE) / 1e18;  Recommendation  Instead of the default values, rewardBuyback1Share and rewardBuyback2Share should be used.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.9 Future-proofness of the onlyEOAorWhitelist modifier ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The onlyEOAorWhitelist modifier is used in various locations throughout the code. It performs a check that asserts the message sender being equal to the transaction origin to assert the calling party is not a smart contract.  This approach may stop working if EIP-3074 and its AUTH and AUTHCALL opcodes get deployed.  While the OpenZeppelin reentrancy guard does not depend on tx.origin, the EOA check does. Its evasion can result in additional attack vectors such as flash loans opening up. It is noteworthy that preventing smart contract interaction with the protocol may limit its opportunities as smart contracts cannot integrate with it in the same way that GrowthDeFi integrates with its third-party service providers.  The onlyEOAorWhitelist modifier may give a false sense of security because it won t allow making a flash loan attack by most of the users. But the same attack can still be made by some people or with more risk:  The owner and the whitelisted contracts are not affected by the modifier.  The modifier can be disabled: **wheat-v1-core-audit/contracts/WhitelistGuard.sol:L21-L28** ```solidity modifier onlyEOAorWhitelist() { \tif (enabled) { \t\taddress _from = _msgSender(); \t\trequire(tx.origin == _from || whitelist.contains(_from), \"access denied\"); \t} \t_; } ```  And in the deployment script, this modifier is disabled for testing purposes, and it s important not to forget to turn it in on the production: wheat-v1-core-audit/migrations/02_deploy_contracts.js:L50 await pancakeSwapFeeCollector.setWhitelistEnabled(false); // allows testing  The attack can usually be split into multiple transactions. Miners can put these transactions closely together and don t take any additional risk. Regular users can take a risk, take the loan, and execute the attack in multiple transactions or even blocks.  Recommendation  It is strongly recommended to monitor the progress of this EIP and its potential implementation on the Binance Smart Chain. If this functionality gets enabled, the development team should update the contract system to use the new opcodes. We also strongly recommend relying less on the fact that only EOA will call the functions. It is better to write the code that can be called by the external smart contracts without compromising its security.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "6.10 Exchange owner might steal users  funds using reentrancy ", "body": "  Resolution  The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.  Description  The practice of pulling funds from a user (by using safeTransferFrom) and then later pushing (some) of the funds back to the user occurs in various places in the Exchange contract. In case one of the used token contracts (or one of its dependent calls) externally calls the Exchange owner, the owner may utilize that to call back Exchange.recoverLostFunds and drain (some) user funds.  Examples  wheat-v1-core-audit/contracts/Exchange.sol:L80-L89  function convertFundsFromInput(address _from, address _to, uint256 _inputAmount, uint256 _minOutputAmount) external override returns (uint256 _outputAmount)  address _sender = msg.sender;  Transfers._pullFunds(_from, _sender, _inputAmount);  _inputAmount = Math._min(_inputAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  _outputAmount = UniswapV2ExchangeAbstraction._convertFundsFromInput(router, _from, _to, _inputAmount, _minOutputAmount);  _outputAmount = Math._min(_outputAmount, Transfers._getBalance(_to)); // deals with potential transfer tax  Transfers._pushFunds(_to, _sender, _outputAmount);  return _outputAmount;  wheat-v1-core-audit/contracts/Exchange.sol:L121-L130  function joinPoolFromInput(address _pool, address _token, uint256 _inputAmount, uint256 _minOutputShares) external override returns (uint256 _outputShares)  address _sender = msg.sender;  Transfers._pullFunds(_token, _sender, _inputAmount);  _inputAmount = Math._min(_inputAmount, Transfers._getBalance(_token)); // deals with potential transfer tax  _outputShares = UniswapV2LiquidityPoolAbstraction._joinPoolFromInput(router, _pool, _token, _inputAmount, _minOutputShares);  _outputShares = Math._min(_outputShares, Transfers._getBalance(_pool)); // deals with potential transfer tax  Transfers._pushFunds(_pool, _sender, _outputShares);  return _outputShares;  wheat-v1-core-audit/contracts/Exchange.sol:L99-L111  function convertFundsFromOutput(address _from, address _to, uint256 _outputAmount, uint256 _maxInputAmount) external override returns (uint256 _inputAmount)  address _sender = msg.sender;  Transfers._pullFunds(_from, _sender, _maxInputAmount);  _maxInputAmount = Math._min(_maxInputAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  _inputAmount = UniswapV2ExchangeAbstraction._convertFundsFromOutput(router, _from, _to, _outputAmount, _maxInputAmount);  uint256 _refundAmount = _maxInputAmount - _inputAmount;  _refundAmount = Math._min(_refundAmount, Transfers._getBalance(_from)); // deals with potential transfer tax  Transfers._pushFunds(_from, _sender, _refundAmount);  _outputAmount = Math._min(_outputAmount, Transfers._getBalance(_to)); // deals with potential transfer tax  Transfers._pushFunds(_to, _sender, _outputAmount);  return _inputAmount;  wheat-v1-core-audit/contracts/Exchange.sol:L139-L143  function recoverLostFunds(address _token) external onlyOwner  uint256 _balance = Transfers._getBalance(_token);  Transfers._pushFunds(_token, treasury, _balance);  Recommendation  Reentrancy guard protection should be added to Exchange.convertFundsFromInput, Exchange.convertFundsFromOutput, Exchange.joinPoolFromInput, Exchange.recoverLostFunds at least, and in general to all public/external functions since gas price considerations are less relevant for contracts deployed on BSC.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/06/growthdefi-wheat/"}, {"title": "4.1 iETH.exchangeRateStored may not be accurate when invoked from external contracts ", "body": "  Resolution                           This issue was addressed in commit   9876e3a by using a modifier to track the current  Description  iETH.exchangeRateStored returns the exchange rate of the contract as a function of the current cash of the contract. In the case of iETH, current cash is calculated as the contract s ETH balance minus msg.value:  code/contracts/iETH.sol:L54-L59  /**  @dev Gets balance of this contract in terms of the underlying  /  function _getCurrentCash() internal view override returns (uint256) {  return address(this).balance.sub(msg.value);  msg.value is subtracted because the majority of iETH methods are payable, and msg.value is implicitly added to a contract s balance before execution begins. If msg.value were not subtracted, the value sent with a call could be used to inflate the contract s exchange rate artificially.  Examples  This problem occurs in multiple locations in the Controller:  beforeMint uses the exchange rate to ensure the supply capacity of the market is not reached. In this case, inflation would prevent the entire supply capacity of the market from being utilized:  code/contracts/Controller.sol:L670-L678  // Check the iToken's supply capacity, -1 means no limit  uint256 _totalSupplyUnderlying =  IERC20Upgradeable(_iToken).totalSupply().rmul(  IiToken(_iToken).exchangeRateStored()  );  require(  _totalSupplyUnderlying.add(_mintAmount) <= _market.supplyCapacity,  \"Token supply capacity reached\"  );  beforeLiquidateBorrow uses the exchange rate via calcAccountEquity to calculate the value of the borrower s collateral. In this case, inflation would increase the account s equity, which could prevent the liquidator from liquidating:  code/contracts/Controller.sol:L917-L919  (, uint256 _shortfall, , ) = calcAccountEquity(_borrower);  require(_shortfall > 0, \"Account does not have shortfall\");  Recommendation  Rather than having the Controller query the iETH.exchangeRateStored, the exchange rate could be passed-in to Controller methods as a parameter.  Ensure no other components in the system rely on iETH.exchangeRateStored after being called from iETH.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.2 Unbounded loop in Controller.calcAccountEquity allows DoS on liquidation ", "body": "  Description  Controller.calcAccountEquity calculates the relative value of a user s supplied collateral and their active borrow positions. Users may mark an arbitrary number of assets as collateral, and may borrow from an arbitrary number of assets. In order to calculate the value of both of these positions, this method performs two loops.  First, to calculate the sum of the value of a user s collateral:  code/contracts/Controller.sol:L1227-L1233  // Calculate value of all collaterals  // collateralValuePerToken = underlyingPrice * exchangeRate * collateralFactor  // collateralValue = balance * collateralValuePerToken  // sumCollateral += collateralValue  uint256 _len = _accountData.collaterals.length();  for (uint256 i = 0; i < _len; i++) {  IiToken _token = IiToken(_accountData.collaterals.at(i));  Second, to calculate the sum of the value of a user s borrow positions:  code/contracts/Controller.sol:L1263-L1268  // Calculate all borrowed value  // borrowValue = underlyingPrice * underlyingBorrowed / borrowFactor  // sumBorrowed += borrowValue  _len = _accountData.borrowed.length();  for (uint256 i = 0; i < _len; i++) {  IiToken _token = IiToken(_accountData.borrowed.at(i));  From dForce, we learned that 200 or more assets would be supported by the Controller. This means that a user with active collateral and borrow positions on all 200 supported assets could force any calcAccountEquity action to perform some 400 iterations of these loops, each with several expensive external calls.  Examples  By modifying dForce s unit test suite, we showed that an attacker could force the cost of calcAccountEquity above the block gas limit. This would prevent all of the following actions, as each relies on calcAccountEquity:  iToken.transfer and iToken.transferFrom  iToken.redeem and iToken.redeemUnderlying  iToken.borrow  iToken.liquidateBorrow and iToken.seize  The following actions would still be possible:  iToken.mint  iToken.repayBorrow and iToken.repayBorrowBehalf  As a result, an attacker may abuse the unbounded looping in calcAccountEquity to prevent the liquidation of underwater positions. We provided dForce with a PoC here: gist.  Recommendation  There are many possible ways to address this issue. Some ideas have been outlined below, and it may be that a combination of these ideas is the best approach:  In general, cap the number of markets and borrowed assets a user may have: The primary cause of the DoS is that the number of collateral and borrow positions held by a user is only restricted by the number of supported assets. The PoC provided above showed that somewhere around 150 collateral positions and 150 borrow positions, the gas costs of calcAccountEquity use most of the gas in a block. Given that gas prices often spike along with turbulent market conditions and that liquidations are far more likely in turbulent market conditions, a cap on active markets / borrows should be much lower than 150 each so as to keep the cost of liquidations as low as possible.  dForce should perform their own gas cost estimates to determine a cap, and choose a safe, low value. Estimates should be performed on the high-level liquidateBorrow method, so as to simulate an actual liquidation event. Additionally, estimates should factor in a changing block gas limit, and the possibility of opcode gas costs changing in future forks. It may be wise to make this cap configurable, so that the limits may be adjusted for future conditions.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.3 Fix utilization rate computation and respect reserves when lending ", "body": "  Resolution                           The dForce team has informed us that the only two interest rate models that are still in use are   2a0e974 and  c11fa9b.  Description  The utilization rate UR of an asset forms the basis for interest calculations and is defined as borrows / ( borrows + cash - reserves).  code/contracts/InterestRateModel/InterestRateModel.sol:L72-L88  /**  @notice Calculate the utilization rate: `_borrows / (_cash + _borrows - _reserves)`  @param _cash Asset balance  @param _borrows Asset borrows  @param _reserves Asset reserves  @return Asset utilization [0, 1e18]  /  function utilizationRate(  uint256 _cash,  uint256 _borrows,  uint256 _reserves  ) internal pure returns (uint256) {  // Utilization rate is 0 when there are no borrows  if (_borrows == 0) return 0;  return _borrows.mul(BASE).div(_cash.add(_borrows).sub(_reserves));  issue 4.4.  Recommendation  If reserves > cash \u2014 or, in other words, available cash is negative \u2014 this means part of the reserves have been borrowed, which ideally shouldn t happen in the first place. However, the reserves grow automatically over time, so it might be difficult to avoid this entirely. We recommend (1) avoiding this situation whenever it is possible and (2) fixing the UR computation such that it deals more gracefully with this scenario. More specifically:  Loan amounts should not be checked to be smaller than or equal to cash but cash - reserves (which might be negative). Note that the current check against cash happens more or less implicitly because the transfer just fails for insufficient cash.  Make the utilization rate computation return 1 if reserves > cash (unless borrows == 0, in which case return 0 as is already the case).  Remark  Internally, the utilization rate and other fractional values are scaled by 1e18. The discussion above has a more conceptual than technical perspective, so we used unscaled numbers. When making changes to the code, care must be taken to apply the scaling.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.4 If Base._updateInterest fails, the entire system will halt ", "body": "  Resolution                           dForce removed   27f9a28.  Description  Before executing most methods, the iETH and iToken contracts update interest accumulated on borrows via the method Base._updateInterest. This method uses the contract s interest rate model to calculate the borrow interest rate. If the calculated value is above maxBorrowRate (0.001e18), the method will revert:  code/contracts/TokenBase/Base.sol:L92-L107  function _updateInterest() internal virtual override {  InterestLocalVars memory _vars;  _vars.currentCash = _getCurrentCash();  _vars.totalBorrows = totalBorrows;  _vars.totalReserves = totalReserves;  // Gets the current borrow interest rate.  _vars.borrowRate = interestRateModel.getBorrowRate(  _vars.currentCash,  _vars.totalBorrows,  _vars.totalReserves  );  require(  _vars.borrowRate <= maxBorrowRate,  \"_updateInterest: Borrow rate is too high!\"  );  If this method reverts, the entire contract may halt and be unrecoverable. The only ways to change the values used to calculate this interest rate lie in methods that must first call Base._updateInterest. In this case, those methods would fail.  One other potential avenue for recovery exists: the Owner role may update the interest rate calculation contract via TokenAdmin._setInterestRateModel:  code/contracts/TokenBase/TokenAdmin.sol:L46-L63  /**  @dev Sets a new interest rate model.  @param _newInterestRateModel The new interest rate model.  /  function _setInterestRateModel(  IInterestRateModelInterface _newInterestRateModel  ) external virtual onlyOwner settleInterest {  // Gets current interest rate model.  IInterestRateModelInterface _oldInterestRateModel = interestRateModel;  // Ensures the input address is the interest model contract.  require(  _newInterestRateModel.isInterestRateModel(),  \"_setInterestRateModel: This is not the rate model contract!\"  );  // Set to the new interest rate model.  interestRateModel = _newInterestRateModel;  However, this method also calls Base._updateInterest before completing the upgrade, so it would fail as well.  Examples  We used interest rate parameters taken from dForce s unit tests to determine whether any of the interest rate models could return a borrow rate that would cause this failure. The default InterestRateModel is deployed using these values:  Plugging these values in to their borrow rate calculations, we determined that the utilization rate of the contract would need to be 2103e18 in order to reach the max borrow rate and trigger a failure. Plugging this in to the formula for utilization rate, we derived the following ratio:  reserves >= (2102/2103)*borrows + cash  With the given interest rate parameters, if token reserves, total borrows, and underlying cash meet the above ratio, the interest rate model would return a borrow rate above the maximum, leading to the failure conditions described above.  Recommendation  Note that the examples above depend on the specific interest rate parameters configured by dForce. In general, with reasonable interest rate parameters and a reasonable reserve ratio, it seems unlikely that the maximum borrow rate will be reached. Consider implementing the following changes as a precaution:  As utilization rate should be between 0 and 1 (scaled by 1e18), prevent utilization rate calculations from returning anything above 1e18. See issue 4.3 for a more thorough discussion of this topic.  Remove the settleInterest modifier from TokenAdmin._setInterestRateModel: In a worst case scenario, this will allow the Owner role to update the interest rate model without triggering the failure in Base._updateInterest.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.5 RewardDistributor requirement prevents transition of Owner role to smart contract ", "body": "  Resolution                           This issue was addressed in commit   4f1e31b by invoking  Description  From dForce, we learned that the eventual plan for the system Owner role is to use a smart contract (a multisig or DAO). However, a requirement in RewardDistributor would prevent the onlyOwner method _setDistributionFactors from working in this case.  _setDistributionFactors calls updateDistributionSpeed, which requires that the caller is an EOA:  code/contracts/RewardDistributor.sol:L179-L189  /**  @notice Update each iToken's distribution speed according to current global speed  @dev Only EOA can call this function  /  function updateDistributionSpeed() public override {  require(msg.sender == tx.origin, \"only EOA can update speeds\");  require(!paused, \"Can not update speeds when paused\");  // Do the actual update  _updateDistributionSpeed();  In the event the Owner role is a smart contract, this statement would necessitate a complicated upgrade to restore full functionality.  Recommendation  Rather than invoking updateDistributionSpeed, have _setDistributionFactors directly call the internal helper _updateDistributionSpeed, which does not require the caller is an EOA.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.6 MSDController._withdrawReserves does not update interest before withdrawal ", "body": "  Resolution  This issue was addressed in commit 2b5946e by changing calcEquity to update the interest of each MSDMinter assigned to an MSD asset.  Note that this method iterates over each MSDMinter, which may cause out-of-gas issues if the number of MSDMinters grows. dForce has informed us that the MSDMinter role will only be held by two contracts per asset (iMSD and MSDS).  Description  MSDController._withdrawReserves allows the Owner to mint the difference between an MSD asset s accumulated debt and earnings:  code/contracts/msd/MSDController.sol:L182-L195  function _withdrawReserves(address _token, uint256 _amount)  external  onlyOwner  onlyMSD(_token)  (uint256 _equity, ) = calcEquity(_token);  require(_equity >= _amount, \"Token do not have enough reserve\");  // Increase the token debt  msdTokenData[_token].debt = msdTokenData[_token].debt.add(_amount);  // Directly mint the token to owner  MSD(_token).mint(owner, _amount);  Debt and earnings are updated each time the asset s iMSD and MSDS contracts are used for the first time in a given block. Because _withdrawReserves does not force an update to these values, it is possible for the withdrawal amount to be calculated using stale values.  Recommendation  Ensure _withdrawReserves invokes iMSD.updateInterest() and MSDS.updateInterest().  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.7 permit functions use deployment-time instead of execution-time chain ID ", "body": "  Resolution                           This has been addressed in commits   a7b8fb0 and  d659f2b. The approach taken by the dForce team is to include the chain ID separately in the digest to be signed and keep the deployment/initialization-time chain ID in the  Description  EIP-2612-style  EIP-712 signatures. We focus this discussion on the  code/contracts/TokenBase/Base.sol:L23-L56  function _initialize(  string memory _name,  string memory _symbol,  uint8 _decimals,  IControllerInterface _controller,  IInterestRateModelInterface _interestRateModel  ) internal virtual {  controller = _controller;  interestRateModel = _interestRateModel;  accrualBlockNumber = block.number;  borrowIndex = BASE;  flashloanFeeRatio = 0.0008e18;  protocolFeeRatio = 0.25e18;  __Ownable_init();  __ERC20_init(_name, _symbol, _decimals);  __ReentrancyGuard_init();  uint256 chainId;  assembly {  chainId := chainid()  DOMAIN_SEPARATOR = keccak256(  abi.encode(  keccak256(  \"EIP712Domain(string name,string version,uint256 chainId,address verifyingContract)\"  ),  keccak256(bytes(_name)),  keccak256(bytes(\"1\")),  chainId,  address(this)  );  The DOMAIN_SEPARATOR is supposed to prevent replay attacks by providing context for the signature; it is hashed into the digest to be signed.  code/contracts/TokenBase/Base.sol:L589-L610  bytes32 _digest =  keccak256(  abi.encodePacked(  \"\\x19\\x01\",  DOMAIN_SEPARATOR,  keccak256(  abi.encode(  PERMIT_TYPEHASH,  _owner,  _spender,  _value,  _currentNonce,  _deadline  );  address _recoveredAddress = ecrecover(_digest, _v, _r, _s);  require(  _recoveredAddress != address(0) && _recoveredAddress == _owner,  \"permit: INVALID_SIGNATURE!\"  );  The chain ID is not necessarily constant, though. In the event of a chain split, only one of the resulting chains gets to keep the original chain ID and the other will have to use a new one. With the current pattern, a signature will be valid on both chains; if the DOMAIN_SEPARATOR is recomputed for every verification, a signature will only be valid on the chain that keeps the original ID \u2014 which is probably the intended behavior.  Remark  The reason why the not necessarily constant chain ID is part of the supposedly constant DOMAIN_SEPARATOR is that EIP-712 predates the introduction of the CHAINID opcode. Originally, it was not possible to query the chain ID via opcode, so it had to be supplied to the constructor of a contract by the deployment script.  Recommendation  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "4.8 iETH.receive() does not support contracts executing during their constructor ", "body": "  Description  iETH.receive() requires that the caller is a contract:  code/contracts/iETH.sol:L187-L195  /**  @notice receive ETH, used for flashloan repay.  /  receive() external payable {  require(  msg.sender.isContract(),  \"receive: Only can call from a contract!\"  );  This method uses the extcodesize of an account to check that the account belongs to a contract. However, contracts currently executing their constructor will have an extcodesize of 0, and will not be able to use this method.  This is unlikely to cause significant issues, but dForce may want to consider supporting this edge case.  Recommendation  Use msg.sender != tx.origin as a more reliable method to detect use by a contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/03/dforce-lending-protocol-review/"}, {"title": "5.1 Every node gets a full validator s bounty    ", "body": "  Resolution  This issue is addressed in Bug/skale 3273 formula fix 435 and SKALE-3273 Fix BountyV2 populating error 438.  The main change is related to how bounties are calculated for each validator. Below are a few notes on these pull requests:  nodesByValidator mapping is no longer used in the codebase and the non-zero values are deleted when calculateBounty() is called for a specific validator. The mapping is kept in the code for compatible storage layout in upgradable proxies.  Some functions such as populate() was developed for the transition to the upgraded contracts (rewrite _effectiveDelegatedSum values based on the new calculation formula). This function is not part of this review and will be removed in the future updates.  Unlike the old architecture, nodesByValidator[validatorId] is no longer used within the system to calculate _effectiveDelegatedSum and bounties. This is replaced by using overall staked amount and duration.  If a validator does not claim their bounty during a month, it is considered as a misbehave and her bounty goes to the bounty pool for the next month.  Description  To get the bounty, every node calls the getBounty function of the SkaleManager contract. This function can be called once per month. The size of the bounty is defined in the BountyV2 contract in the _calculateMaximumBountyAmount function:  code/contracts/BountyV2.sol:L213-L221  return epochPoolSize  .add(_bountyWasPaidInCurrentEpoch)  .mul(  delegationController.getAndUpdateEffectiveDelegatedToValidator(  nodes.getValidatorId(nodeIndex),  currentMonth  .div(effectiveDelegatedSum);  The problem is that this amount actually represents the amount that should be paid to the validator of that node. But each node will get this amount. Additionally, the amount of validator s bounty should also correspond to the number of active nodes, while this formula only uses the amount of delegated funds.  Recommendation  Every node should get only their parts of the bounty.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.2 A node exit prevents some other nodes from exiting for some period   Pending", "body": "  Resolution  Skale team s comment:  Description  When a node wants to exit, the nodeExit function should be called as many times, as there are schains in the node. Each time one schain is getting removed from the node. During every call, all the active schains are getting frozen for 12 hours.  code/contracts/NodeRotation.sol:L84-L105  function freezeSchains(uint nodeIndex) external allow(\"SkaleManager\") {  SchainsInternal schainsInternal = SchainsInternal(contractManager.getContract(\"SchainsInternal\"));  bytes32[] memory schains = schainsInternal.getActiveSchains(nodeIndex);  for (uint i = 0; i < schains.length; i++) {  Rotation memory rotation = rotations[schains[i]];  if (rotation.nodeIndex == nodeIndex && now < rotation.freezeUntil) {  continue;  string memory schainName = schainsInternal.getSchainName(schains[i]);  string memory revertMessage = \"Node cannot rotate on Schain \";  revertMessage = revertMessage.strConcat(schainName);  revertMessage = revertMessage.strConcat(\", occupied by Node \");  revertMessage = revertMessage.strConcat(rotation.nodeIndex.uint2str());  string memory dkgRevert = \"DKG process did not finish on schain \";  ISkaleDKG skaleDKG = ISkaleDKG(contractManager.getContract(\"SkaleDKG\"));  require(  skaleDKG.isLastDKGSuccessful(keccak256(abi.encodePacked(schainName))),  dkgRevert.strConcat(schainName));  require(rotation.freezeUntil < now, revertMessage);  _startRotation(schains[i], nodeIndex);  Because of that, no other node that is running one of these schains can exit during that period. In the worst-case scenario, one malicious node has 128 Schains and calls nodeExit every 12 hours. That means that some nodes will not be able to exit for 64 days.  Recommendation  Make node exiting process less synchronous.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.3 Removing a node require multiple transactions and may be very expensive   Pending", "body": "  Resolution  Skale team s comment:  Description  When removing a node from the network, the owner should redistribute all the schains that are currently on that node to the other nodes. To do so, the validator should call the nodeExit function of the SkaleManager contract. In this function, only one schain is going to be removed from the node. So the node would have to call the nodeExit function as many times as there are schains in the node. Every call iterates over every potential node that can be used as a replacement (like in https://github.com/ConsenSys/skale-network-audit-2020-10/issues/3).  In addition to that, the first call will iterate over all schains in the node, make 4 SSTORE operations and external calls for each schain:  code/contracts/NodeRotation.sol:L204-L210  function _startRotation(bytes32 schainIndex, uint nodeIndex) private {  ConstantsHolder constants = ConstantsHolder(contractManager.getContract(\"ConstantsHolder\"));  rotations[schainIndex].nodeIndex = nodeIndex;  rotations[schainIndex].newNodeIndex = nodeIndex;  rotations[schainIndex].freezeUntil = now.add(constants.rotationDelay());  waitForNewNode[schainIndex] = true;  This may hit the block gas limit even easier than issue 5.4.  If the first transaction does not hit the block s gas limit, the maximum price of deleting a node would be BLOCK_GAS_COST * 128. At the moment, it s around $50,000.  Recommendation  Optimize the process of deleting a node, so it can t hit the gas limit in one transaction, and the overall price should be cheaper.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.4 Adding a new schain may potentially hit the gas limit   Pending", "body": "  Resolution  Skale team s comment:  Description  When adding a new schain, a group of random 16 nodes is randomly selected to run that schain. In order to do so, the _generateGroup function iterates over all the nodes that can be used for that purpose:  code/contracts/SchainsInternal.sol:L522-L541  function _generateGroup(bytes32 schainId, uint numberOfNodes) private returns (uint[] memory nodesInGroup) {  Nodes nodes = Nodes(contractManager.getContract(\"Nodes\"));  uint8 space = schains[schainId].partOfNode;  nodesInGroup = new uint[](numberOfNodes);  uint[] memory possibleNodes = isEnoughNodes(schainId);  require(possibleNodes.length >= nodesInGroup.length, \"Not enough nodes to create Schain\");  uint ignoringTail = 0;  uint random = uint(keccak256(abi.encodePacked(uint(blockhash(block.number.sub(1))), schainId)));  for (uint i = 0; i < nodesInGroup.length; ++i) {  uint index = random % (possibleNodes.length.sub(ignoringTail));  uint node = possibleNodes[index];  nodesInGroup[i] = node;  _swap(possibleNodes, index, possibleNodes.length.sub(ignoringTail).sub(1));  ++ignoringTail;  _exceptionsForGroups[schainId][node] = true;  addSchainForNode(node, schainId);  require(nodes.removeSpaceFromNode(node, space), \"Could not remove space from Node\");  If the total number of nodes exceeds around a few thousands, adding a schain may hit the block gas limit.  Recommendation  Avoid iterating over all nodes when selecting a random node for a schain.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.5 Typos ", "body": "  Description  There are a few typos in the contract source code. This could result in unforeseeable issues in the future development cycles.  Examples  succesful instead of successful:  code/contracts/SkaleDKG.sol:L77-L78  mapping(bytes32 => uint) public lastSuccesfulDKG;  code/contracts/SkaleDKG.sol:L372-L373  _setSuccesfulDKG(schainId);  and many other instances of succesful through out the code.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.6 Redundant Checks in some flows", "body": "  Description  The workflows in the Skale network are complicated and multi layers (multiple calls to different modules). Some checks are done in this process that are redundant and can be removed, based on the current code and the workflow.  Examples  An example of this redundancy, is when completing a node exit procedure. completeExit() checks if the node status is leaving and if so continues:  code/contracts/Nodes.sol:L309-L311  require(isNodeLeaving(nodeIndex), \"Node is not Leaving\");  _setNodeLeft(nodeIndex);  However, in _setNodeLeft() it has an if clause for the status being Active, which will never be true.  code/contracts/Nodes.sol:L795-L803  function _setNodeLeft(uint nodeIndex) private {  nodesIPCheck[nodes[nodeIndex].ip] = false;  nodesNameCheck[keccak256(abi.encodePacked(nodes[nodeIndex].name))] = false;  delete nodesNameToIndex[keccak256(abi.encodePacked(nodes[nodeIndex].name))];  if (nodes[nodeIndex].status == NodeStatus.Active) {  numberOfActiveNodes--;  } else {  numberOfLeavingNodes--;  Recommendation  To properly check the code flows for unreachable code and remove redundant checks.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.7 Presence of empty function   ", "body": "  Resolution                           Implemented in   Bug/skale 3273 formula fix 435.  Description  estimateBounty() is declared but neither implemented nor used in any part of the current code base.  Examples  code/contracts/BountyV2.sol:L142-L159  function estimateBounty(uint /* nodeIndex */) external pure returns (uint) {  revert(\"Not implemented\");  // ConstantsHolder constantsHolder = ConstantsHolder(contractManager.getContract(\"ConstantsHolder\"));  // Nodes nodes = Nodes(contractManager.getContract(\"Nodes\"));  // TimeHelpers timeHelpers = TimeHelpers(contractManager.getContract(\"TimeHelpers\"));  // uint stagePoolSize;  // uint nextStage;  // (stagePoolSize, nextStage) = _getEpochPool(timeHelpers.getCurrentMonth(), timeHelpers, constantsHolder);  // return _calculateMaximumBountyAmount(  //     stagePoolSize,  //     nextStage.sub(1),  //     nodeIndex,  //     constantsHolder,  //     nodes  // );  Recommendation  It is suggested to remove dead code from the code base, or fully implement it before the next step.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.8 Presence of TODO tags in the codebase", "body": "  Description  A few TODO tags are present in the codebase.  Examples  code/contracts/SchainsInternal.sol:L153-L160  // TODO:  // optimize  for (uint i = 0; i + 1 < schainsAtSystem.length; i++) {  if (schainsAtSystem[i] == schainId) {  schainsAtSystem[i] = schainsAtSystem[schainsAtSystem.length.sub(1)];  break;  code/contracts/SchainsInternal.sol:L294-L301  /**  @dev Checks whether schain name is available.  TODO Need to delete - copy of web3.utils.soliditySha3  /  function isSchainNameAvailable(string calldata name) external view returns (bool) {  bytes32 schainId = keccak256(abi.encodePacked(name));  return schains[schainId].owner == address(0) && !usedSchainNames[schainId];  code/contracts/Nodes.sol:L81-L89  // TODO: move outside the contract  struct NodeCreationParams {  string name;  bytes4 ip;  bytes4 publicIp;  uint16 port;  bytes32[2] publicKey;  uint16 nonce;  And a few others in test scripts.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/10/skale-network/"}, {"title": "5.1 Incorrect Final Block Number Can Be Finalized    ", "body": "  Resolution                           fixed by adding a recommended check of   PR-24  Description  In the data finalization function finalizeCompressedBlocksWithProof, finalizationData.finalBlockNumber is the final block number of the compressed block data to be finalized. However, there is no check in the contract or the prover to ensure finalBlockNumber is correct when there is no new data submitted in the finalization, i.e., submissionDataLength == 0 . The prover can submit an incorrect final block number and, as a result, the finalized block number (currentL2BlockNumber) would be incorrect. Consequently, the prover can skip block data in the finalization.  Examples  contracts/LineaRollup.sol:L347  currentL2BlockNumber = _finalizationData.finalBlockNumber;  contracts/LineaRollup.sol:L199-L201  if (stateRootHashes[currentL2BlockNumber] != _finalizationData.parentStateRootHash) {  revert StartingRootHashDoesNotMatch();  Recommendation  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/linea-contracts-update/"}, {"title": "5.2 Finalization Fails for the First Batch of Data Submitted After Migration to the Updated Contract    ", "body": "  Resolution  Linea responded:  Our migration and deployment strategy is to have as close to zero downtime as possible. Because of this, implementing the recommendation of Set the correct initial value for dataFinalStateRootHashes for the initial batch of compressed block data. is going to be difficult as we won t know it at the time (L2 block and state is indeterminable in advance) and would require a pausing of the contracts.  The issue is fixed in PR-24  by wrapping the check  with if (startingDataParentHash != EMPTY_HASH) { to allow it go through the first time after migration, subsequent finalization will be checked.  Description  When submitting the initial batch of compressed block data after the contract update, the finalization will fail.  contracts/LineaRollup.sol:L199-L201  if (stateRootHashes[currentL2BlockNumber] != _finalizationData.parentStateRootHash) {  revert StartingRootHashDoesNotMatch();  contracts/LineaRollup.sol:L283-L294  if (finalizationDataDataHashesLength != 0) {  bytes32 startingDataParentHash = dataParents[_finalizationData.dataHashes[0]];  if (startingDataParentHash != _finalizationData.dataParentHash) {  revert ParentHashesDoesNotMatch(startingDataParentHash, _finalizationData.dataParentHash);  bytes32 startingParentFinalStateRootHash = dataFinalStateRootHashes[startingDataParentHash];  if (startingParentFinalStateRootHash != _finalizationData.parentStateRootHash) {  revert FinalStateRootHashDoesNotMatch(startingParentFinalStateRootHash, _finalizationData.parentStateRootHash);  Recommendation  Set the correct initial value for dataFinalStateRootHashes for the initial batch of compressed block data.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/linea-contracts-update/"}, {"title": "5.3 Prover Can Censor L2 \u2192 L1 Messages   Partially Addressed", "body": "  Resolution  Linea responded that the prover enforces all messages are included in the circuit, however with the circuit code is not opensourced yet, this still need to be verified  Description  In L2 \u2192 L1 messaging, messages are grouped and added to a Merkle tree by the prover. During finalization, the operator (coordinator) submits the Merkle root to L1, and the user SDK rebuilds the tree to which the message is added and generates a Merkle proof to claim against the root finalized on L1. However, the prover can skip messages when building the tree. Consequently, the user cannot claim the skipped message, which might result in frozen funds.  Currently, the prover is a single entity owned by Linea. Hence, this would require malice or negligence on Linea s part.  Examples  contracts/LineaRollup.sol:L314-L315  _addL2MerkleRoots(_finalizationData.l2MerkleRoots, _finalizationData.l2MerkleTreesDepth);  _anchorL2MessagingBlocks(_finalizationData.l2MessagingBlocksOffsets, lastFinalizedBlock);  Recommendation  Decentralize the prover, so messages can be included by different provers.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2024/01/linea-contracts-update/"}, {"title": "5.4 Malicious Operator Might Finalize Data From a Forked Linea Chain    ", "body": "  Resolution                           Linea team responded that   Description  A malicious operator (prover) can add and finalize block data from a forked Linea chain, so transactions on the forked chain can be finalized, causing a loss of funds from the L1.  For example, a malicious operator forks the canonical chain, then the attacker sends the forked chain Ether to L1 with sendMessage from the forked L2. The operator then submits the block data to L1 and finalizes it with finalizeCompressedBlocksWithProof, using the finalization data and proof from the forked chain. (Note that the malicious prover sets the forked chain chainId in its circuit as a constant.) The L1 contract (LineaRollup) doesn t know whether the data and the proof are from the canonical L2 or the forked one. The finalization succeeds, and the attacker can claim the bridged forked chain Ether and steal funds from L1.  As there is currently only one operator and it is owned by the Linea team, this kind of attack is unlikely to happen. However, when the operator and the coordinator are decentralized, the likelihood of this attack increases.  Examples  contracts/LineaRollup.sol:L211-L222  uint256 publicInput = uint256(  keccak256(  abi.encode(  shnarf,  _finalizationData.parentStateRootHash,  _finalizationData.lastFinalizedTimestamp,  _finalizationData.finalBlockNumber,  _finalizationData.finalTimestamp,  _finalizationData.l1RollingHash,  _finalizationData.l1RollingHashMessageNumber,  keccak256(abi.encodePacked(_finalizationData.l2MerkleRoots))  contracts/LineaRollup.sol:L314  _addL2MerkleRoots(_finalizationData.l2MerkleRoots, _finalizationData.l2MerkleTreesDepth);  Recommendation  Add chainId in the FinalizationData as a public input of the verifier function _verifyProof, so the proof from the forked Linea chain will not pass the verification because the chainId won t match.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/linea-contracts-update/"}, {"title": "5.5 The Compressed Block Data Is Not Verified Against Data in the Prover During Data Submission   ", "body": "  Resolution  Linea has acknowledged this issue and will implement the recommended check with the EIP-4844 upgrade using the KZG precompile  Description  proof of equivalence; the  The only difference is if the two commitments don t commit to the same block data (meaning the data submitted doesn t match the data used in the prover), submitData would fail   while in the current implementation, it would fail in the proof verification during the finalization. As a result, if the data submitted doesn t match the data in the prover in the finalization, the operator has to submit the correct data again in order to finalize it. Linea stated they will verify it in the data submission, once EIP-4844 is implemented.  Examples  contracts/LineaRollup.sol:L131-L173  function _submitData(SubmissionData calldata _submissionData) internal returns (bytes32 shnarf) {  shnarf = dataShnarfHashes[_submissionData.dataParentHash];  bytes32 parentFinalStateRootHash = dataFinalStateRootHashes[_submissionData.dataParentHash];  uint256 lastFinalizedBlock = currentL2BlockNumber;  if (_submissionData.firstBlockInData <= lastFinalizedBlock) {  revert FirstBlockLessThanOrEqualToLastFinalizedBlock(_submissionData.firstBlockInData, lastFinalizedBlock);  if (_submissionData.firstBlockInData > _submissionData.finalBlockInData) {  revert FirstBlockGreaterThanFinalBlock(_submissionData.firstBlockInData, _submissionData.finalBlockInData);  if (_submissionData.parentStateRootHash != parentFinalStateRootHash) {  revert StateRootHashInvalid(parentFinalStateRootHash, _submissionData.parentStateRootHash);  bytes32 currentDataHash = keccak256(_submissionData.compressedData);  if (dataFinalStateRootHashes[currentDataHash] != EMPTY_HASH) {  revert DataAlreadySubmitted(currentDataHash);  dataParents[currentDataHash] = _submissionData.dataParentHash;  dataFinalStateRootHashes[currentDataHash] = _submissionData.finalStateRootHash;  bytes32 compressedDataComputedX = keccak256(abi.encode(_submissionData.snarkHash, currentDataHash));  shnarf = keccak256(  abi.encode(  shnarf,  _submissionData.snarkHash,  _submissionData.finalStateRootHash,  compressedDataComputedX,  _calculateY(_submissionData.compressedData, compressedDataComputedX)  );  dataShnarfHashes[currentDataHash] = shnarf;  emit DataSubmitted(currentDataHash, _submissionData.firstBlockInData, _submissionData.finalBlockInData);  contracts/LineaRollup.sol:L384-L413  function _calculateY(  bytes calldata _data,  bytes32 _compressedDataComputedX  ) internal pure returns (bytes32 compressedDataComputedY) {  if (_data.length % 0x20 != 0) {  revert BytesLengthNotMultipleOf32();  bytes4 errorSelector = ILineaRollup.FirstByteIsNotZero.selector;  assembly {  for {  let i := _data.length  } gt(i, 0) {  } {  i := sub(i, 0x20)  let chunk := calldataload(add(_data.offset, i))  if iszero(iszero(and(chunk, 0xFF00000000000000000000000000000000000000000000000000000000000000))) {  let ptr := mload(0x40)  mstore(ptr, errorSelector)  revert(ptr, 0x4)  compressedDataComputedY := addmod(  mulmod(compressedDataComputedY, _compressedDataComputedX, Y_MODULUS),  chunk,  Y_MODULUS  Recommendation  Add the compressed block data verification in the submitData function.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2024/01/linea-contracts-update/"}, {"title": "5.6 Empty Compressed Data Allowed in Data Submission    ", "body": "  Resolution                           fixed by adding a recommended check in   PR-20  Description  In submitData, the coordinator can submit data with empty compressedData in _submissionData, which is not a desired purpose of this function and may cause undefined system behavior.  Examples  contracts/LineaRollup.sol:L115-L124  function submitData(  SubmissionData calldata _submissionData  external  whenTypeNotPaused(PROVING_SYSTEM_PAUSE_TYPE)  whenTypeNotPaused(GENERAL_PAUSE_TYPE)  onlyRole(OPERATOR_ROLE)  _submitData(_submissionData);  Recommendation  Add a check to disallow data submission with empty compressedData.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/linea-contracts-update/"}, {"title": "4.1 An Attacker Can Flood the Support by Creating Many Tickets for Arbitrary Addresses    ", "body": "  Resolution                           The client remediated to this issue and provided the following statement:  The   Description  The POST endpoint called by the create_ticket function does not implement any access control mechanism nor does not seem to implement any kind of DoS protection.  An attacker can create tickets for any arbitrary address by providing the corresponding address_key, even if he does not own the corresponding private key. That is made possible because the backend does not check whether the API key is authorized for the address key that is submitted.  Examples  packages/snap/utils/backend_functions.ts:L5-L18  export async function create_ticket(req_address: string, title: any, description: any, apikey: any) {  const address_key = SHA256(req_address, { outputLength: 32 }).toString();  try{  const requester = \"userbot@consensys.net\";  const ticketData = {ticket: {  subject: title,  requester: requester,  tags: 'anon_' + address_key,  comment:{body: description }  }};  const url = 'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?create=true';  fetch(url, {  Recommendation  When new tickets are submitted, the backend must check that the API key is authorized to create new tickets for that particular address key. Additionally, one should consider implementing proper rate limiting or equivalent DoS protection to prevent the mass creation of tickets by a single user or address, mitigating the risk of support flooding.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.2 Broken Authorization Scheme    ", "body": "  Resolution  The client fixed this issue by implementing a proper authorisation mechanism on the backend, enforcing that API keys can only access objects they own.  Description  Anyone can enumerate all tickets belonging to any wallet address by providing the corresponding address key (SHA256 of the address) to the right API endpoint. Users can also create tickets on behalf of other users by sending a POST request to the API with the corresponding address key. Finally, any user can view and post updates to tickets belonging to other users by injecting arbitrary ticket IDs in GET/POST requests sent to the API. The root cause is that the ticket IDs and address keys are not authenticated against a particular user ID/access token. Thus, it allows unauthorized access to potentially sensitive ticket information and the ability to impersonate users and create tickets and post comments to tickets belonging to other users. This is a critical flaw which must be fixed.  Examples  packages/snap/utils/backend_functions.ts:L5-L17  export async function create_ticket(req_address: string, title: any, description: any, apikey: any) {  const address_key = SHA256(req_address, { outputLength: 32 }).toString();  try{  const requester = \"userbot@consensys.net\";  const ticketData = {ticket: {  subject: title,  requester: requester,  tags: 'anon_' + address_key,  comment:{body: description }  }};  const url = 'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?create=true';  packages/snap/utils/backend_functions.ts:L38-L42  const address_key = SHA256(req_address, { outputLength: 32 }).toString();  try{  const url = 'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets';  const final_url = url + '?address=' + address_key;  packages/snap/utils/backend_functions.ts:L67-L72  export async function get_ticket_comments(ticket_id : any, req_address: any, apikey: any){  let json_ticket_comments = null;  const address_key = SHA256(req_address, { outputLength: 32 }).toString();  try{  const url = 'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?ticketId=' + ticket_id;  packages/snap/utils/backend_functions.ts:L94-L100  export async function update_ticket(ticket_id: any, input_data: any, req_address: any, apikey: any) {  console.log(`Updating ticket ${ticket_id} with comment: `, input_data);  const address_key = SHA256(req_address, { outputLength: 32 }).toString();  try {  const url = 'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?ticketId='  + ticket_id + '&create=false' + '&from_snap';  Recommendation  Implement strict validation and proper access control mechanisms to ensure that users can only create, view and modify tickets they are authorized to access. This could involve verifying user permissions against the ticket ID and address key before granting access or allowing modifications.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.3 Any Website Connected to the Snap Can Access the Address and API Key and Impersonate the User    ", "body": "  Resolution                           The client removed the   Description  Currently, there is no validation of the origin for RPC calls made to the Snap. Thus, any dapp connected to the Snap can call any RPC endpoint, which allows an attacker crafting a malicious dapp to access a user s tickets and impersonate the user to support services. The vulnerability arises because the RPC call handler onRpcRequest() can be invoked by any dapp as it does not validate the origin of the call. This enables any connected dapp to call get_snap_state, exposing critical user information like its userID, API token, and ticket details.  Examples  packages/snap/src/index.ts:L242-L247  export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {  switch (request.method) {  case 'set_snap_state':  let allTicketCommentsCount: any = [];  Recommendation  Implement strict origin validation in the onRpcRequest() handler to ensure that only authorized and intended domains can invoke the Snap. Additionally, consider removing the get_snap_state RPC endpoint if it is not strictly needed.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.4 Missing Notifications in Case Multiple Tickets Are Updated   Partially Addressed", "body": "  Resolution                           The client fixed the issue in commit   Description  Currently, if multiple tickets are updated (have new comments), the Snap will only trigger a notification for the last updated ticket.  The fetchAllTicketCommentsCount function only triggers a notification for the last updated ticket, as the variable updatedTicketId gets overridden in each iteration of the loop by the last updated ticket id. Consequently, updates to other tickets are not notified. This issue is problematic for users who have multiple tickets opened concurrently, as when there are multiple tickets with new comments, all except the last updated ticket go unnoticed.  Examples  packages/snap/src/index.ts:L83-L91  for(let i = 1; i < ticket_ids_and_comment_count.length; i += 3){  if(ticket_ids_and_comment_count[i] > allTicketCommentsCount[i]){  if(ticket_ids_and_comment_count[i+1] == 'agent'){  if (ticket_ids_and_comment_count[i - 1] == allTicketCommentsCount[i - 1]) {  fireAlerts = true;  updatedTicketId = ticket_ids_and_comment_count[i - 1];  console.log(\"There is an update on ticket ID: \", updatedTicketId);  console.log('Firing notifications...');  setSnapState(apiKey, address, allTicketCommentsCount, dialog);  Recommendation  Revise the notification logic to ensure that the user is notified in case multiple tickets are updated.  One could accumulate all updated ticket IDs in an array and then iterate over this array to fire alerts for each updated ticket. In case there are many updated tickets, one should probably notify the user and redirect him to the dapp to read the new messages.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.5 General Design and Performance Considerations: Data Fetching/Caching and State Update   Partially Addressed", "body": "  Resolution                           The client partly addressed this issue in commit   Description  The Snap s current design, which pulls all user tickets and comments from the server every second, poses significant performance and efficiency issues. This constant fetching process occurs for every user who has installed the Snap, leading to concerns, particularly for users with numerous tickets and comments. The Snaps has to re-process the same data repeatedly, despite having a local cache. This approach not only renders the local caching inefficient, as data is re-fetched in every cycle, but also places an unnecessary load on the backend and results in the same data being transmitted many times.  Additionally, it can also be highlighted that the snaps contain unnecessary state updates: every key in the Snap s state is updated every time, even if it hasn t changed. For instance, but not exclusively, here:  packages/snap/src/index.ts:L91  setSnapState(apiKey, address, allTicketCommentsCount, dialog);  packages/snap/src/index.ts:L103  setSnapState(apiKey, address, allTicketCommentsCount, dialog);  Recommendation  We recommend redesigning the data fetching mechanism to enhance efficiency and performance. One should aim to fetch only new or updated comments, potentially by cleverly leveraging ticket and comment IDs/timestamps. This would reduce the load on both the client and the server and minimize unneeded computing and data transfer. If tickets can be updated, one also needs to consider this case. In general, one should also avoid unnecessary Snap state updates by updating only the modified keys.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.6 Markdown Injection in Snaps UI Components   ", "body": "  Resolution  The client acknowledged this issue and has decided not to fix it yet:  This has not been tackled yet. Since this could only be exploited by our own agents, we decided to focus on fixing the other issues first.   Description  Snaps UI components are prone to markdown and control character injections. In this case, one of the risk is the potential for operators to mislead users. For example, an operator could create a comment that falsely appears as if another person has joined the discussion. This is achieved by formatting a comment with a reply followed by a separator and then falsely attributing a message to someone else, as in <officialReply>\\n\\n______________________\\n\\n<OtherPerson>: <OtherComment>.  Considering users should only be able to see their own and the operator s comments, and operators are assumed non-malicious, this vulnerability should not pose significant security concerns. However, one should keep that in mind in case security assumptions evolve.  Examples  packages/snap/src/index.ts:L120-L123  for (let i = 0; i < json.length; i++){  const comment = json[i]['body'];  const sender = json[i]['via']['channel'] == 'api' ? '**You**' : '**Agent**'  formatted_comments += `${sender}: ${comment}\\n\\n______________________\\n\\n`;  Recommendation  As a mitigation, one could implement comprehensive content sanitization and strict validation for markdown rendered in UI components to prevent such misleading representations. Alternatively, one could investigate rendering comments in distinct UI components.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.7 fetchAllTicketCommentsCount()  Is Unoptimized, Overly Complex, and Lacks Untrusted Input Validation    ", "body": "  Resolution  The client addressed this issue in commit 7ef2853d21049986beefaec4f2405814319f7cfb. Additionally, they provided the following statement:   The fetchAllTicketCommentsCount() method has been deleted and rebuilt from scratch. Its new name is checkTicketUpdates and the following improvements were added:  Improved performance (see 4.5 for more details)  Improved error handling  Early exit if some state variables are not available  Reduced the number of  then  statements to reduce complexion and improve readability  Renamed function and variables for better readability   Description  The current fetchAllTicketCommentsCount() function is overly complex and poorly optimized. For instance, the function unnecessarily exhibits a  sequence of  then  promises, which could be avoided. Moreover, the function fetches ticket comments sequentially, whereas a parallel approach would be more efficient. It also parses ticket_count from the received JSON data and uses it to iterate over that data, yet it fails to validate that ticket_count is less than the length of the received data. These examples illustrate that it would be a good idea to refactor and optimize the function to increase the readability and robustness of its code.  We would suggest optimizing the function and simplifying its code to improve readability and robustness. More precisely, one could make the following changes:  Exit early if variables like address or apiKey are not initialized  Sanitize and validate relevant untrusted values, such as the ticket_count  Refactor the data processing logic to be more succinct and readable. Consider using different data structures (e.g., objects) to simplify the logic and improve the robustness of the code.  Leverage parallel data fetching by utilizing Promise.all() to concurrently fetch ticket comments  Streamline the code by removing unnecessary chains of promises,  logging, and verbose comments  Add proper error handling  packages/snap/src/index.ts:L35-L42  const fetchAllTicketCommentsCount = async () => {  let allTicketCommentsCount : any = [];  let fireAlerts = false;  let updatedTicketId = null;  const state = await getSnapState();  const address = state?.address as string  packages/snap/src/index.ts:L53-L77  await get_user_tickets(address, apiKey)  .then( async (json : any) => {  const ticket_count = json['count'];  if (ticket_count > 0)  for(let i = 0; i < ticket_count; i++){  const ticket_id = json['rows'][i]['ticket']['id'];  await get_ticket_comments(ticket_id, address, apiKey).then( (json : any) => {  if(json.length > 0){  const last_comment_source = json[json.length-1]['via']['channel'];  ticket_ids_and_comment_count.push(ticket_id);  ticket_ids_and_comment_count.push(json.length);  if(last_comment_source == 'api')  ticket_ids_and_comment_count.push('user');  else if (last_comment_source == 'web')  ticket_ids_and_comment_count.push('agent');  })  })  .then(() => {  // console.log(ticket_ids_and_comment_count);  // console.log(allTicketCommentsCount);  if(allTicketCommentsCount.length == 0){  console.log('Notification process started.');  packages/snap/src/index.ts:L86-L99  if (ticket_ids_and_comment_count[i - 1] == allTicketCommentsCount[i - 1]) {  fireAlerts = true;  updatedTicketId = ticket_ids_and_comment_count[i - 1];  console.log(\"There is an update on ticket ID: \", updatedTicketId);  console.log('Firing notifications...');  setSnapState(apiKey, address, allTicketCommentsCount, dialog);  }).then(() =>{  // console.log(ticket_ids_and_comment_count);  Recommendation  We recommend completely refactoring the fetchAllTicketCommentsCount() function in line with these suggestions. This refactoring will not only make the code more readable and maintainable but also improve its performance.  As a future improvement, one should also investigate more optimized mechanisms to fetch tickets, such as server-side push instead of client-side pull or smarter algorithms to fetch only new comments. This would significantly improve user performance with many tickets/comments and reduce both server and client-side load. (see issue 4.5 )  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.8 Improper Paramater Sanitization in fetchAllTicketCommentsCount(), updateTicket(), and parseTicketComments()    ", "body": "  Resolution                           The client addressed this issue in commit   Description  Examples  packages/snap/src/index.ts:L35-L44  const fetchAllTicketCommentsCount = async () => {  let allTicketCommentsCount : any = [];  let fireAlerts = false;  let updatedTicketId = null;  const state = await getSnapState();  const address = state?.address as string  const apiKey = state?.apiKey as string  const dialog = state?.dialog as string  packages/snap/src/index.ts:L133-L139  async function updateTicket(ticketId: any, user_comment: any) {  // need a function here as getSnapState() doesn't seem to work in cron  const state = await getSnapState();  const address = state?.address as string  const apiKey = state?.apiKey as string  const update_result = await update_ticket(ticketId, user_comment, address, apiKey);  packages/snap/src/index.ts:L113-L117  async function parseTicketComments(ticketId: any) {  const state = await getSnapState();  const address = state?.address as string  const apiKey = state?.apiKey as string  let formatted_comments = `Login to https://web3tickets.metamask.io to see your personal dashboard with all tickets open for your ethereum account address. \\n\\n`;  Recommendation  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.9 Unused Imports, Inconsistent Coding Style and Unsafe Patterns   ", "body": "  Resolution  The client acknowledged the issue and will improve the codebase in the next releases.  Description  The codebase currently exhibits a lack of consistency in coding style: inconsistent use of semi-colons, inconsistent line lengths, and other stylistic discrepancies. These inconsistencies make the codebase harder to read and maintain, potentially leading to future issues. Similarly, some typescript annotations are tautologies and should be simplified. Unneeded return statements should also be ditched.  The code also exhibits patterns that are generally considered unsafe, e.g., the use of an  if  without brackets, or loose equality (==) instead of strict equality (===). These patterns should be removed in favor of safer patterns.  Additionally, the codebase contains unused imports, resulting in unnecessary complexity. It also appears to transform the bundle (in snaps.config.js.bundlerCustomizer()) by appending the Buffer package, which does not seem to be used in the Snap.  Finally, for conciseness and clarity, one should remove unneeded code and statements, such as unreachable code.  Example  Inconsistent use of semi-colons:  packages/snap/src/index.ts:L135-L137  const state = await getSnapState();  const address = state?.address as string  const apiKey = state?.apiKey as string  The typescript annotation  allTicketCommentsCount: any | [] can be simplified to any:  packages/snap/src/index.ts:L16-L16  const setSnapState = async (apiKey: string | null, address: string | null, allTicketCommentsCount: any | [], dialog : string | null) => {  Unsafe if statement:  packages/snap/src/index.ts:L56-L57  if (ticket_count > 0)  for(let i = 0; i < ticket_count; i++){  Weak equality checks:  packages/snap/src/index.ts:L75-L76  if(allTicketCommentsCount.length == 0){  console.log('Notification process started.');  Unreachable code:  packages/snap/src/index.ts:L229-L230  throw new Error('Method not found.');  break;  This always return true or throws an error. The return statement can safely be removed:  packages/snap/src/index.ts:L259-L260  await setSnapState(request.params.apiKey, request.params.address, allTicketCommentsCount, request.params.dialog);  return true;  Some extra imports:  packages/snap/src/index.ts:L2-L3  import { panel, text, heading, divider } from '@metamask/snaps-ui';  import { create_ticket, get_user_tickets, get_ticket_comments, update_ticket } from '../utils/backend_functions';  Note that these are just a few examples but more instances of some of these patterns are present in the codebase.  Recommendation  Implement the use of a linter to enforce a consistent coding style throughout the codebase. This will not only improve the readability and maintainability of the code but also help to prevent potential bugs that can arise from stylistic inconsistencies. Refactor unsafe code patterns, particularly ensuring that  if  statements and similar constructs are used with brackets to prevent potential bugs. Remove any unused imports and unneeded code segments, such as unreachable code, to enhance the codebase s clarity, safety, and reduce bundle size.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.10 Lack of Inline Code Documentation   ", "body": "  Resolution  The client acknowledged this issue and will  add comments to the most relevant and more complex methods, to explain their purpose and functionality .  Description  The codebase lacks inline documentation. This reduces the maintainability of the codebase as it can make it challenging for new developers and auditors to understand the code as it grows. Proper inline documentation is essential for maintaining code clarity and future maintainability.  Recommendation  We recommend adding inline documentation throughout the codebase. This should include comments explaining the purpose and functionality of key functions. The documentation should be clear, concise, and consistent to ensure it is helpful and understandable for anyone reviewing or working with the code.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.11 Inconsistent Naming Conventions and Misleading Function/Variable Names   Partially Addressed", "body": "  Resolution  The client acknowledged the issue and provided the following statement:  We ve renamed functions and variables, trying to use camelcase for names, as suggested. Added a new feature on top that essentially catches the api token expiration and alerts the user to sign in again in dapp.   Description  The codebase exhibits inconsistent use of naming conventions, alternating between camel case and underscore naming. In general, it is better to avoid mixing conventions to increase the code s readability and maintainability. Additionally, there are instances where function and variable names are misleading, which can cause confusion about their purpose and functionality. For example, a function named fetchAllTicketCommentsCount misleadingly suggests it returns a count, while it actually returns a boolean and a ticket ID.  Examples  packages/snap/src/index.ts:L35-L36  const fetchAllTicketCommentsCount = async () => {  packages/snap/src/index.ts:L52  let ticket_ids_and_comment_count: any[] = [];  packages/snap/src/index.ts:L37  let allTicketCommentsCount : any = [];  Recommendation  Standardize the naming conventions, preferably choosing camel case according to Javascript standard practices. Additionally, review and rename functions and variables to accurately reflect their purpose and output. This will enhance code clarity and significantly improve the auditability and maintainability of the codebase.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/12/web3-tickets/"}, {"title": "4.1 Lack of Origin Check on RPC Requests    ", "body": "  Resolution                           Addressed by   tezoroproject/metamask-snap#41  Description  The Snap does not validate the origin of RPC requests, allowing any arbitrary dApp to connect to the Snap and initiate arbitrary RPC requests. Specifically, any dApp can access the privileged getToken and deleteToken RPC endpoints. Consequently, a malicious dApp could potentially extract a user s Tezoro token from the Snap and impersonate the user in interactions with the Tezoro API. Depending on the permissions associated with this token, the implications could be critical.  Example  packages/snap/src/index.ts:L14-L18  export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {  switch (request.method) {  case 'requestAccounts': {  const data = await ethereum.request({  method: 'eth_requestAccounts',  packages/snap/src/index.ts:L64-L65  case 'getToken': {  const state = await snap.request({  packages/snap/src/index.ts:L34-L35  case 'saveToken': {  const result = await snap.request({  Recommendation  Validate the origin of all incoming RPC requests. Specifically, restrict access to the RPC endpoints to only the Tezoro management dApp. Additionally, consider removing any endpoints that are not essential for the Snap s functionality. For example, the getToken endpoint for extracting the API token might be unnecessary and could be removed to enhance security.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.2 getPriceOfAssetQuotedInUSD Might Return Flawed Asset Prices    ", "body": "  Resolution                           Addressed by   tezoroproject/metamask-snap#42  Description  First, the function getPriceOfAssetQuotedInUSD() operates under the assumption that stablecoins\u2014specifically  USDT ,  USDC ,  DAI ,  USDP , and  TUSD \u2014always maintain a 1:1 price ratio with the USD. Although this is generally expected to be the case, there have been instances where some stablecoins failed to uphold their peg to the USD. In such scenarios, this assumption no longer holds true, resulting in the return of inaccurate balances. Furthermore, it s important to note that the prices returned by this function are quoted in USDT, despite the function s name suggesting that prices are returned in USD. This could lead to discrepancies if  USDT  diverges from its fiat counterpart.  Second, The function getPriceOfAssetQuotedInUSD() assumes that every token name that starts with  W  is a wrapped token. Thus, the initial  W  is removed from the token name before fetching the prices from Binance API. As a result, the subsequent API request made to get the price of the unwrapped token could potentially fail or return an incorrect price, if the token name starts with a  W  but the token is not a wrapped token. For instance, the  WOO  token is present in the list of tokens supported by the Snap. In that case, the price API will error as it will try to fetch the price of theOOUSDT pair instead of WOOUSDT.  Finally, relying on an hardcoded external APIs is sub-optimal. Indeed, it may be that the API may fail, start returning incorrect data, or simply become outdated and stop working.  Example  packages/snap/src/external/get-price-of-asset-quoted-in-usd.ts:L15-L19  if (assetName.startsWith('W')) {  // Assume this is a wrapped token  assetName = assetName.slice(1); // remove W  try {  packages/snap/src/external/get-price-of-asset-quoted-in-usd.ts:L20-L23  const response = await fetch(  `https://api.binance.com/api/v3/ticker/price?symbol=${assetName.toUpperCase()}USDT`,  );  const json = await response.json();  Recommendation  To mitigate this issue, one should avoid making assumptions about token names. Instead, one would ideally fetch token metadata from a trusted source to determine whether a token is wrapped or not, hardcode this information in the token-list, or directly fetch the price of the wrapped token.  Moreover, instead of hardcoding the price API, we would recommend setting up a custom API Gateway which provides a layer of abstraction between the Snap and the external APIs it uses. This would provide flexibility and allow quickly swapping for other external APIs in case they stop behaving properly.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.3 Inaccurate Return Value in checkTokens()    ", "body": "  Resolution                           Addressed by   tezoroproject/metamask-snap#43  Description  The function checkTokens() checks if token exists in parsedState.data and returns {isStatePresent: true, isTokenPresent: true,} if it does not. This is incoherent as isTokenPresent should be false in that case.  Examples  packages/snap/src/check-tokens.ts:L41-L46  if (!token) {  return {  isStatePresent: true,  isTokenPresent: true,  };  packages/snap/src/schemas.ts:L35-L37  export const stateSchema = z.object({  token: z.string().optional(),  });  Recommendation  Fix the return value. isTokenPresent should be false if the token is not present in the state. Alternatively, fix the zod stateSchema to ensure that token is not optional. In that case the safeParse function will fail and the function will return the correct value.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.4 cronJob checkTokens Might Flood User Notifications    ", "body": "  Resolution                           Addressed by   tezoroproject/metamask-snap#44. The snap now sends a single notification.  Description  The snap includes a cron job named checkToken that activates every 15 days to verify which user tokens are backed up and which are not. For each token identified as not backed up (listed in tokenList), the snap issues a notification to the user. If the list of unbacked tokens is extensive, the user will receive many notifications, potentially undermining the effectiveness of these alerts or causing the user to overlook other important notifications. To alleviate this concern, it is recommended to aggregate these notifications. Issuing a single notification, or capping the number of notifications when the size of tokenList surpasses a specific threshold (e.g., 5), could improve the user experience.  Examples  packages/snap/src/index.ts:L122-L125  [...tokensList].map(async (token) => {  await snap.request({  method: 'snap_notify',  params: {  Recommendation  We would recommend to aggregate notifications, summarizing the status of unbacked tokens, at least when their number exceeds a certain reasonable threshold.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.5 deleteToken Should Prompt User for Its Consent    ", "body": "  Resolution                           Addressed by   tezoroproject/metamask-snap#45  Description  As a rule of thumb, every state-changing interaction with the Snap s state should require user confirmation, and the process should be aborted if the user does not consent. This principle is already applied to the saveToken RPC endpoint. To maintain consistency and ensure user control over their data, the deleteToken endpoint should also prompt the user for consent before proceeding to delete the token from the Snap s state.  Examples  packages/snap/src/index.ts:L85-L96  case 'deleteToken': {  await snap.request({  method: 'snap_manageState',  params: {  operation: ManageStateOperation.UpdateState,  newState: {},  encrypted: true,  },  });  return true;  Recommendation  Similarly to the saveToken RPC endpoint, the deleteToken endpoint should ask the user for its consent before deleting the token from the snap s state.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.6 cronJob checkTokens Return Value Is Not Necessary    ", "body": "  Resolution                           Addressed by   tezoroproject/metamask-snap#46  Description  The cronJob checkTokens return value is not necessary as it will never be accessed, and should be omitted.  Examples  packages/snap/src/index.ts:L133-L136  return {  data,  error,  };  Recommendation  Drop the return value  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.7 Potential Markdown Injection in snap_notify    ", "body": "  Resolution                           Fixed by addressing issue   issue 4.4  Description  It should be noted that the snap_notify message is not protected against markdown injection. This vulnerability means that token names could potentially be used to inject malicious characters into the prompt. Since token names are currently sourced from a predefined list of supported tokens, the risk is mitigated for the time being. However, it is important to consider this vulnerability, especially if the list of supported tokens is expanded or modified in the future.  Examples  packages/snap/src/index.ts:L122-L130  [...tokensList].map(async (token) => {  await snap.request({  method: 'snap_notify',  params: {  type: 'native',  message: `Protect ${token} from loss with on-chain backup & will`,  },  });  });  Recommendation  Sanitize the token names to protect against markdown injections.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.8 External/User Input Sanitization    ", "body": "  Resolution                           Fixed in   tezoroproject/metamask-snap#47  Description  It is important that every external or user input is validated to protect against injection vulnerabilities. While the zod library is utilized for validation in most instances within the codebase, there are exceptions where external inputs are not sanitized. This oversight could lead to potential security vulnerabilities.  Examples  packages/snap/src/index.ts:L46-L61  const { params } = request;  await snap.request({  method: 'snap_manageState',  params: {  operation: ManageStateOperation.UpdateState,  newState: {  token: params.token,  },  encrypted: true,  },  });  return true;  return false;  Recommendation  To mitigate potential security risks, make sure to implement comprehensive input validation for all untrusted inputs across the entire codebase. Specifically, for the example provided, utilizing zod to sanitize params.token and throw if the token does not adhere to the expected format would help in preventing bugs and potential injection attacks. Establishing a consistent validation practice will help prevent vulnerabilities related to unsanitized inputs.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/04/tezoro-snap/"}, {"title": "4.1 Public RPC Methods and Consent Management    ", "body": "  Resolution  Addressed with the following changeset: fort-major/msq@7f9cde2.  login now requires user confirmation via a MM consent message;  the session now has 2h expiry; the next signature request after the session s expiration triggers a prompt that allows the user to either refresh the session or to log out;  both prompts clearly state that the website, where the user is logging it, will be able to silently sign messages on behalf of one of user s identities;  Additionally, we were very concerned about the problem, we ve discussed on the call. Namely, that in case of attackers being able to replace the code of the MSQ dapp (via a DNS attack, for example), they will be able to drain user s wallets empty. In this commit we ve also addressed this issue the following way:  the snap sign API now requires you to supply not the hash of the transaction, but it s body instead (we were able to find a way to make this work without changing any client-side API);  the sign snap method then calculates the hash by itself and signs this hash;  optionally, if the API was used from the MSQ website and we detect potentially harmful transaction (one that could move user s funds), then we prompt the user with details of this transaction and sign the message only in case the user confirms it.  This makes users  assets immutable to even such deadly attacks like ones discussed. Currently this harmful transaction detection only works at MSQ website - other websites are free to sign any transactions they want without users noticing it (but they are now semi-protected with session expiry).  Description  User consent may not consistently be enforced. Identities are bound to their origin (URL). Third-party origins are outside the scope of this Snap and are therefore in a lower trust zone where it is uncertain what security measures are in place to protect the dApp from impersonating the user s wallet identity. dApps may be hosted on integrity-protecting endpoints (ipfs/IC), however, this is not enforced. Additionally, even when hosted on integrity-protecting endpoints there are still risks of insider and external attacks on the deployed dApp (Insider changing code, External attacker gaining access to code, Injection, Web Attacks), BGP routing related attacks (typically expensive), and DNS related attacks.  Allowing linked identities to sign with a main origin s identity extends the risk from one public origin to another.  It should be noted that identities on public RPC methods are origin bound. There is no direct way for one public origin to sign with another origin s identity unless it is linked.  Examples  critical functionality: acting on behalf of user  SNAP_METHODS.public.identity.sign - sign with origin bound identity  potential identity leak  SNAP_METHODS.public.identity.getPublicKey - origin bound identity SNAP_METHODS.public.identity.getPseudonym - origin bound pseudonym  no concerns  SNAP_METHODS.public.identity.getLinks - origin links SNAP_METHODS.public.identity.sessionExists - check if session for origin exists  Example: signing  The function handleIdentitySign is responsible for signing a payload with an identity. However, it has been observed that the function proceeds to sign the payload without seeking explicit user confirmation or displaying the payload in a human-readable format. This approach can significantly undermine the security and trust model of MetaMask Snaps by allowing potentially malicious operations to be executed without the user s informed consent.  packages/snap/src/protocols/identity.ts:L268-L280  export async function handleIdentitySign(bodyCBOR: string, origin: TOrigin): Promise<ArrayBuffer> {  const body: IIdentitySignRequest = zodParse(ZIdentitySignRequest, fromCBOR(bodyCBOR));  const manager = await StateManager.make();  let session = (await manager.getOriginData(origin)).currentSession;  if (session === undefined) {  err(ErrorCode.UNAUTHORIZED, \"Log in first\");  const identity = await getSignIdentity(session.deriviationOrigin, session.identityId, body.salt);  return await identity.sign(body.challenge);  Recommendation  When performing critical actions on behalf of the user, always ask for consent. The user must always be notified when a dApp acts on their behalf (especially signing). For API that provides less critical information it should be considered to implement a session based consent mechanism that trades security for convenience where, e.g., linked identities or the public key can only be extracted if the user at least once confirmed this for the current origin (caching the decision).  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.2 Keypairs Generated by Dapps Might Be Unrecoverable Which Could Result in Loss of Funds    ", "body": "  Resolution                           The client acknowledged and mitigated this issue in commit   fort-major/msq@59a0b88. Note that the companion dApp can still sign data with arbitrary salts, but external dApps cannot anymore.  Description  The current Snap implementation allows untrusted dapps to supply their own nonces for the generation of unique private keys tied to their identities. These nonces, which can be arbitrary and are not managed or stored by the Snap, introduce a risk. Specifically, if a dapp fails to securely store these nonces or ceases operation, users may irretrievably lose access to their accounts, potentially resulting in the loss of funds. This issue underscores a vulnerability in the system s design, where the reliance on external parties for the management of crucial security parameters compromises the safety and recoverability of user assets.  Examples  From the documentation:  MSQ is a Snap, it has no cloud storage, so all the data is self-managed by the user. It is safe for users to lose their data (by re-installing the snap, for example) - because they can recover it from their seed phrase later. To achieve that we use deterministic algorithms for entropy derivation.  packages/snap/src/protocols/identity.ts:L298-L309  const body = zodParse(ZIdentityGetPublicKeyRequest, fromCBOR(bodyCBOR));  const manager = await StateManager.make();  let session = (await manager.getOriginData(origin)).currentSession;  if (session === undefined) {  err(ErrorCode.UNAUTHORIZED, \"Log in first\");  const identity = await getSignIdentity(session.deriviationOrigin, session.identityId, body.salt);  return identity.getPublicKey().toRaw();  Recommendation  To mitigate the risk of users losing access to their accounts and funds due to mismanaged or lost nonces by dapps, it is recommended to either:  Enforce Deterministic Nonces: Shift to a system where the Snap generates unique identities using deterministic nonces, ensuring all accounts are recoverable, independent of dapps actions or continuity. This is already implemented to generate  root  dapps identities.  Introduce Disclaimers: Clearly inform users and developers of the risks through disclaimers in the user interface and documentation.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.3 Timestamp Logic Flaws in Snap s Caching Mechanism    ", "body": "  Resolution                           Addressed in commit   4d6b006f2870b8b560e068ae51821d9d962d129a  Description  The Snap employs a custom wrapper around its storage, incorporating a caching mechanism to optimize performance by updating the Snap s storage only when necessary. This caching strategy uses a timestamp-based method, maintaining records of the last storage (LAST_STATE_PERSIST_TIMESTAMP) and state (STATE_UPDATE_TIMESTAMP) updates to decide on the need for persisting the updated state. However, two logical flaws were identified in this approach:  First, in the retrieveStateWrapped() function, where the state is fetched from the Snap s storage, the state update timestamp is only refreshed if the retrieved state is null. Consequently, if an existing state is successfully retrieved, the state update timestamp remains unchanged, which is incoherent.  packages/snap/src/state.ts:L464-L472  if (state == null) {  const s = makeDefaultState();  STATE = s;  STATE_UPDATE_TIMESTAMP = Date.now();  } else {  STATE = zodParse(ZState, fromCBOR(state.data as string));  Second, the persistStateLocal() function, responsible for persisting the state, updates the LAST_STATE_PERSIST_TIMESTAMP before the actual state update is executed. This can lead to an invalid timestamp if the subsequent state update operation (e.g., due to CBOR encoding errors or failures in the internal Snap s storage mechanism) fails, risking state and storage inconsistencies.  packages/snap/src/state.ts:L527-L538  async function persistStateLocal(): Promise<void> {  if (LAST_STATE_PERSIST_TIMESTAMP >= STATE_UPDATE_TIMESTAMP) return;  zodParse(ZState, STATE);  LAST_STATE_PERSIST_TIMESTAMP = Date.now();  await snap.request({  method: \"snap_manageState\",  params: {  operation: \"update\",  newState: { data: toCBOR(STATE) },  Recommendation  Adjust the timestamp update logic as follows:  In retrieveStateWrapped(), ensure the STATE_UPDATE_TIMESTAMP is refreshed whenever a state (not null or empty) is retrieved, not just when the state is null.  In persistStateLocal(), update the LAST_STATE_PERSIST_TIMESTAMP only after the state has been successfully persisted, preventing inaccuracies in the event of a failed state update.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.4 Protected (Administrative Origin) RPC Methods and Consent Management    ", "body": "  Resolution  Addressed with the following changesets: fort-major/msq@7f9cde2 and fort-major/msq@0b9f8d1 (removing whitelisted method names, only allowing  icrc1_transfer)  The client provided the following statement:  login now requires user confirmation via a MM consent message;  the session now has 2h expiry; the next signature request after the session s expiration triggers a prompt that allows the user to either refresh the session or to log out;  both prompts clearly state that the website, where the user is logging it, will be able to silently sign messages on behalf of one of user s identities;  Additionally, we were very concerned about the problem, we ve discussed on the call. Namely, that in case of attackers being able to replace the code of the MSQ dapp (via a DNS attack, for example), they will be able to drain user s wallets empty. In this commit we ve also addressed this issue the following way:  the snap sign API now requires you to supply not the hash of the transaction, but it s body instead (we were able to find a way to make this work without changing any client-side API);  the sign snap method then calculates the hash by itself and signs this hash;  optionally, if the API was used from the MSQ website and we detect potentially harmful transaction (one that could move user s funds), then we prompt the user with details of this transaction and sign the message only in case the user confirms it.  This makes users  assets immutable to even such deadly attacks like ones discussed. Currently this harmful transaction detection only works at MSQ website - other websites are free to sign any transactions they want without users noticing it (but they are now semi-protected with session expiry).  Description  Identities are bound to their origin (URL). Third-party origins are outside the scope of this Snap and are therefore in a lower trust zone where it is unsure what security measures are in place to protect the dApp from impersonating the users  wallet identity. dApps may be hosted on integrity protecting endpoints (ipfs/IC), however, this is not enforced.  Protected RPC functions can only be invoked by the MSQ administrative origin. User consent may not consistently be enforced on the administrative origin.  The administrative origin is identified by the origin URL. According to the client the dApp is hosted on an integrity protecting endpoint (IC). This already protects from direct manipulation of the deployed code, however, it may still be problematic as the Snap and Management dApp are in different trust zones with the dApp being exposed to many external factors that make it more prone to web related attacks. That said, even when hosted on integrity protecting endpoins there are still risks of insider and external attacks on the deployed dApp (Insider changing code, External attacker gaining access to code, Injection, Web Attacks), BGP routing related attacks (typically expensive), and DNS related attacks. In the worst case, an insider/external attacker gaining control of the trusted origin may be able to perform actions on many users behalf s without them knowing (given that the user accesses the management origin).  Examples  critical  SNAP_METHODS.protected.identity.login - log into any origin SNAP_METHODS.protected.identity.editPseudonym SNAP_METHODS.protected.icrc1.editAssetAccount  unclear  SNAP_METHODS.protected.statistics.get SNAP_METHODS.protected.statistics.increment SNAP_METHODS.protected.statistics.reset  potential privacy leak  SNAP_METHODS.protected.state.getAllOriginData - subset of origin data (no keys) SNAP_METHODS.protected.state.getAllAssetData - subset of asset data SNAP_METHODS.protected.identity.getLoginOptions  Recommendation  When performing critical actions on behalf of the user, always ask for consent. The user must always be notified when a dApp acts on their behalf (especially signing). For API that provides less critical information it should be considered to implement a lazy session based consent mechanism that trades security for convenience where, i.e., data can only be extracted from the snap if the user at least once confirmed this for the current session.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.5 Protected_handleIdentityLogin - Unchecked withIdentityId    ", "body": "  Resolution                           Addressed with the following changeset enforcing that a valid   fort-major/msq@59a0b88  Description  protected_handleIdentityLogin is used to create a new session and logs in to a particular origin (e.g., a website). At a certain point, a new session object is created, where identityId: body.withIdentityId is set. The withIdentityId is an unchecked request parameter, which could potentially lead to an inconsistency where the origin set s an invalid ID.  Examples  packages/snap/src/protocols/identity.ts:L76-L101  export async function protected_handleIdentityLogin(bodyCBOR: string): Promise<true> {  const body: IIdentityLoginRequest = zodParse(ZIdentityLoginRequest, fromCBOR(bodyCBOR));  const manager = await StateManager.make();  if (body.withLinkedOrigin !== undefined && body.withLinkedOrigin !== body.toOrigin) {  if (!manager.linkExists(body.withLinkedOrigin, body.toOrigin))  err(ErrorCode.UNAUTHORIZED, \"Unable to login without a link\");  const originData = await manager.getOriginData(body.toOrigin);  if (Object.keys(originData.masks).length === 0) {  unreacheable(\"login - no origin data found\");  const timestamp = new Date().getTime();  originData.currentSession = {  deriviationOrigin: body.withLinkedOrigin ?? body.toOrigin,  identityId: body.withIdentityId,  timestampMs: timestamp,  };  manager.setOriginData(body.toOrigin, originData);  manager.incrementStats({ login: 1 });  return true;  Recommendation  withIdentityId is an id relative to the origins masks, hence, it should be validated against the number of existing masks.  The client validated the issue providing more context:  Moreover, when withLinkedOrigin is provided, it should validate against masks of linked origin data. And if not, then it should validate with the originData.masks.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.6 Protected_handleAddAssetAccount - Should Verify name, symbol Matches assetId    ", "body": "  Resolution                           Addressed with the following changeset, escaping and validating asset data more strictly, verifying that the assetId is valid and displaying symbol and name from it s internal data:   fort-major/msq@59a0b88  Description  The function protected_handleAddAssetAccount adds an account to an existing asset. It takes the asset name/symbol and assetId as inputs and then adds an account to the assetId if the user approves.  Examples  packages/snap/src/protocols/icrc1.ts:L90-L113  export async function protected_handleAddAssetAccount(bodyCBOR: string): Promise<string | null> {  const body = zodParse(ZICRC1AddAssetAccountRequest, fromCBOR(bodyCBOR));  const manager = await StateManager.make();  const agreed = await snap.request({  method: \"snap_dialog\",  params: {  type: \"confirmation\",  content: panel([  heading(`\ud83d\udd12 Confirm New ${body.symbol} Account \ud83d\udd12`),  text(`Are you sure you want to create a new **${body.name}** (**${body.symbol}**) token account?`),  text(`This will allow you to send and receive **${body.symbol}** tokens.`),  divider(),  text(\"**Confirm?** \ud83d\ude80\"),  ]),  },  });  if (!agreed) return null;  const accountName = manager.addAssetAccount(body.assetId);  return accountName;  Recommendation  The function should take an assetId as input parameter only. Then check if the assetId has accounts registered. If that s the case, display the name, symbol that corresponds to the assetId and add an account upon user confirmation.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.7 Entropy / Signature Handling & Hardening    ", "body": "  Resolution                           The following changeset is addressing this issue by being more strict on type and input validation:   fort-major/msq@26a0eec  Description  The functions getBaseEntropy and getSignIdentity lack validation for the correct type of arguments or the presence of control characters that may allow context breaks (newline).  For example, in the SNAP_METHODS.public.identity.requestLink method handler, the body.withOrigin is unsanitized and concatenated directly for the resulting salt. withOrigin is a user provided value and may include \\n which breaks the context of the salt structure.  Examples  packages/snap/src/utils.ts:L75-L89  export async function getSignIdentity(  origin: TOrigin,  identityId: TIdentityId,  salt: Uint8Array,  ): Promise<Secp256k1KeyIdentity> {  // the MSQ site has constant origin  // this will allow us to change the domain name without users losing their funds and accounts  const orig = isMsq(origin) ? \"https://msq.tech\" : origin;  // shared prefix may be used in following updates  const entropy = await getEntropy(orig, identityId, \"identity-sign\\nshared\", salt);  return Secp256k1KeyIdentity.fromSecretKey(entropy);  packages/snap/src/utils.ts:L105-L115  async function getBaseEntropy(origin: TOrigin, identityId: TIdentityId, internalSalt: string): Promise<Uint8Array> {  const generated: string = await snap.request({  method: \"snap_getEntropy\",  params: {  version: 1,  salt: `\\x0amsq-snap\\n${origin}\\n${identityId}\\n${internalSalt}`,  },  });  return hexToBytes(generated.slice(2));  Recommendation  To address this, enforcing that identityId is a positive number, valid id and origin is a valid URL (free from control characters) would mitigate misuse of this functionality.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.8 makeAvatarSvgCustom - Potential SVG HTML Injection, React innerHTML    ", "body": "  Resolution  fort-major/msq@26a0eec by adding  Additional fix addressing shortcomings of the previous solution as escapeHtml cannot be used for HTML-Attrib sanitization, adding regex checks for html color arguments for makeAvatarSvgCustom: fort-major/msq@59a0b88  Description  The function makeAvatarSvgCustom inserts the given arguments directly into a string that represents an XML SVG image. Since the arguments are not sanitized, there is a potential risk for XML-SVG injection, which could include malicious scripts.  Please note that this affects all arguments provided to the function, especially bgColor, but also the ones that are calculating cx,cy because the addition turns into a string concatenation if face[xy] is a string.  The severity rating is based on the current exploitability which is comparable low with the demo implementations of the front-end.  Examples  packages/shared/src/avatar.ts:L23-L89  /**  Generates a custom avatar SVG string based on provided parameters including body color, body angle,  face expression, and optional background and eye colors. This function allows for the creation of a  personalized avatar with specific characteristics defined by the input parameters. The SVG is constructed  with various elements such as circles for the body and eyes, and a custom path for the face expression.  Additional details like eye pupils and mouth are also included, with positions adjusted based on the body angle.  @param {string} id - A unique identifier used to generate clip paths for the eyes, ensuring they are unique within the SVG.  @param {string} bodyColor - The fill color for the avatar's body.  @param {IAngle} bodyAngle - An object containing the center coordinates for the body and face, used to position elements.  @param {string} faceExpression - A string representing the SVG path for the face expression.  @param {string} [bgColor=\"#1E1F28\"] - Optional background color of the SVG. Defaults to a dark gray if not specified.  @param {string} [eyeWhiteColor=\"white\"] - Optional color for the whites of the eyes. Defaults to white if not specified.  @returns {string} A string representation of the SVG for the custom avatar.  /  export function makeAvatarSvgCustom(  id: string,  bodyColor: string,  bodyAngle: IAngle,  faceExpression: string,  bgColor: string = \"#1E1F28\",  eyeWhiteColor: string = \"white\",  ): string {  const { bodyCx, bodyCy, faceX, faceY } = bodyAngle;  const eyeWhite1Cx = faceX + EYE_WHITE_1_CX;  const eyeWhite1Cy = faceY + EYE_WHITE_1_CY;  const eyePupil1Cx = faceX + EYE_PUPIL_1_CX;  const eyePupil1Cy = faceY + EYE_PUPIL_1_CY;  const eyeWhite2Cx = faceX + EYE_WHITE_2_CX;  const eyeWhite2Cy = faceY + EYE_WHITE_2_CY;  const eyePupil2Cx = faceX + EYE_PUPIL_2_CX;  const eyePupil2Cy = faceY + EYE_PUPIL_2_CY;  const mouthX = faceX + MOUTH_X;  const mouthY = faceY + MOUTH_Y;  return `  <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"100\" height=\"100\" style=\"position:relative;width:100%;height:100%;\" viewBox=\"0 0 100 100\" fill=\"none\">  <defs>  <clipPath id=\"clip-eye-1-${id}\">  <circle cx=\"${eyeWhite1Cx}\" cy=\"${eyeWhite1Cy}\" r=\"6\" />  </clipPath>  <clipPath id=\"clip-eye-2-${id}\">  <circle cx=\"${eyeWhite2Cx}\" cy=\"${eyeWhite2Cy}\" r=\"6\" />  </clipPath>  </defs>  <rect id=\"bg\" x=\"0\" y=\"0\" width=\"100\" height=\"100\" fill=\"${bgColor}\"/>  <g id=\"body-group\">  <circle id=\"body\" cx=\"${bodyCx}\" cy=\"${bodyCy}\" r=\"50\" fill=\"${bodyColor}\" />  <circle id=\"eye-white-1\" cx=\"${eyeWhite1Cx}\" cy=\"${eyeWhite1Cy}\" r=\"6\" fill=\"${eyeWhiteColor}\" />  <circle id=\"eye-pupil-1\" cx=\"${eyePupil1Cx}\" cy=\"${eyePupil1Cy}\" r=\"5\" fill=\"#0A0B15\" clip-path=\"url(#clip-eye-1-${id})\" />  <circle id=\"eye-white-2\" cx=\"${eyeWhite2Cx}\" cy=\"${eyeWhite2Cy}\" r=\"6\" fill=\"${eyeWhiteColor}\" />  <circle id=\"eye-pupil-1\" cx=\"${eyePupil2Cx}\" cy=\"${eyePupil2Cy}\" r=\"5\" fill=\"#0A0B15\" clip-path=\"url(#clip-eye-2-${id})\" />  <g transform=\"translate(${mouthX}, ${mouthY})\" id=\"mouth\">  ${faceExpression}  </g>  </g>  </svg>  `;  used in:  apps/site/src/frontend/components/boop-avatar/index.tsx:L37-L54  export function CustomBoopAvatar(props: ICustomBoopAvatarProps) {  return (  <BoopAvatarWrapper  classList={props.classList}  size={props.size}  ref={(r) => {  r.innerHTML = makeAvatarSvgCustom(  props.id,  props.bodyColor,  props.angle,  FACE_EXPRESSIONS[props.faceExpression - 1],  props.bgColor,  props.eyeWhiteColor,  );  }}  />  );  packages/client/src/identity.ts:L69-L83  /**  ## Returns user's avatar for current MSQ identity  This avatar is an auto-generated SVG image  and should be treated as an easy way to render avatars for users without profiles.  @param {string | undefined} bgColor  @returns {Promise<string>} avatar SVG src string as \"data:image/svg+xml...\"  /  getAvatarSrc(bgColor?: string): Promise<string> {  const principal = this.getPrincipal();  const svg = btoa(makeAvatarSvg(principal, bgColor));  return Promise.resolve(`data:image/svg+xml;base64,${svg}`);  apps/demo/src/frontend/pages/index/index.tsx:L73-L76  const profile: IProfile = {  pseudonym: await identity.getPseudonym(),  avatarSrc: await identity.getAvatarSrc(),  };  used with innerHTML  apps/site/src/frontend/components/boop-avatar/index.tsx:L15-L24  export function BoopAvatar(props: IBoopAvatarProps) {  return (  <BoopAvatarWrapper  size={props.size}  ref={(r) => {  r.innerHTML = makeAvatarSvg(props.principal);  }}  />  );  Recommendation  Runtime typecheck provided values (numbers vs. strings). Sanitize and validate arguments before embedding then with HTML or use a templating language to build the SVG (recommended)  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.9 CtrlChar/Markdown Injection   Partially Addressed", "body": "  Resolution  Addressed with the following changeset, wrapping the Snap native UI Text element (accepts Markdown), escaping control characters. Note that the client chose to allow Markdown *_ style elements which is not ideal, as it gives some control over the presentation of data inside the Snap context to the calling dApp.  Changeset: fort-major/msq@2704c68  Description  On certain occasions, the snap may need to present a dialog to the user to request confirmation for an action or data verification. This step is crucial as dapps are not always trusted, and it s essential to prevent scenarios where they can silently sign data or perform critical operations using the user s keys without explicit permission. To create custom user-facing dialogs, MetaMask provides the Snaps UI package, equipped with style-specific components. However, some of these components have been found to have potentially unintended side-effects.  For instance, the text() component can render Markdown or allow for control character injections. Specifically this poses a concern because users trust information displayed by the Snap.  In the code snippet provided below, please note that the variable body is provided by the dApp. It may contain Markdown renderable strings or Control Characters that can disrupt the context of the user-displayed message. It appears that only protected methods (admin origin) are affected by this, which is reflected in the severity rating of this finding.  Examples  protected_handleAddAssetAccount - decodes ZICRC1AddAssetAccountRequest  from CBOR with body potentially containing markdown or control chars.  packages/snap/src/protocols/icrc1.ts:L90-L107  export async function protected_handleAddAssetAccount(bodyCBOR: string): Promise<string | null> {  const body = zodParse(ZICRC1AddAssetAccountRequest, fromCBOR(bodyCBOR));  const manager = await StateManager.make();  const agreed = await snap.request({  method: \"snap_dialog\",  params: {  type: \"confirmation\",  content: panel([  heading(`\ud83d\udd12 Confirm New ${body.symbol} Account \ud83d\udd12`),  text(`Are you sure you want to create a new **${body.name}** (**${body.symbol}**) token account?`),  text(`This will allow you to send and receive **${body.symbol}** tokens.`),  divider(),  text(\"**Confirm?** \ud83d\ude80\"),  ]),  },  });  protected_handleAddAsset - body  packages/snap/src/protocols/icrc1.ts:L59-L78  export async function protected_handleAddAsset(bodyCBOR: string): Promise<IAssetDataExternal[] | null> {  const body = zodParse(ZICRC1AddAssetRequest, fromCBOR(bodyCBOR));  const manager = await StateManager.make();  const assetNames = body.assets.filter((it) => it.name && it.symbol).map((it) => `${it.name} (${it.symbol})`);  if (assetNames.length > 0) {  const agreed = await snap.request({  method: \"snap_dialog\",  params: {  type: \"confirmation\",  content: panel([  heading(`\ud83d\udd12 Confirm New Assets \ud83d\udd12`),  text(`Are you sure you want to add the following tokens to your managed assets list?`),  ...assetNames.map((it) => text(` - **${it}**`)),  divider(),  text(\"**Confirm?** \ud83d\ude80\"),  ]),  },  });  protected_handleShowICRC1TransferConfirm - body  packages/snap/src/protocols/icrc1.ts:L26-L57  export async function protected_handleShowICRC1TransferConfirm(bodyCBOR: string): Promise<boolean> {  const body = zodParse(ZShowICRC1TransferConfirmRequest, fromCBOR(bodyCBOR));  const agreed = await snap.request({  method: \"snap_dialog\",  params: {  type: \"confirmation\",  content: panel([  heading(`\ud83d\udcb3 Confirm ${body.ticker} Transfer \ud83d\udcb3`),  text(\"**Protocol:**\"),  text(\"ICRC-1\"),  text(\"**Initiator:**\"),  text(`\ud83c\udf10 ${originToHostname(body.requestOrigin)}`),  text(\"**From:**\"),  text(body.from),  text(\"**To principal ID:**\"),  text(body.to.owner),  text(\"**To subaccount ID:**\"),  text(body.to.subaccount !== undefined ? bytesToHex(body.to.subaccount) : \"Default subaccount ID\"),  text(\"**Total amount:**\"),  heading(`${body.totalAmountStr} ${body.ticker}`),  divider(),  heading(\"\ud83d\udea8 BE CAREFUL! \ud83d\udea8\"),  text(\"This action is irreversible. You won't be able to recover your funds!\"),  divider(),  text(\"**Confirm?** \ud83d\ude80\"),  ]),  },  });  return Boolean(agreed);  Recommendation  Validate inputs. Encode data in a safe way to be displayed to the user (markdown, control chars). Show the original data provided within a pre-text or code block (copyable). Consider setting markdown: false for text ui components that do not render text.  ../packages/snaps-sdk/src/ui/components/text.ts:L34-L53  /**  Create a {@link Text} node.  @param args - The node arguments. This can be either a string  and a boolean, or an object with a `value` property  and an optional `markdown` property.  @param args.value - The text content of the node.  @param args.markdown - An optional flag to enable or disable markdown. This  is enabled by default.  @returns The text node as object.  @example  const node = text({ value: 'Hello, world!' });  const node = text('Hello, world!');  const node = text({ value: 'Hello, world!', markdown: false });  const node = text('Hello, world!', false);  /  export const text = createBuilder(NodeType.Text, TextStruct, [  'value',  'markdown',  ]);  const node = text({ value: 'Hello, world!', markdown: false });  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.10 Shared/hexToBytes - Incorrect Hex String Handling    ", "body": "  Resolution                           Addressed this with the following changeset, enforcing the input string to be of hex-chars with a correct length:   fort-major/msq@2704c68  Description  The function hexToBytes aims to convert a hex string into a Uint8Array. It splits the string into parts of two characters each and then tries to parse each part into an integer.  However, the function fails to validate that the provided hexString is of valid hex characters only. This may lead to the function interpreting non-hex characters incorrectly as zero bytes while it should throw an exception/report an error condition instead.  In the case where hexString contains non-hex characters, the parseInt will return NaN which in turn gets mapped to [00, ..] elements in the resulting Uint8Array.  Examples  packages/shared/src/encoding.ts:L42-L58  /**  ## Decodes {@link Uint8Array} from hex-string  @see {@link bytesToHex}  @param hexString  @returns  /  export const hexToBytes = (hexString: string): Uint8Array => {  const matches = hexString.match(/.{1,2}/g);  if (matches == null) {  throw new Error(\"Invalid hexstring\");  return Uint8Array.from(matches.map((byte) => parseInt(byte, 16)));  };  Recommendation  Before passing the hex string into parseInt function, add validation checks to verify if the given string is a valid hex string of correct length.  export const hexToBytes = (hexString: string): Uint8Array => {  if (!/^([0-9A-Fa-f]{2})+$/.test(hexString)) {  throw new Error(\"Invalid hexstring\");  const matches = hexString.match(/.{1,2}/g);  return Uint8Array.from(matches.map((byte) => {  const parsed = parseInt(byte, 16);  if (isNaN(parsed)) {  throw new Error('Invalid byte found')  return parsed;  }));  };  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.11 Unused Imports   ", "body": "  Resolution                           Addressed with the following changeset, removing the unused imports in   fort-major/msq@26a0eec and  fort-major/msq@4d6b006 addressing the remaining unused imports.  Description  While reviewing the codebase, we identified multiple instances of unused imports across the project s files. The presence of these unused imports may affect its maintainability and readability.  Examples  (This list is not exhaustive)  IStatisticsData, ZStatisticsData  packages/snap/src/protocols/statistics.ts:L1-L8  import {  IStatisticsData,  type IStatistics,  ZStatisticsData,  zodParse,  fromCBOR,  ZStatisticsIncrementRequest,  } from \"@fort-major/msq-shared\";  jsSHA, Crc32, duplicate import Principal  packages/shared/src/encoding.ts:L1-L5  import { Principal } from \"@dfinity/principal\";  import { Encoder } from \"cbor-x\";  import { Crc32 } from \"@aws-crypto/crc32\";  import jsSHA from \"jssha\";  IMetaMaskEthereumProvider  packages/client/src/client.ts:L1  import { IGetSnapsResponse, IMetaMaskEthereumProvider, ISnapRequest } from \"./types\";  Recommendation  Remove unused imports. Implementing a linter in the development workflow can help automate the detection and removal of such imports, ensuring a cleaner codebase and promoting best coding practices.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/03/msq-snap/"}, {"title": "4.1 Potential Reentrancy Into Strategies ", "body": "  Resolution  EigenLabs Quick Summary: The StrategyBase contract may be vulnerable to a token contract that employs some sort of callback to a function like sharesToUnderlyingView, before the balance change is reflected in the contract.  The shares have been decremented, which would lead to an incorrect return value from sharesToUnderlyingView.  EigenLabs Response: As noted in the report, this is not an issue if the token contract being used does not allow for reentrancy.  For now, we will make it clear both in the contracts as well as the docs that our implementation of StrategyBase.sol does not support tokens with reentrancy.  Because of the way our system is designed, anyone can choose to design a strategy with this in mind!  Description  Nevertheless, other functions could be reentered, for example, sharesToUnderlyingView and underlyingToSharesView, as well as their (supposedly) non-view counterparts.  Let s look at the withdraw function in StrategyBase. First, the amountShares shares are burnt, and at the end of the function, the equivalent amount of token is transferred to the depositor:  src/contracts/strategies/StrategyBase.sol:L108-L143  function withdraw(address depositor, IERC20 token, uint256 amountShares)  external  virtual  override  onlyWhenNotPaused(PAUSED_WITHDRAWALS)  onlyStrategyManager  require(token == underlyingToken, \"StrategyBase.withdraw: Can only withdraw the strategy token\");  // copy `totalShares` value to memory, prior to any decrease  uint256 priorTotalShares = totalShares;  require(  amountShares <= priorTotalShares,  \"StrategyBase.withdraw: amountShares must be less than or equal to totalShares\"  );  // Calculate the value that `totalShares` will decrease to as a result of the withdrawal  uint256 updatedTotalShares = priorTotalShares - amountShares;  // check to avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES || updatedTotalShares == 0,  \"StrategyBase.withdraw: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  // Actually decrease the `totalShares` value  totalShares = updatedTotalShares;  /**  @notice calculation of amountToSend *mirrors* `sharesToUnderlying(amountShares)`, but is different since the `totalShares` has already  been decremented. Specifically, notice how we use `priorTotalShares` here instead of `totalShares`.  /  uint256 amountToSend;  if (priorTotalShares == amountShares) {  amountToSend = _tokenBalance();  } else {  amountToSend = (_tokenBalance() * amountShares) / priorTotalShares;  underlyingToken.safeTransfer(depositor, amountToSend);  If we assume that the token contract has a callback to the recipient of the transfer before the actual balance changes take place, then the recipient could reenter the strategy contract, for example, in sharesToUnderlyingView:  src/contracts/strategies/StrategyBase.sol:L159-L165  function sharesToUnderlyingView(uint256 amountShares) public view virtual override returns (uint256) {  if (totalShares == 0) {  return amountShares;  } else {  return (_tokenBalance() * amountShares) / totalShares;  The crucial point is: If the callback is executed before the actual balance change, then sharesToUnderlyingView will report a bad result because the shares have already been burnt but the token balance has not been updated yet.  For deposits, the token transfer to the strategy happens first, and the shares are minted after that:  src/contracts/core/StrategyManager.sol:L643-L652  function _depositIntoStrategy(address depositor, IStrategy strategy, IERC20 token, uint256 amount)  internal  onlyStrategiesWhitelistedForDeposit(strategy)  returns (uint256 shares)  // transfer tokens from the sender to the strategy  token.safeTransferFrom(msg.sender, address(strategy), amount);  // deposit the assets into the specified strategy and get the equivalent amount of shares in that strategy  shares = strategy.deposit(token, amount);  src/contracts/strategies/StrategyBase.sol:L69-L99  function deposit(IERC20 token, uint256 amount)  external  virtual  override  onlyWhenNotPaused(PAUSED_DEPOSITS)  onlyStrategyManager  returns (uint256 newShares)  require(token == underlyingToken, \"StrategyBase.deposit: Can only deposit underlyingToken\");  /**  @notice calculation of newShares *mirrors* `underlyingToShares(amount)`, but is different since the balance of `underlyingToken`  has already been increased due to the `strategyManager` transferring tokens to this strategy prior to calling this function  /  uint256 priorTokenBalance = _tokenBalance() - amount;  if (priorTokenBalance == 0 || totalShares == 0) {  newShares = amount;  } else {  newShares = (amount * totalShares) / priorTokenBalance;  // checks to ensure correctness / avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(newShares != 0, \"StrategyBase.deposit: newShares cannot be zero\");  uint256 updatedTotalShares = totalShares + newShares;  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES,  \"StrategyBase.deposit: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  // update total share amount  totalShares = updatedTotalShares;  return newShares;  That means if there is a callback in the token s transferFrom function and it is executed after the balance change, a reentering call to sharesToUnderlyingView (for example) will again return a wrong result because shares and token balances are not  in sync.   In addition to the reversed order of token transfer and shares update, there s another vital difference between withdraw and deposit: For withdrawals, the call to the token contract originates in the strategy, while for deposits, it is the strategy manager that initiates the call to the token contract (before calling into the strategy). That s a technicality that has consequences for reentrancy protection: Note that for withdrawals, it is the strategy contract that is reentered, while for deposits, there is not a single contract that is reentered; instead, it is the contract system that is in an inconsistent state when the reentrancy happens. Hence, reentrancy protection on the level of individual contracts is not sufficient.  src/contracts/core/StrategyManager.sol:L244-L286  function depositIntoStrategyWithSignature(  IStrategy strategy,  IERC20 token,  uint256 amount,  address staker,  uint256 expiry,  bytes memory signature  external  onlyWhenNotPaused(PAUSED_DEPOSITS)  onlyNotFrozen(staker)  nonReentrant  returns (uint256 shares)  require(  expiry >= block.timestamp,  \"StrategyManager.depositIntoStrategyWithSignature: signature expired\"  );  // calculate struct hash, then increment `staker`'s nonce  uint256 nonce = nonces[staker];  bytes32 structHash = keccak256(abi.encode(DEPOSIT_TYPEHASH, strategy, token, amount, nonce, expiry));  unchecked {  nonces[staker] = nonce + 1;  bytes32 digestHash = keccak256(abi.encodePacked(\"\\x19\\x01\", DOMAIN_SEPARATOR, structHash));  /**  check validity of signature:  1) if `staker` is an EOA, then `signature` must be a valid ECSDA signature from `staker`,  indicating their intention for this action  2) if `staker` is a contract, then `signature` must will be checked according to EIP-1271  /  if (Address.isContract(staker)) {  require(IERC1271(staker).isValidSignature(digestHash, signature) == ERC1271_MAGICVALUE,  \"StrategyManager.depositIntoStrategyWithSignature: ERC1271 signature verification failed\");  } else {  require(ECDSA.recover(digestHash, signature) == staker,  \"StrategyManager.depositIntoStrategyWithSignature: signature not from staker\");  shares = _depositIntoStrategy(staker, strategy, token, amount);  Hence, querying the staker s nonce in reentrancy would still give a result based on an  incomplete state change.  It is, for example, conceivable that the staker still has zero shares, and yet their nonce is already 1. This particular situation is most likely not an issue, but the example shows that reentrancy can be subtle.  Recommendation  This is fine if the token doesn t allow reentrancy in the first place. As discussed above, among the tokens that do allow reentrancy, some variants of when reentrancy can happen in relation to state changes in the token seem more dangerous than others, but we have also argued that this kind of reasoning can be dangerous and error-prone. Hence, we recommend employing comprehensive and defensive reentrancy protection based on reentrancy guards such as OpenZeppelin s ReentrancyGuardUpgradeable, which is already used in the StrategyManager.  Unfortunately, securing a multi-contract system against reentrancy can be challenging, but we hope the preceding discussion and the following pointers will prove helpful:  External functions in strategies that should only be callable by the strategy manager (such as deposit and withdraw) should have the onlyStrategyManager modifier. This is already the case in the current codebase and is listed here only for completeness.  External functions in strategies for which item 1 doesn t apply (such as sharesToUnderlying and underlyingToShares) should query the strategy manager s reentrancy lock and revert if it is set.  In principle, the restrictions above also apply to public functions, but if a public function is also used internally, checks against reentrancy can cause problems (if used in an internal context) or at least be redundant. In the context of reentrancy protection, it is often easier to split public functions into an internal and an external one.  If view functions are supposed to give reliable results (either internally   which is typically the case   or for other contracts), they have to be protected too.  The previous item also applies to the StrategyManager: view functions that have to provide correct results should query the reentrancy lock and revert if it is set.  Solidity automatically generates getters for public state variables. Again, if these (external view) functions must deliver correct results, the same measures must be taken as for explicit view functions. In practice, the state variable has to become internal or private, and the getter function must be hand-written.  The StrategyBase contract provides some basic functionality. Concrete strategy implementations can inherit from this contract, meaning that some functions may be overridden (and might or might not call the overridden version via super), and new functions might be added. While the guidelines above should be helpful, derived contracts must be reviewed and assessed separately on a case-by-case basis. As mentioned before, reentrancy protection can be challenging, especially in a multi-contract system.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.2 StrategyBase   Inflation Attack Prevention Can Lead to Stuck Funds ", "body": "  Resolution  EigenLabs Quick Summary: The StrategyBase contract sets a minimum initial deposit amount of 1e9.  This is to mitigate ERC-4626 related inflation attacks, where an attacker can front-run a deposit, inflating the exchange rate between tokens and shares.  A consequence of that protection is that any amount less than 1e9 is not withdrawable.  EigenLabs Response: We recognize that this may be notable for tokens such as USDC, where the smallest unit of which is 1e-6.   For now, we will make it clear both in the contracts as well as the docs that our implementation of StrategyBase.sol makes this assumption about the tokens being used in the strategy.  Description  As a defense against what has come to be known as inflation or donation attack in the context of ERC-4626, the StrategyBase contract   from which concrete strategy implementations are supposed to inherit   enforces that the amount of shares in existence for a particular strategy is always either 0 or at least a certain minimum amount that is set to 10^9. This mitigates inflation attacks, which require a small total supply of shares to be effective.  src/contracts/strategies/StrategyBase.sol:L92-L95  uint256 updatedTotalShares = totalShares + newShares;  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES,  \"StrategyBase.deposit: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  src/contracts/strategies/StrategyBase.sol:L123-L127  // Calculate the value that `totalShares` will decrease to as a result of the withdrawal  uint256 updatedTotalShares = priorTotalShares - amountShares;  // check to avoid edge case where share rate can be massively inflated as a 'griefing' sort of attack  require(updatedTotalShares >= MIN_NONZERO_TOTAL_SHARES || updatedTotalShares == 0,  \"StrategyBase.withdraw: updated totalShares amount would be nonzero but below MIN_NONZERO_TOTAL_SHARES\");  This particular approach has the downside that, in the worst case, a user may be unable to withdraw the underlying asset for up to 10^9 - 1 shares. While the extreme circumstances under which this can happen might be unlikely to occur in a realistic setting and, in many cases, the value of 10^9 - 1 shares may be negligible, this is not ideal.  Recommendation  It isn t easy to give a good general recommendation. None of the suggested mitigations are without a downside, and what s the best choice may also depend on the specific situation. We do, however, feel that alternative approaches that can t lead to stuck funds might be worth considering, especially for a default implementation.  One option is internal accounting, i.e., the strategy keeps track of the number of underlying tokens it owns. It uses this number for conversion rate calculation instead of its balance in the token contract. This avoids the donation attack because sending tokens directly to the strategy will not affect the conversion rate. Moreover, this technique helps prevent reentrancy issues when the EigenLayer state is out of sync with the token contract s state. The downside is higher gas costs and that donating by just sending tokens to the contract is impossible; more specifically, if it happens accidentally, the funds are lost unless there s some special mechanism to recover them.  An alternative approach with virtual shares and assets is presented here, and the document lists pointers to more discussions and proposed solutions.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.3 StrategyWrapper   Functions Shouldn t Be virtual (Out of Scope) ", "body": "  Resolution  EigenLabs Quick Summary: The documentation assumes that StrategyWrapper.sol shouldn t be inheritable, and yet its functions are marked  virtual .  It should be noted that this contract was not audited beyond this issue which was noticed accidentally.  EigenLabs Response: This fix was acknowledged and fixed in the following commit: 053ee9d.  Description  The StrategyWrapper contract is a straightforward strategy implementation and   as its NatSpec documentation explicitly states   is not designed to be inherited from:  src/contracts/strategies/StrategyWrapper.sol:L8-L17  /**  @title Extremely simple implementation of `IStrategy` interface.  @author Layr Labs, Inc.  @notice Simple, basic, \"do-nothing\" Strategy that holds a single underlying token and returns it on withdrawals.  Assumes shares are always 1-to-1 with the underlyingToken.  @dev Unlike `StrategyBase`, this contract is *not* designed to be inherited from.  @dev This contract is expressly *not* intended for use with 'fee-on-transfer'-type tokens.  Setting the `underlyingToken` to be a fee-on-transfer token may result in improper accounting.  /  contract StrategyWrapper is IStrategy {  However, all functions in this contract are virtual, which only makes sense if inheriting from StrategyWrapper is possible.  Recommendation  Assuming the NatSpec documentation is correct, and no contract should inherit from StrategyWrapper, remove the virtual keyword from all function definitions. Otherwise, fix the documentation.  Remark  This contract is out of scope, and this finding is only included because we noticed it accidentally. This does not mean we have reviewed the contract or other out-of-scope files.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.4 StrategyBase   Inheritance-Related Issues ", "body": "  Resolution  60141d8.  Description  src/contracts/interfaces/IStrategy.sol:L39-L45  /**  @notice Used to convert an amount of underlying tokens to the equivalent amount of shares in this strategy.  @notice In contrast to `underlyingToSharesView`, this function **may** make state modifications  @param amountUnderlying is the amount of `underlyingToken` to calculate its conversion into strategy shares  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function underlyingToShares(uint256 amountUnderlying) external view returns (uint256);  src/contracts/strategies/StrategyBase.sol:L192-L200  /**  @notice Used to convert an amount of underlying tokens to the equivalent amount of shares in this strategy.  @notice In contrast to `underlyingToSharesView`, this function **may** make state modifications  @param amountUnderlying is the amount of `underlyingToken` to calculate its conversion into strategy shares  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function underlyingToShares(uint256 amountUnderlying) external view virtual returns (uint256) {  return underlyingToSharesView(amountUnderlying);  src/contracts/strategies/StrategyBase.sol:L167-L175  /**  @notice Used to convert a number of shares to the equivalent amount of underlying tokens for this strategy.  @notice In contrast to `sharesToUnderlyingView`, this function **may** make state modifications  @param amountShares is the amount of shares to calculate its conversion into the underlying token  @dev Implementation for these functions in particular may vary signifcantly for different strategies  /  function sharesToUnderlying(uint256 amountShares) public view virtual override returns (uint256) {  return sharesToUnderlyingView(amountShares);  B. The initialize function in the StrategyBase contract is not virtual, which means the name will not be available in derived contracts (unless with different parameter types). It also has the initializer modifier, which is unavailable in concrete strategies inherited from StrategyBase.  Recommendation  A. If state-changing versions of the conversion functions are needed, the view modifier has to be removed from IStrategy.underlyingToShares, StrategyBase.underlyingToShares, and StrategyBase.sharesToUnderlying. They should be removed entirely from the interface and base contract if they re not needed.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.5 StrategyManager - Cross-Chain Replay Attacks After Chain Split  Due to Hard-Coded DOMAIN_SEPARATOR ", "body": "  Resolution  EigenLabs Quick Summary: A. For the implementation of EIP-712 signatures, the domain separator is set in the initialization of the contract, which includes the chain ID. In the case of a chain split, this ID is subject to change.  Thus the domain separator must be recomputed. B. The domain separator is calculated using bytes(\"EigenLayer\")   the EIP-712 spec requires a keccak256 hash, i.e. keccak256(bytes(\"EigenLayer\")). C. The EIP712Domain does not include a version string.  EigenLabs Response: A. We have modified our implementation to dynamically check for the chain ID.  If we detect a change since initialization, we recompute the domain separator.  If not, we use the precomputed value. B. We changed our computation to use keccak256(bytes(\"EigenLayer\")). C. We decided that we would forgo this change for the time being. Changes in A. and B. implemented in this commit: 714dbb6.  Description  A. The StrategyManager contract allows stakers to deposit into and withdraw from strategies. A staker can either deposit themself or have someone else do it on their behalf, where the latter requires an EIP-712-compliant signature. The EIP-712 domain separator is computed in the initialize function and stored in a state variable for later retrieval:  src/contracts/core/StrategyManagerStorage.sol:L23-L24  /// @notice EIP-712 Domain separator  bytes32 public DOMAIN_SEPARATOR;  src/contracts/core/StrategyManager.sol:L149-L153  function initialize(address initialOwner, address initialStrategyWhitelister, IPauserRegistry _pauserRegistry, uint256 initialPausedStatus, uint256 _withdrawalDelayBlocks)  external  initializer  DOMAIN_SEPARATOR = keccak256(abi.encode(DOMAIN_TYPEHASH, bytes(\"EigenLayer\"), block.chainid, address(this)));  Once set in the initialize function, the value can t be changed anymore. In particular, the chain ID is  baked into  the DOMAIN_SEPARATOR during initialization. However, it is not necessarily constant: In the event of a chain split, only one of the resulting chains gets to keep the original chain ID, and the other should use a new one. With the current approach to compute the DOMAIN_SEPARATOR during initialization, store it, and then use the stored value for signature verification, a signature will be valid on both chains after a split   but it should not be valid on the chain with the new ID. Hence, the domain separator should be computed dynamically.  B.  The name in the EIP712Domain is of type string:  src/contracts/core/StrategyManagerStorage.sol:L18-L19  bytes32 public constant DOMAIN_TYPEHASH =  keccak256(\"EIP712Domain(string name,uint256 chainId,address verifyingContract)\");  What s encoded when the domain separator is computed is bytes(\"EigenLayer\"):  src/contracts/core/StrategyManager.sol:L153  DOMAIN_SEPARATOR = keccak256(abi.encode(DOMAIN_TYPEHASH, bytes(\"EigenLayer\"), block.chainid, address(this)));  According to EIP-712,  The dynamic values bytes and string are encoded as a keccak256 hash of their contents.  Hence, bytes(\"EigenLayer\") should be replaced with keccak256(bytes(\"EigenLayer\")).  C. The EIP712Domain does not include a version string:  src/contracts/core/StrategyManagerStorage.sol:L18-L19  bytes32 public constant DOMAIN_TYPEHASH =  keccak256(\"EIP712Domain(string name,uint256 chainId,address verifyingContract)\");  That is allowed according to the specification. However, given that most, if not all, projects, as well as OpenZeppelin s EIP-712 implementation, do include a version string in their EIP712Domain, it might be a pragmatic choice to do the same, perhaps to avoid potential incompatibilities.  Recommendation  Individual recommendations have been given above. Alternatively, you might want to utilize OpenZeppelin s EIP712Upgradeable library, which will take care of these issues. Note that some of these changes will break existing signatures.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.6 StrategyManagerStorage   Miscalculated Gap Size ", "body": "  Resolution  EigenLabs Quick Summary: Gap size in StrategyManagerStorage is set to 41 even though 10 slots are utilized.  General convention is to have 50 total slots available for storage.  EigenLabs Response: Storage gap size fixed, changed from 41 to 40.  This is possible as we aren t storing anything after the gap.  Commit hash: d249641.  Description  Upgradeable contracts should have a  gap  of unused storage slots at the end to allow for adding state variables when the contract is upgraded. The convention is to have a gap whose size adds up to 50 with the used slots at the beginning of the contract s storage.  In StrategyManagerStorage, the number of consecutively used storage slots is 10:  DOMAIN_SEPARATOR  nonces  strategyWhitelister  withdrawalDelayBlocks  stakerStrategyShares  stakerStrategyList  withdrawalRootPending  numWithdrawalsQueued  strategyIsWhitelistedForDeposit  beaconChainETHSharesToDecrementOnWithdrawal  However, the gap size in the storage contract is 41:  src/contracts/core/StrategyManagerStorage.sol:L84  uint256[41] private __gap;  Recommendation  If you don t have to maintain compatibility with an existing deployment, we recommend reducing the storage gap size to 40. Otherwise, we recommend adding a comment explaining that, in this particular case, the gap size and the used storage slots should add up to 51 instead of 50 and that this invariant has to be maintained in future versions of this contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.7 Unnecessary Usage of BytesLib ", "body": "  Resolution  EigenLabs Quick Summary: Several contracts import BytesLib.sol and yet do not use it.  6164a12.  Description  Various contracts throughout the codebase import BytesLib.sol without actually using it. It is only utilized once in EigenPod to convert a bytes array of length 32 to a bytes32.  src/contracts/pods/EigenPod.sol:L189-L190  require(validatorFields[BeaconChainProofs.VALIDATOR_WITHDRAWAL_CREDENTIALS_INDEX] == _podWithdrawalCredentials().toBytes32(0),  \"EigenPod.verifyCorrectWithdrawalCredentials: Proof is not for this EigenPod\");  However, this can also be achieved with an explicit conversion to bytes32, and means provided by the language itself should usually be preferred over external libraries.  Recommendation  Remove the import of BytesLib.sol and the accompanying using BytesLib for bytes; when the library is unused. Consider replacing the single usage in EigenPod with an explicit conversion to bytes32, and consequentially removing the import and using statements.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.8 DelayedWithdrawalRouter   Anyone Can Claim Earlier Than Intended on Behalf of Someone Else", "body": "  Resolution  EigenLabs Quick Summary: DelayedWithdrawalRouter implements a claiming function that allows for a provided recipient address. This function is not permissioned and thus can be called at any time (potentially a griefing attack vector with something like taxes).  EigenLabs Response: For now we have decided to leave this function as is and have added a note/warning about the noted behavior.  Description  The DelayedWithdrawalRouter has two functions to claim delayed withdrawals: one the recipient can call themself (i.e., the recipient is msg.sender) and one to claim on behalf of someone else (i.e., the recipient is given as a parameter):  src/contracts/pods/DelayedWithdrawalRouter.sol:L84-L94  /**  @notice Called in order to withdraw delayed withdrawals made to the caller that have passed the `withdrawalDelayBlocks` period.  @param maxNumberOfDelayedWithdrawalsToClaim Used to limit the maximum number of delayedWithdrawals to loop through claiming.  /  function claimDelayedWithdrawals(uint256 maxNumberOfDelayedWithdrawalsToClaim)  external  nonReentrant  onlyWhenNotPaused(PAUSED_DELAYED_WITHDRAWAL_CLAIMS)  _claimDelayedWithdrawals(msg.sender, maxNumberOfDelayedWithdrawalsToClaim);  src/contracts/pods/DelayedWithdrawalRouter.sol:L71-L82  /**  @notice Called in order to withdraw delayed withdrawals made to the `recipient` that have passed the `withdrawalDelayBlocks` period.  @param recipient The address to claim delayedWithdrawals for.  @param maxNumberOfDelayedWithdrawalsToClaim Used to limit the maximum number of delayedWithdrawals to loop through claiming.  /  function claimDelayedWithdrawals(address recipient, uint256 maxNumberOfDelayedWithdrawalsToClaim)  external  nonReentrant  onlyWhenNotPaused(PAUSED_DELAYED_WITHDRAWAL_CLAIMS)  _claimDelayedWithdrawals(recipient, maxNumberOfDelayedWithdrawalsToClaim);  An attacker can not control where the funds are sent, but they can control when the funds are sent once the withdrawal becomes claimable. It is unclear whether this can become a problem. It is, for example, conceivable that there are negative tax implications if funds arrive earlier than intended at a particular address: For instance, disadvantages for the recipient can arise if funds arrive before the new year starts or before some other funds were sent to the same address (e.g., in case the FIFO principle is applied for taxes).  The downside of allowing only the recipient to claim their withdrawals is that contract recipients must be equipped to make the claim.  Recommendation  If these points haven t been considered yet, we recommend doing so.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.9 Consider Using Custom Errors", "body": "  Resolution  EigenLabs Quick Summary: Suggestion to use custom errors in Solidity.  EigenLabs Response: For now we will continue to use standard error messages for the upcoming deployment.  Description and Recommendation  Custom errors were introduced in Solidity version 0.8.4 and have some advantages over the traditional string-based errors: They are usually more gas-efficient, especially regarding deployment costs, and it is easier to include dynamic information in error messages.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.10 StrategyManager - Immediate Settings Changes Can Have Unintended Side Effects", "body": "  Resolution  EigenLabs Quick Summary: StrategyManager contract allows for setWithdrawalDelayBlocks() to be called such that a user attempting to call completeQueuedWithdrawal() may be prevented from withdrawing.  EigenLabs Response: We have decided to leave this concern unaddressed.  Description  Withdrawal delay blocks can be changed immediately:  src/contracts/core/StrategyManager.sol:L570-L572  function setWithdrawalDelayBlocks(uint256 _withdrawalDelayBlocks) external onlyOwner {  _setWithdrawalDelayBlocks(_withdrawalDelayBlocks);  Allows owner to sandwich e.g. completeQueuedWithdrawal function calls to prevent users from withdrawing their stake due to the following check:  src/contracts/core/StrategyManager.sol:L749-L753  require(queuedWithdrawal.withdrawalStartBlock + withdrawalDelayBlocks <= block.number  || queuedWithdrawal.strategies[0] == beaconChainETHStrategy,  \"StrategyManager.completeQueuedWithdrawal: withdrawalDelayBlocks period has not yet passed\"  );  Recommendation  We recommend introducing a simple delay for settings changes to prevent sandwiching attack vectors. It is worth noting that this finding has been explicitly raised because it allows authorized personnel to target individual transactions and users by extension.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.11 StrategyManager - Unused Modifier", "body": "  Resolution  EigenLabs Quick Summary: StrategyManager contract defines an onlyEigenPod modifier which is not used.  EigenLabs Response: We have removed that modifier.  Commit hash: 50bb6a8.  Description  The StrategyManager contract defines an onlyEigenPod modifier, but it is never used.  src/contracts/core/StrategyManager.sol:L113-L116  modifier onlyEigenPod(address podOwner, address pod) {  require(address(eigenPodManager.getPod(podOwner)) == pod, \"StrategyManager.onlyEigenPod: not a pod\");  _;  Recommendation  The modifier can be removed.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.12 Inconsistent Data Types for Block Numbers", "body": "  Resolution  EigenLabs Quick Summary: Across our contracts we use both uint32 and uint64 to represent blockNumber values.  Description  The block number attribute is declared using uint32 and uint64. This usage should be more consistent.  Examples  uint32  src/contracts/interfaces/IEigenPod.sol:L30-L35  struct PartialWithdrawalClaim {  PARTIAL_WITHDRAWAL_CLAIM_STATUS status;  // block at which the PartialWithdrawalClaim was created  uint32 creationBlockNumber;  // last block (inclusive) in which the PartialWithdrawalClaim can be fraudproofed  uint32 fraudproofPeriodEndBlockNumber;  src/contracts/core/StrategyManager.sol:L393  withdrawalStartBlock: uint32(block.number),  src/contracts/pods/DelayedWithdrawalRouter.sol:L62-L65  DelayedWithdrawal memory delayedWithdrawal = DelayedWithdrawal({  amount: withdrawalAmount,  blockCreated: uint32(block.number)  });  uint64  src/contracts/pods/EigenPod.sol:L175-L176  function verifyWithdrawalCredentialsAndBalance(  uint64 oracleBlockNumber,  src/contracts/pods/EigenPodManager.sol:L216  function getBeaconChainStateRoot(uint64 blockNumber) external view returns(bytes32) {  Recommendation  Use one data type consistently to minimize the risk of conversion errors or truncation during casts. This is a measure aimed at future-proofing the code base.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "4.13 EigenPod   Stray nonReentrant Modifier", "body": "  Resolution  EigenLabs Quick Summary: There is a stray nonReentrant modifier in EigenPod.sol.  EigenLabs Response: Modifier has been removed.  Commit hash: 9f45837.  Description  There is a stray nonReentrant modifier in EigenPod:  src/contracts/pods/EigenPod.sol:L432-L453  /**  @notice Transfers `amountWei` in ether from this contract to the specified `recipient` address  @notice Called by EigenPodManager to withdrawBeaconChainETH that has been added to the EigenPod's balance due to a withdrawal from the beacon chain.  @dev Called during withdrawal or slashing.  @dev Note that this function is marked as non-reentrant to prevent the recipient calling back into it  /  function withdrawRestakedBeaconChainETH(  address recipient,  uint256 amountWei  external  onlyEigenPodManager  nonReentrant  // reduce the restakedExecutionLayerGwei  restakedExecutionLayerGwei -= uint64(amountWei / GWEI_TO_WEI);  emit RestakedBeaconChainETHWithdrawn(recipient, amountWei);  // transfer ETH from pod to `recipient`  _sendETH(recipient, amountWei);  src/contracts/pods/EigenPod.sol:L466-L468  function _sendETH(address recipient, uint256 amountWei) internal {  delayedWithdrawalRouter.createDelayedWithdrawal{value: amountWei}(podOwner, recipient);  This modifier is likely a leftover from an earlier version of the contract in which Ether was sent directly.  Recommendation  Remove the ineffective modifier for more readable code and reduced gas usage. The import of ReentrancyGuardUpgradeable.sol and the inheritance from ReentrancyGuardUpgradeable can also be removed. Also, consider giving the _sendEth function a more appropriate name.  Remark  The two nonReentrant modifiers in DelayedWithdrawalRouter are neither strictly needed. Still, they look considerably less  random  than the one in EigenPod, and could be warranted by a defense-in-depth approach.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/03/eigenlabs-eigenlayer/"}, {"title": "3.1 addPremium   A Back Runner May Cause an Insurance Holder to Lose Their Refunds by Calling addPremium Right After the Original Call    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by returning with no-op in case  Description  addPremium is a public function that can be called by anyone and that distributes the weekly premium payments to the pool manager and the rest of the pool share holders. If the collateral deposited is not enough to cover the total coverage offered to insurance holders for a given week, refunds are allocated pro rata for all insurance holders of that particular week and policy. However, in the current implementation, attackers can call addPremium right after the original call to addPremium but before the call to refund; this will cause the insurance holders to lose their refunds, which will be effectively locked forever in the contract (unless the contract is upgraded).  Examples  contracts/Pool.sol:L313-L314  refundMap[policyIndex_][week] = incomeMap[policyIndex_][week].mul(  allCovered.sub(maximumToCover)).div(allCovered);  Recommendation  addPremium should contain a validation check in the beginning of the function that reverts for the case of incomeMap[policyIndex_][week] = 0.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.2 refund   Attacker Can Lock Insurance Holder s Refunds by Calling refund Before a Refund Was Allocated    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  Examples  contracts/Pool.sol:L341-L367  function refund(  uint256 policyIndex_,  uint256 week_,  address who_  ) external noReenter {  Coverage storage coverage = coverageMap[policyIndex_][week_][who_];  require(!coverage.refunded, \"Already refunded\");  uint256 allCovered = coveredMap[policyIndex_][week_];  uint256 amountToRefund = refundMap[policyIndex_][week_].mul(  coverage.amount).div(allCovered);  coverage.amount = coverage.amount.mul(  coverage.premium.sub(amountToRefund)).div(coverage.premium);  coverage.refunded = true;  IERC20(baseToken).safeTransfer(who_, amountToRefund);  if (eventAggregator != address(0)) {  IEventAggregator(eventAggregator).refund(  policyIndex_,  week_,  who_,  amountToRefund  );  Recommendation  There should be a validation check at the beginning of the function that reverts if  refundMap[policyIndex_][week_] == 0.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.3 addTidal, _updateUserTidal, withdrawTidal   Wrong Arithmetic Calculations    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  To further incentivize sellers, anyone   although it will usually be the pool manager   can send an arbitrary amount of the Tidal token to a pool, which is then supposed to be distributed proportionally among the share owners. There are several flaws in the calculations that implement this mechanism:  A. addTidal:  contracts/Pool.sol:L543-L544  poolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(  amount_.mul(SHARE_UNITS)).div(poolInfo.totalShare);  This should be:  poolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(  amount_.mul(SHARE_UNITS).div(poolInfo.totalShare));  Note the different parenthesization. Without SafeMath:  poolInfo.accTidalPerShare += amount_ * SHARE_UNITS / poolInfo.totalShare;  B. _updateUserTidal:  contracts/Pool.sol:L549-L550  uint256 accAmount = poolInfo.accTidalPerShare.add(  userInfo.share).div(SHARE_UNITS);  This should be:  uint256 accAmount = poolInfo.accTidalPerShare.mul(  userInfo.share).div(SHARE_UNITS);  Note that add has been replaced with mul.  Without SafeMath:  uint256 accAmount = poolInfo.accTidalPerShare * userInfo.share / SHARE_UNITS;  C. withdrawTidal:  contracts/Pool.sol:L568  uint256 accAmount = poolInfo.accTidalPerShare.add(userInfo.share);  As in B, this should be:  uint256 accAmount = poolInfo.accTidalPerShare.mul(  userInfo.share).div(SHARE_UNITS);  Note that add has been replaced with mul and that a division by SHARE_UNITS has been appended.  Without SafeMath:  uint256 accAmount = poolInfo.accTidalPerShare * userInfo.share / SHARE_UNITS;  As an additional minor point,  the division in addTidal will revert with a panic (0x12) if the number of shares in the pool is zero. This case could be handled more gracefully.  Recommendation  Implement the fixes described above. The versions without SafeMath are easier to read and should be preferred; see issue 3.13.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.4 claim   Incomplete and Lenient Implementation    ", "body": "  Resolution  Acknowledged but not fixed in this version. The client provided the following message:  No fix for this version. This one is not a bug in the code but is a missing feature on product logic. The product is good for release without a fix. We may implement related functions in the future.   Description  In the current version of the code, the claim function is lacking crucial input validation logic as well as required state changes. Most of the process is implemented in other contracts or off-chain at the moment and is therefore out of scope for this audit, but there might still be issues caused by potential errors in the process. Moreover, pool manager and committee together have unlimited ownership of the deposits and can essentially withdraw all collateral to any desired address.  Examples  contracts/Pool.sol:L588-L592  function claim(  uint256 policyIndex_,  uint256 amount_,  address receipient_  ) external onlyPoolManager {  Recommendation  To ensure a more secure claiming process, we propose adding the following logic to the claim function:  refund should be called at the beginning of the claim flow, so that the recipient s true coverage amount will be used.  policyIndex should be added as a parameter to this function, so that coverageMap can be used to validate that the amount claimed on behalf of a recipient is covered.  The payout amount should be subtracted in the coveredMap and coverageMap mappings.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.5 buy   Insurance Buyers Trying to Increase Their Coverage Amount Will Lose Their Previous Coverage    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  Examples  contracts/Pool.sol:L266-L280  for (uint256 w = fromWeek_; w < toWeek_; ++w) {  incomeMap[policyIndex_][w] =  incomeMap[policyIndex_][w].add(premium);  coveredMap[policyIndex_][w] =  coveredMap[policyIndex_][w].add(amount_);  require(coveredMap[policyIndex_][w] <= maximumToCover,  \"Not enough to buy\");  coverageMap[policyIndex_][w][_msgSender()] = Coverage({  amount: amount_,  premium: premium,  refunded: false  });  Recommendation  The coverage entry that represents the user s coverage should not be overwritten but should hold the accumulated amount of coverage instead.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.6 Several Issues Related to Upgradeability of Contracts    ", "body": "  Resolution                           Partially fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d as auditor s recommendations were implemented except from introducing  Description  We did not find a proxy contract or factory in the repository, but the README contains the following information:  README.md:L11  Every Pool is a standalone smart contract. It is made upgradeable with OpenZeppelin s Proxy Upgrade Pattern.  README.md:L56  And there will be multiple proxies and one implementation of the Pools, and one proxy and one implementation of EventAggregator.  There are several issues related to upgradeability or, generally, using the contracts as implementations for proxies. All recommendations in this report assume that it is not necessary to remain compatible with an existing deployment.  A. The Pool.sol file imports Initializable.sol from OpenZeppelin s contracts-upgradeable and several other files from their  regular  contracts package.  contracts/Pool.sol:L5-L10  import \"@openzeppelin/contracts-upgradeable/proxy/utils/Initializable.sol\";  import \"@openzeppelin/contracts/utils/Context.sol\";  import \"@openzeppelin/contracts/utils/math/SafeMath.sol\";  import \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";  import \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";  These two should not be mixed, and in an upgradeable context, all files should be imported from contracts-upgradeable. Note that the import of Ownable.sol in NonReentrancy.sol can be removed completely; see issue 3.20.  B. If upgradeability is supposed to work with inheritance, there should be dummy variables at the end of each contract in the inheritance hierarchy. Some of these have to be removed when  real  state variables are added. More precisely, it is conventional to use a fixed-size uint256 array __gap, such that the consecutively occupied slots at the beginning (for the  real  state variables) add up to 50 with the size of the array. If state variables are added later, the gap s size has to be reduced accordingly to maintain this invariant. Currently, the contracts do not declare such a __gap variable.  C. Implementation contracts should not remain uninitalized. To prevent initialization by an attacker   which, in some cases, can have an impact on the proxy   the implementation contract s constructor should call _disableInitializers.  Recommendation  Refamiliarize yourself with the subtleties and pitfalls of upgradeable contracts, in particular regarding state variables and the storage gap. A lot of useful information can be found here.  Only import from contracts-upgradeable, not from contracts.  Add appropriately-sized storage gaps at least to PoolModel, NonReentrancy, and EventAggregator. (Note that adding a storage gap to NonReentrancy will break compatibility with existing deployments.) Ideally, add comments and warnings to each file that state variables may only be added at the end, that the storage gap s size has to be reduced accordingly, and that state variables must not be removed, rearranged, or in any way altered (e.g., type, constant, immutable). No state variables should ever be added to the Pool contract, and a comment should make that clear.  Add a constructor to Pool and EventAggregator that calls _disableInitializers.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.7 initialize   Committee Members Array Can Contain Duplicates    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing a nested loop to de-duplicate committee members.  Description  The initial committee members are given as array argument to the pool s initialize function. When the array is processed, there is no check for duplicates, and duplicates may also end up in the storage array committeeArray.  contracts/Pool.sol:L43-L47  for (uint256 i = 0; i < committeeMembers_.length; ++i) {  address member = committeeMembers_[i];  committeeArray.push(member);  committeeIndexPlusOne[member] = committeeArray.length;  Duplicates will result in a discrepancy between the length of the array   which is later interpreted as the number of committee members   and the actual number of (different) committee members. This could lead to more problems, such as an insufficient committee size to reach the threshold.  Recommendation  The initialize function should verify in the loop that member hasn t been added before. Note that _executeAddToCommittee refuses to add someone who is already in the committee, and the same technique can be employed here.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.8 addPolicy, setPolicy   Missing Input Validation    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description and Recommendation  Both addPolicy and setPolicy are missing essential input validation on two main parameters:  collateralRatio_   Should be validated to be non-zero, and it might be worth adding a range check.  weeklyPremium_   Should be less than RATIO_BASE at least, and it might be worth adding a maximum value check.  Examples  contracts/Pool.sol:L159  function addPolicy(  contracts/Pool.sol:L143  function setPolicy(  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.9 Pool.buy  Users May End Up Paying More Than Intended Due to Changes in policy.weeklyPremium    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  The price that an insurance buyer has to pay for insurance is determined by the duration of the coverage and the weeklyPremium. The price increases as the weeklyPremium increases. If a buy transaction is waiting in the mempool but eventually front-run by another transaction that increases weeklyPremium, the user will end up paying more than they anticipated for the same insurance coverage (assuming their allowance to the Pool contract is unlimited or at least higher than what they expected to pay).  Examples  contracts/Pool.sol:L273-L274  uint256 premium = amount_.mul(policy.weeklyPremium).div(RATIO_BASE);  uint256 allPremium = premium.mul(toWeek_.sub(fromWeek_));  Recommendation  Consider adding a parameter for the maximum amount to pay, and make sure that the transaction will revert if allPremium is greater than this maximum value.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.10 Missing Validation Checks in execute    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendations.  Description  While some validation checks are implemented in the proposal phase, this is not enough to ensure that business logic rules around these changes are completely enforced.  _executeRemoveFromCommittee   While the removeFromCommittee function makes sure that committeeArray.length > committeeThreshold, i.e., that there should always be enough committee members to reach the threshold, the same validation check is not enforced in _executeRemoveFromCommittee. To better illustrate the issue, let s consider the following example: committeeArray.length = 5, committeeThreshold = 4, and now removeFromCommittee is called two times in a row, where the second call is made before the first call reaches the threshold. In this case, both requests will be executed successfully, and we end up with committeeArray.length = 3 and committeeThreshold = 4, which is clearly not desired.  _executeChangeCommitteeThreshold   Applying the same concept here, this function lacks the validation check of threshold_ <= committeeArray.length, leading to the same issue as above. Let s consider the following example: committeeArray.length = 3, committeeThreshold = 2, and now changeCommitteeThresholdis called with threshold_ = 3, but before this request is executed, removeFromCommittee is called. After both requests have been executed successfully, we will end up with committeeThreshold = 3 and committeeArray.length = 2, which is clearly not desired.  Examples  contracts/Pool.sol:L783  function _executeRemoveFromCommittee(address who_) private {  contracts/Pool.sol:L796  function _executeChangeCommitteeThreshold(uint256 threshold_) private {  Recommendation  Apply the same validation checks in the functions that execute the state change.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.11 Reentrancy Concerns    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendations.  Description and Recommendation  A. All external functions in the Pool contract that make calls to the base or the Tidal token   and only these   have a noReenter modifier. That means that it is not possible to reenter the contract through these functions, but it could still be possible to reenter the pool through a different external or public function that does not have such a modifier. Assuming the token contract allows reentrancy, the following could happen, for instance:  Alice calls withdrawReady.  During the call to the token contract, Alice gets control of execution through a callback.  She reenters the pool contract through the withdraw function.  Note that, at this point, userInfo.pendingWithdrawShare has a  wrong  value because we left the Pool contract before this state variable was updated. So the reentering call is operating on inconsistent state.  We didn t find a way to cause actual harm through this or similar reentrancies, but to rely on this kind of reasoning is dangerous, and there s always the risk to miss something. It is, therefore, recommended to add a noReenter modifier to all state-changing external functions, in particular the ones operating with shares.  B. A second concern is reentrancy through view functions. In the example above, note that when we leave the pool contract, it is not only userInfo.pendingWithdrawShare that hasn t been updated yet, it is also poolInfo.pendingWithdrawShare. Hence, if we call, for example, getAvailableCapacity in step number 3, we will get a wrong result.  If this or other view functions are supposed to give reliable results under all circumstances, they should revert if islocked is true. (This state variable is currently private and not accessible in the derived contract Pool, so a small change has to be made in the NonReentrancy contract, too.)  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.12 Hard-Coded Minimum Deposit Amount    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  The deposit function specifies a minimum amount of 1e12 units of the base token for a deposit:  contracts/Pool.sol:L22  uint256 constant AMOUNT_PER_SHARE = 1e18;  contracts/Pool.sol:L369-L376  // Anyone can be a seller, and deposit baseToken (e.g. USDC or WETH)  // to the pool.  function deposit(  uint256 amount_  ) external noReenter {  require(enabled, \"Not enabled\");  require(amount_ >= AMOUNT_PER_SHARE / 1000000, \"Less than minimum\");  Whether that s an appropriate minimum amount or not depends on the base  token. Note that the two example tokens listed above are USDC and WETH. With current ETH prices, 1e12 Wei cost an affordable 0.2 US Cent. USDC, on the other hand, has 6 decimals, so 1e12 units are worth 1 million USD, which is \u2026 steep.  Recommendation  The minimum deposit amount should be configurable.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.13 Unnecessary Use of SafeMath Library    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  Since Solidity v0.8.0, all arithmetic operations are checked by default and revert on over- or underflow. Hence, it is not necessary anymore to use the SafeMath library (or SafeMathUpgradeable). Employing it nonetheless not only wastes gas but also reduces the readability of arithmetic expressions considerably.  Examples  The assignment  poolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(amount_.mul(SHARE_UNITS).div(poolInfo.totalShare));  is a lot easier to read without SafeMath:  poolInfo.accTidalPerShare += amount_ * SHARE_UNITS / poolInfo.totalShare;  See also issue 3.3.  Recommendation  We recommend using the built-in arithmetic operations instead of SafeMath.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.14 Outdated Solidity Version    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  The source files  version pragmas either specify that they need compiler version exactly 0.8.10 or at least 0.8.10:  contracts/Pool.sol:L2  pragma solidity 0.8.10;  contracts/helper/EventAggregator.sol:L2  pragma solidity ^0.8.10;  Solidity v0.8.10 is a fairly dated version that has known security issues. We generally recommend using the latest version of the compiler (at the time of writing, this is v0.8.20), and we also discourage the use of floating pragmas to make sure that the source files are actually compiled and deployed with the same compiler version they have been tested with.  Recommendation  Use the Solidity compiler v0.8.20, and change the version pragma in all Solidity source files to pragma solidity 0.8.20;.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.15 Code Used for Testing Purposes Should Be Removed Before Deployment    ", "body": "  Resolution                           Fixed in   49d6afd6abc463dd3fde3df0df715c475bb3e013 by implementing the auditor s recommendation.  Description  Variables and logic have been added to the code whose only purpose is to make it easier to test. This might cause unexpected behavior if deployed in production. For instance, onlyTest and setTimeExtra should be removed from the code before deployment, as well as timeExtra in getCurrentWeek and getNow.  Examples  contracts/Pool.sol:L55  modifier onlyTest() {  contracts/Pool.sol:L67  function setTimeExtra(uint256 timeExtra_) external onlyTest {  contracts/Pool.sol:L71-L73  function getCurrentWeek() public view returns(uint256) {  return (block.timestamp + TIME_OFFSET + timeExtra) / (7 days);  contracts/Pool.sol:L75-L77  function getNow() public view returns(uint256) {  return block.timestamp + timeExtra;  Recommendation  For the long term, consider mimicking this behavior by using features offered by your testing framework.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.16 Missing Events    ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendations.  Description  Some state-changing functions do not emit an event at all or omit relevant information.  Examples  A. Pool.setEventAggregator should emit an event with the value of eventAggregator_ so that off-chain services will be notified and can automatically adjust.  contracts/Pool.sol:L93-L95  function setEventAggregator(address eventAggregator_) external onlyPoolManager {  eventAggregator = eventAggregator_;  B. Pool.enablePool should emit an event when the pool is dis- or enabled.  contracts/Pool.sol:L581-L583  function enablePool(bool enabled_) external onlyPoolManager {  enabled = enabled_;  C. Pool.execute only logs the requestIndex_ while it should also include the operation and  data to better reflect the state change in the transaction.  contracts/Pool.sol:L756-L760  if (eventAggregator != address(0)) {  IEventAggregator(eventAggregator).execute(  requestIndex_  );  Recommendation  State-changing functions should emit an event to have an audit trail and enable monitoring of smart contract usage.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.17 CommitteeRequest, WithdrawRequest - Should Use an enum Type   ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description and Recommendation  contracts/model/PoolModel.sol:L99-L104  struct CommitteeRequest {  uint256 time;  uint256 vote;  bool executed;  uint8 operation;  bytes data;  Developers have to remember or look up which number denotes which operation:  contracts/Pool.sol:L738-L754  if (cr.operation == 0) {  (uint256 amount, address receipient) = abi.decode(  cr.data, (uint256, address));  _executeClaim(amount, receipient);  } else if (cr.operation == 1) {  address poolManager = abi.decode(cr.data, (address));  _executeChangePoolManager(poolManager);  } else if (cr.operation == 2) {  address newMember = abi.decode(cr.data, (address));  _executeAddToCommittee(newMember);  } else if (cr.operation == 3) {  address oldMember = abi.decode(cr.data, (address));  _executeRemoveFromCommittee(oldMember);  } else if (cr.operation == 4) {  uint256 threshold = abi.decode(cr.data, (uint256));  _executeChangeCommitteeThreshold(threshold);  This is error-prone and tedious. An enum type is a safer and more convenient way to encode the different operations. In fact, this is a textbook scenario for employing  an enum, and we recommend doing so.  B. Withdrawal requests are first created with the withdraw function. After withdrawWaitWeeks1 weeks, they can be advanced to a  pending  status by calling withdrawPending. Finally, after another withdrawWaitWeeks2 weeks, the request can be executed via withdrawReady.  This is currently implemented via two boolean members in the WithdrawRequest struct, pending and executed:  contracts/model/PoolModel.sol:L72-L78  struct WithdrawRequest {  uint256 share;  uint256 time;  bool pending;  bool executed;  bool succeeded;  An object transitioning through a series of states is another excellent use case for enums. In this example, the state could be  modeled with an enum as follows: enum Status { Created, Pending, Executed }. This approach has several advantages compared to the implementation with two boolean variables:  It uses only one variable, instead of two. In particular, setting and querying the state only involves one variable.  It can be easily extended to more states without introducing additional variables.  The object can never be in more than one state at once or in an undefined state. (With the current implementation, it would be possible to have pending == false and executed == true.)  Remark  It is often a good idea to have something like  None  or  NonExistent  as first value in the enum. That makes it easy to distinguish  real  objects from unchanged storage, as in:  Here is no object.  In the two examples above, that is not necessary, but it wouldn t hurt either.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.18 No NatSpec Annotations", "body": "  Description  NatSpec is the de facto standard for the annotation of Solidity files. To quote the Solidity documentation:  It is recommended that Solidity contracts are fully annotated using NatSpec for all public interfaces (everything in the ABI).  The Tidal codebase does not use NatSpec, and there s not a lot of documentation and comments in general.  Recommendation  Use NatSpec documentation and follow the advice in the quote.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.19 vote - Voting  No  Has No Effect   ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  Committee members can vote on proposals with either  yes  or  no . Voting  no  has no effect at all, i.e., there is no state change or event emitted, no return value, etc.  contracts/Pool.sol:L695-L701  function vote(  uint256 requestIndex_,  bool support_  ) external onlyCommittee {  if (!support_) {  return;  This means voting with  no  is pointless, and the option to do so could be removed completely.  Recommendation  Consider removing the bool support_ parameter from the vote function, such that calling vote is always a  yes  vote. Maybe rename the function to make this more explicit.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.20 Unused Import   ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  The file NonReentrancy.sol imports Ownable.sol, but this import is not used.  contracts/common/NonReentrancy.sol:L4  import \"@openzeppelin/contracts/access/Ownable.sol\";  Recommendation  Remove the unnecessary import.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.21 Unnecessary and Outdated Pragma Directive   ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description  The Pool.sol source file uses the pragma directive pragma experimental ABIEncoderV2;:  contracts/Pool.sol:L3  pragma experimental ABIEncoderV2;  ABI coder V2 is the default since Solidity v0.8.0 and is considered non-experimental as of Solidity v0.6.0. Hence, this directive is not necessary and even a bit misleading because the  experimental  status was removed long ago.  Recommendation  This line can be removed. If you want to be explicit for some reason, it should be replaced with pragma abicoder v2;.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "3.22 vote Could Call execute When committeeThreshold Is Reached   ", "body": "  Resolution                           Fixed in   3bbafab926df0ea39f444ef0fd5d2a6197f99a5d by implementing the auditor s recommendation.  Description and Recommendation  In the current version of the code, an additional transaction to execute is needed in case the threshold was reached for a specific request. Instead, execute could be invoked as part of vote when the threshold is reached.  contracts/Pool.sol:L714  cr.vote = cr.vote.add(1);  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/05/tidal/"}, {"title": "4.1 FlasherFTM - Unsolicited invocation of the callback (CREAM auth bypass) ", "body": "  Description  TL;DR: Anyone can call ICTokenFlashloan(crToken).flashLoan(address(FlasherFTM), address(FlasherFTM), info.amount, params) directly and pass validation checks in onFlashLoan(). This call forces it to accept unsolicited flash loans and execute the actions provided under the attacker s FlashLoan.Info.  receiver.onFlashLoan(initiator, token, amount, ...) is called when receiving a flash loan. According to EIP-3156, the initiator is msg.sender so that one can use it to check if the call to receiver.onFlashLoan() was unsolicited or not.  Third-party Flash Loan provider contracts are often upgradeable.  For example, the Geist lending contract configured with this system is upgradeable. Upgradeable contracts bear the risk that one cannot assume that the contract is always running the same code. In the worst case, for example, a malicious proxy admin (leaked keys, insider, \u2026) could upgrade the contract and perform unsolicited calls with arbitrary data to Flash Loan consumers in an attempt to exploit them. It, therefore, is highly recommended to verify that flash loan callbacks in the system can only be called if the contract was calling out to the provider to provide a Flash Loan and that the conditions of the flash loan (returned data, amount) are correct.  Not all Flash Loan providers implement EIP-3156 correctly.  Cream Finance, for example, allows users to set an arbitrary initiator when requesting a flash loan. This deviates from EIP-3156 and was reported to the Cream development team as a security issue. Hence, anyone can spoof that initiator and potentially bypass authentication checks in the consumers  receiver.onFlashLoan(). Depending on the third-party application consuming the flash loan is doing with the funds, the impact might range from medium to critical with funds at risk. For example, projects might assume that the flash loan always originates from their trusted components, e.g., because they use them to refinance switching funds between pools or protocols.  Examples  The FlasherFTM contract assumes that flash loans for the Flasher can only be initiated by authorized callers (isAuthorized) - for a reason - because it is vital that the FlashLoan.Info calldata info parameter only contains trusted data:  code/contracts/fantom/flashloans/FlasherFTM.sol:L66-L79  /**  @dev Routing Function for Flashloan Provider  @param info: struct information for flashLoan  @param _flashnum: integer identifier of flashloan provider  /  function initiateFlashloan(FlashLoan.Info calldata info, uint8 _flashnum) external isAuthorized override {  if (_flashnum == 0) {  _initiateGeistFlashLoan(info);  } else if (_flashnum == 2) {  _initiateCreamFlashLoan(info);  } else {  revert(Errors.VL_INVALID_FLASH_NUMBER);  code/contracts/fantom/flashloans/FlasherFTM.sol:L46-L55  modifier isAuthorized() {  require(  msg.sender == _fujiAdmin.getController() ||  msg.sender == _fujiAdmin.getFliquidator() ||  msg.sender == owner(),  Errors.VL_NOT_AUTHORIZED  );  _;  The Cream Flash Loan initiation code requests the flash loan via ICTokenFlashloan(crToken).flashLoan(receiver=address(this), initiator=address(this), ...):  code/contracts/fantom/flashloans/FlasherFTM.sol:L144-L158  /**  @dev Initiates an CreamFinance flashloan.  @param info: data to be passed between functions executing flashloan logic  /  function _initiateCreamFlashLoan(FlashLoan.Info calldata info) internal {  address crToken = info.asset == _FTM  ? 0xd528697008aC67A21818751A5e3c58C8daE54696  : _crMappings.addressMapping(info.asset);  // Prepara data for flashloan execution  bytes memory params = abi.encode(info);  // Initialize Instance of Cream crLendingContract  ICTokenFlashloan(crToken).flashLoan(address(this), address(this), info.amount, params);  contracts/CCollateralCapErc20.sol:L187  address initiator,  code/contracts/fantom/flashloans/FlasherFTM.sol:L162-L175  Recommendation  /  function onFlashLoan(  address sender,  address underlying,  uint256 amount,  uint256 fee,  bytes calldata params  ) external override returns (bytes32) {  // Check Msg. Sender is crToken Lending Contract  // from IronBank because ETH on Cream cannot perform a flashloan  address crToken = underlying == _WFTM  ? 0xd528697008aC67A21818751A5e3c58C8daE54696  : _crMappings.addressMapping(underlying);  require(msg.sender == crToken && address(this) == sender, Errors.VL_NOT_AUTHORIZED);  Recommendation  Cream Finance  We ve reached out to the Cream developer team, who have confirmed the issue. They are planning to implement countermeasures. Our recommendation can be summarized as follows:  Implement the EIP-3156 compliant version of flashLoan() with initiator hardcoded to msg.sender.  FujiDAO (and other flash loan consumers)  We recommend not assuming that FlashLoan.Info contains trusted or even validated data when a third-party flash loan provider provides it! Developers should ensure that the data received was provided when the flash loan was requested.  The contract should reject unsolicited flash loans. In the scenario where a flash loan provider is exploited, the risk of an exploited trust relationship is less likely to spread to the rest of the system.  The Cream initiator provided to the onFlashLoan() callback cannot be trusted until the Cream developers fix this issue. The initiator can easily be spoofed to perform unsolicited flash loans. We, therefore, suggest:  Validate that the initiator value is the flashLoan() caller. This conforms to the standard and is hopefully how the Cream team is fixing this, and  Ensure the implementation tracks its own calls to flashLoan() in a state-variable semaphore, i.e. store the flash loan data/hash in a temporary state-variable that is only set just before calling flashLoan() until being called back in onFlashLoan(). The received data can then be verified against the stored artifact. This is a safe way of authenticating and verifying callbacks.  Values received from untrusted third parties should always be validated with the utmost scrutiny.  Smart contract upgrades are risky, so we recommend implementing the means to pause certain flash loan providers.  Ensure that flash loan handler functions should never re-enter the system. This provides additional security guarantees in case a flash loan provider gets breached.  Note: The Fuji development team implemented a hotfix to prevent unsolicited calls from Cream by storing the hash(FlashLoan.info) in a state variable just before requesting the flash loan. Inside the onFlashLoan callback, this state is validated and cleared accordingly.  An improvement to this hotfix would be, to check _paramsHash before any external calls are made and clear it right after validation at the beginning of the function. Additionally, hash==0x0 should be explicitly disallowed. By doing so, the check also serves as a reentrancy guard and helps further reduce the risk of a potentially malicious flash loan re-entering the function.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.2 Lack of reentrancy protection in token interactions ", "body": "  Description  Therefore, it is crucial to strictly adhere to the checks-effects pattern and safeguard affected methods using a mutex.  Examples  code/contracts/fantom/libraries/LibUniversalERC20FTM.sol:L26-L40  function univTransfer(  IERC20 token,  address payable to,  uint256 amount  ) internal {  if (amount > 0) {  if (isFTM(token)) {  (bool sent, ) = to.call{ value: amount }(\"\");  require(sent, \"Failed to send Ether\");  } else {  token.safeTransfer(to, amount);  withdraw is nonReentrant while paybackAndWithdraw is not, which appears to be inconsistent  code/contracts/fantom/FujiVaultFTM.sol:L172-L182  /**  @dev Paybacks the underlying asset and withdraws collateral in a single function call from activeProvider  @param _paybackAmount: amount of underlying asset to be payback, pass -1 to pay full amount  @param _collateralAmount: amount of collateral to be withdrawn, pass -1 to withdraw maximum amount  /  function paybackAndWithdraw(int256 _paybackAmount, int256 _collateralAmount) external payable {  updateF1155Balances();  _internalPayback(_paybackAmount);  _internalWithdraw(_collateralAmount);  code/contracts/fantom/FujiVaultFTM.sol:L232-L241  /**  @dev Paybacks Vault's type underlying to activeProvider - called by users  @param _repayAmount: token amount of underlying to repay, or  pass any 'negative number' to repay full ammount  Emits a {Repay} event.  /  function payback(int256 _repayAmount) public payable override {  updateF1155Balances();  _internalPayback(_repayAmount);  depositAndBorrow is not nonReentrant while borrow() is which appears to be inconsistent  code/contracts/fantom/FujiVaultFTM.sol:L161-L171  /**  @dev Deposits collateral and borrows underlying in a single function call from activeProvider  @param _collateralAmount: amount to be deposited  @param _borrowAmount: amount to be borrowed  /  function depositAndBorrow(uint256 _collateralAmount, uint256 _borrowAmount) external payable {  updateF1155Balances();  _internalDeposit(_collateralAmount);  _internalBorrow(_borrowAmount);  code/contracts/fantom/FujiVaultFTM.sol:L222-L230  /**  @dev Borrows Vault's type underlying amount from activeProvider  @param _borrowAmount: token amount of underlying to borrow  Emits a {Borrow} event.  /  function borrow(uint256 _borrowAmount) public override nonReentrant {  updateF1155Balances();  _internalBorrow(_borrowAmount);  depositAndBorrow  updateBalances  internalDeposit ->  ERC777(collateralAsset).safeTransferFrom()  ---> calls back!  ---callback:beforeTokenTransfer---->  !! depositAndBorrow  updateBalances  internalDeposit  --> ERC777.safeTransferFrom()  <--  _deposit  mint  internalBorrow  mint  _borrow  ERC777(borrowAsset).univTransfer(msg.sender) --> might call back  <-------------------------------  _deposit  mint  internalBorrow  mint  _borrow  --> ERC777(borrowAsset).univTransfer(msg.sender) --> might call back  <--  Recommendation  Consider decorating methods that may call back to untrusted sources (i.e., native token transfers, callback token operations) as nonReentrant and strictly follow the checks-effects pattern for all contracts in the code-base.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.3 Lack of segregation of duties, excessive owner permissions, misleading authentication modifiers ", "body": "  Descriptio  In the FujiERC1155 contract, the onlyPermit modifier should not include owner.  code/contracts/abstracts/fujiERC1155/F1155Manager.sol:L34-L37  modifier onlyPermit() {  require(addrPermit[_msgSender()] || msg.sender == owner(), Errors.VL_NOT_AUTHORIZED);  _;  However, the owner can also wholly mess up accounting as they are permitted to call updateState(), which should only be callable by vaults:  code/contracts/FujiERC1155.sol:L53-L59  function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit {  uint256 total = totalSupply(_assetID);  if (newBalance > 0 && total > 0 && newBalance > total) {  uint256 newIndex = (indexes[_assetID] * newBalance) / total;  indexes[_assetID] = uint128(newIndex);  The same is true for FujiERC1155.{mint|mintBatch|burn|burnBatch|addInitializeAsset} unless there is a reason to allow owner to freely burn/mint/initialize tokens and updateState for borrowed assets to arbitrary values.  FujiVault - owner is part of isAuthorized and can change the system out-of-band. controller does not implement means to call functions it has permissions to.  Multiple methods in FujiVault are decorated with the access control isAuthorized that grants the owner and the currently configured controller access. The controller, however, does not implement any means to call some of the methods on the Vault.  Furthermore, the owner is part of isAuthorized, too, and can switch out the debt-management token while one is already configured without any migration. This is likely to create an inconsistent state with the Vault, and no one will be able to withdraw their now non-existent token.  code/contracts/fantom/FujiVaultFTM.sol:L65-L74  /**  @dev Throws if caller is not the 'owner' or the '_controller' address stored in {FujiAdmin}  /  modifier isAuthorized() {  require(  msg.sender == owner() || msg.sender == _fujiAdmin.getController(),  Errors.VL_NOT_AUTHORIZED  );  _;  The owner can call methods  out of band,  bypassing steps the contract system would enforce otherwise, e.g. controller calling setActiveProvider.  It is assumed that setOracle, setFactor should probably be onlyOwner instead.  code/contracts/fantom/FujiVaultFTM.sol:L354-L367  function setFujiERC1155(address _fujiERC1155) external isAuthorized {  require(_fujiERC1155 != address(0), Errors.VL_ZERO_ADDR);  fujiERC1155 = _fujiERC1155;  vAssets.collateralID = IFujiERC1155(_fujiERC1155).addInitializeAsset(  IFujiERC1155.AssetType.collateralToken,  address(this)  );  vAssets.borrowID = IFujiERC1155(_fujiERC1155).addInitializeAsset(  IFujiERC1155.AssetType.debtToken,  address(this)  );  emit F1155Changed(_fujiERC1155);  Note ensure that setProviders can only ever be set by a trusted entity or multi-sig as the Vault delegatecalls the provider logic (via VaultControlUpgradeable) and, hence, the provider has total control over the Vault storage!  FliquidatorFTM - Unnecessary and confusing modifier FliquidatorFTM.isAuthorized  The contract is already Claimable; therefore, use the already existing modifier Claimable.onlyOwner instead.  code/contracts/fantom/FliquidatorFTM.sol:L86-L91  code/contracts/abstracts/claimable/Claimable.sol:L48-L51  /  modifier isAuthorized() {  require(msg.sender == owner(), Errors.VL_NOT_AUTHORIZED);  _;  code/contracts/abstracts/claimable/Claimable.sol:L48-L51  modifier onlyOwner() {  require(_msgSender() == owner(), \"Ownable: caller is not the owner\");  _;  Use Claimable.onlyOwner instead.  FlasherFTM - owner should not be able to call initiateFlashloan directly; misleading comment.  code/contracts/fantom/flashloans/FlasherFTM.sol:L42-L54  /**  @dev Throws if caller is not 'owner'.  /  modifier isAuthorized() {  require(  msg.sender == _fujiAdmin.getController() ||  msg.sender == _fujiAdmin.getFliquidator() ||  msg.sender == owner(),  Errors.VL_NOT_AUTHORIZED  );  _;  FujiERC1155 - All vaults have equal permission to mint/burn/initializeAssets for every vault  All vaults need to be in the onlyPermit ACL whitelist. No additional checks enforce that the calling vault can only modify its token balances. Furthermore, FujiVaultFTM is upgradeable; thus, the contract logic may be altered to allow the vault to modify any other token id s balance. To reduce this risk and the potential of an exploited contract affecting other token balances in the system, it is suggested to change the coarse onlyPermit ACL to one that checks that the calling vault can only manage their token IDs.  Recommendation  Reconsider the authentication concept and make it more transparent. Segregate duties and clearly define roles and capabilities. Avoid having overly powerful actors and reduce their capabilities to the bare minimum needed to segregate risk. If an actor is part of an ACL in a third-party contract, they also should have the means to call that method in a controlled way or else remove them from the ACL. To avoid conveying a false sense of trust towards certain actors within the smart contract system, it is suggested to use the centralized onlyOwner decorator for methods only the owner can call. This more accurately depicts  who can do what  in the system and makes it easier to trust the project team managing it.  Avoid excessively powerful owners that can change/mint/burn anything in the system as this is a risk for the general consistency.  Remove owner from methods/modifiers they don t need to be part of/have access to.  Ensure owner is a time-locked multi-sig or governance contract. Rename authentication modifiers to describe better what callers they allow.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.4 Unchecked Return Values - ICErc20 repayBorrow ", "body": "  Description  ICErc20.repayBorrow returns a non-zero uint on error. Multiple providers do not check for this error condition and might return success even though repayBorrow failed, returning an error code.  This can potentially allow a malicious user to call paybackAndWithdraw() while not repaying by causing an error in the sub-call to Compound.repayBorrow(), which ends up being silently ignored. Due to the missing success condition check, execution continues normally with _internalWithdraw().  Also, see issue 4.5.  code/contracts/interfaces/compound/ICErc20.sol:L11-L12  function repayBorrow(uint256 repayAmount) external returns (uint256);  The method may return an error due to multiple reasons:  contracts/CToken.sol:L808-L816  function repayBorrowInternal(uint repayAmount) internal nonReentrant returns (uint, uint) {  uint error = accrueInterest();  if (error != uint(Error.NO_ERROR)) {  // accrueInterest emits logs on errors, but we still want to log the fact that an attempted borrow failed  return (fail(Error(error), FailureInfo.REPAY_BORROW_ACCRUE_INTEREST_FAILED), 0);  // repayBorrowFresh emits repay-borrow-specific logs on errors, so we don't need to  return repayBorrowFresh(msg.sender, msg.sender, repayAmount);  contracts/CToken.sol:L855-L873  if (allowed != 0) {  return (failOpaque(Error.COMPTROLLER_REJECTION, FailureInfo.REPAY_BORROW_COMPTROLLER_REJECTION, allowed), 0);  /* Verify market's block number equals current block number */  if (accrualBlockNumber != getBlockNumber()) {  return (fail(Error.MARKET_NOT_FRESH, FailureInfo.REPAY_BORROW_FRESHNESS_CHECK), 0);  RepayBorrowLocalVars memory vars;  /* We remember the original borrowerIndex for verification purposes */  vars.borrowerIndex = accountBorrows[borrower].interestIndex;  /* We fetch the amount the borrower owes, with accumulated interest */  (vars.mathErr, vars.accountBorrows) = borrowBalanceStoredInternal(borrower);  if (vars.mathErr != MathError.NO_ERROR) {  return (failOpaque(Error.MATH_ERROR, FailureInfo.REPAY_BORROW_ACCUMULATED_BALANCE_CALCULATION_FAILED, uint(vars.mathErr)), 0);  Examples  Multiple providers, here are some examples:  code/contracts/fantom/providers/ProviderCream.sol:L168-L173  // Check there is enough balance to pay  require(erc20token.balanceOf(address(this)) >= _amount, \"Not-enough-token\");  erc20token.univApprove(address(cyTokenAddr), _amount);  cyToken.repayBorrow(_amount);  code/contracts/fantom/providers/ProviderScream.sol:L170-L172  require(erc20token.balanceOf(address(this)) >= _amount, \"Not-enough-token\");  erc20token.univApprove(address(cyTokenAddr), _amount);  cyToken.repayBorrow(_amount);  code/contracts/mainnet/providers/ProviderCompound.sol:L139-L155  if (_isETH(_asset)) {  // Create a reference to the corresponding cToken contract  ICEth cToken = ICEth(cTokenAddr);  cToken.repayBorrow{ value: msg.value }();  } else {  // Create reference to the ERC20 contract  IERC20 erc20token = IERC20(_asset);  // Create a reference to the corresponding cToken contract  ICErc20 cToken = ICErc20(cTokenAddr);  // Check there is enough balance to pay  require(erc20token.balanceOf(address(this)) >= _amount, \"Not-enough-token\");  erc20token.univApprove(address(cTokenAddr), _amount);  cToken.repayBorrow(_amount);  Recommendation  Check for cyToken.repayBorrow(_amount) != 0 or Error.NO_ERROR.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.5 Unchecked Return Values - IComptroller exitMarket, enterMarket ", "body": "  Description  IComptroller.exitMarket(), IComptroller.enterMarkets() may return a non-zero uint on error but none of the Providers check for this error condition. Together with issue 4.10, this might suggest that unchecked return values may be a systemic problem.  Here s the upstream implementation:  contracts/Comptroller.sol:L179-L187  if (amountOwed != 0) {  return fail(Error.NONZERO_BORROW_BALANCE, FailureInfo.EXIT_MARKET_BALANCE_OWED);  /* Fail if the sender is not permitted to redeem all of their tokens */  uint allowed = redeemAllowedInternal(cTokenAddress, msg.sender, tokensHeld);  if (allowed != 0) {  return failOpaque(Error.REJECTION, FailureInfo.EXIT_MARKET_REJECTION, allowed);  /**  @notice Removes asset from sender's account liquidity calculation  @dev Sender must not have an outstanding borrow balance in the asset,  or be providing necessary collateral for an outstanding borrow.  @param cTokenAddress The address of the asset to be removed  @return Whether or not the account successfully exited the market  /  function exitMarket(address cTokenAddress) external returns (uint) {  CToken cToken = CToken(cTokenAddress);  /* Get sender tokensHeld and amountOwed underlying from the cToken */  (uint oErr, uint tokensHeld, uint amountOwed, ) = cToken.getAccountSnapshot(msg.sender);  require(oErr == 0, \"exitMarket: getAccountSnapshot failed\"); // semi-opaque error code  /* Fail if the sender has a borrow balance */  if (amountOwed != 0) {  return fail(Error.NONZERO_BORROW_BALANCE, FailureInfo.EXIT_MARKET_BALANCE_OWED);  /* Fail if the sender is not permitted to redeem all of their tokens */  uint allowed = redeemAllowedInternal(cTokenAddress, msg.sender, tokensHeld);  if (allowed != 0) {  return failOpaque(Error.REJECTION, FailureInfo.EXIT_MARKET_REJECTION, allowed);  Examples  Unchecked return value exitMarket  All Providers exhibit the same issue, probably due to code reuse. (also see https://github.com/ConsenSysDiligence/fuji-protocol-audit-2022-02/issues/19). Some examples:  code/contracts/fantom/providers/ProviderCream.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  code/contracts/fantom/providers/ProviderScream.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  code/contracts/mainnet/providers/ProviderCompound.sol:L46-L51  function _exitCollatMarket(address _cTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cTokenAddress);  code/contracts/mainnet/providers/ProviderIronBank.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  Unchecked return value enterMarkets (Note that IComptroller returns NO_ERROR when already joined to enterMarkets.  All Providers exhibit the same issue, probably due to code reuse. (also see https://github.com/ConsenSysDiligence/fuji-protocol-audit-2022-02/issues/19). For example:  code/contracts/fantom/providers/ProviderCream.sol:L39-L46  function _enterCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  address[] memory cyTokenMarkets = new address[](1);  cyTokenMarkets[0] = _cyTokenAddress;  comptroller.enterMarkets(cyTokenMarkets);  Recommendation  Require that return value is ERROR.NO_ERROR or 0.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.6 Fliquidator - excess funds of native tokens are not returned ", "body": "  Description  Examples  code/contracts/fantom/FliquidatorFTM.sol:L148-L150  if (vAssets.borrowAsset == FTM) {  require(msg.value >= debtTotal, Errors.VL_AMOUNT_ERROR);  } else {  Recommendation  Consider returning excess funds. Consider making _constructParams public to allow the caller to pre-calculate the debtTotal that needs to be provided with the call.  Consider removing support for native token FTM entirely to reduce the overall code complexity. The wrapped equivalent can be used instead.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.7 Unsafe arithmetic casts ", "body": "  Description  The reason for using signed integers in some situations appears to be to use negative values as an indicator to withdraw everything. Using a whole bit of uint256 for this is quite a lot when using type(uint256).max would equal or better serve as a flag to withdraw everything.  Furthermore, even though the code uses solidity 0.8.x, which safeguards arithmetic operations against under/overflows, arithmetic typecast is not protected.  Also, see issue 4.9 for a related issue.  \u21d2  solidity-shell  \ud83d\ude80 Entering interactive Solidity ^0.8.11 shell. '.help' and '.exit' are your friends.  \u00bb  \u2139\ufe0f  ganache-mgr: starting temp. ganache instance ...  \u00bb  uint(int(-100))  115792089237316195423570985008687907853269984665640564039457584007913129639836  \u00bb  int256(uint(2**256-100))  100  Examples  code/contracts/fantom/FliquidatorFTM.sol:L167-L178  // Compute how much collateral needs to be swapt  uint256 collateralInPlay = _getCollateralInPlay(  vAssets.collateralAsset,  vAssets.borrowAsset,  debtTotal + bonus  );  // Burn f1155  _burnMulti(addrs, borrowBals, vAssets, _vault, f1155);  // Withdraw collateral  IVault(_vault).withdrawLiq(int256(collateralInPlay));  code/contracts/fantom/FliquidatorFTM.sol:L264-L276  // Compute how much collateral needs to be swapt for all liquidated users  uint256 collateralInPlay = _getCollateralInPlay(  vAssets.collateralAsset,  vAssets.borrowAsset,  _amount + _flashloanFee + bonus  );  // Burn f1155  _burnMulti(_addrs, _borrowBals, vAssets, _vault, f1155);  // Withdraw collateral  IVault(_vault).withdrawLiq(int256(collateralInPlay));  code/contracts/fantom/FliquidatorFTM.sol:L334-L334  uint256 amount = _amount < 0 ? debtTotal : uint256(_amount);  code/contracts/fantom/FujiVaultFTM.sol:L213-L220  function withdrawLiq(int256 _withdrawAmount) external override nonReentrant onlyFliquidator {  // Logic used when called by Fliquidator  _withdraw(uint256(_withdrawAmount), address(activeProvider));  IERC20Upgradeable(vAssets.collateralAsset).univTransfer(  payable(msg.sender),  uint256(_withdrawAmount)  );  pot. unsafe truncation (unlikely)  code/contracts/FujiERC1155.sol:L53-L59  function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit {  uint256 total = totalSupply(_assetID);  if (newBalance > 0 && total > 0 && newBalance > total) {  uint256 newIndex = (indexes[_assetID] * newBalance) / total;  indexes[_assetID] = uint128(newIndex);  Recommendation  If negative values are only used as a flag to indicate that all funds should be used for an operation, use type(uint256).max instead. It is wasting less value-space for a simple flag than using the uint256 high-bit range. Avoid typecast where possible. Use SafeCast instead or verify that the casts are safe because the values they operate on cannot under- or overflow. Add inline code comments if that s the case.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.8 Missing input validation on flash close fee factors ", "body": "  Description  The FliquidatorFTM contract allows authorized parties to set the flash close fee factor. The factor is provided as two integers denoting numerator and denominator. Due to a lack of boundary checks, it is possible to set unrealistically high factors, which go well above 1. This can have unexpected effects on internal accounting and the impact of flashloan balances.  Examples  code/contracts/fantom/FliquidatorFTM.sol:L657-L659  function setFlashCloseFee(uint64 _newFactorA, uint64 _newFactorB) external isAuthorized {  flashCloseF.a = _newFactorA;  flashCloseF.b = _newFactorB;  Recommendation  Add a requirement making sure that flashCloseF.a <= flashCloseF.b.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.9 Separation of concerns and consistency in vaults ", "body": "  Description  The FujiVaultFTM contract contains multiple balance-changing functions. Most notably, withdraw is passed an int256 denoted amount parameter. Negative values of this parameter are given to the _internalWithdraw function, where they trigger the withdrawal of all collateral. This approach can result in accounting mistakes in the future as beyond a certain point in the vault s accounting; amounts are expected to be only positive. Furthermore, the concerns of withdrawing and entirely withdrawing are not separated.  The above issue applies analogously to the payback function and its dependency on _internalPayback.  For consistency, withdrawLiq also takes an int256 amount parameter. This function is only accessible to the Fliquidator contract and withdraws collateral from the active provider. However, all occurrences of the _withdrawAmount parameter are cast to uint256.  Examples  The withdraw entry point:  code/contracts/fantom/FujiVaultFTM.sol:L201-L204  function withdraw(int256 _withdrawAmount) public override nonReentrant {  updateF1155Balances();  _internalWithdraw(_withdrawAmount);  _internalWithdraw s negative amount check:  code/contracts/fantom/FujiVaultFTM.sol:L654-L657  uint256 amountToWithdraw = _withdrawAmount < 0  ? providedCollateral - neededCollateral  : uint256(_withdrawAmount);  The withdrawLiq entry point for the Fliquidator:  code/contracts/fantom/FujiVaultFTM.sol:L213-L220  function withdrawLiq(int256 _withdrawAmount) external override nonReentrant onlyFliquidator {  // Logic used when called by Fliquidator  _withdraw(uint256(_withdrawAmount), address(activeProvider));  IERC20Upgradeable(vAssets.collateralAsset).univTransfer(  payable(msg.sender),  uint256(_withdrawAmount)  );  Recommendation  Similarly, withdrawLiq s parameter should be a uint256 to prevent unnecessary casts.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.10 Aave/Geist Interface declaration mismatch and unchecked return values ", "body": "  Description  The two lending providers, Geist & Aave, do not seem to be directly affiliated even though one is a fork of the other. However, the interfaces may likely diverge in the future. Using the same interface declaration for both protocols might become problematic with future upgrades to either protocol. The interface declaration does not seem to come from the original upstream project. The interface IAaveLendingPool does not declare any return values while some of the functions called in Geist or Aave return them.  Note: that we have not verified all interfaces for correctness. However, we urge the client to only use official interface declarations from the upstream projects and verify that all other interfaces match.  Examples  The ILendingPool configured in ProviderAave (0xB53C1a33016B2DC2fF3653530bfF1848a515c8c5 -> implementation: 0xc6845a5c768bf8d7681249f8927877efda425baf)  code/contracts/mainnet/providers/ProviderAave.sol:L19-L21  function _getAaveProvider() internal pure returns (IAaveLendingPoolProvider) {  return IAaveLendingPoolProvider(0xB53C1a33016B2DC2fF3653530bfF1848a515c8c5);  The IAaveLendingPool does not declare return values for any function, while upstream does.  code/contracts/interfaces/aave/IAaveLendingPool.sol:L1-L46  // SPDX-License-Identifier: MIT  pragma solidity ^0.8.0;  interface IAaveLendingPool {  function flashLoan(  address receiverAddress,  address[] calldata assets,  uint256[] calldata amounts,  uint256[] calldata modes,  address onBehalfOf,  bytes calldata params,  uint16 referralCode  ) external;  function deposit(  address _asset,  uint256 _amount,  address _onBehalfOf,  uint16 _referralCode  ) external;  function withdraw(  address _asset,  uint256 _amount,  address _to  ) external;  function borrow(  address _asset,  uint256 _amount,  uint256 _interestRateMode,  uint16 _referralCode,  address _onBehalfOf  ) external;  function repay(  address _asset,  uint256 _amount,  uint256 _rateMode,  address _onBehalfOf  ) external;  function setUserUseReserveAsCollateral(address _asset, bool _useAsCollateral) external;  Methods: withdraw(), repay() return uint256 in the original implementation for Aave, see:  https://etherscan.io/address/0xc6845a5c768bf8d7681249f8927877efda425baf#code  The ILendingPool configured for Geist:  Methods withdraw(), repay() return uint256 in the original implementation for Geist, see:  https://ftmscan.com/address/0x3104ad2aadb6fe9df166948a5e3a547004862f90#code  Note: that the actual amount withdrawn does not necessarily need to match the amount provided with the function argument. Here s an excerpt of the upstream LendingProvider.withdraw():  ...  if (amount == type(uint256).max) {  amountToWithdraw = userBalance;  ...  return amountToWithdraw;  And here s the code in Fuji that calls that method. This will break the withdrawAll functionality of LendingProvider if token isFTM.  code/contracts/fantom/providers/ProviderGeist.sol:L151-L165  function withdraw(address _asset, uint256 _amount) external payable override {  IAaveLendingPool aave = IAaveLendingPool(_getAaveProvider().getLendingPool());  bool isFtm = _asset == _getFtmAddr();  address _tokenAddr = isFtm ? _getWftmAddr() : _asset;  aave.withdraw(_tokenAddr, _amount, address(this));  // convert WFTM to FTM  if (isFtm)  {  address unwrapper = _getUnwrapper();  IERC20(_tokenAddr).univTransfer(payable(unwrapper), _amount);  IUnwrapper(unwrapper).withdraw(_amount);  Similar for repay(), which returns the actual amount repaid.  Recommendation  Always use the original interface unless only a minimal subset of functions is used.  Use the original upstream interfaces of the corresponding project (link via the respective npm packages if available).  Avoid omitting parts of the function declaration! Especially when it comes to return values.  Check return values. Use the value returned from withdraw() AND repay()  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.11 Missing slippage protection for rewards swap ", "body": "  Description  In FujiVaultFTM.harvestRewards a swap transaction is generated using a call to SwapperFTM.getSwapTransaction. In all relevant scenarios, this call uses a minimum output amount of zero, which de-facto deactivates slippage checks. Most values from harvesting rewards can thus be siphoned off by sandwiching such calls.  Examples  amountOutMin is 0, effectively disabling slippage control in the swap method.  code/contracts/fantom/SwapperFTM.sol:L49-L55  transaction.data = abi.encodeWithSelector(  IUniswapV2Router01.swapExactETHForTokens.selector,  0,  path,  msg.sender,  type(uint256).max  );  Only success required  code/contracts/fantom/FujiVaultFTM.sol:L565-L567  // Swap rewards -> collateralAsset  (success, ) = swapTransaction.to.call{ value: swapTransaction.value }(swapTransaction.data);  require(success, \"failed to swap rewards\");  Recommendation  Use a slippage check such as for liquidator swaps:  code/contracts/fantom/FliquidatorFTM.sol:L476-L479  require(  (priceDelta * SLIPPAGE_LIMIT_DENOMINATOR) / priceFromOracle < SLIPPAGE_LIMIT_NUMERATOR,  Errors.VL_SWAP_SLIPPAGE_LIMIT_EXCEED  );  Or specify a non-zero amountOutMin argument in calls to IUniswapV2Router01.swapExactETHForTokens.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.12 Unpredictable behavior due to admin front running or general bad timing ", "body": "  Description  In several cases, the owner of deployed contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, contract owners (a 2/3 EOA Gnosis Multisig) could use front running to make malicious changes just ahead of incoming transactions, or purely accidental adverse effects could occur due to unfortunate timing of changes.  Some instances of this are more important than others, but in general, users of the system should have assurances about the behavior of the action they re about to take.  Examples  FujiAdmin  The owner of FujiAdmin is 0x0e1484c9a9f9b31ff19300f082e843415a575f4f and this address is a proxy to a Gnosis Safe: Mastercopy 1.2.0 implementation, requiring 2/3 signatures to execute transactions. All three signees are EOA s.  code/artifacts/1-core.deploy:L958-L960  \"FujiAdmin\": {  \"address\": \"0x4cB46032e2790D8CA10be6d0001e8c6362a76adA\",  \"abi\": [  Controller, FujiOracle  The owner of controller seems to be a single EOA:  https://etherscan.io/address/0x3f366802F4e7576FC5DAA82890Cc6e04c85f3736#readContract  The owner of FujiOracle seems to be a single EOA:  https://etherscan.io/address/0xadF849079d415157CbBdb21BB7542b47077734A8#readContract  The owner of FujiERC1155 seems to be a single EOA:  https://etherscan.io/address/0xa2d62f8b02225fbFA1cf8bF206C8106bDF4c692b#readProxyContract  FujiAdmin (fantom)  Deployer is 0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148 which is an EOA.  code/artifacts/250-core.deploy:L1-L5  \"FujiAdmin\": {  \"address\": \"0xaAb2AAfBFf7419Ff85181d3A846bA9045803dd67\",  \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",  \"abi\": [  FujiAdmin.owner is 0x40578f7902304e0e34d7069fb487ee57f841342e which is a GnosisSafeProxy  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, all onlyOwner functionality requires two steps with a mandatory time window between them. The first step merely tells users that a particular change is coming, and the second step commits that change after a reasonable waiting period.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.13 FujiOracle - _getUSDPrice does not detect stale oracle prices; General Oracle Risks ", "body": "  Description  The external Chainlink oracle, which provides index price information to the system, introduces risk inherent to any dependency on third-party data sources. For example, the oracle could fall behind or otherwise fail to be maintained, resulting in outdated data being fed to the index price calculations. Oracle reliance has historically resulted in crippled on-chain systems, and complications that lead to these outcomes can arise from things as simple as network congestion.  This is more extreme in lesser-known tokens with fewer ChainLink Price feeds to update the price frequently.  Ensuring that unexpected oracle return values are correctly handled will reduce reliance on off-chain components and increase the resiliency of the smart contract system that depends on them.  The codebase, as is, relies on chainLinkOracle.latestRoundData() and does not check the timestamp or answeredIn round of the returned price.  Examples  Here s how the oracle is consumed, skipping any fields that would allow checking for stale data:  code/contracts/FujiOracle.sol:L66-L77  /**  @dev Calculates the USD price of asset.  @param _asset: the asset address.  Returns the USD price of the given asset  /  function _getUSDPrice(address _asset) internal view returns (uint256 price) {  require(usdPriceFeeds[_asset] != address(0), Errors.ORACLE_NONE_PRICE_FEED);  (, int256 latestPrice, , , ) = AggregatorV3Interface(usdPriceFeeds[_asset]).latestRoundData();  price = uint256(latestPrice);  Here s the implementation of the v0.6 FluxAggregator Chainlink feed with a note that timestamps should be checked.  contracts/src/v0.6/FluxAggregator.sol:L489-L490  Recommendation  @return updatedAt is the timestamp when the round last was updated (i.e.  answer was last computed)  Recommendation  Perform sanity checks on the price returned by the oracle. If the price is older, not within configured limits, revert or handle in other means.  The oracle does not provide any means to remove a potentially broken price-feed (e.g., by updating its address to address(0) or by pausing specific feeds or the complete oracle). The only way to pause an oracle right now is to deploy a new oracle contract. Therefore, consider adding minimally invasive functionality to pause the price-feeds if the oracle becomes unreliable.  Monitor the oracle data off-chain and intervene if it becomes unreliable.  On-chain, realistically, both answeredInRound and updatedAt must be checked within acceptable bounds.  answeredInRound == latestRound - in this case, data may be assumed to be fresh while it might not be because the feed was entirely abandoned by nodes (no one starting a new round). Also, there s a good chance that many feeds won t always be super up-to-date (it might be acceptable to allow a threshold). A strict check might lead to transactions failing (race; e.g., round just timed out).  roundId + threshold >= answeredInRound - would allow a deviation of threshold rounds. This check alone might still result in stale data to be used if there are no more rounds. Therefore, this should be combined with updatedAt + threshold >= block.timestamp.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.14 Unclaimed or front-runnable proxy implementations ", "body": "  Description  Various smart contracts in the system require initialization functions to be called. The point when these calls happen is up to the deploying address. Deployment and initialization in one transaction are typically safe, but it can potentially be front-run if the initialization is done in a separate transaction.  A frontrunner can call these functions to silently take over the contracts and provide malicious parameters or plant a backdoor during the deployment.  Leaving proxy implementations uninitialized further aides potential phishing attacks where users might claim that - just because a contract address is listed in the official documentation/code-repo - a contract is a legitimate component of the system. At the same time, it is  only  a proxy implementation that an attacker claimed. For the end-user, it might be hard to distinguish whether this contract is part of the system or was a maliciously appropriated implementation.  Examples  code/contracts/mainnet/FujiVault.sol:L97-L102  function initialize(  address _fujiadmin,  address _oracle,  address _collateralAsset,  address _borrowAsset  ) external initializer {  FujiVault was initialized many days after deployment, and FujiVault inherits VaultBaseUpgradeable, which exposes a delegatecall that can be used to selfdestruct the contract s implementation.  Another FujiVault was deployed by deployer initialized in a 2-step approach that can theoretically silently be front-run.  code/artifacts/250-core.deploy:L2079-L2079  \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",  Transactions of deployer:  https://ftmscan.com/txs?a=0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148&p=2  The specific contract was initialized 19 blocks after deployment.  https://ftmscan.com/address/0x8513c2db99df213887f63300b23c6dd31f1d14b0  FujiAdminFTM (and others) don t seem to be initialized. (low prior; no risk other than pot. reputational damage)  code/artifacts/250-core.deploy:L1-L7  \"FujiAdmin\": {  \"address\": \"0xaAb2AAfBFf7419Ff85181d3A846bA9045803dd67\",  \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",  \"abi\": [  \"anonymous\": false,  Recommendation  It is recommended to use constructors wherever possible to immediately initialize proxy implementations during deploy-time. The code is only run when the implementation is deployed and affects the proxy initializations. If other initialization functions are used, we recommend enforcing deployer access restrictions or a standardized, top-level initialized boolean, set to true on the first deployment and used to prevent future initialization.  Using constructors and locked-down initialization functions will significantly reduce potential developer errors and the possibility of attackers re-initializing vital system components.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.15 Unused Import ", "body": "  Description  The following dependency is imported but never used:  code/contracts/mainnet/flashloans/Flasher.sol:L13-L13  import \"../../interfaces/IFujiMappings.sol\";  Recommendation  Remove the unused import.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.16 WFTM - Use of incorrect interface declarations ", "body": "  Description  WETH and  WFTM implementations are different.  code/contracts/fantom/WFTMUnwrapper.sol:L7-L23  contract WFTMUnwrapper {  address constant wftm = 0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83;  receive() external payable {}  /**  @notice Convert WFTM to FTM and transfer to msg.sender  @dev msg.sender needs to send WFTM before calling this withdraw  @param _amount amount to withdraw.  /  function withdraw(uint256 _amount) external {  IWETH(wftm).withdraw(_amount);  (bool sent, ) = msg.sender.call{ value: _amount }(\"\");  require(sent, \"Failed to send FTM\");  code/contracts/fantom/providers/ProviderGeist.sol:L115-L116  // convert FTM to WFTM  if (isFtm) IWETH(_tokenAddr).deposit{ value: _amount }();  Also see issues: issue 4.4, issue 4.5, issue 4.10  Recommendation  We recommend using the correct interfaces for all contracts instead of partial stubs. Do not modify the original function declarations, e.g., by omitting return value declarations. The codebase should also check return values where possible or explicitly state why values can safely be ignored in inline comments or the function s natspec documentation block.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.17 Inconsistent isFTM, isETH checks ", "body": "  Description  LibUniversalERC20FTM.isFTM() and LibUniversalERC20.isETH() identifies native assets by matching against two distinct addresses while some components only check for one.  Examples  The same is true for FTM.  Flasher only identifies a native asset transfer by matching asset against _ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE while univTransfer() identifies it using 0x0 || 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE  code/contracts/mainnet/flashloans/Flasher.sol:L122-L141  function callFunction(  address sender,  Account.Info calldata account,  bytes calldata data  ) external override {  require(msg.sender == _dydxSoloMargin && sender == address(this), Errors.VL_NOT_AUTHORIZED);  account;  FlashLoan.Info memory info = abi.decode(data, (FlashLoan.Info));  uint256 _value;  if (info.asset == _ETH) {  // Convert WETH to ETH and assign amount to be set as msg.value  _convertWethToEth(info.amount);  _value = info.amount;  } else {  // Transfer to Vault the flashloan Amount  // _value is 0  IERC20(info.asset).univTransfer(payable(info.vault), info.amount);  LibUniversalERC20  code/contracts/mainnet/libraries/LibUniversalERC20.sol:L8-L16  library LibUniversalERC20 {  using SafeERC20 for IERC20;  IERC20 private constant _ETH_ADDRESS = IERC20(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE);  IERC20 private constant _ZERO_ADDRESS = IERC20(0x0000000000000000000000000000000000000000);  function isETH(IERC20 token) internal pure returns (bool) {  return (token == _ZERO_ADDRESS || token == _ETH_ADDRESS);  code/contracts/mainnet/libraries/LibUniversalERC20.sol:L26-L40  function univTransfer(  IERC20 token,  address payable to,  uint256 amount  ) internal {  if (amount > 0) {  if (isETH(token)) {  (bool sent, ) = to.call{ value: amount }(\"\");  require(sent, \"Failed to send Ether\");  } else {  token.safeTransfer(to, amount);  There are multiple other instances of this  code/contracts/mainnet/Fliquidator.sol:L162-L162  uint256 _value = vAssets.borrowAsset == ETH ? debtTotal : 0;  Recommendation  Consider using a consistent way to identify native asset transfers (i.e. ETH, FTM) by using LibUniversalERC20.isETH(). Alternatively, the system can be greatly simplified by expecting WFTM and only working with it. This simplification will remove all special cases where the library must handle non-ERC20 interfaces.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.18 FujiOracle - setPriceFeed should check asset and priceFeed decimals ", "body": "  Description  getPriceOf() assumes that all price feeds return prices with identical decimals, but setPriceFeed does not enforce this. Potential misconfigurations can have severe effects on the system s internal accounting.  Examples  code/contracts/FujiOracle.sol:L27-L36  /**  @dev Sets '_priceFeed' address for a '_asset'.  Can only be called by the contract owner.  Emits a {AssetPriceFeedChanged} event.  /  function setPriceFeed(address _asset, address _priceFeed) public onlyOwner {  require(_priceFeed != address(0), Errors.VL_ZERO_ADDR);  usdPriceFeeds[_asset] = _priceFeed;  emit AssetPriceFeedChanged(_asset, _priceFeed);  Recommendation  We recommend adding additional checks to detect unexpected changes in assets  properties. Safeguard price feeds by enforcing priceFeed == address(0) || priceFeed.decimals() == 8. This allows the owner to disable a priceFeed (setting it to zero) and otherwise ensure that the feed is compatible and indeed returns 8 decimals.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.19 Unchecked function return values for low-level calls", "body": "  Description  It should be noted that the swapping and harvesting transactions sometimes return values to the function caller. While the low-level call is checked for  success , the return values are not actively handled. This can be intentional but should be verified.  Before calling the external contract, there is no check whether a contract is deployed at that address. Since destinations seem to be hardcoded in the Swapper/Harvester modules, we assume this has been ensured before deploying the contract. However, we suggest checking that code is deployed at the destination address, especially for upgradeable contracts.  We raise this as an informational finding as both the Harvester and Swapper flows using token.balanceOf(this), which might make this check obsolete. However, potential future third-party Swapper/Harvester additions to the protocol might return error codes that need to be checked for.  Examples  Geist/Uniswap and WFTM methods may return amounts or error codes  code/contracts/fantom/FujiVaultFTM.sol:L549-L551  // Claim rewards  (bool success, ) = harvestTransaction.to.call(harvestTransaction.data);  require(success, \"failed to harvest rewards\");  code/contracts/fantom/FujiVaultFTM.sol:L565-L567  // Swap rewards -> collateralAsset  (success, ) = swapTransaction.to.call{ value: swapTransaction.value }(swapTransaction.data);  require(success, \"failed to swap rewards\");  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.20 Use the compiler to resolve function selectors for interfaces", "body": "  Description  Function signatures of known contract and interface types are available to the compiler. We recommend using abi.encodeWithSelector(IProvider.withdraw.selector, ...) instead of the more error prone abi.encodeWithSignature(\"withdraw(address,uint256)\", ...) equivalent. Using the former method avoids hard-to-detect errors stemming from typos, interface changes, etc.  Examples  code/contracts/abstracts/vault/VaultBaseUpgradeable.sol:L57-L84  /**  @dev Executes withdraw operation with delegatecall.  @param _amount: amount to be withdrawn  @param _provider: address of provider to be used  /  function _withdraw(uint256 _amount, address _provider) internal {  bytes memory data = abi.encodeWithSignature(  \"withdraw(address,uint256)\",  vAssets.collateralAsset,  _amount  );  _execute(_provider, data);  /**  @dev Executes borrow operation with delegatecall.  @param _amount: amount to be borrowed  @param _provider: address of provider to be used  /  function _borrow(uint256 _amount, address _provider) internal {  bytes memory data = abi.encodeWithSignature(  \"borrow(address,uint256)\",  vAssets.borrowAsset,  _amount  );  _execute(_provider, data);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.21 Reduce code complexity", "body": "  Description  Throughout the codebase, snippets of code and whole functions have been copy-pasted. This duplication significantly increases code complexity and the potential for bugs. We recommend re-using code across modules or providing library contracts that implement re-usable code fragments.  Examples  Providers should use LibUniversalERC20FTM.isFTM instead of re-implementing Helper.isFTM.  code/contracts/fantom/providers/ProviderCream.sol:L17-L19  function _isFTM(address token) internal pure returns (bool) {  return (token == address(0) || token == address(0xFFfFfFffFFfffFFfFFfFFFFFffFFFffffFfFFFfF));  code/contracts/fantom/providers/ProviderScream.sol:L17-L19  function _isFTM(address token) internal pure returns (bool) {  return (token == address(0) || token == address(0xFFfFfFffFFfffFFfFFfFFFFFffFFFffffFfFFFfF));  ProviderGeist should provide an internal method instead of implementing multiple variants of the isFtm to token address mapping. E.g., both calls do the same thing. They select a different return value from the external call. Avoid re-implementing an inconsistent isFtm variant. Require that isFtm && amount != 0 on deposit/payback.  code/contracts/fantom/providers/ProviderGeist.sol:L57-L67  function getBorrowBalance(address _asset) external view override returns (uint256) {  IAaveDataProvider aaveData = _getAaveDataProvider();  bool isFtm = _asset == _getFtmAddr();  address _tokenAddr = isFtm ? _getWftmAddr() : _asset;  (, , uint256 variableDebt, , , , , , ) = aaveData.getUserReserveData(_tokenAddr, msg.sender);  return variableDebt;  code/contracts/fantom/providers/ProviderGeist.sol:L43-L52  function getBorrowRateFor(address _asset) external view override returns (uint256) {  IAaveDataProvider aaveData = _getAaveDataProvider();  (, , , , uint256 variableBorrowRate, , , , , ) = IAaveDataProvider(aaveData).getReserveData(  _asset == _getFtmAddr() ? _getWftmAddr() : _asset  );  return variableBorrowRate;  Also, note the unnecessary double cast IAaveDataProvider.  code/contracts/fantom/providers/ProviderGeist.sol:L73-L87  function getBorrowBalanceOf(address _asset, address _who)  external  view  override  returns (uint256)  IAaveDataProvider aaveData = _getAaveDataProvider();  bool isFtm = _asset == _getFtmAddr();  address _tokenAddr = isFtm ? _getWftmAddr() : _asset;  (, , uint256 variableDebt, , , , , , ) = aaveData.getUserReserveData(_tokenAddr, _who);  return variableDebt;  Consider removing support for the native currency altogether in favor of only accepting pre-wrapped WFTM (WETH). This should remove a lot of glue code currently implemented to auto-wrap/unwrap native currency.  Unused functionality  code/contracts/fantom/providers/ProviderCream.sol:L52-L57  function _exitCollatMarket(address _cyTokenAddress) internal {  // Create a reference to the corresponding network Comptroller  IComptroller comptroller = IComptroller(_getComptrollerAddress());  comptroller.exitMarket(_cyTokenAddress);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.22 Unusable state variable in dYdX provider", "body": "  Description  Remove the state variable donothing . Providers are always called via staticcall or delegatecall and should not hold any state.  code/contracts/mainnet/providers/ProviderDYDX.sol:L93-L95  bool public donothing = true;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.23 Use enums instead of hardcoded integer literals", "body": "  Description  Hardcoded integers are used throughout the codebase to denote states and distinguish between states. The code s complexity can be significantly reduced by using descriptive enum values.  Examples  2 should be InterestRateMode.VARIABLE  code/contracts/fantom/providers/ProviderGeist.sol:L184-L184  aave.repay(_tokenAddr, _amount, 2, address(this));  code/contracts/fantom/providers/ProviderGeist.sol:L136-L136  aave.borrow(_tokenAddr, _amount, 2, 0, address(this));  _farmProtocolNum and harvestType should be refactored to their enum equivalents:  code/contracts/mainnet/Harvester.sol:L20-L32  if (_farmProtocolNum == 0) {  transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B;  transaction.data = abi.encodeWithSelector(  bytes4(keccak256(\"claimComp(address)\")),  msg.sender  );  claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888;  } else if (_farmProtocolNum == 1) {  uint256 harvestType = abi.decode(_data, (uint256));  if (harvestType == 0) {  // claim  (, address[] memory assets) = abi.decode(_data, (uint256, address[]));  label the flashloan providers with an enum representing their name  code/contracts/fantom/flashloans/FlasherFTM.sol:L72-L78  if (_flashnum == 0) {  _initiateGeistFlashLoan(info);  } else if (_flashnum == 2) {  _initiateCreamFlashLoan(info);  } else {  revert(Errors.VL_INVALID_FLASH_NUMBER);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.24 Redundant harvest check in vault", "body": "  Description  In the FujiVaultFTM.harvestRewards function, the check for a returned token s address in the if condition and require statement overlap with tokenReturned != address(0).  Examples  code/contracts/mainnet/FujiVault.sol:L553-L555  if (tokenReturned != address(0)) {  uint256 tokenBal = IERC20Upgradeable(tokenReturned).univBalanceOf(address(this));  require(tokenReturned != address(0) && tokenBal > 0, Errors.VL_HARVESTING_FAILED);  Recommendation  We recommend removing one of the statements for gas savings and increased readability.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.25 Redundant use of immutable for constants", "body": "  Description  The FlasherFTM contract declares immutable state variables even though they are never set in the constructor. Consider declaring them as constant instead unless they are to be set on construction time. See the Solidity Documentation for further details:  [\u2026]  For constant variables, the value has to be fixed at compile-time, while for immutable, it can still be assigned at construction time. [\u2026]  Examples  code/contracts/mainnet/flashloans/Flasher.sol:L37-L44  address private immutable _aaveLendingPool = 0x7d2768dE32b0b80b7a3454c06BdAc94A69DDc7A9;  address private immutable _dydxSoloMargin = 0x1E0447b19BB6EcFdAe1e4AE1694b0C3659614e4e;  // IronBank  address private immutable _cyFlashloanLender = 0x1a21Ab52d1Ca1312232a72f4cf4389361A479829;  address private immutable _cyComptroller = 0xAB1c342C7bf5Ec5F02ADEA1c2270670bCa144CbB;  // need to be payable because of the conversion ETH <> WETH  code/contracts/fantom/flashloans/FlasherFTM.sol:L36-L39  address private immutable _geistLendingPool = 0x9FAD24f572045c7869117160A571B2e50b10d068;  IFujiMappings private immutable _crMappings =  IFujiMappings(0x1eEdE44b91750933C96d2125b6757C4F89e63E20);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.26 Redeclaration of constant values in multiple contracts", "body": "  Description  Throughout the codebase, constant values are redeclared in various contracts. This duplication makes the code harder to maintain and increases the risk for bugs. A central contract, e.g., Constants.sol, ConstantsFTM.sol, and ConstantsETH.sol, to declare the constants used throughout the codebase instead of redeclaring them in multiple source units can fix this issue. Ideally, for example, an address constant for an external component is only configured in a single place but consumed by multiple contracts. This will significantly reduce the potential for misconfiguration.  Avoid hardcoded addresses and use meaningful, constant names for them.  Note that the solidity compiler is going to inline constants where possible.  Examples  code/contracts/mainnet/WETHUnwrapper.sol:L7-L9  contract WETHUnwrapper {  address constant weth = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  code/contracts/mainnet/Swapper.sol:L16-L19  address public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  address public constant WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  address public constant SUSHI_ROUTER_ADDR = 0xd9e1cE17f2641f24aE83637ab66a2cca9C378B9F;  code/contracts/mainnet/FujiVault.sol:L32-L34  address public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  code/contracts/mainnet/Fliquidator.sol:L31-L31  address public constant ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  code/contracts/mainnet/providers/ProviderCompound.sol:L14-L18  contract HelperFunct {  function _isETH(address token) internal pure returns (bool) {  return (token == address(0) || token == address(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE));  code/contracts/mainnet/libraries/LibUniversalERC20.sol:L10-L14  IERC20 private constant _ETH_ADDRESS = IERC20(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE);  IERC20 private constant _ZERO_ADDRESS = IERC20(0x0000000000000000000000000000000000000000);  function isETH(IERC20 token) internal pure returns (bool) {  code/contracts/mainnet/flashloans/Flasher.sol:L34-L36  address private constant _ETH = 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE;  address private constant _WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;  Use meaningful names instead of hardcoded addresses  code/contracts/mainnet/Harvester.sol:L20-L29  if (_farmProtocolNum == 0) {  transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B;  transaction.data = abi.encodeWithSelector(  bytes4(keccak256(\"claimComp(address)\")),  msg.sender  );  claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888;  } else if (_farmProtocolNum == 1) {  uint256 harvestType = abi.decode(_data, (uint256));  Avoid unnamed hardcoded inlined addresses  code/contracts/fantom/providers/ProviderCream.sol:L157-L162  if (_isFTM(_asset)) {  // Transform FTM to WFTM  IWETH(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83).deposit{ value: _amount }();  _asset = address(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83);  comptroller address - can also be private constant state variables as the compiler/preprocessor will inline them.  code/contracts/fantom/providers/ProviderCream.sol:L21-L31  function _getMappingAddr() internal pure returns (address) {  return 0x1eEdE44b91750933C96d2125b6757C4F89e63E20; // Cream fantom mapper  function _getComptrollerAddress() internal pure returns (address) {  return 0x4250A6D3BD57455d7C6821eECb6206F507576cD2; // Cream fantom  function _getUnwrapper() internal pure returns(address) {  return 0xee94A39D185329d8c46dEA726E01F91641E57346;  WFTM multiple re-declarations  code/contracts/fantom/WFTMUnwrapper.sol:L7-L9  contract WFTMUnwrapper {  address constant wftm = 0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83;  code/contracts/fantom/providers/ProviderGeist.sol:L27-L29  function _getWftmAddr() internal pure returns (address) {  return 0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83;  code/contracts/fantom/providers/ProviderCream.sol:L79-L81  IWETH(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83).deposit{ value: _amount }();  _asset = address(0x21be370D5312f44cB42ce377BC9b8a0cEF1A4C83);  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.27 Always use the best available type", "body": "  Description  Declare state variables with the best type available and downcast to address if needed. Typecasting inside the corpus of a function is unneeded when the parameter s type is known beforehand. Declare the best type in function arguments, state vars. Always return the best type available instead of falling back to address.  Examples  There are many more instances of this, but here s a list of samples:  Should be declared with the correct types/interfaces instead of address  code/contracts/FujiAdmin.sol:L14-L20  address private _flasher;  address private _fliquidator;  address payable private _ftreasury;  address private _controller;  address private _vaultHarvester;  Should return the correct type/interfaces instead of address  code/contracts/FujiAdmin.sol:L144-L147  Should declare the argument with the correct type instead of casting in the function body.  /  function getSwapper() external view override returns (address) {  return _swapper;  Should declare the argument with the correct type instead of casting in the function body.  code/contracts/Controller.sol:L73-L80  function doRefinancing(  address _vaultAddr,  address _newProvider,  uint8 _flashNum  ) external isValidVault(_vaultAddr) onlyOwnerOrExecutor {  IVault vault = IVault(_vaultAddr);  Should make the FujiVaultFTM.fujiERC1155 state variable of type IFujiERC1155  code/contracts/fantom/FujiVaultFTM.sol:L438-L445  IFujiERC1155(fujiERC1155).updateState(  vAssets.borrowID,  IProvider(activeProvider).getBorrowBalance(vAssets.borrowAsset)  );  IFujiERC1155(fujiERC1155).updateState(  vAssets.collateralID,  IProvider(activeProvider).getDepositBalance(vAssets.collateralAsset)  );  Return the best type available  code/contracts/fantom/providers/ProviderCream.sol:L25-L31  function _getComptrollerAddress() internal pure returns (address) {  return 0x4250A6D3BD57455d7C6821eECb6206F507576cD2; // Cream fantom  function _getUnwrapper() internal pure returns(address) {  return 0xee94A39D185329d8c46dEA726E01F91641E57346;  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2022/03/fuji-protocol/"}, {"title": "4.1 Incorrect Priviliges setOperatorAddresses   ", "body": "  Description  The function setOperatorAddresses instead of allowing the Operator to update its own, as well as the Fee Recipient address, incorrectly provides the privileges to the Fee Recipient. As a result, the Fee Recipient can modify the operator address as and when needed, to DoS the operator and exploit the system. Additionally, upon reviewing the documentation, we found that there are no administrative rights defined for the Fee Recipient, hence highlighting the incorrect privilege allocation.  src/contracts/StakingContract.sol:L412-L424  function setOperatorAddresses(  uint256 _operatorIndex,  address _operatorAddress,  address _feeRecipientAddress  ) external onlyActiveOperatorFeeRecipient(_operatorIndex) {  _checkAddress(_operatorAddress);  _checkAddress(_feeRecipientAddress);  StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();  operators.value[_operatorIndex].operator = _operatorAddress;  operators.value[_operatorIndex].feeRecipient = _feeRecipientAddress;  emit ChangedOperatorAddresses(_operatorIndex, _operatorAddress, _feeRecipientAddress);  Recommendation  The modifier should be onlyActiveOperatorOrAdmin allowing only the operator itself or admin of the system, to update the necessary addresses.  Also, for transferring crucial privileges from one address to another, the operator s address should follow a 2-step approach like transferring ownership.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.2 Unconstrained Snapshot While Setting Operator Limit ", "body": "  Description  src/contracts/StakingContract.sol:L468-L473  if (  operators.value[_operatorIndex].limit < _limit &&  StakingContractStorageLib.getLastValidatorEdit() > _snapshot  ) {  revert LastEditAfterSnapshot();  Recommendation  If the functionality is not needed, consider removing it. Otherwise, add some necessary logic to either constrain the last validator edit or add public functions for the users to access it.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.3 Missing Input Validation ", "body": "  Description  There is no zero address check in function addOperator for the operator address and fee recipient. Also, the function doesn t check whether the operator already exists.  src/contracts/StakingContract.sol:L392-L405  function addOperator(address _operatorAddress, address _feeRecipientAddress) external onlyAdmin returns (uint256) {  StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();  StakingContractStorageLib.OperatorInfo memory newOperator;  if (operators.value.length == 1) {  revert MaximumOperatorCountAlreadyReached();  newOperator.operator = _operatorAddress;  newOperator.feeRecipient = _feeRecipientAddress;  operators.value.push(newOperator);  uint256 operatorIndex = operators.value.length - 1;  emit NewOperator(_operatorAddress, _feeRecipientAddress, operatorIndex);  return operatorIndex;  No zero address check in function setTreasury for updating the treasury address.  src/contracts/StakingContract.sol:L214-L217  function setTreasury(address _newTreasury) external onlyAdmin {  emit ChangedTreasury(_newTreasury);  StakingContractStorageLib.setTreasury(_newTreasury);  Functions activateOperator and deactivateOperator as the name hints, allow the admin to activate or deactivate an operator. However, the functions don t check for already activated/deactivated operators, and may still emit DeactivatedOperator/ActivatedOperator events for an operator. It may trigger false alarms for off-chain monitoring tools and create unnecessary panic.  src/contracts/StakingContract.sol:L479-L502  /// @notice Deactivates an operator and changes the fee recipient address and the staking limit  /// @param _operatorIndex Operator Index  /// @param _temporaryFeeRecipient Temporary address to receive funds decided by the system admin  function deactivateOperator(uint256 _operatorIndex, address _temporaryFeeRecipient) external onlyAdmin {  StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();  operators.value[_operatorIndex].limit = 0;  emit ChangedOperatorLimit(_operatorIndex, 0);  operators.value[_operatorIndex].deactivated = true;  emit DeactivatedOperator(_operatorIndex);  operators.value[_operatorIndex].feeRecipient = _temporaryFeeRecipient;  emit ChangedOperatorAddresses(_operatorIndex, operators.value[_operatorIndex].operator, _temporaryFeeRecipient);  _updateAvailableValidatorCount(_operatorIndex);  /// @notice Activates an operator, without changing its 0 staking limit  /// @param _operatorIndex Operator Index  /// @param _newFeeRecipient Sets the fee recipient address  function activateOperator(uint256 _operatorIndex, address _newFeeRecipient) external onlyAdmin {  StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();  operators.value[_operatorIndex].deactivated = false;  emit ActivatedOperator(_operatorIndex);  operators.value[_operatorIndex].feeRecipient = _newFeeRecipient;  emit ChangedOperatorAddresses(_operatorIndex, operators.value[_operatorIndex].operator, _newFeeRecipient);  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.4 Hardcoded Operator Limit Logic ", "body": "  Description  The contract defines some hardcoded limits which is not the right approach for upgradeable contracts and opens doors for accidental mistakes, if not handled with care.  The operators for the current version are limited to 1. If the auditee team decides to open the system to work with more operators but fails to change the limit while upgrading, the upgraded contract will have no effect, and will still disallow any more operators to be added.  src/contracts/StakingContract.sol:L392-L398  function addOperator(address _operatorAddress, address _feeRecipientAddress) external onlyAdmin returns (uint256) {  StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();  StakingContractStorageLib.OperatorInfo memory newOperator;  if (operators.value.length == 1) {  revert MaximumOperatorCountAlreadyReached();  Also, the function _depositOnOneOperator hardcodes the operator Index as 0 since the contract only supports one operator.  src/contracts/StakingContract.sol:L893-L896  function _depositOnOneOperator(uint256 _depositCount, uint256 _totalAvailableValidators) internal {  StakingContractStorageLib.setTotalAvailableValidators(_totalAvailableValidators - _depositCount);  _depositValidatorsOfOperator(0, _depositCount);  Recommendation  A better approach could be to constrain the limit of operators that can be added with a storage variable or constant, provided at the time of contract initialization. The contract should also consider supporting dynamic operator deposits for future versions instead of the default hardcoded index.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.5 StakingContract - PubKey Length Checks Not Always Enforced ", "body": "  Description  addValidators checks that the provided bytes pubKey is a multiple of the expected pubkey length while functions like setWithdrawer do not enforce similar length checks. This is an inconsistency that should be avoided.  addValidators enforcing input length checks  src/contracts/StakingContract.sol:L530-L542  function addValidators(  uint256 _operatorIndex,  uint256 _keyCount,  bytes calldata _publicKeys,  bytes calldata _signatures  ) external onlyActiveOperator(_operatorIndex) {  if (_keyCount == 0) {  revert InvalidArgument();  if (_publicKeys.length % PUBLIC_KEY_LENGTH != 0 || _publicKeys.length / PUBLIC_KEY_LENGTH != _keyCount) {  revert InvalidPublicKeys();  setWithdrawer  accepting any length for a pubKey. Note that _getPubKeyRoot  will take any input provided and concat it the zero bytes.  src/contracts/StakingContract.sol:L426-L445  /// @notice Set withdrawer for public key  /// @dev Only callable by current public key withdrawer  /// @param _publicKey Public key to change withdrawer  /// @param _newWithdrawer New withdrawer address  function setWithdrawer(bytes calldata _publicKey, address _newWithdrawer) external {  if (!StakingContractStorageLib.getWithdrawerCustomizationEnabled()) {  revert Forbidden();  _checkAddress(_newWithdrawer);  bytes32 pubkeyRoot = _getPubKeyRoot(_publicKey);  StakingContractStorageLib.WithdrawersSlot storage withdrawers = StakingContractStorageLib.getWithdrawers();  if (withdrawers.value[pubkeyRoot] != msg.sender) {  revert Unauthorized();  emit ChangedWithdrawer(_publicKey, _newWithdrawer);  withdrawers.value[pubkeyRoot] = _newWithdrawer;  src/contracts/StakingContract.sol:L775-L777  function _getPubKeyRoot(bytes memory _publicKey) internal pure returns (bytes32) {  return sha256(abi.encodePacked(_publicKey, bytes16(0)));  similarly, the withdraw family of functions does not enforce a pubkey length either. However, it is unlikely that someone finds a pubkey that matches a root for the attackers address.  src/contracts/StakingContract.sol:L696-L702  /// @notice Withdraw the Execution Layer Fee for a given validator public key  /// @dev Funds are sent to the withdrawer account  /// @param _publicKey Validator to withdraw Execution Layer Fees from  function withdrawELFee(bytes calldata _publicKey) external {  _onlyWithdrawerOrAdmin(_publicKey);  _deployAndWithdraw(_publicKey, EXECUTION_LAYER_SALT_PREFIX, StakingContractStorageLib.getELDispatcher());  Nevertheless, the methods should be hardened so as not to give a malicious actor the freedom to use an unexpected input size for the pubKey argument.  Recommendation  Enforce pubkey length checks when accepting a single pubkey as bytes similar to the batch functions that check for a multiple of \u00b4PUBLIC_KEY_LENGTH\u00b4. Alternatively, declare the function argument as bytes48 (however, in this case inputs may be auto-padded to fit the expected length, pot. covering situations that otherwise would throw an error)  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.6 Unpredictable Behavior Due to Admin Front Running or General Bad Timing ", "body": "  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.  Some instances of this are more important than others, but in general, users of the system should have assurances about the behavior of the action they re about to take.  Examples  Upgradeable TU proxy  Fee changes take effect immediately  src/contracts/StakingContract.sol:L504-L512  /// @notice Change the Operator fee  /// @param _operatorFee Fee in Basis Point  function setOperatorFee(uint256 _operatorFee) external onlyAdmin {  if (_operatorFee > StakingContractStorageLib.getOperatorCommissionLimit()) {  revert InvalidFee();  StakingContractStorageLib.setOperatorFee(_operatorFee);  emit ChangedOperatorFee(_operatorFee);  src/contracts/StakingContract.sol:L513-L522  /// @notice Change the Global fee  /// @param _globalFee Fee in Basis Point  function setGlobalFee(uint256 _globalFee) external onlyAdmin {  if (_globalFee > StakingContractStorageLib.getGlobalCommissionLimit()) {  revert InvalidFee();  StakingContractStorageLib.setGlobalFee(_globalFee);  emit ChangedGlobalFee(_globalFee);  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.7 Potentially Uninitialized Implementations ", "body": "  Description  Most contracts in the system are meant to be used with a proxy pattern. First, the implementations are deployed, and then proxies are deployed that delegatecall into the respective implementations following an initialization call  (hardhat, with same transaction). However, the implementations are initialized explicitly nor are they protected from other actors claiming/initializing them. This allows anyone to call initialization functions on implementations for use with phishing attacks (i.e. contract implementation addresses are typically listed on the official project website as valid contracts) which may affect the reputation of the system.  None of the implementations allow unprotected delegatecalls or selfdesturcts. lowering the severity of this finding.  Examples  src/contracts/StakingContract.sol:L151-L162  function initialize_1(  address _admin,  address _treasury,  address _depositContract,  address _elDispatcher,  address _clDispatcher,  address _feeRecipientImplementation,  uint256 _globalFee,  uint256 _operatorFee,  uint256 globalCommissionLimitBPS,  uint256 operatorCommissionLimitBPS  ) external init(1) {  src/contracts/AuthorizedFeeRecipient.sol:L21-L32  /// @notice Initializes the receiver  /// @param _dispatcher Address that will handle the fee dispatching  /// @param _publicKeyRoot Public Key root assigned to this receiver  function init(address _dispatcher, bytes32 _publicKeyRoot) external {  if (initialized) {  revert AlreadyInitialized();  initialized = true;  dispatcher = IFeeDispatcher(_dispatcher);  publicKeyRoot = _publicKeyRoot;  stakingContract = msg.sender; // The staking contract always calls init  src/contracts/FeeRecipient.sol:L18-L27  /// @param _publicKeyRoot Public Key root assigned to this receiver  function init(address _dispatcher, bytes32 _publicKeyRoot) external {  if (initialized) {  revert AlreadyInitialized();  initialized = true;  dispatcher = IFeeDispatcher(_dispatcher);  publicKeyRoot = _publicKeyRoot;  Recommendation  Petrify contracts in the constructor and disallow other actors from claiming/initializing the implementations.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.8 Operator May DoS the Withdrawal or Make It More Expensive ", "body": "  Description  While collecting fees, the operator may:  cause DoS for the funds/rewards withdrawal by reverting the call, thus reverting the whole transaction. By doing this, it won t be receiving any rewards, but so the treasury and withdrawer.  make the withdrawal more expensive by sending a huge chunk of returndata. As the returndata is copied into memory in the caller s context, it will add an extra gas overhead for the withdrawer making it more expensive.  or mint gas token  src/contracts/ConsensusLayerFeeDispatcher.sol:L105-L110  if (operatorFee > 0) {  (status, data) = operator.call{value: operatorFee}(\"\");  if (status == false) {  revert FeeRecipientReceiveError(data);  Recommendation  A possible solution could be to make a low-level call in an inline assembly block, restricting the returndata to a couple of bytes, and instead of reverting on the failed call, emit an event, flagging the call that failed.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.9 ProxyAdmin May Cause DoS for SYS_ADMIN ", "body": "  Description  As the TransparentUpgradeableProxy doesn t allow the ProxyAdmin to delegate calls to the implementation, it means the SYS_ADMIN can t be the same as ProxyAdmin. Now, talking about the design, the proxy defines a system-wide feature to pause or unpause. If the proxyAdmin pauses the staking contract, it implies no one can interact with it, not even the SYS_ADMIN, which might not be what the auditee team wants. There may be multiple scenarios where the auditee team wants to intervene manually in the system even if the system is paused, for instance, withdrawing funds while restricting the withdrawer.  if (msg.sender == _getAdmin()) {  bytes memory ret;  bytes4 selector = msg.sig;  if (selector == ITransparentUpgradeableProxy.upgradeTo.selector) {  ret = _dispatchUpgradeTo();  } else if (selector == ITransparentUpgradeableProxy.upgradeToAndCall.selector) {  ret = _dispatchUpgradeToAndCall();  } else if (selector == ITransparentUpgradeableProxy.changeAdmin.selector) {  ret = _dispatchChangeAdmin();  } else if (selector == ITransparentUpgradeableProxy.admin.selector) {  ret = _dispatchAdmin();  } else if (selector == ITransparentUpgradeableProxy.implementation.selector) {  ret = _dispatchImplementation();  } else {  revert(\"TransparentUpgradeableProxy: admin cannot fallback to proxy target\");  assembly {  return(add(ret, 0x20), mload(ret))  Recommendation  The system-wide pause feature should allow the SYS_ADMIN to call the staking functions if the system is paused. Something like:  function _beforeFallback() internal override {  if (StorageSlot.getBooleanSlot(_PAUSE_SLOT).value == false || msg.sender == stakingContract.getAdmin() || msg.sender == address(0)) {  super._beforeFallback();  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.10 Ambiguous/Misleading Return Data ", "body": "  Description  The NATSPEC comment says that the getters return address(0) or boolean false if the key doesn t exist in the contract. However, the functions don t check for any such logic. As a result, for any disabled validator, the functions may still return existing values besides the expected null values, conveying incorrect information to the users.  src/contracts/StakingContract.sol:L258-L263  /// @notice Retrieve withdrawer of public key  /// @notice In case the validator is not enabled, it will return address(0)  /// @param _publicKey Public Key to check  function getWithdrawer(bytes calldata _publicKey) external view returns (address) {  return _getWithdrawer(_getPubKeyRoot(_publicKey));  src/contracts/StakingContract.sol:L265-L270  /// @notice Retrieve withdrawer of public key root  /// @notice In case the validator is not enabled, it will return address(0)  /// @param _publicKeyRoot Hash of the public key  function getWithdrawerFromPublicKeyRoot(bytes32 _publicKeyRoot) external view returns (address) {  return _getWithdrawer(_publicKeyRoot);  src/contracts/StakingContract.sol:L272-L277  /// @notice Retrieve whether the validator exit has been requested  /// @notice In case the validator is not enabled, it will return false  /// @param _publicKeyRoot Public Key Root to check  function getExitRequestedFromRoot(bytes32 _publicKeyRoot) external view returns (bool) {  return _getExitRequest(_publicKeyRoot);  src/contracts/StakingContract.sol:L279-L284  /// @notice Return true if the validator already went through the exit logic  /// @notice In case the validator is not enabled, it will return false  /// @param _publicKeyRoot Public Key Root of the validator  function getWithdrawnFromPublicKeyRoot(bytes32 _publicKeyRoot) external view returns (bool) {  return StakingContractStorageLib.getWithdrawnMap().value[_publicKeyRoot];  src/contracts/StakingContract.sol:L286-L290  /// @notice Retrieve the enabled status of public key root, true if the key is in the contract  /// @param _publicKeyRoot Hash of the public key  function getEnabledFromPublicKeyRoot(bytes32 _publicKeyRoot) external view returns (bool) {  return StakingContractStorageLib.getOperatorIndexPerValidator().value[_publicKeyRoot].enabled;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.11 ConsensusLayerFeeDispatcher/ExecutionLayerFeeDispatcher - Should Hardcode autoPetrify With Highest Initializable Version Instead of User Provided Argument ", "body": "  Description  The version to auto-initialize is not hardcoded with the constructor. On deployment, the deployer may accidentally use the wrong version, allowing anyone to call initialize on the contract.  Examples  src/contracts/ConsensusLayerFeeDispatcher.sol:L47-L50  /// @notice Constructor method allowing us to prevent calls to initCLFR by setting the appropriate version  constructor(uint256 _version) {  VERSION_SLOT.setUint256(_version);  src/contracts/ExecutionLayerFeeDispatcher.sol:L47-L57  /// @notice Constructor method allowing us to prevent calls to initCLFR by setting the appropriate version  constructor(uint256 _version) {  VERSION_SLOT.setUint256(_version);  /// @notice Initialize the contract by storing the staking contract and the public key in storage  /// @param _stakingContract Address of the Staking Contract  function initELD(address _stakingContract) external init(1) {  STAKING_CONTRACT_ADDRESS_SLOT.setAddress(_stakingContract);  Recommendation  Similar to the init(1) modifier, it is suggested to track the highest version as a const int with the contract and auto-initialize to the highest version in the constructor instead of taking the highest version as a deployment argument.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.12 StakingContract - Misleading Comment ", "body": "  Description  The comment notes that the expected caller is admin while the modifier checks that msg.sender is an active operator.  src/contracts/StakingContract.sol:L109-L114  /// @notice Ensures that the caller is the admin  modifier onlyActiveOperator(uint256 _operatorIndex) {  _onlyActiveOperator(_operatorIndex);  _;  Recommendation  Rectify the comment to accurately describe the intention of the method/modifier.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.13 Impractical Checks for Global/Operator Fees and the Commission Limits ", "body": "  Description  The contract initialization sets up the global and operator fees and also their commission limits. However, the checks just make sure that the fees or commission limit is up to 100% which is not a very practical check. Any unusual value, for instance, if set to 100% will mean the whole rewards/funds will be non-exempted and taxed as global fees, which we believe will never be a case practically.  src/contracts/StakingContract.sol:L168-L175  if (_globalFee > BASIS_POINTS) {  revert InvalidFee();  StakingContractStorageLib.setGlobalFee(_globalFee);  if (_operatorFee > BASIS_POINTS) {  revert InvalidFee();  StakingContractStorageLib.setOperatorFee(_operatorFee);  src/contracts/StakingContract.sol:L188-L197  function initialize_2(uint256 globalCommissionLimitBPS, uint256 operatorCommissionLimitBPS) public init(2) {  if (globalCommissionLimitBPS > BASIS_POINTS) {  revert InvalidFee();  StakingContractStorageLib.setGlobalCommissionLimit(globalCommissionLimitBPS);  if (operatorCommissionLimitBPS > BASIS_POINTS) {  revert InvalidFee();  StakingContractStorageLib.setOperatorCommissionLimit(operatorCommissionLimitBPS);  src/contracts/StakingContract.sol:L516-L522  function setGlobalFee(uint256 _globalFee) external onlyAdmin {  if (_globalFee > StakingContractStorageLib.getGlobalCommissionLimit()) {  revert InvalidFee();  StakingContractStorageLib.setGlobalFee(_globalFee);  emit ChangedGlobalFee(_globalFee);  src/contracts/StakingContract.sol:L506-L512  function setOperatorFee(uint256 _operatorFee) external onlyAdmin {  if (_operatorFee > StakingContractStorageLib.getOperatorCommissionLimit()) {  revert InvalidFee();  StakingContractStorageLib.setOperatorFee(_operatorFee);  emit ChangedOperatorFee(_operatorFee);  Recommendation  The fees should be checked with a more practical limit. For instance, checking against a min - max limit, like 20% - 40%.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.14 Proxy Design Inconsistencies ", "body": "  Description  The feature to pause/unpause the contract is available in the proxy, however, via the inherited isAdmin modifier the functions still delegate calls to the implementation, if the caller is not the Admin and the system is not paused. This can produce unintended results (including the pause function being callable in the delegate target or its fallback potentially being executable).  Function isPaused() is a state-changing function but should be view.  If the returndata for the call to function isPaused() is less than 32 bytes, the function will still succeed, but with a decoding error, since the function expects an abi encoded boolean.  The documentation outlines that  no non-view functions can be called,  but even the view functions will become uncallable by other contracts in the paused state.  It should be avoided to implement a  hack  to allow off-chain view function calls to bypass pause mode (msg.sender == address(0)). This is highly client/library specific, and implementing bypasses for off-chain components on-chain may introduce inconsistencies and unwanted side effects.  Examples  src/contracts/TUPProxy.sol:L23-L47  function isPaused() external ifAdmin returns (bool) {  return StorageSlot.getBooleanSlot(_PAUSE_SLOT).value;  /// @dev Pauses system  function pause() external ifAdmin {  StorageSlot.getBooleanSlot(_PAUSE_SLOT).value = true;  /// @dev Unpauses system  function unpause() external ifAdmin {  StorageSlot.getBooleanSlot(_PAUSE_SLOT).value = false;  /// @dev Overrides the fallback method to check if system is not paused before  /// @dev Address Zero is allowed to perform calls even if system is paused. This allows  /// view functions to be called when the system is paused as rpc providers can easily  /// set the sender address to zero.  function _beforeFallback() internal override {  if (StorageSlot.getBooleanSlot(_PAUSE_SLOT).value == false || msg.sender == address(0)) {  super._beforeFallback();  } else {  revert CallWhenPaused();  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.15 Contracts Should Inherit From Their Interfaces ", "body": "  Description  The following contracts should enforce correct interface implementation by inheriting from the interface declarations.  Examples  src/contracts/StakingContract.sol:L10-L15  /// @title Ethereum Staking Contract  /// @author Kiln  /// @notice You can use this contract to store validator keys and have users fund them and trigger deposits.  contract StakingContract {  using StakingContractStorageLib for bytes32;  src/contracts/interfaces/IStakingContractFeeDetails.sol:L3-L20  interface IStakingContractFeeDetails {  function getWithdrawerFromPublicKeyRoot(bytes32 _publicKeyRoot) external view returns (address);  function getTreasury() external view returns (address);  function getOperatorFeeRecipient(bytes32 pubKeyRoot) external view returns (address);  function getGlobalFee() external view returns (uint256);  function getOperatorFee() external view returns (uint256);  function getExitRequestedFromRoot(bytes32 _publicKeyRoot) external view returns (bool);  function getWithdrawnFromPublicKeyRoot(bytes32 _publicKeyRoot) external view returns (bool);  function toggleWithdrawnFromPublicKeyRoot(bytes32 _publicKeyRoot) external;  src/contracts/FeeRecipient.sol:L4-L6  import \"./interfaces/IFeeDispatcher.sol\";  contract FeeRecipient {  src/contracts/interfaces/IFeeRecipient.sol:L3-L8  interface IFeeRecipient {  function init(address _dispatcher, bytes32 _publicKeyRoot) external;  function withdraw() external;  Recommendation  Inherit from interface.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.16 Misleading Error Statements ", "body": "  Description  The contracts define custom errors to revert transactions on failed operations or invalid input, however, they convey little to no information, making it difficult for the off-chain monitoring tools to track relevant updates.  src/contracts/StakingContract.sol:L28-L51  error Forbidden();  error InvalidFee();  error Deactivated();  error NoOperators();  error InvalidCall();  error Unauthorized();  error DepositFailure();  error DepositsStopped();  error InvalidArgument();  error UnsortedIndexes();  error InvalidPublicKeys();  error InvalidSignatures();  error InvalidWithdrawer();  error InvalidZeroAddress();  error AlreadyInitialized();  error InvalidDepositValue();  error NotEnoughValidators();  error InvalidValidatorCount();  error DuplicateValidatorKey(bytes);  error FundedValidatorDeletionAttempt();  error OperatorLimitTooHigh(uint256 limit, uint256 keyCount);  error MaximumOperatorCountAlreadyReached();  error LastEditAfterSnapshot();  error PublicKeyNotInContract();  For instance, the init modifier is used to initialize the contracts with the current Version. The Version initialization ensures that the provided version must be an increment of the previous version, if not, it reverts with an error as AlreadyInitialized(). However, the error doesn t convey an appropriate message correctly, as any version other than the expected version will signify that the version has already been initialized.  src/contracts/ConsensusLayerFeeDispatcher.sol:L37-L40  modifier init(uint256 _version) {  if (_version != VERSION_SLOT.getUint256() + 1) {  revert AlreadyInitialized();  src/contracts/ExecutionLayerFeeDispatcher.sol:L37-L40  modifier init(uint256 _version) {  if (_version != VERSION_SLOT.getUint256() + 1) {  revert AlreadyInitialized();  src/contracts/StakingContract.sol:L81-L84  modifier init(uint256 _version) {  if (_version != StakingContractStorageLib.getVersion() + 1) {  revert AlreadyInitialized();  Recommendation  Use a more meaningful statement with enough information to track off-chain for all the custom errors in every contract in scope. For instance, add the current and supplied versions as indexed parameters, like: IncorrectVersionInitialization(current version, supplied version);  Also, the function can be simplified as  function initELD(address _stakingContract) external init(VERSION_SLOT.getUint256() + 1) {  STAKING_CONTRACT_ADDRESS_SLOT.setAddress(_stakingContract);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.17 Inconsistent Storage Slot Design", "body": "  Description  Some storage slots are constructed by hashing a string as:  src/contracts/libs/StakingContractStorageLib.sol:L110-L111  bytes32 internal constant OPERATORS_SLOT = keccak256(\"StakingContract.operators\");  while others subtract 1 from the hashed string as:  src/contracts/libs/StakingContractStorageLib.sol:L351-L352  bytes32 internal constant WITHDRAWN_MAPPING_SLOT = bytes32(uint256(keccak256(\"StakingContract.withdrawn\")) - 1);  Recommendation  It is recommended that a consistent design should be adopted.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "4.18 Opportunities for Code Improvements", "body": "  Description  If operatorFee is 100% of globalFee, the check will still be satisfied, even though there is no reward for the treasury.  src/contracts/ConsensusLayerFeeDispatcher.sol:L99-L104  if (globalFee > 0) {  (status, data) = treasury.call{value: globalFee - operatorFee}(\"\");  if (status == false) {  revert TreasuryReceiveError(data);  Instead the check should be if ((globalFee - operatorFee) > 0)  Both the conditional branches can be combined in a single check with a logical or operator to increase readability  src/contracts/StakingContract.sol:L100-L107  modifier onlyActiveOperatorOrAdmin(uint256 _operatorIndex) {  if (msg.sender == StakingContractStorageLib.getAdmin()) {  _;  } else {  _onlyActiveOperator(_operatorIndex);  _;  Instead of referencing the outer struct, the functions should reference the nested struct. For instance, StakingContractStorageLib.OperatorsSlot struct, the functions can reference the StakingContractStorageLib.OperatorInfo based on the operator s index. This will increase the code readability, as OperatorInfo is what actually stores the operator s information.  src/contracts/StakingContract.sol:L460-L474  StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();  if (operators.value[_operatorIndex].deactivated) {  revert Deactivated();  uint256 publicKeyCount = operators.value[_operatorIndex].publicKeys.length;  if (publicKeyCount < _limit) {  revert OperatorLimitTooHigh(_limit, publicKeyCount);  if (  operators.value[_operatorIndex].limit < _limit &&  StakingContractStorageLib.getLastValidatorEdit() > _snapshot  ) {  revert LastEditAfterSnapshot();  operators.value[_operatorIndex].limit = _limit;  Solidity provides the feature to index calldata arrays. Hence, instead of using an external library  src/contracts/StakingContract.sol:L553-L554  bytes memory publicKey = BytesLib.slice(_publicKeys, i * PUBLIC_KEY_LENGTH, PUBLIC_KEY_LENGTH);  bytes memory signature = BytesLib.slice(_signatures, i * SIGNATURE_LENGTH, SIGNATURE_LENGTH);  the byte slicing can be done as :  bytes calldata publicKey =_publicKeys[i * PUBLIC_KEY_LENGTH:PUBLIC_KEY_LENGTH];  bytes calldata signature = _signatures[i * SIGNATURE_LENGTH:SIGNATURE_LENGTH];  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/kilnfi-staking-consensys/"}, {"title": "3.1 Owners can never be removed    ", "body": "  Resolution                           This has been fixed in   paxosglobal/simple-multisig#5, and appropriate tests have been added.  Description  The intention of setOwners() is to replace the current set of owners with a new set of owners. However, the isOwner mapping is never updated, which means any address that was ever considered an owner is permanently considered an owner for purposes of signing transactions.  Recommendation  In setOwners_(), before adding new owners, loop through the current set of owners and clear their isOwner booleans, as in the following code:  for (uint256 i = 0; i < ownersArr.length; i++) {  isOwner[ownersArr[i]] = false;  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/11/paxos/"}, {"title": "5.1 Attacker can abuse swapLiquidity function to drain users  funds    ", "body": "  Resolution                           Solved by removing   Description  The swapLiquidity function allows liquidity providers to atomically swap their collateral. The function takes a receiverAddressargument that normally points to an ISwapAdapter implementation trusted by the user.  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L490-L517  vars.fromReserveAToken.burn(  msg.sender,  receiverAddress,  amountToSwap,  fromReserve.liquidityIndex  );  // Notifies the receiver to proceed, sending as param the underlying already transferred  ISwapAdapter(receiverAddress).executeOperation(  fromAsset,  toAsset,  amountToSwap,  address(this),  params  );  vars.amountToReceive = IERC20(toAsset).balanceOf(receiverAddress);  if (vars.amountToReceive != 0) {  IERC20(toAsset).transferFrom(  receiverAddress,  address(vars.toReserveAToken),  vars.amountToReceive  );  if (vars.toReserveAToken.balanceOf(msg.sender) == 0) {  _usersConfig[msg.sender].setUsingAsCollateral(toReserve.id, true);  vars.toReserveAToken.mint(msg.sender, vars.amountToReceive, toReserve.liquidityIndex);  However, since an attacker can pass any address as the receiverAddress, they can arbitrarily transfer funds from other contracts that have given allowances to the LendingPool contract (for example, another ISwapAdapter).  The amountToSwap is defined by the caller and can be very small. The attacker gets the difference between IERC20(toAsset).balanceOf(receiverAddress) value of toAsset and the amountToSwap of fromToken.  Remediation  Ensure that no funds can be stolen from contracts that have granted allowances to the LendingPool contract.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.2 Griefing attack by taking flash loan on behalf of user ", "body": "  Description  When taking a flash loan from the protocol, the arbitrary receiverAddress  address can be passed as the argument:  code/contracts/lendingpool/LendingPool.sol:L547-L554  function flashLoan(  address receiverAddress,  address asset,  uint256 amount,  uint256 mode,  bytes calldata params,  uint16 referralCode  ) external override {  That may allow anyone to execute a flash loan on behalf of other users. In order to make that attack, the receiverAddress should give the allowance to the LendingPool contract to make a transfer for the amount of currentAmountPlusPremium.  Example  If someone is giving the allowance to the LendingPool contract to make a deposit, the attacker can execute a flash loan on behalf of that user, forcing the user to pay fees from the flash loan. That will also prevent the victim from making a successful deposit transaction.  Remediation  Make sure that only the user can take a flash loan.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.3 Interest rates are updated incorrectly ", "body": "  Resolution  This issue was independently discovered by the Aave developers and had already been fixed by the end of the audit.  The function updateInterestRates() updates the borrow rates of a reserve. Since the rates depend on the available liquidity they must be recalculated each time liquidity changes. The function takes the amount of liquidity added or removed as the input and is called ahead of minting or burning ATokens. However, in LendingPoolCollateralManager an interest rate update is performed after aTokens have been burned, resulting in an incorrect interest rate.  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L377-L382  vars.collateralAtoken.burn(  user,  receiver,  vars.maxCollateralToLiquidate,  collateralReserve.liquidityIndex  );  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L427-L433  //updating collateral reserve  collateralReserve.updateInterestRates(  collateral,  address(vars.collateralAtoken),  0,  vars.maxCollateralToLiquidate  );  Recommendation  Update interest rates before calling collateralAtoken.burn().  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.4 Unhandled return values of transfer and transferFrom ", "body": "  Resolution  ERC20 implementations are not always consistent. Some implementations of transfer and transferFrom could return  false  on failure instead of reverting. It is safer to wrap such calls into require() statements to these failures. Unsafe transferFrom calls were found in the following locations:  code/contracts/lendingpool/LendingPool.sol:L578  IERC20(asset).transferFrom(receiverAddress, vars.aTokenAddress, vars.amountPlusPremium);  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L407  IERC20(principal).transferFrom(receiver, vars.principalAToken, vars.actualAmountToLiquidate);  code/contracts/lendingpool/LendingPoolCollateralManager.sol:L507-L511  IERC20(toAsset).transferFrom(  receiverAddress,  address(vars.toReserveAToken),  vars.amountToReceive  );  Recommendation  Check the return value and revert on 0/false or use OpenZeppelin s SafeERC20 wrapper functions.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.5 Re-entrancy attacks with ERC-777 ", "body": "  Resolution                           The issue was partially mitigated in   Description  Some tokens may allow users to perform re-entrancy while calling the transferFrom function. For example, it would be possible for an attacker to  borrow  a large amount of ERC-777 tokens from the lending pool by re-entering the deposit function from within transferFrom.  code/contracts/lendingpool/LendingPool.sol:L91-L118  function deposit(  address asset,  uint256 amount,  address onBehalfOf,  uint16 referralCode  ) external override {  _whenNotPaused();  ReserveLogic.ReserveData storage reserve = _reserves[asset];  ValidationLogic.validateDeposit(reserve, amount);  address aToken = reserve.aTokenAddress;  reserve.updateState();  reserve.updateInterestRates(asset, aToken, amount, 0);  bool isFirstDeposit = IAToken(aToken).balanceOf(onBehalfOf) == 0;  if (isFirstDeposit) {  _usersConfig[onBehalfOf].setUsingAsCollateral(reserve.id, true);  IAToken(aToken).mint(onBehalfOf, amount, reserve.liquidityIndex);  //transfer to the aToken contract  IERC20(asset).safeTransferFrom(msg.sender, aToken, amount);  emit Deposit(asset, msg.sender, onBehalfOf, amount, referralCode);  Because the safeTransferFrom call is happening at the end of the deposit function, the deposit will be fully processed before the tokens are actually transferred.  So at the beginning of the transfer, the attacker can re-enter the call to withdraw their deposit. The withdrawal will succeed even though the attacker s tokens have not yet been transferred to the lending pool. Essentially, the attacker is granted a flash-loan but without paying fees.  Additionally, after these calls, interest rates will be skewed because interest rate update relies on the actual current balance.  Remediation  Do not whitelist ERC-777 or other re-entrable tokens to prevent this kind of attack.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.6 Potential manipulation of stable interest rates using flash loans ", "body": "  Resolution  This type of manipulation is difficult to prevent completely especially when flash loans are available. In practice however, attacks are mitigated by the following factors:  Liquidity providers attempting to increase users  stable rates would have to pay a high flash loan premium. Users could also immediately swap to variable interest meaning that the attack could result in a net loss for the LP. In practice, it is likely that this makes the attack economically unfeasible.  Under normal conditions, users would only gain a relatively small advantage by lowering their stable rate due to the design of the stable rate curve. If a user attempted to manipulate their stable rate during a liquidity crisis, Aave could immediately rebalance them and bring the rate back to normal.  Flash loans allow users to borrow large amounts of liquidity from the protocol. It is possible to adjust the stable rate up or down by momentarily removing or adding large amounts of liquidity to reserves.  LPs increasing the interest rate of borrowers  The function rebalanceStableBorrowRate() increases the stable interest rate of a user if the current liquidity rate is higher than the user s stable rate. A liquidity provider could trigger an artificial  liquidity crisis  in a reserve and increase the stable interest rates of borrowers by atomically performing the following steps:  Take a flash loan to take a large number of tokens from a reserve  Re-balance the stable rate of the emptied reserves  borrowers  Repay the flash loan (plus premium)  Withdraw the collateral and repay the flash loan  Individual borrowers would then have to switch to the variable rate to return to a lower interest rate.  User borrowing at an artificially lowered interest rate  Users wanting to borrow funds could attempt to get a lower interest rate by temporarily adding liquidity to a reserve (which could e.g. be flash borrowed from a different protocol). While there s a check that prevents users from borrowing an asset while also adding a higher amount of the same asset as collateral, this can be bypassed rather easily by depositing the collateral from a different address (via smart contracts). Aave would then have to rebalance the user to restore an appropriate interest rate.  In practice, users would gain only a relatively small advantage here due to the design of the stable rate curve.  Recommendation  This type of manipulation is difficult to prevent especially when flash loans are available. The safest option to prevent the first variant would be to restrict access to rebalanceStableBorrowRate() to admins. In any case, Aave should monitor the protocol at all times to make sure that interest rates are being rebalanced to sane values.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.7 Code quality could be improved ", "body": "  Some minor code quality improvements are recommended to improve readability.  Explicitly set the visibility for of variables:  code/contracts/tokenization/StableDebtToken.sol:L23-L24  mapping(address => uint40) _timestamps;  uint40 _totalSupplyTimestamp;  code/contracts/configuration/LendingPoolAddressesProviderRegistry.sol:L17-L18  mapping(address => uint256) addressesProviders;  address[] addressesProvidersList;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.8 Attacker can front-run delegator when changing allowance ", "body": "  Users can grant allowances to borrow debt assets to other users using the delegateAllowance function. Similar to the classical ERC20 approve attack, it is possible for a malicious user to front-run the delegator when they attempt to change the allowance and borrow the sum of the old and new values.  Example scenario:  Bob creates an allowance of 100 DAI for Malice: delegateBorrowAllowance(DAI, Malice, 100)  Later, Bob attempts to lower the allowance to 90: delegateBorrowAllowance(DAI, Malice, 90)  Malice borrows a total of 190 DAI by first frontrunning Bob s second transaction borrowing 100 DAI and then borrowing another 90 DAI after Bob s transaction was mined.  Recommentation  A commonly used way of preventing this attack is using increaseAllowance() and decreaseAllowance() functions specifically for increasing and decreasing allowances.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "5.9 Description of flash loan function is inconsistent with code ", "body": "  The function flashLoan in LendingPool.sol takes an argument mode that specifies the interest rate mode. If the mode is ReserveLogic.InterestRateMode.NONE the function call is treated as a flash loan, if not a normal borrow is executed.  However, inline comments in the function describe the behaviour as  If the transfer didn t succeed, the receiver either didn t return the funds, or didn t approve the transfer . It is unclear how this relates to the actual code or why it is possible to specify a mode in the first place.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/09/aave-protocol-v2/"}, {"title": "4.1 No Proper Trusted Setup   ", "body": "  Description  Linea uses Plonk proof system, which needs a preprocessed CRS (Common Reference String) for proving and verification, the Plonk system security is based on the existence of a trusted setup ceremony to compute the CRS, the current verifier uses a CRS created by one single party, which requires fully trust of the party to delete the toxic waste (trapdoor) which can be used to generate forged proof, undermining the security of the entire system  contracts/Verifier.sol:L29-L37  uint256 constant g2_srs_0_x_0 = 11559732032986387107991004021392285783925812861821192530917403151452391805634;  uint256 constant g2_srs_0_x_1 = 10857046999023057135944570762232829481370756359578518086990519993285655852781;  uint256 constant g2_srs_0_y_0 = 4082367875863433681332203403145435568316851327593401208105741076214120093531;  uint256 constant g2_srs_0_y_1 = 8495653923123431417604973247489272438418190587263600148770280649306958101930;  uint256 constant g2_srs_1_x_0 = 18469474764091300207969441002824674761417641526767908873143851616926597782709;  uint256 constant g2_srs_1_x_1 = 17691709543839494245591259280773972507311536864513996659348773884770927133474;  uint256 constant g2_srs_1_y_0 = 2799122126101651639961126614695310298819570600001757598712033559848160757380;  uint256 constant g2_srs_1_y_1 = 3054480525781015242495808388429905877188466478626784485318957932446534030175;  Recommendation  Conduct a proper MPC to generate CRS like the Powers of Tau MPC or use a trustworthy CRS generated by an exisiting audited trusted setup like Aztec s ignition  ", "labels": ["Consensys", "Critical", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.2 Broken Lagrange Polynomial Evaluation at Zeta   ", "body": "  Description  The Verifier calculates the Lagrange Polynomial at \u03b6 with an efficient scheme as: Lj(\u03b6) = \u03c9i/n * (\u03b6n-1)/(\u03b6-\u03c9i)  which has also been pointed out in the plonk paper. However, the computation ignores the fact that \u03b6 can also be a root of unity, which means \u03b6n - 1 will be 0 for any \u03b6 that is a root of unity.  Thus, the formula will yield the Lagrange polynomial evaluation as 0, which is incorrect. Because the property of the Lagrange polynomial is: Lj(\u03b6) = 1,  if i=j and 0 otherwise, where \u03b6 belongs to domain H = \u03c9i, \u2200 0<=i< n(n being the domain size)  If we consider the same evaluation for \u03b6 at the root of unity in the second formula, it will correctly satisfy the property of the Lagrange polynomial stated above.  Hence, there is a need to fix the computation considering the case highlighted.  The problematic instances can be found in functions:  compute_ith_lagrange_at_z  batch_compute_lagranges_at_z  compute_alpha_square_lagrange_0  Recommendation  Consider adopting a strategy to use the second formula for the computation of Lagrange Polynomial evaluation at \u03b6 if \u03b6 is a root of unity.  ", "labels": ["Consensys", "Critical", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.3 Broken Logic for Modular Multiplicative Inverse   ", "body": "  Description  The multiplicate inverse of an element \u03b1 in a finite field Fpn can be calculated as \u03b1pn - 2. \u03b1 can be any field element except 0 or the point at infinity.  This totally makes sense as there exists no field element x such that 0 * x = 1 mod p  However, it is allowed here and it is calculated like any other field element. It doesn t revert, because 0 raised to any power modulo p will yield 0.  Thus the calculation points to a broken logic that defines the modular multiplicative inverse of 0 as 0.  Recommendation  The point at infinity can bring many mathematical flaws to the system. Hence require the utmost attention to be fixed.  ", "labels": ["Consensys", "Critical", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.4 Missing Verifying Paring Check Result    ", "body": "  Description  In function batch_verify_multi_points, the SNARK paring check is done by calling paring pre-compile let l_success := staticcall(sub(gas(), 2000),8,mPtr,0x180,0x00,0x20)  and the only the execution status is stored in the final success state (state_success), but the the paring check result which is stored in 0x00 is not stored and checked, which means if the paring check result is 0 (pairing check failed), the proof would still pass verification, e.g. invalid proof with incorrect proof element proof_openings_selector_commit_api_at_zeta would pass the paring check. As a result it breaks the SNARK paring verification.  Examples  contracts/Verifier.sol:L586-L588  let l_success := staticcall(sub(gas(), 2000),8,mPtr,0x180,0x00,0x20)  // l_success := true  mstore(add(state, state_success), and(l_success,mload(add(state, state_success))))  Another example is, if either of the following is sent as a point at infinity or (0,0) as (x,y) co-ordinate:  commitment to the opening proof polynomial  Wz  commitment to the opening proof polynomial  Wzw  The proof will still work, since the pairing result is not being checked.  Recommendation  Verify paring check result and store it in the final success state after calling the paring pre-compile  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.5 Gas Greifing and Missing Return Status Check for staticcall(s), May Lead to Unexpected Outcomes   Partially Addressed", "body": "  Description  The gas supplied to the staticcall(s), is calculated by subtracting 2000 from the remaining gas at this point in time. However, if not provided enough gas, the staticcall(s) may fail and there will be no return data, and the execution will continue with the stale data that was previously there at the memory location specified by the return offset with the staticcall(s).  1- Predictable Derivation of Challenges  Examples  contracts/Verifier.sol:L261  pop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1b), size, mPtr, 0x20)) //0x1b -> 000..\"gamma\"  contracts/Verifier.sol:L269  pop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1c), 0x24, mPtr, 0x20)) //0x1b -> 000..\"gamma\"  contracts/Verifier.sol:L279  pop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1b), 0x65, mPtr, 0x20)) //0x1b -> 000..\"gamma\"  contracts/Verifier.sol:L293  pop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1c), 0xe4, mPtr, 0x20))  contracts/Verifier.sol:L694  pop(staticcall(sub(gas(), 2000), 0x2, add(mPtr,start_input), size_input, add(state, state_gamma_kzg), 0x20))  If the staticcall(s) fails, it will make the challenge values to be predictable and may help the prover in forging proofs and launching other adversarial attacks.  2- Incorrect Exponentiation  Functions compute_ith_lagrange_at_z, compute_pi, and verify compute modular exponentiation by making a staticcall to the precompile modexp as:  contracts/Verifier.sol:L335  pop(staticcall(sub(gas(), 2000),0x05,mPtr,0xc0,0x00,0x20))  contracts/Verifier.sol:L441  pop(staticcall(sub(gas(), 2000),0x05,mPtr,0xc0,mPtr,0x20))  contracts/Verifier.sol:L889  pop(staticcall(sub(gas(), 2000),0x05,mPtr,0xc0,mPtr,0x20))  However, if not supplied enough gas, the staticcall(s) will fail, thus returning no result and the execution will continue with the stale data.  3. Incorrect Point Addition and Scalar Multiplication  contracts/Verifier.sol:L555  pop(staticcall(sub(gas(), 2000),7,folded_evals_commit,0x60,folded_evals_commit,0x40))  contracts/Verifier.sol:L847  let l_success := staticcall(sub(gas(), 2000),6,mPtr,0x80,dst,0x40)  contracts/Verifier.sol:L858  let l_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,dst,0x40)  contracts/Verifier.sol:L868  let l_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,mPtr,0x40)  contracts/Verifier.sol:L871  l_success := and(l_success, staticcall(sub(gas(), 2000),6,mPtr,0x80,dst, 0x40))  However, it will not be practically possible to conduct a gas griefing attack for staticcall(s) at the start of the top-level transaction. As it will require an attacker to pass a very low amount of gas to make the staticcall fail, but at the same time, that would not be enough to make the top-level transaction execute entirely and not run out of gas. But, this can still be conducted for the staticcall(s) that are executed at the near end of the top-level transaction.  Recommendation  Check the returned status of the staticcall and revert if any of the staticcall s return status has been 0.  Also fix the comments mentioned for every staticcall, for instance: the function derive_beta says 0x1b -> 000..\"gamma\" while the memory pointer holds the ASCII value of string beta  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.6 Verifier Doesn t Support Zero or Multiple BSB22 Commitments   ", "body": "  Description  The verifier currently supports single BSB22 commitment as Gnark only supports single Commit(..) call. If there is no or multiple BSB22 commitments/Commit calls, the verifier would fail in proof verification.  Examples  tmpl/template_verifier.go:L57-L58  uint256 constant vk_selector_commitments_commit_api_0_x = {{ (fpptr .Qcp.X).String }};  uint256 constant vk_selector_commitments_commit_api_0_y = {{ (fpptr .Qcp.Y).String }};  Recommendation  Add support for  zero or multiple BSB22 commitments  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.7 Missing Tests for Edge Cases ", "body": "  Description  There are no test cases for invalid proof and public input such as proof elements not on curve, proof element is points of infinity, all proof elements are zero, wrong proof element, proof scalar element bigger than scalar field modulus, proof scalar element wrapping around scalar field modulus, public input greater than scalar field modulus etc. and no or multiple BSB22 commitments. There is only test for valid proof and one BSB22 commitment. Tests for all edge cases are crucial to check proof soundness in SNARK, missing it may result in missing some critical bugs, e.g. issue issue 4.4 issue issue 4.6  Recommendation  Add missing test cases  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.8 Missing Scalar Field Range Check in Scalar Multiplication    ", "body": "  Description  Plonk paper. The scalar multiplication functions  Examples  contracts/Verifier.sol:L852-L873  function point_mul(dst,src,s, mPtr) {  // let mPtr := add(mload(0x40), state_last_mem)  let state := mload(0x40)  mstore(mPtr,mload(src))  mstore(add(mPtr,0x20),mload(add(src,0x20)))  mstore(add(mPtr,0x40),s)  let l_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,dst,0x40)  mstore(add(state, state_success), and(l_success,mload(add(state, state_success))))  // dst <- dst + [s]src (Elliptic curve)  function point_acc_mul(dst,src,s, mPtr) {  let state := mload(0x40)  mstore(mPtr,mload(src))  mstore(add(mPtr,0x20),mload(add(src,0x20)))  mstore(add(mPtr,0x40),s)  let l_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,mPtr,0x40)  mstore(add(mPtr,0x40),mload(dst))  mstore(add(mPtr,0x60),mload(add(dst,0x20)))  l_success := and(l_success, staticcall(sub(gas(), 2000),6,mPtr,0x80,dst, 0x40))  mstore(add(state, state_success), and(l_success,mload(add(state, state_success))))  Recommendation  Add scalar field range check on scalar multiplication functions point_mul and point_acc_mul or the scalar field proof elements.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.9 Missing Public Inputs Range Check    ", "body": "  Description  The public input is an array of uint256 numbers, there is no check if each public input is less than SNARK scalar field  modulus r_mod, as mentioned in the step 3 of the verifier s algorithm in the Plonk paper. Since public inputs are involved computation of Pi in the plonk gate which is in the SNARK scalar field, without the check, it might cause scalar field overflow and the verification contract would fail and revert. To prevent overflow and other unintended behavior there should be a range check for the public inputs.  Examples  contracts/Verifier.sol:L470  function Verify(bytes memory proof, uint256[] memory public_inputs)  contracts/Verifier.sol:L367-L383  sum_pi_wo_api_commit(add(public_inputs,0x20), mload(public_inputs), zeta)  pi := mload(mload(0x40))  function sum_pi_wo_api_commit(ins, n, z) {  let li := mload(0x40)  batch_compute_lagranges_at_z(z, n, li)  let res := 0  let tmp := 0  for {let i:=0} lt(i,n) {i:=add(i,1)}  tmp := mulmod(mload(li), mload(ins), r_mod)  res := addmod(res, tmp, r_mod)  li := add(li, 0x20)  ins := add(ins, 0x20)  mstore(mload(0x40), res)  Recommendation  Add range check for the public inputs require(input[i] < r_mod, \"public inputs greater than snark scalar field\");  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.10 Missing Field Element Check and on Curve Point Check for Proof Elements   ", "body": "  Description  There is no prime field element check and on curve point for proof elements proof_l_com_x, proof_l_com_y,proof_r_com_x,proof_r_com_y, proof_o_com_x, proof_o_com_y, proof_h_0_x, proof_h_0_y,proof_h_1_x, proof_h_1_y,proof_h_2_x, proof_h_2_y, proof_batch_opening_at_zeta, proof_opening_at_zeta_omega, proof_selector_commit_api_commitment, as mentioned in  step 1 Validate ([a]1, [b]1, [c]1, [z]1, [tlo]1, [tmid]1, [thi]1, [Wz]1, [Wz\u03c9]1) \u2208 G9  of the verifier s algorithm in the Plonk paper. Although there is field element check and curve point check in ECCADD, ECCMUL and ECCParing precompiles on those elements, in which the precompile would revert on failed check but it would consume gas on revert and there is no error information. It s better to check explicitly and revert on fail to prevent unintended behavior of the verification contract.  Recommendation  Add field element, group element and curve point check for proof elements and revert if the check fails. `  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.11 Missing Length Check for proof    ", "body": "  Description  The Verify function has the following signature:  Also, if mistakenly appended extra bits to the proof, it will not affect the proof verification as the Verifier doesn t account for any extra bits after the y coordinate of the last commitment. But it will surely make the verification expensive, as it will still be copied down into memory.  Recommendation  Add an appropriate length check at some point in the pipeline to ensure this doesn t cause any unintended problems.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.12 Allowing Program Execution Even After a Failed Step May Lead to Unnecessary Wastage of Gas ", "body": "  Description  The Verifier stores the result of computations obtained in different steps of Verifier algorithm. The result is stored at a designated memory location state_success by doing bitwise & with the previous result, and if the final result at the end of all the steps comes out to be 1 or true, it verifies the proof.  However, it makes no sense to continue with the rest of the operations, if any step results into a failure, as the proof verification will be failing anyways. But, it will result into wastage of more gas for the zkEVM Operator.  The functions which update the state_success state are:  point_mul  point_add  point_acc_mul  verify_quotient_poly_eval_at_zeta  batch_verify_multi_points  Recommendation  It would be best to revert, the moment any step fails.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.13 Loading Arbitrary Data as Wire Commitments   ", "body": "  Description  Function load_wire_commitments_commit_api as the name suggests, loads wire commitments from the proof into the memory array wire_commitments. The array is made to hold 2 values per commitment or the size of the array is 2 * vk_nb_commitments_commit_api, which makes sense as these 2 values are the x & y co-ordinates of the commitments.  contracts/Verifier.sol:L453-L454  uint256[] memory wire_committed_commitments = new uint256[](2*vk_nb_commitments_commit_api);  load_wire_commitments_commit_api(wire_committed_commitments, proof);  Coming back to the functionload_wire_commitments_commit_api, it extracts both the x & y coordinates of a commitment in a single iteration. However, the loop runs 2 * vk_nb_commitments_commit_api, or in other words, twice as many of the required iterations. For instance, if there is 1 commitment, it will run two times. The first iteration will pick up the actual coordinates and the second one can pick any arbitrary data from the proof(if passed) and load it into memory. Although, this data which has been loaded in an extra iteration seems harmless but still adds an overhead for the processing.  contracts/Verifier.sol:L307  for {let i:=0} lt(i, mul(vk_nb_commitments_commit_api,2)) {i:=add(i,1)}  Recommendation  The number of iterations should be equal to the size of commitments, i.e., vk_nb_commitments_commit_api. So consider switching from:  to:  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.14 Broken Logic for Batch Inverting Field Elements ", "body": "  Description  The function compute_pi calculates public input polynomial evaluation at \ud835\udf01 without api commit as: PI(\ud835\udf01) = \u2211i\u2208[\u2113] \u03c9iLi(\ud835\udf01)  The function first calculates: n-1 * (\ud835\udf01n-1)  and then calls a function batch_invert to find modular multiplicate inverses of: \ud835\udf01 - \u03c9i where 0 < i < n ; n is the number of public inputs and \ud835\udf010 = 1.  The explanation of which is provided by the author as:  However, it doesn t account for the fact that elements passed to the function can be 0 as well. The reason is, \ud835\udf01 can be a root of unity. Since the function first calculates an aggregated inverse, thus even if a single element is 0, the aggregated inverse will be 0, and thus all the individual inverses will be 0, which is contrary to the desired logic of finding individual inverses.  Recommendation  Consider fixing the computation as per the recommendations in issue 4.2  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.15 Unused State Fields   ", "body": "  Description  There are three state fields which exist but are neither defined nor used in the computation:  state_su and state_sv  state_alpha_square_lagrange_one  It is unclear whether or not these were intended to play a part or not.  Examples  contracts/Verifier.sol:L138-L140  // challenges related to KZG  uint256 constant state_sv = 0x80;  uint256 constant state_su = 0xa0;  contracts/Verifier.sol:L166  uint256 constant state_alpha_square_lagrange_one = 0x200;  Recommendation  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.16 Overwritten Assignment to state_success    ", "body": "  Description  In the function verify_quotient_poly_eval_at_zeta(aproof) we have the following two lines at the end:  mstore(add(state, state_success), mload(computed_quotient))  mstore(add(state, state_success),eq(mload(computed_quotient), mload(s2)))  In essence, this is writing to the state_success variable twice and, hence, the first assignment is lost.  Its unclear to me what exactly was intended here, but I m assuming the first assignment can be removed.  Examples  Recommendation  Determine the correct course of action: either removing the first assignment, or using an and() to combine the two values together.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.17 PlonkVerifier.abi Is Out of Sync ", "body": "  Description  Running make solc creates a PlonkVerifier.abi file that is empty ([]). This seems correct for the existing Verifier.sol file. In contrast, the committed PlonkVerifier.abi refers to a function PrintUint256 that is not in the current Verifier.sol.  This probably means that the committed PlonkVerifier.abi was generated from a different Verifier.sol file. It would be good to make sure that the committed files are correct.  Possibly related: the .bin files generated (using 0.8.19+commit.7dd6d404.Darwin.appleclang) are different from the committed ones, so they probably are also out of sync. Else, the exact compiler used should be documented.  Examples  Recommendation  Document the compiler used. Ensure that the committed files are consistent. Ensure that there is a single source of truth (1 solidity file + compiler is better than 1 solidity file + 1 bin file that can get out of sync).  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.18 Makefile: Target Order ", "body": "  Description  The target all in the Makefile ostensibly wants to run the targets clean and solc in that order.  all: clean solc  However prerequisites in GNU Make are not ordered, and they might even run in parallel. In this case, this could cause spurious behavior like overwrite errors or files being deleted just after being created.  Recommendation  The Make way to ensure that targets run one after the other is  all: clean  $(MAKE) solc  Also all should be listed in the PHONY targets.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.19 Floating Pragma", "body": "  Description  The Verifier can be compiled with any minor version of compiler 0.8. It may lead to inconsistent behavior or produce unintended results, due to the bugs that have been identified in specific compiler versions. For instance, an optimizer bug that was discovered in 0.8.13, which effects unused memory/storage writes in an inline assembly block, which is similar to the pattern being followed by the Verifier. Although, we have not observed a negative effect of the said bug, but we still recommend working with a fixed compiler version, to avoid potential compiler-specific issues.  contracts/Verifier.sol:L17  pragma solidity ^0.8.0;  Recommendation  The contract should be tested and compiled with a fixed compiler version  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.20 Deviation in Implementation From the Intended Approach/Plonk Paper & Other General Observations", "body": "  Description  We have observed some deviations in the implementation in some computations w.r.t the intended approach described by the author and also with the Plonk Paper. We have also observed some computations which require a thorough lookup  1- Function compute_gamma_kzg defines the process to derive an unpredictable value gamma with the following inputs mentioned by the authors  However, instead of using [Pii] and Pii(\u03b6), i.e., commitments to the custom gates  wire values and their evaluations at \u03b6, the function uses [Qci] and Qci(\u03b6), which are the commitments to the custom gates  selectors and their evaluations at \u03b6  contracts/Verifier.sol:L669-L671  mstore(add(mPtr,offset), vk_selector_commitments_commit_api_0_x)  mstore(add(mPtr,add(offset, 0x20)), vk_selector_commitments_commit_api_0_y)  offset := add(offset, 0x40)  2- Approach to compute linearized polynomial  Unlike the other assignment polynomials, where the input polynomial evaluation at \u03b6 is multiplied by its respective selector commitment, For instance: L(\u03b6)[Ql]  In the case of custom gates, it is being computed in reverse as: Qci(\u03b6)[Pii]  contracts/Verifier.sol:L726-L735  let commits_api_at_zeta := add(aproof, proof_openings_selector_commit_api_at_zeta)  let commits_api := add(aproof, add(proof_openings_selector_commit_api_at_zeta, mul(vk_nb_commitments_commit_api, 0x20)))  for {let i:=0} lt(i, vk_nb_commitments_commit_api) {i:=add(i,1)}  mstore(mPtr, mload(commits_api))  mstore(add(mPtr, 0x20), mload(add(commits_api, 0x20)))  point_acc_mul(l_state_linearised_polynomial,mPtr,mload(commits_api_at_zeta),add(mPtr,0x40))  commits_api_at_zeta := add(commits_api_at_zeta, 0x20)  commits_api := add(commits_api, 0x40)  3- Approach to compute linearized polynomial and quotient polynomial evaluation at zeta  The signs for the parts of the expressions to compute r\u0304 are reversed, if compared with the plonk paper  And the sign for the term in t\u00af computation has been reversed as well: \u03b1z\u00afw(l\u00af+\u03b2s\u00af\u03c31+\u03b3)(r\u00af+\u03b2s\u00af\u03c32+\u03b3)(o\u00af+\u03b3)  contracts/Verifier.sol:L832  mstore(computed_quotient, addmod(mload(computed_quotient), mload(s1), r_mod))  contracts/Verifier.sol:L782  s2 := sub(r_mod, s2)  Recommendation  There is a need to thoroughly review the highlighted code sections  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.21 Proof Size Can Be Reduced by Removing Linearization Polynomial Evaluation at Zeta", "body": "  Description  The Linearization Polynomial evaluation at zeta (r\u00af) can be removed from the proof as the verifier computes the Linearization Polynomial anyways. The Verifier s algorithm in the latest release of the plonk paper has also been updated, considering the same. However, it might require changing/adjusting a lot of code sections, hence it is not recommended for the current version of the verifier but definitely can be considered for future updates.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.22 Unnecessary Memory Read/Write Operations", "body": "  Description  Function verify_quotient_poly_eval_at_zeta tries to find whether m\u00af+PI(\u03b6)+\u03b1z\u00afw(l\u00af+\u03b2s\u00af\u03c31+\u03b3)(r\u00af+\u03b2s\u00af\u03c32+\u03b3)(o\u00af+\u03b3)\u2212L1(\u03b6)\u03b12 is equal to t\u00afZH(\u03b6) or not, where x\u00af=x(\u03b6) and m\u00af is the linearization polynomial evaluation at \u03b6  However, instead of directly working with local assembly variables, the function uses memory operations mload and mstore which is unnecessary and adds gas overhead for the calculations.  For instance,  Also, the function stores the computed quotient at the memory location of state_success as:  It is unnecessary as it is logically reserved for the calculation results, and also because of the next line which is actually storing the desired comparison result.  Recommendation  The same calculations can be achieved by simply using the local assembly variables. Also, the unnecessary memory written to state_success for computed_quotient mentioned above can be removed.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.23 Unused Evaluation of Z(x)-1 at Zeta   ", "body": "  Description  The solidity variable zeta_power_n_minus_one is defined at Line 361, and calculated at Lines 445 and 446.  However, this variable does not appear to be actually used anywhere.   Instead, there are several places where the value is recalculated from scratch.  Examples  contracts/Verifier.sol:L361  uint256 zeta_power_n_minus_one;  contracts/Verifier.sol:L445-L446  zeta_power_n_minus_one := pow(zeta, vk_domain_size, mload(0x40))  zeta_power_n_minus_one := addmod(zeta_power_n_minus_one, sub(r_mod, 1), r_mod)  Recommendation  Presumably, this variable can be removed without any consequences.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.24 Spurious Debugging Code", "body": "  Description  There are two examples of debugging code left within the code base.  Examples  check := mload(add(mem, state_check_var))  mstore(add(state, state_check_var), acc_gamma)  mstore(add(state, state_check_var), mload(add(folded_quotients, 0x20)))  Recommendation  Remove debugging code prior to deployment.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.25 Utils - Possible Code Improvements", "body": "  Descriptions and Recommendations  The contract defines function hash_fr to calculate field elements over finite field F from message hash, expanded with expand_msg. However, we found opportunities for a couple of code improvements that may enhance code readability.  A. The following code excerpt prepends a byte string of 64 zeroes for the padding Z_pad  contracts/Utils.sol:L39-L41  for (uint i=0; i<64; i++){  tmp = abi.encodePacked(tmp, zero);  However, as it is a static value and need not be calculated dynamically, it can be simplified by predefining it as an initial value for tmp as:  bytes memory tmp = hex'00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000';  B. The following code excerpt performs a xor operation with b0 and b1 bytes32 hash strings in a for a loop.  contracts/Utils.sol:L51-L56  tmp = abi.encodePacked(uint8(b0[0]) ^ uint8(b1[0]));  for (uint i=1; i<32; i++){  tmp = abi.encodePacked(tmp, uint8(b0[i]) ^ uint8(b1[i]));  tmp = abi.encodePacked(tmp, uint8(2), dst, sizeDomain);  Instead of performing xor operation byte-by-byte, solidity provides the capability to xor the whole fixed byte string at once, hence it can be simplified as:  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-plonk-verifier/"}, {"title": "4.1 Initialization flaws    ", "body": "  Resolution                           This has been fixed in   Description  OpenZeppelin Contracts Upgradeable is to have a  Examples  The __ERC20WrapperGluwacoin_init function is implemented as follows:  code/contracts/ERC20WrapperGluwacoin.sol:L36-L48  function __ERC20WrapperGluwacoin_init(  string memory name,  string memory symbol,  IERC20 token  ) internal initializer {  __Context_init_unchained();  __ERC20_init_unchained(name, symbol);  __ERC20ETHless_init_unchained();  __ERC20Reservable_init_unchained();  __AccessControlEnumerable_init_unchained();  __ERC20Wrapper_init_unchained(token);  __ERC20WrapperGluwacoin_init_unchained();  And the C3 linearization is:  The calls __ERC165_init_unchained(); and __AccessControl_init_unchained(); are missing, and __ERC20Wrapper_init_unchained(token); should move between __ERC20_init_unchained(name, symbol); and __ERC20ETHless_init_unchained();.  Recommendation  Review all *_init functions, add the missing *_init_unchained calls, and fix the order of these calls.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"}, {"title": "4.2 Flaw in _beforeTokenTransfer call chain and missing tests    ", "body": "  Resolution                           This has been fixed in   Description  In OpenZeppelin s ERC-20 implementation, the virtual _beforeTokenTransfer function provides a hook that is called before tokens are transferred, minted, or burned. In the Gluwacoin codebase, it is used to check whether the unreserved balance (as opposed to the regular balance, which is checked by the ERC-20 implementation) of the sender is sufficient to allow this transfer or burning.  In ERC20WrapperGluwacoin, ERC20Reservable, and ERC20Wrapper, the _beforeTokenTransfer function is implemented in the following way:  code/contracts/ERC20WrapperGluwacoin.sol:L54-L61  function _beforeTokenTransfer(  address from,  address to,  uint256 amount  ) internal override(ERC20Upgradeable, ERC20Wrapper, ERC20Reservable) {  ERC20Wrapper._beforeTokenTransfer(from, to, amount);  ERC20Reservable._beforeTokenTransfer(from, to, amount);  code/contracts/abstracts/ERC20Reservable.sol:L156-L162  function _beforeTokenTransfer(address from, address to, uint256 amount) internal virtual override (ERC20Upgradeable) {  if (from != address(0)) {  require(_unreservedBalance(from) >= amount, \"ERC20Reservable: transfer amount exceeds unreserved balance\");  super._beforeTokenTransfer(from, to, amount);  code/contracts/abstracts/ERC20Wrapper.sol:L176-L178  function _beforeTokenTransfer(address from, address to, uint256 amount) internal virtual override (ERC20Upgradeable) {  super._beforeTokenTransfer(from, to, amount);  Finally, the C3-linearization of the contracts is:  Moreover, while reviewing the correctness and coverage of the tests is not in scope for this engagement, we happened to notice that there are no tests that check whether the unreserved balance is sufficient for transferring or burning tokens.  Recommendation  ERC20WrapperGluwacoin._beforeTokenTransfer should just call super._beforeTokenTransfer. Moreover, the _beforeTokenTransfer implementation can be removed from ERC20Wrapper.  We would like to stress the importance of careful and comprehensive testing in general and of this functionality in particular, as it is crucial for the system s integrity. We also encourage investigating whether there are more such omissions and an evaluation of the test quality and coverage in general.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"}, {"title": "4.3 Hard-coded decimals    ", "body": "  Resolution                           In   Description  The Gluwacoin wrapper token should have the same number of decimals as the wrapped ERC-20. Currently, the number of decimals is hard-coded to 6. This limits flexibility or requires source code changes and recompilation if a token with a different number of decimals is to be wrapped.  code/contracts/ERC20WrapperGluwacoin.sol:L32-L34  function decimals() public pure override returns (uint8) {  return 6;  Recommendation  We recommend supplying the number of decimals as an initialization parameter and storing it in a state variable. That increases gas consumption of the decimals function, but we doubt this view function will be frequently called from a contract, and even if it was, we think the benefits far outweigh the costs. Moreover, we believe the decimals logic (i.e., function decimals and the new state variable) should be implemented in the ERC20Wrapper contract   which holds the basic ERC-20 functionality of the wrapper token   and not in ERC20WrapperGluwacoin, which is the base contract of the entire system.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/10/gluwacoin-erc-20-wrapper/"}, {"title": "4.1 Bridge Token Would Be Locked and Cannot Bridge to Native Token    ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   66 with the final commit hash as  Description  Examples  contracts/TokenBridge.sol:L217-L229  if (nativeMappingValue == NATIVE_STATUS) {  // Token is native on the local chain  IERC20(_nativeToken).safeTransfer(_recipient, _amount);  } else {  bridgedToken = nativeMappingValue;  if (nativeMappingValue == EMPTY) {  // New token  bridgedToken = deployBridgedToken(_nativeToken, _tokenMetadata);  bridgedToNativeToken[bridgedToken] = _nativeToken;  nativeToBridgedToken[_nativeToken] = bridgedToken;  BridgedToken(bridgedToken).mint(_recipient, _amount);  contracts/TokenBridge.sol:L272-L279  function setDeployed(address[] memory _nativeTokens) external onlyMessagingService fromRemoteTokenBridge {  address nativeToken;  for (uint256 i; i < _nativeTokens.length; i++) {  nativeToken = _nativeTokens[i];  nativeToBridgedToken[_nativeTokens[i]] = DEPLOYED_STATUS;  emit TokenDeployed(_nativeTokens[i]);  Recommendation  Add an condition nativeMappingValue = DEPLOYED_STATUS for native token transfer in confirmDeployment  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.2 User Cannot Withdraw Funds if Bridging Failed or Delayed   ", "body": "  Description  If the bridging failed due to the single coordinator is down, censoring the message, or bridge token contract is set to a bad or wrong contract address by setCustomContract, user s funds will stuck in the TokenBridge contract until coordinator is online or stop censoring, there is no way to withdraw the deposited funds  Examples  contracts/TokenBridge.sol:L341-L348  function setCustomContract(  address _nativeToken,  address _targetContract  ) external onlyOwner isNewToken(_nativeToken) {  nativeToBridgedToken[_nativeToken] = _targetContract;  bridgedToNativeToken[_targetContract] = _nativeToken;  emit CustomContractSet(_nativeToken, _targetContract);  Recommendation  Add withdraw functionality to let user withdraw the funds under above circumstances or at least add withdraw functionality for Admin (admin can send the funds to the user manually), ultimately decentralize coordinator and sequencer to reduce bridging failure risk.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.3 Bridges Don t Support Multiple Native Tokens, Which May Lead to Incorrect Bridging    ", "body": "  Resolution  The recommendations are implemented by the Linea team in the pull request 65 with the final commit hash as 27c2e53c8da206d1d2e06abbdadee1e728219763. The Bridges now support only one native token and revert, if attempted to bridge a native token with the same address on the other layer. The team mentions the reason for choosing this design is to support their specific use case, and for the cases where the users want to have same native tokens on both the layers, third-party liquidity bridges would be a better option.  Note:- Although, the introduced check adds a scenario, where if the owner adds a bridge for a native token on the source layer via setCustomContract , the protocol will disallow any bridging from it. However, after having a discussion, the team concluded that the function setCustomContract  is intended to be called on the destination layer and the team will take the necessary precautions while dealing with it.  Update: A related issue 4.7 with a similar root cause has also been identified and addressed with a fix that also correct this issue.  Description  Currently, the system design does not support the scenarios where native tokens with the same addresses (which is possible with the same deployer and nonce) on different layers can be bridged.  The reason is the mappings don t differentiate between the native tokens on two different Layers.  Examples  contracts/TokenBridge.sol:L208-L220  function completeBridging(  address _nativeToken,  uint256 _amount,  address _recipient,  bytes calldata _tokenMetadata  ) external onlyMessagingService fromRemoteTokenBridge {  address nativeMappingValue = nativeToBridgedToken[_nativeToken];  address bridgedToken;  if (nativeMappingValue == NATIVE_STATUS) {  // Token is native on the local chain  IERC20(_nativeToken).safeTransfer(_recipient, _amount);  } else {  Recommendation  Redesign the approach to handle the same native tokens on different layers. One possible approach could be to define the set of mappings for each layer.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.4 No Check for Initializing Parameters of TokenBridge    ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   68 with the final commit hash as  Description  Examples  contracts/TokenBridge.sol:L97-L111  function initialize(  address _securityCouncil,  address _messageService,  address _tokenBeacon,  address[] calldata _reservedTokens  ) external initializer {  __Pausable_init();  __Ownable_init();  setMessageService(_messageService);  tokenBeacon = _tokenBeacon;  for (uint256 i = 0; i < _reservedTokens.length; i++) {  setReserved(_reservedTokens[i]);  _transferOwnership(_securityCouncil);  Recommendation  Add non-zero address check for _securityCouncil, _messageService, _tokenBeacon and _reservedTokens  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.5 Owner Can Update Arbitrary Status for New Native Token Without Confirmation    ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   67 with the final commit hash as  Description  The function setCustomContract allows the owner to update arbitrary status for new native tokens without confirmation, bypassing the bridge protocol.  It can set DEPLOYED_STATUS for a new native token, even if there exists no bridged token for it.  It can set NATIVE_STATUS for a new native token even if it s not.  It can set RESERVED_STATUS disallowing any new native token to be bridged.  Examples  contracts/TokenBridge.sol:L341-L348  function setCustomContract(  address _nativeToken,  address _targetContract  ) external onlyOwner isNewToken(_nativeToken) {  nativeToBridgedToken[_nativeToken] = _targetContract;  bridgedToNativeToken[_targetContract] = _nativeToken;  emit CustomContractSet(_nativeToken, _targetContract);  Recommendation  The function should not allow _targetContract to be any state code  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.6 Owner May Exploit Bridged Tokens    ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   67 with the final commit hash as  Description  The function setCustomContract allows the owner, to define a custom ERC20 contract for the native token. However, it doesn t check whether the target contract has already been defined as a bridge to a native token or not. As a result, the owner may take advantage of the design flaw and bridge another new native token that has not been bridged yet, to an already existing target(already a bridge for another native token). Now, if a user tries to bridge this native token, the token bridge on the source chain will take the user s tokens, and instead of deploying a new bridge on the destination chain, tokens will be minted to the _recipient on an existing bridge defined by the owner, or it can be any random EOA address to create a DoS.  The owner can also try to front-run calls to completeBridging for new Native Tokens on the destination chain, by setting a different bridge via setCustomContract. Although, the team states that the role will be controlled by a multi-sig which makes frontrunning less likely to happen.  Examples  contracts/TokenBridge.sol:L341-L348  function setCustomContract(  address _nativeToken,  address _targetContract  ) external onlyOwner isNewToken(_nativeToken) {  nativeToBridgedToken[_nativeToken] = _targetContract;  bridgedToNativeToken[_targetContract] = _nativeToken;  emit CustomContractSet(_nativeToken, _targetContract);  contracts/TokenBridge.sol:L220-L229  } else {  bridgedToken = nativeMappingValue;  if (nativeMappingValue == EMPTY) {  // New token  bridgedToken = deployBridgedToken(_nativeToken, _tokenMetadata);  bridgedToNativeToken[bridgedToken] = _nativeToken;  nativeToBridgedToken[_nativeToken] = bridgedToken;  BridgedToken(bridgedToken).mint(_recipient, _amount);  Recommendation  Make sure, a native token should bridge to a single target contract. A possible approach could be to check whether the bridgedToNativeToken for a target is EMPTY or not. If it s not EMPTY, it means it s already a bridge for a native token and the function should revert. The same can be achieved by adding the modifier isNewToken(_targetContract).  Note:- However, it doesn t resolve the issue of frontrunning, even if the likelihood is less.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.7 Incorrect Bridging Due to Address Collision & Inconsistent State of Native Tokens    ", "body": "  Resolution                           The Linea team has proposed another   PR to fix the issues associated with token address collision during bridging. Namely, the chain ID associated with the token s native chain is now introduced as a key to the token address mapping, and the chain ID is added to the salt when deploying bridged token contracts, ensuring uniqueness. Consequently, this PR addresses the raised concerns though introduction of a more complex state to the bridge contract may raise the difficulty of upgrading the contract, thus redeployments are recommended in future for any changes.  Description  In the second round of the audit, we discovered an edge case that may exist because of an address collision of native tokens. In the first round, we found issue 4.3 explaining how the bridges only support a single native token on both layers and may cause incorrect bridging. In response to that, the Linea team implemented a change that reverts whenever there is an attempt to bridge a native token with the same address on the other layer.  However, the issue still exists because of the inconsistent state of native tokens while bridging. The reason is, there could be an attempt to bridge a token with the same address on both layers at the same time, which could be done deliberately by an attacker by monitoring the bridging call at source layer and frontrunning them on the destination layer. As a consequence, both the tokens will get the NATIVE_STATUS on both layers, as the bridges can t check the state of a token on the other layer while bridging. Now, the bridging that was initiated for a native token on the source layer will be completed with the native token on the destination layer, as the bridging was initiated at the same time.  For the issue, the Linea team came back with the solution in the PR 1041 with the final commit a875e67e0681ce387825127a08f1f924991a274c. The solution implemented adds a flag _isNativeLayer while sending the message to Message Service on source layer  and is used to verify the state of native token on destination layer while calling completeBridging.  The logic adds two conditional checks: 1- If _isNativeLayer  is false, it means the token is not native on the source layer, and if for the same address the status is either Native or Deployed, then the native token of the destination layer should be bridged. 2- If the flag is true, it means the token is native on the source layer, and if there exists a collision of the Native or Deployed status, then a new bridge token should be created.  As an example, the B tokens can be bridged to mint C bridgedTokens and vice-versa as and when needed. So, now it is mandatory to call confirmDeployment in order to allow condition 1 to be satisfied for this bridging and avoid this kind of bad minting.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.8 Updating Message Service Does Not Emit Event    ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   69 with the final commit hash as  Description  The function setMessageService allows the owner to update the message service address. However, it does not emit any event reflecting the change. As a result, in case the owner gets compromised, it can silently add a malicious message service, exploiting users  funds. Since, there was no event emitted, off-chain monitoring tools wouldn t be able to trigger alarms and users would continue using rogue message service until and unless tracked manually.  Examples  contracts/TokenBridge.sol:L237-L240  function setMessageService(address _messageService) public onlyOwner {  messageService = IMessageService(_messageService);  Recommendation  Consider emitting an event reflecting the update from the old message service to the new one.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.9 Lock Solidity Version in pragma ", "body": "  Description  Contracts should be deployed with the same compiler version they have been tested with.  Locking the pragma helps ensure that contracts do not accidentally get deployed using, for example, the latest compiler which may have higher risks of undiscovered bugs. Contracts may also be deployed by others and the pragma indicates the compiler version intended by the original authors.  See Locking Pragmas in Ethereum Smart Contract Best Practices.  Examples  contracts/interfaces/IMessageService.sol:L2  pragma solidity ^0.8.19;  contracts/BridgedToken.sol:L2  pragma solidity ^0.8.19;  contracts/TokenBridge.sol:L2  pragma solidity ^0.8.19;  contracts/interfaces/ITokenBridge.sol:L2  pragma solidity ^0.8.19;  Recommendation  Lock the Solidity version to the latest version before deploying the contracts to production.  pragma solidity 0.8.19;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.10 Upgradeability Concerns    ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   70 with the final commit hash as  Description  1- Using Standard Interfaces and Libraries instead of Upgradeable ones.  contracts/TokenBridge.sol:L5-L10  import { IERC20Permit } from \"@openzeppelin/contracts/token/ERC20/extensions/draft-IERC20Permit.sol\";  import { IERC20Metadata } from \"@openzeppelin/contracts/token/ERC20/extensions/IERC20Metadata.sol\";  import { IERC20 } from \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";  import { OwnableUpgradeable } from \"@openzeppelin/contracts-upgradeable/access/OwnableUpgradeable.sol\";  import { PausableUpgradeable } from \"@openzeppelin/contracts-upgradeable/security/PausableUpgradeable.sol\";  import { SafeERC20 } from \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";  We recommend using upgradeable interfaces and libraries to avoid any unexpected issues while contract upgrades, also increasing code readability.  2- Using Deprecated Files The contract imports file draft-IERC20Permit from the npm module openzeppelin/contracts.  contracts/TokenBridge.sol:L5  import { IERC20Permit } from \"@openzeppelin/contracts/token/ERC20/extensions/draft-IERC20Permit.sol\";  However, the file has been deprecated now, as the EIP2612 has been finalized. We recommend using the correct file which is extensions/IERC20Permit.sol or the corresponding upgradeable one.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.11 TokenBridge Does Not Follow a 2-Step Approach for Ownership Transfers    ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   71 with the final commit hash as  Description  TokenBridge defines a privileged role Owner, however, it uses a single-step approach, which immediately transfers the ownership to the new address. If accidentally passed an incorrect address, the current owner will immediately lose control over the system as there is no fail-safe mechanism.  A safer approach would be to first propose the ownership to the new owner, and let the new owner accept the proposal to be the new owner. It will add a fail-safe mechanism for the current owner as in case it proposes ownership to an incorrect address, it will not immediately lose control, and may still propose again to a correct address.  Examples  contracts/TokenBridge.sol:L22  contract TokenBridge is ITokenBridge, PausableUpgradeable, OwnableUpgradeable {  Recommendation  Consider moving to a 2-step approach for the ownership transfers as recommended above. Note:-  Openzeppelin provides another helper utility as Ownable2StepUpgradeable which follows the recommended approach  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "4.12 Code Corrections   ", "body": "  Resolution                           The recommendations are implemented by the Linea team in the pull request   72 with the final commit hash as  Description  1- Importing ITokenBridge twice The contract defines the import of interface ITokenBridge twice, out of which one can be removed.  contracts/TokenBridge.sol:L4  import { ITokenBridge } from \"./interfaces/ITokenBridge.sol\";  contracts/TokenBridge.sol:L14  import { ITokenBridge } from \"./interfaces/ITokenBridge.sol\";  2- Using bytes32 for a bytes4 selector The contract stores a 4-byte selector of _PERMIT_SELECTOR in a bytes32 type.  contracts/TokenBridge.sol:L24-L25  bytes32 private constant _PERMIT_SELECTOR =  bytes4(keccak256(bytes(\"permit(address,address,uint256,uint256,uint8,bytes32,bytes32)\")));  The selector is then compared with the first 4 bytes of _permitData to make sure, the calldata is indeed a calldata of permit function.  contracts/TokenBridge.sol:L435-L436  if (bytes4(_permitData[:4]) != _PERMIT_SELECTOR)  revert InvalidPermitData(bytes4(_permitData[:4]), _PERMIT_SELECTOR);  The check will work as intended, however, if it fails, the error will revert with info containing 32 bytes of _PERMIT_SELECTOR as 0xd505accf00000000000000000000000000000000000000000000000000000000, which may create unnecessary confusion for end users. As a recommendation, consider using a bytes4 type for _PERMIT_SELECTOR and also for the custom error InvalidPermitData.  contracts/interfaces/ITokenBridge.sol:L20  error InvalidPermitData(bytes4 permitData, bytes32 permitSelector);  5 Findings from Other Sources  This section includes Issues that are reported externally by the Whitehats apart from this audit, either via bug bounty platforms or with other means of responsible disclosure. The details are included as is as shared by the Whitehats.  ERC-777 tokens can re-enter TokenBridge s deposit function and bridge an unlimited amount of tokens  The TokenBridge contract implements a L1 <-> L2 token bridge that is open to be used with any token. Tokens supporting the ERC-777 standard provide before and after token transfer hooks. The before transfer hook is invoked on the sender, while the after transfer hook is invoked on the recipient. An attacker can abuse this behaviour by re-entering the deposit function over and over, artificially inflating the amount of bridged tokens. Thi is due to the TokenBridge caching the token balance before transfer:  By re-entering the function multiple times with a minimal amount, for example 1 wei, and sending a larger amount during the final re-entrancy, all invokations of the function will calculate _amount as the final larger amount. This results in the actually deposited amount being multiplied by the number of re-entrancies. This mints more tokens to the attacker than should be, that could then be bridged-back to withdraw the excess funds of other previous legitimate depositors, or kept on the remote chain and sold there.  Recommendation: Mark the deposit function as nonReentrant, for example by using OZ s ReentrancyGuard.  Our Review: Following the recommendation, the team added the Reentrancy Guard, which the auditing team has reviewed. But adding the Reentrancy Guard in the inheritance hierarchy may mess up the storage layout while making the upgrade, as Openzeppelin s ReentrancyGuardUpgradeable pushes more buffer space and may collide with the existing storage slot, so we recommend either going for a fresh deployment or making necessary changes in the files to make sure there is no collision and a consistent storage layout.  Update from Linea Team: The recommendation was taken into consideration and a fresh version was deployed on the Mainnet, which is also the first version deployed on the Mainnet covering the fixes.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/06/linea-canonical-token-bridge/"}, {"title": "5.1 Anyone can remove a maker s pending pool join status    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2250 by removing the two-step handshake for a maker to join a pool.  Description  Using behavior described in issue 5.6, it is possible to delete the pending join status of any maker in any pool by passing in NIL_POOL_ID to removeMakerFromStakingPool. Note that the attacker in the following example must not be a confirmed member of any pool:  The attacker calls addMakerToStakingPool(NIL_POOL_ID, makerAddress). In this case, makerAddress can be almost any address, as long as it has not called joinStakingPoolAsMaker (an easy example is address(0)). The key goal of this call is to increment the number of makers in pool 0: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L262 _poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();  The attacker calls removeMakerFromStakingPool(NIL_POOL_ID, targetAddress). This function queries getStakingPoolIdOfMaker(targetAddress) and compares it to the passed-in pool id. Because the target is an unconfirmed maker, their staking pool id is NIL_POOL_ID: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L166-L173 bytes32 makerPoolId = getStakingPoolIdOfMaker(makerAddress); if (makerPoolId != poolId) {     LibRichErrors.rrevert(LibStakingRichErrors.MakerPoolAssignmentError(         LibStakingRichErrors.MakerPoolAssignmentErrorCodes.MakerAddressNotRegistered,         makerAddress,         makerPoolId     )); }  The check passes, and the target s _poolJoinedByMakerAddress struct is deleted. Additionally, the number of makers in pool 0 is decreased:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177  delete _poolJoinedByMakerAddress[makerAddress];  _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  This can be used to prevent any makers from being confirmed into a pool.  Recommendation  See issue 5.6.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.2 Delegated stake weight reduction can be bypassed by using an external contract   ", "body": "  Resolution  From the development team:  Although it is possible to bypass the weight reduction via external smart contracts, we believe there is some value to having a lower delegated stake weight as the default behavior. This can still approximate the intended behavior and should give a very slight edge to pool operators that own their stake.  Description  Staking pools allow ZRX holders to delegate their staked ZRX to a market maker in exchange for a configurable percentage of the stake reward (accrued over time through exchange fees). When staking as expected through the 0x contracts, the protocol favors ZRX staked directly by the operator of the pool, assigning a lower weight (90%) to ZRX staked by delegation. In return, delegated members receive a configurable portion of the operator s stake reward.  Using a smart contract, it is possible to represent ZRX owned by any number of parties as ZRX staked by a single party. This contract can serve as the operator of a pool with a single member\u2014itself. The advantages are clear for ZRX holders:  ZRX staked through this contract will be given full (100%) stake weight.  Because stake weight is a factor in reward allocation, the ZRX staked through this contract receives a higher proportion of the stake reward.  Recommendation  Remove stake weight reduction for delegated stake.  ", "labels": ["Consensys", "Major", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.3 MixinParams.setParams bypasses safety checks made by standard StakingProxy upgrade path.    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2279. Now the parameter validity is asserted in  Description  The staking contracts use a set of configurable parameters to determine the behavior of various parts of the system. The parameters dictate the duration of epochs, the ratio of delegated stake weight vs operator stake, the minimum pool stake, and the Cobb-Douglas numerator and denominator. These parameters can be configured in two ways:  An authorized address can deploy a new Staking contract (perhaps with altered parameters), and configure the StakingProxy to delegate to this new contract. This is done by calling   StakingProxy.detachStakingContract: code/contracts/staking/contracts/src/StakingProxy.sol:L82-L90 /// @dev Detach the current staking contract. /// Note that this is callable only by an authorized address. function detachStakingContract()     external     onlyAuthorized {     stakingContract = NIL_ADDRESS;     emit StakingContractDetachedFromProxy(); }   StakingProxy.attachStakingContract(newContract): code/contracts/staking/contracts/src/StakingProxy.sol:L72-L80 /// @dev Attach a staking contract; future calls will be delegated to the staking contract. /// Note that this is callable only by an authorized address. /// @param _stakingContract Address of staking contract. function attachStakingContract(address _stakingContract)     external     onlyAuthorized {     _attachStakingContract(_stakingContract); }   During the latter call, the StakingProxy performs a delegatecall to Staking.init, then checks the values of the parameters set during initialization: code/contracts/staking/contracts/src/StakingProxy.sol:L208-L219 // Call `init()` on the staking contract to initialize storage. (bool didInitSucceed, bytes memory initReturnData) = stakingContract.delegatecall(     abi.encodeWithSelector(IStorageInit(0).init.selector) ); if (!didInitSucceed) {     assembly {         revert(add(initReturnData, 0x20), mload(initReturnData))     } }  // Assert initialized storage values are valid _assertValidStorageParams();  An authorized address can call MixinParams.setParams at any time and set the contract s parameters to arbitrary values.  The latter method introduces the possibility of setting unsafe or nonsensical values for the contract parameters: epochDurationInSeconds can be set to 0, cobbDouglassAlphaNumerator can be larger than cobbDouglassAlphaDenominator, rewardDelegatedStakeWeight can be set to a value over 100% of the staking reward, and more.  Note, too, that by using MixinParams.setParams to set all parameters to 0, the Staking contract can be re-initialized by way of Staking.init. Additionally, it can be re-attached by way of StakingProxy.attachStakingContract, as the delegatecall to Staking.init will succeed.  Recommendation  Ensure that calls to setParams check that the provided values are within the same range currently enforced by the proxy.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.4 Authorized addresses can indefinitely stall ZrxVaultBackstop catastrophic failure mode    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2295 by removing the  Description  The ZrxVaultBackstop contract was added to allow anyone to activate the staking system s  catastrophic failure  mode if the StakingProxy is in  read-only  mode for at least 40 days. To enable this behavior, the StakingProxy contract was modified to track the last timestamp at which  read-only  mode was activated. This is done by way of StakingProxy.setReadOnlyMode:  code/contracts/staking/contracts/src/StakingProxy.sol:L92-L104  /// @dev Set read-only mode (state cannot be changed).  function setReadOnlyMode(bool shouldSetReadOnlyMode)  external  onlyAuthorized  // solhint-disable-next-line not-rely-on-time  uint96 timestamp = block.timestamp.downcastToUint96();  if (shouldSetReadOnlyMode) {  stakingContract = readOnlyProxy;  readOnlyState = IStructs.ReadOnlyState({  isReadOnlyModeSet: true,  lastSetTimestamp: timestamp  });  Because the timestamp is updated even if  read-only  mode is already active, any authorized address can prevent ZrxVaultBackstop from activating catastrophic failure mode by repeatedly calling setReadOnlyMode.  Recommendation  If  read-only  mode is already active, setReadOnlyMode(true) should result in a no-op.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.5 Pool 0 can be used to temporarily prevent makers from joining another pool    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1.  Description  removeMakerFromStakingPool reverts if the number of makers currently in the pool is 0, due to safeSub catching an underflow:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L177  _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  Because of this, edge behavior described in issue 5.6 can allow an attacker to temporarily prevent makers from joining a pool:  The attacker calls addMakerToStakingPool(NIL_POOL_ID, victimAddress). This sets the victim s MakerPoolJoinStatus.confirmed field to true and increases the number of makers in pool 0 to 1: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L257-L262 poolJoinStatus = IStructs.MakerPoolJoinStatus({     poolId: poolId,     confirmed: true }); _poolJoinedByMakerAddress[makerAddress] = poolJoinStatus; _poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();  The attacker calls removeMakerFromStakingPool(NIL_POOL_ID, randomAddress). The net effect of this call simply decreases the number of makers in pool 0 by 1, back to 0: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177 delete _poolJoinedByMakerAddress[makerAddress]; _poolById[poolId].numberOfMakers = uint256(_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();  Typically, the victim should be able to remove themselves from pool 0 by calling removeMakerFromStakingPool(NIL_POOL_ID, victimAddress), but because the attacker can set the pool s number of makers to 0, the aforementioned underflow causes this call to fail. The victim must first understand what is happening in MixinStakingPool before they are able to remedy the situation:  The victim must call addMakerToStakingPool(NIL_POOL_ID, randomAddress2) to increase pool 0 s number of makers back to 1.  The victim can now call removeMakerFromStakingPool(NIL_POOL_ID, victimAddress), and remove their confirmed status.  Additionally, if the victim in question currently has a pending join, the attacker can use issue 5.1 to first remove their pending status before locking them in pool 0.  Recommendation  See issue 5.1.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.6 Recommendation: Fix weak assertions in MixinStakingPool stemming from use of NIL_POOL_ID    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1.  Description  The modifier onlyStakingPoolOperatorOrMaker(poolId) is used to authorize actions taken on a given pool. The sender must be either the operator or a confirmed maker of the pool in question. However, the modifier queries getStakingPoolIdOfMaker(maker), which returns NIL_POOL_ID if the maker s MakerPoolJoinStatus struct is not confirmed. This implicitly makes anyone a maker of the nonexistent  pool 0 :  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L189-L200  function getStakingPoolIdOfMaker(address makerAddress)  public  view  returns (bytes32)  IStructs.MakerPoolJoinStatus memory poolJoinStatus = _poolJoinedByMakerAddress[makerAddress];  if (poolJoinStatus.confirmed) {  return poolJoinStatus.poolId;  } else {  return NIL_POOL_ID;  joinStakingPoolAsMaker(poolId) makes no existence checks on the provided pool id, and allows makers to become pending makers in nonexistent pools.  addMakerToStakingPool(poolId, maker) makes no existence checks on the provided pool id, allowing makers to be added to nonexistent pools (as long as the sender is an operator or maker in the pool).  Recommendation  Avoid use of 0x00...00 for NIL_POOL_ID. Instead, use 2**256 - 1.  Implement stronger checks for pool existence. Each time a pool id is supplied, it should be checked that the pool id is between 0 and nextPoolId.  onlyStakingPoolOperatorOrMaker should revert if poolId == NIL_POOL_ID or if poolId is not in the valid range: (0, nextPoolId).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.7 LibMath functions fail to catch a number of overflows    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2255 and  0xProject/0x-monorepo#2311.  Description  The __add(), __mul(), and __div() functions perform arithmetic on 256-bit signed integers, and they all miss some specific overflows.  Addition Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L359-L376  /// @dev Adds two numbers, reverting on overflow.  function _add(int256 a, int256 b) private pure returns (int256 c) {  c = a + b;  if (c > 0 && a < 0 && b < 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.SUBTRACTION_OVERFLOW,  a,  ));  if (c < 0 && a > 0 && b > 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.ADDITION_OVERFLOW,  a,  ));  The two overflow conditions it tests for are:  Adding two positive numbers shouldn t result in a negative number.  Adding two negative numbers shouldn t result in a positive number.  __add(-2**255, -2**255) returns 0 without reverting because the overflow didn t match either of the above conditions.  Multiplication Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L332-L345  /// @dev Returns the multiplication two numbers, reverting on overflow.  function _mul(int256 a, int256 b) private pure returns (int256 c) {  if (a == 0) {  return 0;  c = a * b;  if (c / a != b) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.MULTIPLICATION_OVERFLOW,  a,  ));  The function checks via division for most types of overflows, but it fails to catch one particular case. __mul(-2**255, -1) returns -2**255 without error.  Division Overflows  code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L347-L357  /// @dev Returns the division of two numbers, reverting on division by zero.  function _div(int256 a, int256 b) private pure returns (int256 c) {  if (b == 0) {  LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(  LibFixedMathRichErrors.BinOpErrorCodes.DIVISION_BY_ZERO,  a,  ));  c = a / b;  It does not check for overflow. Due to this, __div(-2**255, -1) erroneously returns -2**255.  Recommendation  For addition, the specific case of __add(-2**255, -2**255) can be detected by using a >= 0 check instead of > 0, but the below seems like a clearer check for all cases:  // if b is negative, then the result should be less than a  if (b < 0 && c >= a) { /* subtraction overflow */ }  // if b is positive, then the result should be greater than a  if (b > 0 && c <= a) { /* addition overflow */ }  For multiplication and division, the specific values of -2**255 and -1 are the only missing cases, so that can be explicitly checked in the __mul() and __div() functions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.8 Recommendation: Remove MixinAbstract and fold MixinStakingPoolRewards into MixinFinalizer and MixinStake   ", "body": "  Resolution  The development team investigated this suggestion, but they were ultimately uncomfortable making such a large change in this cycle. This can be considered again in a future version of the code.  Description  issue 5.12,  issue 5.11,  issue 5.10, and  issue 5.9,  Move MixinStakingPoolRewards.withdrawDelegatorRewards into MixinStake. As per the comments above this function, its behavior is very similar to functions in MixinStake: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L35-L56 /// @dev Syncs rewards for a delegator. This includes transferring WETH ///      rewards to the delegator, and adding/removing ///      dependencies on cumulative rewards. ///      This is used by a delegator when they want to sync their rewards ///      without delegating/undelegating. It's effectively the same as ///      delegating zero stake. /// @param poolId Unique id of pool. function withdrawDelegatorRewards(bytes32 poolId)     external {     address member = msg.sender;      _withdrawAndSyncDelegatorRewards(         poolId,         member     );      // Update stored balance with synchronized version; this prevents     // redundant withdrawals.     _delegatedStakeToPoolByOwner[member][poolId] =         _loadSyncedBalance(_delegatedStakeToPoolByOwner[member][poolId]); }  Move the rest of the MixinStakingPoolRewards functions into MixinFinalizer. This change allows the MixinStakingPoolRewards and MixinAbstract files to be removed. MixinStakingPool can now inherit directly from MixinFinalizer.  After implementing all recommendations mentioned here, the inheritance graph of the staking contracts is much simpler. The previous graph is pictured here:  The new graph is pictured here:  Further improvements may consider:  Having MixinStorage inherit MixinConstants and IStakingEvents  Moving _loadCurrentBalance into MixinStorage. Currently MixinStakeBalances only inherits from MixinStakeStorage because of this function.  After implementing the above, MixinExchangeFees is no longer dependent on MixinStakingPool and can inherit directly from MixinExchangeManager  A sample inheritance graph including the above is pictured below:  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.9 Recommendation: remove confusing access to activePoolsThisEpoch    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2276. Along with other state cleanup, these functions and  Description  MixinFinalizer provides two functions to access activePoolsThisEpoch:  _getActivePoolsFromEpoch returns a storage pointer to the mapping: code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L211-L225 /// @dev Get a mapping of active pools from an epoch. ///      This uses the formula `epoch % 2` as the epoch index in order ///      to reuse state, because we only need to remember, at most, two ///      epochs at once. /// @return activePools The pools that were active in `epoch`. function _getActivePoolsFromEpoch(     uint256 epoch )     internal     view     returns (mapping (bytes32 => IStructs.ActivePool) storage activePools) {     activePools = _activePoolsByEpoch[epoch % 2];     return activePools; }  _getActivePoolFromEpoch invokes _getActivePoolsFromEpoch, then loads an ActivePool struct from a passed-in poolId: code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L195-L209 /// @dev Get an active pool from an epoch by its ID. /// @param epoch The epoch the pool was/will be active in. /// @param poolId The ID of the pool. /// @return pool The pool with ID `poolId` that was active in `epoch`. function _getActivePoolFromEpoch(     uint256 epoch,     bytes32 poolId )     internal     view     returns (IStructs.ActivePool memory pool) {     pool = _getActivePoolsFromEpoch(epoch)[poolId];     return pool; }  Ultimately, the two functions are syntax sugar for activePoolsThisEpoch[epoch % 2], with the latter also accessing a value within the mapping. Because of the naming similarity, and because one calls the other, this abstraction is more confusing that simply accessing the state variable directly.  Additionally, by removing these functions and adopting the long-form syntax, MixinExchangeFees no longer needs to inherit MixinFinalizer.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.10 Recommendation: remove MixinFinalizer._getUnfinalizedPoolRewardsFromState   ", "body": "  Resolution  The development team decided to keep this function for its optimization on storage loads. It s will still be used internally by getters that are important for client-side code.  Description  MixinFinalizer._getUnfinalizedPoolRewardsFromState is a simple wrapper around the library function LibCobbDouglas.cobbDouglas:  code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L250-L286  /// @dev Computes the reward owed to a pool during finalization.  /// @param pool The active pool.  /// @param state The current state of finalization.  /// @return rewards Unfinalized rewards for this pool.  function _getUnfinalizedPoolRewardsFromState(  IStructs.ActivePool memory pool,  IStructs.UnfinalizedState memory state  private  view  returns (uint256 rewards)  // There can't be any rewards if the pool was active or if it has  // no stake.  if (pool.feesCollected == 0) {  return rewards;  // Use the cobb-douglas function to compute the total reward.  rewards = LibCobbDouglas.cobbDouglas(  state.rewardsAvailable,  pool.feesCollected,  state.totalFeesCollected,  pool.weightedStake,  state.totalWeightedStake,  cobbDouglasAlphaNumerator,  cobbDouglasAlphaDenominator  );  // Clip the reward to always be under  // `rewardsAvailable - totalRewardsPaid`,  // in case cobb-douglas overflows, which should be unlikely.  uint256 rewardsRemaining = state.rewardsAvailable.safeSub(state.totalRewardsFinalized);  if (rewardsRemaining < rewards) {  rewards = rewardsRemaining;  After implementing issue 5.11, this function is only called a single time, in MixinFinalizer.finalizePool:  code/contracts/staking/contracts/src/sys/MixinFinalizer.sol:L119-L129  // Noop if the pool was not active or already finalized (has no fees).  if (pool.feesCollected == 0) {  return;  // Clear the pool state so we don't finalize it again, and to recoup  // some gas.  delete _getActivePoolsFromEpoch(prevEpoch)[poolId];  // Compute the rewards.  uint256 rewards = _getUnfinalizedPoolRewardsFromState(pool, state);  Because it is only used a single time, and because it obfuscates an essential library call during the finalization process, the function should be removed and folded into finalizePool. Additionally, the first check for pool.feesCollected == 0 can be removed, as this case is covered in finalizePool already (see above).  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.11 Recommendation: remove complicating getters from MixinStakingPoolRewards   ", "body": "  Resolution  These getters are useful for client-side code, such as the staking interface.  Description  MixinStakingPoolRewards has two external view functions that contribute complexity to essential functions, as well as the overall inheritance tree:  computeRewardBalanceOfOperator, used to compute the reward balance of a pool s operator on an unfinalized pool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L55-L69 /// @dev Computes the reward balance in ETH of the operator of a pool. /// @param poolId Unique id of pool. /// @return totalReward Balance in ETH. function computeRewardBalanceOfOperator(bytes32 poolId)     external     view     returns (uint256 reward) {     // Because operator rewards are immediately withdrawn as WETH     // on finalization, the only factor in this function are unfinalized     // rewards.     IStructs.Pool memory pool = _poolById[poolId];     // Get any unfinalized rewards.     (uint256 unfinalizedTotalRewards, uint256 unfinalizedMembersStake) =         _getUnfinalizedPoolRewards(poolId);  computeRewardBalanceOfDelegator, used to compute the reward balance of a delegator for an unfinalized pool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L80-L99 /// @dev Computes the reward balance in ETH of a specific member of a pool. /// @param poolId Unique id of pool. /// @param member The member of the pool. /// @return totalReward Balance in ETH. function computeRewardBalanceOfDelegator(bytes32 poolId, address member)     external     view     returns (uint256 reward) {     IStructs.Pool memory pool = _poolById[poolId];     // Get any unfinalized rewards.     (uint256 unfinalizedTotalRewards, uint256 unfinalizedMembersStake) =         _getUnfinalizedPoolRewards(poolId);      // Get the members' portion.     (, uint256 unfinalizedMembersReward) = _computePoolRewardsSplit(         pool.operatorShare,         unfinalizedTotalRewards,         unfinalizedMembersStake     );  These two functions are the sole reason for the existence of MixinFinalizer._getUnfinalizedPoolRewards, one of the two functions in MixinAbstract:  code/contracts/staking/contracts/src/sys/MixinAbstract.sol:L40-L52  /// @dev Computes the reward owed to a pool during finalization.  ///      Does nothing if the pool is already finalized.  /// @param poolId The pool's ID.  /// @return totalReward The total reward owed to a pool.  /// @return membersStake The total stake for all non-operator members in  ///         this pool.  function _getUnfinalizedPoolRewards(bytes32 poolId)  internal  view  returns (  uint256 totalReward,  uint256 membersStake  );  These functions also necessitate two additional parameters in MixinStakingPoolRewards._computeDelegatorReward, which are used a single time to call _computeUnfinalizedDelegatorReward:  code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L253-L259  // 1/3 Unfinalized rewards earned in `currentEpoch - 1`.  reward = _computeUnfinalizedDelegatorReward(  delegatedStake,  _currentEpoch,  unfinalizedMembersReward,  unfinalizedMembersStake  );  By removing the functions computeRewardBalanceOfOperator and computeRewardBalanceOfDelegator, the following simplifications can be made:  _getUnfinalizedPoolRewards can be removed from both MixinAbstract and MixinFinalizer  The parameters unfinalizedMembersReward and unfinalizedMembersStake can be removed from _computeDelegatorReward  The function _computeUnfinalizedDelegatorReward can be removed  A branch of now-unused logic in _computeDelegatorReward can be removed  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.12 Recommendation: remove unneeded dependency on MixinStakeBalances   ", "body": "  Resolution  From the development team:  We re going to keep this abstraction to future-proof balance queries.  Description  MixinStakeBalances has two functions used by inheriting contracts:  getStakeDelegatedToPoolByOwner, which provides shorthand to access _delegatedStakeToPoolByOwner: code/contracts/staking/contracts/src/stake/MixinStakeBalances.sol:L84-L95 /// @dev Returns the stake delegated to a specific staking pool, by a given staker. /// @param staker of stake. /// @param poolId Unique Id of pool. /// @return Stake delegated to pool by staker. function getStakeDelegatedToPoolByOwner(address staker, bytes32 poolId)     public     view     returns (IStructs.StoredBalance memory balance) {     balance = _loadCurrentBalance(_delegatedStakeToPoolByOwner[staker][poolId]);     return balance; }  getTotalStakeDelegatedToPool, which provides shorthand to access _delegatedStakeByPoolId: code/contracts/staking/contracts/src/stake/MixinStakeBalances.sol:L97-L108 /// @dev Returns the total stake delegated to a specific staking pool, ///      across all members. /// @param poolId Unique Id of pool. /// @return Total stake delegated to pool. function getTotalStakeDelegatedToPool(bytes32 poolId)     public     view     returns (IStructs.StoredBalance memory balance) {     balance = _loadCurrentBalance(_delegatedStakeByPoolId[poolId]);     return balance; }  Each of these functions is used only a single time:  MixinExchangeFees.payProtocolFee: code/contracts/staking/contracts/src/fees/MixinExchangeFees.sol:L78 uint256 poolStake = getTotalStakeDelegatedToPool(poolId).currentEpochBalance;  MixinExchangeFees._computeMembersAndWeightedStake: code/contracts/staking/contracts/src/fees/MixinExchangeFees.sol:L143-L146 uint256 operatorStake = getStakeDelegatedToPoolByOwner(     _poolById[poolId].operator,     poolId ).currentEpochBalance;  By replacing these function invocations in MixinExchangeFees with the long-form access to each state variable, MixinStakeBalances will no longer need to be included in the inheritance trees for several contracts.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.13 Misleading MoveStake event when moving stake from UNDELEGATED to UNDELEGATED    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2280. If  Description  Although moving stake between the same status (UNDELEGATED <=> UNDELEGATED) should be a no-op, calls to moveStake succeed even for invalid amount and nonsensical poolId. The resulting MoveStake event can log garbage, potentially confusing those observing events.  Examples  When moving between UNDELEGATED and UNDELEGATED, each check and function call results in a no-op, save the final event:  Neither from nor to are StakeStatus.DELEGATED, so these checks are passed: code/contracts/staking/contracts/src/stake/MixinStake.sol:L115-L129 if (from.status == IStructs.StakeStatus.DELEGATED) {     _undelegateStake(         from.poolId,         staker,         amount     ); }  if (to.status == IStructs.StakeStatus.DELEGATED) {     _delegateStake(         to.poolId,         staker,         amount     ); }  The primary state changing function, _moveStake, immediately returns because the from and to balance pointers are equivalent: code/contracts/staking/contracts/src/stake/MixinStakeStorage.sol:L47-L49 if (_arePointersEqual(fromPtr, toPtr)) {     return; }  Finally, the MoveStake event is invoked, which can log completely invalid values for amount, from.poolId, and to.poolId: code/contracts/staking/contracts/src/stake/MixinStake.sol:L141-L148 emit MoveStake(     staker,     amount,     uint8(from.status),     from.poolId,     uint8(to.status),     to.poolId );  Recommendation  If amount is 0 or if moving between UNDELEGATED and UNDELEGATED, this function should no-op or revert. An explicit check for this case should be made near the start of the function.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.14 The staking contracts contain several artifacts of a quickly-changing codebase    ", "body": "  Resolution                           These issues were addressed in a variety of fixes, most notably   0xProject/0x-monorepo#2262.  Examples  address payable is used repeatedly, but payments use WETH:   MixinStakingPool.createStakingPool: code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L54 address payable operator = msg.sender;   ZrxVault.stakingProxyAddress: code/contracts/staking/contracts/src/ZrxVault.sol:L38 address payable public stakingProxyAddress;   ZrxVault.setStakingProxy: code/contracts/staking/contracts/src/ZrxVault.sol:L76 function setStakingProxy(address payable _stakingProxyAddress)   IZrxVault.setStakingProxy: code/contracts/staking/contracts/src/interfaces/IZrxVault.sol:L53 function setStakingProxy(address payable _stakingProxyAddress)   struct IStructs.Pool: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L114 address payable operator;   MixinStake.stake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L38 address payable staker = msg.sender;   MixinStake.unstake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L63 address payable staker = msg.sender;   MixinStake.moveStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L119 address payable staker = msg.sender;   MixinStake._delegateStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L181 address payable staker,   MixinStake._undelegateStake: code/contracts/staking/contracts/src/stake/MixinStake.sol:L210 address payable staker,  Some identifiers are used multiple times for different purposes:  currentEpoch is:   A state variable: code/contracts/staking/contracts/src/immutable/MixinStorage.sol:L86 uint256 public currentEpoch = INITIAL_EPOCH;   A function parameter: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L323 uint256 currentEpoch,   A struct field: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L62 uint32 currentEpoch;  Several comments are out of date:   Many struct comments reference fees and rewards denominated in ETH, while only WETH is used: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L36-L38 /// @param rewardsAvailable Rewards (ETH) available to the epoch ///        being finalized (the previous epoch). This is simply the balance ///        of the contract at the end of the epoch.   UnfinalizedState.totalFeesCollected should specify that it is tracking fees attributed to a pool. Fees not attributed to a pool are still collected, but are not recorded: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L41 /// @param totalFeesCollected The total fees collected for the epoch being finalized.   UnfinalizedState.totalWeightedStake is copy-pasted from totalFeesCollected: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L42 /// @param totalWeightedStake The total fees collected for the epoch being finalized.   Pool.initialized seems to be copy-pasted from an older version of the struct StoredBalance or StakeBalance: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L108 /// @param initialized True iff the balance struct is initialized.  The final contracts produce several compiler warnings:   Several functions are intentionally marked view to allow overriding implementations to read from state. These can be silenced by adding block.timestamp; or similar statements to the functions.   One function is erroneously marked view, and should be changed to pure: code/contracts/staking/contracts/src/staking_pools/MixinStakingPoolRewards.sol:L315-L330 /// @dev Computes the unfinalized rewards earned by a delegator in the last epoch. /// @param unsyncedStake Unsynced delegated stake to pool by staker /// @param currentEpoch The epoch in which this call is executing /// @param unfinalizedMembersReward Unfinalized total members reward (if any). /// @param unfinalizedMembersStake Unfinalized total members stake (if any). /// @return reward Balance in WETH. function _computeUnfinalizedDelegatorReward(     IStructs.StoredBalance memory unsyncedStake,     uint256 currentEpoch,     uint256 unfinalizedMembersReward,     uint256 unfinalizedMembersStake )     private     view     returns (uint256) {  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.15 Remove unneeded fields from StoredBalance and Pool structs    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2248. As part of a larger refactor, these fields were removed.  Description  Both structs have fields that are only written to, and never read:  StoredBalance.isInitialized: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L61 bool isInitialized;  Pool.initialized: code/contracts/staking/contracts/src/interfaces/IStructs.sol:L113 bool initialized;  Recommendation  The unused fields should be removed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.16 Remove unnecessary fallback function in Staking contract    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2277.  Description  The Staking contract has a payable fallback function that is never used. Because it is used with a proxy contract, this pattern introduces silent failures when calls are made to the contract with no matching function selector.  Recommendation  Remove the fallback function from Staking.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.17 Pool IDs can just be incrementing integers    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2250. Pool IDs now start at 1 and increment by 1 each time.  Description  Pool IDs are currently bytes32 values that increment by 2**128. After discussion with the development team, it seems that this was in preparation for a feature that was ultimately not used. Pool IDs should instead just be incrementing integers.  Examples  code/contracts/staking/contracts/src/immutable/MixinConstants.sol:L30-L34  // The upper 16 bytes represent the pool id, so this would be pool id 1. See MixinStakinPool for more information.  bytes32 constant internal INITIAL_POOL_ID = 0x0000000000000000000000000000000100000000000000000000000000000000;  // The upper 16 bytes represent the pool id, so this would be an increment of 1. See MixinStakinPool for more information.  uint256 constant internal POOL_ID_INCREMENT_AMOUNT = 0x0000000000000000000000000000000100000000000000000000000000000000;  code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L271-L280  /// @dev Computes the unique id that comes after the input pool id.  /// @param poolId Unique id of pool.  /// @return Next pool id after input pool.  function _computeNextStakingPoolId(bytes32 poolId)  internal  pure  returns (bytes32)  return bytes32(uint256(poolId).safeAdd(POOL_ID_INCREMENT_AMOUNT));  Recommendation  Make pool IDs uint256 values and simply add 1 to generate the next ID.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "5.18 LibProxy.proxyCall() may overwrite important memory    ", "body": "  Resolution                           This is fixed in   0xProject/0x-monorepo#2301. This function has been rewritten in Solidity and now avoids manual memory management.  Description  LibProxy.proxyCall() copies from call data to memory, starting at address 0:  code/contracts/staking/contracts/src/libs/LibProxy.sol:L52-L71  assembly {  // store selector of destination function  let freeMemPtr := 0  if gt(customEgressSelector, 0) {  mstore(0x0, customEgressSelector)  freeMemPtr := add(freeMemPtr, 4)  // adjust the calldata offset, if we should ignore the selector  let calldataOffset := 0  if gt(ignoreIngressSelector, 0) {  calldataOffset := 4  // copy calldata to memory  calldatacopy(  freeMemPtr,  calldataOffset,  calldatasize()  The first 64 bytes of memory are treated as  scratch space  by the Solidity compiler. Writing beyond that point is dangerous, as it will overwrite the free memory pointer and the  zero slot  which is where length-0 arrays point.  Although the current callers of proxyCall() don t appear to use any memory after calling proxyCall(), future changes to the code may introduce very serious and subtle bugs due to this unsafe handling of memory.  Recommendation  Use the actual free memory pointer to determine where it s safe to write to memory.  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "6.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  The full set of MythX results for both the exchange and staking contracts are available in a separate report.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "6.2 Surya", "body": "  Surya is an utility tool for smart contract systems. It provides a number of visual outputs and information about structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  S\u016brya s Description Report  Files Description Table  ReadOnlyProxy.sol  6ec64526446ebff87ec5528ee3b2786338cc4fa0  Staking.sol  67ddcb9ab75e433882e28d9186815990b7084c61  StakingProxy.sol  248f562d014d0b1ca6de3212966af3e52a7deef1  ZrxVault.sol  6c3249314868a2f5d0984122e8ab1413a5b521c9  fees/MixinExchangeFees.sol  9ac3b696baa8ba09305cfc83d3c08f17d9d528e1  fees/MixinExchangeManager.sol  46f48136a49919cdb5588dc1b3d64c977c3367f2  immutable/MixinConstants.sol  97c2ac83ef97a09cfd485cb0d4b119ba0902cc79  immutable/MixinDeploymentConstants.sol  424f22c45df8e494c4a78f239ea07ff0400d694b  immutable/MixinStorage.sol  8ad475b0e424e7a3ff65eedf2e999cba98f414c8  interfaces/IStaking.sol  ec1d7f214e3fd40e14716de412deee9769359bc0  interfaces/IStakingEvents.sol  25f16b814c4df9d2002316831c3f727d858456c4  interfaces/IStakingProxy.sol  02e35c6b51e08235b2a01d30a8082d60d9d61bee  interfaces/IStorage.sol  eeaa798c262b46d1874e904cf7de0423d4132cee  interfaces/IStorageInit.sol  b9899b03e474ea5adc3b4818a4357f71b8d288d4  interfaces/IStructs.sol  fee17d036883d641afb1222b75eec8427f3cdb96  interfaces/IZrxVault.sol  9067154651675317e000cfa92de9741e50c1c809  libs/LibCobbDouglas.sol  242d62d71cf8bc09177d240c0db59b83f9bb4e96  libs/LibFixedMath.sol  36311e7be09a947fa4e6cd8c544cacd13d65833c  libs/LibFixedMathRichErrors.sol  39cb3e07bbce3272bbf090e87002d5834d288ec2  libs/LibProxy.sol  29abe52857a782c8da39b053cc54e02e295c1ae2  libs/LibSafeDowncast.sol  ae16ed2573d64802793320253b060b9507729c3d  libs/LibStakingRichErrors.sol  f5868ef6066a18277c932e59c0a516ec58920b00  stake/MixinStake.sol  ade59ed356fe72521ffd2ef12ff8896c852f11f8  stake/MixinStakeBalances.sol  cde6ca1a6200570ba18dd6d392ffabf68c2bb464  stake/MixinStakeStorage.sol  cadf34d9d341efd2a85dd13ec3cd4ce8383e0f73  staking_pools/MixinCumulativeRewards.sol  664ea3e35376c81492457dc17832a4d0d602c8ae  staking_pools/MixinStakingPool.sol  74ba9cb2db29b8dd6376d112e9452d117a391b18  staking_pools/MixinStakingPoolRewards.sol  a3b4e5c9b1c3568c94923e2dd9a93090ebdf8536  sys/MixinAbstract.sol  99fd4870c20d8fa03cfa30e8055d3dfb348ed5cd  sys/MixinFinalizer.sol  cc658ed07241c1804cec75b12203be3cd8657b9b  sys/MixinParams.sol  7b395f4da7ed787d7aa4eb915f15377725ff8168  sys/MixinScheduler.sol  2fab6b83a6f9e1d0dd1b1bdcea4b129d166aef1d  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  ReadOnlyProxy  Implementation  MixinStorage  <Fallback>  External    NO   revertDelegateCall  External    NO   Staking  Implementation  IStaking, MixinParams, MixinStake, MixinExchangeFees  <Fallback>  External    NO   init  Public    onlyAuthorized  StakingProxy  Implementation  IStakingProxy, MixinStorage  <Constructor>  Public    MixinStorage  <Fallback>  External    NO   attachStakingContract  External    onlyAuthorized  detachStakingContract  External    onlyAuthorized  setReadOnlyMode  External    onlyAuthorized  batchExecute  External    NO   _assertValidStorageParams  Internal \ud83d\udd12  _attachStakingContract  Internal \ud83d\udd12  ZrxVault  Implementation  Authorizable, IZrxVault  <Constructor>  Public    Authorizable  setStakingProxy  External    onlyAuthorized  enterCatastrophicFailure  External    onlyAuthorized  setZrxProxy  External    onlyAuthorized onlyNotInCatastrophicFailure  depositFrom  External    onlyStakingProxy onlyNotInCatastrophicFailure  withdrawFrom  External    onlyStakingProxy onlyNotInCatastrophicFailure  withdrawAllFrom  External    onlyInCatastrophicFailure  balanceOf  External    NO   _withdrawFrom  Internal \ud83d\udd12  _assertSenderIsStakingProxy  Private \ud83d\udd10  _assertInCatastrophicFailure  Private \ud83d\udd10  _assertNotInCatastrophicFailure  Private \ud83d\udd10  MixinExchangeFees  Implementation  MixinExchangeManager, MixinStakingPool, MixinFinalizer  payProtocolFee  External    onlyExchange  getActiveStakingPoolThisEpoch  External    NO   _computeMembersAndWeightedStake  Private \ud83d\udd10  _assertValidProtocolFee  Private \ud83d\udd10  MixinExchangeManager  Implementation  IStakingEvents, MixinStorage  addExchangeAddress  External    onlyAuthorized  removeExchangeAddress  External    onlyAuthorized  MixinConstants  Implementation  MixinDeploymentConstants  MixinDeploymentConstants  Implementation  getWethContract  Public    NO   getZrxVault  Public    NO   MixinStorage  Implementation  MixinConstants, Authorizable  IStaking  Interface  moveStake  External    NO   payProtocolFee  External    NO   stake  External    NO   IStakingEvents  Interface  IStakingProxy  Interface  <Fallback>  External    NO   attachStakingContract  External    NO   detachStakingContract  External    NO   IStorage  Interface  stakingContract  External    NO   readOnlyProxy  External    NO   readOnlyProxyCallee  External    NO   nextPoolId  External    NO   numMakersByPoolId  External    NO   currentEpoch  External    NO   currentEpochStartTimeInSeconds  External    NO   protocolFeesThisEpochByPool  External    NO   activePoolsThisEpoch  External    NO   validExchanges  External    NO   epochDurationInSeconds  External    NO   rewardDelegatedStakeWeight  External    NO   minimumPoolStake  External    NO   maximumMakersInPool  External    NO   cobbDouglasAlphaNumerator  External    NO   cobbDouglasAlphaDenominator  External    NO   IStorageInit  Interface  init  External    NO   IStructs  Interface  IZrxVault  Interface  setStakingProxy  External    NO   enterCatastrophicFailure  External    NO   setZrxProxy  External    NO   depositFrom  External    NO   withdrawFrom  External    NO   withdrawAllFrom  External    NO   balanceOf  External    NO   LibCobbDouglas  Library  cobbDouglas  Internal \ud83d\udd12  LibFixedMath  Library  one  Internal \ud83d\udd12  add  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  mulDiv  Internal \ud83d\udd12  uintMul  Internal \ud83d\udd12  abs  Internal \ud83d\udd12  invert  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toFixed  Internal \ud83d\udd12  toInteger  Internal \ud83d\udd12  ln  Internal \ud83d\udd12  exp  Internal \ud83d\udd12  _mul  Private \ud83d\udd10  _div  Private \ud83d\udd10  _add  Private \ud83d\udd10  LibFixedMathRichErrors  Library  SignedValueError  Internal \ud83d\udd12  UnsignedValueError  Internal \ud83d\udd12  BinOpError  Internal \ud83d\udd12  LibProxy  Library  proxyCall  Internal \ud83d\udd12  LibSafeDowncast  Library  downcastToUint96  Internal \ud83d\udd12  downcastToUint64  Internal \ud83d\udd12  downcastToUint32  Internal \ud83d\udd12  LibStakingRichErrors  Library  OnlyCallableByExchangeError  Internal \ud83d\udd12  ExchangeManagerError  Internal \ud83d\udd12  InsufficientBalanceError  Internal \ud83d\udd12  OnlyCallableByPoolOperatorOrMakerError  Internal \ud83d\udd12  MakerPoolAssignmentError  Internal \ud83d\udd12  BlockTimestampTooLowError  Internal \ud83d\udd12  OnlyCallableByStakingContractError  Internal \ud83d\udd12  OnlyCallableIfInCatastrophicFailureError  Internal \ud83d\udd12  OnlyCallableIfNotInCatastrophicFailureError  Internal \ud83d\udd12  OperatorShareError  Internal \ud83d\udd12  PoolExistenceError  Internal \ud83d\udd12  InvalidProtocolFeePaymentError  Internal \ud83d\udd12  InvalidStakeStatusError  Internal \ud83d\udd12  InitializationError  Internal \ud83d\udd12  InvalidParamValueError  Internal \ud83d\udd12  ProxyDestinationCannotBeNilError  Internal \ud83d\udd12  PreviousEpochNotFinalizedError  Internal \ud83d\udd12  MixinStake  Implementation  MixinStakingPool  stake  External    NO   unstake  External    NO   moveStake  External    NO   _delegateStake  Private \ud83d\udd10  _undelegateStake  Private \ud83d\udd10  _getBalancePtrFromStatus  Private \ud83d\udd10  MixinStakeBalances  Implementation  MixinStakeStorage  getGlobalActiveStake  External    NO   getGlobalInactiveStake  External    NO   getGlobalDelegatedStake  External    NO   getTotalStake  External    NO   getActiveStake  External    NO   getInactiveStake  External    NO   getStakeDelegatedByOwner  External    NO   getWithdrawableStake  Public    NO   getStakeDelegatedToPoolByOwner  Public    NO   getTotalStakeDelegatedToPool  Public    NO   _computeWithdrawableStake  Internal \ud83d\udd12  MixinStakeStorage  Implementation  MixinScheduler  _moveStake  Internal \ud83d\udd12  _loadSyncedBalance  Internal \ud83d\udd12  _loadUnsyncedBalance  Internal \ud83d\udd12  _increaseCurrentAndNextBalance  Internal \ud83d\udd12  _decreaseCurrentAndNextBalance  Internal \ud83d\udd12  _increaseNextBalance  Internal \ud83d\udd12  _decreaseNextBalance  Internal \ud83d\udd12  _storeBalance  Private \ud83d\udd10  _arePointersEqual  Private \ud83d\udd10  MixinCumulativeRewards  Implementation  MixinStakeBalances  _initializeCumulativeRewards  Internal \ud83d\udd12  _isCumulativeRewardSet  Internal \ud83d\udd12  _forceSetCumulativeReward  Internal \ud83d\udd12  _computeMemberRewardOverInterval  Internal \ud83d\udd12  _getMostRecentCumulativeReward  Internal \ud83d\udd12  _getCumulativeRewardAtEpoch  Internal \ud83d\udd12  MixinStakingPool  Implementation  MixinAbstract, MixinStakingPoolRewards  createStakingPool  External    NO   decreaseStakingPoolOperatorShare  External    onlyStakingPoolOperatorOrMaker  joinStakingPoolAsMaker  External    NO   addMakerToStakingPool  External    onlyStakingPoolOperatorOrMaker  removeMakerFromStakingPool  External    onlyStakingPoolOperatorOrMaker  getStakingPoolIdOfMaker  Public    NO   getStakingPool  Public    NO   _addMakerToStakingPool  Internal \ud83d\udd12  _computeNextStakingPoolId  Internal \ud83d\udd12  _assertStakingPoolExists  Internal \ud83d\udd12  _assertNewOperatorShare  Private \ud83d\udd10  _assertSenderIsPoolOperatorOrMaker  Private \ud83d\udd10  MixinStakingPoolRewards  Implementation  MixinAbstract, MixinCumulativeRewards  withdrawDelegatorRewards  External    NO   computeRewardBalanceOfOperator  External    NO   computeRewardBalanceOfDelegator  External    NO   _withdrawAndSyncDelegatorRewards  Internal \ud83d\udd12  _syncPoolRewards  Internal \ud83d\udd12  _computePoolRewardsSplit  Internal \ud83d\udd12  _computeDelegatorReward  Private \ud83d\udd10  _computeUnfinalizedDelegatorReward  Private \ud83d\udd10  _increasePoolRewards  Private \ud83d\udd10  _decreasePoolRewards  Private \ud83d\udd10  MixinAbstract  Implementation  finalizePool  Public    NO   _getUnfinalizedPoolRewards  Internal \ud83d\udd12  MixinFinalizer  Implementation  MixinStakingPoolRewards  endEpoch  External    NO   finalizePool  Public    NO   _getUnfinalizedPoolRewards  Internal \ud83d\udd12  _getActivePoolFromEpoch  Internal \ud83d\udd12  _getActivePoolsFromEpoch  Internal \ud83d\udd12  _wrapEth  Internal \ud83d\udd12  _getAvailableWethBalance  Internal \ud83d\udd12  _getUnfinalizedPoolRewardsFromState  Private \ud83d\udd10  _creditRewardsToPool  Private \ud83d\udd10  MixinParams  Implementation  IStakingEvents, MixinStorage  setParams  External    onlyAuthorized  getParams  External    NO   _initMixinParams  Internal \ud83d\udd12  _assertParamsNotInitialized  Internal \ud83d\udd12  _setParams  Private \ud83d\udd10  MixinScheduler  Implementation  IStakingEvents, MixinStorage  getCurrentEpochEarliestEndTimeInSeconds  Public    NO   _initMixinScheduler  Internal \ud83d\udd12  _goToNextEpoch  Internal \ud83d\udd12  _assertSchedulerNotInitialized  Internal \ud83d\udd12  Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/10/0x-v3-staking/"}, {"title": "4.1 Lack of Fee Limits for V3 Transactions    ", "body": "  Resolution  Fixed in PR-266 by introducing limits on the v3 transaction fees through two separate maximums:  MAX_ESCAPE_TIP_STRK = 1 STRK which limits the result of multiplying tip * L2_GAS.max_amount.  MAX_ESCAPE_MAX_FEE_STRK = 50 STRK which limits the sum of all of the following:  L1_GAS.max_price_per_unit * L1_GAS.max_amount, L2_GAS.max_price_per_unit * L2_GAS.max_amount, tip * L2_GAS.max_amount, the tip described above.  Description  Currently, the only sequencer is operated by StarkWare and charges a fair price for the L1 costs. (And L2 fees are not being collected yet.) Hence, even if a transaction specifies a very high fee limit, the sequencer takes only what is really needed and not everything that the transaction limit(s) would allow. For the sake of brevity, let us call such a sequencer  nice . In a more decentralized Starknet future, there will probably be more sequencers, and they may not necessarily be nice, meaning they may take more in fees than what the L1 costs demand of them. Also, it is not impossible that the rules for the StarkWare sequencer change at some point in the future (although it seems reasonable to assume that this would be properly announced). But   to summarize this discussion   currently there is only sequencer, and it is nice. The attack we describe below requires a  non-nice  sequencer   and, as we will explain shortly, a malicious Guardian   and is therefore, at the time of writing this report, not feasible, even assuming the Guardian acts with malice.  As explained in more detail in the System Overview of our previous report, there is an escape mechanism which (1) allows users to reclaim control of their account if the Guardian fails to cooperate and (2) allows Guardians to assign a new owner if the original owner lost access to their key. Crucially and unlike other account activities, escape-related actions require only a single signer. Hence, a general attack scenario that the account should implement protective measures against is a malicious Guardian trying to drain the account by signing an escape transaction with an excessive fee limit. A similar situation arises if, instead of the Guardian being malicious, a third party comes into possession of the owner s private key, but to keep the discussion more concise, we ll consider this subsumed under  malicious Guardian.  As mentioned above, such an attack is not possible with a nice sequencer, but   ideally   the account contract should not rely on that.  Examining the relevant code, we see that the fee restriction logic for v1 transactions   which is known from earlier versions of the contract   is still present:  src/account/argent_account.cairo:L47-L48  /// Limits fee in escapes  const MAX_ESCAPE_MAX_FEE: u128 = 50000000000000000; // 0.05 ETH  src/account/argent_account.cairo:L772-L774  } else if tx_info.version == TX_V1 || tx_info.version == TX_V1_ESTIMATE {  // other fields not available on V1  assert(tx_info.max_fee <= MAX_ESCAPE_MAX_FEE, 'argent/max-fee-too-high');  And there is a limit on  the total tip, i.e., tip * L2_GAS.max_amount, for v3 transactions:  src/account/argent_account.cairo:L49-L50  /// Limits tip in escapes  const MAX_ESCAPE_TIP: u128 = 1_000000000000000000; // 1 STRK  src/account/argent_account.cairo:L758-L771  // Limit the maximum tip while escaping (max_fee returns 0 on TX_V3)  let max_l2_gas: u64 = loop {  match tx_info.resource_bounds.pop_front() {  Option::Some(r) => { if *r.resource == 'L2_GAS' {  break *r.max_amount;  } },  Option::None => {  // L2_GAS not found  break 0;  };  };  let max_tip = tx_info.tip * max_l2_gas.into();  assert(max_tip <= MAX_ESCAPE_TIP, 'argent/tip-too-high');  However, no limit is imposed on the amount of STRK for L1_GAS or L2_GAS. Regarding L1_GAS, this means that a malicious Guardian could specify an excessive max_price_per_unit in an escape transaction and   with the help of a non-nice sequencer   drain the account s entire STRK balance. Since the sequencer can pocket the difference between max_price_per_unit and  what is really needed on L1, it is also conceivable that the two parties collude for an attack.  For L2_GAS, the situation is more difficult to assess because the fee market has not been implemented yet. It is very well possible that the  base fee  will be set by the network    similar to the base fee on Ethereum, see EIP-1559. In this case, constraining only the tip, as is currently the case, would be sufficient to prevent the L2_GAS part of this kind of attack, as long as one is comfortable with risking to pay any fee the market demands. Nevertheless, as the details of the L2 fee mechanism have not yet been specified, we advise caution.  Recommendation  For both L1_GAS and L2_GAS, the amount of STRK that can be spent on fees should be limited, similar to the limit on max_fee for v1 transactions. There are several different   and equally viable   ways to do this: separately for L1_GAS and L2_GAS or together; including the tip in a (higher) limit or handling it separately.  One aspect of the L2 fees that has, to the best of our knowledge, not been decided yet is whether the tip will be considered part of the max_price_per_unit (similar to EIP-1559) or if it can be on top of that. Hence, to be on the safe side, the latter should be considered possible, and the tip should not be left unconstrained.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/argent-account-argent-multisig-starknet-transaction-v3-updates/"}, {"title": "4.2 Newer Cairo Version Available    ", "body": "  Resolution                           When we informed the client about this, we learned that they had already updated the Cairo version on the development branch to 2.4.3 (commit   72c14f0), so they were aware of this.  Description  The Cairo version used in the commit hash specified for the audit is 2.4.0. Cairo receives updates in quick succession, and, at the time of conducting the review, newer versions are available. Version 2.4.1, in particular, comes with some bug fixes.  Recommendation  We recommend utilizing the latest available Cairo version or at least a version without known bugs.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/argent-account-argent-multisig-starknet-transaction-v3-updates/"}, {"title": "4.3 Discrepancy Between Actual OUTSIDE_EXECUTION_TYPE_HASH and Comments    ", "body": "  Resolution                           Fixed in   PR-266 by adjusting the comments, removing the  Description  In the outside_execution.cairo file, we have a hardcoded value const OUTSIDE_EXECUTION_TYPE_HASH: felt252 = 0x11ff76fe3f640fa6f3d60bbd94a3b9d47141a2c96f87fdcfbeb2af1d03f7050. The comment above it indicates that to derive it, we need to use the hash H('OutsideExecution(caller:felt,nonce:felt,execute_after:felt,execute_before:felt,calls_len:felt,calls:Call*)'), where  H  would be the starknet_keccak (which is confirmed with other values):  src/common/outside_execution.cairo:L33-L34  // H('OutsideExecution(caller:felt,nonce:felt,execute_after:felt,execute_before:felt,calls_len:felt,calls:Call*)')  const OUTSIDE_EXECUTION_TYPE_HASH: felt252 = 0x11ff76fe3f640fa6f3d60bbd94a3b9d47141a2c96f87fdcfbeb2af1d03f7050;  However, doing the above hash results in the value 0x28df6ab27eb241200f2ba2177e1ad2c81bd92b71bfd8b8fa40ced4d3b55d66d. If we add the Call struct to the string as well, we get 'OutsideExecution(caller:felt,nonce:felt,execute_after:felt,execute_before:felt,calls_len:felt,calls:Call*)Call(to:felt,selector:felt,calldata_len:felt,calldata:felt*)', which when hashed yields 0x2838a3633cc7cd97bbf5b9f800d77b6d891154b8abdf6f41132c40e5a9ace2c that also doesn t match.  Finally, if we observe the function hash_outside_execution, we can see that it computes the selector with the string 'OutsideExecution(caller:felt,nonce:felt,execute_after:felt,execute_before:felt,calls_len:felt,calls:OutsideCall*)OutsideCall(to:felt,selector:felt,calldata_len:felt,calldata:felt*)':  src/common/outside_execution.cairo:L101-L103  selector!(  \"OutsideExecution(caller:felt,nonce:felt,execute_after:felt,execute_before:felt,calls_len:felt,calls:OutsideCall*)OutsideCall(to:felt,selector:felt,calldata_len:felt,calldata:felt*)\"  If we take this string and hash it, we indeed get 0x11ff76fe3f640fa6f3d60bbd94a3b9d47141a2c96f87fdcfbeb2af1d03f7050. However, this does ask for OutsideCall in the OutsideExecution struct, whereas the code may suggest it should be just Call.  Recommendation  While there is no real impact as the hash constant is the correct one, it would be beneficial to get the comments and the code in sync with the intended values.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/argent-account-argent-multisig-starknet-transaction-v3-updates/"}, {"title": "4.4 Self-Written Versions of  get_execution_info and get_tx_info   ", "body": "  Resolution                           Fixed in   PR-266 by using the  starknet package.  Description  Functions that retrieve the execution and transaction information have been implemented in transaction_version.cairo:  src/common/transaction_version.cairo:L37-L45  #[inline(always)]  fn get_execution_info() -> Box<starknet::info::v2::ExecutionInfo> {  starknet::syscalls::get_execution_info_v2_syscall().unwrap_syscall()  #[inline(always)]  fn get_tx_info() -> Box<starknet::info::v2::TxInfo> {  get_execution_info().unbox().tx_info  However, the same functionality is provided by the starknet package, which is used anyway.  Recommendation  Although there is no difference in behavior, we recommend utilizing the functions from Cairo s starknet package. The self-written versions can then be removed from transaction_version.cairo.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/argent-account-argent-multisig-starknet-transaction-v3-updates/"}, {"title": "4.5 __validate_deploy__ Function Doesn t Have Its Own Transaction Version Check   ", "body": "  Resolution                           Fixed in   PR-266 by decoupling the transaction checks through the introduction of  Description  In the current version of Starknet, accounts have required functions that validate specific types of transactions, such as INVOKE, DECLARE, and DEPLOY_ACCOUNT, as seen here.  In argent_account.cairo there are indeed those functions, where it is also checked that the transaction going through those functions is compliant with specific transaction parameters:  src/account/argent_account.cairo:L202-L205  fn __validate__(ref self: ContractState, calls: Array<Call>) -> felt252 {  assert_caller_is_null();  let tx_info = get_tx_info().unbox();  assert_correct_invoke_version(tx_info.version);  src/account/argent_account.cairo:L329-L331  fn __validate_declare__(self: @ContractState, class_hash: felt252) -> felt252 {  let tx_info = get_tx_info().unbox();  assert_correct_declare_version(tx_info.version);  src/account/argent_account.cairo:L337-L341  fn __validate_deploy__(  self: @ContractState, class_hash: felt252, contract_address_salt: felt252, owner: felt252, guardian: felt252  ) -> felt252 {  let tx_info = get_tx_info().unbox();  assert_correct_invoke_version(tx_info.version);  The checks:  src/common/transaction_version.cairo:L14-L28  #[inline(always)]  fn assert_correct_invoke_version(tx_version: felt252) {  assert(  tx_version == TX_V3 || tx_version == TX_V1 || tx_version == TX_V3_ESTIMATE || tx_version == TX_V1_ESTIMATE,  'argent/invalid-tx-version'  #[inline(always)]  fn assert_correct_declare_version(tx_version: felt252) {  assert(  tx_version == TX_V3 || tx_version == TX_V2 || tx_version == TX_V3_ESTIMATE || tx_version == TX_V2_ESTIMATE,  'argent/invalid-declare-version'  Recommendation  While that is technically fine (although the documentation isn t consistent in all places about this) as it indeed does support v3 and v1 transactions, it would be beneficial to have a separate check just for the DEPLOY_ACCOUNT transactions for better maintainability of the codebase.  ", "labels": ["Consensys", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2024/01/argent-account-argent-multisig-starknet-transaction-v3-updates/"}, {"title": "7.1 Permissionless nature of proxy factory might cause confusion when parsing events  ", "body": "  Resolution  Update from the iExec team:  The iExec offchain platform does not listen to GenericFactory. This factory is intended to be public and available to anyone and is just a tool used for deployment.  Description  The permissionless nature of the factory (the GenericFactory contract) meant to deploy the ERC1538Proxy and the instances of its several delegates might create confusion when parsing events.  Since there is no access control being enforced through the use of modifiers on said factory, any account can use its deployment public methods to deploy a contract. This means that the supporting off-chain infrastructure making use of the fired events to look for deployed instances of either the iExec proxies or its delegates might get hindered by an ill-intended actor that abuses its functions.  Recommendation  Use a modifier enforcing some sort of access control (easily done through the inherited Ownable contract) to make sure only iExec can deploy from the factory and, therefore, increase the readability of logged events.  This becomes more important as time goes by and updates to the architecture are performed or any past analysis needs to be done on deployed modules.  ", "labels": ["Consensys", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"}, {"title": "7.2 System deployer is fully trusted in this version of the PoCo system   ", "body": "  Resolution  Update from the iExec team:  After deployment, ownership is planned to be transferred to a multisig. This is just the first step towards a more decentralised governance on the protocol. We will consider adding an intermediary contract that enforces the lock period. This would however, prevent us from any kind of  emergency  update. The long term goal is it involve the community in the process, using a DAO or a similar solution.  Description  The introduction of ERC1538-compliant proxies to construct the PoCo system has many benefits. It heightens modularity, reduces the number of external calls between the system s components and allows for easy expansion of the system s capabilities without disruption of the service or need for off-chain infrastructure upgrade. However, the last enumerated benefit is in fact a double-edged sword.  Even though ERC1538 enables easy upgradeability it also completely strips the PoCo system of all of its prior trustless nature. In this version the iExec development team should be entirely trusted by every actor in the system not to change the deployed on-chain delegates for new ones.  Also the deployer, owner, has permission to change some of the system variables, such as m_callbackgas for Oracle callback gas limit. This indirectly can lock the system, for example it could result in IexecPocoDelegate.executeCallback() reverting which prevents the finalization of corresponding task.  Recommendation  The best, easiest solution for the trust issue would be to immediately revoke ownership of the proxy right after deployment. This way the modular deployment would still be possible but no power to change the deployed on-chain code would exist.  A second best solution would be to force a timespan period before any change to the proxy methods (and its delegates) is made effective. This way any actor in the system can still monitor for possible changes and  leave  the system before they are implemented.  In this last option the  lock  period should, obviously, be greater than the amount of time it takes to verify a Task of the bigger category but it is advisable to decide on it by anthropomorphic rules and use a longer,  human-friendly  time lock of, for example, 72 hours.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"}, {"title": "7.3 importScore() in IexecMaintenanceDelegate can be used to wrongfully reset worker scores   ", "body": "  Resolution  Update from the iExec team:  In order to perform this attack, one would first have to gain reputation on the new version, and lose it. They would then be able to restore its score from the old version.  We feel the risk is acceptable for a few reasons:  It can only be done once per worker  Considering the score dynamics discussed in the  Trust in the PoCo  document, it is more interesting for a worker to import its reputation in the beginning rather then creating a new one, since bad contributions only remove part of the reputation  Only a handful of workers have reputation in the old system (180), and their score is low (average 7, max 22)  We might force the import all 180 workers with reputation >0. A script to identify the relevant addresses is already available.  Description  The import of worker scores from the previous PoCo system deployed on chain is made to be asynchronous. And, even though the pull pattern usually makes a system much more resilient, in this case, it opens up the possibility for an attack that undermines the trust-based game-theoretical balance the PoCo system relies on. As can be seen in the following function:  code/poco-dev/contracts/modules/delegates/IexecMaintenanceDelegate.sol:L51-L57  function importScore(address _worker)  external override  require(!m_v3_scoreImported[_worker], \"score-already-imported\");  m_workerScores[_worker] = m_workerScores[_worker].max(m_v3_iexecHub.viewScore(_worker));  m_v3_scoreImported[_worker] = true;  A motivated attacker could attack the system providing bogus results for computation tasks therefore reducing his own reputation (mirrored by the low worker score that would follow).  After the fact, the attacker could reset its score to the previous high value attained in the previously deployed PoCo system (v3) and undo all the wrongdoings he had done at no reputational cost.  Recommendation  Check that each worker interacting with the PoCo system has already imported his score. Otherwise import it synchronously with a call at the time of their first interaction.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"}, {"title": "7.4 Outdated documentation   ", "body": "  Resolution                           Update from the iExec team:   Description  There are many changes within the system from the initial version that are not reflected in the documentation.  It is necessary to have updated documentation for the time of the audit, as the specification dictates the correct behaviour of the code base.  Examples  Entities such as iExecClerk are the main point of entry in the documentation, however they have been replaced by proxy implementation in the code base (V5).  Recommendation  Up date documentation to reflect the recent changes and design in the code base.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"}, {"title": "7.5 Domain separator in iExecMaintenanceDelegate has a wrong version field   ", "body": "  Resolution                           Issue was fixed in   iExecBlockchainComputing/PoCo-dev@ebee370  Description  The domain separator used to comply with the EIP712 standard in iExecMaintenanceDelegate has a wrong version field.  code/poco-dev/contracts/modules/delegates/IexecMaintenanceDelegate.sol:L77-L86  function _domain()  internal view returns (IexecLibOrders_v5.EIP712Domain memory)  return IexecLibOrders_v5.EIP712Domain({  name:              \"iExecODB\"  , version:           \"3.0-alpha\"  , chainId:           _chainId()  , verifyingContract: address(this)  });  In the above snippet we can see the code is still using the version field from an old version of the PoCo protocol, \"3.0-alpha\".  Recommendation  Change the version field to: \"5.0-alpha\"  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"}, {"title": "7.6 Limit the length of task.contributors to prevent reaching gasBlockLimit   ", "body": "  Resolution  Update from the iExec team:  Any hardcoded lock would be a restriction in the future if thee block size increases. In addition to that, workers are strongly incentivised to not contribute if it would result in a deadlocked task. Schedulers are incentivised to not authorise too many workers to contribute (they also lose stake if a task get deadlocked). So the development team has assessed the risk as low.  In the unlikely event the described flaw still happens, the task will get in a deadlocked state, until at some point the block size limit is increased and a claim becomes possible. Because in a world where block size increases are possible, deadlocks are not eternal.  Description  It is recommended to limit the length of arrays that the contract iterates through to prevent system halts. task.contributors is used within iExec contract in many functions, and main functions such as claim(), reOpen(), and most importantly contribute() (through calling checkConsensus()) iterate through this list.  Given that contributions are not free and they could only block the task they are contributing to, this is a low impact issue.  Recommendation  The fix is trivial to implement and only requires to limit the number of items in task.contributors to the maximum imagined for the system (based on client communication this number could be 20, although further testing should be done to make sure with this number does not reach the blockGasLimit, possibly with future changes in the opcode pricing).  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"}, {"title": "7.7 The updateContract() method in ERC1538UpdateDelegate is incorrectly implemented ", "body": "  Resolution                           Issue was fixed in   iExecBlockchainComputing/iexec-solidity@e6be083  Description  The updateContract() method in ERC1538UpdateDelegate does not behave as intended for some specific streams of bytes (meant to be parsed as function signatures).  The mentioned function takes as input, among other things, a string (which is, canonically, a dynamically-sized bytes array) and tries to parse it as a conjunction of function signatures.  As is evident in:  code/iexec-solidity/contracts/ERC1538/ERC1538Update.sol:L39  if (char == 0x3B) // 0x3B = ';'  Inside the function, ; is being used as a  reserved  character, serving as a delimiter between each function signature.  However, if two semicolons are used in succession, the second one will not be checked and will be made part of the function signature being sent into the _setFunc() method.  Example of faulty input  someFunc;;someOtherFuncWithSemiColon;  Recommendation  Replace the line that increases the pos counter at the end of the function:  code/iexec-solidity/contracts/ERC1538/ERC1538Update.sol:L47  start = ++pos;  WIth this line of code:  start = pos + 1;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/03/iexec-poco/"}, {"title": "5.1 zNS - Domain bid might be approved by non owner account    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by storing the domain request data on-chain.  Description  The spec allows anyone to place a bid for a domain, while only parent domain owners are allowed to approve a bid. Bid placement is actually enforced and purely informational. In practice, approveDomainBid allows any parent domain owner to approve bids (signatures) for any other domain even if they do not own it. Once approved, anyone can call fulfillDomainBid to create a domain.  Examples  zNS/contracts/StakingController.sol:L95-L103  function approveDomainBid(  uint256 parentId,  string memory bidIPFSHash,  bytes memory signature  ) external authorizedOwner(parentId) {  bytes32 hashOfSig = keccak256(abi.encode(signature));  approvedBids[hashOfSig] = true;  emit DomainBidApproved(bidIPFSHash);  Recommendation  Consider adding a validation check that allows only the parent domain owner to approve bids on one of its domains.  Reconsider the design of the system introducing more on-chain guarantees for bids.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.2 zAuction, zNS - Bids cannot be cancelled, never expire, and the auction lifecycle is unclear    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by refactoring the StakingController to control the lifecycle of bids instead of handling this off-chain.  Addressed with zer0-os/zAuction@135b2aa for zAuction by adding a bid/saleOffer expiration for bids. The client also provided the following statement:  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.6 added expireblock and startblock to zauction, expireblock to zsale", "body": " Decided not to add a cancel function. Paying gas to cancel isn t ideal, and it can be used as a griefing function. though that s still possible to do by moving weth but differently  The stateless nature of auctions may make it hard to enforce bid/sale expirations and it is not possible to cancel a bid/offer that should not be valid anymore. The expiration reduces the risk of old offers being used as they now automatically invalidate after time, however, it is still likely that multiple valid offers may be present at the same time. As outlined in the recommendation, one option would be to allow someone who signed a commitment to explicitly cancel it in the contract. Another option would be to create a stateful auction where the entity that puts up something for  starts  an auction, creating an auction id, requiring bidders to bid on that auction id. Once a bid is accepted the auction id is invalidated which invalidates all bids that might be floating around.  zer0-os/zAuction@2f92aa1 for  Description  The lifecycle of a bid both for zAuction and zNS is not clear, and has many flaws.  zAuction - Consider the case where a bid is placed, then the underlying asset in being transferred to a new owner. The new owner can now force to sell the asset even though it s might not be relevant anymore.  zAuction - Once a bid was accepted and the asset was transferred, all other bids need to be invalidated automatically, otherwise and old bid might be accepted even after the formal auction is over.  zAuction, zNS - There is no way for the bidder to cancel an old bid. That might be useful in the event of a significant change in market trend, where the old pricing is no longer relevant. Currently, in order to cancel a bid, the bidder can either withdraw his ether balance from the zAuctionAccountant, or disapprove WETH which requires an extra transaction that might be front-runned by the seller.  Examples  zAuction/contracts/zAuction.sol:L35-L45  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  Recommendation  Consider adding an expiration field to the message signed by the bidder both for zAuction and zNS. Consider adding auction control, creating an auctionId, and have users bid on specific auctions. By adding this id to the signed message, all other bids are invalidated automatically and users would have to place new bids for a new auction. Optionally allow users to cancel bids explicitly.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.3 zNS - Insufficient protection against replay attacks    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by avoiding the use of digital signatures and storing the domain request data on-chain.  Description  There is no dedicated data structure to prevent replay attacks on StakingController. approvedBids mapping offers only partial mitigation, due to the fact that after a domain bid is fulfilled, the only mechanism in place to prevent a replay attack is the Registrar contract that might be replaced in the case where StakingController is being re-deployed with a different Registrar instance. Additionally, the digital signature used for domain bids does not identify the buyer request uniquely enough. The bidder s signature could be replayed in future similar contracts that are deployed with a different registrar or in a different network.  Examples  zNS/contracts/StakingController.sol:L176-L183  function createBid(  uint256 parentId,  uint256 bidAmount,  string memory bidIPFSHash,  string memory name  ) public pure returns(bytes32) {  return keccak256(abi.encode(parentId, bidAmount, bidIPFSHash, name));  Recommendation  Consider adding a dedicated mapping to store the a unique identifier of a bid, as well as adding address(this), block.chainId, registrar and nonce to the message that is being signed by the bidder.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.4 zNS - domain name collisions    ", "body": "  Resolution                           Addressed with   zer0-os/ZNS@ab7d62a by disallowing empty names for domain registrations. The name validation in off-chain components (e.g. subgraph components) has not been verified.  Description  Domain registration accepts an empty (zero-length) name. This may allow a malicious entity to register two different NFT s for the same visually indinstinguishable text representation of a domain. Similar to this the domain name is mapped to an NFT via a subgraph that connects parent names to the new subdomain using a domain separation character (dot/slash/\u2026). Someone might be able to register a.b to cats.cool which might resolve to the same domain as if someone registers cats.cool.a and then cats.cool.a.b.  Examples  0/cats/ = 0xfe  0/cats/<empty-string/ = 0xfe.keccak(\"\")  zNS/contracts/Registrar.sol:L76-L96  function registerDomain(  uint256 parentId,  string memory name,  address domainOwner,  address minter  ) external override onlyController returns (uint256) {  // Create the child domain under the parent domain  uint256 labelHash = uint256(keccak256(bytes(name)));  address controller = msg.sender;  // Domain parents must exist  require(_exists(parentId), \"Zer0 Registrar: No parent\");  // Calculate the new domain's id and create it  uint256 domainId =  uint256(keccak256(abi.encodePacked(parentId, labelHash)));  _createDomain(domainId, domainOwner, minter, controller);  emit DomainCreated(domainId, name, labelHash, parentId, minter, controller);  return domainId;  Recommendation  Disallow empty subdomain names. Disallow domain separators in names (in the offchain component or smart contract).  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.5 zAuction, zNS - gas griefing by spamming offchain fake bids   ", "body": "  Resolution  Addressed and acknowledged with changes from zer0-os/zAuction@135b2aa. The client provided the following remark:  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.19 I have attempted to order the requires sensibly, putting the least expensive first. Please advise if the ordering is optimal. gas griefing will be mitigated in the dapp with off-client checks", "body": "  Description  The execution status of both zAuction.acceptBid and StakingController.fulfillDomainBid transactions depend on the bidder, as his approval is needed, his signature is being validated, etc. However, these transactions can be submitted by accounts that are different from the bidder account, or for accounts that do not have the required funds/deposits available, luring the account that has to perform the on-chain call into spending gas on a transaction that is deemed to fail (gas griefing). E.g. posting high-value fake bids for zAuction without having funds deposited or WETH approved.  Examples  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  zAuction/contracts/zAuction.sol:L35-L44  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  IERC721 nftcontract = IERC721(nftaddress);  accountant.Exchange(bidder, msg.sender, bid);  nftcontract.transferFrom(msg.sender, bidder, tokenid);  emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);  Recommendation  Revert early for checks that depend on the bidder before performing gas-intensive computations.  Consider adding a dry-run validation for off-chain components before transaction submission.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.6 zNS - anyone can front-run fulfillDomainBid to lock the domain setting or set different metadata    ", "body": "  Resolution                           Addressed with   zer0-os/ZNS@ab7d62a by restricting the method to only be callable by the requester.  Description  Anyone observing a call to fulfillDomainBid can front-run this call for the original bidder, provide different metadata/royalty amount, or lock the metadata, as these parameters are not part of the bidder s signature. The impact is limited as both metadata, royalty amount, and lock state can be changed by the domain owner after creation.  Examples  zNS/contracts/StakingController.sol:L120-L143  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  Recommendation  Consider adding metadata, royaltyAmount, and lockOnCreation to the message signed by the bidder if the parent should have some control over metadata and lockstatus and restrict access to this function to msg.sender==recoveredbidder.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.7 zNS- Using a digital signature as a hash preimage    ", "body": "  Resolution  Addressed with zer0-os/zNS@ab7d62a by avoiding the use of digital signatures.  Description  Using the encoded signature (r,s,v) or the hash of the signature to prevent replay or track if signatures have been seen/used is not recommended in general, as it may introduce signature malleability issues, as two different signature params (r,s,v) may be producable that validly sign the same data.  The impact for this codebase, however, is limited, due to the fact that openzeppelins ECDSA wrapper library is used which checks for malleable ECDSA signatures (high s value). We still decided to keep this as a medium issue to raise awareness, that it is bad practice to rely on the hash of signatures instead of the hash of the actual signed data for checks.  In another instance in zAuction, a global random nonce is used to prevent replay attacks. This is suboptimal and instead, the hash of the signed data (including a nonce) should be used.  Examples  zNS/contracts/StakingController.sol:L120-L152  function fulfillDomainBid(  uint256 parentId,  uint256 bidAmount,  uint256 royaltyAmount,  string memory bidIPFSHash,  string memory name,  string memory metadata,  bytes memory signature,  bool lockOnCreation,  address recipient  ) external {  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);  address recoveredBidder = recover(recoveredBidHash, signature);  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");  bytes32 hashOfSig = keccak256(abi.encode(signature));  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);  registrar.setDomainMetadataUri(id, metadata);  registrar.setDomainRoyaltyAmount(id, royaltyAmount);  registrar.transferFrom(controller, recoveredBidder, id);  if (lockOnCreation) {  registrar.lockDomainMetadataForOwner(id);  approvedBids[hashOfSig] = false;  emit DomainBidFulfilled(  metadata,  name,  recoveredBidder,  id,  parentId  );  zAuction/contracts/zAuction.sol:L35-L39  function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {  address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);  require(bidder == recoveredbidder, 'zAuction: incorrect bidder');  require(!randUsed[rand], 'Random nonce already used');  randUsed[rand] = true;  Recommendation  Consider creating the bid identifier by hashing the concatenation of all bid parameters instead. Ensure to add replay protection https://github.com/ConsenSys/zer0-zns-audit-2021-05/issues/19. Always check for the hash of the signed data instead of the hash of the encoded signature to track whether a signature has been seen before.  Consider implementing Ethereum typed structured data hashing and signing according to EIP-712.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.8 zNS - Registrar skips __ERC721Pausable_init()    ", "body": "  Resolution                           Addressed with   zer0-os/ZNS@ab7d62a  Description  The initialization function of registrar skips the chained initializer __ERC721Pausable_init to initialize __ERC721_init(\"Zer0 Name Service\", \"ZNS\"). This basically skips the following initialization calls:  abstract contract ERC721PausableUpgradeable is Initializable, ERC721Upgradeable, PausableUpgradeable {  function __ERC721Pausable_init() internal initializer {  __Context_init_unchained();  __ERC165_init_unchained();  __Pausable_init_unchained();  __ERC721Pausable_init_unchained();  Examples  zNS/contracts/Registrar.sol:L39-L45  function initialize() public initializer {  __Ownable_init();  __ERC721_init(\"Zer0 Name Service\", \"ZNS\");  // create the root domain  _createDomain(0, msg.sender, msg.sender, address(0));  Recommendation  consider calling the missing initializers to register the interface for erc165 if needed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.9 zNS - Registrar is ERC721PausableUpgradeable but there is no way to actually pause it    ", "body": "  Resolution                           Addressed with   zer0-os/ZNS@ab7d62a by exposing pausable functionality to the contract owner.  Description  The registrar is ownable and pausable but the functionality to pause the contract is not implemented.  zNS/contracts/Registrar.sol:L8-L12  contract Registrar is  IRegistrar,  OwnableUpgradeable,  ERC721PausableUpgradeable  Recommendation  Simplification is key. Remove the pausable functionality if the contract is not meant to be paused or consider implementing an external pause() function decorated onlyOwner.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.10 zNS - Avoid no-ops    ", "body": "  Resolution                           Addressed with   zer0-os/ZNS@ab7d62a.  Description  Code paths that are causing transactions to be ended with an ineffective outcome or no-operation (no actual state changes) are not advisable, as they consume more gas, hide misconfiguration or error cases (e.g. adding the same controller multiple times), and may impact other processes that rely upon transaction s logs.  Examples  Reject adding an already existing controller, and removing non existing controller.  zNS/contracts/Registrar.sol:L51-L67  /**  @notice Authorizes a controller to control the registrar  @param controller The address of the controller  /  function addController(address controller) external override onlyOwner {  controllers[controller] = true;  emit ControllerAdded(controller);  /**  @notice Unauthorizes a controller to control the registrar  @param controller The address of the controller  /  function removeController(address controller) external override onlyOwner {  controllers[controller] = false;  emit ControllerRemoved(controller);  Recommendation  Consider reverting code paths that end up in ineffective outcomes (i.e. no-operation) as early as possible.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/zer0-zns/"}, {"title": "5.1 InfinityPool Contract Authorization Bypass Attack    ", "body": "  Resolution  Addressed by not allowing the  Description  An attacker could create their own credential and set the Agent ID to 0, which would bypass the subjectIsAgentCaller modifier. The attacker could use this attack to borrow funds from the pool, draining any available liquidity. For example, only an Agent should be able to borrow funds from the pool and call the borrow function:  src/Pool/InfinityPool.sol:L302-L325  function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {  // 1e18 => 1 FIL, can't borrow less than 1 FIL  if (vc.value < WAD) revert InvalidParams();  // can't borrow more than the pool has  if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();  Account memory account = _getAccount(vc.subject);  // fresh account, set start epoch and epochsPaid to beginning of current window  if (account.principal == 0) {  uint256 currentEpoch = block.number;  account.startEpoch = currentEpoch;  account.epochsPaid = currentEpoch;  GetRoute.agentPolice(router).addPoolToList(vc.subject, id);  account.principal += vc.value;  account.save(router, vc.subject, id);  totalBorrowed += vc.value;  emit Borrow(vc.subject, vc.value);  // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier  asset.transfer(msg.sender, vc.value);  The following modifier checks that the caller is an Agent:  src/Pool/InfinityPool.sol:L96-L101  modifier subjectIsAgentCaller(VerifiableCredential memory vc) {  if (  GetRoute.agentFactory(router).agents(msg.sender) != vc.subject  ) revert Unauthorized();  _;  Recommendation  Ensure only an Agent can call borrow and pass the subjectIsAgentCaller modifier.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.2 Agent Data Oracle Signed Credential Front-Running Attack    ", "body": "  Resolution  Mitigated by allowing only the  Description  Recommendation  Ensure an Agent can always have new credentials that are needed. One solution would be to allow only an Agent s owner to request the credentials. The problem is that the beneficiary is also supposed to do that, but the beneficiary may also be a contract.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.3 Wrong Accounting for totalBorrowed in the InfinityPool.writeOff Function    ", "body": "  Resolution  Fixed.  Description  Here is a part of the InfinityPool.writeOff function:  src/Pool/InfinityPool.sol:L271-L287  // transfer the assets into the pool  // whatever we couldn't pay back  uint256 lostAmt = principalOwed > recoveredFunds ? principalOwed - recoveredFunds : 0;  uint256 totalOwed = interestPaid + principalOwed;  asset.transferFrom(  msg.sender,  address(this),  totalOwed > recoveredFunds ? recoveredFunds : totalOwed  );  // write off only what we lost  totalBorrowed -= lostAmt;  // set the account with the funds the pool lost  account.principal = lostAmt;  account.save(router, agentID, id);  The totalBorrowed is decreased by the lostAmt value. Instead, it should be decreased by the original account.principal value to acknowledge the loss.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.4 Wrong Accounting for totalBorrowed in the InfinityPool.pay Function    ", "body": "  Resolution                           Addressed as recommended in two pull rquests:   1,  2.  Description  If the Agent pays more than the current interest debt, the remaining payment will be accounted as repayment of the principal debt:  src/Pool/InfinityPool.sol:L382-L401  // pay interest and principal  principalPaid = vc.value - interestOwed;  // the fee basis only applies to the interest payment  feeBasis = interestOwed;  // protect against underflow  totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;  // fully paid off  if (principalPaid >= account.principal) {  // remove the account from the pool's list of accounts  GetRoute.agentPolice(router).removePoolFromList(vc.subject, id);  // return the amount of funds overpaid  refund = principalPaid - account.principal;  // reset the account  account.reset();  } else {  // interest and partial principal payment  account.principal -= principalPaid;  // move the `epochsPaid` cursor to mark the account as \"current\"  account.epochsPaid = block.number;  Let s focus on the totalBorrowed changes:  src/Pool/InfinityPool.sol:L387  totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;  This value is supposed to be decreased by the principal that is repaid. So there are 2 mistakes in the calculation:  Should be totalBorrowed instead of 0.  The principalPaid cannot be larger than the account.principal in that calculation.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.5 The beneficiaryWithdrawable Function Can Be Called by Anyone    ", "body": "  Resolution  Fixed by removing beneficiary logic completely.  Description  The beneficiaryWithdrawable function is supposed to be called by the Agent when a beneficiary is trying to withdraw funds:  src/Agent/AgentPolice.sol:L320-L341  function beneficiaryWithdrawable(  address recipient,  address sender,  uint256 agentID,  uint256 proposedAmount  ) external returns (  uint256 amount  ) {  AgentBeneficiary memory beneficiary = _agentBeneficiaries[agentID];  address benneficiaryAddress = beneficiary.active.beneficiary;  // If the sender is not the owner of the Agent or the beneficiary, revert  if(  !(benneficiaryAddress == sender || (IAuth(msg.sender).owner() == sender && recipient == benneficiaryAddress) )) {  revert Unauthorized();  beneficiary,  amount  ) = beneficiary.withdraw(proposedAmount);  // update the beneficiary in storage  _agentBeneficiaries[agentID] = beneficiary;  This function reduces the quota that is supposed to be transferred during the withdraw call:  src/Agent/Agent.sol:L343-L352  sendAmount = agentPolice.beneficiaryWithdrawable(receiver, msg.sender, id, sendAmount);  else if (msg.sender != owner()) {  revert Unauthorized();  // unwrap any wfil needed to withdraw  _poolFundsInFIL(sendAmount);  // transfer funds  payable(receiver).sendValue(sendAmount);  The issue is that anyone can call this function directly, and the quota will be reduced without funds being transferred.  Recommendation  Ensure only the Agent can call this function.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.6 An Agent Can Borrow Even With Existing Debt in Interest Payments    ", "body": "  Resolution  Mitigated by adding a limit to the remaining interest debt when borrowing. So an agent should have an interest debt that is no larger than 1 day.  Description  To borrow funds, an Agent has to call the borrow function of the pool:  src/Pool/InfinityPool.sol:L302-L325  function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {  // 1e18 => 1 FIL, can't borrow less than 1 FIL  if (vc.value < WAD) revert InvalidParams();  // can't borrow more than the pool has  if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();  Account memory account = _getAccount(vc.subject);  // fresh account, set start epoch and epochsPaid to beginning of current window  if (account.principal == 0) {  uint256 currentEpoch = block.number;  account.startEpoch = currentEpoch;  account.epochsPaid = currentEpoch;  GetRoute.agentPolice(router).addPoolToList(vc.subject, id);  account.principal += vc.value;  account.save(router, vc.subject, id);  totalBorrowed += vc.value;  emit Borrow(vc.subject, vc.value);  // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier  asset.transfer(msg.sender, vc.value);  Let s assume that the Agent already had some funds borrowed. During this function execution, the current debt status is not checked. The principal debt increases after borrowing, but account.epochsPaid remains the same. So the pending debt will instantly increase as if the borrowing happened on account.epochsPaid.  Recommendation  Ensure the debt is paid when borrowing more funds.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.7 The AgentPolice.distributeLiquidatedFunds() Function Can Have Undistributed Residual Funds    ", "body": "  Resolution  Mitigated by returning the excess funds in  Description  When an Agent is liquidated, the liquidator (owner of the protocol) is supposed to try to redeem as many funds as possible and re-distribute them to the pools:  src/Agent/AgentPolice.sol:L185-L191  function distributeLiquidatedFunds(uint256 agentID, uint256 amount) external {  if (!liquidated[agentID]) revert Unauthorized();  // transfer the assets into the pool  GetRoute.wFIL(router).transferFrom(msg.sender, address(this), amount);  _writeOffPools(agentID, amount);  The problem is that in the pool, it s accounted that the amount of funds can be larger than the debt. In that case, the pool won t transfer more funds than the pool needs:  src/Pool/InfinityPool.sol:L275-L289  uint256 totalOwed = interestPaid + principalOwed;  asset.transferFrom(  msg.sender,  address(this),  totalOwed > recoveredFunds ? recoveredFunds : totalOwed  );  // write off only what we lost  totalBorrowed -= lostAmt;  // set the account with the funds the pool lost  account.principal = lostAmt;  account.save(router, agentID, id);  emit WriteOff(agentID, recoveredFunds, lostAmt, interestPaid);  If that happens, the remaining funds will be stuck in the AgentPolice contract.  Recommendation  Return the residual funds to the Agent s owner or process them in some way so they are not lost.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.8 An Agent Can Be Upgraded Even if There Is No New Implementation    ", "body": "  Resolution  Mitigated by introducing a new version control mechanism. This solution also adds centralized power. The owner can create a new deployer with an arbitrary (even lower) version number, while agents can only upgrade to a higher version. Also, agents are forced to upgrade to a new version in another  pull request.  Description  Agents can be upgraded to a new implementation, and only the Agent s owner can call the upgrade function:  src/Agent/AgentFactory.sol:L51-L72  function upgradeAgent(  address agent  ) external returns (address newAgent) {  IAgent oldAgent = IAgent(agent);  address owner = IAuth(address(oldAgent)).owner();  uint256 agentId = agents[agent];  // only the Agent's owner can upgrade, and only a registered agent can be upgraded  if (owner != msg.sender || agentId == 0) revert Unauthorized();  // deploy a new instance of Agent with the same ID and auth  newAgent = GetRoute.agentDeployer(router).deploy(  router,  agentId,  owner,  IAuth(address(oldAgent)).operator()  );  // Register the new agent and unregister the old agent  agents[newAgent] = agentId;  // transfer funds from old agent to new agent and mark old agent as decommissioning  oldAgent.decommissionAgent(newAgent);  // delete the old agent from the registry  agents[agent] = 0;  The issue is that the owner can trigger the upgrade even if no new implementation exists. Multiple possible problems derive from it.  Upgrading to the current implementation of the Agent will break the logic because the current version is not calling the migrateMiner function, so all the miners will stay with the old Agent, and their funds will be lost.  The owner can accidentally trigger multiple upgrades simultaneously, leading to a loss of funds (https://github.com/ConsenSysDiligence/glif-audit-2023-04/issues/2).  The owner also has no control over the new version of the Agent. To increase decentralization, it s better to pass the deployer s address as a parameter additionally.  Recommendation  Ensure the upgrades can only happen when there is a new version of an Agent, and the owner controls this version.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.9 Potential Re-Entrancy Issues When Upgrading the Contracts    ", "body": "  Resolution                           The issue is   mitigated by removing the old agent before the potential re-entrancy.  Description  The protocol doesn t have any built-in re-entrancy protection mechanisms. That mainly explains by using the wFIL token, which is not supposed to give that opportunity. And also by carefully using FIL transfers.  However, there are some places in the code where things may go wrong in the future. For example, when upgrading an Agent:  src/Agent/AgentFactory.sol:L51-L72  function upgradeAgent(  address agent  ) external returns (address newAgent) {  IAgent oldAgent = IAgent(agent);  address owner = IAuth(address(oldAgent)).owner();  uint256 agentId = agents[agent];  // only the Agent's owner can upgrade, and only a registered agent can be upgraded  if (owner != msg.sender || agentId == 0) revert Unauthorized();  // deploy a new instance of Agent with the same ID and auth  newAgent = GetRoute.agentDeployer(router).deploy(  router,  agentId,  owner,  IAuth(address(oldAgent)).operator()  );  // Register the new agent and unregister the old agent  agents[newAgent] = agentId;  // transfer funds from old agent to new agent and mark old agent as decommissioning  oldAgent.decommissionAgent(newAgent);  // delete the old agent from the registry  agents[agent] = 0;  Here, we see the oldAgent.decommissionAgent(newAgent); call happens before the oldAgent is deleted. Inside this function, we see:  src/Agent/Agent.sol:L200-L212  function decommissionAgent(address _newAgent) external {  // only the agent factory can decommission an agent  AuthController.onlyAgentFactory(router, msg.sender);  // if the newAgent has a mismatching ID, revert  if(IAgent(_newAgent).id() != id) revert Unauthorized();  // set the newAgent in storage, which marks the upgrade process as starting  newAgent = _newAgent;  uint256 _liquidAssets = liquidAssets();  // Withdraw all liquid funds from the Agent to the newAgent  _poolFundsInFIL(_liquidAssets);  // transfer funds to new agent  payable(_newAgent).sendValue(_liquidAssets);  Here, the FIL is transferred to a new contract which is currently unimplemented and unknown. Potentially, the fallback function of this contract could trigger a re-entrancy attack. If that s the case, during the execution of this function, there will be two contracts that are active agents with the same ID, and the attacker can try to use that maliciously.  Recommendation  Be very cautious with further implementations of agents and pools. Also, consider using reentrancy protection in public functions.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.10 InfinityPool Is Subject to a Donation With Inflation Attack if Emtied.    ", "body": "  Resolution  this issue will not be fixed in the current version of the contracts since some of the shares were already minted. The next iteration of the pool will have a more generic fix to this issue.  Description  Since InfinityPool is an implementation of the ERC4626 vault, it is too susceptible to inflation attacks. An attacker could front-run the first deposit and inflate the share price to an extent where the following deposit will be less than the value of 1 wei of share resulting in 0 shares minted. The attacker could conduct the inflation by means of self-destructing of another contract. In the case of GLIF this attack is less likely on the first pool since GLIF team accepts predeposits so some amount of shares was already minted. We do suggest fixing this issue before the next pool is deployed and no pre-stake is generated.  Examples  src/Pool/InfinityPool.sol:L491-L516  /*//////////////////////////////////////////////////////////////  4626 LOGIC  //////////////////////////////////////////////////////////////*/  /**  @dev Converts `assets` to shares  @param assets The amount of assets to convert  @return shares - The amount of shares converted from assets  /  function convertToShares(uint256 assets) public view returns (uint256) {  uint256 supply = liquidStakingToken.totalSupply(); // Saves an extra SLOAD if totalSupply is non-zero.  return supply == 0 ? assets : assets * supply / totalAssets();  /**  @dev Converts `shares` to assets  @param shares The amount of shares to convert  @return assets - The amount of assets converted from shares  /  function convertToAssets(uint256 shares) public view returns (uint256) {  uint256 supply = liquidStakingToken.totalSupply(); // Saves an extra SLOAD if totalSupply is non-zero.  return supply == 0 ? shares : shares * totalAssets() / supply;  Recommendation  Since the pool does not need to accept donations, the easiest way to handle this case is to use virtual price, where the balance of the contract is duplicated in a separate variable.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.11 MaxWithdraw Should Potentially Account for the Funds Available in the Ramp.    ", "body": "  Resolution                           Partially fixed in   https://github.com/glif-confidential/pools/issues/462 but the ramp balance is still not accounted for.  Description  Since InfinityPool is ERC4626 it should also support the MaxWithdraw method. According to the EIP it should include any withdrawal limitation that the participant could encounter. At the moment the MaxWithdraw function returns the maximum amount of IOU tokens rather than WFIL. Since IOU token is not the asset token of the vault, this behavior is not ideal.  Examples  src/Pool/InfinityPool.sol:L569-L571  function maxWithdraw(address owner) public view returns (uint256) {  return convertToAssets(liquidStakingToken.balanceOf(owner));  Recommendation  We suggest considering returning the maximum amount of WFIL withdrawal which should account for Ramp balance.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.12 The Upgradeability of MinerRegistry, AgentPolice, and Agent Is Overcomplicated and Has a Hight Chance of Errors.   ", "body": "  Description  During the engagement, we have identified a few places that signify that the Agent, MinerRegistry and AgentPolice can be upgraded, for example:  Ability to migrate the miner from one version of the Agent to another inside the migrateMiner.  Ability to refreshRoutes that would update the AgentPolice and MinerRegistry addresses for a given Agent.  Ability to decommission pool. We believe that this functionality is present it is not very well thought through. For example, both MinerRegistry and AgentPolice are not upgradable but have mappings inside of them.  src/Agent/AgentPolice.sol:L51-L60  mapping(uint256 => bool) public liquidated;  /// @notice `_poolIDs` maps agentID to the pools they have actively borrowed from  mapping(uint256 => uint256[]) private _poolIDs;  /// @notice `_credentialUseBlock` maps signature bytes to when a credential was used  mapping(bytes32 => uint256) private _credentialUseBlock;  /// @notice `_agentBeneficiaries` maps an Agent ID to its Beneficiary struct  mapping(uint256 => AgentBeneficiary) private _agentBeneficiaries;  src/Agent/MinerRegistry.sol:L18-L20  mapping(bytes32 => bool) private _minerRegistered;  mapping(uint256 => uint64[]) private _minersByAgent;  That means that any time these contracts would need to be upgraded, the contents of those mappings will need to be somehow recreated in the new contract. That is not trivial since it is not easy to obtain all values of a mapping. This will also require an additional protocol-controlled setter ala kickstart mapping functions that are not ideal.  In the case of Agent if the contract was upgradable there would be no need for a process of migrating miners that can be tedious and opens possibilities for errors. Since protocol has a lot of centralization and trust assumptions already, having upgradability will not contribute to it a lot.  We also believe that during the upgrade of the pool, the PoolToken will stay the same in the new pool. That means that the minting and burning permissions of the share tokens have to be carefully updated or checked in a manner that does not require the address of the pool to be constant. Since we did not have access to this file, we can not check if that is done correctly.  Recommendation  Consider using upgradable contracts or have a solid upgrade plan that is well-tested before an emergency situation occurs.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.13 Mint Function in the Infinity Pool Will Emit the Incorrect Value.    ", "body": "  Resolution  Fixed by emitting the right value.  Description  Examples  src/Pool/InfinityPool.sol:L449-L457  function mint(uint256 shares, address receiver) public isOpen returns (uint256 assets) {  if(shares == 0) revert InvalidParams();  // These transfers need to happen before the mint, and this is forcing a higher degree of coupling than is ideal  assets = previewMint(shares);  asset.transferFrom(msg.sender, address(this), assets);  liquidStakingToken.mint(receiver, shares);  assets = convertToAssets(shares);  emit Deposit(msg.sender, receiver, assets, shares);  Recommendation  Use the assets value computed by the previewMint when emitting the event.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.14 Incorrect Operator Used    ", "body": "  Resolution  Fixed.  Description  Minor typo in the InfinityPool where the -= should be replaced with -.  Examples  src/Pool/InfinityPool.sol:L200  return balance -= feesCollected;  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.15 Potential Overpayment Due to Rounding Imprecision   ", "body": "  Resolution  The issue is acknowledged and the potential loss is considered tolerable.  Description  Inside the InifintyPool the pay function might accept unaccounted files. Imagine a situation where an Agent is trying to repay only the fees portion of the debt. In that case, the following branch will be executed:  src/Pool/InfinityPool.sol:L373-L381  if (vc.value <= interestOwed) {  // compute the amount of epochs this payment covers  // vc.value is not WAD yet, so divWadDown cancels the extra WAD in interestPerEpoch  uint256 epochsForward = vc.value.divWadDown(interestPerEpoch);  // update the account's `epochsPaid` cursor  account.epochsPaid += epochsForward;  // since the entire payment is interest, the entire payment is used to compute the fee (principal payments are fee-free)  feeBasis = vc.value;  } else {  The issue is if the value does not divide by the interestPerEpoch exactly, any remainder will remain in the InfinityPool.  src/Pool/InfinityPool.sol:L376  uint256 epochsForward = vc.value.divWadDown(interestPerEpoch);  Recommendation  Since the remainder will most likely not be too large this is not critical, but ideally, those remaining funds would be included in the refund variable.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.16 jumpStartAccount Should Be Subject to the Same Approval Checks as Regular Borrow.    ", "body": "  Resolution  Will not be fixed due to the complexity of the fix which will require passing verified credentials to be executed.  Description  InfinityPool contract has the ability to kick start an account that will have a debt position in this pool.  Examples  src/Pool/InfinityPool.sol:L673-L689  function jumpStartAccount(address receiver, uint256 agentID, uint256 accountPrincipal) external onlyOwner {  Account memory account = _getAccount(agentID);  // if the account is already initialized, revert  if (account.principal != 0) revert InvalidState();  // create the account  account.principal = accountPrincipal;  account.startEpoch = block.number;  account.epochsPaid = block.number;  // save the account  account.save(router, agentID, id);  // add the pool to the agent's list of borrowed pools  GetRoute.agentPolice(router).addPoolToList(agentID, id);  // mint the iFIL to the receiver, using principal as the deposit amount  liquidStakingToken.mint(receiver, convertToShares(accountPrincipal));  // account for the new principal in the total borrowed of the pool  totalBorrowed += accountPrincipal;  Recommendation  We suggest that this action is subject to the same rules as the standard borrow action. Thus checks on DTE, LTV and DTI should be done if possible.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "5.17 No Miner Migration Is Happening in the Current Implementation of the Agent  ", "body": "  Description  All miners should be transferred from the old Agent to a new one when upgrading an Agent. To do so, the new Agent is supposed to call the migrateMiner function for every miner:  src/Agent/Agent.sol:L219-L235  function migrateMiner(uint64 miner) external {  if (newAgent != msg.sender) revert Unauthorized();  uint256 newId = IAgent(newAgent).id();  if (  // first check to make sure the agentFactory knows about this \"agent\"  GetRoute.agentFactory(router).agents(newAgent) != newId ||  // then make sure this is the same agent, just upgraded  newId != id ||  // check to ensure this miner was registered to the original agent  !minerRegistry.minerRegistered(id, miner)  ) revert Unauthorized();  // propose an ownership change (must be accepted in v2 agent)  miner.changeOwnerAddress(newAgent);  emit MigrateMiner(msg.sender, newAgent, miner);  The problem is that this function is not called in the current Agent implementation. Since it s just the first version of an Agent contract, it s not a big issue. There is only one edge case where this may be a vulnerability. That may happen if the owner of an Agent decides to upgrade the contract to the same version. It is possible to do, and in that case, the miners  funds will be lost.  Recommendation  It s important to remember to call migrateMiner in a new version and not allow upgrading to the same implementation.  ", "labels": ["Consensys", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/04/glif-filecoin-infinitypool/"}, {"title": "4.1 ERC1400ERC20 whitelist circumvents partition restrictions    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#13.  Description  ERC1400/1410 enable  partially fungible tokens  in that not all tokens are equivalent. A specific use case is placing restrictions on some tokens, such as lock-up periods.  The whitelist in ERC1400ERC20 circumvents these restrictions. When a token holder uses the ERC20 transfer function, tokens are transferred from that user s  default partitions , which a user can choose themselves by calling ERC1410.setDefaultPartitions. This means they can transfer tokens from any partition, and the only restriction that s placed on the transfer is that the recipient must be whitelisted.  It should be noted that the comment and error message around the whitelisting feature suggests that it is meant to be applied to both the sender and recipient:  code/contracts/token/ERC20/ERC1400ERC20.sol:L24-L30  /**  @dev Modifier to verify if sender and recipient are whitelisted.  /  modifier isWhitelisted(address recipient) {  require(_whitelisted[recipient], \"A3: Transfer Blocked - Sender lockup period not ended\");  _;  Remediation  There are many possibilities, but here are concrete suggestions for addressing this:  Require whitelisting both the sender and recipient, and make sure that whitelisted accounts only own (and will only own) unrestricted tokens.  Make sure that the only whitelisted recipients are those that apply partition restrictions when receiving tokens. (I.e. they implement the modified ERC777 receiving hook, examine the source partition, and reject transfers that should not occur.)  Instead of implementing the ERC20 interface on top of the ERC1400 token, support transferring out of the ERC1400 token and into a standard ERC20 token. Partition restrictions can then be applied on the ERC1400 transfer, and once ERC20 tokens are obtained, they can be transferred without restriction.  Don t allow token holders to set their own default partitions. Rather, have the token specify a single, unrestricted partition that is used for all ERC20 transfers.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.2 Certificate controllers do not always constrain the last argument    ", "body": "  Resolution                           The existing back end already does its own ABI encoding, which means it s not vulnerable to this issue. Documentation has been added in   https://gitlab.com/ConsenSys/client/fr/dauriel/smart-contracts/certificate-controller/merge_requests/9 to ensure future maintainers understand this potential issue.  Description  The certificate controllers (CertificateControllerNonce and CertificateControllerSalt) are used by passing a signature as a final argument in a function call. This signature is over the other arguments to the function. Specifically, the signature must match the call data that precedes the signature.  The way this is implemented assumes standard ABI encoding of parameters, but there s actually some room for manipulation by a malicious user. This manipulation can allow the user to change some of the call data without invalidating the signature.  The following code is from CertificateControllerNonce, but similar logic applies to CertificateControllerSalt:  code2/contracts/CertificateControllerNonce.sol:L127-L134  bytes memory payload;  assembly {  let payloadsize := sub(calldatasize, 160)  payload := mload(0x40) // allocate new memory  mstore(0x40, add(payload, and(add(add(payloadsize, 0x20), 0x1f), not(0x1f)))) // boolean trick for padding to 0x40  mstore(payload, payloadsize) // set length  calldatacopy(add(add(payload, 0x20), 4), 4, sub(payloadsize, 4))  Here the signature is over all call data except the final 160 bytes. 160 bytes makes sense because the byte array is length 97, and it s preceded by a 32-byte size. This is a total of 129 bytes, and typical ABI encoded pads this to the next multiple of 32, which is 160.  If an attacker does not pad their arguments, they can use just 129 bytes for the signature or even 128 bytes if the v value happens to be 0. This means that when checking the signature, not only will the signature be excluded, but also the 31 or 32 bytes that come before the signature. This means the attacker can call a function with a different final argument than the one that was signed.  That final argument is, in many cases, the number of tokens to transfer, redeem, or issue.  Mitigating factors  For this to be exploitable, the attacker has to be able to obtain a signature over shortened call data.  If the signer accepts raw arguments and does its own ABI encoding with standard padding, then there s likely no opportunity for an attacker to exploit this vulnerability. (They can shorten the call data length when they make the function call later, but the signature won t match.)  Remediation  We have two suggestions for how to address this:  Instead of signatures being checked directly against call data, compute a new hash based on the decoded values, e.g. keccak256(abi.encode(argument1, argument2, ...)).  Address this at the signing layer (off chain) by doing the ABI encoding there and denying an attacker the opportunity to construct their own call data.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.3 Salt-based certificate controller is subject to signature replay    ", "body": "  Resolution                           This is fixed in   https://gitlab.com/ConsenSys/client/fr/dauriel/smart-contracts/certificate-controller/merge_requests/8.  Description  The salt-based certificate controller prevents signature replay by storing each full signature. Only a signature that is exactly identical to a previously-used signature will be rejected.  For ECDSA signatures, each signature has a second S value (and flipped V to match) that will recover the same address. An attacker can produce such a second signature trivially without knowing the signer s private key. This gives an attacker a way to produce a new unique signature based on a previously used one. This effectively means every signature can be used twice.  code2/contracts/CertificateControllerSalt.sol:L25-L32  modifier isValidCertificate(bytes memory data) {  require(  _certificateSigners[msg.sender] || _checkCertificate(data, 0, 0x00000000),  \"A3: Transfer Blocked - Sender lockup period not ended\"  );  _usedCertificate[data] = true; // Use certificate  References  See https://smartcontractsecurity.github.io/SWC-registry/docs/SWC-117.  Remediation  Instead of rejecting used signatures based on the full signature value, keep track of used salts (which are then better referred to as  nonces ).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.4 EIP-1400 is missing canTransfer* functions   ", "body": "  Description  The EIP-1400 states defines the interface to be implemented containing the 3 functions:  // Transfer Validity  function canTransfer(address _to, uint256 _value, bytes _data) external view returns (byte, bytes32);  function canTransferFrom(address _from, address _to, uint256 _value, bytes _data) external view returns (byte, bytes32);  function canTransferByPartition(address _from, address _to, bytes32 _partition, uint256 _value, bytes _data) external view returns (byte, bytes32, bytes32);  These functions were not implemented in ERC1400, thus making the implementation not completely compatible with EIP-1400.  In case the deployed contract needs to be added as a  lego block  part of a another application, there is a high chance that it will not correctly function. That external application could potentially call the EIP-1400 functions canTransfer, canTransferFrom or canTransferByPartition, in which case the transaction will likely fail.  This means that the current implementation will not be able to become part of external markets, exchanges or applications that need to interact with a generic EIP-1400 implementation.  Remediation  Even if the functions do not correctly reflect the transfer possibility, their omission can break other contracts interacting with the implementation.  A suggestion would be to add these functions and make them always return true. This way the contracts interacting with the current implementation do not break when they call these functions, while the actual transfer of the tokens is still limited by the current logic.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.5 ERC777 incompatibilities    ", "body": "  Resolution                           This is fixed in   Description  As noted in the README, the ERC777 contract is not actually compatible with ERC 777.  Functions and events have been renamed, and the hooks ERC777TokensRecipient and ERC777TokensSender have been modified to add a partition parameter.  This means no tools that deal with standard ERC 777 contracts will work with this code s tokens.  Remediation  We suggest renaming these contracts to not use the term  ERC777 , as they lack compatibility. Most importantly, we recommend not using the interface names  ERC777TokensRecipient  and  ERC777TokensSender  when looking up the appropriate hook contracts via ERC 1820. Contracts that handle that interface will not be capable of handling the modified interface used here.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.6 Buffer over-read in ERC1410._getDestinationPartition    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#16.  Description  There s no check that data is at least 64 bytes long, so the following code can read past the end of data:  code/contracts/token/ERC1410/ERC1410.sol:L348-L361  function _getDestinationPartition(bytes32 fromPartition, bytes memory data) internal pure returns(bytes32 toPartition) {  bytes32 changePartitionFlag = 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff;  bytes32 flag;  assembly {  flag := mload(add(data, 32))  if(flag == changePartitionFlag) {  assembly {  toPartition := mload(add(data, 64))  } else {  toPartition = fromPartition;  The only caller is _transferByPartition, which only checks that data.length > 0:  code/contracts/token/ERC1410/ERC1410.sol:L263-L264  if(operatorData.length != 0 && data.length != 0) {  toPartition = _getDestinationPartition(fromPartition, data);  Depending on how the compiler chooses to lay out memory, the next data in memory is probably the operatorData buffer, so data may inadvertently be read from there.  Remediation  Check for sufficient length (at least 64 bytes) before attempting to read it.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.7 ERC20/ERC777 compatibility: ERC20 transfer functions should not revert if the recipient is a contract without a registered ERC777TokensRecipient implementation    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#17.  Description  This will block transfers to a contract that doesn t have an ERC777TokensRecipient implementation. This is in violation of ERC 777, which says:  If the recipient is a contract, which has not registered an ERC777TokensRecipient implementation; then the token contract:  MUST revert if the tokensReceived hook is called from a mint or send call.  SHOULD continue processing the transaction if the tokensReceived hook is called from an ERC20 transfer or transferFrom call.  Remediation  Make sure that ERC20-compatible transfer calls do not set preventLocking to true.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.8 ERC777 compatibility: authorizeOperator and revokeOperator should revert when the caller and operator are the same account    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#19.  Description  From ERC 777:  The autohrizeOperator implementation does not do that:  code/contracts/token/ERC777/ERC777.sol:L144-L147  function authorizeOperator(address operator) external {  _authorizedOperator[operator][msg.sender] = true;  emit AuthorizedOperator(operator, msg.sender);  The same holds for revokeOperator:  code/contracts/token/ERC777/ERC777.sol:L155-L158  function revokeOperator(address operator) external {  _authorizedOperator[operator][msg.sender] = false;  emit RevokedOperator(operator, msg.sender);  Remediation  Add require(operator != msg.sender) to those two functions.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.9 Token receiver can mint gas tokens with sender s gas   ", "body": "  Description  When a transfer is executed, there are hooks activated on the sender s and on the receiver s side.  This is possible because the contract implements ERC1820Client which allows any address to define an implementation:  contracts/ERC1820Client.sol:L16-L19  function setInterfaceImplementation(string memory _interfaceLabel, address _implementation) internal {  bytes32 interfaceHash = keccak256(abi.encodePacked(_interfaceLabel));  ERC1820REGISTRY.setInterfaceImplementer(address(this), interfaceHash, _implementation);  Considering the receiver s side:  contracts/ERC1400.sol:L1016-L1020  recipientImplementation = interfaceAddr(to, ERC1400_TOKENS_RECIPIENT);  if (recipientImplementation != address(0)) {  IERC1400TokensRecipient(recipientImplementation).tokensReceived(msg.sig, partition, operator, from, to, value, data, operatorData);  The sender has to pay for the gas for the transaction to go through.  Because the receiver can define a contract to be called when receiving the tokens, and the sender has to pay for the gas, the receiver can mint gas tokens (or waste the gas).  Remediation  Because this is the way Ethereum works and the implementation allows calling external methods, there s no recommended remediation for this issue. It s just something the senders need to be aware of.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.10 Missing ERC Functions    ", "body": "  Resolution  This is fixed in https://github.com/ConsenSys/ERC1400/pull/18.  Description  There exist some functions, such as isOperator() ,that are part of the ERC1410 spec. Removing functions expected by ERC may break things like block explorers that expect to be able to query standard contracts for relevant metadata.  Remediation  It would be good to explicitly state any expected incompatibilities.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.11 Inaccurate error message in ERC777ERC20.approve    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#20.  Description  If the spender is address 0, the revert message says that the receiver is not eligible.  code/contracts/token/ERC20/ERC777ERC20.sol:L153  require(spender != address(0), \"A6: Transfer Blocked - Receiver not eligible\");  Remediation  Fix the revert message to match the actual issue.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.12 Non-standard treatment of a from address of 0    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#21.  Description  A number of functions throughout the system treat a from address of 0 as equivalent to msg.sender. In some cases, this seems to violate existing standards (e.g. in ERC20 transfers). In other cases, it is merely surprising.  ERC1400ERC20.transferFrom and ERC777ERC20.transferFrom both treat a from address as 0 as equivalent to msg.sender. This is unexpected behavior for an ERC20 token.  Examples  code/contracts/ERC1400.sol:L206-L214  function canOperatorTransferByPartition(bytes32 partition, address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  view  returns (byte, bytes32, bytes32)  if(!_checkCertificate(operatorData, 0, 0x8c0dee9c)) { // 4 first bytes of keccak256(operatorTransferByPartition(bytes32,address,address,uint256,bytes,bytes))  return(hex\"A3\", \"\", partition); // Transfer Blocked - Sender lockup period not ended  } else {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/ERC1400.sol:L417-L421  function redeemFrom(address from, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC20/ERC1400ERC20.sol:L180-L181  function transferFrom(address from, address to, uint256 value) external isWhitelisted(to) returns (bool) {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC20/ERC777ERC20.sol:L179-L180  function transferFrom(address from, address to, uint256 value) external isWhitelisted(to) returns (bool) {  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC777/ERC777.sol:L194-L198  function transferFromWithData(address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC777/ERC777.sol:L226-L230  function redeemFrom(address from, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC1410/ERC1410.sol:L130-L142  function operatorTransferByPartition(  bytes32 partition,  address from,  address to,  uint256 value,  bytes calldata data,  bytes calldata operatorData  external  isValidCertificate(operatorData)  returns (bytes32)  address _from = (from == address(0)) ? msg.sender : from;  code/contracts/token/ERC1410/ERC1410.sol:L430-L434  function transferFromWithData(address from, address to, uint256 value, bytes calldata data, bytes calldata operatorData)  external  isValidCertificate(operatorData)  address _from = (from == address(0)) ? msg.sender : from;  Remediation  Remove this fallback logic and always use the from address that was passed in. This avoids surprises where, for example, an uninitialized value leads to loss of funds.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.13 ERC1410 s redeem and redeemFrom should revert    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#22.  Description  ERC1410 contains two functions: redeem and redeemFrom that  erase  the underlying ERC777 versions of these functions because those functions don t handle partitions.  These functions silently succeed, while they should probably fail by reverting.  Examples  code/contracts/token/ERC1410/ERC1410.sol:L441-L453  /**  [NOT MANDATORY FOR ERC1410 STANDARD][OVERRIDES ERC777 METHOD]  @dev Empty function to erase ERC777 redeem() function since it doesn't handle partitions.  /  function redeem(uint256 /*value*/, bytes calldata /*data*/) external { // Comments to avoid compilation warnings for unused variables.  /**  [NOT MANDATORY FOR ERC1410 STANDARD][OVERRIDES ERC777 METHOD]  @dev Empty function to erase ERC777 redeemFrom() function since it doesn't handle partitions.  /  function redeemFrom(address /*from*/, uint256 /*value*/, bytes calldata /*data*/, bytes calldata /*operatorData*/) external { // Comments to avoid compilation warnings for unused variables.  Remediation  Add a revert() (possibly with a reason) so callers know that the call failed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.14 Unclear why operatorData.length is checked in _transferByPartition    ", "body": "  Resolution                           This code is actually correct. When   Description  It s unclear why operatorData.length is being checked here:  code/contracts/token/ERC1410/ERC1410.sol:L263-L264  if(operatorData.length != 0 && data.length != 0) {  toPartition = _getDestinationPartition(fromPartition, data);  Remediation  Consider removing that check.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.15 Global partition enumeration can run into gas limits    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#25.  Description  In ERC1410, partitions are created on demand by issuing or transferring tokens, and these new partitions are added to the array _totalPartitions. When one of these partitions is later emptied, it s removed from that array with the following code in _removeTokenFromPartition:  code/contracts/token/ERC1410/ERC1410.sol:L303-L313  // If the total supply is zero, finds and deletes the partition.  if(_totalSupplyByPartition[partition] == 0) {  for (uint i = 0; i < _totalPartitions.length; i++) {  if(_totalPartitions[i] == partition) {  _totalPartitions[i] = _totalPartitions[_totalPartitions.length - 1];  delete _totalPartitions[_totalPartitions.length - 1];  _totalPartitions.length--;  break;  Finding the partition requires iterating over the entire array. This means that _removeTokenFromPartition can become very expensive and eventually bump up against the block gas limit if lots of partitions are created. This could be an attack vector for a malicious operator.  The same issue applies to a token holder s list of partitions, where transferring tokens in a large number of partitions to that token holder may block them from being able to transfer tokens out:  code/contracts/token/ERC1410/ERC1410.sol:L291-L301  // If the balance of the TokenHolder's partition is zero, finds and deletes the partition.  if(_balanceOfByPartition[from][partition] == 0) {  for (uint i = 0; i < _partitionsOf[from].length; i++) {  if(_partitionsOf[from][i] == partition) {  _partitionsOf[from][i] = _partitionsOf[from][_partitionsOf[from].length - 1];  delete _partitionsOf[from][_partitionsOf[from].length - 1];  _partitionsOf[from].length--;  break;  Remediation  Removing an item from a set can be accomplished in constant time if the set uses both an array (for storing the values) and a mapping of values to their index in that array. See https://programtheblockchain.com/posts/2018/06/03/storage-patterns-set/ for one example of doing this.  It also may be reasonable to cap the number of possible partitions or lock them down to a constant set of values on deployment, depending on the use case for the token.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.16 Optimization: redundant delete in ERC1400. _removeTokenFromPartition    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#23.  Description  Reducing the size of an array automatically deletes the removed elements, so the first of these two lines is redundant:  code/contracts/token/ERC1410/ERC1410.sol:L296-L297  delete _partitionsOf[from][_partitionsOf[from].length - 1];  _partitionsOf[from].length--;  The same applies here:  code/contracts/token/ERC1410/ERC1410.sol:L308-L309  delete _totalPartitions[_totalPartitions.length - 1];  _totalPartitions.length--;  Remediation  Remove the redundant deletions to save a little gas.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "4.17 Avoid hardcoding function selectors    ", "body": "  Resolution                           This is fixed in   ConsenSys/ERC1400#24.  Description  In ERC1400, hardcoded function selectors can be replaced with this.transferByPartition.selector and this.operatorTransferByPartition.selector.  Examples  code/contracts/ERC1400.sol:L184  if(!_checkCertificate(data, 0, 0xf3d490db)) { // 4 first bytes of keccak256(transferByPartition(bytes32,address,uint256,bytes))  code/contracts/ERC1400.sol:L211  if(!_checkCertificate(operatorData, 0, 0x8c0dee9c)) { // 4 first bytes of keccak256(operatorTransferByPartition(bytes32,address,address,uint256,bytes,bytes))  Remediation  Replace the hardcoded function selectors with this.<method>.selector.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/codefi-erc1400-assessment/"}, {"title": "5.1 safeRagequit makes you lose funds     in Pull Pattern", "body": "  Resolution  Description  safeRagequit and ragequit functions are used for withdrawing funds from the LAO. The difference between them is that ragequit function tries to withdraw all the allowed tokens and safeRagequit function withdraws only some subset of these tokens, defined by the user.  It s needed in case the user or GuildBank is blacklisted in some of the tokens and the transfer reverts. The problem is that even though you can quit in that case, you ll lose the tokens that you exclude from the list.  To be precise, the tokens are not completely lost, they will belong to the LAO and can still potentially be transferred to the user who quit. But that requires a lot of trust, coordination, time and anyone can steal some part of these tokens.  Recommendation  Implementing pull pattern for token withdrawals should solve the issue. Users will be able to quit the LAO and burn their shares but still keep their tokens in the LAO s contract for some time if they can t withdraw them right now.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.2 Creating proposal is not trustless     in Pull Pattern", "body": "  Resolution  this issue no longer exists in the Pull Pattern update, due to the fact that emergency processing and in function ERC20 transfers are removed.  Description  Usually, if someone submits a proposal and transfers some amount of tribute tokens, these tokens are transferred back if the proposal is rejected. But if the proposal is not processed before the emergency processing, these tokens will not be transferred back to the proposer. This might happen if a tribute token or a deposit token transfers are blocked.  code/contracts/Moloch.sol:L407-L411  if (!emergencyProcessing) {  require(  proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),  \"failing vote token transfer failed\"  );  Tokens are not completely lost in that case, they now belong to the LAO shareholders and they might try to return that money back. But that requires a lot of coordination and time and everyone who ragequits during that time will take a part of that tokens with them.  Recommendation  Pull pattern for token transfers would solve the issue.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.3 Emergency processing can be blocked     in Pull Pattern", "body": "  Resolution  Emergency Processing no longer exists in the Pull Pattern update.  Description  The main reason for the emergency processing mechanism is that there is a chance that some token transfers might be blocked. For example, a sender or a receiver is in the USDC blacklist. Emergency processing saves from this problem by not transferring tribute token back to the user (if there is some) and rejecting the proposal.  code/contracts/Moloch.sol:L407-L411  if (!emergencyProcessing) {  require(  proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),  \"failing vote token transfer failed\"  );  The problem is that there is still a deposit transfer back to the sponsor and it could be potentially blocked too. If that happens, proposal can t be processed and the LAO is blocked.  Recommendation  Implementing pull pattern for all token withdrawals would solve the problem. The alternative solution would be to also keep the deposit tokens in the LAO, but that makes sponsoring the proposal more risky for the sponsor.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.4 Token Overflow might result in system halt or loss of funds    ", "body": "  Resolution                           Fixed in   fd2da6, and  32ad9b  by allowing overflows in most balance calculations (e.g.  Description  If a token overflows, some functionality such as processProposal, cancelProposal will break due to safeMath reverts. The overflow could happen because the supply of the token was artificially inflated to oblivion.  This issue was pointed out by Heiko Fisch in Telegram chat.  Examples  Any function using internalTransfer() can result in an overflow:  contracts/Moloch.sol:L631-L634  function max(uint256 x, uint256 y) internal pure returns (uint256) {  return x >= y ? x : y;  Recommendation  We recommend to allow overflow for broken or malicious tokens. This is to prevent system halt or loss of funds. It should be noted that in case an overflow occurs, the balance of the token will be incorrect for all token holders in the system.  rageKick, rageQuit were fixed by not using safeMath within the function code, however this fix is risky and not recommended, as there are other overflows in other functions that might still result in system halt or loss of funds.  One suggestion is having a function named unsafeInternalTransfer() which does not use safeMath for the cases that overflow should be allowed. This mainly adds better readability to the code.  It is still a risky fix and a better solution should be planned.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.5 Whitelisted tokens limit    ", "body": "  Resolution  mitigated by having separate limits for number of whitelisted tokens (for non-zero balance and for zero balance) in 486f1b3 and follow up commits. That s helpful because it s much cheaper to process tokens with zero balance in the guild bank and you can have much more whitelisted tokens overall.  uint256 constant MAX_TOKEN_WHITELIST_COUNT = 400; // maximum number of whitelisted tokens  uint256 constant MAX_TOKEN_GUILDBANK_COUNT = 200; // maximum number of tokens with non-zero balance in guildbank  uint256 public totalGuildBankTokens = 0; // total tokens with non-zero balance in guild bank  It should be noted that this is an estimated limit based on the manual calculations and current OP code gas costs. DAO members should consider splitting the DAO into two if more than 100 tokens with non-zero balance are used in the DAO to be safe.  Description  _ragequit function is iterating over all whitelisted tokens:  contracts/Moloch.sol:L507-L513  for (uint256 i = 0; i < tokens.length; i++) {  uint256 amountToRagequit = fairShare(userTokenBalances[GUILD][tokens[i]], sharesAndLootToBurn, initialTotalSharesAndLoot);  // deliberately not using safemath here to keep overflows from preventing the function execution (which would break ragekicks)  // if a token overflows, it is because the supply was artificially inflated to oblivion, so we probably don't care about it anyways  userTokenBalances[GUILD][tokens[i]] -= amountToRagequit;  userTokenBalances[memberAddress][tokens[i]] += amountToRagequit;  If the number of tokens is too big, a transaction can run out of gas and all funds will be blocked forever. Ballpark estimation of this number is around 300 tokens based on the current OpCode gas costs and the block gas limit.  Recommendation  A simple solution would be just limiting the number of whitelisted tokens.  If the intention is to invest in many new tokens over time, and it s not an option to limit the number of whitelisted tokens, it s possible to add a function that removes tokens from the whitelist. For example, it s possible to add a new type of proposals, that is used to vote on token removal if the balance of this token is zero.  Before voting for that, shareholders should sell all the balance of that token.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.6 Summoner can steal funds using bailout     in Pull Pattern", "body": "  Resolution  Description  Currently, there are 2 major reasons for using the bailout function:  Kick someone out of the LAO. If the shareholders vote for kicking somebody, the kicked user goes to jail at first. If the LAO kicks someone, it s important not to steal user s funds, but remove them from profit-sharing as soon as possible. Currently, because the user can potentially block some token transfers, funds can t be transferred and the user is still having loot and is participation in a profit-sharing. In order to avoid that, bailout function was introduced. It allows anyone to transfer kicked user s funds to the summoner if the user does not call safeRagequit (which forces the user to lose some funds). The intention is for the summoner to transfer these funds to the kicked member afterwards. The issue here is that it requires a lot of trust to the summoner on the one hand, and requires more time to kick the member out of the LAO.   lost private key  problem. If someone s private key was lost, shareholders can allow summoner to transfer funds from any user whose keys were lost. The problem is that any member s funds can be stolen by the LAO members and the summoner like that. So every member should keep track of that kind of proposal and is forced to do the ragequit if that proposal passes. That decreases trustlessness because if a user is not tracking the system for some time, the user s money can possibly be stolen.  Recommendation  To solve these issues, these 2 intentions should be split into 2 different mechanisms. By implementing pull pattern for token transfers, kicked member won t be able to block the ragekick and the LAO members would be able to kick anyone much quicker. There is no need to keep the bailout for this intention.  If  lost private key  problem should be addressed in the LAO, the time period for the funds recovery should be big because there is no need to do the recovery asap. Recovery can be done without a preliminary kick and can even cover not only the shares and loot, but also tokens that should be withdrawn (if pull pattern is implemented)  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.7 Sponsorship front-running     in Pull Pattern", "body": "  Resolution  this issue no longer exists in the Pull Pattern update with Major severity, as mentioned in the recommendation, the front-running vector is still open but no rationale exist for such a behaviour.  Description  If proposal submission and sponsorship are done in 2 different transactions, it s possible to front-run the sponsorProposal function by any member. The incentive to do that is to be able to block the proposal afterwards. It s sometimes possible to block the proposal by getting blacklisted at depositToken. In that case, the proposal won t be accepted and the emergency processing is going to happen next. Currently, if the attacker can become whitelisted again, he might even not lose the deposit tokens. If not, it will block the whole system forever and everyone would have to ragequit (but that s the part of another issue).  Recommendation  Pull pattern for token transfers will solve the issue. Front-running will still be possible but it doesn t affect anything.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.8 Delegate assignment front-running   ", "body": "  Description  Any member can front-run another member s delegateKey assignment.  if you try to submit an address as your delegateKey, someone else can try to assign your delegate address tp themselves. While incentive of this action is unclear, it s possible to block some address from being a delegate forever. ragekick and ragequit do not free the delegate address and the delegate itself also cannot change the address.  The possible attack could be that a well-known hard-to-replace multisig address is assigned as a delegateKey and someone else take this address to block it.  Also, if the malicious member is about to ragequit or be kicked, it s possible to do this attack without losing anything.  The only way to free the delegate is to make it a member, but then it can never be a delegate after.  Recommendation  Make it possible for a delegateKey to approve delegateKey assignment or cancel the current delegation. And additionally, it may be valuable to clear the delegate address in the _ragequit function.  Commit-reveal methods can also be used to mitigate this attack.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.9 No votes are still valid after the ragequit/ragekick   ", "body": "  Description  Shareholders can vote for the upcoming proposals 2 weeks before they can be executed. If they ragequit or get ragekicked, their votes are still considered valid. And while the LAO does not allow anyone to ragequit before the last proposal with Yes vote is processed, it s still possible to quit the LAO and having active No votes on some proposals.  It s not naturally expected behaviour because by that time a user ragequits, they are not part of the LAO and do not have any voting power. Moreover, there is no incentive not to vote No just to fail all the possible proposals, because the user won t be sharing any consequences of the result of these proposals. And even incentivized to vote No for every proposal just as the act of revenge for the ragekick.  Recommendation  The problem is mitigated by the fact that all rejected proposals can be submitted again and be processed a few weeks after.  It s possible to remove all the No votes from the proposals after user s ragekick/ragequit.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.10 Dilution bound should be a fixed-point number   ", "body": "  Resolution  a per-proposal dilution bound was considered for the v1, but kept it global in the interest of code simplicity.  Description  The dilution bound is designed to mitigate an issue where a proposal is passed, then many users ragequit from the DAO and the remaining members have to pay more than they initially intended to. Because of that, the proposal will be automatically rejected if the total amount of shares becomes dilutionBound times less than it was before. The problem is that dilutionBound is an integer value and it s impossible to configure it to decimal values such as 1.2, for example.  Recommendation  Make dilutionBound a fixed-point number.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.11 Whitelist proposal duplicate   ", "body": "  Description  Every time when a whitelist proposal is sponsored, it s checked that there is no other sponsored whitelist proposal with the same token. This is done in order to avoid proposal duplicates.  code/contracts/Moloch.sol:L277-L281  // whitelist proposal  if (proposal.flags[4]) {  require(!tokenWhitelist[address(proposal.tributeToken)], \"cannot already have whitelisted the token\");  require(!proposedToWhitelist[address(proposal.tributeToken)], 'already proposed to whitelist');  proposedToWhitelist[address(proposal.tributeToken)] = true;  The issue is that even though you can t sponsor a duplicate proposal, you can still submit a new proposal with the same token.  Recommendation  Check that there is currently no sponsored proposal with the same token on proposal submission.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "5.12 Moloch - bool[6] flags can be changed to a dedicated structure   ", "body": "  Resolution                           The Moloch team decided to leave the   Description  The Moloch contract uses a structure that includes an array of bools to store a few flags about the proposal:  code/contracts/Moloch.sol:L88  bool[6] flags; // [sponsored, processed, didPass, cancelled, whitelist, guildkick]  This makes reasoning about the correctness of the code a bit complicated because one needs to remember what each item in the flag list stands for. The make the reader s life simpler a dedicated structure can be created that incorporates all of the required flags.  Examples  bool[6] memory flags; // [sponsored, processed, didPass, cancelled, whitelist, guildkick]  Recommendation  Based on the provided examples change the bool[6] flags to the proposed examples.  Flags as bool array with enum (proposed)  This second contract implements the flags as a defined structure with each named element representing a specific flag. This method makes clear which flag is accessed because they are referred to by the name, not by the index.  This third contract has the least amount of changes to the code and uses an enum structure to handle the index.  pragma solidity 0.5.15;  contract FlagsEnum {  struct Proposal {  address applicant;  uint value;  bool[3] flags; // [sponsored, processed, kicked]  enum ProposalFlags {  SPONSORED,  PROCESSED,  KICKED  uint proposalCount;  mapping(uint256 => Proposal) public proposals;  function addProposal(uint _value, bool _sponsored, bool _processed, bool _kicked) public returns (uint) {  Proposal memory proposal = Proposal({  applicant: msg.sender,  value: _value,  flags: [_sponsored, _processed, _kicked]  });  proposals[proposalCount] = proposal;  proposalCount += 1;  return (proposalCount);  function getProposal(uint _proposalId) public view returns (address, uint, bool, bool, bool) {  return (  proposals[_proposalId].applicant,  proposals[_proposalId].value,  proposals[_proposalId].flags[uint(ProposalFlags.SPONSORED)],  proposals[_proposalId].flags[uint(ProposalFlags.PROCESSED)],  proposals[_proposalId].flags[uint(ProposalFlags.KICKED)]  );  6 Tool-Based Analysis  Several tools were used to perform automated analysis of the reviewed contracts. These issues were reviewed by the audit team, and relevant issues are listed in the Issue Details section.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "6.1 MythX", "body": "  MythX is a security analysis API for Ethereum smart contracts. It performs multiple types of analysis, including fuzzing and symbolic execution, to detect many common vulnerability types. The tool was used for automated vulnerability discovery for all audited contracts and libraries. More details on MythX can be found at mythx.io.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "6.2 Ethlint", "body": "  Ethlint is an open source project for linting Solidity code. Only security-related issues were reviewed by the audit team.  Below is the raw output of the Ethlint vulnerability scan:  contracts/GuildBank.sol  13:8     warning    Provide an error message for require()                                                     error-reason  23:12    warning    Provide an error message for require()                                                     error-reason  34:8     warning    Provide an error message for require()                                                     error-reason  36:27    warning    There should be no whitespace or comments between the opening brace '{' and first item.    whitespace  36:37    warning    There should be no whitespace or comments between the last item and closing brace '}'.     whitespace  contracts/Moloch.sol  34:4      warning    Line exceeds the limit of 145 characters                                   max-len  41:4      warning    Line exceeds the limit of 145 characters                                   max-len  42:4      warning    Line exceeds the limit of 145 characters                                   max-len  76:8      warning    Line exceeds the limit of 145 characters                                   max-len  169:24    warning    Avoid using 'now' (alias to 'block.timestamp').                            security/no-block-members  262:8     warning    Line exceeds the limit of 145 characters                                   max-len  272:49    error      String literal must be quoted with double quotes.                          quotes  280:74    error      String literal must be quoted with double quotes.                          quotes  285:57    error      String literal must be quoted with double quotes.                          quotes  362:8     warning    Line exceeds the limit of 145 characters                                   max-len  517:8     warning    Line exceeds the limit of 145 characters                                   max-len  540:13    warning    Assignment operator must have exactly single space on both sides of it.    operator-whitespace  559:8     warning    Error message exceeds max length of 76 characters                          error-reason  583:8     warning    Error message exceeds max length of 76 characters                          error-reason  641:15    warning    Avoid using 'now' (alias to 'block.timestamp').                            security/no-block-members  contracts/oz/ERC20.sol  128:8    warning    Provide an error message for require()    error-reason  143:8    warning    Provide an error message for require()    error-reason  157:8    warning    Provide an error message for require()    error-reason  171:8    warning    Provide an error message for require()    error-reason  172:8    warning    Provide an error message for require()    error-reason  contracts/oz/SafeMath.sol  10:8    warning    Provide an error message for require()    error-reason  17:8    warning    Provide an error message for require()    error-reason  24:8    warning    Provide an error message for require()    error-reason  32:8    warning    Provide an error message for require()    error-reason  contracts/test-helpers/Submitter.sol  9:2     error    Only use indent of 4 spaces.    indentation  11:2    error    Only use indent of 4 spaces.    indentation  13:2    error    Only use indent of 4 spaces.    indentation  15:0    error    Only use indent of 4 spaces.    indentation  17:2    error    Only use indent of 4 spaces.    indentation  39:0    error    Only use indent of 4 spaces.    indentation  41:2    error    Only use indent of 4 spaces.    indentation  51:0    error    Only use indent of 4 spaces.    indentation  53:2    error    Only use indent of 4 spaces.    indentation  63:0    error    Only use indent of 4 spaces.    indentation  contracts/tokens/ClaimsToken.sol  95:1     error      Only use indent of 4 spaces.              indentation  98:1     error      Only use indent of 4 spaces.              indentation  100:1    error      Only use indent of 4 spaces.              indentation  102:1    error      Only use indent of 4 spaces.              indentation  105:1    error      Only use indent of 4 spaces.              indentation  112:0    error      Only use indent of 4 spaces.              indentation  121:1    error      Only use indent of 4 spaces.              indentation  129:0    error      Only use indent of 4 spaces.              indentation  140:1    error      Only use indent of 4 spaces.              indentation  148:0    error      Only use indent of 4 spaces.              indentation  154:1    error      Only use indent of 4 spaces.              indentation  160:0    error      Only use indent of 4 spaces.              indentation  167:1    error      Only use indent of 4 spaces.              indentation  173:0    error      Only use indent of 4 spaces.              indentation  180:1    error      Only use indent of 4 spaces.              indentation  184:0    error      Only use indent of 4 spaces.              indentation  190:1    error      Only use indent of 4 spaces.              indentation  197:0    error      Only use indent of 4 spaces.              indentation  203:1    error      Only use indent of 4 spaces.              indentation  208:0    error      Only use indent of 4 spaces.              indentation  216:1    error      Only use indent of 4 spaces.              indentation  226:0    error      Only use indent of 4 spaces.              indentation  232:1    error      Only use indent of 4 spaces.              indentation  234:1    error      Only use indent of 4 spaces.              indentation  237:0    error      Only use indent of 4 spaces.              indentation  239:1    error      Only use indent of 4 spaces.              indentation  243:2    warning    Provide an error message for require()    error-reason  246:0    error      Only use indent of 4 spaces.              indentation  251:1    error      Only use indent of 4 spaces.              indentation  260:0    error      Only use indent of 4 spaces.              indentation  268:1    error      Only use indent of 4 spaces.              indentation  276:0    error      Only use indent of 4 spaces.              indentation  contracts/tokens/Token.sol  25:8    warning    Provide an error message for require()    error-reason  \u2716 44 errors, 28 warnings found.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "6.3 Surya", "body": "  Surya is a utility tool for smart contract systems. It provides a number of visual outputs and information about the structure of smart contracts. It also supports querying the function call graph in multiple ways to aid in the manual inspection and control flow analysis of contracts.  Below is a complete list of functions with their visibility and modifiers:  Files Description Table  contracts/GuildBank.sol  d4329bc7836a1800eb2376da05f76f1783700b9a  contracts/Moloch.sol  8f55cc17fcf0488acdc9dd2261dca1e08f42c4ac  contracts/oz/ERC20.sol  6db943e86683ce536b8e75e79d7fb80a02b855ae  contracts/oz/IERC20.sol  f249341b598ed60fdb987fc6dd05b6cd15da7b6b  contracts/oz/ReentrancyGuard.sol  115a19532af141450ea30ad141aecb76b79035b4  contracts/oz/SafeMath.sol  b86ab5a6679fd597c3a0412d31080893beeb653a  contracts/test-helpers/Submitter.sol  7b29e3178cb4c7848851a8c92661a0e12fee7489  contracts/tokens/ClaimsToken.sol  11bb8b648de195efbca13df15e10b3e6a75fcab6  contracts/tokens/Token.sol  7c193d22ad069e368aba4fa9bc3d4c28e8e1973b  Contracts Description Table  Function Name  Visibility  Mutability  Modifiers  GuildBank  Implementation  <Constructor>  Public    withdraw  Public    onlyOwner  withdrawToken  Public    onlyOwner  fairShare  Internal \ud83d\udd12  Moloch  Implementation  ReentrancyGuard  <Constructor>  Public    submitProposal  Public    nonReentrant  submitWhitelistProposal  Public    nonReentrant  submitGuildKickProposal  Public    nonReentrant  _submitProposal  Internal \ud83d\udd12  sponsorProposal  Public    nonReentrant onlyDelegate  submitVote  Public    nonReentrant onlyDelegate  processProposal  Public    nonReentrant  processWhitelistProposal  Public    nonReentrant  processGuildKickProposal  Public    nonReentrant  _didPass  Internal \ud83d\udd12  _validateProposalForProcessing  Internal \ud83d\udd12  _returnDeposit  Internal \ud83d\udd12  ragequit  Public    nonReentrant onlyMember  safeRagequit  Public    nonReentrant onlyMember  _ragequit  Internal \ud83d\udd12  ragekick  Public    nonReentrant  bailout  Public    nonReentrant  cancelProposal  Public    nonReentrant  updateDelegateKey  Public    nonReentrant onlyShareholder  max  Internal \ud83d\udd12  getCurrentPeriod  Public    NO   getProposalQueueLength  Public    NO   getProposalFlags  Public    NO   canRagequit  Public    NO   canBailout  Public    NO   hasVotingPeriodExpired  Public    NO   getMemberProposalVote  Public    NO   ERC20  Implementation  IERC20  totalSupply  Public    NO   balanceOf  Public    NO   allowance  Public    NO   transfer  Public    NO   approve  Public    NO   transferFrom  Public    NO   increaseAllowance  Public    NO   decreaseAllowance  Public    NO   _transfer  Internal \ud83d\udd12  _mint  Internal \ud83d\udd12  _burn  Internal \ud83d\udd12  _approve  Internal \ud83d\udd12  _burnFrom  Internal \ud83d\udd12  IERC20  Interface  transfer  External    NO   approve  External    NO   transferFrom  External    NO   totalSupply  External    NO   balanceOf  External    NO   allowance  External    NO   ReentrancyGuard  Implementation  <Constructor>  Internal \ud83d\udd12  SafeMath  Library  mul  Internal \ud83d\udd12  div  Internal \ud83d\udd12  sub  Internal \ud83d\udd12  add  Internal \ud83d\udd12  Submitter  Implementation  <Constructor>  Public    submitProposal  Public    NO   submitWhitelistProposal  Public    NO   submitGuildKickProposal  Public    NO   ERC20Detailed  Implementation  IERC20  <Constructor>  Public    name  Public    NO   symbol  Public    NO   decimals  Public    NO   IClaimsToken  Interface  withdrawFunds  External    NO   availableFunds  External    NO   totalReceivedFunds  External    NO   ClaimsToken  Implementation  IClaimsToken, ERC20, ERC20Detailed  <Constructor>  Public    ERC20Detailed  transfer  Public    NO   transferFrom  Public    NO   totalReceivedFunds  External    NO   availableFunds  Public    NO   _registerFunds  Internal \ud83d\udd12  _calcUnprocessedFunds  Internal \ud83d\udd12  _claimFunds  Internal \ud83d\udd12  _prepareWithdraw  Internal \ud83d\udd12  ClaimsTokenERC20Extension  Implementation  IClaimsToken, ClaimsToken  <Constructor>  Public    ClaimsToken  withdrawFunds  External    NO   tokenFallback  Public    onlyFundsToken  Token  Implementation  ERC20  <Constructor>  Public    updateTransfersEnabled  External    NO   updateTransfersReturningFalse  External    NO   transfer  Public    NO   Legend  Function can modify state  Function is payable  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/01/the-lao/"}, {"title": "6.1 RocketDaoNodeTrusted - DAO takeover during deployment/bootstrapping ", "body": "  Resolution  The node registration is enabled by default (node.registration.enabled) but the client intends to change this to disabling the registration until bootstrap mode finished.  We are intending to set node registrations to false during deployment, then open it up when we need to register our oDAO nodes  Description  The initial deployer of the RocketStorage contract is set as the Guardian/Bootstrapping role. This guardian can bootstrap the TrustedNode and Protocol DAO, add members, upgrade components, change settings.  Right after deploying the DAO contract the member count is zero. The Guardian can now begin calling any of the bootstrapping functions to add members, change settings, upgrade components, interact with the treasury, etc. The bootstrapping configuration by the Guardian is unlikely to all happen within one transaction which might allow other parties to interact with the system while it is being set up.  RocketDaoNodeTrusted also implements a recovery mode that allows any registered node to invite themselves directly into the DAO without requiring approval from the Guardian or potential other DAO members as long as the total member count is below daoMemberMinCount (3). The Guardian itself is not counted as a DAO member as it is a supervisory role.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L202-L215  /**** Recovery ***************/  // In an explicable black swan scenario where the DAO loses more than the min membership required (3), this method can be used by a regular node operator to join the DAO  // Must have their ID, email, current RPL bond amount available and must be called by their current registered node account  function memberJoinRequired(string memory _id, string memory _email) override public onlyLowMemberMode onlyRegisteredNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {  // Ok good to go, lets add them  (bool successPropose, bytes memory responsePropose) = getContractAddress('rocketDAONodeTrustedProposals').call(abi.encodeWithSignature(\"proposalInvite(string,string,address)\", _id, _email, msg.sender));  // Was there an error?  require(successPropose, getRevertMsg(responsePropose));  // Get the to automatically join as a member (by a regular proposal, they would have to manually accept, but this is no ordinary situation)  (bool successJoin, bytes memory responseJoin) = getContractAddress(\"rocketDAONodeTrustedActions\").call(abi.encodeWithSignature(\"actionJoinRequired(address)\", msg.sender));  // Was there an error?  require(successJoin, getRevertMsg(responseJoin));  This opens up a window during the bootstrapping phase where any Ethereum Address might be able to register as a node (RocketNodeManager.registerNode) if node registration is enabled (default=true) rushing into RocketDAONodeTrusted.memberJoinRequired adding themselves (up to 3 nodes) as trusted nodes to the DAO. The new DAO members can now take over the DAO by issuing proposals, waiting 2 blocks to vote/execute them (upgrade, change settings while Guardian is changing settings, etc.). The Guardian role can kick the new DAO members, however, they can invite themselves back into the DAO.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsNode.sol:L19-L19  setSettingBool(\"node.registration.enabled\", true);  Recommendation  Disable the DAO recovery mode during bootstrapping. Disable node registration by default and require the guardian to enable it. Ensure that bootstrapDisable (in both DAO contracts) performs sanity checks as to whether the DAO bootstrapping finished and permissions can effectively be revoked without putting the DAO at risk or in an irrecoverable state (enough members bootstrapped, vital configurations like registration and other settings are configured, \u2026).  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.2 RocketTokenRETH - sandwiching opportunity on price updates    ", "body": "  Resolution  This issue is being addressed in a currently pending pull request. By introducing a delay between an rETH deposit and a subsequent transfer or burn, sandwiching a price update transaction is not possible anymore. Specifically, a deposit delay of circa one day is introduced:  https://github.com/rocket-pool/rocketpool/pull/201/files#diff-0387338dc5dd7edd0a03766cfdaaee42d021d4e781239d5ebbff359c81497839R146-R150  // This is called by the base ERC20 contract before all transfer, mint, and burns  function _beforeTokenTransfer(address from, address, uint256) internal override {  // Don't run check if this is a mint transaction  if (from != address(0)) {  // Check which block the user's last deposit was  bytes32 key = keccak256(abi.encodePacked(\"user.deposit.block\", from));  uint256 lastDepositBlock = getUint(key);  if (lastDepositBlock > 0) {  // Ensure enough blocks have passed  RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));  uint256 blocksPassed = block.number.sub(lastDepositBlock);  require(blocksPassed > rocketDAOProtocolSettingsNetwork.getRethDepositDelay(), \"Not enough time has passed since deposit\");  // Clear the state as it's no longer necessary to check this until another deposit is made  deleteUint(key);  In the current version, it is correctly enforced that a deposit delay of zero is not possible.  Description  The rETH token price is not coupled to the amount of rETH tokens in circulation on the Ethereum chain. The price is reported by oracle nodes and committed to the system via a voting process. The price of rETH changes If 51% of nodes observe and submit the same price information. If nodes fail to find price consensus for a block, then the rETH price might be stale.  There is an opportunity for the user to front-run the price update right before it is committed. If the next price is higher than the previous (typical case), this gives an instant opportunity to perform a risk-free ETH -> rETH -> ETH exchange for profit. In the worst case, one could drain all the ETH held by the RocketTokenRETH contract + excess funds stored in the vault.  Note: there seems to be a \"network.submit.balances.frequency\" price and balance submission frequency of 24hrs. However, this frequency is not enforced, and it is questionable if it makes sense to pin the price for 24hrs.  Note: the total supply of the RocketTokenRETH contract may be completely disconnected from the reported total supply for RETH via oracle nodes.  Examples  The amount of ETH was only staked during this one process for the price update duration and unlikely to be useful to the system. This way, a whale (only limited by the max deposit amount set on deposit) can drain the RocketTokenRETH contract from all its ETH and excess eth funds.  mempool observed: submitPrice tx (an effective transaction that changes the price) wrapped with buying rETH and selling rETH for ETH:  RocketDepositPool.deposit() at old price => mints rETH at current rate  RocketNetworkPrices.submitPrices(newRate)  RocketTokenRETH.burn(balanceOf(msg.sender) => burns rETH for ETH at new rate  deposit (virtually no limit with 1000ETH being the limit right now)  rocketpool-2.5-Tokenomics-updates/contracts/contract/deposit/RocketDepositPool.sol:L63-L67  require(rocketDAOProtocolSettingsDeposit.getDepositEnabled(), \"Deposits into Rocket Pool are currently disabled\");  require(msg.value >= rocketDAOProtocolSettingsDeposit.getMinimumDeposit(), \"The deposited amount is less than the minimum deposit size\");  require(getBalance().add(msg.value) <= rocketDAOProtocolSettingsDeposit.getMaximumDepositPoolSize(), \"The deposit pool size after depositing exceeds the maximum size\");  // Mint rETH to user account  rocketTokenRETH.mint(msg.value, msg.sender);  trustedNodes submitPrice (changes params for getEthValue and getRethValue)  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updatePrices(_block, _rplPrice);  immediately burn at new rate (as params for getEthValue changed)  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRETH.sol:L107-L124  function burn(uint256 _rethAmount) override external {  // Check rETH amount  require(_rethAmount > 0, \"Invalid token burn amount\");  require(balanceOf(msg.sender) >= _rethAmount, \"Insufficient rETH balance\");  // Get ETH amount  uint256 ethAmount = getEthValue(_rethAmount);  // Get & check ETH balance  uint256 ethBalance = getTotalCollateral();  require(ethBalance >= ethAmount, \"Insufficient ETH balance for exchange\");  // Update balance & supply  _burn(msg.sender, _rethAmount);  // Withdraw ETH from deposit pool if required  withdrawDepositCollateral(ethAmount);  // Transfer ETH to sender  msg.sender.transfer(ethAmount);  // Emit tokens burned event  emit TokensBurned(msg.sender, _rethAmount, ethAmount, block.timestamp);  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.3 RocketDaoNodeTrustedActions - Incomplete implementation of member challenge process    ", "body": "  Resolution                           As of the Smartnode s   Description  Nodes do not seem to monitor ActionChallengeMade events so that they could react to challenges  Nodes do not implement actionChallengeDecide and, therefore, cannot successfully stop a challenge  Funds/Tribute sent along with the challenge will be locked forever in the RocketDAONodeTrustedActions contract. There s no means to recover the funds.  It is questionable whether the incentives are aligned well enough for anyone to challenge stale nodes. The default of 1 eth compared to the risk of the  malicious  or  stale  node exiting themselves is quite high. The challenger is not incentivized to challenge someone other than for taking over the DAO. If the tribute is too low, this might incentivize users to grief trusted nodes and force them to close a challenge.  Requiring that the challenge initiator is a different registered node than the challenge finalized is a weak protection since the system is open to anyone to register as a node (even without depositing any funds.)  block time is subject to fluctuations. With the default of 43204 blocks, the challenge might expire at 5 days (10 seconds block time), 6.5 days (13 seconds Ethereum target median block time), 7 days (14 seconds), or more with historic block times going up to 20 seconds for shorter periods.  A minority of trusted nodes may use this functionality to boot other trusted node members off the DAO issuing challenges once a day until the DAO member number is low enough to allow them to reach quorum for their own proposals or until the member threshold allows them to add new nodes without having to go through the proposal process at all.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsMembers.sol:L22-L24  setSettingUint('members.challenge.cooldown', 6172);              // How long a member must wait before performing another challenge, approx. 1 day worth of blocks  setSettingUint('members.challenge.window', 43204);               // How long a member has to respond to a challenge. 7 days worth of blocks  setSettingUint('members.challenge.cost', 1 ether);               // How much it costs a non-member to challenge a members node. It's free for current members to challenge other members.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L204-L206  // In the event that the majority/all of members go offline permanently and no more proposals could be passed, a current member or a regular node can 'challenge' a DAO members node to respond  // If it does not respond in the given window, it can be removed as a member. The one who removes the member after the challenge isn't met, must be another node other than the proposer to provide some oversight  // This should only be used in an emergency situation to recover the DAO. Members that need removing when consensus is still viable, should be done via the 'kick' method.  Recommendation  Implement the challenge-response process before enabling users to challenge other nodes. Implement means to detect misuse of this feature for griefing e.g. when one trusted node member forces another trusted node to defeat challenges over and over again (technical controls, monitoring).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.4 RocketDAOProtocolSettings/RocketDAONodeTrustedSettings - anyone can set/overwrite settings until contract is declared  deployed    ", "body": "  Resolution  The client is aware of and acknowledges this potential issue. As with the current contracts the deployed flag is always set in the constructor and there will be no window for someone else to interact with the contract before this flag is set. The following statement was provided:  [\u2026] this method is purely to set the initial default vars. It shouldn t be run again due to the deployment flag being flagged incase that contract is upgraded and those default vars aren t removed.  Additionally, it was suggested to add safeguards to the access restricting modifier, to only allowing the guardian to change settings if a settings contract  forgets  to set the deployed flag in the constructor (Note: the deployed flag must be set with the deploing transaction or else there might be a window for someone to interact with the contract before it is fully configured).  Description  The onlyDAOProtocolProposal modifier guards all state-changing methods in this contract. However, analog to issue 6.5, the access control is disabled until the variable settingsNameSpace.deployed is set. If this contract is not deployed and configured in one transaction, anyone can update the contract while left unprotected on the blockchain.  See issue 6.5 for a similar issue.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L18-L23  modifier onlyDAOProtocolProposal() {  // If this contract has been initialised, only allow access from the proposals contract  if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress('rocketDAOProtocolProposals') == msg.sender, \"Only DAO Protocol Proposals contract can update a setting\");  _;  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L18-L22  modifier onlyDAONodeTrustedProposal() {  // If this contract has been initialised, only allow access from the proposals contract  if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress('rocketDAONodeTrustedProposals') == msg.sender, \"Only DAO Node Trusted Proposals contract can update a setting\");  _;  There are at least 9 more occurrences of this pattern.  Recommendation  Restrict access to the methods to a temporary trusted account (e.g. guardian) until the system bootstrapping phase ends by setting deployed to true.  ", "labels": ["Consensys", "Critical", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.5 RocketStorage - anyone can set/update values before the contract is initialized    ", "body": "  Resolution  Fixed by restricting access to the guardian while the contract is not yet initialized. The relevant changeset is rocket-pool/rocketpool@495a51f. The client provided the following statement:  tx.origin is only used in this deployment instance and should be safe since no external contracts are interacted with  The client is aware of the implication of using tx.origin and that the guardian should never be used to interact with third-party contracts as the contract may be able to impersonate the guardian changing settings in the storage contract during that transaction.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/rocketpool-rp3.0-updates/contracts/contract/RocketStorage.sol#L31-L32  Description  According to the deployment script, the contract is deployed, and settings are configured in multiple transactions. This also means that for a period of time, the contract is left unprotected on the blockchain. Anyone can delete/set any value in the centralized data store. An attacker might monitor the mempool for new deployments of the RocketStorage contract and front-run calls to contract.storage.initialised setting arbitrary values in the system.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L31  modifier onlyLatestRocketNetworkContract() {  // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled  if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {  // Make sure the access is permitted to only contracts in our Dapp  require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");  _;  Recommendation  Restrict access to the methods to a temporary trusted account (e.g. guardian) until the system bootstrapping phase ends by setting initialised to true.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.6 RocketDAOProposals - Unpredictable behavior due to short vote delay    Addressed", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by changing the default delay  Description  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L167-L170  require(_startBlock > block.number, \"Proposal start block must be in the future\");  require(_durationBlocks > 0, \"Proposal cannot have a duration of 0 blocks\");  require(_expiresBlocks > 0, \"Proposal cannot have a execution expiration of 0 blocks\");  require(_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");  The default vote delay configured in the system is 1 block.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsProposals.sol:L21-L21  setSettingUint('proposal.vote.delay.blocks', 1);                 // How long before a proposal can be voted on after it is created. Approx. Next Block  A vote is immediately passed when the required quorum is reached which allows it to be executed. This means that a group that is holding enough voting power can propose a change, wait for two blocks (block.number (of time of proposal creation) + configuredDelay (1) + 1 (for ACTIVE state), then vote and execute for the proposal to pass for it to take effect almost immediately after only 2 blocks (<30seconds).  Settings can be changed after 30 seconds which might be unpredictable for other DAO members and not give them enough time to oppose and leave the DAO.  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change after two blocks. The only guarantee is that users can be sure the settings don t change for the next block if no proposal is active.  We recommend giving the user advance notice of changes with a delay. For example, all upgrades should require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.7 RocketRewardPool - Unpredictable staking rewards as stake can be added just before claiming and rewards may be paid to to operators that do not provide a service to the system   Partially Addressed", "body": "  Resolution  Partially addressed in branch rp3.0-updates (rocket-pool/rocketpool@b424ca1) by changing the withdrawal requirements to 150% of the effective RPL.  The client provided the following statement:  Node operators can now only withdraw RPL above their 150% effective RPL stake.  Description  Nodes/TrustedNodes earn rewards based on the current share of the effective RPL stake provided backing the number of Minipools they run. The reward is paid out regardless of when the effective node stake was provided, as long as it is present just before the call to claim(). This means the reward does not take into account how long the stake was provided. The effective RPL stake is the nodes RPL stake capped at a maximum of halfDepositUserAmount * 150% * nr_of_minipools(node) / RPLPrice. If the node does not run any Minipools, the effective RPL stake is zero.  Since effective stake can be added just before calling the claim() method (effectively trying to get a reward for a period that passed without RPL being staked for the full duration), this might create an unpredictable outcome for other participants, as adding significant stake (requires creating Minipools and staking the max per pool; the stake is locked for at least the duration of a reward period rpl.rewards.claim.period.blocks) shifts the shares users get for the fixed total amount of rewards. This can be unfair if the first users claimed their reward, and then someone is artificially inflating the total amount of shares by adding more stake to get a bigger part of the remaining reward. However, this comes at the cost of the registered node having to create more Minipools to stake more, requiring an initial deposit (16ETH, or 0ETH under certain circumstances for trusted nodes) by the actor attempting to get a larger share of the rewards. The risk of losing funds for this actor, however, is rather low, as they can immediately dissolve() and close() the Minipool to refund their node deposit as NETH right after claiming the reward only losing the gas spent on the various transactions.  This can be extended to a node operator creating a Minipool and staking the maximum amount before calling claim to remove the Minipool right after, freeing up the ETH that was locked in the Minipool until the next reward period starts. The node operator is not providing any service to the network, loses some value in ETH for gas but may compensate that with the RPL staking rewards. If the node amassed a significant amount of RPL stake, they might even try to flash-loan enough ETH to spawn Minipools to inflate their effective stake and earn most of the rewards to return the loan RPL profit.  By staking just before claiming, the node effectively can earn rewards for 2 reward periods by only staking RPL for the duration of one period (claim the previous period, leave it in for 14 days, claim another period, withdraw).  The stake can be withdrawn at the earliest 14 days after staking. However, it can be added back at any time, and the stake addition takes effect immediately. This allows for optimizing the staking reward as follows (assuming we front-run other claimers to maximize profits and perform all transactions in one block):  Note that withdraw() can be called right at the time the new reward period starts:  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L165-L166  require(block.number.sub(getNodeRPLStakedBlock(msg.sender)) >= rocketDAOProtocolSettingsRewards.getRewardsClaimIntervalBlocks(), \"The withdrawal cooldown period has not passed\");  // Get & check node's current RPL stake  Examples  A node may choose to register and stake some RPL to collect rewards but never actually provide registered node duties, e.g., operating a Minipool.  Node shares for a passed reward epoch are unpredictable as nodes may change their stake (adding) after/before users claim their rewards.  A node can maximize its rewards by adding stake just before claiming it  A node can stake to claim rewards, wait 14 days, withdraw, lend on a platform and return the stake in time to claim the next period.  Recommendation  Review the incentive model for the RPL rewards. Consider adjusting it so that nodes that provide a service get a better share of the rewards. Consider accruing rewards for the duration the stake was provided instead of taking a snapshot whenever the node calls claim(). Require stake to be locked for > 14 days instead of >=14 days (withdraw()) or have users skip the first reward period after staking.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.8 RocketNodeStaking - Node operators can reduce slashing impact by withdrawing excess staked RPL    ", "body": "  Resolution  The RocketNodeStaking.withdrawRPL method now reverts if a node operator attempts to withdraw an RPL amount that results in the leftover RPL stake being smaller than the maximum required stake. This prevents operators from withdrawing excess RPL to avoid the impact of a slashing.  https://github.com/rocket-pool/rocketpool/blob/rp3.0-updates/contracts/contract/node/RocketNodeStaking.sol#L187  Description  Oracle nodes update the Minipools  balance and progress it to the withdrawable state when they observe the minipools stake to become withdrawable. If the observed stakingEndBalance is less than the user deposit for that pool, the node operator is punished for the difference.  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L89-L94  rocketMinipoolManager.setMinipoolWithdrawalBalances(_minipoolAddress, _stakingEndBalance, nodeAmount);  // Apply node penalties by liquidating RPL stake  if (_stakingEndBalance < userDepositBalance) {  RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));  rocketNodeStaking.slashRPL(minipool.getNodeAddress(), userDepositBalance - _stakingEndBalance);  The amount slashed is at max userDepositBalance - stakingEndBalance. The userDepositBalance is at least 16 ETH (minipool.half/.full) and at max 32 ETH (minipool.empty). The maximum amount to be slashed is therefore 32 ETH (endBalance = 0, minipool.empty).  https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/32; note that the RPL token is potentially affected by a similar issue as one can stake RPL, wait for the cooldown period & wait for the price to change, and withdraw stake at higher RPL price/ETH). The  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L188-L196  uint256 rplSlashAmount = calcBase.mul(_ethSlashAmount).div(rocketNetworkPrices.getRPLPrice());  // Cap slashed amount to node's RPL stake  uint256 rplStake = getNodeRPLStake(_nodeAddress);  if (rplSlashAmount > rplStake) { rplSlashAmount = rplStake; }  // Transfer slashed amount to auction contract  rocketVault.transferToken(\"rocketAuctionManager\", getContractAddress(\"rocketTokenRPL\"), rplSlashAmount);  // Update RPL stake amounts  decreaseTotalRPLStake(rplSlashAmount);  decreaseNodeRPLStake(_nodeAddress, rplSlashAmount);  If the node does not have a sufficient RPL stake to cover the losses, the slashing amount is capped at whatever amount of RPL the node has left staked.  The minimum amount of RPL a node needs to have staked if it operates minipools is calculated as follows:  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L115-L120  // Calculate minimum RPL stake  return rocketDAOProtocolSettingsMinipool.getHalfDepositUserAmount()  .mul(rocketDAOProtocolSettingsNode.getMinimumPerMinipoolStake())  .mul(rocketMinipoolManager.getNodeMinipoolCount(_nodeAddress))  .div(rocketNetworkPrices.getRPLPrice());  With the current configuration, this would resolve in a minimum stake of 16 ETH * 0.1 (10% collateralization) * 1 (nr_minipools) * RPL_Price for a node operating 1 minipool. This means a node operator basically only needs to have 10% of 16 ETH staked to operate one minipool.  An operator can withdraw their stake at any time, but they have to wait at least 14 days after the last time they staked (cooldown period). They can, at max, withdraw all but the minimum stake required to run the pools (nr_of_minipools * 16 ETH * 10%). This also means that after the cooldown period, they can reduce their stake to 10% of the half deposit amount (16ETH), then perform a voluntary exit on ETH2 so that the minipool becomes withdrawable. If they end up with less than the userDepositBalance in staking rewards, they would only get slashed the 1.6 ETH at max (10% of 16ETH half deposit amount for 1 minipool) even though they incurred a loss that may be up to 32 ETH (empty Minipool empty amount).  Furthermore, if a node operator runs multiple minipools, let s say 5, then they would have to provide at least 5*16ETH*0.1 = 8ETH as a security guarantee in the form of staked RPL. If the node operator incurs a loss with one of their minipools, their 8 ETH RPL stake will likely be slashed in full. Their other - still operating - minipools are not backed by any RPL anymore, and they effectively cannot be slashed anymore. This means that a malicious node operator can create multiple minipools, stake the minimum amount of RPL, get slashed for one minipool, and still operate the others without having the minimum RPL needed to run the minipools staked (getNodeMinipoolLimit).  The RPL stake is donated to the RocketAuctionManager, where they can attempt to buy back RPL potentially at a discount.  Note: Staking more RPL (e.g., to add another Minipool) resets the cooldown period for the total RPL staked (not only for the newly added)  Recommendation  It is recommended to redesign the withdrawal process to prevent users from withdrawing their stake while slashable actions can still occur. A potential solution may be to add a locking period in the process. A node operator may schedule the withdrawal of funds, and after a certain time has passed, may withdraw them. This prevents the immediate withdrawal of funds that may need to be reduced while slashable events can still occur. E.g.:  A node operator requests to withdraw all but the minimum required stake to run their pools.  The funds are scheduled for withdrawal and locked until a period of X days has passed.  (optional) In this period, a slashable event occurs. The funds for compensation are taken from the user s stake including the funds scheduled for withdrawal.  After the time has passed, the node operator may call a function to trigger the withdrawal and get paid out.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.9 RocketTokenRPL - inaccurate inflation rate and potential for manipulation lowering the real APY    Addressed", "body": "  Resolution                           The main issue was addressed in branch   rocket-pool/rocketpool@b424ca1) by recording the timestamp up to when inflation was updated to instead of the current block timestamp (  Description  RocketTokenRPL allows users to swap their fixed-rate tokens to the inflationary RocketTokenRPL ERC20 token via a swapToken function. The DAO defines the inflation rate of this token and is initially set to be 5% APY. This APY is configured as a daily inflation rate (APD) with the corresponding 1 day in blocks inflation interval in the rocketDAOProtocolSettingsInflation contract. The DAO members control the inflation settings.  Anyone can call inflationMintTokens to inflate the token, which mints tokens to the contracts RocketVault. Tokens are minted for discreet intervals since the last time inflationMintTokens was called (recorded as inflationCalcBlock). The inflation is then calculated for the passed intervals without taking the current not yet completed interval. However, the inflationCalcBlock is set to the current block.number, effectively skipping some  time /blocks of the APY calculation.  The more often inflationMintTokens is called, the higher the APY likelihood dropping below the configured 5%. In the worst case, one could manipulate the APY down to 2.45% (assuming that the APD for a 5% APY was configured) by calling inflationMintTokens close to the end of every second interval. This would essentially restart the APY interval at block.number, skipping blocks of the current interval that have not been accounted for.  Note: updating the inflation rate will directly affect past inflation intervals that have not been minted! this might be undesirable, and it could be considered to force an inflation mint if the APY changes  Note: if the interval is small enough and there is a history of unaccounted intervals to be minted, and the Ethereum network is congested, gas fees may be high and block limits hit, the calculations in the for loop might be susceptible to DoS the inflation mechanism because of gas constraints.  Note: The inflation seems only to be triggered regularly on RocketRewardsPool.claim (or at any point by external actors). If the price establishes based on the total supply of tokens, then this may give attackers an opportunity to front-run other users trading large amounts of RPL that may previously have calculated their prices based on the un-inflated supply.  Note: that the discrete interval-based inflation (e.g., once a day) might create dynamics that put pressure on users to trade their RPL in windows instead of consecutively  Examples  the inflation intervals passed is the number of completed intervals. The current interval that is started is not included.  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L108-L119  function getInlfationIntervalsPassed() override public view returns(uint256) {  // The block that inflation was last calculated at  uint256 inflationLastCalculatedBlock = getInflationCalcBlock();  // Get the daily inflation in blocks  uint256 inflationInterval = getInflationIntervalBlocks();  // Calculate now if inflation has begun  if(inflationLastCalculatedBlock > 0) {  return (block.number).sub(inflationLastCalculatedBlock).div(inflationInterval);  }else{  return 0;  the inflation calculation calculates the to-be-minted tokens for the inflation rate at newTokens = supply * rateAPD^intervals - supply  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L126-L148  function inflationCalculate() override public view returns (uint256) {  // The inflation amount  uint256 inflationTokenAmount = 0;  // Optimisation  uint256 inflationRate = getInflationIntervalRate();  // Compute the number of inflation intervals elapsed since the last time we minted infation tokens  uint256 intervalsSinceLastMint = getInlfationIntervalsPassed();  // Only update  if last interval has passed and inflation rate is > 0  if(intervalsSinceLastMint > 0 && inflationRate > 0) {  // Our inflation rate  uint256 rate = inflationRate;  // Compute inflation for total inflation intervals elapsed  for (uint256 i = 1; i < intervalsSinceLastMint; i++) {  rate = rate.mul(inflationRate).div(10 ** 18);  // Get the total supply now  uint256 totalSupplyCurrent = totalSupply();  // Return inflation amount  inflationTokenAmount = totalSupplyCurrent.mul(rate).div(10 ** 18).sub(totalSupplyCurrent);  // Done  return inflationTokenAmount;  Recommendation  Properly track inflationCalcBlock as the end of the previous interval, as this is up to where the inflation was calculated, instead of the block at which the method was invoked.  Ensure APY/APD and interval configuration match up. Ensure the interval is not too small (potential gas DoS blocking inflation mint and RocketRewardsPool.claim).  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.10 Trusted node participation risk and potential client  optimizations     ", "body": "  Resolution  The development team considers this issue fixed as monitoring on the correct behaviour of node software is added to the system.  Description  The system might end up in a stale state with minipools never being setWithdrawable or network and prices being severely outdated because trusted nodes don t fulfill their duty of providing oracle values. Minipools not being able to advance to the Withdrawable state will severely harm the system as no rewards can be paid out. Outdated balances and prices may affect token economics around the tokens involved (specifically rETH price depends on oracle observations).  There is an incentive to be an oracle node as you get paid to provide oracle node duties when enrolled with the DAO. However, it is not enforced that nodes actually fulfill their duty of calling the respective onlyTrustedNode oracle functions to submit prices/balances/minipool rewards.  Therefore, a smart Rocket Pool trusted node operator might consider patching their client software to not or only sporadically fulfill their duties to save considerable amounts of gas, making more profit than other trusted nodes would.  There is no means to directly incentivize trusted nodes to call certain functions as they get their rewards anyway. The only risk they run is that other trusted nodes might detect their antisocial behavior and attempt to kick them out of the DAO. To detect this, monitoring tools and processes need to be established; it is questionable whether users would participate in high maintenance DAO operators.  Furthermore, trusted nodes might choose to gas optimize their submissions to avoid calling the actual action once quorum was established. They can, for example, attempt to submit prices as early as possible, avoiding that they re the first to hit the 51% threshold.  Recommendation  Create monitoring tools and processes to detect participants that do not fulfill their trusted DAO duties. Create direct incentives for trusted nodes to provide oracle services by, e.g., recording their participation rate and only payout rewards based on how active they are.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.11 RocketDAONodeTrustedUpgrade - upgrade does not prevent the use of the same address multiple times creating an inconsistency where getContractAddress returns outdated information    ", "body": "  Resolution                           A check has been introduced to make sure that the new contract address is not already in use by checking against the corresponding   Description  When adding a new contract, it is checked whether the address is already in use. This check is missing when upgrading a named contract to a new implementation, potentially allowing someone to register one address to multiple names creating an inconsistent configuration.  The crux of this is, that, getContractAddress() will now return a contract address that is not registered anymore (while getContractName may throw). getContractAddress can therefore not relied upon when checking ACL.  add contract name=test, address=0xfefe  >  sets contract.exists.0xfefe=true  sets contract.name.0xfefe=test  sets contract.address.test=0xfefe  sets contract.abi.test=abi  add another contract name=badcontract, address=0xbadbad  > sets contract.exists.0xbadbad=true sets contract.name.0xbadbad=badcontract sets contract.address.badcontract=0xbadbad sets contract.abi.badcontract=abi  update contract name=test, address=0xbadbad  reusing badcontradcts address, the address is now bound to 2 names (test, badcontract) overwrites contract.exists.0xbadbad=true` (even though its already true) updates contract.name.0xbadbad=test (overwrites the reference to badcontract; badcontracts config is now inconsistent) updates contract.address.test=0xbadbad (ok, expected) updates contract.abi.test=abi (ok, expected) removes contract.name.0xfefe (ok) removes contract.exists.0xfefe (ok)  update contract name=test, address=0xc0c0 sets contract.exists.0xc0c0=true sets contract.name.0xc0c0=test (ok, expected) updates contract.address.test=0xc0c0 (ok, expected) updates contract.abi.test=abi (ok, expected) removes contract.name.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now) removes contract.exists.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now)  After this, badcontract is partially cleared, getContractName(0xbadbad) throws while getContractAddress(badcontract) returns 0xbadbad which is already unregistered (contract.exists.0xbadbad=false)  Examples  check in `_addContract``  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L76-L76  require(_contractAddress != address(0x0), \"Invalid contract address\");  no checks in upgrade.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L53-L59  require(_contractAddress != address(0x0), \"Invalid contract address\");  require(_contractAddress != oldContractAddress, \"The contract address cannot be set to its current address\");  // Register new contract  setBool(keccak256(abi.encodePacked(\"contract.exists\", _contractAddress)), true);  setString(keccak256(abi.encodePacked(\"contract.name\", _contractAddress)), _name);  setAddress(keccak256(abi.encodePacked(\"contract.address\", _name)), _contractAddress);  setString(keccak256(abi.encodePacked(\"contract.abi\", _name)), _contractAbi);  Recommendation  Check that the address being upgraded to is not yet registered and properly clean up contract.address.<name|abi>.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.12 Rocketpool CLI - Lax data validation and output sanitation    Addressed", "body": "  Resolution  Addressed with v1.0.0-rc1 by sanitizing non-printables from strings stored in the smart contract.  This effectively mitigates terminal-based control character injection attacks. However, might still be used to inject context-sensitive information that may be consumed by different protocols/presentation layers (web, terminal by displaying falsified information next to fields).  E-mail and timezone format validation was introduced with https://github.com/rocket-pool/rocketpool-go/blob/c8738633ab973503b79c7dee5c2f78d7e44e48ae/dao/trustednode/proposals.go#L22 and rocket-pool/rocketpool-go@6e72501.  It is recommended to further tighten the checks on untrusted information enforcing an expected format of information and reject to interact with nodes/data that does not comply with the expected formats (e.g. email being in an email format, timezone information is a valid timezone, and does not contain extra information, \u2026).  Description  ValidateTimezoneLocation and ValidateDAOMemberEmail are only used to validate user input from the command line. Timezone location information and member email addresses are stored in the smart contract s string storage, e.g., using the setTimezoneLocation function of the RocketNodeManager contract. This function only validates that a minimum length of 4 has been given.  Through direct interaction with the contract, an attacker can submit arbitrary information, which is not validated on the CLI s side. With additional integrations of the Rocketpool smart contracts, the timezone location field may be used by an attacker to inject malicious code (e.g., for cross-site scripting attacks) or injecting false information (e.g. Balance: 1000 RPL or Status: Trusted), which is directly displayed on a user-facing application.  On the command line, control characters such as newline characters can be injected to alter how text is presented to the user, effectively exploiting user trust in the official application.  Examples  rocketpool-go-2.5-Tokenomics/node/node.go:L134-L153  wg.Go(func() error {  var err error  timezoneLocation, err = GetNodeTimezoneLocation(rp, nodeAddress, opts)  return err  })  // Wait for data  if err := wg.Wait(); err != nil {  return NodeDetails{}, err  // Return  return NodeDetails{  Address: nodeAddress,  Exists: exists,  WithdrawalAddress: withdrawalAddress,  TimezoneLocation: timezoneLocation,  }, nil  smartnode-2.5-Tokenomics/rocketpool-cli/odao/members.go:L34-L44  for _, member := range members.Members {  fmt.Printf(\"--------------------\\n\")  fmt.Printf(\"\\n\")  fmt.Printf(\"Member ID:            %s\\n\", member.ID)  fmt.Printf(\"Email address:        %s\\n\", member.Email)  fmt.Printf(\"Joined at block:      %d\\n\", member.JoinedBlock)  fmt.Printf(\"Last proposal block:  %d\\n\", member.LastProposalBlock)  fmt.Printf(\"RPL bond amount:      %.6f\\n\", math.RoundDown(eth.WeiToEth(member.RPLBondAmount), 6))  fmt.Printf(\"Unbonded minipools:   %d\\n\", member.UnbondedValidatorCount)  fmt.Printf(\"\\n\")  Recommendation  Validate user input before storing it on the blockchain. Validate and sanitize stored user tainted data before presenting it. Establish a register of data validation rules (e.g., email format, timezone format, etc.). Reject nodes operating with nodes that do not honor data validation rules.  Validate the correct format of variables (e.g., timezone location, email, name, \u2026) on the storage level (if applicable) and the lowest level of the go library to offer developers a strong foundation to build on and mitigate the risk in future integrations. Furthermore, on-chain validation might not be implemented (due to increased gas consumption) should be mentioned in the developer documentation security section as they need to be handled with special caution by consumer applications. Sanitize output before presenting it to avoid control character injections in terminal applications or other presentation technologies (e.g., SQL or HTML).  Review all usage of the fmt lib (especially Sprintf and string handling/concatenating functions). Ensure only sanitized data can reach this sink. Review the logging library and ensure it is hardened against control character injection by encoding non-printables and CR-LF.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.13 Rocketpool CLI - Various command injection vectors    Addressed", "body": "  Resolution                           Initially, the client implemented the suggested fix using   https://github.com/rocket-pool/smartnode/compare/extra-escapes.  Description  Various commands in the Rocketpool CLI make use of the readOutput and printOutput functions. These do not perform sanitization of user-supplied inputs and allow an attacker to supply malicious values which can be used to execute arbitrary commands on the user s system.  Examples  All commands using the Client.readOutput, Client.printOutput and Client.compose functions are affected.  Furthermore, Client.callAPI is used for API-related calls throughout the Rocketpool service. However, it does not validate that the values passed into it are valid API commands. This can lead to arbitrary command execution, also inside the container using docker exec.  Recommendation  Perform strict validation on all user-supplied parameters. If parameter values need to be inserted into a command template string, the %q format string or other restrictive equivalents should be used.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.14 RocketStorage - Risk concentration by giving all registered contracts permissions to change any settings in RocketStorage   ", "body": "  Resolution  The client provided the following statement:  We ve looked at adding access control contracts using namespaces, but the increase in gas usage would be significant and could hinder upgrades.  Description  The ACL for changing settings in the centralized RocketStorage allows any registered contract (listed under contract.exists) to change settings that belong to other parts of the system.  The concern is that if someone finds a way to add their malicious contract to the registered contact list, they will override any setting in the system. The storage is authoritative when checking certain ACLs. Being able to set any value might allow an attacker to gain control of the complete system. Allowing any contract to overwrite other contracts  settings dramatically increases the attack surface.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L32  modifier onlyLatestRocketNetworkContract() {  // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled  if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {  // Make sure the access is permitted to only contracts in our Dapp  require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");  _;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L78-L85  function setAddress(bytes32 _key, address _value) onlyLatestRocketNetworkContract override external {  addressStorage[_key] = _value;  /// @param _key The key for the record  function setUint(bytes32 _key, uint _value) onlyLatestRocketNetworkContract override external {  uIntStorage[_key] = _value;  Recommendation  Allow contracts to only change settings related to their namespace.  ", "labels": ["Consensys", "Major", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.15 RocketDAOProposals - require a minimum participation quorum for DAO proposals    Addressed", "body": "  Resolution  Addressed by requiring the DAO minimum viable user count as the minium quorum with rocket-pool/rocketpool@11bc18c (in bootstrap mode). The check for the bootstrap mode has since been removed following our remark  [\u2026] the problem here was not so much the bootstrap mode but rather that the dao membership may fall below the recovery mode threshold. The question is, whether it should still be allowed to propose and execute votes if the memberCount at proposal time is below that treshold (e.g. malicious member boots off other members, sends new proposals (quorum required=1), dao membrers rejoin but cannot reject that proposal anymore). Question is if quorum should be at least the recovery treshold.  And the following feedback from the client:  [\u2026] had that as allowed to happen if bootstrap mode was enabled. I ve just disabled the check for bootstrap mode now so that any proposals can t be made if the min member count is below the amount required. This means new members can only be added in this case via the emergency join function before new proposals can be added  Description  If the DAO falls below the minimum viable membership threshold, voting for proposals still continues as DAO proposals do not require a minimum participation quorum. In the worst case, this would allow the last standing DAO member to create a proposal that would be passable with only one vote even if new members would be immediately ready to join via the recovery mode (which has its own risks) as the minimum votes requirement for proposals is set as >0.  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L170-L170  require(_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L57-L69  function propose(string memory _proposalMessage, bytes memory _payload) override public onlyTrustedNode(msg.sender) onlyLatestContract(\"rocketDAONodeTrustedProposals\", address(this)) returns (uint256) {  // Load contracts  RocketDAOProposalInterface daoProposal = RocketDAOProposalInterface(getContractAddress('rocketDAOProposal'));  RocketDAONodeTrustedInterface daoNodeTrusted = RocketDAONodeTrustedInterface(getContractAddress('rocketDAONodeTrusted'));  RocketDAONodeTrustedSettingsProposalsInterface rocketDAONodeTrustedSettingsProposals = RocketDAONodeTrustedSettingsProposalsInterface(getContractAddress(\"rocketDAONodeTrustedSettingsProposals\"));  // Check this user can make a proposal now  require(daoNodeTrusted.getMemberLastProposalBlock(msg.sender).add(rocketDAONodeTrustedSettingsProposals.getCooldown()) <= block.number, \"Member has not waited long enough to make another proposal\");  // Record the last time this user made a proposal  setUint(keccak256(abi.encodePacked(daoNameSpace, \"member.proposal.lastblock\", msg.sender)), block.number);  // Create the proposal  return daoProposal.add(msg.sender, 'rocketDAONodeTrustedProposals', _proposalMessage, block.number.add(rocketDAONodeTrustedSettingsProposals.getVoteDelayBlocks()), rocketDAONodeTrustedSettingsProposals.getVoteBlocks(), rocketDAONodeTrustedSettingsProposals.getExecuteBlocks(), daoNodeTrusted.getMemberQuorumVotesRequired(), _payload);  Sidenote: Since a proposals acceptance quorum is recorded on proposal creation, this may lead to another scenario where proposals acceptance quorum may never be reached if members leave the DAO. This would require a re-submission of the proposal.  Recommendation  Do not accept proposals if the member count falls below the minimum DAO membercount threshold.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.16 RocketDAONodeTrustedUpgrade - inconsistent upgrade blacklist    Addressed", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by updating the blacklist.  Description  upgradeContract defines a hardcoded list of contracts that cannot be upgraded because they manage their own settings (statevars) or they hold value in the system.  the list is hardcoded and cannot be extended when new contracts are added via addcontract. E.g. what if another contract holding value is added to the system? This would require an upgrade of the upgrade contract to update the whitelist (gas hungry, significant risk of losing access to the upgrade mechanisms if a bug is being introduced).  a contract named rocketPoolToken is blacklisted from being upgradeable but the system registers no contract called rocketPoolToken. This may be an oversight or artifact of a previous iteration of the code. However, it may allow a malicious group of nodes to add a contract that is not yet in the system which cannot be removed anymore as there is no removeContract functionality and upgradeContract to override the malicious contract will fail due to the blacklist.  Note that upgrading RocketTokenRPL requires an account balance migration as contracts in the system may hold value in RPL (e.g. a lot in AuctionManager) that may vanish after an upgrade. The contract is not exempt from upgrading. A migration may not be easy to perform as the system cannot be paused to e.g. snapshot balances.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L41-L49  function _upgradeContract(string memory _name, address _contractAddress, string memory _contractAbi) internal {  // Check contract being upgraded  bytes32 nameHash = keccak256(abi.encodePacked(_name));  require(nameHash != keccak256(abi.encodePacked(\"rocketVault\")),        \"Cannot upgrade the vault\");  require(nameHash != keccak256(abi.encodePacked(\"rocketPoolToken\")),    \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"rocketTokenRETH\")),     \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"rocketTokenNETH\")), \"Cannot upgrade token contracts\");  require(nameHash != keccak256(abi.encodePacked(\"casperDeposit\")),      \"Cannot upgrade the casper deposit contract\");  // Get old contract address & check contract exists  Recommendation  Consider implementing a whitelist of contracts that are allowed to be upgraded instead of a more error-prone blacklist of contracts that cannot be upgraded.  Provide documentation that outlines what contracts are upgradeable and why.  Create a process to verify the blacklist before deploying/operating the system.  Plan for migration paths when upgrading contracts in the system  Any proposal that reaches the upgrade contract must be scrutinized for potential malicious activity (e.g. as any registered contract can directly modify storage or may contain subtle backdoors. Upgrading without performing a thorough security inspection may easily put the DAO at risk)  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.17 RocketDAONodeTrustedActions - member cannot be kicked if the vault does not hold enough RPL to cover the bond    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by returning the bond if enough RPL is in the treasury and else continue without returning the bond. This way the member kick action does not block and the member can be kicked regardless of the RPL balance.  Description  If a DAO member behaves badly other DAO members may propose the node be evicted from the DAO. If for some reason, RocketVault does not hold enough RPL to pay back the DAO member bond actionKick will throw. The node is not evicted.  Now this is a somewhat exotic scenario as the vault should always hold the bond for the members in the system. However, if the node was kicked for stealing RPL (e.g. passing an upgrade proposal to perform an attack) it might be impossible to execute the eviction.  Recommendation  Ensure that there is no way a node can influence a succeeded kick proposal to fail. Consider burning the bond (by keeping it) as there is a reason for evicting the node or allow them to redeem it in a separate step.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.18 RocketMinipoolStatus - DAO Membership changes can result in votes getting stuck    ", "body": "  Resolution                           This issue has been fixed in PR   https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/204 by introducing a public method that allows anyone to manually trigger a DAO consensus threshold check and a subsequent balance update in case the issue s example scenario occurs.  Description  Changes in the DAO s trusted node members are reflected in the RocketDAONodeTrusted.getMemberCount() function. When compared with the vote on consensus threshold, a DAO-driven decision is made, e.g., when updating token price feeds and changing Minipool states.  Especially in the early phase of the DAO, the functions below can get stuck as execution is restricted to DAO members who have not voted yet. Consider the following scenario:  The DAO consists of five members  Two members vote to make a Minipool withdrawable  The other three members are inactive, the community votes, and they get kicked from the DAO  The two remaining members have no way to change the Minipool state now. All method calls to trigger the state update fails because the members have already voted before.  Note: votes of members that are kicked/leave are still count towards the quorum!  Examples  Setting a Minipool into the withdrawable state:  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L62-L65  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  setMinipoolWithdrawable(_minipoolAddress, _stakingStartBalance, _stakingEndBalance);  Submitting a block s network balances:  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L94-L97  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updateBalances(_block, _totalEth, _stakingEth, _rethSupply);  Submitting a block s RPL price information:  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72  RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  updatePrices(_block, _rplPrice);  Recommendation  The conditional check and update of price feed information, Minipool state transition, etc., should be externalized into a separate public function. This function is also called internally in the existing code. In case the DAO gets into the scenario above, anyone can call the function to trigger a reevaluation of the condition with updated membership numbers and thus get the process unstuck.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.19 Trusted/Oracle-Nodes can vote multiple times for different outcomes ", "body": "  Description  Trusted/oracle nodes submit various ETH2 observations to the RocketPool contracts. When 51% of nodes submitted the same observation, the result is stored in the contract. However, while it is recorded that a node already voted for a specific minipool (being withdrawable & balance) or block (price/balance), a re-submission with different parameters for the same minipool/block is not rejected.  Since the oracle values should be distinct, clear, and there can only be one valid value, it should not be allowed for trusted nodes to change their mind voting for multiple different outcomes within one block or one minipool  Examples  RocketMinipoolStatus - a trusted node can submit multiple different results for one minipool  Note that setBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress)), true);  is recorded but never checked. (as for the other two instances)  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L48-L57  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress, _stakingStartBalance, _stakingEndBalance));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.count\", _minipoolAddress, _stakingStartBalance, _stakingEndBalance));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  RocketNetworkBalances - a trusted node can submit multiple different results for the balances at a specific block  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L80-L92  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, _block, _totalEth, _stakingEth, _rethSupply));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.balances.submitted.count\", _block, _totalEth, _stakingEth, _rethSupply));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, _block)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  // Emit balances submitted event  emit BalancesSubmitted(msg.sender, _block, _totalEth, _stakingEth, _rethSupply, block.timestamp);  // Check submission count & update network balances  RocketNetworkPrices - a trusted node can submit multiple different results for the price at a specific block  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L55-L67  // Get submission keys  bytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, _block, _rplPrice));  bytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.prices.submitted.count\", _block, _rplPrice));  // Check & update node submission status  require(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");  setBool(nodeSubmissionKey, true);  setBool(keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, _block)), true);  // Increment submission count  uint256 submissionCount = getUint(submissionCountKey).add(1);  setUint(submissionCountKey, submissionCount);  // Emit prices submitted event  emit PricesSubmitted(msg.sender, _block, _rplPrice, block.timestamp);  // Check submission count & update network prices  Recommendation  Only allow one vote per minipool/block. Don t give nodes the possibility to vote multiple times for different outcomes.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.20 RocketTokenNETH - Pot. discrepancy between minted tokens and deposited collateral    ", "body": "  Resolution  This issue is obsoleted by the fact that the nETH contract was removed completely. The client provided the following statement:  nETH has been removed completely.  Description  The nETH token is paid to node operators when minipool becomes withdrawable. nETH is supposed to be backed by ETH 1:1. However, in most cases, this will not be the case.  The nETH minting and deposition of collateral happens in two different stages of a minipool. nETH is minted in the minipool state transition from Staking to Withdrawable when the trusted/oracle nodes find consensus on the fact that the minipool became withdrawable (submitWinipoolWithdrawable).  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L63-L65  if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {  setMinipoolWithdrawable(_minipoolAddress, _stakingStartBalance, _stakingEndBalance);  When consensus is found on the state of the minipool, nETH tokens are minted to the minipool address according to the withdrawal amount observed by the trusted/oracle nodes. At this stage, ETH backing the newly minted nETH was not yet provided.  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L80-L87  uint256 nodeAmount = getMinipoolNodeRewardAmount(  minipool.getNodeFee(),  userDepositBalance,  minipool.getStakingStartBalance(),  minipool.getStakingEndBalance()  );  // Mint nETH to minipool contract  if (nodeAmount > 0) { rocketTokenNETH.mint(nodeAmount, _minipoolAddress); }  The minipool.receive() function receives the ETH  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L109-L112  receive() external payable {  (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature(\"receiveValidatorBalance()\"));  if (!success) { revert(getRevertMessage(data)); }  and forwards it to minipooldelegate.receiveValidatorBalance  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L227-L231  require(msg.sender == rocketDAOProtocolSettingsNetworkInterface.getSystemWithdrawalContractAddress(), \"The minipool's validator balance can only be sent by the eth1 system withdrawal contract\");  // Set validator balance withdrawn status  validatorBalanceWithdrawn = true;  // Process validator withdrawal for minipool  rocketNetworkWithdrawal.processWithdrawal{value: msg.value}();  Which calculates the nodeAmount based on the ETH received and submits it as collateral to back the previously minted nodeAmount of nETH.  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkWithdrawal.sol:L46-L60  uint256 totalShare = rocketMinipoolManager.getMinipoolWithdrawalTotalBalance(msg.sender);  uint256 nodeShare = rocketMinipoolManager.getMinipoolWithdrawalNodeBalance(msg.sender);  uint256 userShare = totalShare.sub(nodeShare);  // Get withdrawal amounts based on shares  uint256 nodeAmount = 0;  uint256 userAmount = 0;  if (totalShare > 0) {  nodeAmount = msg.value.mul(nodeShare).div(totalShare);  userAmount = msg.value.mul(userShare).div(totalShare);  // Set withdrawal processed status  rocketMinipoolManager.setMinipoolWithdrawalProcessed(msg.sender);  // Transfer node balance to nETH contract  if (nodeAmount > 0) { rocketTokenNETH.depositRewards{value: nodeAmount}(); }  // Transfer user balance to rETH contract or deposit pool  Looking at how the nodeAmount of nETH that was minted was calculated and comparing it to how nodeAmount of ETH is calculated, we can observe the following:  the nodeAmount of nETH minted is an absolute number of tokens based on the rewards observed by the trusted/oracle nodes. the nodeAmount is stored in the storage and later used to calculate the collateral deposit in a later step.  the nodeAmount calculated when depositing the collateral is first assumed to be a nodeShare (line 47), while it is actually an absolute number. the nodeShare is then turned into a nodeAmount relative to the ETH supplied to the contract.  Due to rounding errors, this might not always exactly match the nETH minted (see https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/26).  The collateral calculation is based on the ETH value provided to the contract. If this value does not exactly match what was reported by the oracle/trusted nodes when minting nETH, less/more collateral will be provided.  Note: excess collateral will be locked in the nETH contract as it is unaccounted for in the nETH token contract and therefore cannot be redeemed. Note: providing less collateral will go unnoticed and mess up the 1:1 nETH:ETH peg. In the worst case, there will be less nETH than ETH. Not everybody will be able to redeem their ETH.  Note: keep in mind that the receive() function might be subject to gas restrictions depending on the implementation of the withdrawal contract (.call() vs. .transfer())  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L201-L210  uint256 nethBalance = rocketTokenNETH.balanceOf(address(this));  if (nethBalance > 0) {  // Get node withdrawal address  RocketNodeManagerInterface rocketNodeManager = RocketNodeManagerInterface(getContractAddress(\"rocketNodeManager\"));  address nodeWithdrawalAddress = rocketNodeManager.getNodeWithdrawalAddress(nodeAddress);  // Transfer  require(rocketTokenNETH.transfer(nodeWithdrawalAddress, nethBalance), \"nETH balance was not successfully transferred to node operator\");  // Emit nETH withdrawn event  emit NethWithdrawn(nodeWithdrawalAddress, nethBalance, block.timestamp);  For reference, depositRewards (providing collateral) and mint are not connected at all, hence the risk of nETH being an undercollateralized token.  rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenNETH.sol:L28-L42  function depositRewards() override external payable onlyLatestContract(\"rocketNetworkWithdrawal\", msg.sender) {  // Emit ether deposited event  emit EtherDeposited(msg.sender, msg.value, block.timestamp);  // Mint nETH  // Only accepts calls from the RocketMinipoolStatus contract  function mint(uint256 _amount, address _to) override external onlyLatestContract(\"rocketMinipoolStatus\", msg.sender) {  // Check amount  require(_amount > 0, \"Invalid token mint amount\");  // Update balance & supply  _mint(_to, _amount);  // Emit tokens minted event  emit TokensMinted(_to, _amount, block.timestamp);  Recommendation  It looks like nETH might not be needed at all, and it should be discussed if the added complexity of having a potentially out-of-sync nETH token contract is necessary and otherwise remove it from the contract system as the nodeAmount of ETH can directly be paid out to the withdrawalAddress in the receiveValidatorBalance or withdraw transitions.  If nETH cannot be removed, consider minting nodeAmount of nETH directly to  withdrawalAddress on withdraw instead of first minting uncollateralized tokens. This will also reduce the gas footprint of the Minipool.  Ensure that the initial nodeAmount calculation matches the minted nETH and deposited to the contract as collateral (absolute amount vs. fraction).  Enforce that nETH requires collateral to be provided when minting tokens.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.21 RocketMiniPoolDelegate - on destroy() leftover ETH is sent to RocketVault where it cannot be recovered    ", "body": "  Resolution  Leftover ETH is now sent to the node operator address as expected.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/rocketpool-rp3.0-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol#L294  Description  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L314-L321  // Destroy the minipool  function destroy() private {  // Destroy minipool  RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));  rocketMinipoolManager.destroyMinipool();  // Self destruct & send any remaining ETH to vault  selfdestruct(payable(getContractAddress(\"rocketVault\")));  Recommendation  Implement means to recover and reuse ETH that was forcefully sent to the contract by MiniPool instances.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.22 RocketDAO - personally identifiable member information (PII) stored on-chain   ", "body": "  Resolution  Acknowledged with the following statement:  This is by design, need them to be publicly accountable. We ll advise their node should not be running on the same machine as their email software though.  Description  Like a DAO user s e-mail address, PII is stored on-chain and can, therefore, be accessed by anyone. This may allow de-pseudonymize users (and correlate Ethereum addresses to user email addresses) and be used for spamming or targeted phishing campaigns putting the DAO users at risk.  Examples  rocketpool-go-2.5-Tokenomics/dao/trustednode/dao.go:L173-L183  // Return  return MemberDetails{  Address: memberAddress,  Exists: exists,  ID: id,  Email: email,  JoinedBlock: joinedBlock,  LastProposalBlock: lastProposalBlock,  RPLBondAmount: rplBondAmount,  UnbondedValidatorCount: unbondedValidatorCount,  }, nil  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L110-L112  function getMemberEmail(address _nodeAddress) override public view returns (string memory) {  return getString(keccak256(abi.encodePacked(daoNameSpace, \"member.email\", _nodeAddress)));  Recommendation  Avoid storing PII on-chain where it is readily available for anyone.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.23 Rocketpool CLI - Insecure SSH HostKeyCallback    ", "body": "  Resolution  A proper host key callback function to validate the remote party s authenticity is now defined.  https://github.com/ConsenSys/rocketpool-audit-2021-03/blob/0a5f680ae0f4da0c5639a241bd1605512cba6004/smartnode-1.0.0-rc1/shared/services/rocketpool/client.go#L114-L117  Description  The SSH client factory returns instances that have an insecure HostKeyCallback set. This means that SSH servers  public key will not be validated and thus initialize a potentially insecure connection. The function should not be used for production code.  Examples  smartnode-2.5-Tokenomics/shared/services/rocketpool/client.go:L87  HostKeyCallback: ssh.InsecureIgnoreHostKey(),  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.24 Deployment - Docker containers running as root ", "body": "  Description  By default, Docker containers run commands as the root user. This means that there is little to no resistance for an attacker who has managed to break into the container and execute commands. This effectively negates file permissions already set into the system, such as storing wallet-related information with 0600 as an attacker will most likely drop into the container as root already.  Examples  Missing USER instructions affect both SmartNode Dockerfiles:  smartnode-2.5-Tokenomics/docker/rocketpool-dockerfile:L25-L36  # Start from ubuntu image  FROM ubuntu:20.10  # Install OS dependencies  RUN apt-get update && apt-get install -y ca-certificates  # Copy binary  COPY --from=builder /go/bin/rocketpool /go/bin/rocketpool  # Container entry point  ENTRYPOINT [\"/go/bin/rocketpool\"]  smartnode-2.5-Tokenomics/docker/rocketpool-pow-proxy-dockerfile:L24-L35  # Start from ubuntu image  FROM ubuntu:20.10  # Install OS dependencies  RUN apt-get update && apt-get install -y ca-certificates  # Copy binary  COPY --from=builder /go/bin/rocketpool-pow-proxy /go/bin/rocketpool-pow-proxy  # Container entry point  ENTRYPOINT [\"/go/bin/rocketpool-pow-proxy\"]  Recommendation  In the Dockerfiles, create an unprivileged user and use the USER instruction to switch. Only then, the entrypoint launching the SmartNode or the POW Proxy should be defined.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.25 RocketPoolMinipool - should check for address(0x0)    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by changing requiring that the contract address is not  Description  The two implementations for getContractAddress() in Minipool/Delegate are not checking whether the requested contract s address was ever set before. If it were never set, the method would return address(0x0), which would silently make all delegatecalls  succeed without executing any code. In contrast, RocketBase.getContractAddress() fails if the requested contract is not known.  It should be noted that this can happen if rocketMinipoolDelegate is not set in global storage, or it was cleared afterward, or if _rocketStorageAddress points to a contract that implements a non-throwing fallback function (may not even be storage at all).  Examples  Missing checks  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L170-L172  function getContractAddress(string memory _contractName) private view returns (address) {  return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L91-L93  function getContractAddress(string memory _contractName) private view returns (address) {  return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  Checks implemented  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketBase.sol:L84-L92  function getContractAddress(string memory _contractName) internal view returns (address) {  // Get the current contract address  address contractAddress = getAddress(keccak256(abi.encodePacked(\"contract.address\", _contractName)));  // Check it  require(contractAddress != address(0x0), \"Contract not found\");  // Return  return contractAddress;  Recommendation  Similar to RocketBase.getContractAddress() require that the contract is set.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.26 RocketDAONodeTrustedAction - ambiguous event emitted in actionChallengeDecide    ", "body": "  Resolution  Instead of emitting an event even though the challenge period has not passed yet, the function call will now revert if the challenge window has not passed yet.  Description  actionChallengeDecide succeeds and emits challengeSuccess=False in case the challenged node defeats the challenge. It also emits the same event if another node calls actionChallengeDecided before the refute window passed. This ambiguity may make a defeated challenge indistinguishable from a challenge that was attempted to be decided too early (unless the component listening for the event also checks the refute window).  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L244-L260  // Allow the challenged member to refute the challenge at anytime. If the window has passed and the challenge node does not run this method, any member can decide the challenge and eject the absent member  // Is it the node being challenged?  if(_nodeAddress == msg.sender) {  // Challenge is defeated, node has responded  deleteUint(keccak256(abi.encodePacked(daoNameSpace, \"member.challenged.block\", _nodeAddress)));  }else{  // The challenge refute window has passed, the member can be ejected now  if(getUint(keccak256(abi.encodePacked(daoNameSpace, \"member.challenged.block\", _nodeAddress))).add(rocketDAONodeTrustedSettingsMembers.getChallengeWindow()) < block.number) {  // Node has been challenged and failed to respond in the given window, remove them as a member and their bond is burned  _memberRemove(_nodeAddress);  // Challenge was successful  challengeSuccess = true;  // Log it  emit ActionChallengeDecided(_nodeAddress, msg.sender, challengeSuccess, block.timestamp);  Recommendation  Avoid ambiguities when emitting events. Consider throwing an exception in the else branch if the refute window has not passed yet (minimal gas savings; it s clear that the call failed; other components can rely on the event only being emitted if there was a decision.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.27 RocketDAOProtocolProposals, RocketDAONodeTrustedProposals - unused enum ProposalType    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the unused code.  Description  The enum ProposalType is defined but never used.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L29-L35  enum ProposalType {  Invite,             // Invite a registered node to join the trusted node DAO  Leave,              // Leave the DAO  Replace,            // Replace a current trusted node with a new registered node, they take over their bond  Kick,               // Kick a member from the DAO with optional penalty applied to their RPL deposit  Setting             // Change a DAO setting (Quorum threshold, RPL deposit size, voting periods etc)  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/RocketDAOProtocolProposals.sol:L28-L31  enum ProposalType {  Setting             // Change a DAO setting (Node operator min/max fees, inflation rate etc)  Recommendation  Remove unnecessary code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.28 RocketDaoNodeTrusted - Unused events    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the unused code.  Description  The MemberJoined MemberLeave events are not used within RocketDaoNodeTrusted.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L19-L23  // Events  event MemberJoined(address indexed _nodeAddress, uint256 _rplBondAmount, uint256 time);  event MemberLeave(address indexed _nodeAddress, uint256 _rplBondAmount, uint256 time);  Recommendation  Consider removing the events. Note: RocketDAONodeTrustedAction is emitting ActionJoin and ActionLeave event.s  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.29 RocketDAOProposal - expired, and defeated proposals can be canceled    ", "body": "  Resolution  Proposals can now only be cancelled if they are pending or active.  Description  The method emits an event that might trigger other components to perform actions.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L155-L159  } else {  // Check the votes, was it defeated?  // if (votesFor <= votesAgainst || votesFor < getVotesRequired(_proposalID))  return ProposalState.Defeated;  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L239-L250  function cancel(address _member, uint256 _proposalID) override public onlyDAOContract(getDAO(_proposalID)) {  // Firstly make sure this proposal that hasn't already been executed  require(getState(_proposalID) != ProposalState.Executed, \"Proposal has already been executed\");  // Make sure this proposal hasn't already been successful  require(getState(_proposalID) != ProposalState.Succeeded, \"Proposal has already succeeded\");  // Only allow the proposer to cancel  require(getProposer(_proposalID) == _member, \"Proposal can only be cancelled by the proposer\");  // Set as cancelled now  setBool(keccak256(abi.encodePacked(daoProposalNameSpace, \"cancelled\", _proposalID)), true);  // Log it  emit ProposalCancelled(_proposalID, _member, block.timestamp);  Recommendation  Preserve the true outcome. Do not allow to cancel proposals that are already in an end-state like canceled, expired, defeated.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.30 RocketDAOProposal - preserve the proposals correct state after expiration    ", "body": "  Resolution                           Proposals that have been defeated now will show up as such even when expired. The new default value is   Description  The state of proposals is resolved to give a preference to a proposal being expired over the actual result which may be defeated. The preference for a proposal s status is checked in order: cancelled? -> executed? -> expired? -> succeeded? -> pending? -> active? -> defeated (default)  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L137-L159  if (getCancelled(_proposalID)) {  // Cancelled by the proposer?  return ProposalState.Cancelled;  // Has it been executed?  } else if (getExecuted(_proposalID)) {  return ProposalState.Executed;  // Has it expired?  } else if (block.number >= getExpires(_proposalID)) {  return ProposalState.Expired;  // Vote was successful, is now awaiting execution  } else if (votesFor >= getVotesRequired(_proposalID)) {  return ProposalState.Succeeded;  // Is the proposal pending? Eg. waiting to be voted on  } else if (block.number <= getStart(_proposalID)) {  return ProposalState.Pending;  // The proposal is active and can be voted on  } else if (block.number <= getEnd(_proposalID)) {  return ProposalState.Active;  } else {  // Check the votes, was it defeated?  // if (votesFor <= votesAgainst || votesFor < getVotesRequired(_proposalID))  return ProposalState.Defeated;  Recommendation  consider checking for voteAgainst explicitly and return defeated instead of expired if a proposal was defeated and is queried after expiration. Preserve the actual proposal result.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.31 RocketRewardsPool - registerClaimer should check if a node is already disabled before decrementing rewards.pool.claim.interval.claimers.total.next    ", "body": "  Resolution                           In the case a submitted   Description  The other branch in registerClaimer does not check whether the provided _claimerAddress is already disabled (or invalid). This might lead to inconsistencies where rewards.pool.claim.interval.claimers.total.next is decremented because the caller provided an already deactivated address.  This issue is flagged as minor since we have not found an exploitable version of this issue in the current codebase. However, we recommend safeguarding the implementation instead of relying on the caller to provide sane parameters. Registered Nodes cannot unregister, and Trusted Nodes are unregistered when they leave.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/rewards/RocketRewardsPool.sol:L296-L316  function registerClaimer(address _claimerAddress, bool _enabled) override external onlyClaimContract {  // The name of the claiming contract  string memory contractName = getContractName(msg.sender);  // Record the block they are registering at  uint256 registeredBlock = 0;  // How many users are to be included in next interval  uint256 claimersIntervalTotalUpdate = getClaimingContractUserTotalNext(contractName);  // Ok register  if(_enabled) {  // Make sure they are not already registered  require(getClaimingContractUserRegisteredBlock(contractName, _claimerAddress) == 0, \"Claimer is already registered\");  // Update block number  registeredBlock = block.number;  // Update the total registered claimers for next interval  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.interval.claimers.total.next\", contractName)), claimersIntervalTotalUpdate.add(1));  }else{  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.interval.claimers.total.next\", contractName)), claimersIntervalTotalUpdate.sub(1));  // Save the registered block  setUint(keccak256(abi.encodePacked(\"rewards.pool.claim.contract.registered.block\", contractName, _claimerAddress)), registeredBlock);  Recommendation  Ensure that getClaimingContractUserRegisteredBlock(contractName, _claimerAddress) returns !=0 before decrementing the .total.next.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.32 RocketNetworkPrices - Price feed update lacks block number sanity check    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by only allowing price submissions for blocks in the range of  Description  Trusted nodes submit the RPL price feed. The function is called specifying a block number and the corresponding RPL price for that block. If a DAO vote goes through for that block-price combination, it is written to storage. In the unlikely scenario that a vote confirms a very high block number such as uint(-1), all future price updates will fail due to the require check below.  This issue becomes less likely the more active members the DAO has. Thus, it s considered a minor issue that mainly affects the initial bootstrapping process.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L53-L54  // Check block  require(_block > getPricesBlock(), \"Network prices for an equal or higher block are set\");  Recommendation  The function s _block parameter should be checked to prevent large block numbers from being submitted. This check could, e.g., specify that node operators are only allowed to submit price updates for a maximum of x blocks ahead of block.number.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.33 RocketDepositPool - Potential gasDoS in assignDeposits   ", "body": "  Resolution  The client acknowledges this issue.  Description  assignDeposits seems to be a gas heavy function, with many external calls in general, and few of them are inside the for loop itself. By default, rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments() returns 2, which is not a security concern. Through a DAO vote, the settings key deposit.assign.maximum can be set to a value that exhausts the block gas limit and effectively deactivates the deposit assignment process.  rocketpool-2.5-Tokenomics-updates/contracts/contract/deposit/RocketDepositPool.sol:L115-L116  for (uint256 i = 0; i < rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments(); ++i) {  // Get & check next available minipool capacity  Recommendation  The rocketDAOProtocolSettingsDeposit.getMaximumDepositAssignments() return value  could be cached outside the loop. Additionally, a check should be added that prevents unreasonably high values.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.34 RocketNetworkWithdrawal - ETH dust lockup due to rounding errors    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by calculating  Description  There s a potential ETH dust lockup when processing a withdrawal due to rounding errors when performing a division.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkWithdrawal.sol:L46-L55  uint256 totalShare = rocketMinipoolManager.getMinipoolWithdrawalTotalBalance(msg.sender);  uint256 nodeShare = rocketMinipoolManager.getMinipoolWithdrawalNodeBalance(msg.sender);  uint256 userShare = totalShare.sub(nodeShare);  // Get withdrawal amounts based on shares  uint256 nodeAmount = 0;  uint256 userAmount = 0;  if (totalShare > 0) {  nodeAmount = msg.value.mul(nodeShare).div(totalShare);  userAmount = msg.value.mul(userShare).div(totalShare);  Recommendation  Calculate userAmount as msg.value - nodeAmount instead. This should also save some gas.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.35 RocketAuctionManager - calcBase should be declared constant    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by declaring  Description  Declaring the same constant value calcBase multiple times as local variables to some methods in RocketAuctionManager carries the risk that if that value is ever updated, one of the value assignments might be missed. It is therefore highly recommended to reduce duplicate code and declare the value as a public constant. This way, it is clear that the same calcBase is used throughout the contract, and there is a single point of change in case it ever needs to be changed.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L136-L139  function getLotPriceByTotalBids(uint256 _index) override public view returns (uint256) {  uint256 calcBase = 1 ether;  return calcBase.mul(getLotTotalBidAmount(_index)).div(getLotTotalRPLAmount(_index));  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L151-L154  function getLotClaimedRPLAmount(uint256 _index) override public view returns (uint256) {  uint256 calcBase = 1 ether;  return calcBase.mul(getLotTotalBidAmount(_index)).div(getLotCurrentPrice(_index));  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L173-L174  // Calculation base value  uint256 calcBase = 1 ether;  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L216-L217  uint256 bidAmount = msg.value;  uint256 calcBase = 1 ether;  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L247-L249  // Calculate RPL claim amount  uint256 calcBase = 1 ether;  uint256 rplAmount = calcBase.mul(bidAmount).div(currentPrice);  Recommendation  Consider declaring calcBase as a private const state var instead of re-declaring it with the same value in multiple, multiple functions. Constant, literal state vars are replaced in a preprocessing step and do not require significant additional gas when accessed than normal state vars.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.36 RocketDAO* - daoNamespace is missing a trailing dot; should be declared constant/immutable    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by adding a trailing dot to the  Description  string private daoNameSpace = 'dao.trustednodes' is missing a trailing dot, or else there s no separator when concatenating the namespace with the vars.  Examples  requests dao.trustednodesmember.index instead of dao.trustednodes.member.index  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L83-L86  function getMemberAt(uint256 _index) override public view returns (address) {  AddressSetStorageInterface addressSetStorage = AddressSetStorageInterface(getContractAddress(\"addressSetStorage\"));  return addressSetStorage.getItem(keccak256(abi.encodePacked(daoNameSpace, \"member.index\")), _index);  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L32-L33  // The namespace for any data stored in the trusted node DAO (do not change)  string private daoNameSpace = 'dao.trustednodes';  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L22-L26  // Calculate using this as the base  uint256 private calcBase = 1 ether;  // The namespace for any data stored in the trusted node DAO (do not change)  string private daoNameSpace = 'dao.trustednodes';  rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/RocketDAOProtocol.sol:L12-L13  // The namespace for any data stored in the network DAO (do not change)  string private daoNameSpace = 'dao.protocol';  Recommendation  Remove the daoNameSpace and add the prefix to the respective variables directly.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.37 RocketVault - consider rejecting zero amount deposit/withdrawal requests    ", "body": "  Resolution  Addressed in branch rp3.0-updates (rocket-pool/rocketpool@b424ca1) by requiring that ETH and tokenAmounts are not zero.  Note that components that used to raise no exception when attempting to deposit/withdraw/transfer zero amount tokens/ETH may now throw which can be used to block certain functionalities (slashAmount==0).  The client provided the following statement:  We ll double check this. Currently the only way a slashAmount is 0 is if we allow node operators to not stake RPL (min 10% required currently). Though there isn t a check for 0 in the slash function atm, I ll add one now just as a safety check.  Description  Consider disallowing zero amount token transfers unless the system requires this to work. In most cases, zero amount token transfers will emit an event (that potentially triggers off-chain components). In some cases, they allow the caller without holding any balance to call back to themselves (pot. reentrancy) or the caller provided token address.  depositEther allows to deposit zero ETH  emits EtherDeposited  withdrawEther allows to withdraw zero ETH  calls back to withdrawer (msg.sender)! emits EtherWithdrawn  (depositToken checks for amount >0)  withdrawToken allows zero amount token withdrawals  calls into user provided (actually a network contract) tokenAddress) emits TokenWithdrawn  transferToken allows zero amount token transfers  emits TokenTransfer  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L50-L57  function depositEther() override external payable onlyLatestNetworkContract {  // Get contract key  bytes32 contractKey = keccak256(abi.encodePacked(getContractName(msg.sender)));  // Update contract balance  etherBalances[contractKey] = etherBalances[contractKey].add(msg.value);  // Emit ether deposited event  emit EtherDeposited(contractKey, msg.value, block.timestamp);  Recommendation  Zero amount transfers are no-operation calls in most cases and should be avoided. However, as all vault actions are authenticated (to registered system contracts), the risk of something going wrong is rather low. Nevertheless, it is recommended to deny zero amount transfers to avoid running code unnecessarily (gas consumption), emitting unnecessary events, or potentially call back to callers/token address for ineffective transfers.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.38 RocketVault - methods returning static return values and unchecked return parameters    ", "body": "  Resolution  The unused boolean return values have been removed and reverts have been introduced instead.  Description  The Token* methods in RocketVault either throw or return true, but they can never return false. If the method fails, it will always throw. Therefore, it is questionable if the static return value is needed at all. Furthermore, callees are in most cases not checking the return value of  Examples  static return value true  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L93-L96  // Emit token transfer  emit TokenDeposited(contractKey, _tokenAddress, _amount, block.timestamp);  // Done  return true;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L113-L115  emit TokenWithdrawn(contractKey, _tokenAddress, _amount, block.timestamp);  // Done  return true;  rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketVault.sol:L134-L137  // Emit token withdrawn event  emit TokenTransfer(contractKeyFrom, contractKeyTo, _tokenAddress, _amount, block.timestamp);  // Done  return true;  return value not checked  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L149-L150  rocketVault.depositToken(\"rocketNodeStaking\", rplTokenAddress, _amount);  // Update RPL stake amounts & node RPL staked block  rocketpool-2.5-Tokenomics-updates/contracts/contract/auction/RocketAuctionManager.sol:L252-L252  rocketVault.withdrawToken(msg.sender, getContractAddress(\"rocketTokenRPL\"), rplAmount);  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L172-L172  rocketVault.withdrawToken(msg.sender, getContractAddress(\"rocketTokenRPL\"), _amount);  rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L193-L193  rocketVault.transferToken(\"rocketAuctionManager\", getContractAddress(\"rocketTokenRPL\"), rplSlashAmount);  Recommendation  Define a clear interface for these functions. Remove the static return value in favor of having the method throw on failure (which is already the current behavior).  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.39 Deployment - Overloaded Ubuntu base image ", "body": "  Description  The SmartNode and the corresponding proxy Dockerfiles base their builds on the ubuntu:20.10 image. This image introduces many unrelated tools that significantly increase the container s attack surface and the tools an attacker has at their disposal once they have gained access to the container. Some of these tools include:  apt  bash/sh  perl  Examples  smartnode-2.5-Tokenomics/docker/rocketpool-dockerfile:L26-L26  FROM ubuntu:20.10  smartnode-2.5-Tokenomics/docker/rocketpool-pow-proxy-dockerfile:L25-L25  FROM ubuntu:20.10  Recommendation  Consider using a smaller and more restrictive base image such as Alpine. Additionally, AppArmor or Seccomp policies should be used to prevent unexpected and potentially malicious activities during the container s lifecycle. As an illustrative example, a SmartNode container does not need to load/unload kernel modules or loading a BPF to capture network traffic.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "6.40 RocketMinipoolDelegate - enforce that the delegate contract cannot be called directly    ", "body": "  Resolution                           Addressed in branch   rocket-pool/rocketpool@b424ca1) by removing the constructor and therefore the initialization code from the RocketMinipoolDelegate contract. The contract cannot be used directly anymore as all relevant methods are decorated  Description  This contract is not meant to be consumed directly and will only be delegate called from Minipool. Being able to call it directly might even create the problem that, in the worst case, someone might be able to selfdestruct the contract rendering all other contracts that link to it dysfunctional. This might even not be easily detectable because delegatecall to an EOA will act as a NOP.  The access control checks on the methods currently prevent methods from being called directly on the delegate. They require state variables to be set correctly, or the delegate is registered as a valid minipool in the system. Both conditions are improbable to be fulfilled, hence, mitigation any security risk. However, it looks like this is more of a side-effect than a design decision, and we would recommend not explicitly stating that the delegate contract cannot be used directly.  Examples  rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L65-L70  constructor(address _rocketStorageAddress) {  // Initialise RocketStorage  require(_rocketStorageAddress != address(0x0), \"Invalid storage address\");  rocketStorage = RocketStorageInterface(_rocketStorageAddress);  Recommendation  Remove the initialization from the constructor in the delegate contract. Consider adding a flag that indicates that the delegate contract is initialized and only set in the Minipool contract and not in the logic contract (delegate). On calls, check that the contract is initialized.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/04/rocketpool/"}, {"title": "4.1 Token approvals can be stolen in DAOfiV1Router01.addLiquidity() ", "body": "  Description  DAOfiV1Router01.addLiquidity() creates the desired pair contract if it does not already exist, then transfers tokens into the pair and calls DAOfiV1Pair.deposit(). There is no validation of the address to transfer tokens from, so an attacker could pass in any address with nonzero token approvals to DAOfiV1Router. This could be used to add liquidity to a pair contract for which the attacker is the pairOwner, allowing the stolen funds to be retrieved using DAOfiV1Pair.withdraw().  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L57-L85  function addLiquidity(  LiquidityParams calldata lp,  uint deadline  ) external override ensure(deadline) returns (uint256 amountBase) {  if (IDAOfiV1Factory(factory).getPair(  lp.tokenBase,  lp.tokenQuote,  lp.slopeNumerator,  lp.n,  lp.fee  ) == address(0)) {  IDAOfiV1Factory(factory).createPair(  address(this),  lp.tokenBase,  lp.tokenQuote,  msg.sender,  lp.slopeNumerator,  lp.n,  lp.fee  );  address pair = DAOfiV1Library.pairFor(  factory, lp.tokenBase, lp.tokenQuote, lp.slopeNumerator, lp.n, lp.fee  );  TransferHelper.safeTransferFrom(lp.tokenBase, lp.sender, pair, lp.amountBase);  TransferHelper.safeTransferFrom(lp.tokenQuote, lp.sender, pair, lp.amountQuote);  amountBase = IDAOfiV1Pair(pair).deposit(lp.to);  Recommendation  Transfer tokens from msg.sender instead of lp.sender.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "4.2 The deposit of a new pair can be stolen ", "body": "  Description  To create a new pair, a user is expected to call the same addLiquidity() (or the addLiquidityETH()) function of the router contract seen above:  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L57-L85  function addLiquidity(  LiquidityParams calldata lp,  uint deadline  ) external override ensure(deadline) returns (uint256 amountBase) {  if (IDAOfiV1Factory(factory).getPair(  lp.tokenBase,  lp.tokenQuote,  lp.slopeNumerator,  lp.n,  lp.fee  ) == address(0)) {  IDAOfiV1Factory(factory).createPair(  address(this),  lp.tokenBase,  lp.tokenQuote,  msg.sender,  lp.slopeNumerator,  lp.n,  lp.fee  );  address pair = DAOfiV1Library.pairFor(  factory, lp.tokenBase, lp.tokenQuote, lp.slopeNumerator, lp.n, lp.fee  );  TransferHelper.safeTransferFrom(lp.tokenBase, lp.sender, pair, lp.amountBase);  TransferHelper.safeTransferFrom(lp.tokenQuote, lp.sender, pair, lp.amountQuote);  amountBase = IDAOfiV1Pair(pair).deposit(lp.to);  This function checks if the pair already exists and creates a new one if it does not. After that, the first and only deposit is made to that pair.  The attacker can front-run that call and create a pair with the same parameters (thus, with the same address) by calling the createPair function of the DAOfiV1Factory contract. By calling that function directly, the attacker does not have to make the deposit when creating a new pair. The initial user will make this deposit, whose funds can now be withdrawn by the attacker.  Recommendation  There are a few factors/bugs that allowed this attack. All or some of them should be fixed:  The createPair function of the DAOfiV1Factory contract can be called directly by anyone without depositing with any router address as the parameter. The solution could be to allow only the router to create a pair.  The addLiquidity function checks that the pair does not exist yet. If the pair exists already, a deposit should only be made by the owner of the pair. But in general, a new pair shouldn t be deployed without depositing in the same transaction.  The pair s address does not depend on the owner/creator. It might make sense to add that information to the salt.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "4.3 Incorrect token decimal conversions can lead to loss of funds ", "body": "  Description  The _convert() function in DAOfiV1Pair is used to accommodate tokens with varying decimals() values. There are three cases in which it implicitly returns 0 for any amount, the most notable of which is when token.decimals() == resolution.  As a result of this, getQuoteOut() reverts any time either baseToken or quoteToken have decimals == INTERNAL_DECIMALS (currently hardcoded to 8).  The result of this is that no swaps can be performed in one of these pools, and the deposit() function will return an incorrect amountBaseOut of baseToken to the depositor, the balance of which can then be withdrawn by the pairOwner.  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L108-L130  function _convert(address token, uint256 amount, uint8 resolution, bool to) private view returns (uint256 converted) {  uint8 decimals = IERC20(token).decimals();  uint256 diff = 0;  uint256 factor = 0;  converted = 0;  if (decimals > resolution) {  diff = uint256(decimals.sub(resolution));  factor = 10 ** diff;  if (to && amount >= factor) {  converted = amount.div(factor);  } else if (!to) {  converted = amount.mul(factor);  } else if (decimals < resolution) {  diff = uint256(resolution.sub(decimals));  factor = 10 ** diff;  if (to) {  converted = amount.mul(factor);  } else if (!to && amount >= factor) {  converted = amount.div(factor);  Recommendation  The _convert() function should return amount when token.decimals() == resolution. Additionally, implicit return values should be avoided whenever possible, especially in functions that implement complex mathematical operations.  BancorFormula.power(baseN, baseD, _, _) does not support baseN < baseD, and checks should be added to ensure that any call to the BancorFormula conforms to the expected input ranges.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "4.4 The swapExactTokensForETH checks the wrong return value ", "body": "  Description  The following lines are intended to check that the amount of tokens received from a swap is greater than the minimum amount expected from this swap (sp.amountOut):  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L341-L345  uint amountOut = IWETH10(WETH).balanceOf(address(this));  require(  IWETH10(sp.tokenOut).balanceOf(address(this)).sub(balanceBefore) >= sp.amountOut,  'DAOfiV1Router: INSUFFICIENT_OUTPUT_AMOUNT'  );  Instead, it calculates the difference between the initial receiver s balance and the balance of the router.  Recommendation  Check the intended value.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "4.5 DAOfiV1Pair.deposit() accepts deposits of zero, blocking the pool ", "body": "  Description  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L223-L239  function deposit(address to) external override lock returns (uint256 amountBaseOut) {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_DEPOSIT');  require(deposited == false, 'DAOfiV1: DOUBLE_DEPOSIT');  reserveBase = IERC20(baseToken).balanceOf(address(this));  reserveQuote = IERC20(quoteToken).balanceOf(address(this));  // this function is locked and the contract can not reset reserves  deposited = true;  if (reserveQuote > 0) {  // set initial supply from reserveQuote  supply = amountBaseOut = getBaseOut(reserveQuote);  if (amountBaseOut > 0) {  _safeTransfer(baseToken, to, amountBaseOut);  reserveBase = reserveBase.sub(amountBaseOut);  emit Deposit(msg.sender, reserveBase, reserveQuote, amountBaseOut, to);  Recommendation  Require a minimum deposit amount in both baseToken and quoteToken, and do not rely on any assumptions about the distribution of baseToken as part of the security model.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "4.6 Restricting DAOfiV1Pair functions to calls from router makes DAOfiV1Router01 security critical ", "body": "  Description  The DAOfiV1Pair functions deposit(), withdraw(), and swap() are all restricted to calls from the router in order to avoid losses from user error. However, this means that any unidentified issue in the Router could render all pair contracts unusable, potentially locking the pair owner s funds.  Additionally, DAOfiV1Factory.createPair() allows any nonzero address to be provided as the router, so pairs can be initialized with a malicious router that users would be forced to interact with to utilize the pair contract.  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L223-L224  function deposit(address to) external override lock returns (uint256 amountBaseOut) {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_DEPOSIT');  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L250-L251  function withdraw(address to) external override lock returns (uint256 amountBase, uint256 amountQuote) {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_WITHDRAW');  code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L292-L293  function swap(address tokenIn, address tokenOut, uint256 amountIn, uint256 amountOut, address to) external override lock {  require(msg.sender == router, 'DAOfiV1: FORBIDDEN_SWAP');  Recommendation  Do not restrict DAOfiV1Pair functions to calls from router, but encourage users to use a trusted router to avoid losses from user error. If this restriction is kept, consider including the router address in the deployment salt for the pair or hardcoding the address of a trusted router in DAOfiV1Factory instead of taking the router as a parameter to createPair().  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "4.7 Pair contracts can be easily blocked ", "body": "  Description  The existing mitigation for this issue is to create a new pool with slightly different parameters. This creates significant cost for the creator of a pair, forces them to deploy a pair with sub-optimal parameters, and could potentially block all interesting pools for a token pair.  The salt used to determine unique pair contracts in DAOfiV1Factory.createPair():  code/daofi-v1-core/contracts/DAOfiV1Factory.sol:L77-L84  require(getPair(baseToken, quoteToken, slopeNumerator, n, fee) == address(0), 'DAOfiV1: PAIR_EXISTS'); // single check is sufficient  bytes memory bytecode = type(DAOfiV1Pair).creationCode;  bytes32 salt = keccak256(abi.encodePacked(baseToken, quoteToken, slopeNumerator, n, fee));  assembly {  pair := create2(0, add(bytecode, 32), mload(bytecode), salt)  IDAOfiV1Pair(pair).initialize(router, baseToken, quoteToken, pairOwner, slopeNumerator, n, fee);  pairs[salt] = pair;  Recommendation  Consider adding additional parameters to the salt that defines a unique pair, such as the pairOwner. Modifying the parameters included in the salt can also be used to partially mitigate other security concerns raised in this report.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "4.8 DAOfiV1Router01.removeLiquidityETH() does not support tokens with no return value ", "body": "  Description  While the rest of the system uses the safeTransfer* pattern, allowing tokens that do not return a boolean value on transfer() or transferFrom(), DAOfiV1Router01.removeLiquidityETH() throws and consumes all remaining gas if the base token does not return true.  Note that the deposit in this case can still be withdrawn without unwrapping the Eth using removeLiquidity().  code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L157-L167  function removeLiquidityETH(  LiquidityParams calldata lp,  uint deadline  ) external override ensure(deadline) returns (uint amountToken, uint amountETH) {  IDAOfiV1Pair pair = IDAOfiV1Pair(DAOfiV1Library.pairFor(factory, lp.tokenBase, WETH, lp.slopeNumerator, lp.n, lp.fee));  require(msg.sender == pair.pairOwner(), 'DAOfiV1Router: FORBIDDEN');  (amountToken, amountETH) = pair.withdraw(address(this));  assert(IERC20(lp.tokenBase).transfer(lp.to, amountToken));  IWETH10(WETH).withdraw(amountETH);  TransferHelper.safeTransferETH(lp.to, amountETH);  Recommendation  Be consistent with the use of safeTransfer*, and do not use assert() in cases where the condition can be false.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/02/daofi/"}, {"title": "5.1 Reward rate changes are not taken into account in LP staking ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented two new Emission logic for pToken & Other Reward Tokens in StakeLP which mandatorily distributes rewards only after updating the Reward Pool, thereby fixing this potential issue.  Description  When users update their reward (e.g., by calling the calculateRewards function), the reward amount is calculated according to all reward rate changes after the last update. So it does not matter when and how frequently you update the reward; in the end, you re going to have the same amount.  On the other hand, we can t say the same about the lp staking provided in the StakeLPCoreV8 contract. The amount of these rewards depends on when you call the  calculateRewardsAndLiquidity function, and the reward amount can even decrease over time.  Two main factors lead to this:  Changes in the reward rate. If the reward rate is decreased at some point, it s getting partially propagated to all the rewards there were not distributed yet. So the reward of the users that didn t call the calculateRewardsAndLiquidity function may decrease. On the other hand, if the reward rate is supposed to increase, it s better to wait and not call calculateRewardsAndLiquidity for as long as possible.  Not every liquidity provider will stake their LP tokens. When users provide liquidity but do not stake the LP tokens, the reward for these Stokens is still going to the Holder contract. These rewards getting proportionally distributed to the users that are staking their LP tokens. Basically, these rewards are added to the current reward rate but change more frequently. The same logic applies to that rewards; if you expect the unstaked LP tokens to increase, it s in your interest not to withdraw your rewards. But if they are decreasing, it s better to gather the rewards as early as possible.  Recommendation  The most preferred staking solution is to have an algorithm that is not giving people an incentive to gather the rewards earlier or later.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.2 The withdrawUnstakedTokens may run out of gas ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented a batchingLimit variable which enforces a definite number of iterations during withdrawal of unstaked tokens, instead of indefinite iterations.  Description  The withdrawUnstakedTokens is iterating over all batches of unstaked tokens. One user, if unstaked many times, could get their tokens stuck in the contract.  code/contracts/LiquidStakingV2.sol:L369-L403  function withdrawUnstakedTokens(address staker)  public  virtual  override  whenNotPaused  require(staker == _msgSender(), \"LQ20\");  uint256 _withdrawBalance;  uint256 _unstakingExpirationLength = _unstakingExpiration[staker]  .length;  uint256 _counter = _withdrawCounters[staker];  for (  uint256 i = _counter;  i < _unstakingExpirationLength;  i = i.add(1)  ) {  //get getUnstakeTime and compare it with current timestamp to check if 21 days + epoch difference has passed  (uint256 _getUnstakeTime, , ) = getUnstakeTime(  _unstakingExpiration[staker][i]  );  if (block.timestamp >= _getUnstakeTime) {  //if 21 days + epoch difference has passed, then add the balance and then mint uTokens  _withdrawBalance = _withdrawBalance.add(  _unstakingAmount[staker][i]  );  _unstakingExpiration[staker][i] = 0;  _unstakingAmount[staker][i] = 0;  _withdrawCounters[staker] = _withdrawCounters[staker].add(1);  require(_withdrawBalance > 0, \"LQ21\");  emit WithdrawUnstakeTokens(staker, _withdrawBalance, block.timestamp);  _uTokens.mint(staker, _withdrawBalance);  Recommendation  Limit the number of processed unstaked batches, and possibly add pagination.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.3 The _calculatePendingRewards can run out of gas ", "body": "  Resolution  Comment from pSTAKE Finance team:  A solution of maintaining cumulative timeshare values in an array and implementing binary search drastically lowers the iterations, has been implemented for calculating rewards for Other Reward Tokens. Also, for moving reward rate, strategically it will only be set max once a month making number of iterations very limited.  Description  The reward rate in STokens can be changed, and the history of these changes are stored in the contract:  code/contracts/STokensV2.sol:L124-L139  function setRewardRate(uint256 rewardRate)  public  virtual  override  returns (bool success)  // range checks for rewardRate. Since rewardRate cannot be more than 100%, the max cap  // is _valueDivisor * 100, which then brings the fees to 100 (percentage)  require(rewardRate <= _valueDivisor.mul(100), \"ST17\");  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"ST2\");  _rewardRate.push(rewardRate);  _lastMovingRewardTimestamp.push(block.timestamp);  emit SetRewardRate(rewardRate);  return true;  When the reward is calculated for each user, all changes of the _rewardRate are considered. So there is a for loop that iterates over all changes since the last reward update. If the reward rate was changed many times, the _calculatePendingRewards function could run out of gas.  Recommendation  Provide an option to partially update the reward, so the full update can be split in multiple transactions.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.4 Increase test coverage ", "body": "  Resolution  Comment from pSTAKE Finance team:  Created deep-dive test for each unique scenarios locally and tested before the code was deployed.  Description  Test coverage is fairly limited. LPStaking tests only cover the happy path. StakeLPCoreV8 has no tests. Many test descriptions are inaccurate.  Examples  Test description inaccuracy examples:  This tests that a Staker can mint new tokens, but does not check to make sure that Stakers are the ONLY group that can mint. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/LiquidStakingTest.js#L82  This test only shows that an unauthorized address can t use the stake function to mint tokens. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/LiquidStakingTest.js#L99  This test actually tests for the inverse case. https://github.com/ConsenSys/persistence-pstake-audit-2021-08/blob/3821182ca14e0e98ab9fccd47cbe0f1ce39ae54c/code/test/STokensTest.js#L82  Recommendation  Increase test coverage for entire codebase. Add tests for the inherited contracts from OpenZeppelin. Test for edge cases, and multiple expected cases. Ensure that the test description matches the functionality that is actually tested.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.5 The calculateRewards should not be callable by the whitelisted contract ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have created a require condition in Smart Contract code to disallow whitelisted contracts from calling the function  Description  The calculateRewards function should only be called for non-whitelisted addresses:  code/contracts/STokensV2.sol:L348-L359  function calculateRewards(address to)  public  virtual  override  whenNotPaused  returns (bool success)  require(to == _msgSender(), \"ST5\");  uint256 reward = _calculateRewards(to);  emit TriggeredCalculateRewards(to, reward, block.timestamp);  return true;  For all the whitelisted addresses, the calculateHolderRewards function is called. But if the calculateRewards function is called by the whitelisted address directly, the function will execute, and the rewards will be distributed to the caller instead of the intended recipients.  Recommendation  While this scenario is unlikely to happen, adding the additional check in the calculateRewards is a good option.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.6 Presence of testnet code ", "body": "  Resolution  Comment from pSTAKE Finance team:  The testnet code has been re-considered as out of scope for audit  Description  Based on the discussions with pStake team and in-line comments, there are a few instances of code and commented code in the code base under audit that are not finalized for mainnet deployment.  Examples  code/contracts/PSTAKE.sol:L25-L37  function initialize(address pauserAddress) public virtual initializer {  __ERC20_init(\"pSTAKE Token\", \"PSTAKE\");  __AccessControl_init();  __Pausable_init();  _setupRole(DEFAULT_ADMIN_ROLE, _msgSender());  _setupRole(PAUSER_ROLE, pauserAddress);  // PSTAKE IS A SIMPLE ERC20 TOKEN HENCE 18 DECIMAL PLACES  _setupDecimals(18);  // pre-allocate some tokens to an admin address which will air drop PSTAKE tokens  // to each of holder contracts. This is only for testnet purpose. in Mainnet, we  // will use a vesting contract to allocate tokens to admin in a certain schedule  _mint(_msgSender(), 5000000000000000000000000);  The initialize function currently mints all the tokens to msg.sender, however the goal for mainnet is to use a vesting contract which is not present in the current code.  Recommendation  It is recommended to fully test the final code before deployment to the mainnet.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.7 Re-entrancy from LP token transfers ", "body": "  Resolution  Comment from pSTAKE Finance team:  Have implemented the nonReentrant modifier from the ReentrancyGuardUpgradeable OpenZeppelin contract in addition to strictly keeping to Checks-Effects-Interactions pattern throughout relevant areas  Description  The StakeLPCoreV8 contract is designed to stake LP tokens. These LP tokens are not directly controlled or developed by the protocol, so it can t be easily verified that no re-entrancy can happen during token transfers.  Recommendation  During the review, we did not find any specific ways to build the attack using the re-entrancy of LP tokens, but it is still better to have the re-entrancy protection modifiers in functions that use LP tokens transfers.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.8 Sanity check on all important variables ", "body": "  Resolution  Comment from pSTAKE Finance team:  Post the implementation of new emission logic there have been a rearrangement of some variables, but the rest have been sanity tested and corrected  Description  Most of the functionalities have proper sanity checks when it comes to setting system-wide variables, such as whitelist addresses. However there are a few key setters that lack such sanity checks.  Examples  Sanity check (!= address(0)) on all token contracts.  code/contracts/StakeLPCoreV8.sol:L303-L333  function setUTokensContract(address uAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP9\");  _uTokens = IUTokens(uAddress);  emit SetUTokensContract(uAddress);  /**  @dev Set 'contract address', called from constructor  @param sAddress: stoken contract address  Emits a {SetSTokensContract} event with '_contract' set to the stoken contract address.  /  function setSTokensContract(address sAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP10\");  _sTokens = ISTokens(sAddress);  emit SetSTokensContract(sAddress);  /**  @dev Set 'contract address', called from constructor  @param pstakeAddress: pStake contract address  Emits a {SetPSTAKEContract} event with '_contract' set to the stoken contract address.  /  function setPSTAKEContract(address pstakeAddress) public virtual override {  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LP11\");  _pstakeTokens = IPSTAKE(pstakeAddress);  emit SetPSTAKEContract(pstakeAddress);  Sanity check on unstakingLockTime to be in the acceptable range (21 hours to 21 days)  code/contracts/LiquidStakingV2.sol:L105-L121  /**  @dev Set 'unstake props', called from admin  @param unstakingLockTime: varies from 21 hours to 21 days  Emits a {SetUnstakeProps} event with 'fee' set to the stake and unstake.  /  function setUnstakingLockTime(uint256 unstakingLockTime)  public  virtual  returns (bool success)  require(hasRole(DEFAULT_ADMIN_ROLE, _msgSender()), \"LQ3\");  _unstakingLockTime = unstakingLockTime;  emit SetUnstakingLockTime(unstakingLockTime);  return true;  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "5.9 Remove unused/commented code", "body": "  Description  There are a few snippets of commented code in the code base. It is suggested to remove and clean any unused and commented code in the final code.  Examples  code/contracts/PSTAKE.sol:L50-L73  /* function mint(address to, uint256 tokens) public virtual override returns (bool success) {  require(_msgSender() == _stakeLPCoreContract, \"PS1\");  // minted by STokens contract  _mint(to, tokens);  return true;  } */  /*  @dev Burn utokens for the provided 'address' and 'amount'  @param from: account address, tokens: number of tokens  Emits a {BurnTokens} event with 'from' set to address and 'tokens' set to amount of tokens.  Requirements:  - `amount` cannot be less than zero.  /  /* function burn(address from, uint256 tokens) public virtual override returns (bool success) {  require((tx.origin == from && _msgSender()==_liquidStakingContract) ||  // staking operation  (tx.origin == from && _msgSender() == _wrapperContract), \"UT2\"); // unwrap operation  _burn(from, tokens);  return true;  } */  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/08/pstake-finance/"}, {"title": "4.1 Markdown and Control Character Injection    ", "body": "  Resolution  Configuration values reflected in the confirmation prompt are implicitly validated by getting user consent on config changes through the fil_configure RPC call. Furthermore, confirmation dialogue values have been (mostly) displayed using the copyable function, resulting in escaped output.  This issue has been addressed in revision 1a8715f42cfc9f721e8faab8a7a2610f53592f94.  Description  The snap uses MetaMask s Snaps UI package to present dialogs to users for data verification and action confirmations. While these dialogs ensure dapps don t silently execute operations without user consent, some UI components have vulnerabilities. For instance, the text() component can render Markdown or be susceptible to control character injections. Specifically, in FilSnap s context, when users are prompted to sign a message showing a gas cost estimate if the message contains Markdown-renderable text, the user might unintentionally sign an inaccurate message. It s critical to note that the variable ctx.config in the provided code snippet could contain untrusted data, potentially altering the context of the displayed message. Malicious manipulation of the snap context is outlined in issue 4.3.  packages/snap/src/rpc/sign-message.ts:L68-L89  const conf = await snapDialog(ctx.snap, {  type: 'confirmation',  content: panel([  heading(`Send ${Token.fromAttoFIL(message.value).toFIL().toString()} to`),  copyable(message.to),  divider(),  heading('Details'),  text(  `Gas _(estimated)_: **${gas.toFIL().toFormat({  decimalPlaces: ctx.config.unit?.decimals,  suffix: ` ${ctx.config.unit?.symbol}`,  })}**`  ),  text(  `Total _(amount + gas)_: **${total.toFIL().toFormat({  decimalPlaces: ctx.config.unit?.decimals,  suffix: ` ${ctx.config.unit?.symbol}`,  })}**`  ),  ]),  })  Recommendation  Prioritize input validation. Encode data securely when presenting it to users. Display original data within an escaped pre-text or code block that prevents rendering non-standard characters or Markdown. Subsequently, add any derived or decoded data to offer users a comprehensive understanding.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/metamask/partner-snaps-filsnap/"}, {"title": "4.2 Directly Exposed Private Key Export    ", "body": "  Resolution  While the exportPrivateKey function is still present and exposed via the fil_exportPrivateKey RPC call, it now displays a warning and only surfaces the user s private key in a secured dialogue. This is analogous to MetaMask s approach and will make users consider their actions carefully. Automated attacks without user intervention are no longer possible since users must copy-paste their keys.  This issue has been addressed in commit c88a9ee1359e9a35735ce5d7b18b4cfcd2de0326.  Description  The snap can access the BIP44 entropy for Filecoin s private keys, granting it considerable power over MetaMask s private keys. Specifically, the fil_exportPrivateKey command lets dapps obtain the private key programmatically, pending user consent. However, there s a heightened risk of users indiscriminately granting this permission. To maintain MetaMask s security integrity, the snap should mirror the same rigorous security standards to mitigate the effect of phishing attacks and malicious dapps.  packages/snap/src/rpc/export-private-key.ts:L19-L34  export async function exportPrivateKey(  ctx: SnapContext  ): Promise<ExportPrivateKeyResponse> {  const conf = await snapDialog(ctx.snap, {  type: 'confirmation',  content: panel([heading(`Do you want to export your private key?`)]),  })  if (conf) {  return {  result: base64pad.encode(ctx.account.privateKey),  error: null,  return serializeError('User denied private key export')  Recommendation  The development team should reconsider providing such a sensitive functionality. Instead of programmatically exposing the private key, use a dialog that prompts users to copy the private key manually. This approach, consistent with MetaMask s default, encourages users to deliberate their actions more thoroughly.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/metamask/partner-snaps-filsnap/"}, {"title": "4.3 fil_configure Allows Anyone to Change the Snap s Configuration   Partially Addressed", "body": "  Resolution  This issue has been partially addressed and practically fixed. While it is still possible for any site to change the snap s configuration, its values are now maintained under the origin site s namespace. Furthermore, when connecting to a site, the user must manually confirm the proposed configuration values.  This issue has been fixed in 1a8715f42cfc9f721e8faab8a7a2610f53592f94.  Description  The fil_configure command, accessible by any dapp, accepts parameters of type ConfigureParams. This allows for modifying key configuration details like the derivation path, RPC details, network state, and unit data:  type ConfigureParams = {  derivationPath?: string | undefined;  rpc?: {  url: string;  token: string;  } | undefined;  network?: \"mainnet\" | \"testnet\" | undefined;  unit?: {  symbol: string;  decimals: number;  image?: string | undefined;  customViewUrl?: string | undefined;  } | undefined;  An attacker exploiting this open access can manipulate elements such as the network state or RPC URL, which could mislead users when signing messages. Furthermore, given the shared instance nature of the snap across multiple pages, a malicious dapp can influence the behavior of the snap in other dapps.  The MetaMask Snaps team has responded to this particular issue by stating that they will soon add a mutex to the SDKs state object to mitigate the problem partially. Nonetheless, even with a mutex in place, it s important to remember that business logic can still execute in an outdated state.  Recommendation  Evaluate the necessity of exposing specific configuration options through fil_configure. If it doesn t offer significant utility, consider removing it entirely, favoring a hardcoded configuration approach. This would ensure that critical token data and RPC URLs are modifiable only through controlled snap updates.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/08/metamask/partner-snaps-filsnap/"}, {"title": "4.4 Lack of Signature Dialog Context and RPC Origin    ", "body": "  Resolution                           The signature UI dialogue now displays the user account used for signing and the origin site that triggered the process. Furthermore, details such as the RPC URL and the network name are displayed. This issue has been fully addressed in   1a8715f42cfc9f721e8faab8a7a2610f53592f94 and  6ddb227738ed3aa041c18131eee65a98e17acdf4.  Description  The FilSnap signing dialog doesn t indicate which user account is used for signing. This omission can be exploited by malicious dapps, leading users to believe they re signing with one account when in fact, another is being used. Such transparency gaps, especially in an environment integrating multiple dapps and accounts, can mislead users into providing unintended signatures.  For reference, MetaMask displays these details during its signing requests, a practice FilSnap should adopt.  packages/snap/src/rpc/sign-message.ts:L68-L88  const conf = await snapDialog(ctx.snap, {  type: 'confirmation',  content: panel([  heading(`Send ${Token.fromAttoFIL(message.value).toFIL().toString()} to`),  copyable(message.to),  divider(),  heading('Details'),  text(  `Gas _(estimated)_: **${gas.toFIL().toFormat({  decimalPlaces: ctx.config.unit?.decimals,  suffix: ` ${ctx.config.unit?.symbol}`,  })}**`  ),  text(  `Total _(amount + gas)_: **${total.toFIL().toFormat({  decimalPlaces: ctx.config.unit?.decimals,  suffix: ` ${ctx.config.unit?.symbol}`,  })}**`  ),  ]),  })  Recommendation  Display the user account used for signing and the origin domain of the RPC call during the signing dialog, ensuring users have full context before confirming any transactions.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/metamask/partner-snaps-filsnap/"}, {"title": "4.5 Missing Address Protection    ", "body": "  Resolution                           Address protection has been introduced with commit   1a8715f42cfc9f721e8faab8a7a2610f53592f94. Before an origin site can access the snap s RPC calls, it has to call the  Description  While MetaMask hides wallet addresses by default, requiring users to expose them to dapps manually, the snap s fil_getAddress and fil_getAccountInfo RPC endpoints always disclose the current address to any connected dapp, even if that address has not been connected to the page. This allows potentially untrusted dapps to silently retrieve all user addresses, bypassing MetaMask s intentional security design.  Recommendation  Adopt security protocols similar to MetaMask s main wallet. Let users select which addresses they share with dapps and prevent automatic exposure of non-allowlisted wallet addresses without explicit user permission.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/metamask/partner-snaps-filsnap/"}, {"title": "4.6 Missing Timeout in RPC.call    ", "body": "  Resolution                           A timeout option has been added to the   e1908f9ea05e309c7f1d260ecc18584503155cb4 addressing this issue.  Description  Within the dependency iso-filecoin, there s an oversight in the RPC class. Specifically, the call method lacks a timeout parameter. Due to this missing timeout, the snap execution can experience delays, which could lead to an aborted request. The code excerpt provided shows the missing timeout in the fetch call.  const res = await this.fetch(this.api, {  method: 'POST',  headers: this.headers,  body: JSON.stringify({  jsonrpc: '2.0',  method,  params,  id: 1,  }),  })  Recommendation  To mitigate potential delays and ensure smoother operation, it s recommended to introduce a timeout to this call method. Implementing a timeout can prevent prolonged waits and enhance the resilience of the method against unexpected delays.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/metamask/partner-snaps-filsnap/"}, {"title": "4.7 Unnecessary Distribution of Private Key Information    ", "body": "  Resolution                           In commit   1a8715f42cfc9f721e8faab8a7a2610f53592f94 the  Description  The snap context includes an account field containing the private key. This context houses sensitive data and is distributed across all request handlers. Due to this setup, redundant access to the private key is provided to several functions, e.g.:  fil_getAccountInfo,  fil_getBalance,  fil_getMessages,  fil_sendMessage, and  fil_getGasForMessage.  packages/snap/src/index.ts:L42-L47  const context: SnapContext = {  config,  rpc,  account,  snap,  Recommendation  It s essential to ensure the security and privacy of critical user data. As such, access to private key data should be restricted, particularly for handler functions. Adjust the implementation by adding another SnapContext type that holds the account s private key information and only pass it to handler functions that immediately need to handle it. Alternatively, add a lazy-loading object into the existing SnapContext that only loads the private key when called by an authorized function.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/metamask/partner-snaps-filsnap/"}, {"title": "4.1 Insufficient tests ", "body": "  Resolution  Comment from NUTS Finance team:  We have added more mainnet fork test coverage for single plus assets. We will continue to add more test cases on edge cases. For the test coverage command, it s strange that if we run all test cases with  truffle test , the LiquidityGauge test case fails. However, it passes if we run it by ourselves. We have done some debugging but cannot figure it out yet. We believe that our test cases are all valid.  Description  It is crucial to write tests with possibly 100% coverage for smart contract systems. Given that BTCPlus has inner complexity and also integrates many DeFi projects, using unit testing and fuzzing in all code paths is essential to a secure system.  Currently there are only 63 unit tests (with 1 failing) for the main components (Plus/Composite token, Governance, Liquidity Gauge, etc) which are only testing the predetermined code execution paths. There are also DeFi protocol specific tests that are not well organized to be able to find the coverage on the system.  Recommendation  Write proper tests for all possible code flows and specially edge cases (Price volatility, token transfer failure, 0 amounts, etc). It is useful to have one command to run all tests and have a code coverage report at the end. Also using libraries like eth-gas-reporter it s possible to know the gas usage of different functionalities in order to optimize and prevent lock ups in the future.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.2 Simplify the harvest method in each SinglePlus    ", "body": "  Resolution  Comment from NUTS Finance team:  We have replaced all safeApprove() usage with approve() and used block.timestamp as the expiration date.  Description  The BadgerSBTCCrvPlus single plus contract implements a custom harvest method.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L52-L56  /**  @dev Harvest additional yield from the investment.  Only governance or strategist can call this function.  /  function harvest(address[] calldata _tokens, uint256[] calldata _cumulativeAmounts, uint256 _index, uint256 _cycle,  This method can only be called by the strategist because of the onlyStrategist modifier.  This method has a few steps which take one asset and transform it into another asset a few times.  It first claims the Badger tokens:  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L58-L59  // 1. Harvest from Badger Tree  IBadgerTree(BADGER_TREE).claim(_tokens, _cumulativeAmounts, _index, _cycle, _merkleProof, _amountsToClaim);  Then it transforms the Badger tokens into WBTC using Uniswap.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L61-L72  // 2. Sushi: Badger --> WBTC  uint256 _badger = IERC20Upgradeable(BADGER).balanceOf(address(this));  if (_badger > 0) {  IERC20Upgradeable(BADGER).safeApprove(SUSHISWAP, 0);  IERC20Upgradeable(BADGER).safeApprove(SUSHISWAP, _badger);  address[] memory _path = new address[](2);  _path[0] = BADGER;  _path[1] = WBTC;  IUniswapRouter(SUSHISWAP).swapExactTokensForTokens(_badger, uint256(0), _path, address(this), block.timestamp.add(1800));  This step can be simplified in two ways.  First, the safeApprove method isn t useful because its usage is not recommended anymore.  The OpenZeppelin version 4 implementation states the method is deprecated and its usage is discouraged.  contracts/token/ERC20/utils/SafeERC20Upgradeable.sol:L29-L30  * @dev Deprecated. This function has issues similar to the ones found in      * {IERC20-approve}, and its usage is discouraged.  @dev Deprecated. This function has issues similar to the ones found in  {IERC20-approve}, and its usage is discouraged.  Thus, the SafeERC20Upgradeable.sol is not needed anymore and the import can be removed.  @dev Deprecated. This function has issues similar to the ones found in  {IERC20-approve}, and its usage is discouraged.  Thus, the SafeERC20Upgradeable.sol is not needed anymore and the import can be removed.  Another step is swapping the tokens on Uniswap.  code/BTC-Plus/contracts/single/eth/BadgerSBTCCrv%2B.sol:L71  IUniswapRouter(SUSHISWAP).swapExactTokensForTokens(_badger, uint256(0), _path, address(this), block.timestamp.add(1800));  In this case, the last argument block.timestamp.add(1800) is the deadline. This is useful when the transaction is sent to the network and a deadline is needed to expire the transaction. However, the execution is right now and there s no need for a future expiration date.  Removing the safe math addition will have the same end effect, the tokens will be swapped and the call is not at risk to expire.  Recommendation  Remove safeApprove and favor using approve. This also removes the need of having SafeERC20Upgradeable.sol included.  Do not use safe math when sending the expiration date. Use block.timestamp for the same effect and a reduced gas cost.  Apply the same principles for other Single Plus Tokens.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.3 Reduce complexity in modifiers related to governance and strategist    ", "body": "  Resolution  Comment from NUTS Finance team:  The code size seems to be an issue for us. For example, the code size of the CompositePlus contract is more than 21k. If you could provide more suggestions on how to reduce the contract code size, we d appreciate it.  Description  The modifier onlyGovernance:  code/BTC-Plus/contracts/Plus.sol:L101-L104  modifier onlyGovernance() {  _checkGovernance();  _;  Calls the internal function _checkGovernance:  code/BTC-Plus/contracts/Plus.sol:L97-L99  function _checkGovernance() internal view {  require(msg.sender == governance, \"not governance\");  There is no other case where the internal method _checkGovernance is called directly.  One can reduce complexity by removing the internal function and moving its code directly in the modifier. This will increase code size but reduce gas used and code complexity.  There are multiple similar instances:  code/BTC-Plus/contracts/Plus.sol:L106-L113  function _checkStrategist() internal view {  require(msg.sender == governance || strategists[msg.sender], \"not strategist\");  modifier onlyStrategist {  _checkStrategist();  _;  code/BTC-Plus/contracts/governance/GaugeController.sol:L298-L305  function _checkGovernance() internal view {  require(msg.sender == governance, \"not governance\");  modifier onlyGovernance() {  _checkGovernance();  _;  code/BTC-Plus/contracts/governance/LiquidityGauge.sol:L450-L457  function _checkGovernance() internal view {  require(msg.sender == IGaugeController(controller).governance(), \"not governance\");  modifier onlyGovernance() {  _checkGovernance();  _;  Recommendation  Consider removing the internal function and including its body in the modifier directly if the code size is not an issue.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.4 safeMath is integrated in Solidity 0.8.0^    ", "body": "  Resolution  Comment from NUTS Finance team:  We ve replaced all SafeMath usage with native math.  Description  The code base is using Solidity 0.8.0 which has safeMath integrated in the compiler. In addition, the codebase also utilizes OpenZepplin SafeMath library for arithmetic operations.  Recommendation  Removing safeMath from the code base results in gas usage optimization and also clearer code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.5 Lack of up to date documentation", "body": "  Resolution  Comment from NUTS Finance team:  We are continuing to enhance our docs about the latest design.  Description  This is a complicated system with many design decisions that are resulted from integration with other DeFi projects. In the code base there are many hard coded values or anti-patterns that are not documented and creates an unhealthy and hard to maintain code base.  Examples  TOKENLESS_PRODUCTION = 40; is not documented in the LiquidityGauge.sol.  uint256 _balance = balanceOf(_account);  uint256 _supply = totalSupply();  uint256 _limit = _balance.mul(TOKENLESS_PRODUCTION).div(100);  if (_votingTotal > 0) {  uint256 _boosting = _supply.mul(_votingBalance).mul(100 - TOKENLESS_PRODUCTION).div(_votingTotal).div(100);  _limit = _limit.add(_boosting);  Based on the conversation with the NUTS finance developer, this is due to fact that this is a fork of the way Curve s DAO contract works.  Recommendation  We recommend to have dedicated up to date documents on the system overview of the system and each module. In addition, in-line documentation in the code base helps to understand the code base and increases the readability of the code.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2021/05/nuts-finance-btcplus/"}, {"title": "4.1 Intentional secret reuse can block borrower and lender from accepting liquidation payment    ", "body": "  Resolution                           This is fixed in   AtomicLoans/atomicloans-eth-contracts#65.  Description  For Dave (the liquidator) to claim the collateral he s purchasing, he must reveal secret D. Once that secret is revealed, Alice and Bob (the borrower and lender) can claim the payment.  Secrets must be provided via the Sales.provideSecret() function:  code/ethereum/contracts/Sales.sol:L193-L200  function provideSecret(bytes32 sale, bytes32 secret_) external {  require(sales[sale].set);  if      (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashA) { secretHashes[sale].secretA = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashB) { secretHashes[sale].secretB = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashC) { secretHashes[sale].secretC = secret_; }  else if (sha256(abi.encodePacked(secret_)) == secretHashes[sale].secretHashD) { secretHashes[sale].secretD = secret_; }  else                                                                          { revert(); }  Note that if Dave chooses the same secret hash as either Alice, Bob, or Charlie (arbiter), there is no way to set secretHashes[sale].secretD because one of the earlier conditionals will execute.  For Alice and Bob to later receive payment, they must be able to provide Dave s secret:  code/ethereum/contracts/Sales.sol:L218-L222  function accept(bytes32 sale) external {  require(!accepted(sale));  require(!off(sale));  require(hasSecrets(sale));  require(sha256(abi.encodePacked(secretHashes[sale].secretD)) == secretHashes[sale].secretHashD);  Dave can exploit this to obtain the collateral for free:  Dave looks at Alice s secret hashes to see which will be used in the sale.  Dave begins the liquidation process, using the same secret hash.  Alice and Bob reveal their secrets A and B through the process of moving the collateral.  Dave now knows the preimage for the secret hash he provided. It was revealed by Alice already.  Dave uses that secret to obtain the collateral.  Alice and Bob now want to receive payment, but they re unable to provide Dave s secret to the Sales smart contract due to the order of conditionals in provideSecret().  After an expiration, Dave can claim a refund.  Mitigating factors  Alice and Bob could notice that Dave chose a duplicate secret hash and refuse to proceed with the sale. This is not something they are likely to do.  Recommendation  Either change the way provideSecret() works to allow for duplicate secret hashes or reject duplicate hashes in create().  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.2 There is no way to convert between custom and non-custom funds   ", "body": "  Resolution  Users who want to switch between custom and non-custom funds can create a new address to do so. This is not actually a big burden because lenders need to use agent software to manage their funds anyway. That workflow typically involves generating a new address because the private key needs to be given to the agent software.  Description  Each fund is created using either Funds.create() or Funds.createCustom(). Both enforce a limitation that there can only be one fund per account:  code/ethereum/contracts/Funds.sol:L348-L355  function create(  uint256  maxLoanDur_,  uint256  maxFundDur_,  address  arbiter_,  bool     compoundEnabled_,  uint256  amount_  ) external returns (bytes32 fund) {  require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address  code/ethereum/contracts/Funds.sol:L383-L397  function createCustom(  uint256  minLoanAmt_,  uint256  maxLoanAmt_,  uint256  minLoanDur_,  uint256  maxLoanDur_,  uint256  maxFundDur_,  uint256  liquidationRatio_,  uint256  interest_,  uint256  penalty_,  uint256  fee_,  address  arbiter_,  bool     compoundEnabled_,  uint256  amount_  ) external returns (bytes32 fund) {  require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address  These functions are the only place where bools[fund].custom is set, and there s no way to delete a fund once it exists. This means there s no way for a given account to switch between a custom and non-custom fund.  This could be a problem if, for example, the default parameters change in a way that a user finds unappealing. They may want to switch to using a custom fund but find themselves unable to do so without moving to a new Ethereum account.  Recommendation  Either allow funds to be deleted or allow funds to be switched between custom and non-custom.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.3 Funds.maxFundDur has no effect if maxLoanDur is set    ", "body": "  Resolution                           This is fixed in   AtomicLoans/atomicloans-eth-contracts#68.  Description  Funds.maxFundDur specifies the maximum amount of time a fund should be active. It s checked in request() to ensure the duration of the loan won t exceed that time, but the check is skipped if maxLoanDur is set:  code/ethereum/contracts/Funds.sol:L510-L514  if (maxLoanDur(fund) > 0) {  require(loanDur_       <= maxLoanDur(fund));  } else {  require(now + loanDur_ <= maxFundDur(fund));  Examples  If a user sets maxLoanDur (the maximum loan duration) to 1 week and sets the maxFundDur (timestamp when all loans should be complete) to December 1st, then there can actually be a loan that ends on December 7th.  Recommendation  Check against maxFundDur even when maxLoanDur is set.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.4 In Funds, maxFundDur is misnamed    ", "body": "  Resolution                           This is fixed in   AtomicLoans/atomicloans-eth-contracts#66.  Description  This is a timestamp, not a duration.  Recommendation  Rename to something with  timestamp  or perhaps  expiration  in the name.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "4.5 Funds.update() lets users update fields that may not have any effect    ", "body": "  Resolution                           This is fixed in   AtomicLoans/atomicloans-eth-contracts#67.  Description  Funds.update() allows users to update the following fields which are only used if bools[fund].custom is set:  minLoanamt  maxLoanAmt  minLoanDur  interest  penalty  fee  liquidationRatio  If bools[fund].custom is not set, then these changes have no effect. This may be misleading to users.  Examples  code/ethereum/contracts/Funds.sol:L454-L478  function update(  bytes32  fund,  uint256  minLoanAmt_,  uint256  maxLoanAmt_,  uint256  minLoanDur_,  uint256  maxLoanDur_,  uint256  maxFundDur_,  uint256  interest_,  uint256  penalty_,  uint256  fee_,  uint256  liquidationRatio_,  address  arbiter_  ) external {  require(msg.sender == lender(fund));  funds[fund].minLoanAmt       = minLoanAmt_;  funds[fund].maxLoanAmt       = maxLoanAmt_;  funds[fund].minLoanDur       = minLoanDur_;  funds[fund].maxLoanDur       = maxLoanDur_;  funds[fund].maxFundDur       = maxFundDur_;  funds[fund].interest         = interest_;  funds[fund].penalty          = penalty_;  funds[fund].fee              = fee_;  funds[fund].liquidationRatio = liquidationRatio_;  funds[fund].arbiter          = arbiter_;  Recommendation  This could be addressed by creating two update functions: one for custom funds and one for non-custom funds. Only the update for custom funds would allow setting these values.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/09/atomic-loans/"}, {"title": "6.1 FAIR can be stolen using ERC-777 hooks    ", "body": "  Resolution  fixed by completely removing ERC-777 support.  Description  The sell() function calls out to user-configured hooks when burning incoming FAIR tokens. The buy() function does the same if the DAT s currency is ERC-777 compliant.  Either of these hooks might invoke malicious code to re-enter the DAT, allowing them to sell and/or buy FAIR tokens at an unintentionally favourable price.  Such attacks may leave the DAT undercollateralized, resulting in other investors being unable to redeem their FAIR for currency.  Example  Here are some ordered extracts from the code invoked when DAT.buy() is called, when the DAT s currency is an ERC-777 compliant token.  code/contracts/DecentralizedAutonomousTrust.vy:L629  tokenValue: uint256 = self.estimateBuyValue(_currencyValue)  The code above does a calculation using FAIR.totalSupply as input. The higher FAIR.totalSupply is, the more expensive FAIR tokens become.  code/contracts/DecentralizedAutonomousTrust.vy:L502-L503  if(self.isCurrencyERC777):  self.currency.operatorSend(_from, self, _quantityToInvest, \"\", \"\")  Per the ERC-777 standard, the code above invokes an arbitrary tokensToSend() hook configured by the buyer.  code/contracts/DecentralizedAutonomousTrust.vy:L654  self.fair.mint(msg.sender, _to, tokenValue, \"\", \"\")  The code above increments FAIR.totalSupply, effectively increasing the price of FAIR tokens. This happens after the other two code extracts have completed.  An attacker can exploit re-entrancy during the tokensToSend() hook, to purchase further tokens at a (perhaps extremely) favourable price before FAIR.totalSupply is incremented.  If the price at the time of the initial buy() is very low (as it will be when totalSupply is small or zero), then they may be able to buy huge amounts of FAIR at that very low price.  Recommendation  Prevent reentrancy by adding a mutex (using Vyper s @nonreentrant() decorator) across all functions that result in ERC-777 token transfers (of FAIR or an ERC-777 currency).  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.2 BigDiv does not prevent overflow in some cases where it should    ", "body": "  Resolution  Fixed in the Solidity implementation.  Description  BigDiv.vy has been created with the aim of allowing calculations like (a * b) / d to succeed where an intermediate step (e.g. a * b) might overflow but the end result is <= MAX_UINT256.  All of the functions sometimes fail in this aim if the numerators are large and of the same order of magnitude. (E.g. for bigDiv2x1, it fails if _numA / MAX_BEFORE_SQUARE = numB / MAX_BEFORE_SQUARE > 0)  The chances of this issue being hit accidentally or exploited deliberately in the current code will both greatly depend on the DAT s configuration and its state. (If the numbers are amenable, an attacker could conceivably front run transactions and adjust FAIR balances in a way that causes targeted transactions to fail.)  Having functions that unexpectedly fail is dangerous for future consumers of this code, and the (simplest possible) fix is small.  Examples  The following code overflows in the code as audited, but succeeds (returning MAX_INT) if MAX_BEFORE_SQUARE is altered as suggested in issue 6.4.  bigDiv2x1 also overflows for some simple cases where the result is far below MAX_UINT256. E.g.:  Recommendations  1. Fix overflows  The following code appears in each BigDiv function:  code/contracts/BigDiv.vy:L30-L31  if(factor == 0):  factor = 1  Replacing every instance of these two lines with simply factor += 1 will avoid overflows. It will also reduce the (currently undocumented) accuracy of the result in some cases, so see recommendations in issue 6.4.  2. Add automated regression tests for all BigDiv functions  We have already written some basic test code and can supply it on request.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.3 Square roots are not calculated accurately for inputs below ~10^30    ", "body": "  Resolution  This has been addressed.  Description  The rounding performed when calculating square roots results in an extreme loss of precision for numbers < ~10^30.  This may or may not be OK. Typically the numbers being square rooted will be significantly larger than 10^30, but when supply is low and the value of a buy / pay is also low, this rounding could have a dramatic effect.  In any case, the square rooting logic and its limitations could be be better documented and tested.  Examples  In both places where _toDecimalWithPlaces is used, it is surrounded by the same code, which combines with _toDecimalWithPlaces to calculate a square root of a uint256:  code/contracts/DecentralizedAutonomousTrust.vy:L792-L808  # Math: Truncates last 18 digits from tokenValue here  tokenValue /= DIGITS_UINT  # Math: Truncates another 8 digits from tokenValue (losing 26 digits in total)  # This will cause small values to round to 0 tokens for the payment (the payment is still accepted)  # Math: Max supported tokenValue is 1.7e+56. If supply is at the hard-cap tokenValue would be 1e38, leaving room  # for a _currencyValue up to 1.7e33 (or 1.7e15 after decimals)  decimalValue: decimal = self._toDecimalWithPlaces(tokenValue)  decimalValue = sqrt(decimalValue)  # Unshift results  # Math: decimalValue has a max value of 2^127 - 1 which after sqrt can always be multiplied  # here without overflow  decimalValue *= DIGITS_DECIMAL  tokenValue = convert(decimalValue, uint256)  This code casts the number to a decimal so that Vyper s sqrt can be used, after first doing some rounding to prevent overflow during the cast. After all of this is done, it casts back to a uint256.  The result of the rounding + casts is reasonably accurate square root for very large integers, but it loses a lot of accuracy for smaller integers.  E.g. an integer as  small  as 12345678901234567890123456 results in a  square root  value of 0.  Recommendations  1. Reduce code duplication and document assumption / limitations  By moving the common surrounding code inside the _toDecimalWithPlaces function, that function could be renamed (e.g. to integerSqrt) and the limitations of the whole square root calculation could be more easily documented.  2. Test the documented limitations of the integerSqrt operation  To verify the documented limitations, thereby reducing the chances of this code being misused by a different developer at a later stage of the same project.  3. If accuracy for smaller integers is important, improve it  Greater accuracy may be achievable by writing or importing a function that approximates square roots using integer arithmetic and Newton s Method, without ever casting to a decimal.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.4 BigDiv estimates some values that could be easily calculated    ", "body": "  Resolution  Fixed in the port to Solidity.  Description  The accuracy of BigDiv s functions is neither documented clearly nor directly tested. (The csv tests should exercise much of the BigDiv code, but it s hard to see exactly what calculations are being done.)  BigDiv returns estimates in some cases where it could easily calculate a precise answer. Having spoken offline about FAIR s requirements, we believe the lack of accuracy itself is probably not a problem right now, but it creates a small risk of BigDiv being accidentally misused in future scenarios where its level of accuracy is insufficient (perhaps by a different developer, during a new phase of the FAIR project).  In any case, BigDiv s behaviour could be better documented and tested.  Examples  For comparison, we define a simpler function:  @public  @constant  def simpleDiv(  _numA: uint256,  _numB: uint256,  _den: uint256  ) -> uint256:  return _numA * _numB / _den  In some cases where both bigDiv2x1 and simpleDiv both succeed, bigDiv2x1 is less accurate than simpleDiv:  a='1'  b='99993402823669209384634633746074317682114579999'  BigDiv.bigDiv2x1(a, b, '8', false) -- succeeds, approximate answer  simpleDiv(a, b, '8')               -- succeeds, exact answer  Also, the constants MAX_BEFORE_SQUARE and MAX_BEFORE_CUBE seem to have been miscalculated, resulting in estimation happening slightly more often than necessary.  Recommendations  1. Document expected accuracy / rounding  To prevent accidental misuse of these functions in the future.  2. Add automated regression tests for all BigDiv functions  We have written some basic unit test code as part of our audit, and can supply it on request.  3. If maximising accuracy is important, improve it  There is some low-hanging fruit here, such as:  increasing MAX_BEFORE_SQUARE and MAX_BEFORE_CUBE to 340282366920938463463374607431768211456 and 48740834812604276470692695, respectively.  Per code logic, these numbers are really the first numbers that cannot be squared and cubed, so you may also wish to rename the constants. Note that MAX_BEFORE_SQUARE is also defined in the DAT contract  Add a check for overflow before resorting to estimation. E.g. for bigDiv2x...:  if(MAX_UINT256 / _numA > _numB):  # No rounding required. Return exact result  return _numA * _numB / _den  This latter change may reduce gas consumption as well as improving accuracy.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.5 Unused code in BigDiv functions    ", "body": "  Resolution  Fixed.  Description  Some parameters and associated logic can be removed from BigDiv s functions. This would simplify the code, as well as the analysis and testing of the code.  Examples  The _roundUp parameter is always false in the following functions:  bigDiv2x1  bigDiv3x1  bigDiv3x3  Associated conditionals are numerous. E.g. bigDiv3x3 s code branches 7 times on the value of _roundUp, even though it is always false.  Recommendation  Remove unused code and associated logic.  Add tests for code that remains.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.6 FAIR - Calling transferFrom should not emit the Approval event   ", "body": "  Resolution  Closed as WontFix.  This behavior is a de facto standard based on it s usage in the OpenZeppelin implementation of ERC20.  Description  The method transferFrom() sends some already approved tokens to some address:  code/contracts/FAIR.vy:L427-L439  @public  def transferFrom(  _from: address,  _to: address,  _value: uint256  ) -> bool:  \"\"\"  @notice Transfers `_value` amount of tokens from address `_from` to address `_to` if authorized.  \"\"\"  self.allowances[_from][msg.sender] -= _value  self._send(msg.sender, _from, _to, _value, False, \"\", \"\")  log.Approval(_from, msg.sender, self.allowances[_from][msg.sender])  return True  But it also emits an Approval event.  code/contracts/FAIR.vy:L438  log.Approval(_from, msg.sender, self.allowances[_from][msg.sender])  The event does not seem to create problems, it basically updates the remaining approved tokens.  Examples  The EIP 20 documentation states that the event should be emitted when a successful call to approve happens. It does not say if it should (or should not) be used when successfully calling transferFrom().  https://eips.ethereum.org/EIPS/eip-20#approval  It does not seem to violate the EIP 20 or EIP 777 standard and it helps any off-chain service monitoring the contract, keep track of how many remaining approved tokens are left, without having any previous state.  However any re-entrancy issues will make the transaction emit multiple events, each event having different amounts approved, the last emitted event having the highest value, which will be the incorrect one.  Recommendation  We suggest removing the emitted log because it can create problems.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.7 On-chain logic cannot reliably prevent a malicious beneficiary from purchasing tokens at a discount    ", "body": "  Resolution  closed as WontFix. Fraudulent token purchases by the Beneficiary are prevented by the associated legal agreements, not by on-chain logic.  The extra code should actually be thought of as enabling a legitimate method for the Beneficiary  purchase tokens at a fair price.  Description  The buy() function contains unique logic for identifying and processing an investment by the beneficiary:  code/contracts/DecentralizedAutonomousTrust.vy:L650-L658  elif(self.state == STATE_RUN):  if(_to != self.beneficiary):  self._distributeInvestment(_currencyValue)  self.fair.mint(msg.sender, _to, tokenValue, \"\", \"\")  if(self.state == STATE_RUN):  if(_to == self.beneficiary):  self._applyBurnThreshold() # must mint before this call  Because the beneficiary receives a portion amount invested, without this logic the beneficiary organization could purchase FAIRs for a fraction of the price compared to external investors.  However, this logic can be easily circumvented by a dishonest beneficiary using another address for investments.  The Fairmint team has explained that they are aware that this protection can be circumvented. The  legal layer  is necessary to enforce good behaviour, and the beneficiary would be committing fraud in case they purchased FAIRs using another address. Thus the extra code should actually be thought of as enabling the Beneficiary to legitimately purchase tokens.  Recommendation  This functionality introduces extra code. Consider reducing complexity by removing this functionality if it is not essential.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.8 FAIR is not ERC-777 compliant    ", "body": "  Resolution  fixed by removing ERC-777 support  Description  A comment at the top of FAIR.vy describes it as an  ERC-777 and ERC-20 compliant token .  But by the code s own acknowledgement, it is not fully ERC-777 compliant in its current state.  Examples  The contract is non-compliant with ERC-777 in at least the following ways:  Does not allow per-user revocation of the default operator (the DAT)  Does not call the ERC777 tokensToSend and tokensReceived hooks within transfer and transferFrom  It is (correctly, given the points above) not ERC820-registered as an ERC777Token  This list may not be exhaustive.  Recommendation  Implementing the standard fully may improve interop, so implementing all missing logic should be considered.  If not in full compliance:  avoid publishing any documentation that could be construed as claiming ERC-777 compliance, including code comments.  in accordance with other findings, consider removing ERC-777 compliance, and restricting the functionality to ERC-20.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.9 Not compliant with ERC1404    ", "body": "  Resolution  Fixed. Porting to solidity enabled compliance with ERC1404.  Description  The ERC1404 standard is an extension of the ERC20 standard. Here it has been implemented as a standalone contract, but does not contain all of the extra functions required by ERC1404.  As such, neither the FAIR contract nor the ERC1404 contract is ERC1404-compliant.  Recommendations  Rename the ERC1404 contract to be something more generic like Whitelist. This is more descriptive, and avoids confusion between Whitelist.approve() and the completely unrelated approve() function that an ERC1404-compliant contract should inherit from ERC20.  Fully implement ERC1404 in FAIR by adding messageForTransferRestriction(), if and only if the standard can be changed to accommodate Vyper s types. If it cannot, drop all claims or implications of ERC1404 support.  To further reduce confusion, consider renaming approve(), and perhaps splitting it into 2 separate functions. E.g. allow() and deny().  7 Code quality recommendations  This sections compiles suggestions which do not pose a direct threat to security, but would otherwise improve the quality of the code.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "7.1 DecentralizedAutonomousTrust.sol", "body": "  The method _authorizeTransfer could be rewritten as a modifier, if desired.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "7.2 Whitelist.sol", "body": "  The argument _isSell is not used in the method authorizeTransfer().  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "7.3 Sqrt.sol", "body": "  SafeMath.sol is imported to Sqrt.sol, but is not used.  8 Gas efficiency optimization recommendations  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "8.1 DecentralizedAutonomousTrust.sol", "body": "  The BigDiv.sol and Sqrt.sol contracts are deployed separately and their methods are accessed as external calls. This is more expensive than accessing the functions as libraries. Ie. library BigDiv  and using BigDiv for uint.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2019/11/fairmint-continuous-securities-offering/"}, {"title": "6.1 Re-Entrancy Risks Associated With External Calls With Other Liquid Staking Systems.    ", "body": "  Resolution                           Fixed in   commit f43b7cd5135872143cc35f40cae95870446d0413 by introducing reentrancy guards.  Description  As part of the strategy to integrate with Liquid Staking tokens for Ethereum staking, the Lybra Protocol vaults are required to make external calls to Liquid Staking systems.  For example, the depositEtherToMint function in the vaults makes external calls to deposit Ether and receive the LSD tokens back. While external calls to untrusted third-party contracts may be dangerous, in this case, the Lybra Protocol already extends trust assumptions to these third parties simply through the act of accepting their tokens as collateral. Indeed, in some cases the contract addresses are even hardcoded into the contract and called directly instead of relying on some registry:  contracts/lybra/pools/LybraWstETHVault.sol:L21-L40  contract LybraWstETHVault is LybraPeUSDVaultBase {  Ilido immutable lido;  //WstETH = 0x7f39C581F595B53c5cb19bD0b3f8dA6c935E2Ca0;  //Lido = 0xae7ab96520DE3A18E5e111B5EaAb095312D7fE84;  constructor(address _lido, address _asset, address _oracle, address _config) LybraPeUSDVaultBase(_asset, _oracle, _config) {  lido = Ilido(_lido);  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 sharesAmount = lido.submit{value: msg.value}(address(configurator));  require(sharesAmount != 0, \"ZERO_DEPOSIT\");  lido.approve(address(collateralAsset), msg.value);  uint256 wstETHAmount = IWstETH(address(collateralAsset)).wrap(msg.value);  depositedAsset[msg.sender] += wstETHAmount;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,wstETHAmount, block.timestamp);  In that case, depending on the contract, it may be known what contract is being called, and the risk may be assessed as far as what logic may be executed.  However, in the cases of BETH and rETH, the calls are being made into a proxy and a contract registry of a DAO (RocketPool s DAO) respectively.  contracts/lybra/pools/LybraWbETHVault.sol:L15-L32  contract LybraWBETHVault is LybraPeUSDVaultBase {  //WBETH = 0xa2e3356610840701bdf5611a53974510ae27e2e1  constructor(address _asset, address _oracle, address _config)  LybraPeUSDVaultBase(_asset, _oracle, _config) {}  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 preBalance = collateralAsset.balanceOf(address(this));  IWBETH(address(collateralAsset)).deposit{value: msg.value}(address(configurator));  uint256 balance = collateralAsset.balanceOf(address(this));  depositedAsset[msg.sender] += balance - preBalance;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);  contracts/lybra/pools/LybraRETHVault.sol:L25-L42  constructor(address _rocketStorageAddress, address _rETH, address _oracle, address _config)  LybraPeUSDVaultBase(_rETH, _oracle, _config) {  rocketStorage = IRocketStorageInterface(_rocketStorageAddress);  function depositEtherToMint(uint256 mintAmount) external payable override {  require(msg.value >= 1 ether, \"DNL\");  uint256 preBalance = collateralAsset.balanceOf(address(this));  IRocketDepositPool(rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", \"rocketDepositPool\")))).deposit{value: msg.value}();  uint256 balance = collateralAsset.balanceOf(address(this));  depositedAsset[msg.sender] += balance - preBalance;  if (mintAmount > 0) {  _mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());  emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);  As a result, it is impossible to make any guarantees for what logic will be executed during the external calls. Namely, reentrancy risks can t be ruled out, and the damage could be critical to the system. While the trust in these parties isn t in question, it would be best practice to avoid any additional reentrancy risks by placing reentrancy guards. Indeed, in the LybraRETHVault and LybraWbETHVault contracts, one can see the possible damage as the calls are surrounded in a preBalance <-> balance pattern.  The whole of third party Liquid Staking systems  operations need not be compromised, only these particular parts would be enough to cause critical damage to the Lybra Protocol.  Recommendation  After conversations with the Lybra Finance team, it has been assessed that reentrancy guards are appropriate in this scenario to avoid any potential reentrancy risk, which is exactly the recommendation this audit team would provide.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.2 The Deployer of GovernanceTimelock Gets Privileged Access to the System.    ", "body": "  Resolution                           As per discussions with the Lybra Finance team, this has been acknowledged as a temporary measure to configure anything before the launch of V2. Following the discussions, the Lybra Finance team has revoked the deployer s permissions in   transaction 0x12c95eec095f7e24abc6a127f378f9f0fb3a0021aeac82b487c11afa01b793af and updated the  commit 77e8bc3664fb1b195fd718c2ce1d49af8530f981 to instead introduce a multisig address that will have the  Description  The GovernanceTimelock contract is responsible for Roles Based Access Control management and checks in the Lybra Protocol. It offers two functions specifically that check if an address has the required role - checkRole and checkOnlyRole:  contracts/lybra/governance/GovernanceTimelock.sol:L24-L30  function checkRole(bytes32 role, address _sender) public view  returns(bool){  return hasRole(role, _sender) || hasRole(DAO, _sender);  function checkOnlyRole(bytes32 role, address _sender) public view  returns(bool){  return hasRole(role, _sender);  In checkRole, the contract also lets an address with the role DAO bypass the check altogether, making it a powerful role.  For initial role management, when the GovernanceTimelock contract gets deployed, its constructor logic initializes a few roles, assigns relevant admin roles, and, notably, assigns the DAO role to the contract, and the DAO and the GOV role to the deployer.  contracts/lybra/governance/GovernanceTimelock.sol:L14-L23  constructor(uint256 minDelay, address[] memory proposers, address[] memory executors, address admin) TimelockController(minDelay, proposers, executors, admin) {  _setRoleAdmin(DAO, GOV);  _setRoleAdmin(TIMELOCK, GOV);  _setRoleAdmin(ADMIN, GOV);  _grantRole(DAO, address(this));  _grantRole(DAO, msg.sender);  _grantRole(GOV, msg.sender);  The assignment of such powerful roles to a single private key with the deployer has inherent risks. Specifically in our case, the DAO role alone as we saw may bypass many checks within the Lybra Protocol, and the GOV role even has role management privileges.  However, it does make sense to assign such roles at the beginning of the deployment to finish initialization and assign the rest of the roles. One could argue that having access to the DAO role in the early stages of the system s life could allow for quick disaster recovery in the event of incidents as well. Though, it is still dangerous to hold privileges for such a system in a single address as we have seen over the last years in security incidents that have to do with compromised keys.  Recommendation  While redesigning the deployment process to account for a lesser-privileged deployer would be ideal, the Lybra Finance team should at least transfer ownership as soon as the deployment is complete to minimize compromised private key risk.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.3 The configurator.getEUSDMaxLocked() Condition Can Be Bypassed During a Flashloan    ", "body": "  Resolution                           Fixed in   f6c3afb5e48355c180417b192bd24ba294f77797 by checking  Description  When converting EUSD tokens to peUSD, there is a check that limits the total amount of EUSD that can be converted:  contracts/lybra/token/PeUSDMainnet.sol:L74-L77  function convertToPeUSD(address user, uint256 eusdAmount) public {  require(_msgSender() == user || _msgSender() == address(this), \"MDM\");  require(eusdAmount != 0, \"ZA\");  require(EUSD.balanceOf(address(this)) + eusdAmount <= configurator.getEUSDMaxLocked(),\"ESL\");  The issue is that there is a way to bypass this restriction. An attacker can get a flash loan (in EUSD) from this contract, essentially reducing the visible amount of locked tokens (EUSD.balanceOf(address(this))).  Recommendation  Multiple approaches can solve this issue. One would be adding reentrancy protection. Another one could be keeping track of the borrowed amount for a flashloan.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.4 Liquidation Keepers Automatically Become eUSD Debt Providers for Other Liquidations.    ", "body": "  Resolution                           Fixed in   commit bbcf1867ef66cfdcd4b4fd26df39518048fbde1f by adding an alternative check to the allowance flag to see if  Description  One of the most important mechanisms in the Lybra Protocol is the liquidation of poorly collateralized vaults. For example, if a vault is found to have a collateralization ratio that is too small, a liquidator may provide debt tokens to the protocol and retrieve the vault collateral at a discount:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L148-L170  function liquidation(address provider, address onBehalfOf, uint256 assetAmount) external virtual {  uint256 assetPrice = getAssetPrice();  uint256 onBehalfOfCollateralRatio = (depositedAsset[onBehalfOf] * assetPrice * 100) / borrowed[onBehalfOf];  require(onBehalfOfCollateralRatio < badCollateralRatio, \"Borrowers collateral ratio should below badCollateralRatio\");  require(assetAmount * 2 <= depositedAsset[onBehalfOf], \"a max of 50% collateral can be liquidated\");  require(EUSD.allowance(provider, address(this)) != 0, \"provider should authorize to provide liquidation EUSD\");  uint256 eusdAmount = (assetAmount * assetPrice) / 1e18;  _repay(provider, onBehalfOf, eusdAmount);  uint256 reducedAsset = assetAmount * 11 / 10;  totalDepositedAsset -= reducedAsset;  depositedAsset[onBehalfOf] -= reducedAsset;  uint256 reward2keeper;  if (provider == msg.sender) {  collateralAsset.safeTransfer(msg.sender, reducedAsset);  } else {  reward2keeper = (reducedAsset * configurator.vaultKeeperRatio(address(this))) / 110;  collateralAsset.safeTransfer(provider, reducedAsset - reward2keeper);  collateralAsset.safeTransfer(msg.sender, reward2keeper);  emit LiquidationRecord(provider, msg.sender, onBehalfOf, eusdAmount, reducedAsset, reward2keeper, false, block.timestamp);  To liquidate the vault, the liquidator needs to transfer debt tokens from the provider address, which in turn needs to have had approved allowance of the token for the vault:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L154  require(EUSD.allowance(provider, address(this)) != 0, \"provider should authorize to provide liquidation EUSD\");  The allowance doesn t need to be large, it only needs to be non-zero. While it is true that in the superLiquidation function the allowance check is for eusdAmount, which is the amount associated with assetAmount (the requested amount of collateral to be liquidated), the liquidator could simply call the maximum of the allowance the provider has given to the vault and then repeat the liquidation process. The allowance does not actually decrease throughout the liquidation process.  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L191  require(EUSD.allowance(provider, address(this)) >= eusdAmount, \"provider should authorize to provide liquidation EUSD\");  Notably, this address doesn t have to be the same one as the liquidator. In fact, there are no checks on whether the liquidator has an agreement or allowance from the provider to use their tokens in this particular vault s liquidation. The contract only checks to see if the provider has EUSD allowance for the vault, and how to split the rewards if the provider is different from the liquidator:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L162-L168  if (provider == msg.sender) {  collateralAsset.safeTransfer(msg.sender, reducedAsset);  } else {  reward2keeper = (reducedAsset * configurator.vaultKeeperRatio(address(this))) / 110;  collateralAsset.safeTransfer(provider, reducedAsset - reward2keeper);  collateralAsset.safeTransfer(msg.sender, reward2keeper);  In fact, this is a design choice of the system to treat the allowance to the vault as an agreement to become a public provider of debt tokens for the liquidation process. It is important to note that there are incentives associated with being a provider as they get the collateral asset at a discount.  However, it is not obvious from documentation at the time of the audit nor the code that an address having a non-zero EUSD allowance for the vault automatically allows other users to use that address as a provider. Indeed, many general-purpose liquidator bots use their tokens during liquidations, using the same address for both the liquidator and the provider. As a result, this would put that address at the behest of any other user who would want to utilize these tokens in liquidations. The user might not be comfortable doing this trade in any case, even at a discount.  In fact, due to this mechanism, even during consciously initiated liquidations MEV bots could spot this opportunity and front-run the liquidator s transaction. A frontrunner could put themselves as the keeper and the original user as the provider, grabbing the reward2keeper fee and leaving the original address with fewer rewards and failed gas after the liquidation.  Recommendation  While the mechanism is understood to be done for convenience and access to liquidity as a design decision, this could put unaware users in unfortunate situations of having performed a trade without explicit consent. Specifically, the MEV attack vector could be executed and repeated without fail by a capable actor monitoring the mempool. Consider having a separate, explicit flag for allowing others to use a user s tokens during liquidation, thus also accommodating solo liquidators by removing the MEV attack vector. Consider explicitly mentioning these mechanisms in the documentation as well.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.5 Use the Same Solidity Version Across Contracts.    ", "body": "  Resolution                           Fixed in   commit 33af5c92044cd84c7f69eb8a55316d1e8535ea84 and  commit b1c6ac26b262ec6011c14297583d67d9e3e94326.  Description  Most contracts use the same Solidity version with pragma solidity ^0.8.17. The only exception is the StakingRewardsV2 contract which has pragma solidity ^0.8.  contracts/lybra/miner/stakerewardV2pool.sol:L2  pragma solidity ^0.8;  Recommendation  If all contracts will be tested and utilized together, it would be best to utilize and document the same version within all contract code to avoid any issues and inconsistencies that may arise across Solidity versions.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.6 Duplication of Bad Collateral Ratio   ", "body": "  Resolution  The Lybra Finance team has acknowledged this as a choice by design and provided the following note:  The liquidation ratio for each eUSD vault is fixed, and this has been stated in our docs. Therefore, we will keep it unchanged.  Description  It is possible to set a bad collateral ratio in the LybraConfigurator contract for any vault:  contracts/lybra/configuration/LybraConfigurator.sol:L137-L141  function setBadCollateralRatio(address pool, uint256 newRatio) external onlyRole(DAO) {  require(newRatio >= 130 * 1e18 && newRatio <= 150 * 1e18 && newRatio <= vaultSafeCollateralRatio[pool] + 1e19, \"LNA\");  vaultBadCollateralRatio[pool] = newRatio;  emit SafeCollateralRatioChanged(pool, newRatio);  But in the LybraEUSDVaultBase contract, this value is fixed and cannot be changed:  contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L19  uint256 public immutable badCollateralRatio = 150 * 1e18;  This duplication of values can be misleading at some point. It s better to make sure you cannot change the bad collateral ratio in the LybraConfigurator contract for some types of vaults.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.7 Missing Events.    ", "body": "  Resolution                           Fixed in   commit 518ef434c6f89c7747373b6ae178d9665d3637f2.  Description  In a few cases in the Lybra Protocol system, there are contracts that are missing events in significant scenarios, such as important configuration changes like a price oracle change. Consider implementing more events in the below examples.  Examples  No events in the contract:  contracts/lybra/miner/esLBRBoost.sol:L10-L30  contract esLBRBoost is Ownable {  esLBRLockSetting[] public esLBRLockSettings;  mapping(address => LockStatus) public userLockStatus;  IMiningIncentives public miningIncentives;  // Define a struct for the lock settings  struct esLBRLockSetting {  uint256 duration;  uint256 miningBoost;  // Define a struct for the user's lock status  struct LockStatus {  uint256 lockAmount;  uint256 unlockTime;  uint256 duration;  uint256 miningBoost;  // Constructor to initialize the default lock settings  constructor(address _miningIncentives) {  Missing an event during a premature unlock:  contracts/lybra/miner/ProtocolRewardsPool.sol:L125-L135  function unlockPrematurely() external {  require(block.timestamp + exitCycle - 3 days > time2fullRedemption[msg.sender], \"ENW\");  uint256 burnAmount = getReservedLBRForVesting(msg.sender) - getPreUnlockableAmount(msg.sender);  uint256 amount = getPreUnlockableAmount(msg.sender) + getClaimAbleLBR(msg.sender);  if (amount > 0) {  LBR.mint(msg.sender, amount);  unstakeRatio[msg.sender] = 0;  time2fullRedemption[msg.sender] = 0;  grabableAmount += burnAmount;  Missing events for setting important configurations such as setToken, setLBROracle, and setPools:  contracts/lybra/miner/EUSDMiningIncentives.sol:L87-L102  function setToken(address _lbr, address _eslbr) external onlyOwner {  LBR = _lbr;  esLBR = _eslbr;  function setLBROracle(address _lbrOracle) external onlyOwner {  lbrPriceFeed = AggregatorV3Interface(_lbrOracle);  function setPools(address[] memory _vaults) external onlyOwner {  require(_vaults.length <= 10, \"EL\");  for (uint i = 0; i < _vaults.length; i++) {  require(configurator.mintVault(_vaults[i]), \"NOT_VAULT\");  vaults = _vaults;  Missing events for setting important  configurations such as setRewardsDuration and setBoost:  contracts/lybra/miner/stakerewardV2pool.sol:L121-L130  // Allows the owner to set the rewards duration  function setRewardsDuration(uint256 _duration) external onlyOwner {  require(finishAt < block.timestamp, \"reward duration not finished\");  duration = _duration;  // Allows the owner to set the boost contract address  function setBoost(address _boost) external onlyOwner {  esLBRBoost = IesLBRBoost(_boost);  Missing event during what is essentially staking LBR into esLBR (such as in ProtocolRewardsPool.stake()). Consider an appropriate event here such as StakeLBR:  contracts/lybra/miner/esLBRBoost.sol:L55-L58  if(useLBR) {  IesLBR(miningIncentives.LBR()).burn(msg.sender, lbrAmount);  IesLBR(miningIncentives.esLBR()).mint(msg.sender, lbrAmount);  Recommendation  Implement additional events as appropriate.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.8 Incorrect Interfaces    ", "body": "  Resolution                           Fixed in   commit 90285107de8a6754954c303cd69d97b5fdb4e248,  commit 0ac9cd732b601d0baef2690ef9f9f02cda989331, and  commit 518ef434c6f89c7747373b6ae178d9665d3637f2.  Description  In a few cases, incorrect interfaces are used on top of contracts. Though the effect is the same as the contracts are just tokens and follow the same interfaces, it is best practice to implement correct interfaces.  IPeUSD is used instead of IEUSD  contracts/lybra/configuration/LybraConfigurator.sol:L60  IPeUSD public EUSD;  IPeUSD is used instead of IEUSD  contracts/lybra/configuration/LybraConfigurator.sol:L109  if (address(EUSD) == address(0)) EUSD = IPeUSD(_eusd);  IesLBR instead of ILBR  contracts/lybra/miner/ProtocolRewardsPool.sol:L29  IesLBR public LBR;  IesLBR instead of ILBR  contracts/lybra/miner/ProtocolRewardsPool.sol:L57  LBR = IesLBR(_lbr);  Recommendation  Implement correct interfaces for consistency.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "6.9 The ETH Staking Rewards Distribution Tradeoff", "body": "  Description  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2023/08/lybra-finance/"}, {"title": "5.1 Eliminate assembly code by using ABI decode    ", "body": "  Resolution                           All assembly code was replaced with proper use of   Description  There are several locations where assembly code is used to access and decode byte arrays (including uses inside loops). Even though assembly code was used for gas optimization, it reduces the readability (and future updatability) of the code.  Examples  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L39-L44  assembly {  flag := mload(add(_data, 32))  if (flag == CHANGE_PARTITION_FLAG) {  assembly {  toPartition := mload(add(_data, 64))  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L43-L44  assembly {  toPartition := mload(add(_data, 64))  Same code as above is also present here: /flexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1403 flexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1407  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1463-L1470  for (uint256 i = 116; i <= _operatorData.length; i = i + 32) {  bytes32 temp;  assembly {  temp := mload(add(_operatorData, i))  proof[index] = temp;  index++;  Recommendation  As discussed in the mid-audit meeting, it is a good solution to use ABI decode since all uses of assembly simply access 32-byte chunks of data from user input. This should eliminate all assembly code and make the code significantly more clean. In addition, it might allow for more compact encoding in some cases (for instance, by eliminating or reducing the size of the flags).  This suggestion can be also applied to Merkle Root verifications/calculation code, which can reduce the for loops and complexity of these functions.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.2 Ignored return value for transferFrom call    ", "body": "  Resolution                           Fixed by adding a   Description  When burning swap tokens the return value of the transferFrom call is ignored. Depending on the token s implementation this could allow an attacker to mint an arbitrary amount of Amp tokens.  Note that the severity of this issue could have been Critical if Flexa token was any arbitrarily tokens. We quickly verified that Flexa token implementation would revert if the amount exceeds the allowance, however it might not be the case for other token implementations.  code/amp-contracts/contracts/Amp.sol:L619-L620  swapToken.transferFrom(_from, swapTokenGraveyard, amount);  Recommendation  The code should be changed like this:  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.3 No integration tests for the two main components    ", "body": "  Resolution  amp-contracts added as a submodule to collateral-manager and full integration tests added  It is recommended to write test suites that achieve high code coverage to prevent missing obvious bugs that tests could cover.  Description  The existing tests cover each of the two main components and each set of tests mocks the other component. While this is good for unit testing some issues might be missed without proper system/integration tests that cover all components.  Recommendation  Consider adding system/integration tests for all components. As we ve seen in the recent issues in multi-contract smart contract systems, it s becoming more crucial to have a full test suits for future changes to the code base. Not having inter-component tests, could result in issues in the next development and deployment cycles.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.4 Potentially insufficient validation for operator transfers    ", "body": "  Resolution  removing operatorTransferByPartition and simplifying the interfaces to only tranferByPartition  This removes the existing tranferByPartition, converting operatorTransferByPartition to it. The reason for this is to make the client interface simpler, where there is one method to transfer by partition, and that method can be called by either a sender wanting to transfer from their own address, or an operator wanting to transfer from a different token holder address. We found that it was redundant to have multiple methods, and the client convenience wasn t worth the confusion.  Description  For operator transfers, the current validation does not require the sender to be an operator (as long as the transferred value does not exceed the allowance):  code/amp-contracts/contracts/Amp.sol:L755-L759  require(  _isOperatorForPartition(_partition, msg.sender, _from) ||  (_value <= _allowedByPartition[_partition][_from][msg.sender]),  EC_53_INSUFFICIENT_ALLOWANCE  );  It is unclear if this is the intention or whether the logical or should be a logical and.  Recommendation  Confirm that the code matches the intention. If so, consider documenting the behavior (for instance, by changing the name of function operatorTransferByPartition.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.5 Potentially missing nonce check   ", "body": "  Resolution  Nothing was done here, as Dave M writes:  The first two are working as intended, and the third does check that the value is monotonically increasing.  Description  When executing withdrawals in the collateral manager the per-address withdrawal nonce is simply updated without checking that the new nonce is one greater than the previous one (see Examples). It seems like without such a check it might be  easy to make mistakes and causing issues with ordering of withdrawals.  Examples  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L663-L664  addressToWithdrawalNonce[_partition][supplier] = withdrawalRootNonce;  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L845-L846  addressToWithdrawalNonce[_partition][supplier] = maxWithdrawalRootNonce;  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1155-L1156  maxWithdrawalRootNonce = _nonce;  Recommendation  Consider adding more validation and sanity checks for nonces on per-address withdrawals.  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.6 Unbounded loop when validating Merkle proofs    ", "body": "  Resolution                           The loop was removed by switching to   Description  It seems like the loop for validating Merkle proofs is unbounded. If possible it would be good to have an upper bound to prevent DoS-like attacks. It seems like the depth of the tree, and thus, the length of the proof could be bounded.  This could also simplify the decoding and make it more robust. For instance, in _decodeWithdrawalOperatorData it is unclear what happens if the data length is not a multiple of 32. It seems like it might result in out-of-bound reads.  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1460-L1470  uint256 proofNb = (_operatorData.length - 84) / 32;  bytes32[] memory proof = new bytes32[](proofNb);  uint256 index = 0;  for (uint256 i = 116; i <= _operatorData.length; i = i + 32) {  bytes32 temp;  assembly {  temp := mload(add(_operatorData, i))  proof[index] = temp;  index++;  Recommendation  Consider enforcing a bound on the length of Merkle proofs.  Also note that if similar mitigation method as issue 5.1 is used, this method can be replaced by a simpler function using ABI Decode, which does not have any unbounded issues as the sizes of the hashes are fixed (or can be indicated in the passed objects)  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.7 Mitigation for possible reentrancy in token transfers    ", "body": "  Resolution  Fixed as recommended.  Description  ERC777 adds significant features to the token implementation, however there are some known risks associated with this token, such as possible reentrancy attack vector. Given that the Amp token uses hooks to communicate to Collateral manager, it seems that the environment is trusted and safe. However, a minor modification to the implementation can result in safer implementation of the token transfer.  Examples  In Amp.sol --> _transferByPartition()  code/amp-contracts/contracts/Amp.sol:L1152-L1177  require(  _balanceOfByPartition[_from][_fromPartition] >= _value,  EC_52_INSUFFICIENT_BALANCE  );  bytes32 toPartition = _fromPartition;  if (_data.length >= 64) {  toPartition = _getDestinationPartition(_fromPartition, _data);  _callPreTransferHooks(  _fromPartition,  _operator,  _from,  _to,  _value,  _data,  _operatorData  );  _removeTokenFromPartition(_from, _fromPartition, _value);  _transfer(_from, _to, _value);  _addTokenToPartition(_to, toPartition, _value);  _callPostTransferHooks(  toPartition,  Recommendation  It is suggested to move any condition check that is checking the balance to after the external call. However _callPostTransferHooks needs to be called after the state changes, so the suggested mitigation here is to move the require at line 1152 to after _callPreTransferHooks() function (e.g. line 1171).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.8 Potentially inconsistent input validation    ", "body": "  Resolution  transferWithData was removed as a resolution of another filed issue, the rest are documented properly.  The msg.sender cannot be authorized or revoked from being an operator for itself. This should also be clear from the natspec comments now.  Description  There are some functions that might require additional input validation (similar to other functions):  Examples  Amp.transferWithData: require(_isOperator(msg.sender, _from), EC_58_INVALID_OPERATOR); like in  code/amp-contracts/contracts/Amp.sol:L699  require(_isOperator(msg.sender, _from), EC_58_INVALID_OPERATOR);  Amp.authorizeOperatorByPartition: require(_operator != msg.sender); like in  code/amp-contracts/contracts/Amp.sol:L789  require(_operator != msg.sender);  Amp.revokeOperatorByPartition: require(_operator != msg.sender); like in  code/amp-contracts/contracts/Amp.sol:L800  require(_operator != msg.sender);  Recommendation  Consider adding additional input validation.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.9 ERC20 compatibility of Amp token using defaultPartition    ", "body": "  Resolution  This fix resulted in significant changes to the token allowance work flow. The new implementation of balanceOf represents the total balance of tokens at that address (across any partition), instead of only default partition.  The approve + allowance based operations were using a distinct global allowance mapping, while the rest of the ERC20 compat operations were using the partition state mappings with the default partition. This makes the allowance operations behave the same as the balance based operations.  Description  It is somewhat unclear how the Amp token ensures ERC20 compatibility. While the default partition is used in some places (for instance, in function balanceOf) there are also separate fields for (aggregated) balances/allowances. This seems to introduce some redundancy and raises certain questions about when which fields are relevant.  Examples  _allowed is used in function allowance instead of _allowedByPartition with the default partition  An Approval event should be emitted when approving the default partition  code/amp-contracts/contracts/Amp.sol:L1494  emit ApprovalByPartition(_partition, _tokenHolder, _spender, _amount);  increaseAllowance() vs. increaseAllowanceByPartition()  Recommendation  After the mid-audit discussion, it was clear that the general balanceOf method (with no partition) is not needed and can be replaced with a balanceOf function that returns balance of the default partition, similarly for allowance, the general increaseAllowance function can simply call increaseAllowanceByPartition using default partition (same for decreaseAllowance).  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.10 Duplicate code better be moved to shared library    ", "body": "  Resolution                           aforementioned functions were moved to a shared library   Description  There are some functionalities that the code is duplicated between different smart contracts.  Examples  _getDestinationPartition() is present in both PartitionBase.sol and FlexaCollateralManager.sol  Note that in PartitionBase the usage results in dead code in the contract.  code/amp-contracts/contracts/Amp.sol:L1158-L1160  if (_data.length >= 64) {  toPartition = _getDestinationPartition(_fromPartition, _data);  code/amp-contracts/contracts/partitions/PartitionsBase.sol:L33-L36  toPartition = _fromPartition;  if (_data.length < 64) {  return toPartition;  _splitPartition() is present in FlexaCollateralManager.sol, PartitionBase.sol with slightly different implementations. One has an extra return value for subPartition which is not used in the code under audit  Recommendation  Use a shared library for these functions, possibly ParitionBased.sol can be used in Collateral Manager.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.11 Additional validation for canReceive    ", "body": "  Resolution                           Added proper checks and merged   Description  For FlexaCollateralManager.tokensReceived there is validation to ensure that only the Amp calls the function. In contrast, there is no such validation for canReceive and it is unclear if this is the intention.  Examples  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L492-L493  require(msg.sender == amp, \"Invalid sender\");  Recommendation  Consider adding a conjunct msg.sender == amp in function _canReceive.  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L468-L470  function _canReceive(address _to, bytes32 _destinationPartition) internal view returns (bool) {  return _to == address(this) && partitions[_destinationPartition];  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.12 Update to Solidity 0.6.10    ", "body": "  Resolution                           Updated to   Description  Due to an issue found in 0.6.9, it is recommended to update the compiler version to latest version 0.6.10.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.13 Discrepancy between code and comments    ", "body": "  Description  There are some discrepancies between (uncommented) code and the documentations comment:  Examples  code/amp-contracts/contracts/Amp.sol:L459-L462  // Indicate token verifies Amp, ERC777 and ERC20 interfaces  ERC1820Implementer._setInterface(AMP_INTERFACE_NAME);  ERC1820Implementer._setInterface(ERC20_INTERFACE_NAME);  // ERC1820Implementer._setInterface(ERC777_INTERFACE_NAME);  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L268-L279  /**  @notice Indicates a supply refund was executed  @param supplier Address whose refund authorization was executed  @param partition Partition from which the tokens were transferred  @param amount Amount of tokens transferred  /  event SupplyRefund(  address indexed supplier,  bytes32 indexed partition,  uint256 amount,  uint256 indexed nonce  );  Recommendation  Consider updating either the code or the comment.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.14 Several fields could potentially be private   ", "body": "  Resolution  : Comment from Flexa team:  We audited the suggested fields, and determined that we would like them to be public for transparency and/or functionality reasons.  Description  Several fields in Amp could possibly be private:  Examples  swapToken:  code/amp-contracts/contracts/Amp.sol:L261  ISwapToken public swapToken;  swapTokenGraveyard:  code/amp-contracts/contracts/Amp.sol:L268  address public constant swapTokenGraveyard = 0x000000000000000000000000000000000000dEaD;  collateralManagers:  code/amp-contracts/contracts/Amp.sol:L236  address[] public collateralManagers;  partitionStrategies:  code/amp-contracts/contracts/Amp.sol:L248  bytes4[] public partitionStrategies;  The same hold for several fields in FlexaCollateralManager. For instance:  partitions:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L78  mapping(bytes32 => bool) public partitions;  nonceToSupply:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L144  mapping(uint256 => Supply) public nonceToSupply;  withdrawalRootToNonce:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L163  mapping(bytes32 => uint256) public withdrawalRootToNonce;  Recommendation  Double-check that you really want to expose those fields.  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "5.15 Several fields could be declared immutable   ", "body": "  Resolution  Comment from Flexa team:  We tried to add this, but found that it made validating the contract on Etherscan impossible. We have added comments to a reader of the contract indicating the fields are immutable after deployment, though.  Description  Several fields could be declared immutable to make clear that they never change after construction:  Examples  Amp._name:  code/amp-contracts/contracts/Amp.sol:L129  string internal _name;  Amp._symbol:  code/amp-contracts/contracts/Amp.sol:L134  string internal _symbol;  Amp.swapToken:  code/amp-contracts/contracts/Amp.sol:L261  ISwapToken public swapToken;  FlexaCollateralManager.amp:  code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L73  address public amp;  Recommendation  Use the immutable annotation in Solidity (see Immutable).  ", "labels": ["Consensys", "Minor", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/06/amp/"}, {"title": "6.1 Test code present in the code base    ", "body": "  Resolution                           Fixed in   lukso-network/rICO-smart-contracts@edb880c.  Description  Test code are present in the code base. This is mainly a reminder to fix those before production.  Examples  rescuerAddress and freezerAddress are not even in the function arguments.  code/contracts/ReversibleICO.sol:L243-L247  whitelistingAddress = _whitelistingAddress;  projectAddress = _projectAddress;  freezerAddress = _projectAddress; // TODO change, here only for testing  rescuerAddress = _projectAddress; // TODO change, here only for testing  Recommendation  Make sure all the variable assignments are ready for production before deployment to production.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.2 FreezerAddress has more power than required   ", "body": "  Resolution                           This issue is acknowledged by the client and the behaviour has been documented in   security measurements.  Description  FreezerAddress is designed to have the ability of freezing the contract in case of emergency. However, indirectly, there are other changes in the system that can result from the freeze.  Examples  FreezerAddress can extend the rICO time frame. Given that the frozenPeriod is deducted from the blockNumber in stage calculations, the buyPhaseEndBlock is technically equals to buyPhaseEndBlock + frozenPeriod  FreezerAddress can call disableEscapeHatch(), which disables the escape hatch and rendering RescuerAddress useless.  Recommendation  If these behaviors are intentional they should be well documented and specified. If not, they should be removed.  In the case they are, indeed, intentional the audit team believes that, for Example 1., there should be some event fired to serve as notification for the participants (possibly followed by off-chain infrastructure to warn them through email or other communication channel).  ", "labels": ["Consensys", "Medium", "Acknowledged"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.3 frozenPeriod is subtracted twice for calculating the current price    ", "body": "  Resolution                           Found in parallel to the audit team and has been mitigated in   lukso-network/rICO-smart-contracts@ebc4bce . The issue was further simplified by adding  lukso-network/rICO-smart-contracts@e4c9ed5 to remove ambiguity when calculating current block number.  Description  If the contract had been frozen, the current stage price will calculate the price by subtracting the frozenPeriod twice and result in wrong calculation.  getCurrentBlockNumber() subtracts frozenPeriod once, and then getStageAtBlock() will also subtract the same number again.  Examples  code/contracts/ReversibleICO.sol:L617-L619  function getCurrentStage() public view returns (uint8) {  return getStageAtBlock(getCurrentBlockNumber());  code/contracts/ReversibleICO.sol:L711-L714  function getCurrentBlockNumber() public view returns (uint256) {  return uint256(block.number)  .sub(frozenPeriod); // make sure we deduct any frozenPeriod from calculations  code/contracts/ReversibleICO.sol:L654-L656  function getStageAtBlock(uint256 _blockNumber) public view returns (uint8) {  uint256 blockNumber = _blockNumber.sub(frozenPeriod); // adjust the block by the frozen period  Recommendation  Make sure frozenPeriod calculation is done correctly. It could be solved by renaming getCurrentBlockNumber() to reflect the calculation done inside the function.  e.g. :  getCurrentBlockNumber() : gets current block number  getCurrentEffectiveBlockNumber() : calculates the effective block number deducting frozenPeriod  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.4 Lockup condition in getStageAtBlock()    ", "body": "  Resolution  Even though the freeze pattern does indeed create a lot of additional complexity to the protocol, the particular require mentioned in the issue corpus by the audit team was found to never be triggered in a harmful way by rICO s development team.  In the light of this new discovery, we are greatly reducing the severity of the issue to  Minor . The reason why it is still kept as an issue is that the implementation of the freezing mechanism could still be greatly improved as we saw in the presented fixes here:  lukso-network/rICO-smart-contracts@e4c9ed5  The changes resulted in a much more resilient rICO implementation.  Description  Given that the contract has been frozen at least once, if the frozenPeriod is longer than the period before the freeze event (starting from commitPhaseStartBlock till the freezeStart), the following require in getStageAtBlock() will revert due to the fact that blockNumber < commitPhaseStartBlock:  uint256 blockNumber = _blockNumber.sub(frozenPeriod); // adjust the block by the frozen period  require(blockNumber >= commitPhaseStartBlock && blockNumber <= buyPhaseEndBlock, \"Block outside of rICO period.\");  Note that the issue here is also related to the way currentBlockNumber is calculated (See issue 6.3 and Separate currentBlock from currentEffectiveBlock.  getCurrentStage() is called for every accept or cancelation of contributions and this lockup can result in total system halt.  Recommendation  Given that in the init function, the following condition is checked:  require(_commitPhaseStartBlock > getCurrentBlockNumber(), \"Start block cannot be set in the past.\");  The check in the getStageAtBlock() can be removed. However this is assuming that the correct calculation of the currentEffectiveBlockNumber is used.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "6.5 emit events for significant state changes    ", "body": "  Resolution                           This issue was discussed in the code walk through meeting and was fixed, by adding proper events to the code base in   lukso-network/rICO-smart-contracts@77517a4, before the end of the audit.  Description  Events are useful for UI changes and user notifications. The code base overall can use more use of events to update the UI and participants.  One of the most important aspects that must emit events, are when system state and functionality are changed. These functions require to emit events for better visibility to the participants:  freeze()  unfreeze()  disableEscapeHatch()  escapeHatch()  Recommendation  emit events when system state is changed.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/04/rico/"}, {"title": "4.1 Node Operators Can Stake Validators That Were Not Proposed by Them. ", "body": "  In GeodeFi system node operators are meant to add the new validators in two steps:  Proposal step where 1 ETH of the pre-stake deposit is committed.  Stake step, where the 1 ETH pre-stake is reimbursed to the node operator, and the 32ETH user stake is sent to a validator.  The issue itself stems from the fact that node operators are allowed to stake the validators of the other node operators. In the stake() function there is no check of the validator s operatorId against the operator performing the stake. Meaning that node operator A can stake validators of node operator B.  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L1478-L1558  function stake(  PooledStaking storage self,  DSML.IsolatedStorage storage DATASTORE,  uint256 operatorId,  bytes[] calldata pubkeys  ) external {  _authenticate(DATASTORE, operatorId, false, true, [true, false]);  require(  (pubkeys.length > 0) && (pubkeys.length <= DCL.MAX_DEPOSITS_PER_CALL),  \"SML:1 - 50 validators\"  );  uint256 _verificationIndex = self.VERIFICATION_INDEX;  for (uint256 j = 0; j < pubkeys.length; ) {  require(  _canStake(self, pubkeys[j], _verificationIndex),  \"SML:NOT all pubkeys are stakeable\"  );  unchecked {  j += 1;  bytes32 activeValKey = DSML.getKey(operatorId, rks.activeValidators);  bytes32 proposedValKey = DSML.getKey(operatorId, rks.proposedValidators);  uint256 poolId = self.validators[pubkeys[0]].poolId;  bytes memory withdrawalCredential = DATASTORE.readBytes(poolId, rks.withdrawalCredential);  uint256 lastIdChange = 0;  for (uint256 i = 0; i < pubkeys.length; ) {  uint256 newPoolId = self.validators[pubkeys[i]].poolId;  if (poolId != newPoolId) {  uint256 sinceLastIdChange;  unchecked {  sinceLastIdChange = i - lastIdChange;  DATASTORE.subUint(poolId, rks.secured, (DCL.DEPOSIT_AMOUNT * (sinceLastIdChange)));  DATASTORE.subUint(poolId, proposedValKey, (sinceLastIdChange));  DATASTORE.addUint(poolId, activeValKey, (sinceLastIdChange));  lastIdChange = i;  poolId = newPoolId;  withdrawalCredential = DATASTORE.readBytes(poolId, rks.withdrawalCredential);  DCL.depositValidator(  pubkeys[i],  withdrawalCredential,  self.validators[pubkeys[i]].signature31,  (DCL.DEPOSIT_AMOUNT - DCL.DEPOSIT_AMOUNT_PRESTAKE)  );  self.validators[pubkeys[i]].state = VALIDATOR_STATE.ACTIVE;  unchecked {  i += 1;  uint256 sinceLastIdChange;  unchecked {  sinceLastIdChange = pubkeys.length - lastIdChange;  if (sinceLastIdChange > 0) {  DATASTORE.subUint(poolId, rks.secured, DCL.DEPOSIT_AMOUNT * (sinceLastIdChange));  DATASTORE.subUint(poolId, proposedValKey, (sinceLastIdChange));  DATASTORE.addUint(poolId, activeValKey, (sinceLastIdChange));  _increaseWalletBalance(DATASTORE, operatorId, DCL.DEPOSIT_AMOUNT_PRESTAKE * pubkeys.length);  emit Stake(pubkeys);  This issue can later be escalated to a point where funds can be stolen. Consider the following case:  The attacker creates 10 validators directly through the ETH2 deposit contract using himself as the withdrawal address.  Attacker node operator proposes to add 10 validators and adds the 10ETH as pre-stake deposit. Since validators already exist withdrawal credentials will remain those of the Attacker. As a result of those actions, we have inflated the number of proposed validators the attacker has inside the Geode system.  Attacker then takes the validator keys proposed by someone else and stakes them. Since there is no check described above that is allowed. His proposed validators count will also decrease without a revert due to steps above.  As a result of that step, attacker will receive the pre-stake of the operator that actually proposed those validators. The attacker will immediately proceed to call decreaseWallet() to withdraw the funds.  The attacker will then withdraw the pre-stake he deposited in the initial validators with faulty withdrawal credential.  This way an attacker could profit 10ETH.  This can be prevented by making sure that validator s operatorId is checked on the stake() function call.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.2 Cannot Blame Operator for Proposed Validator ", "body": "  In the current code, anyone can blame an operator who does not withdraw in time:  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L931-L946  function blameOperator(  PooledStaking storage self,  DSML.IsolatedStorage storage DATASTORE,  bytes calldata pk  ) external {  require(  self.validators[pk].state == VALIDATOR_STATE.ACTIVE,  \"SML:validator is never activated\"  );  require(  block.timestamp > self.validators[pk].createdAt + self.validators[pk].period,  \"SML:validator is active\"  );  _imprison(DATASTORE, self.validators[pk].operatorId, pk);  There is one more scenario where the operator should be blamed. When a validator is in the PROPOSED state, only the operator can call the stake function to actually stake the rest of the funds. Before that, the funds of the pool will be locked under the rks.secured variable. So the malicious operator can lock up 31 ETH of the pool indefinitely by locking up only 1 ETH of the attacker. There is currently no way to release these 31 ETH.  We recommend introducing a mechanism that allows one to blame the operator for not staking for a long time after it was approved.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.3 Validators Array Length Has to Be Updated When the Validator Is Alienated. ", "body": "  In GeodeFi when the node operator creates a validator with incorrect withdrawal credentials or signatures the Oracle has the ability to alienate this validator. In the process of alienation, the validator status is updated.  contracts/Portal/modules/StakeModule/libs/OracleExtensionLib.sol:L111-L136  function _alienateValidator(  SML.PooledStaking storage STAKE,  DSML.IsolatedStorage storage DATASTORE,  uint256 verificationIndex,  bytes calldata _pk  ) internal {  require(STAKE.validators[_pk].index <= verificationIndex, \"OEL:unexpected index\");  require(  STAKE.validators[_pk].state == VALIDATOR_STATE.PROPOSED,  \"OEL:NOT all pubkeys are pending\"  );  uint256 operatorId = STAKE.validators[_pk].operatorId;  SML._imprison(DATASTORE, operatorId, _pk);  uint256 poolId = STAKE.validators[_pk].poolId;  DATASTORE.subUint(poolId, rks.secured, DCL.DEPOSIT_AMOUNT);  DATASTORE.addUint(poolId, rks.surplus, DCL.DEPOSIT_AMOUNT);  DATASTORE.subUint(poolId, DSML.getKey(operatorId, rks.proposedValidators), 1);  DATASTORE.addUint(poolId, DSML.getKey(operatorId, rks.alienValidators), 1);  STAKE.validators[_pk].state = VALIDATOR_STATE.ALIENATED;  emit Alienated(_pk);  An additional thing that has to be done during the alienation process is that the validator s count should be decreased in order for the monopoly threshold to be calculated correctly. That is because the length of the validators array is used twice in the OpeartorAllowance function:  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L975  uint256 numOperatorValidators = DATASTORE.readUint(operatorId, rks.validators);  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L988  uint256 numPoolValidators = DATASTORE.readUint(poolId, rks.validators);  Without the update of the array length, the monopoly threshold as well as the time when the fallback operator will be able to participate is going to be computed incorrectly.  It could be beneficial to not refer to rks.validators in the operator allowance function and instead use the rks.proposedValidators + rks.alienatedValidators + rks.activeValidators. This way allowance function can always rely on the most up to date data.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.4 A Potential Controller Update Issue. ", "body": "  We identified a potential issue in the code that is out of our current scope. In the GeodeModuleLib, there is a function that allows a controller of any ID to update the controller address:  contracts/Portal/modules/GeodeModule/libs/GeodeModuleLib.sol:L299-L309  function changeIdCONTROLLER(  DSML.IsolatedStorage storage DATASTORE,  uint256 id,  address newCONTROLLER  ) external onlyController(DATASTORE, id) {  require(newCONTROLLER != address(0), \"GML:CONTROLLER can not be zero\");  DATASTORE.writeAddress(id, rks.CONTROLLER, newCONTROLLER);  emit ControllerChanged(id, newCONTROLLER);  It s becoming tricky with the upgradability mechanism. The current version of any package is stored in the following format:  DATASTORE.readAddress(versionId, rks. CONTROLLER). So the address of the current implementation of any package is stored as rks.CONTROLLER. That means if someone can hack the implementation address and make a transaction on its behalf to change the controller, this attacker can change the current implementation to a malicious one.  While this issue may not be exploitable now, many new packages are still to be implemented. So you need to ensure that nobody can get any control over the implementation contract.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.5 The Price Change Limit Could Prevent the Setting of the Correct Price. ", "body": "  In the share price update logic of OracleExtensionLib, there is a function called   ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "4.6 Potential for a Cross-Site-Scripting When Creating a Pool. ", "body": "  When creating a new staking pool, the creator has the ability to name it. While it does not present many issues on the chain, if this name is ever displayed on the UI it has to be handled carefully. An attacker could include a malicious script in the name and that could potentially be executed in the victim s browser.  contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L358  DATASTORE.writeBytes(poolId, rks.NAME, name);  We suggest that proper escaping is used when displaying the names of the pool on the UI. We do not recommend adding string validation on the chain.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2023/05/geode-liquid-staking/"}, {"title": "3.1 A reverting fallback function will lock up all payouts    ", "body": "  Resolution  Replace the push method to pull pattern.  Remove transfer of ETH in the process of execution, and store ETH amount to mapping(address => uint256) ethBalance (contracts/Inheritance/ETHExchange.sol, L21)  Add function withdrawETH to send ethBalance[msg.sender] (contracts/Inheritance/ETHExchange.sol, L158-L162)  Description  In BoxExchange.sol, the internal function _transferEth() reverts if the transfer does not succeed:  code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958-L963  function _transferETH(address _recipient, uint256 _amount) private {  (bool success, ) = _recipient.call{value: _amount}(  abi.encodeWithSignature(\"\")  );  require(success, \"Transfer Failed\");  The _payment() function processes a list of transfers to settle the transactions in an ExchangeBox. If any of the recipients of an Eth transfer is a smart contract that reverts, then the entire payout will fail and will be unrecoverable.  Recommendation  Implement a queuing mechanism to allow buyers/sellers to initiate the withdrawal on their own using a  pull-over-push pattern.   Ignore a failed transfer and leave the responsibility up to users to receive them properly.  ", "labels": ["Consensys", "Critical", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.2 Force traders to mint gas token    ", "body": "  Resolution  Replace push funds with Pull Pattern.  Remove transfer of ETH in the process of execution, and store ETH amount to mapping(address => uint256) ethBalance (contracts/Inheritance/ETHExchange.sol, L21)  Add function withdrawETH to send ethBalance[msg.sender] (contracts/Inheritance/ETHExchange.sol, L158-L162)  Description  Attack scenario:  Alice makes a large trade via the Fairswap_iDOLvsEth exchange. This will tie up her iDOL until the box is executed.  Mallory makes a small trades to buy ETH immediately afterwards, the trades are routed through an attack contract.  Alice needs to execute the box to get her iDOL out.  Because the gas amount is unlimited, when you Mallory s ETH is paid out to her attack contract,  mint a lot of GasToken.  If Alice has $100 worth of ETH tied up in the exchange, you can basically ransom her for $99 of gas token or else she ll never see her funds again.  Examples  code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958  function _transferETH(address _recipient, uint256 _amount) private {  Recommendation  When sending ETH, a pull-payment model is generally preferable.  This would require setting up a queue, allowing users to call a function to initiate a withdrawal.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.3 Missing Proper Access Control    ", "body": "  Resolution  Comment from Lien Protocol:  remove comment out lines to activate time control functions at Auction.sol and AuctionBoard.sol (a contract divided from Auction.sol)  Description  Some functions do not have proper access control and are public, meaning that anyone can call them. This will result in system take over depending on how critical those functionalities are.  Examples  Anyone can set IDOLContract in MainContracts.Auction.sol, which is a critical aspect of the auction contract, and it cannot be changed after it is set:  code/MainContracts/contracts/Auction.sol:L144-L148  Recommendation  /  function setIDOLContract(address contractAddress) public {  require(address(_IDOLContract) == address(0), \"IDOL contract is already registered\");  _setStableCoinContract(contractAddress);  Recommendation  Make the setIDOLContract() function internal and call it from the constructor, or only allow the deployer to set the value.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.4 Code is not production-ready    ", "body": "  Resolution  Comment from Lien Protocol:  remove comment out lines and update code to activate time control functions at AuctionTimeControl.sol  Description  Similar to other discussed issues, several areas of the code suggest that the system is not production-ready. This results in narrow test scenarios that do not cover production code flow.  Examples  In MainContracts/contracts/AuctionTimeControl.sol the following functions are commented out and replaced with same name functions that simply return True for testing purposes:  isNotStartedAuction  inAcceptingBidsPeriod  inRevealingValuationPeriod  inReceivingBidsPeriod  code/MainContracts/contracts/AuctionTimeControl.sol:L30-L39  /*  // Indicates any auction has never held for a specified BondID  function isNotStartedAuction(bytes32 auctionID) public virtual override returns (bool) {  uint256 closingTime = _auctionClosingTime[auctionID];  return closingTime == 0;  // Indicates if the auctionID is in bid acceptance status  function inAcceptingBidsPeriod(bytes32 auctionID) public virtual override returns (bool) {  uint256 closingTime = _auctionClosingTime[auctionID];  code/MainContracts/contracts/AuctionTimeControl.sol:L67-L78  // TEST  function isNotStartedAuction(bytes32 auctionID)  public  virtual  override  returns (bool)  return true;  // TEST  function inAcceptingBidsPeriod(bytes32 auctionID)  These commented-out functions contain essential functionality for the Auction contract. For example, inRevealingValuationPeriod is used to allow revealing of the bid price publicly:  code/MainContracts/contracts/Auction.sol:L403-L406  require(  inRevealingValuationPeriod(auctionID),  \"it is not the time to reveal the value of bids\"  );  Recommendation  Remove the test functions and use the production code for testing. The tests must have full coverage of the production code to be considered complete.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.5 Unable to compile contracts    ", "body": "  Resolution                           The related code was updated on the 7th day of the audit, and fixed this issue for   Description  In the Fairswap_iDOLvsImmortalOptionsrepository:  Compilation with truffle fails due to a missing file: contracts/testTokens/TestBondMaker.sol. Compilation with solc fails due to an undefined interface function:  In the Fairswap_iDOLvsLien repository:  Compilation with truffle fails due to a missing file: ./ERC20RegularlyRecord.sol. The correct filename is ./TestERC20RegularlyRecord.sol.  Recommendation  Ensure all contracts are easily compilable by following simple instructions in the README.  ", "labels": ["Consensys", "Major", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.6 Unreachable code due to checked conditions    ", "body": "  Resolution  Comment from Lien Protocol:  Fix Unreachable codes in Auction.sol which were made for the tests.  Description  The code flow in MainContracts.Auction.sol revealBid() is that it first checks if the function has been called during the reveal period, which means  after closing  and  before the end of the reveal period.   code/MainContracts/contracts/Auction.sol:L508-L517  function revealBid(  bytes32 auctionID,  uint256 price,  uint256 targetSBTAmount,  uint256 random  ) public override {  require(  inRevealingValuationPeriod(auctionID),  \"it is not the time to reveal the value of bids\"  );  However, later in the same function, code exists to introduce  Penalties for revealing too early.  This checks to see if the function was called before closing, which should not be possible given the previous check.  code/MainContracts/contracts/Auction.sol:L523-L537  /**  @dev Penalties for revealing too early.  Some participants may not follow the rule and publicate their bid information before the reveal process.  In such a case, the bid price is overwritten by the bid with the strike price (slightly unfavored price).  /  uint256 bidPrice = price;  /**  @dev FOR TEST CODE RUNNING: The following if statement in L533 should be replaced by the comment out  /  if (inAcceptingBidsPeriod(auctionID)) {  // if (false) {  (, , uint256 solidStrikePriceE4, ) = _getBondFromAuctionID(auctionID);  bidPrice = _exchangeSBT2IDOL(solidStrikePriceE4.mul(10**18));  Recommendation  Double-check the logic in these functions. If revealing should be allowed (but penalized in the earlier stage), the first check should be changed. However, based on our understanding, the first check is correct, and the second check for early reveal is redundant and should be removed.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.7 TODO tags present in the code    ", "body": "  Resolution  Comment from Lien Protocol:  All TODOs in repositories were solved and removed.  Description  There are a few instances of TODO tags in the codebase that must be addressed before production as they correspond to commented-out code that makes up essential parts of the system.  Examples  code/MainContracts/contracts/Auction.sol:L310-L311  // require(strikePriceIDOLAmount > 10**10, 'at least 100 iDOL is required for the bid Amount'); // require $100 for spam protection // TODO  require(  code/MainContracts/contracts/BondMaker.sol:L392-L394  bytes32[] storage bondIDs = bondGroup.bondIDs;  // require(msg.value.mul(998).div(1000) > amount, 'fail to transfer Ether'); // TODO  code/MainContracts/contracts/BondMaker.sol:L402-L404  _issueNewBond(bondID, msg.sender, amount);  // transferETH(bondTokenAddress, msg.value - amount); // TODO  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.8 Documented function getERC20TokenDividend() does not exist    ", "body": "  Resolution  Comment from Lien Protocol:  Fairswap Fix ReadMe (README.md)  Description  In the README of Fairswap_iDOLvsLien, a function is listed which is not implemented in the codebase:  getERC20TokenDividend() function withdraws ETH and baseToken dividends for the Lien token stored in the exchange.(the dividends are stored in the contract at this moment)  Recommendation  Implement the function, or update the documentation  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.9 Fairswap interfaces are inconsistent    ", "body": "  Resolution  Comment from Lien Protocol:  Implement interface(contracts/Inheritance/TokenExchange.sol L11, contracts/Inheritance/ETHExchange.sol L12)  Description  There are unexpected inconsistencies between the three Fairswap contract interfaces, which may cause issues for composability with external contracts.  Examples  The function used to submit orders between the base and settlement currency has a different name across the three exchanges:  In Fairswap_iDOLvsETH it is called: orderEThToToken().  In Fairswap_iDOLvsLien it is called: OrderBaseToSettlement() (capitalized).  In Fairswap_iDOLvsImmmortalOptions it is called: orderBaseToSettlement().  Recommendation  Implement the desired interface in a separate file, and inherit it on the exchange contracts to ensure they are implemented as intended.  ", "labels": ["Consensys", "Medium", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.10 Fairswap: inconsistent checks on _executionOrder()    ", "body": "  Resolution  Comment from Lien Protocol:  Fairswap- Integrate if statements about executeUnexecutedBox() (contracts/Inheritance/TokenExchange.sol L161, contracts/Inheritance/ETHExchange.sol L143, contracts/Inheritance/BoxExchange.sol L401-L405)  Description  The _executionOrder() function should only be called under specific conditions. However, these conditions are not always consistently defined.  Examples  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L218  if (nextBoxNumber > 1 && nextBoxNumber > nextExecuteBoxNumber) {  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L312  if (nextBoxNumber > 1 && nextBoxNumber > nextExecuteBoxNumber) {  code/Fairswap_iDOLvsLien/contracts/BoxExchange.sol:L647  if (nextBoxNumber > 1 && nextBoxNumber >= nextExecuteBoxNumber) {  Recommendation  Reduce duplicate code by defining an internal function to perform this check. A clear, descriptive name will help to clarify the intention.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "3.11 Inconsistency in DecimalSafeMath implementations    ", "body": "  Resolution  Comment from Lien Protocol:  Integrate and rename DecimalSafeMath to RateMath (contracts/Inheritance/RateMath.sol)  Description  There are two different implementations of DecimalSafeMath in the 3 FairSwap repositories.  Examples  FairSwap_iDOLvsLien/contracts/util/DecimalSafeMath.sol#L4-L11  library DecimalSafeMath {  function decimalDiv(uint256 a, uint256 b)internal pure returns (uint256) {  // assert(b > 0); // Solidity automatically throws when dividing by 0  uint256 a_ = a * 1000000000000000000;  uint256 c = a_ / b;  // assert(a == b * c + a % b); // There is no case in which this doesn't hold  return c;  Fairswap_iDOLvsETH/contracts/util/DecimalSafeMath.sol#L3-L11  library DecimalSafeMath {  function decimalDiv(uint256 a, uint256 b)internal pure returns (uint256) {  // assert(b > 0); // Solidity automatically throws when dividing by 0  uint256 c = (a * 1000000000000000000) / b;  // assert(a == b * c + a % b); // There is no case in which this doesn't hold  return c;  Recommendation  Try removing duplicate code/libraries and using a better inheritance model to include one file in all FairSwaps.  ", "labels": ["Consensys", "Minor", "Fixed"], "html_url": "https://consensys.io/diligence/audits/2020/05/lien-protocol/"}, {"title": "5.1 Oracle updates can be manipulated to perform atomic front-running attack    Addressed", "body": "  Resolution  The issue was mitigated by updating the Oracle price only once per block and consistently only using the old value throughout the block instead of querying the Oracle when adding or removing liquidity. Arbitrageurs can now no longer do the profitable trade within a single transaction which also precludes the possibility of using flash loans to amplify the attack.  Description  It is possible to atomically arbitrage rate changes in a risk-free way by  sandwiching  the Oracle update between two transactions. The attacker would send the following 2 transactions at the moment the Oracle update appears in the mempool:  The first transaction, which is sent with a higher gas price than the Oracle update transaction, converts a very small amount. This  locks in  the conversion weights for the block since handleExternalRateChange() only updates weights once per block. By doing this, the arbitrageur ensures that the stale Oracle price is initially used when doing the first conversion in the following transaction.  The second transaction, which is sent at a slightly lower gas price than the transaction that updates the Oracle, does the following:  Perform a large conversion at the old weight;  Add a small amount of Liquidity to trigger rebalancing;  Convert back at the new rate.  The attacker can also leverage the incentive generated by the formula by converting such that primary reserve balance == primary reserve staked balance.  The attacker can obtain liquidity for step 2 using a flash loan. The attack will deplete the reserves of the pool. An example is shown in section 5.4.  Recommendation  Do not allow users to trade at a stale Oracle rate and trigger an Oracle price update in the same transaction.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.2 Slippage and fees can be manipulated by a trader    Addressed", "body": "  Resolution  The issue was addressed by introducing an exit fee mechanism. When a liquidity provider wants to withdraw some liquidity, the smart contract returns fewer tokens if the primary reserve is not in the balanced state. So in most cases, the manipulations described in the issue should potentially be non-profitable anymore. Although, in some cases, the traders still may have some incentive to add liquidity before making the trade and remove it after to get a part of the fees (i.e., if the pool is going to be in a balanced state after the trade).  Description  Users are making trades against the liquidity pool (converter) with slippage and fees defined in the converter contract and Bancor formula. The following steps can be done to optimize trading costs:  Instead of just making a trade, a user can add a lot of liquidity (of both tokens, or only one of them) to the pool after taking a flash loan, for example.  Make the trade.  Remove the added liquidity.  Because the liquidity is increased on the first step, slippage is getting smaller for this trade. Additionally, the trader receives a part of the fees for this trade by providing liquidity.  One of the reasons why this is possible is described in another issue issue 5.3.  This technique of reducing slippage could be used by the trader to get more profit from any frontrunning/arbitrage opportunity and can help to deplete the reserves.  Example  Consider the initial state with an amplification factor of 20 and zero fees:  Here a user can make a trade with the following rate:  > Convert 9000000 TKN into 8612440 BNT.  But if the user adds 100% of the liquidity in both tokens before the trade, the slippage will be lower:  > Convert 9000000 TKN into 8801955 BNT.  Recommendation  Fixing this issue requires some modification of the algorithm.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.3 Loss of the liquidity pool is not equally distributed    Addressed", "body": "  Resolution  The issue was addressed by adding a new fee mechanism called  adjusted fees .  This mechanism aims to decrease the deficit of the reserves over time. If there is a deficit of reserves, it is usually present on the secondary token side, because there is a strong incentive to bring the primary token to the balanced state. Roughly speaking, the idea is that if the secondary token has a deficit in reserves, there are additional fees for trading that token. These fees are not distributed across the liquidity providers like the regular fees. Instead, they are just populating the reserve, decreasing the existing deficit.  Loss is still not distributed across the liquidity providers, and there is a possibility that there are not enough funds for everyone to withdraw them. In the case of a run on reserves, LPs will be able to withdraw funds on a first-come-first-serve basis.  Description  All stakeholders in the liquidity pool should be able to withdraw the same amount as they staked plus a share of fees that the converter earned during their staking period.  code/contracts/converter/LiquidityPoolV2Converter.sol:L491-L505  IPoolTokensContainer(anchor).burn(_poolToken, msg.sender, _amount);  // calculate how much liquidity to remove  // if the entire supply is liquidated, the entire staked amount should be sent, otherwise  // the price is based on the ratio between the pool token supply and the staked balance  uint256 reserveAmount = 0;  if (_amount == initialPoolSupply)  reserveAmount = balance;  else  reserveAmount = _amount.mul(balance).div(initialPoolSupply);  // sync the reserve balance / staked balance  reserves[reserveToken].balance = reserves[reserveToken].balance.sub(reserveAmount);  uint256 newStakedBalance = stakedBalances[reserveToken].sub(reserveAmount);  stakedBalances[reserveToken] = newStakedBalance;  The problem is that sometimes there might not be enough funds in reserve (for example, due to this issue https://github.com/ConsenSys/bancor-audit-2020-06/issues/4). So the first ones who withdraw their stakes receive all the tokens they own. But the last stakeholders might not be able to get their funds back because the pool is empty already.  So under some circumstances, there is a chance that users can lose all of their staked funds.  This issue also has the opposite side: if the liquidity pool makes an extra profit, the stakers do not owe this profit and cannot withdraw it.  Recommendation  Distribute losses evenly across the liquidity providers.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.4 Oracle front-running could deplete reserves over time    Addressed", "body": "  Resolution  To mitigate this issue, the Bancor team has added a mechanism that adjusts the effective weights once per block based on its internal price feed. The conversion rate re-anchors to the external oracle price once the next oracle update comes in. This mechanism should help to cause the weight rebalancing caused by the external Oracle update to be less pronounced, thereby limiting the profitability of Oracle frontrunning. It should be noted that it also adds another layer of complexity to the system. It is difficult to predict the actual effectiveness and impact of this mitigation measure without simulating the system under real-world conditions.  Description  Bancor s weight rebalancing mechanism uses Chainlink price oracles to dynamically update the weights of the assets in the pool to track the market price. Due to Oracle price updates being visible in the mempool before they are included in a block, it is always possible to know about Oracle updates in advance and attempt to make a favourable conversion which takes the future rebalancing into account, followed by the reverse conversion after the rebalancing has occurred. This can be done with high liquidity and medium risk since transaction ordering on the Ethereum blockchain is largely predictable.  Over time, this could deplete the secondary reserve as the formula compensates by rebalancing the weights such that the secondary token is sold slightly below its market rate (this is done to create an incentive to bring the primary reserve back to the amount staked by liquidity providers).  Example  Consider the initial state with an amplification factor of 20 and zero fees:  The frontrunner sees a Chainlink transaction in the mempool that changes Oracle B rate to 10,500. He sends a transaction with a slightly higher gas price than the Oracle update.  Convert 1,000,000 TKN into 999,500 BNT.  The intermediate state:  In the following block, the frontrunner sends another transaction with a high gas price (the goal is to be first to convert at the new rate set by the Oracle update):  Convert 999,500 BNT back into TKN.  The state is:  The frontrunner can now leverage the incentive created by the formula to bring back TKN reserve balance to staked TKN balance by converting TKN back to BNT:  Convert 4,994 TKN to BNT  The final state is:  The pool is now balanced and the frontrunner has gained 4,969 BNT.  Recommendation  This appears to be a fundamental problem caused by the fact that rebalancing is predictable. It is difficult to assess the actual impact of this issue without also reviewing components external to the scope of this audit (Chainlink) and extensively testing the system under real-world conditions.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.5 Use of external calls with a fixed amount of gas   ", "body": "  Resolution  It was decided to accept this minor risk as the usage of .call() might introduce other unexpected behavior.  Description  The converter smart contract uses the Solidity transfer() function to transfer Ether.  .transfer() and .send() forward exactly 2,300 gas to the recipient. The goal of this hardcoded gas stipend was to prevent reentrancy vulnerabilities, but this only makes sense under the assumption that gas costs are constant. Recently EIP 1884 was included in the Istanbul hard fork. One of the changes included in EIP 1884 is an increase to the gas cost of the SLOAD operation, causing a contract s fallback function to cost more than 2300 gas.  Examples  code/contracts/converter/ConverterBase.sol:L228  _to.transfer(address(this).balance);  code/contracts/converter/LiquidityPoolV2Converter.sol:L370  if (_targetToken == ETH_RESERVE_ADDRESS)  code/contracts/converter/LiquidityPoolV2Converter.sol:L509  msg.sender.transfer(reserveAmount);  Recommendation  It s recommended to stop using .transfer() and .send() and instead use .call(). Note that .call() does nothing to mitigate reentrancy attacks, so other precautions must be taken. To prevent reentrancy attacks, it is recommended that you use the checks-effects-interactions pattern.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.6 Use of assert statement for input validation    Addressed", "body": "  Resolution  Assertions are no longer used in the final version reviewed.  Description  Solidity assertion should only be used to assert invariants, i.e. statements that are expected to always hold if the code behaves correctly. Note that all available gas is consumed when an assert-style exception occurs.  Examples  It appears that assert() is used in one location within the test scope to catch invalid user inputs:  code/contracts/converter/LiquidityPoolV2Converter.sol:L354  assert(amount < targetReserveBalance);  Recommendation  Using require() instead of assert().  6 Bytecode Verification  Bytecode-level checking helps to ensure that the code behaves correctly for all input values. In this audit we used Mythx deep analysis to verify a small number of basic properties on the weight rebalancing and conversion functions and to detect conditions that would cause runtime exceptions. MythX uses symbolic execution and input fuzzing to explore a large amount of possible inputs and program states.  Note that the Bancor formula is compiled with solc-0.4.25 / 20,000 optimization passes.  We checked whether the following properties hold for all inputs:  [P1] Function balancedWeights: Sum of weights returned by must equal MAX_WEIGHT  [P2a] Function crossReserveTargetAmount: Output amount must not be greater than target reserve balance  [P2b] Function crossReserveTargetAmount: If reserve balances are equal and source weight < target weight, target amount must be lower than input amount  Note that balancedWeights is known to revert when (t * p) / (r * q) * log( s / t) is not in the range [-1/e, 1/e], where:  t is the primary reserve staked balance  s is the primary reserve current balance  r is the secondary reserve current balance  q is the primary reserve rate  p is the secondary reserve rate  The following preconditions were set on the input to reflect realistic input ranges. For balancedWeights:  For crossReserveTargetAmount:  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "6.1 Results", "body": "  No violations of the properties tested were found. Our tools also did not identify any cases that would cause the function to revert for the given input ranges.  ", "labels": ["Consensys"], "html_url": "https://consensys.io/diligence/audits/2020/06/bancor-v2-amm-security-audit/"}, {"title": "5.1 [Out of Scope] ReferralFeeReceiver - anyone can steal all the funds that belong to ReferralFeeReceiver    Fix Unverified", "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol#2 and the reentrancy in FeeReceiver in 1inch-exchange/1inch-liquidity-protocol@e9c6a03  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  Note: This issue was raised in components that were being affected by the scope reduction as outlined in the section  Scope  and are, therefore, only shallowly validated. Nevertheless, we find it important to communicate such potential findings and ask the client to further investigate.  The ReferralFeeReceiver receives pool shares when users swap() tokens in the pool. A ReferralFeeReceiver may be used with multiple pools and, therefore, be a lucrative target as it is holding pool shares.  Any token or ETH that belongs to the ReferralFeeReceiver is at risk and can be drained by any user by providing a custom mooniswap pool contract that references existing token holdings.  It should be noted that none of the functions in ReferralFeeReceiver verify that the user-provided mooniswap pool address was actually deployed by the linked MooniswapFactory. The factory provides certain security guarantees about mooniswap pool contracts (e.g. valid mooniswap contract, token deduplication, tokenA!=tokenB, enforced token sorting, \u2026), however, since the ReferralFeeReceiver does not verify the user-provided mooniswap address they are left unchecked.  Additional Notes  freezeEpoch - (callable by anyone) performs a pool.withdraw() with the minAmounts check being disabled. This may allow someone to call this function at a time where the contract actually gets a bad deal.  trade - (callable by anyone) can intentionally be used to perform bad trades (front-runnable)  trade - (callable by anyone) appears to implement inconsistent behavior when sending out availableBalance. ETH is sent to tx.origin (the caller) while tokens are sent to the user-provided mooniswap address.  code/contracts/ReferralFeeReceiver.sol:L91-L95  if (path[0].isETH()) {  tx.origin.transfer(availableBalance);  // solhint-disable-line avoid-tx-origin  } else {  path[0].safeTransfer(address(mooniswap), availableBalance);  multiple methods - since mooniswap is a user-provided address there are a lot of opportunities to reenter the contract. Consider adding reentrancy guards as another security layer (e.g. claimCurrentEpoch and others).  multiple methods - do not validate the amount of tokens that are returned, causing an evm assertion due to out of bounds index access.  code/contracts/ReferralFeeReceiver.sol:L57-L59  IERC20[] memory tokens = mooniswap.getTokens();  uint256 token0Balance = tokens[0].uniBalanceOf(address(this));  uint256 token1Balance = tokens[1].uniBalanceOf(address(this));  in GovernanceFeeReceiver anyone can intentionally force unwrapping of pool tokens or perform swaps in the worst time possible. e.g. The checks for withdraw(..., minAmounts) is disabled.  code/contracts/governance/GovernanceFeeReceiver.sol:L18-L26  function unwrapLPTokens(Mooniswap mooniswap) external validSpread(mooniswap) {  mooniswap.withdraw(mooniswap.balanceOf(address(this)), new uint256[](0));  function swap(IERC20[] memory path) external validPath(path) {  (uint256 amount,) = _maxAmountForSwap(path, path[0].uniBalanceOf(address(this)));  uint256 result = _swap(path, amount, payable(address(rewards)));  rewards.notifyRewardAmount(result);  Examples  Let s assume the following scenario:  ReferralFeeReceiver holds DAI token and we want to steal them.  An attacker may be able to drain the contract from DAI token via  claimFrozenToken if  they control the mooniswap address argument and provide a malicious contract  user.share[mooniswap][firstUnprocessedEpoch] > 0 - this can be arbitrarily set in updateReward  token.epochBalance[currentEpoch].token0Balance > 0 - this can be manipulated in freezeEpoch by providing a malicious mooniswap contract  they own a worthless ERC20 token e.g. named ATTK  The following steps outline the attack:  The attacker calls into updateReward to set user.share[mooniswap][currentEpoch] to a value that is greater than zero to make sure that share in claimFrozenEpoch takes the _transferTokenShare path.  code/contracts/ReferralFeeReceiver.sol:L38-L50  function updateReward(address referral, uint256 amount) external override {  Mooniswap mooniswap = Mooniswap(msg.sender);  TokenInfo storage token = tokenInfo[mooniswap];  UserInfo storage user = userInfo[referral];  uint256 currentEpoch = token.currentEpoch;  // Add new reward to current epoch  user.share[mooniswap][currentEpoch] = user.share[mooniswap][currentEpoch].add(amount);  token.epochBalance[currentEpoch].totalSupply = token.epochBalance[currentEpoch].totalSupply.add(amount);  // Collect all processed epochs and advance user token epoch  _collectProcessedEpochs(user, token, mooniswap, currentEpoch);  The attacker then calls freezeEpoch() providing the malicious mooniswap contract address controlled by the attacker.  The malicious contract returns token that is controlled by the attacker (e.g. ATTK) in a call to mooniswap.getTokens();  The contract then stores the current balance of the attacker-controlled token in token0Balance/token1Balance. Note that the token being returned here by the malicious contract can be different from the one we re checking out in the last step (balance manipulation via ATTK, checkout of DAI in the last step).  Then the contract calls out to the malicious mooniswap contract. This gives the malicious contract an easy opportunity to send some attacker-controlled token (ATTK) to the ReferralFeeReceiver in order to freely manipulate the frozen tokenbalances (tokens[0].uniBalanceOf(address(this)).sub(token0Balance);).  Note that the used token addresses are never stored anywhere. The balances recorded here are for an attacker-controlled token (ATTK), not the actual one that we re about to steal (e.g. DAI)  The token balances are now set-up for checkout in the last step (claimFrozenEpoch).  code/contracts/ReferralFeeReceiver.sol:L52-L64  function freezeEpoch(Mooniswap mooniswap) external validSpread(mooniswap) {  TokenInfo storage token = tokenInfo[mooniswap];  uint256 currentEpoch = token.currentEpoch;  require(token.firstUnprocessedEpoch == currentEpoch, \"Previous epoch is not finalized\");  IERC20[] memory tokens = mooniswap.getTokens();  uint256 token0Balance = tokens[0].uniBalanceOf(address(this));  uint256 token1Balance = tokens[1].uniBalanceOf(address(this));  mooniswap.withdraw(mooniswap.balanceOf(address(this)), new uint256[](0));  token.epochBalance[currentEpoch].token0Balance = tokens[0].uniBalanceOf(address(this)).sub(token0Balance);  token.epochBalance[currentEpoch].token1Balance = tokens[1].uniBalanceOf(address(this)).sub(token1Balance);  token.currentEpoch = currentEpoch.add(1);  A call to claimFrozenEpoch checks-out the previously frozen token balance.  The claim > 0 requirement was fulfilled in step 1.  The token balance was prepared for the attacker-controlled token (ATTK) in step 2, but we re now checking out DAI.  When the contract calls out to the attackers mooniswap contract the call to IERC20[] memory tokens = mooniswap.getTokens(); returns the address of the token to be stolen (e.g. DAI) instead of the attacker-controlled token (ATTK) that was used to set-up the balance records.  Subsequently, the valuable target tokens (DAI) are sent out to the caller in _transferTokenShare.  code/contracts/ReferralFeeReceiver.sol:L153-L162  if (share > 0) {  EpochBalance storage epochBalance = token.epochBalance[firstUnprocessedEpoch];  uint256 totalSupply = epochBalance.totalSupply;  user.share[mooniswap][firstUnprocessedEpoch] = 0;  epochBalance.totalSupply = totalSupply.sub(share);  IERC20[] memory tokens = mooniswap.getTokens();  epochBalance.token0Balance = _transferTokenShare(tokens[0], epochBalance.token0Balance, share, totalSupply);  epochBalance.token1Balance = _transferTokenShare(tokens[1], epochBalance.token1Balance, share, totalSupply);  epochBalance.inchBalance = _transferTokenShare(inchToken, epochBalance.inchBalance, share, totalSupply);  Recommendation  Enforce that the user-provided mooniswap contract was actually deployed by the linked factory. Other contracts cannot be trusted. Consider implementing token sorting and de-duplication (tokenA!=tokenB) in the pool contract constructor as well. Consider employing a reentrancy guard to safeguard the contract from reentrancy attacks.  Improve testing. The methods mentioned here are not covered at all. Improve documentation and provide a specification that outlines how this contract is supposed to be used.  Review the  additional notes  provided with this issue.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.2 GovernanceMothership - notifyFor allows to arbitrarily create new or override other users stake in governance modules    Fix Unverified", "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol@2ce549d and added tests with 1inch-exchange/1inch-liquidity-protocol@e0dc46b  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  The notify* methods are called to update linked governance modules when an accounts stake changes in the Mothership. The linked modules then update their own balances of the user to accurately reflect the account s real stake in the Mothership.  Besides notify there s also a method named notifyFor which is publicly accessible. It is assumed that the method should be used similar to notify to force an update for another account s balance.  However, invoking the method forces an update in the linked modules for the provided address, but takes balanceOf(msg.sender) instead of balanceOf(account). This allows malicious actors to:  Arbitrarily change other accounts stake in linked governance modules (e.g. zeroing stake, increasing stake) based on the callers stake in the mothership  Duplicate stake out of thin air to arbitrary addresses (e.g. staking in mothership once and calling notifyFor many other account addresses)  Examples  publicly accessible method allows forcing stake updates for arbitrary users  code/contracts/inch/GovernanceMothership.sol:L48-L50  function notifyFor(address account) external {  _notifyFor(account, balanceOf(msg.sender));  the method calls the linked governance modules  code/contracts/inch/GovernanceMothership.sol:L73-L78  function _notifyFor(address account, uint256 balance) private {  uint256 modulesLength = _modules.length();  for (uint256 i = 0; i < modulesLength; ++i) {  IGovernanceModule(_modules.at(i)).notifyStakeChanged(account, balance);  which will arbitrarily mint or burn stake in the BalanceAccounting of Factory or Reward (or other linked governance modules)  code/contracts/governance/BaseGovernanceModule.sol:L29-L31  function notifyStakeChanged(address account, uint256 newBalance) external override onlyMothership {  _notifyStakeChanged(account, newBalance);  code/contracts/governance/MooniswapFactoryGovernance.sol:L144-L160  function _notifyStakeChanged(address account, uint256 newBalance) internal override {  uint256 balance = balanceOf(account);  if (newBalance > balance) {  _mint(account, newBalance.sub(balance));  } else if (newBalance < balance) {  _burn(account, balance.sub(newBalance));  } else {  return;  uint256 newTotalSupply = totalSupply();  _defaultFee.updateBalance(account, _defaultFee.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_FEE, _emitDefaultFeeVoteUpdate);  _defaultSlippageFee.updateBalance(account, _defaultSlippageFee.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_SLIPPAGE_FEE, _emitDefaultSlippageFeeVoteUpdate);  _defaultDecayPeriod.updateBalance(account, _defaultDecayPeriod.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_DECAY_PERIOD, _emitDefaultDecayPeriodVoteUpdate);  _referralShare.updateBalance(account, _referralShare.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_REFERRAL_SHARE, _emitReferralShareVoteUpdate);  _governanceShare.updateBalance(account, _governanceShare.votes[account], balance, newBalance, newTotalSupply, _DEFAULT_GOVERNANCE_SHARE, _emitGovernanceShareVoteUpdate);  code/contracts/governance/GovernanceRewards.sol:L72-L79  function _notifyStakeChanged(address account, uint256 newBalance) internal override updateReward(account) {  uint256 balance = balanceOf(account);  if (newBalance > balance) {  _mint(account, newBalance.sub(balance));  } else if (newBalance < balance) {  _burn(account, balance.sub(newBalance));  Recommendation  Remove notifyFor or change it to take the balance of the correct account _notifyFor(account, balanceOf(msg.sender)).  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.3 Users can  increase  their voting power by voting for the max/min values ", "body": "  Description  Many parameters in the system are determined by the complicated governance mechanism. These parameters are calculated as a result of the voting process and are equal to the weighted average of all the votes that stakeholders make. The idea is that every user is voting for the desired value. But if the result value is smaller (larger) than the desired, the user can change the vote for the max (min) possible value. That would shift the result towards the desired one and basically  increase  this stakeholder s voting power. So every user is more incentivized to vote for the min/max value than for the desired one.  The issue s severity is not high because all parameters have reasonable max value limitations, so it s hard to manipulate the system too much.  Recommendation  Reconsider the voting mechanism.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.4 The uniTransferFrom function can potentially be used with invalid params    Fix Unverified", "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol@d0ffb6f.  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  The system is using the UniERC20 contract to incapsulate transfers of both ERC-20 tokens and ETH. This contract has uniTransferFrom function that can be used for any ERC-20 or ETH:  code/contracts/libraries/UniERC20.sol:L36-L48  function uniTransferFrom(IERC20 token, address payable from, address to, uint256 amount) internal {  if (amount > 0) {  if (isETH(token)) {  require(msg.value >= amount, \"UniERC20: not enough value\");  if (msg.value > amount) {  // Return remainder if exist  from.transfer(msg.value.sub(amount));  } else {  token.safeTransferFrom(from, to, amount);  This issue s severity is not high because the function is always called with the proper parameters in the current codebase.  Recommendation  Make sure that the uniTransferFrom function is always called with expected parameters.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.5 MooniswapGovernance - votingpower is not accurately reflected when minting pool tokens    Fix Unverified", "body": "  Resolution  According to the client, this issue is addressed in 1inch-exchange/1inch-liquidity-protocol@eb869fd  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  When a user provides liquidity to the pool, pool-tokens are minted. The minting event triggers the _beforeTokenTransfer callback in MooniswapGovernance which updates voting power reflecting the newly minted stake for the user.  There seems to be a copy-paste error in the way balanceTo is determined that sets balanceTo to zero if new token were minted (from==address(0)). This means, that in a later call to _updateOnTransfer only the newly minted amount is considered when adjusting voting power.  Examples  If tokens are newly minted from==address(0) and therefore balanceTo -> 0.  code/contracts/governance/MooniswapGovernance.sol:L100-L114  function _beforeTokenTransfer(address from, address to, uint256 amount) internal override {  uint256 balanceFrom = (from != address(0)) ? balanceOf(from) : 0;  uint256 balanceTo = (from != address(0)) ? balanceOf(to) : 0;  uint256 newTotalSupply = totalSupply()  .add(from == address(0) ? amount : 0)  .sub(to == address(0) ? amount : 0);  ParamsHelper memory params = ParamsHelper({  from: from,  to: to,  amount: amount,  balanceFrom: balanceFrom,  balanceTo: balanceTo,  newTotalSupply: newTotalSupply  });  now, balanceTo is zero which would adjust voting power to amount instead of the user s actual balance + the newly minted token.  code/contracts/governance/MooniswapGovernance.sol:L150-L153  if (params.to != address(0)) {  votingData.updateBalance(params.to, voteTo, params.balanceTo, params.balanceTo.add(params.amount), params.newTotalSupply, defaultValue, emitEvent);  Recommendation  balanceTo should be zero when burning (to == address(0)) and balanceOf(to) when minting.  e.g. like this:  uint256 balanceTo = (to != address(0)) ? balanceOf(to) : 0;  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.6 MooniswapGovernance - _beforeTokenTransfer should not update voting power on transfers to self    Fix Unverified", "body": "  Resolution  Addressed 1inch-exchange/1inch-liquidity-protocol@7c7126d  (This fix is as reported by the developer team, but has not been verified by Diligence).  Description  Mooniswap governance is based on the liquidity voting system that is also employed by the mothership or for factory governance. In contrast to traditional voting systems where users vote for discrete values, the liquidity voting system derives a continuous weighted averaged  consensus  value from all the votes. Thus it is required that whenever stake changes in the system, all the parameters that can be voted upon are updated with the new weights for a specific user.  The Mooniswap pool is governed by liquidity providers and liquidity tokens are the stake that gives voting rights in MooniswapGovernance. Thus whenever liquidity tokens are transferred to another address, stake and voting values need to be updated. This is handled by MooniswapGovernance._beforeTokenTransfer().  In the special case where someone triggers a token transfer where the from address equals the to address, effectively sending the token to themselves, no update on voting power should be performed. Instead, voting power is first updated with balance - amount and then with balance + amount which in the worst case means it is updating first to a zero balance and then to 2x the balance.  Ultimately this should not have an effect on the overall outcome but is unnecessary and wasting gas.  Examples  beforeTokenTransfer callback in Mooniswap does not check for the NOP case where from==to  code/contracts/governance/MooniswapGovernance.sol:L100-L119  function _beforeTokenTransfer(address from, address to, uint256 amount) internal override {  uint256 balanceFrom = (from != address(0)) ? balanceOf(from) : 0;  uint256 balanceTo = (from != address(0)) ? balanceOf(to) : 0;  uint256 newTotalSupply = totalSupply()  .add(from == address(0) ? amount : 0)  .sub(to == address(0) ? amount : 0);  ParamsHelper memory params = ParamsHelper({  from: from,  to: to,  amount: amount,  balanceFrom: balanceFrom,  balanceTo: balanceTo,  newTotalSupply: newTotalSupply  });  _updateOnTransfer(params, mooniswapFactoryGovernance.defaultFee, _emitFeeVoteUpdate, _fee);  _updateOnTransfer(params, mooniswapFactoryGovernance.defaultSlippageFee, _emitSlippageFeeVoteUpdate, _slippageFee);  _updateOnTransfer(params, mooniswapFactoryGovernance.defaultDecayPeriod, _emitDecayPeriodVoteUpdate, _decayPeriod);  which leads to updateBalance being called on the same address twice, first with currentBalance - amountTransferred and then with currentBalance + amountTransferred.  code/contracts/governance/MooniswapGovernance.sol:L147-L153  if (params.from != address(0)) {  votingData.updateBalance(params.from, voteFrom, params.balanceFrom, params.balanceFrom.sub(params.amount), params.newTotalSupply, defaultValue, emitEvent);  if (params.to != address(0)) {  votingData.updateBalance(params.to, voteTo, params.balanceTo, params.balanceTo.add(params.amount), params.newTotalSupply, defaultValue, emitEvent);  Recommendation  Do not update voting power on LP token transfers where from == to.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.7 Unpredictable behavior for users due to admin front running or general bad timing ", "body": "  Description  In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.  Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.  In general users of the system should have assurances about the behavior of the action they re about to take.  Examples  MooniswapFactoryGovernance - Admin opportunity to lock swapFor with a referral when setting an invalid referralFeeReceiver  setReferralFeeReceiver and setGovernanceFeeReceiver takes effect immediately.  code/contracts/governance/MooniswapFactoryGovernance.sol:L92-L95  function setReferralFeeReceiver(address newReferralFeeReceiver) external onlyOwner {  referralFeeReceiver = newReferralFeeReceiver;  emit ReferralFeeReceiverUpdate(newReferralFeeReceiver);  setReferralFeeReceiver can be used to set an invalid receiver address (or one that reverts on every call) effectively rendering Mooniswap.swapFor unusable if a referral was specified in the swap.  code/contracts/Mooniswap.sol:L281-L286  if (referral != address(0)) {  referralShare = invIncrease.mul(referralShare).div(_FEE_DENOMINATOR);  if (referralShare > 0) {  if (referralFeeReceiver != address(0)) {  _mint(referralFeeReceiver, referralShare);  IReferralFeeReceiver(referralFeeReceiver).updateReward(referral, referralShare);  Locking staked token  At any point in time and without prior notice to users an admin may accidentally or intentionally add a broken governance sub-module to the system that blocks all users from unstaking their 1INCH token. An admin can recover from this by removing the broken sub-module, however, with malicious intent tokens may be locked forever.  Since 1INCH token gives voting power in the system, tokens are considered to hold value for other users and may be traded on exchanges. This raises concerns if tokens can be locked in a contract by one actor.  An admin adds an invalid address or a malicious sub-module to the governance contract that always reverts on calls to notifyStakeChanged.  code/contracts/inch/GovernanceMothership.sol:L63-L66  function addModule(address module) external onlyOwner {  require(_modules.add(module), \"Module already registered\");  emit AddModule(module);  code/contracts/inch/GovernanceMothership.sol:L73-L78  function _notifyFor(address account, uint256 balance) private {  uint256 modulesLength = _modules.length();  for (uint256 i = 0; i < modulesLength; ++i) {  IGovernanceModule(_modules.at(i)).notifyStakeChanged(account, balance);  Admin front-running to prevent user stake sync  An admin may front-run users while staking in an attempt to prevent submodules from being notified of the stake update. This is unlikely to happen as it incurs costs for the attacker (front-back-running) to normal users but may be an interesting attack scenario to exclude a whale s stake from voting.  For example, an admin may front-run stake() or notoify*() by briefly removing all governance submodules from the mothership and re-adding them after the users call succeeded. The stake-update will not be propagated to the sub-modules. A user may only detect this when they are voting (if they had no stake before) or when they actually check their stake. Such an attack might likely stay unnoticed unless someone listens for addmodule removemodule events on the contract.  An admin front-runs a transaction by removing all modules and re-adding them afterwards to prevent the stake from propagating to the submodules.  code/contracts/inch/GovernanceMothership.sol:L68-L71  function removeModule(address module) external onlyOwner {  require(_modules.remove(module), \"Module was not registered\");  emit RemoveModule(module);  Admin front-running to prevent unstake from propagating  An admin may choose to front-run their own unstake(), temporarily removing all governance sub-modules, preventing unstake() from syncing the action to sub-modules while still getting their previously staked tokens out. The governance sub-modules can be re-added right after unstaking. Due to double-accounting of the stake (in governance and in every sub-module) their stake will still be exercisable in the sub-module even though it was removed from the mothership. Users can only prevent this by manually calling a state-sync on the affected account(s).  Recommendation  The underlying issue is that users of the system can t be sure what the behavior of a function call will be, and this is because the behavior can change at any time.  We recommend giving the user advance notice of changes with a time lock. For example, make all system-parameter and upgrades require two steps with a mandatory time window between them. The first step merely broadcasts to users that a particular change is coming, and the second step commits that change after a suitable waiting period. This allows users that do not accept the change to withdraw immediately.  Furthermore, users should be guaranteed to be able to redeem their staked tokens. An entity - even though trusted - in the system should not be able to lock tokens indefinitely.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.8 The owner can borrow token0/token1 in the rescueFunds ", "body": "  Description  If some random tokens/funds are accidentally transferred to the pool, the owner can call the rescueFunds function to withdraw any funds manually:  code/contracts/Mooniswap.sol:L331-L340  function rescueFunds(IERC20 token, uint256 amount) external nonReentrant onlyOwner {  uint256 balance0 = token0.uniBalanceOf(address(this));  uint256 balance1 = token1.uniBalanceOf(address(this));  token.uniTransfer(msg.sender, amount);  require(token0.uniBalanceOf(address(this)) >= balance0, \"Mooniswap: access denied\");  require(token1.uniBalanceOf(address(this)) >= balance1, \"Mooniswap: access denied\");  require(balanceOf(address(this)) >= _BASE_SUPPLY, \"Mooniswap: access denied\");  There s no restriction on which funds the owner can try to withdraw and which token to call. It s theoretically possible to transfer pool tokens and then return them to the contract (e.g. in the case of ERC-777). That action would be similar to a free flash loan.  Recommendation  Explicitly check that the token is not equal to any of the pool tokens.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2020/12/1inch-liquidity-protocol/"}, {"title": "5.1 The virtual price may not correspond to the actual price in the pool ", "body": "  Description  A Curve pool has a function that returns a  virtual price  of the LP token; this price is resistant to flash-loan attacks and any manipulations in the Curve pool. While this price formula works well in some cases, there may be a significant period when a trade cannot be executed with this price. So the deposit or withdrawal will also be done under another price and will have a different result than the one estimated under the  virtual price .  When depositing into Curve, Brahma is doing it in 2 steps. First, when depositing the user s ETH to the Vault, the user s share is calculated according to the  virtual price . And then, in a different transaction, the funds are deposited into the Curve pool. These funds only consist of ETH, and if the deposit price does not correspond (with 0.3% slippage) to the virtual price, it will revert.  So we have multiple problems here:  If the chosen slippage parameter is very low, the funds will not be deposited/withdrawn for a long time due to reverts.  If the slippage is large enough, the attacker can manipulate the price to steal the slippage. Additionally, because of the 2-steps deposit, the amount of Vault s share minted to the users may not correspond to the LP tokens minted during the second step.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.2 ConvexPositionHandler._claimRewards incorrectly calculates amount of LP tokens to unstake ", "body": "  Description  ConvexPositionHandler._claimRewards is an internal function that harvests Convex reward tokens and takes the generated yield in ETH out of the Curve pool by calculating the difference in LP token price. To do so, it receives the current share price of the curve LP tokens and compares it to the last one stored in the contract during the last rewards claim. The difference in share price is then multiplied by the LP token balance to get the ETH yield via the yieldEarned variable:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L293-L300  uint256 currentSharePrice = ethStEthPool.get_virtual_price();  if (currentSharePrice > prevSharePrice) {  // claim any gain on lp token yields  uint256 contractLpTokenBalance = lpToken.balanceOf(address(this));  uint256 totalLpBalance = contractLpTokenBalance +  baseRewardPool.balanceOf(address(this));  uint256 yieldEarned = (currentSharePrice - prevSharePrice) *  totalLpBalance;  However, to receive this ETH yield, LP tokens need to be unstaked from the Convex pool and then converted via the Curve pool. To do this, the contract introduces lpTokenEarned:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L302  uint256 lpTokenEarned = yieldEarned / NORMALIZATION_FACTOR; // 18 decimal from virtual price  This calculation is incorrect. It uses yieldEarned which is denominated in ETH and simply divides it by the normalization factor to get the correct number of decimals, which still returns back an amount denominated in ETH, whereas an amount denominated in LP tokens should be returned instead.  This could lead to significant accounting issues including losses in the  no-loss  parts of the vault s strategy as 1 LP token is almost always guaranteed to be worth more than 1 ETH. So, when the intention is to withdraw X ETH worth of an LP token, withdrawing X LP tokens will actually withdraw Y ETH worth of an LP token, where Y>X. As a result, less than expected ETH will remain in the Convex handler part of the vault, and the ETH yield will go to the Lyra options, which are much riskier. In the event Lyra options don t work out and there is more ETH withdrawn than expected, there is a possibility that this would result in a loss for the vault.  Recommendation  The fix is straightforward and that is to calculate lpTokenEarned using the currentSharePrice already received from the Curve pool. That way, it is the amount of LP tokens that will be sent to be unwrapped and unstaked from the Convex and Curve pools. This will also take care of the normalization factor.  uint256 lpTokenEarned = yieldEarned / currentSharePrice;  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.3 The WETH tokens are not taken into account in the ConvexTradeExecutor.totalFunds function ", "body": "  Description  The totalFunds function of every executor should include all the funds that belong to the contract:  code/contracts/ConvexTradeExecutor.sol:L21-L23  function totalFunds() public view override returns (uint256, uint256) {  return ConvexPositionHandler.positionInWantToken();  The ConvexTradeExecutor uses this function for calculations:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137  function positionInWantToken()  public  view  override  returns (uint256, uint256)  uint256 stakedLpBalanceInETH,  uint256 lpBalanceInETH,  uint256 ethBalance  ) = _getTotalBalancesInETH(true);  return (  stakedLpBalanceInETH + lpBalanceInETH + ethBalance,  block.number  );  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L337-L365  function _getTotalBalancesInETH(bool useVirtualPrice)  internal  view  returns (  uint256 stakedLpBalance,  uint256 lpTokenBalance,  uint256 ethBalance  uint256 stakedLpBalanceRaw = baseRewardPool.balanceOf(address(this));  uint256 lpTokenBalanceRaw = lpToken.balanceOf(address(this));  uint256 totalLpBalance = stakedLpBalanceRaw + lpTokenBalanceRaw;  // Here, in order to prevent price manipulation attacks via curve pools,  // When getting total position value -> its calculated based on virtual price  // During withdrawal -> calc_withdraw_one_coin() is used to get an actual estimate of ETH received if we were to remove liquidity  // The following checks account for this  uint256 totalLpBalanceInETH = useVirtualPrice  ? _lpTokenValueInETHFromVirtualPrice(totalLpBalance)  : _lpTokenValueInETH(totalLpBalance);  lpTokenBalance = useVirtualPrice  ? _lpTokenValueInETHFromVirtualPrice(lpTokenBalanceRaw)  : _lpTokenValueInETH(lpTokenBalanceRaw);  stakedLpBalance = totalLpBalanceInETH - lpTokenBalance;  ethBalance = address(this).balance;  This function includes ETH balance, LP balance, and staked balance. But WETH balance is not included here. WETH tokens are initially transferred to the contract, and before the withdrawal, the contract also stores WETH.  Recommendation  Include WETH balance into the totalFunds.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.4 LyraPositionHandlerL2 inaccurate modifier onlyAuthorized may lead to funds loss if keeper is compromised ", "body": "  Description  The LyraPositionHandlerL2 contract is operated either by the L2 keeper or  by the L1 LyraPositionHandler via the L2CrossDomainMessenger. This is implemented through the onlyAuthorized modifier:  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L187-L195  modifier onlyAuthorized() {  require(  ((msg.sender == L2CrossDomainMessenger &&  OptimismL2Wrapper.messageSender() == positionHandlerL1) ||  msg.sender == keeper),  \"ONLY_AUTHORIZED\"  );  _;  This is set on:  withdraw()  openPosition()  closePosition()  setSlippage()  deposit()  sweep()  setSocketRegistry()  setKeeper()  Functions 1-3 have a corresponding implementation on the L1 LyraPositionHandler, so they could indeed be called by it with the right parameters. However, 4-8 do not have an implemented way to call them from L1, and this modifier creates an unnecessarily expanded list of authorised entities that can call them.  Additionally, even if their implementation is provided, it needs to be done carefully because msg.sender in their case is going to end up being the L2CrossDomainMessenger. For example, the sweep() function sends any specified token to msg.sender, with the intention likely being that the recipient is under the team s or the governance s control   yet, it will be L2CrossDomainMessenger and the tokens will likely be lost forever instead.  On the other hand, the setKeeper() function would need a way to be called by something other than the keeper because it is intended to change the keeper itself. In the event that the access to the L2 keeper is compromised, and the L1 LyraPositionHandler has no way to call setKeeper() on the LyraPositionHandlerL2, the whole contract and its funds will be compromised as well. So, there needs to be some way to at least call the setKeeper() by something other than the keeper to ensure security of the funds on L2.  Examples  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L153-L184  function closePosition(bool toSettle) public override onlyAuthorized {  LyraController._closePosition(toSettle);  UniswapV3Controller._estimateAndSwap(  false,  LyraController.sUSD.balanceOf(address(this))  );  /*///////////////////////////////////////////////////////////////  MAINTAINANCE FUNCTIONS  //////////////////////////////////////////////////////////////*/  /// @notice Sweep tokens  /// @param _token Address of the token to sweepr  function sweep(address _token) public override onlyAuthorized {  IERC20(_token).transfer(  msg.sender,  IERC20(_token).balanceOf(address(this))  );  /// @notice socket registry setter  /// @param _socketRegistry new address of socket registry  function setSocketRegistry(address _socketRegistry) public onlyAuthorized {  socketRegistry = _socketRegistry;  /// @notice keeper setter  /// @param _keeper new keeper address  function setKeeper(address _keeper) public onlyAuthorized {  keeper = _keeper;  Recommendation  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.5 Harvester.harvest swaps have no slippage parameters ", "body": "  Description  As part of the vault strategy, all reward tokens for staking in the Convex ETH-stETH pool are claimed and swapped into ETH. The swaps for these tokens are done with no slippage at the moment, i.e. the expected output amount for all of them is given as 0.  In particular, one reward token that is most susceptible to slippage is LDO, and its swap is implemented through the Uniswap router:  code/contracts/ConvexExecutor/Harvester.sol:L142-L155  function _swapLidoForWETH(uint256 amountToSwap) internal {  IUniswapSwapRouter.ExactInputSingleParams  memory params = IUniswapSwapRouter.ExactInputSingleParams({  tokenIn: address(ldo),  tokenOut: address(weth),  fee: UNISWAP_FEE,  recipient: address(this),  deadline: block.timestamp,  amountIn: amountToSwap,  amountOutMinimum: 0,  sqrtPriceLimitX96: 0  });  uniswapRouter.exactInputSingle(params);  The swap is called with amountOutMinimum: 0, meaning that there is no slippage protection in this swap. This could result in a significant loss of yield from this reward as MEV bots could  sandwich  this swap by manipulating the price before this transaction and immediately reversing their action after the transaction, profiting at the expense of our swap. Moreover, the Uniswap pools seem to have low liquidity for the LDO token as opposed to Balancer or Sushiswap, further magnifying slippage issues and susceptibility to frontrunning.  The other two tokens - CVX and CRV - are being swapped through their Curve pools, which have higher liquidity and are less susceptible to slippage. Nonetheless, MEV strategies have been getting more advanced and calling these swaps with 0 as expected output may place these transactions in danger of being frontrun and  sandwiched  as well.  code/contracts/ConvexExecutor/Harvester.sol:L120-L126  if (cvxBalance > 0) {  cvxeth.exchange(1, 0, cvxBalance, 0, false);  // swap CRV to WETH  if (crvBalance > 0) {  crveth.exchange(1, 0, crvBalance, 0, false);  In these calls .exchange , the last 0 is the min_dy argument in the Curve pools swap functions that represents the minimum expected amount of tokens received after the swap, which is 0 in our case.  Recommendation  Introduce some slippage parameters into the swaps.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.6 Harvester.rewardTokens doesn t account for LDO tokens ", "body": "  Description  As part of the vault s strategy, the reward tokens for participating in Curve s ETH-stETH pool and Convex staking are claimed and swapped for ETH. This is done by having the ConvexPositionHandler contract call the reward claims API from Convex via baseRewardPool.getReward(), which transfers the reward tokens to the handler s address. Then, the tokens are iterated through and sent to the harvester to be swapped from ConvexPositionHandler by getting their list from harvester.rewardTokens() and calling harvester.harvest()  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L274-L290  // get list of tokens to transfer to harvester  address[] memory rewardTokens = harvester.rewardTokens();  //transfer them  uint256 balance;  for (uint256 i = 0; i < rewardTokens.length; i++) {  balance = IERC20(rewardTokens[i]).balanceOf(address(this));  if (balance > 0) {  IERC20(rewardTokens[i]).safeTransfer(  address(harvester),  balance  );  // convert all rewards to WETH  harvester.harvest();  However, harvester.rewardTokens() doesn t have the LDO token s address in its list, so they will not be transferred to the harvester to be swapped.  code/contracts/ConvexExecutor/Harvester.sol:L77-L82  function rewardTokens() external pure override returns (address[] memory) {  address[] memory rewards = new address[](2);  rewards[0] = address(crv);  rewards[1] = address(cvx);  return rewards;  As a result, harvester.harvest() will not be able to execute its _swapLidoForWETH() function since its ldoBalance will be 0. This results in missed rewards and therefore yield for the vault as part of its normal flow.  There is a possible mitigation in the current state of the contract that would require governance to call sweep() on the LDO balance from the BaseTradeExecutor contract (that ConvexPositionHandler inherits) and then transferring those LDO tokens to the harvester contract to perform the swap at a later rewards claim. This, however, requires transactions separate from the intended flow of the system as well as governance intervention.  Recommendation  Add the LDO token address to the rewardTokens() function by adding the following line rewards[2] = address(ldo);  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.7 Keeper design complexity ", "body": "  Description  The current design of the protocol relies on the keeper being operated correctly in a complex manner. Since the offchain code for the keeper wasn t in scope of this audit, the following is a commentary on the complexity of the keeper operations in the context of the contracts. Keeper logic such as the order of operations and function argument parameters with log querying are some examples where if the keeper doesn t execute them correctly, there may be inconsistencies and issues with accounting of vault shares and vault funds resulting in unexpected behaviour. While it may represent little risk or issues to the current Brahma-fi team as the vault is recently live, the keeper logic and exact steps should be well documented so that public keepers (if and when they are enabled) can execute the logic securely and future iterations of the vault code can account for any intricacies of the keeper logic.  Examples  1. Order of operations: Convex rewards & new depositors profiting at the expense of old depositors  yielded reward tokens. As part of the vault s strategy, the depositors  ETH is provided to Curve and the LP tokens are staked in Convex, which yield rewards such as CRV, CVX, and LDO tokens. As new depositors provide their ETH, the vault shares minted for their deposits will be less compared to old deposits as they account for the increasing value of LP tokens staked in these pools. In other words, if the first depositor provides 1 ETH, then when a new depositor provides 1 ETH much later, the new depositor will get less shares back as the totalVaultFunds() will increase:  code/contracts/Vault.sol:L97-L99  shares = totalSupply() > 0  ? (totalSupply() * amountIn) / totalVaultFunds()  : amountIn;  code/contracts/Vault.sol:L127-L130  function totalVaultFunds() public view returns (uint256) {  return  IERC20(wantToken).balanceOf(address(this)) + totalExecutorFunds();  code/contracts/ConvexTradeExecutor.sol:L21-L23  function totalFunds() public view override returns (uint256, uint256) {  return ConvexPositionHandler.positionInWantToken();  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137  function positionInWantToken()  public  view  override  returns (uint256, uint256)  uint256 stakedLpBalanceInETH,  uint256 lpBalanceInETH,  uint256 ethBalance  ) = _getTotalBalancesInETH(true);  return (  stakedLpBalanceInETH + lpBalanceInETH + ethBalance,  block.number  );  However, this does not account for the reward tokens yielded throughout that time. From the smart contract logic alone, there is no requirement to first execute the reward token harvest. It is up to the keeper to execute ConvexTradeExecutor.claimRewards in order to claim and swap their rewards into ETH, which only then will be included into the yield in the above ConvexPositionHandler.positionInWantToken function. If this is not done prior to processing new deposits and minting new shares, new depositors would unfairly benefit from the reward tokens  yield that was generated before they deposited but accounted for in the vault funds only after they deposited.  2. Order of operations: closing Lyra options before processing new deposits.  The other part of the vault s strategy is utilising the yield from Convex to purchase options from Lyra on Optimism. While Lyra options are risky and can become worthless in the event of bad trades, only yield is used for them, therefore keeping user deposits  initial value safe. However, their value could also yield significant returns, increasing the overall funds of the vault. Just as with ConvexTradeExecutor, LyraTradeExecutor also has a totalFunds() function that feeds into the vault s totalVaultFunds() function. In Lyra s case, however, it is a manually set value by the keeper that is supposed to represent the value of Lyra L2 options:  code/contracts/LyraTradeExecutor.sol:L42-L53  function totalFunds()  public  view  override  returns (uint256 posValue, uint256 lastUpdatedBlock)  return (  positionInWantToken.posValue +  IERC20(vaultWantToken()).balanceOf(address(this)),  positionInWantToken.lastUpdatedBlock  );  code/contracts/LyraTradeExecutor.sol:L61-L63  function setPosValue(uint256 _posValue) public onlyKeeper {  LyraPositionHandler._setPosValue(_posValue);  code/contracts/LyraExecutor/LyraPositionHandler.sol:L218-L221  function _setPosValue(uint256 _posValue) internal {  positionInWantToken.posValue = _posValue;  positionInWantToken.lastUpdatedBlock = block.number;  Solely from the smart contract logic, there is a possibility that a user deposits when Lyra options are valued high, meaning the total vault funds are high as well, thus decreasing the amount of shares the user would have received if it weren t for the Lyra options  value. Consequently, if after the deposit the Lyra options become worthless, decreasing the total vault funds, the user s newly minted shares will now represent less than what they have deposited.  While this is not currently mitigated by smart contract logic, it may be worked around by the keeper first settling and closing all Lyra options and transferring all their yielded value in ETH, if any, to the Convex trade executor. Only then the keeper would process new deposits and mint new shares. This order of operations is critical to maintain the vault s intended safe strategy of maintaining the user s deposited value, and is dependent entirely on the keeper offchain logic.  Recommendation  Document the exact order of operations, steps, necessary logs and parameters that keepers need to keep track of in order for the vault strategy to succeed.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.8 Vault.deposit - Possible front running attack ", "body": "  Description  To determine the number of shares to mint to a depositor, (totalSupply() * amountIn) / totalVaultFunds() is used. Potential attackers can spot a call to Vault.deposit and front-run it with a transaction that sends tokens to the contract, causing the victim to receive fewer shares than what he expected.  In case totalVaultFunds() is greater than totalSupply() * amountIn, then the number of shares the depositor receives will be 0, although amountIn of tokens will be still pulled from the depositor s balance.  An attacker with access to enough liquidity and to the mem-pool data can spot a call to Vault.deposit(amountIn, receiver) and front-run it by sending at least totalSupplyBefore * (amountIn - 1) + 1 tokens to the contract . This way, the victim will get 0 shares, but amountIn will still be pulled from its account balance. Now the price for a share is inflated, and all shareholders can redeem this profit using  Vault.withdraw.  Recommendation  The specific case that s mentioned in the last paragraph can be mitigated by adding a validation check to Vault.Deposit enforcing that shares > 0. However, it will not solve the general case since the victim can still lose value due to rounding errors. In order to fix that, Vault.Deposit should validate that shares >= amountMin where amountMin is an argument that should be determined by the depositor off-chain.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.9 Approving MAX_UINT amount of ERC20 tokens ", "body": "  Description  Approving the maximum value of uint256 is a known practice to save gas. However, this pattern was proven to increase the impact of an attack many times in the past, in case the approved contract gets hacked.  Examples  code/contracts/BaseTradeExecutor.sol:L19  IERC20(vaultWantToken()).approve(vault, MAX_INT);  code/contracts/Batcher/Batcher.sol:L48  IERC20(vaultInfo.tokenAddress).approve(vaultAddress, type(uint256).max);  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L106-L112  IERC20(LP_TOKEN).safeApprove(ETH_STETH_POOL, type(uint256).max);  // Approve max LP tokens to convex booster  IERC20(LP_TOKEN).safeApprove(  address(CONVEX_BOOSTER),  type(uint256).max  );  code/contracts/ConvexExecutor/Harvester.sol:L65-L69  crv.safeApprove(address(crveth), type(uint256).max);  // max approve CVX to CVX/ETH pool on curve  cvx.safeApprove(address(cvxeth), type(uint256).max);  // max approve LDO to uniswap swap router  ldo.safeApprove(address(uniswapRouter), type(uint256).max);  code/contracts/LyraL2/LyraPositionHandlerL2.sol:L63-L71  IERC20(wantTokenL2).safeApprove(  address(UniswapV3Controller.uniswapRouter),  type(uint256).max  );  // approve max susd balance to uniV3 router  LyraController.sUSD.safeApprove(  address(UniswapV3Controller.uniswapRouter),  type(uint256).max  );  Recommendation  Consider approving the exact amount that s needed to be transferred, or alternatively, add an external function that allows the revocation of approvals.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.10 Batcher.depositFunds may allow for more deposits than vaultInfo.maxAmount ", "body": "  Description  As part of a gradual rollout strategy, the Brahma-fi system of contracts has a limit of how much can be deposited into the protocol.  This is implemented through the Batcher contract that allows users to deposit into it and keep the amount they have deposited in the depositLedger[recipient] state variable. In order to cap how much is deposited, the user s input amountIn is evaluated within the following statement:  code/contracts/Batcher/Batcher.sol:L109-L116  require(  IERC20(vaultInfo.vaultAddress).totalSupply() +  pendingDeposit -  pendingWithdrawal +  amountIn <=  vaultInfo.maxAmount,  \"MAX_LIMIT_EXCEEDED\"  );  However, while pendingDeposit, amountIn, and vaultInfo.maxAmount are denominated in the vault asset token (WETH in our case), IERC20(vaultInfo.vaultAddress).totalSupply() and pendingWithdrawal represent vault shares tokens, creating potential mismatches in this evaluation.  Recommendation  Consider either documenting this potential discrepancy or keeping track of all deposits in a state variable and using that inside the require statement..  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.11 The Deposit and Withdraw event are always emitted with zero amount ", "body": "  Description  The events emitted during the deposit or withdraw are supposed to contain the relevant amounts of tokens involved in these actions. But in fact the current balance of the address is used in both cases. These balances will be equal to zero by that time:  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L151-L155  IWETH9(address(wantToken)).withdraw(depositParams._amount);  _convertEthIntoLpToken(address(this).balance);  emit Deposit(address(this).balance);  code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L207-L209  IWETH9(address(wantToken)).deposit{value: address(this).balance}();  emit Withdraw(address(this).balance);  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.12 BaseTradeExecutor.confirmDeposit | confirmWithdraw - Violation of the  checks-effects-interactions  pattern ", "body": "  Description  Both confirmDeposit, confirmWithdraw might be re-entered by the keeper (in case it is a contract), in case the derived contract allows the execution of untrusted code.  Examples  code/contracts/BaseTradeExecutor.sol:L57-L61  function confirmDeposit() public override onlyKeeper {  require(depositStatus.inProcess, \"DEPOSIT_COMPLETED\");  _confirmDeposit();  depositStatus.inProcess = false;  code/contracts/BaseTradeExecutor.sol:L69-L73  function confirmWithdraw() public override onlyKeeper {  require(withdrawalStatus.inProcess, \"WIHDRW_COMPLETED\");  _confirmWithdraw();  withdrawalStatus.inProcess = false;  Recommendation  Although the impact is very limited, it is recommended to implement the  checks-effects-interactions  in both functions.  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "5.13 Batcher doesn t work properly with arbitrary tokens ", "body": "  Description  The Batcher and the Vault contracts initially operate with ETH and WETH. But the contracts are supposed to be compatible with any other ERC-20 tokens.  For example, in the Batcher.deposit function, there is an option to transfer ETH instead of the token, which should only be happening if the token is WETH. Also, the token is named WETH, but if the intention is to use the Batcher contract with arbitrary tokens token, it should be named differently.  code/contracts/Batcher/Batcher.sol:L89-L100  if (ethSent > 0) {  amountIn = ethSent;  WETH.deposit{value: ethSent}();  /// If no wei sent, use amountIn and transfer WETH from txn sender  else {  IERC20(vaultInfo.tokenAddress).safeTransferFrom(  msg.sender,  address(this),  amountIn  );  ", "labels": ["Consensys", "Minor"], "html_url": "https://consensys.io/diligence/audits/2022/05/brahma-fi/"}, {"title": "4.1 Gold order size should be limited    Addressed", "body": "  Resolution                           Addressed in   horizon-games/SkyWeaver-contracts#9 by adding a limit for cold cards amount in one order.  Description  When a user submits an order to buy gold cards, it s possible to buy a huge amount of cards. _commit function uses less gas than mineGolds, which means that the user can successfully commit to buying this amount of cards and when it s time to collect them, mineGolds function may run out of gas because it iterates over all card IDs and mints them:  code/contracts/shop/GoldCardsFactory.sol:L375-L376  // Mint gold cards  skyweaverAssets.batchMint(_order.cardRecipient, _ids, amounts, \"\");  Recommendation  Limit a maximum gold card amount in one order.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.2 Price and refund changes may cause failures    Addressed", "body": "  Resolution  Addressed in horizon-games/SkyWeaver-contracts#3.  Fix involves burning the weave when the commit occurs instead of when the minting of the gold cards occur.  Description  Price and refund for gold cards are used in 3 different places: commit, mint, refund.  Weave tokens spent during the commit phase  code/contracts/shop/GoldCardsFactory.sol:L274-L279  function _commit(uint256 _weaveAmount, GoldOrder memory _order)  internal  // Check if weave sent is sufficient for order  uint256 total_cost = _order.cardAmount.mul(goldPrice).add(_order.feeAmount);  uint256 refund_amount = _weaveAmount.sub(total_cost); // Will throw if insufficient amount received  but they are burned rngDelay blocks after  code/contracts/shop/GoldCardsFactory.sol:L371-L373  // Burn the non-refundable weave  uint256 weave_to_burn = (_order.cardAmount.mul(goldPrice)).sub(_order.cardAmount.mul(goldRefund));  weaveContract.burn(weaveID, weave_to_burn);  If the price is increased between these transactions, mining cards may fail because it should burn more weave tokens than there are tokens in the smart contract. Even if there are enough tokens during this particular transaction, someone may fail to melt a gold card later.  If the price is decreased, some weave tokens will be stuck in the contract forever without being burned.  Recommendation  Store goldPrice and goldRefund in GoldOrder.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.3 Re-entrancy attack allows to buy EternalHeroes cheaper    Addressed", "body": "  Resolution  Addressed in horizon-games/SkyWeaver-contracts#4.  Minting tokens before sending refunds. Subsequent PR will also add re-entrancy guard for all shops.  And re-entrancy guard added here: horizon-games/SkyWeaver-contracts#10  Description  When buying eternal heroes in _buy  function of EternalHeroesFactory contract, a buyer can do re-entracy before items are minted.  code/contracts/shop/EternalHeroesFactory.sol:L278-L284  uint256 refundAmount = _arcAmount.sub(total_cost);  if (refundAmount > 0) {  arcadeumCoin.safeTransferFrom(address(this), _recipient, arcadeumCoinID, refundAmount, \"\");  // Mint tokens to recipient  factoryManager.batchMint(_recipient, _ids, amounts_to_mint, \"\");  Since price should increase after every N items are minted, it s possible to buy more items with the old price.  Recommendation  Add re-entrancy protection or mint items before sending the refund.  ", "labels": ["Consensys", "Major"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.4 Supply limitation misbehaviors    Addressed", "body": "  Resolution                           Logic remains unchanged as it s the desired behaviour. But the issue is mitigated in   horizon-games/SkyWeaver-contracts#5 by renaming the term  currentSupply  to  currentIssuance  and  maxSupply  to  maxIssuance  for maximum clarity.  Description  In SWSupplyManager contract, the owner can limit supply for any token ID by setting maxSupply:  code/contracts/shop/SWSupplyManager.sol:L149-L165  function setMaxSupplies(uint256[] calldata _ids, uint256[] calldata _newMaxSupplies) external onlyOwner() {  require(_ids.length == _newMaxSupplies.length, \"SWSupplyManager#setMaxSupply: INVALID_ARRAYS_LENGTH\");  // Can only *decrease* a max supply  // Can't set max supply back to 0  for (uint256 i = 0; i < _ids.length; i++ ) {  if (maxSupply[_ids[i]] > 0) {  require(  0 < _newMaxSupplies[i] && _newMaxSupplies[i] < maxSupply[_ids[i]],  \"SWSupplyManager#setMaxSupply: INVALID_NEW_MAX_SUPPLY\"  );  maxSupply[_ids[i]] = _newMaxSupplies[i];  emit MaxSuppliesChanged(_ids, _newMaxSupplies);  The problem is that you can set maxSupply that is lower than currentSupply, which would be an unexpected state to have.  Also, if some tokens are burned, their currentSupply is not decreasing:  code/contracts/shop/SWSupplyManager.sol:L339-L345  function burn(  uint256 _id,  uint256 _amount)  external  _burn(msg.sender, _id, _amount);  This unexpected behaviour may lead to burning all of the tokens without being able to mint more.  Recommendation  Properly track currentSupply by modifying it in burn function. Consider having a following restriction require(_newMaxSupplies[i] > currentSupply[_ids[i]]) in setMaxSupplies function.  ", "labels": ["Consensys", "Medium"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.5 Owner can modify gold cards distribution after someone committed to buy   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  This issue will be addressed by having the owner be a delayed multisig, such that users will have time to witness a change in the distribution that is about to occur.  Description  When a user commits to buying a gold card (and sends weave), there is an expected distribution of possible outcomes. But the problem is that owner can change distribution by calling registerIDs and deregisterIDs  functions.  Additionally, owner can buy any specific gold card avoiding RNG mechanism. It can be done by deleting all the unwanted cards, mining the card and then returning them back. And if owner removes every card from the list, nothing is going to be minted.  Recommendation  There are a few possible recommendations:  Fix a distribution for every order after commit(costly solution).  Make it an explicit part of the trust model (increases trust to the admins).  Cancel pending orders if gold cards IDs are changed.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.6 A buyer of a gold card can manipulate randomness   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  We hereby assume that Horizon will always be willing to mine gold cards even at a loss considering the amount of gold cards that can be created per week is limited. If in practice this becomes a problem, we can upgrade this factory.  Description  When a user is buying a gold card, _commit function is called. After rngDelay number of blocks, someone should call mineGolds function to actually mint the card. If this function is not called during 255 blocks (around 1 hour), a user should call recommit to try to mint a gold card again with a new random seed. So if the user doesn t like a card that s going to be minted (randomly), user can try again until a card is good. The issue is medium because anyone can call mineGolds function in order to prevent this behaviour. But it costs money and there s no incentive for anyone to do so.  Recommendation  Create a mechanism to avoid this kind of manipulation. For example, make sure there is an incentive for someone to call mineGolds function  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.7 A refund is sent to recipient   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  It s unlikely users will send inexact amount since price is fixed. If this becomes a problem in practice we can re-deploy the factory with this added functionality.  Description  When a refund is sent, it s sent to recipient. In case if a user wants to keep game items and money separate, it makes sense to send a refund back to from address.  Recommendation  Since there may be different use cases, consider adding refundAddress to order structure.  ", "labels": ["Consensys", "Medium", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "4.8 Randomness can be manipulated by miners   ", "body": "  Resolution  The client decided not to fix this issue with the following comment:  For miners to be able to profit, they would have to forfeit multiple blocks and the desired gold cards would have to be very expensive in the first place (e.g. in the $10k) for it to be worth it for them. In practice, there are also other oppotunities for miners that offer better returns, but if it ever turned out to be a problem, we would see it coming and we can then use a more secure and expensive source of RNG as the gold cards would be very expensive and the additional cost would be worth it.  Description  Random number generator uses future blockhash as a seed. So it s possible for miners to manipulate that value in order to get a better gold card. The issue is minor because it only makes sense if the cost of the card is high enough to do the extra work on the miner side.  Recommendation  Use better RNG algorithms if the price of gold cards is high enough for the miners to start manipulation.  ", "labels": ["Consensys", "Minor", "Won't Fix"], "html_url": "https://consensys.io/diligence/audits/2020/04/skyweaver/"}, {"title": "5.1 TimeLock spam prevention can be bypassed    Addressed", "body": "  Resolution                           This was addressed in   commit aa6fc49fbf3230d7f02956b33a3150c6885ee93f by parsing the input evm script and ensuring only a single external call is made. Additionally,  commit 453179e98159413d38196b6a5373cdd729483567 added  Description  The TimeLock app is a forwarder that requires users to lock some token before forwarding an EVM callscript. Its purpose is to introduce a  spam penalty  to hamper repeat actions within an Aragon org. In the context of a Dandelion org, this spam penalty is meant to stop users from repeatedly creating votes in DandelionVoting, as subsequent votes are buffered by a configurable number of blocks (DandelionVoting.bufferBlocks). Spam prevention is important, as the more votes are buffered, the longer it takes before  non-spam  votes are able to be executed.  By allowing arbitrary calls to be executed, the TimeLock app opens several potential vectors for bypassing spam prevention.  Examples  Using a callscript to transfer locked tokens to the sender  By constructing a callscript that executes a call to the lock token address, the sender execute calls to the lock token on behalf of TimeLock. Any function can be executed, making it possible to not only transfer  locked  tokens back to the sender, but also steal other users  locked tokens by way of transfer.  Using a batched callscript to call DandelionVoting.newVote repeatedly  Callscripts can be batched, meaning they can execute multiple calls before finishing. Within a Dandelion org, the spam prevention mechanism is used for the DandelionVoting.newVote function. A callscript that batches multiple calls to this function can execute newVote several times per call to TimeLock.forward. Although multiple new votes are created, only one spam penalty is incurred, making it trivial to extend the buffer imposed on  non-spam  votes.  Using a callscript to re-enter TimeLock and forward or withdrawAllTokens to itself  A callscript can be used to re-enter TimeLock.forward, as well as any other TimeLock functions. Although this may not be directly exploitable, it does seem unintentional that many of the TimeLock contract functions are accessible to itself in this manner.  Recommendation  Add the TimeLock contract s own address to the evmscript blacklist  Add the TimeLock lock token address to the evmscript blacklist  To fix spamming through batched callscripts, one option is to have users pass in a destination and calldata, and manually perform a call. Alternatively, CallsScript can be forked and altered to only execute a single external call to a single destination.  ", "labels": ["Consensys", "Critical"], "html_url": "https://consensys.io/diligence/audits/2019/12/dandelion-organizations/"}]