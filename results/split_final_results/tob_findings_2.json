[{"title": "16. Use of defer in a loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "The Solana watcher uses defer within an innite loop (gure 16.1). Deferred calls are executed when their enclosing function returns. Since the enclosing loop is not exited under normal circumstances, the deferred calls are never executed and constitute a waste of resources. for { select { case <-ctx.Done(): return nil default: rCtx, cancel := context.WithTimeout(ctx, time.Second*300) // 5 minute defer cancel() ... } } Figure 16.1: node/pkg/watchers/solana/client.go#L244L271 Sample code demonstrating the problem appears in appendix E. Exploit Scenario Alice runs her Wormhole node in an environment with constrained resources. Alice nds that her node is not able to achieve the same uptime as other Wormhole nodes. The underlying cause is resource exhaustion caused by the Solana watcher. Recommendations Short term, rewrite the code in gure 16.1 to eliminate the use of defer in the for loop. The easiest and most straightforward way would likely be to move the code in the default case into its own named function. Eliminating this use of defer in a loop will eliminate a potential source of resource exhaustion. Long term, regularly review uses of defer to ensure they do not appear in a loop. To the best of our knowledge, there are not publicly available detectors for problems like this. However, regular manual review should be sucient to spot them.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "17. Finalizer is allowed to be nil ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-wormhole-securityreview.pdf", "body": "The conguration of a chains watcher can allow a nalizer to be nil, which may allow newly introduced bugs to go unnoticed. Whenever a chains RPC does not have a notion of safe or nalized blocks, the watcher polls the chain for the latest block using BlockPollConnector. After fetching a block, the watcher checks whether it is nal in accordance with the respective chains PollFinalizer implementation. // BlockPollConnector polls for new blocks instead of subscribing when using SubscribeForBlocks. It allows to specify a // finalizer which will be used to only return finalized blocks on subscriptions. type BlockPollConnector struct { Connector time.Duration Delay useFinalized bool publishSafeBlocks bool finalizer blockFeed errFeed PollFinalizer ethEvent.Feed ethEvent.Feed } Figure 17.1: node/pkg/watchers/evm/connectors/poller.go#L24L34 However, the method pollBlocks allows BlockPollConnector to have a nil PollFinalizer (see gure 17.2). This is unnecessary and may permit edge cases that could otherwise be avoided by requiring all BlockPollConnectors to use the DefaultFinalizer explicitly if a nalizer is not required (the default nalizer accepts all blocks as nal). This will ensure that the watcher does not incidentally process a block received from blockFeed that is not in the canonical chain . if b.finalizer != nil { finalized, err := b.finalizer.IsBlockFinalized(timeout, block) if err != nil { logger.Error(\"failed to check block finalization\", zap.Uint64(\"block\", block.Number.Uint64()), zap.Error(err)) finalization (%d): %w\", block.Number.Uint64(), err) return lastPublishedBlock, fmt.Errorf(\"failed to check block } if !finalized { break } } b.blockFeed.Send(block) lastPublishedBlock = block Figure 17.2: node/pkg/watchers/evm/connectors/poller.go#L149L164 Exploit Scenario A developer adds a new chain to the watcher using BlockPollConnector and forgets to add a PollFinalizer. Because a nalizer is not required to receive the latest blocks, transactions that were not included in the blockchain are considered valid, and funds are incorrectly transferred without corresponding deposits. Recommendations Short term, rewrite the block poller to require a nalizer. This makes the conguration of the block poller explicit and claries that a DefaultFinalizer is being used, indicating that no extra validations are being performed. Long term, document the conguration and assumptions of each chain. Then, see if any changes could be made to the code to clarify the developers intentions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Lack of build instructions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The Drift Protocol repository does not contain instructions to build, compile, test, or run the project. The projects README should include at least the following information:    Instructions for building the project Instructions for running the built artifacts Instructions for running the projects tests The closest thing we have found to build instructions appears in a script in the drift-sim repository (gure 1.1). As shown in the gure below, building the project is non-trivial. Users should not be required to rediscover these steps on their own. git submodule update --init --recursive # build v2 cd driftpy/protocol-v2 yarn && anchor build # build dependencies for v2 cd deps/serum-dex/dex && anchor build && cd ../../.. # go back to top-level cd ../../ Figure 1.1: drift-sim/setup.sh Additionally, the project relies on serum-dex , which currently has an open issue regarding outdated build instructions. Thus, if a user visits the serum-dex repository to learn how to build the dependency, they will be misled. Exploit Scenario Alice attempts to build and deploy her own copy of the Drift Protocol smart contract. Without instructions, Alice deploys it incorrectly. Users of Alices copy of the smart contract suer nancial loss. Recommendations Short term, add the minimal information listed above to the projects README . This will help users to build, run, and test the project . Long term, as the project evolves, ensure that the README is updated. This will help ensure that the README does not communicate incorrect information to users . References  Documentation points to do.sh", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Inadequate testing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The Anchor tests are not run as part of Drift Protocols CI process. Moreover, the script responsible for running the Anchor tests does not run all of them. Integrating all Anchor tests into the CI process and updating the script so it runs all tests will help ensure they are run regularly and consistently. Figure 2.1 shows a portion of the projects main GitHub workow, which runs the projects unit tests. However, the le makes no reference to the projects Anchor tests. - name : Run unit tests run : cargo test --lib # run unit tests Figure 2.1: .github/workflows/main.yml#L52L53 Furthermore, the script used to run the Anchor tests runs only some of them. The relevant part of the script appears in gure 2.2. The test_files array contains the names of nearly all of the les containing tests in the tests directory. However, the array lacks the following entries, and consequently does not run their tests:  ksolver.ts  tokenFaucet.ts test_files =( postOnlyAmmFulfillment.ts imbalancePerpPnl.ts ... # 42 entries cancelAllOrders.ts ) Figure 2.2: test-scripts/run-anchor-tests.sh#L7L53 Exploit Scenario Alice, a Drift Protocol developer, unwittingly introduces a bug into the codebase. The test would be revealed by the Anchor tests. However, because the Anchor tests are not run in CI, the bug goes unnoticed. Recommendations Short term:   Adjust the main GitHub workow so that it runs the Anchor tests. Adjust the run-anchor-tests.sh script so that it runs all Anchor tests (including those in ksolver.ts and tokenFaucet.ts ). Taking these steps will help to ensure that all Anchor tests are run regularly and consistently. Long term, revise the run-anchor-tests.sh script so that the test_files array is not needed. Move les that do not contain tests into a separate directory, so that only les containing tests remain. Then, run the tests in all les in the tests directory. Adopting such an approach will ensure that newly added tests are automatically run.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Invalid audit.toml prevents cargo audit from being run ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The projects anchor.toml le contains an invalid key. This makes running cargo audit on the project impossible. The relevant part of the audit.toml le appears in gure 3.1. The packages key is unrecognized by cargo audit . As a result, cargo audit produces the error in gure 3.2 when run on the protocol-v2 repository. [packages] source = \"all\" # \"all\", \"public\" or \"local\" Figure 3.1: .cargo/audit.toml#L27L28 error: cargo-audit fatal error: parse error: unknown field `packages`, expected one of `advisories`, `database`, `output`, `target`, `yanked` at line 30 column 1 Figure 3.2: Error produced by cargo audit when run on the protocol-v2 repository Exploit Scenario A vulnerability is discovered in a protocol-v2 dependency. A RUSTSEC advisory is issued for the vulnerability, but because cargo audit cannot be run on the repository, the vulnerability goes unnoticed. Users suer nancial loss. Recommendations Short term, either remove the packages table from the anchor.toml le or replace it with a table recognized by cargo audit . In the projects current state, cargo audit cannot be run on the project. Long term, regularly run cargo audit in CI and verify that it runs to completion without producing any errors or warnings. This will help the project receive the full benets of running cargo audit by identifying dependencies with RUSTSEC advisories.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Race condition in Drift SDK ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "A race condition in the Drift SDK causes client programs to operate on non-existent or possibly stale data. The race condition aects many of the projects Anchor tests, making them unreliable. Use of the SDK in production could have nancial implications. When running the Anchor tests, the error in gure 4.1 appears frequently. The data eld that the error refers to is read by the getUserAccount function (gure 4.2). This function tries to read the data eld from a DataAndSlot object obtained by calling getUserAccountAndSlot (gure 4.3). That DataAndSlot object is set by the handleRpcResponse function (gure 4.4). TypeError: Cannot read properties of undefined (reading 'data') at User.getUserAccount (sdk/src/user.ts:122:56) at DriftClient.getUserAccount (sdk/src/driftClient.ts:663:37) at DriftClient.<anonymous> (sdk/src/driftClient.ts:1005:25) at Generator.next (<anonymous>) at fulfilled (sdk/src/driftClient.ts:28:58) at processTicksAndRejections (node:internal/process/task_queues:96:5) Figure 4.1: Error that appears frequently when running the Anchor tests public getUserAccount(): UserAccount { return this .accountSubscriber.getUserAccountAndSlot(). data ; } Figure 4.2: sdk/src/user.ts#L121L123 public getUserAccountAndSlot(): DataAndSlot<UserAccount> { this .assertIsSubscribed(); return this .userDataAccountSubscriber. dataAndSlot ; } Figure 4.3: sdk/src/accounts/webSocketUserAccountSubscriber.ts#L72L75 handleRpcResponse(context: Context , accountInfo?: AccountInfo <Buffer>): void { ... if (newBuffer && (!oldBuffer || !newBuffer.equals(oldBuffer))) { this .bufferAndSlot = { buffer: newBuffer , slot: newSlot , }; const account = this .decodeBuffer(newBuffer); this .dataAndSlot = { data: account , slot: newSlot , }; this .onChange(account); } } Figure 4.4: sdk/src/accounts/webSocketAccountSubscriber.ts#L55L95 If a developer calls getUserAccount but handleRpcResponse has not been called since the last time the account was updated, stale data will be returned. If handleRpcResponse has never been called for the account in question, an error like that shown in gure 4.1 arises. Note that a developer can avoid the race by calling WebSocketAccountSubscriber.fetch (gure 4.5). However, the developer must manually identify locations where such calls are necessary. Errors like the one shown in gure 4.1 appear frequently when running the Anchor tests, which suggests that identifying such locations is nontrivial. async fetch(): Promise < void > { const rpcResponse = await this .program.provider.connection.getAccountInfoAndContext( this .accountPublicKey, ( this .program.provider as AnchorProvider).opts.commitment ); this .handleRpcResponse(rpcResponse.context, rpcResponse?.value); } Figure 4.5: sdk/src/accounts/webSocketAccountSubscriber.ts#L46L53 We suspect this problem applies to not just user accounts, but any account fetched via a subscription mechanism (e.g., state accounts or perp market accounts). Note that despite the apparent race condition, Drift Protocol states that the tests run reliably for them. Exploit Scenario Alice, unaware of the race condition, writes client code that uses the Drift SDK. Alices code unknowingly operates on stale data and proceeds with a transaction, believing it will result in nancial gain. However, when processed with actual on-chain data, the transaction results in nancial loss for Alice. Recommendations Short term, rewrite all account getter functions so that they automatically call WebSocketAccountSubscriber.fetch . This will eliminate the need for developers to deal with the race manually. Long term, investigate whether using a subscription mechanism is actually needed. Another Solana RPC call could solve the same problem yet be more ecient than a subscription combined with a manual fetch.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "5. Loose size coupling between function invocation and requirement ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The implementation of the emit_stack function relies on the caller to use a suciently large buer space to hold a Base64-encoded representation of the discriminator along with the serialized event. Failure to provide sucient space will result in an out-of-bounds attempt on either the write operation or the in the base64::encode_config_slice call. emit_stack::<_, 424 >(order_action_record); Figure 5.1: programs/drift/src/controller/orders.rs#L545 pub fn emit_stack <T: AnchorSerialize + Discriminator, const N: usize >(event: T ) { let mut data_buf = [ 0 u8 ; N]; let mut out_buf = [ 0 u8 ; N]; emit_buffers(event, & mut data_buf[..], & mut out_buf[..]) } pub fn emit_buffers <T: AnchorSerialize + Discriminator>( event: T , data_buf: & mut [ u8 ], out_buf: & mut [ u8 ], ) { let mut data_writer = std::io::Cursor::new(data_buf); data_writer .write_all(&<T as Discriminator>::discriminator()) .unwrap(); borsh::to_writer(& mut data_writer, &event).unwrap(); let data_len = data_writer.position() as usize ; let out_len = base64::encode_config_slice( &data_writer.into_inner()[ 0 ..data_len], base64::STANDARD, out_buf, ); let msg_bytes = &out_buf[ 0 ..out_len]; let msg_str = unsafe { std:: str ::from_utf8_unchecked(msg_bytes) }; msg!(msg_str); } Figure 5.2: programs/drift/src/state/events.rs#L482L511 Exploit Scenario A maintainer of the smart contract is unaware of this implicit size requirement and adds a call to emit_stack using too small a buer, or changes are made to a type without a corresponding change to all places where emit_stack uses that type. If the changed code is not covered by tests, the problem will manifest during contract operation, and could cause an instruction to panic, thereby reverting the transaction. Recommendations Short term, add a size constant to the type, and calculate the amount of space required for holding the respective buers. This ensures that changes to a type's size can be made throughout the code. Long term, create a trait to be used by the types with which emit_stack is intended to work. This can be used to handle the size of the type, and also any other future requirement for types used by emit_stack .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. The zero-copy feature in Anchor is experimental ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "Several structs for keeping state use Anchors zero-copy functionality. The Anchor documentation states that this is still an experimental feature that should be used only when Borsh serialization cannot be used without hitting the stack or heap limits. Exploit Scenario The Anchor framework has a bug in the zero-copy feature, or updates it with a breaking change, in a way that aects the security model of the Drift smart contract. An attacker discovers this problem and leverages it to steal funds from the contract. #[account(zero_copy)] #[derive(Default, Eq, PartialEq, Debug)] #[repr(C)] pub struct User { pub authority: Pubkey , pub delegate: Pubkey , pub name: [ u8 ; 32 ], pub spot_positions: [SpotPosition; 8 ], pub perp_positions: [PerpPosition; 8 ], pub orders: [Order; 32 ], pub last_add_perp_lp_shares_ts: i64 , pub total_deposits: u64 , pub total_withdraws: u64 , pub total_social_loss: u64 , // Fees (taker fees, maker rebate, referrer reward, filler reward) and pnl for perps pub settled_perp_pnl: i64 , // Fees (taker fees, maker rebate, filler reward) for spot pub cumulative_spot_fees: i64 , pub cumulative_perp_funding: i64 , pub liquidation_margin_freed: u64 , // currently unimplemented // currently unimplemented pub liquidation_start_ts: i64 , pub next_order_id: u32 , pub max_margin_ratio: u32 , pub next_liquidation_id: u16 , pub sub_account_id: u16 , pub status: UserStatus , pub is_margin_trading_enabled: bool , pub padding: [ u8 ; 26 ], } Figure 6.1: Example of a struct using zero copy Recommendations Short term, evaluate if it is possible to move away from using zero copy without hitting the stack or heap limits, and do so if possible. Not relying on experimental features reduces the risk of exposure to bugs in the Anchor framework. Long term, adopt a conservative stance by using stable versions of packages and features. This reduces both risk and time spent on maintaining compatibility with code still in ux.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Hard-coded indices into account data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The implementations for both PerpMarketMap and SpotMarketMap use hard-coded indices into the accounts data in order to retrieve the marked_index property without having to deserialize all the data. // market index 1160 bytes from front of account let market_index = u16 ::from_le_bytes(*array_ref![data, 1160 , 2 ]); Figure 7.1: programs/drift/src/state/perp_market_map.rs#L110L111 let market_index = u16 ::from_le_bytes(*array_ref![data, 684 , 2 ]); Figure 7.2: programs/drift/src/state/spot_market_map.rs#L174 Exploit Scenario Alice, a Drift Protocol developer, changes the layout of the structure or the width of the market_index property but fails to update one or more of the hard-coded indices. Mallory notices this bug and nds a way to use it to steal funds. Recommendations Short term, add consts that include the value of the indices and the type size. Also add comments explaining the calculation of the values. This ensures that by updating the constants, all code relying on the operation will retrieve the correct part of the unlying data. Long term, add an implementation to the struct to unpack the market_index from the serialized state. This reduces the maintenance burden of updating the code that accesses data in this way.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Missing verication of maker and maker_stats accounts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The handle_place_and_take_perp_order and handle_place_and_take_spot_order functions retrieve two additional accounts that are passed to the instruction: maker and maker_stats . However, there is no check that the two accounts are linked (i.e., that their authority is the same). Due to time constraints, we were unable to determine the impact of this nding. pub fn get_maker_and_maker_stats <'a>( account_info_iter: & mut Peekable<Iter<AccountInfo<'a>>>, ) -> DriftResult <(AccountLoader<'a, User>, AccountLoader<'a, UserStats>)> { let maker_account_info = next_account_info(account_info_iter).or( Err (ErrorCode::MakerNotFound))?; validate!( maker_account_info.is_writable, ErrorCode::MakerMustBeWritable )?; let maker: AccountLoader <User> = AccountLoader::try_from(maker_account_info).or( Err (ErrorCode::CouldNotDeserializeMak er))?; let maker_stats_account_info = next_account_info(account_info_iter).or( Err (ErrorCode::MakerStatsNotFound))?; validate!( maker_stats_account_info.is_writable, ErrorCode::MakerStatsMustBeWritable )?; let maker_stats: AccountLoader <UserStats> = AccountLoader::try_from(maker_stats_account_info) .or( Err (ErrorCode::CouldNotDeserializeMakerStats))?; Ok ((maker, maker_stats)) } Figure 8.1: programs/drift/src/instructions/optional_accounts.rs#L47L74 Exploit Scenario Mallory passes two unlinked accounts of the correct type in the places for maker and maker_stats , respectively. This causes the contract to operate outside of its intended use. Recommendations Short term, add a check that the authority of the accounts are the same. Long term, add all code for authentication of accounts to the front of instruction handlers. This increases the clarity of the checks and helps with auditing the authentication.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "9. Panics used for error handling ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "In several places, the code panics when an arithmetic overow or underow occurs. Panics should be reserved for programmer errors (e.g., assertion violations). Panicking on user errors dilutes the utility of the panic operation. An example appears in gure 9.1. The adjust_amm function uses both the question mark operator ( ? ) and unwrap to handle errors resulting from peg related calculations. An overow or underow could result from an invalid input to the function. An error should be returned in such cases. budget_delta_peg = budget_i128 .safe_add(adjustment_cost.abs())? .safe_mul(PEG_PRECISION_I128)? .safe_div(per_peg_cost)?; budget_delta_peg_magnitude = budget_delta_peg.unsigned_abs(); new_peg = if budget_delta_peg > 0 { ... } else if market.amm.peg_multiplier > budget_delta_peg_magnitude { market .amm .peg_multiplier .safe_sub(budget_delta_peg_magnitude) .unwrap() } else { 1 }; Figure 9.1: programs/drift/src/math/repeg.rs#L349L369 Running Clippy with the following command identies 66 locations in the drift package where expect or unwrap is used: cargo clippy -p drift -- -A clippy::all -W clippy::expect_used -W clippy::unwrap_used Many of those uses appear to be related to invalid input. Exploit Scenario Alice, a Drift Protocol developer, observes a panic in the Drift Protocol codebase. Alice ignores the panic, believing that it is caused by user error, but it is actually caused by a bug she introduced. Recommendations Short term, reserve the use of panics for programmer errors. Have relevant areas of the code return Result::Err on user errors. Adopting such a policy will help to distinguish the two types of errors when they occur. Long term, consider denying the following Clippy lints:  clippy::expect_used  clippy::unwrap_used  clippy::panic Although this will not prevent all panics, it will prevent many of them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Testing code used in production ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "In some locations in the Drift Protocol codebase, testing code is mixed with production code with no way to discern between them. Testing code should be clearly indicated as such and guarded by #[cfg(test)] to avoid being called in production. Examples appear in gures 10.1 and 10.2. The OracleMap struct has a quote_asset_price_data eld that is used only when get_price_data is passed a default Pubkey . Similarly, the AMM implementation contains functions that are used only for testing and are not guarded by #[cfg(test)] . pub struct OracleMap <'a> { oracles: BTreeMap <Pubkey, AccountInfoAndOracleSource<'a>>, price_data: BTreeMap <Pubkey, OraclePriceData>, pub slot: u64 , pub oracle_guard_rails: OracleGuardRails , pub quote_asset_price_data: OraclePriceData , } impl <'a> OracleMap<'a> { ... pub fn get_price_data (& mut self , pubkey: & Pubkey ) -> DriftResult <&OraclePriceData> { if pubkey == &Pubkey::default() { return Ok (& self .quote_asset_price_data); } Figure 10.1: programs/drift/src/state/oracle_map.rs#L22L47 impl AMM { pub fn default_test () -> Self { let default_reserves = 100 * AMM_RESERVE_PRECISION; // make sure tests dont have the default sqrt_k = 0 AMM { Figure 10.2: programs/drift/src/state/perp_market.rs#L490L494 Drift Protocol has indicated that the quote_asset_price_data eld (gure 10.1) is used in production. This raises concerns because there is currently no way to set the contents of this eld, and no assets price is perfectly constant (e.g., even stablecoins prices uctuate). For this reason, we have changed this ndings severity from Informational to Undetermined. Exploit Scenario Alice, a Drift Protocol developer, introduces code that calls the default_test function, not realizing it is intended only for testing. Alice introduces a bug as a result. Recommendations Short term, to the extent possible, avoid mixing testing and production code by, for example, using separate data types and storing the code in separate les. When testing and production code must be mixed, clearly mark the testing code as such, and guard it with #[cfg(test)] . These steps will help to ensure that testing code is not deployed in production. Long term, as new code is added to the codebase, ensure that the aforementioned standards are maintained. Testing code is not typically held to the same standards as production code, so it is more likely to include bugs.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "11. Inconsistent use of checked arithmetic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "In several locations, the Drift Protocol codebase uses unchecked arithmetic. For example, in calculate_margin_requirement_and_total_collateral_and_liability_info (gure 11.1), the variable num_perp_liabilities is used as an operand in both a checked and an unchecked operation. To protect against overows and underows, unchecked arithmetic should be used sparingly. num_perp_liabilities += 1 ; } with_isolated_liability &= margin_requirement > 0 && market.contract_tier == ContractTier::Isolated; } if num_spot_liabilities > 0 { validate!( margin_requirement > 0 , ErrorCode::InvalidMarginRatio, \"num_spot_liabilities={} but margin_requirement=0\" , num_spot_liabilities )?; } let num_of_liabilities = num_perp_liabilities.safe_add(num_spot_liabilities) ?; Figure 11.1: programs/drift/src/math/margin.rs#L499L515 Note that adding the following to the crate root will cause Clippy to fail the build whenever unchecked arithmetic is used: #![deny(clippy::integer_arithmetic)] Exploit Scenario Alice, a Drift Protocol developer, unwittingly introduces an arithmetic overow bug into the codebase. The bug would have been revealed by the use of checked arithmetic. However, because unchecked arithmetic is used, the bug goes unnoticed. Recommendations Short term, add the #![deny(clippy::integer_arithmetic)] attribute to the drift crate root. Add #[allow(clippy::integer_arithmetic)] in rare situations where code is performance critical and its safety can be guaranteed through other means. Taking these steps will reduce the likelihood of overow or underow bugs residing in the codebase. Long term, if additional Solana programs are added to the codebase, ensure the #![deny(clippy::integer_arithmetic)] attribute is also added to them. This will reduce the likelihood that newly introduced crates contain overow or underow bugs.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "12. Inconsistent and incomplete exchange status checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "Drift Protocols representation of the exchanges status has several problems:    The exchanges status is represented using an enum , which does not allow more than one individual operation to be paused (gures 12.1 and 12.2). As a result, an administrator could inadvertently unpause one operation by trying to pause another (gure 12.3). The ExchangeStatus variants do not map cleanly to exchange operations. For example, handle_transfer_deposit checks whether the exchange status is WithdrawPaused (gure 12.4). The functions name suggests that the function checks whether transfers or deposits are paused. The ExchangeStatus is checked in multiple inconsistent ways. For example, in handle_update_funding_rate (gure 12.5), both an access_control attribute and the body of the function include a check for whether the exchange status is FundingPaused . pub enum ExchangeStatus { Active, FundingPaused, AmmPaused, FillPaused, LiqPaused, WithdrawPaused, Paused, } Figure 12.1: programs/drift/src/state/state.rs#L36L44 #[account] #[derive(Default)] #[repr(C)] pub struct State { pub admin: Pubkey , pub whitelist_mint: Pubkey , ... pub exchange_status: ExchangeStatus , pub padding: [ u8 ; 17 ], } Figure 12.2: programs/drift/src/state/state.rs#L8L33 pub fn handle_update_exchange_status ( ctx: Context <AdminUpdateState>, exchange_status: ExchangeStatus , ) -> Result <()> { ctx.accounts.state.exchange_status = exchange_status; Ok (()) } Figure 12.3: programs/drift/src/instructions/admin.rs#L1917L1923 #[access_control( withdraw_not_paused (&ctx.accounts.state) )] pub fn handle_transfer_deposit ( ctx: Context <TransferDeposit>, market_index: u16 , amount: u64 , ) -> anchor_lang :: Result <()> { Figure 12.4: programs/drift/src/instructions/user.rs#L466L473 #[access_control( market_valid(&ctx.accounts.perp_market) funding_not_paused (&ctx.accounts.state) valid_oracle_for_perp_market(&ctx.accounts.oracle, &ctx.accounts.perp_market) )] pub fn handle_update_funding_rate ( ctx: Context <UpdateFundingRate>, perp_market_index: u16 , ) -> Result <()> { ... let is_updated = controller::funding::update_funding_rate( perp_market_index, perp_market, & mut oracle_map, now, &state.oracle_guard_rails, matches! (state.exchange_status, ExchangeStatus::FundingPaused ), None , )?; ... } Figure 12.5: programs/drift/src/instructions/keeper.rs#L1027L1078 The Medium post describing the incident that occurred around May 11, 2022 suggests that the exchanges pausing mechanisms contributed to the incidents subsequent fallout: The protocol did not have a kill-switch where only withdrawals were halted. The protocol was paused in the second pause to prevent a further drain of user funds This suggests that the pausing mechanisms should receive heightened attention to reduce the damage should another incident occur. Exploit Scenario Mallory tricks an administrator into pausing funding after withdrawals have already been paused. By pausing funding, the administrator unwittingly unpauses withdrawals. Recommendations Short term:    Represent the exchanges status as a set of ags. This will allow individual operations to be paused independently of one another. Ensure exchange statuses map cleanly to the operations that can be paused. Add documentation where there is potential for confusion. This will help ensure developers check the proper exchange statuses. Adopt a single approach for checking the exchanges status and apply it consistently throughout the codebase. If an exception must be made for a check, explain why in a comment near that check. Adopting such a policy will reduce the likelihood that a missing check goes unnoticed. Long term, periodically review the exchange status checks. Since the exchange status checks represent a form of access control, they deserve heightened scrutiny. Moreover, the exchanges pausing mechanisms played a role in past incidents.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Spot market access controls are incomplete ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "Functions in admin.rs involving perpetual markets verify that the market is valid, i.e., not delisted (gure 13.1). However, functions involving spot markets do not include such checks (e.g., gure 13.2). Drift Protocol has indicated that the spot market implementation is incomplete. #[access_control( market_valid(&ctx.accounts.perp_market) )] pub fn handle_update_perp_market_expiry ( ctx: Context <AdminUpdatePerpMarket>, expiry_ts: i64 , ) -> Result <()> { Figure 13.1: programs/drift/src/instructions/admin.rs#L676L682 _ pub fn handle_update_spot_market_expiry ( ctx: Context <AdminUpdateSpotMarket>, expiry_ts: i64 , ) -> Result <()> { Figure 13.2: programs/drift/src/instructions/admin.rs#L656L660 A similar example concerning whether the exchange is paused appears in gure 13.3 and 13.4. #[access_control( exchange_not_paused(&ctx.accounts.state) )] pub fn handle_place_perp_order (ctx: Context <PlaceOrder>, params: OrderParams ) -> Result <()> { Figure 13.3: programs/drift/src/instructions/user.rs#L687L690 _ pub fn handle_place_spot_order (ctx: Context <PlaceOrder>, params: OrderParams ) -> Result <()> { Figure 13.4: programs/drift/src/instructions/user.rs#L1022L1023 Exploit Scenario Mallory tricks an administrator into making a call that re-enables an expiring spot market. Mallory prots by trading against the should-be-expired spot market. Recommendations Short term, add the missing access controls to the spot market functions in admin.rs . This will ensure that an administrator cannot accidentally perform an operation on an expired spot market. Long term, add tests to verify that each function involving spot markets fails when invoked on an expired spot market. This will increase condence that the access controls have been implemented correctly.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "14. Oracles can be invalid in at most one way ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The Drift Protocol codebase represents oracle validity using an enum , which does not allow an oracle to be invalid in more than one way. Furthermore, the code that determines an oracles validity imposes an implicit hierarchy on the ways an oracle could be invalid. This design is fragile and likely to cause future problems. The OracleValidity enum is shown in gure 14.1, and the code that determines an oracles validity is shown in gure 14.2. Note that if an oracle is, for example, both too volatile and too uncertain, the oracle will be labeled simply TooVolatile . A caller that does not account for this fact and simply checks whether an oracle is TooUncertain could overlook oracles that are both too volatile and too uncertain. pub enum OracleValidity { Invalid, TooVolatile, TooUncertain, StaleForMargin, InsufficientDataPoints, StaleForAMM, Valid, } Figure 14.1: programs/drift/src/math/oracle.rs#L21L29 pub fn oracle_validity ( last_oracle_twap: i64 , oracle_price_data: & OraclePriceData , valid_oracle_guard_rails: & ValidityGuardRails , ) -> DriftResult <OracleValidity> { ... let oracle_validity = if is_oracle_price_nonpositive { OracleValidity::Invalid } else if is_oracle_price_too_volatile { OracleValidity::TooVolatile } else if is_conf_too_large { OracleValidity::TooUncertain } else if is_stale_for_margin { OracleValidity::StaleForMargin } else if !has_sufficient_number_of_data_points { OracleValidity::InsufficientDataPoints } else if is_stale_for_amm { OracleValidity::StaleForAMM } else { OracleValidity::Valid }; Ok (oracle_validity) } Figure 14.2: programs/drift/src/math/oracle.rs#L163L230 Exploit Scenario Alice, a Drift Protocol developer, is unaware of the implicit hierarchy among the OracleValidity variants. Alice writes code like oracle_validity != OracleValidity::TooUncertain and unknowingly introduces a bug into the codebase. Recommendations Short term, represent oracle validity as a set of ags. This will allow oracles to be invalid in more than one way, which will result in more robust and maintainable code. Long term, thoroughly test all code that relies on oracle validity. This will help ensure the codes correctness following the aforementioned change.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Code duplication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "Various les in the programs/drift directory contain duplicate code, which can lead to incomplete xes or inconsistent behavior (e.g., because the code is modied in one location but not all). As an example, the code in gure 15.1 appears nearly verbatim in the functions liquidate_perp , liquidate_spot , liquidate_borrow_for_perp_pnl , and liquidate_perp_pnl_for_deposit . // check if user exited liquidation territory let (intermediate_total_collateral, intermediate_margin_requirement_with_buffer) = if !canceled_order_ids.is_empty() || lp_shares > 0 { ... // 37 lines ( intermediate_total_collateral, intermediate_margin_requirement_plus_buffer, ) } else { (total_collateral, margin_requirement_plus_buffer) }; Figure 15.1: programs/drift/src/controller/liquidation.rs#L201L246 In some places, the text itself is not obviously duplicated, but the logic it implements is clearly duplicated. An example appears in gures 15.2 and 15.3. Such logical code duplication suggests the code does not use the right abstractions. // Update Market open interest if let PositionUpdateType::Open = update_type { if position.quote_asset_amount == 0 && position.base_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_add( 1 )?; } market.number_of_users_with_base = market.number_of_users_with_base.safe_add( 1 )?; } else if let PositionUpdateType::Close = update_type { if new_base_asset_amount == 0 && new_quote_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_sub( 1 )?; } market.number_of_users_with_base = market.number_of_users_with_base.safe_sub( 1 )?; } Figure 15.2: programs/drift/src/controller/position.rs#L162L175 if position.quote_asset_amount == 0 && position.base_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_add( 1 )?; } position.quote_asset_amount = position.quote_asset_amount.safe_add(delta)?; market.amm.quote_asset_amount = market.amm.quote_asset_amount.safe_add(delta.cast()?)?; if position.quote_asset_amount == 0 && position.base_asset_amount == 0 { market.number_of_users = market.number_of_users.safe_sub( 1 )?; } Figure 15.3: programs/drift/src/controller/position.rs#L537L547 Exploit Scenario Alice, a Drift Protocol developer, is asked to x a bug in liquidate_perp . Alice does not realize that the bug also applies to liquidate_spot , liquidate_borrow_for_perp_pnl , and liquidate_perp_pnl_for_deposit , and xes the bug in only liquidate_perp . Eve discovers that the bug is not xed in one of the other three functions and exploits it. Recommendations Short term:   Refactor liquidate_perp , liquidate_spot , liquidate_borrow_for_perp_pnl , and liquidate_perp_pnl_for_deposit to eliminate the code duplication. This will reduce the likelihood of an incomplete x for a bug aecting more than one of these functions. Identify cases where the code uses the same logic, and implement abstractions to capture that logic. Ensure that code that relies on such logic uses the new abstractions. Consolidating similar pieces of code will make the overall codebase easier to reason about. Long term, adopt code practices that discourage code duplication. This will help to prevent this problem from recurring.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "16. Inconsistent use of integer types ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The Drift Protocol codebase uses integer types inconsistently; data of similar kinds is represented using dierently sized types or types with dierent signedness. Conversions from one integer type to another present an opportunity for the contracts to fail and should be avoided. For example, the pow method expects a u32 argument. However, in some places u128 values must be cast to u32 values, even though those values are intended to be used as exponents (gures 16.1, 16.2, and 16.3). let expo_diff = (spot_market.insurance_fund.shares_base - insurance_fund_stake.if_base) . cast::< u32 >() ?; let rebase_divisor = 10_ u128 .pow(expo_diff); Figure 16.1: programs/drift/src/controller/insurance.rs#L154L157 #[zero_copy] #[derive(Default, Eq, PartialEq, Debug)] #[repr(C)] pub struct InsuranceFund { pub vault: Pubkey , pub total_shares: u128 , pub user_shares: u128 , pub shares_base: u128 , pub unstaking_period: i64 , // if_unstaking_period pub last_revenue_settle_ts: i64 , pub revenue_settle_period: i64 , pub total_factor: u32 , // percentage of interest for total insurance pub user_factor: u32 , // percentage of interest for user staked insurance // exponent for lp shares (for rebasing) } Figure 16.2: programs/drift/src/state/spot_market.rs#L352L365 #[account(zero_copy)] #[derive(Default, Eq, PartialEq, Debug)] #[repr(C)] pub struct InsuranceFundStake { pub authority: Pubkey , if_shares: u128 , pub last_withdraw_request_shares: u128 , // get zero as 0 when not in escrow pub if_base: u128 , // exponent for if_shares decimal places (for rebase) pub last_valid_ts: i64 , pub last_withdraw_request_value: u64 , pub last_withdraw_request_ts: i64 , pub cost_basis: i64 , pub market_index: u16 , pub padding: [ u8 ; 14 ], } Figure 16.3: programs/drift/src/state/insurance_fund_stake.rs#L10L24 The following command reveals 689 locations where the cast method appears to be used: grep -r -I '\\.cast\\>' programs/drift Each such use could lead to a denial of service if an attacker puts the contract into a state where the cast always errors. Many of these uses could be eliminated by more consistent use of integer types. Note that Drift Protocol has indicated that some of the observed inconsistencies are related to reducing rent costs. Exploit Scenario Mallory manages to put the contract into a state such that one of the nearly 700 uses of cast always returns an error. The contract becomes unusable for Alice, who needs to execute a code path involving the vulnerable cast . Recommendations Short term, review all uses of cast to see which might be eliminated by changing the types of the operands. This will reduce the overall number of cast s and reduce the likelihood that one could lead to denial of service. Long term, as new code is introduced into the codebase, review the types used to hold similar kinds of data. This will reduce the likelihood that new cast s are needed.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. Use of opaque constants in tests ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "Several of the Drift Protocol tests use constants with no explanation for how they were derived, which makes it dicult to assess whether the tests are functioning correctly. Ten examples appear in gure 17.1. In each case, a variable or eld is compared against a constant consisting of 612 random-looking digits. Without an explanation for how these digits were obtained, it is dicult to tell whether the constant expresses the correct value. assert_eq! (user.spot_positions[ 0 ].scaled_balance, 45558159000 ); assert_eq! (user.spot_positions[ 1 ].scaled_balance, 406768999 ); ... assert_eq! (margin_requirement, 44744590 ); assert_eq! (total_collateral, 45558159 ); assert_eq! (margin_requirement_plus_buffer, 45558128 ); ... assert_eq! (token_amount, 406769 ); assert_eq! (token_value, 40676900 ); assert_eq! (strict_token_value_1, 4067690 ); // if oracle price is more favorable than twap ... assert_eq! (liquidator.spot_positions[ 0 ].scaled_balance, 159441841000 ); ... assert_eq! (liquidator.spot_positions[ 1 ].scaled_balance, 593824001 ); Figure 17.1: programs/drift/src/controller/liquidation/tests.rs#L1618L1687 Exploit Scenario Mallory discovers that a constant used in a Drift Protocol test was incorrectly derived and that the tests were actually verifying incorrect behavior. Mallory uses the bug to siphon funds from the Drift Protocol exchange. Recommendations Short term, where possible, compute values using an explicit formula rather than an opaque constant. If using an explicit formula is not possible, include a comment explaining how the constant was derived. This will help to ensure that correct behavior is being tested for. Moreover, the process of giving such explicit formulas could reveal errors. Long term, write scripts to identify constants with high entropy, and run those scripts as part of your CI process. This will help to ensure the aforementioned standards are maintained.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Accounts from contexts are not always used by the instruction ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The context denition for the initialize instruction denes a drift_signer account. However, this account is not used by the instruction. It appears to be a remnant used to pass the address of the state PDA account; however, the need to do this was eliminated by the use of find_program_address to calculate the address. Also, in the initialize_insurance_fund_stake instruction, the spot_market , user_stats , and state accounts from the context are not used by the instruction. #[derive(Accounts)] pub struct Initialize <'info> { #[account(mut)] pub admin: Signer <'info>, #[account( init, seeds = [b \"drift_state\" .as_ref()], space = std::mem::size_of::<State>() + 8, bump, payer = admin )] pub state: Box <Account<'info, State>>, pub quote_asset_mint: Box <Account<'info, Mint>>, /// CHECK: checked in `initialize` pub drift_signer: AccountInfo <'info>, pub rent: Sysvar <'info, Rent>, pub system_program: Program <'info, System>, pub token_program: Program <'info, Token>, } Figure 18.1: programs/drift/src/instructions/admin.rs#L1989L2007 Exploit Scenario Alice, a Drift Protocol developer, assumes that the drift_signer account is used by the instruction, and she uses a dierent address for the account, expecting this account to hold the contract state after the initialize instruction has been called. Recommendations Short term, remove the unused account from the context. This eliminates the possibility of confusion around the use of the accounts. Long term, employ a process where a refactoring of an instructions code is followed by a review of the corresponding context denition. This ensures that the context is in sync with the instruction handlers.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "19. Unaligned references are allowed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "The Drift Protocol codebase uses the #![allow(unaligned_references)] attribute. This allows the use of unaligned references throughout the program and could mask serious problems in future updates to the contract. #![allow(clippy::too_many_arguments)] #![allow(unaligned_references)] #![allow(clippy::bool_assert_comparison)] #![allow(clippy::comparison_chain)] Figure 19.1: programs/drift/src/lib.rs#L1L4 Exploit Scenario Alice, a Drift Protocol developer, accidentally introduces errors caused by the use of unaligned references, aecting the contract operation and leading to a loss of funds. Recommendations Short term, remove the attributes. This ensures that the check for unaligned references correctly ag such cases. Long term, be conservative with the use of attributes used to suppress warnings or errors throughout the codebase. If possible, apply them to only the minimum possible amount of code. This minimizes the risk of problems stemming from the suppressed checks.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "20. Size of created accounts derived from in-memory representation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-12-driftlabs-driftprotocol-securityreview.pdf", "body": "When state accounts are initialized, the size of the account is set to std::mem::size_of::<ACCOUNT_TYPE>() + 8 , where the eight extra bytes are used for the discriminator. The structs for the state types all have a trailing eld with padding, seemingly to ensure the account size is aligned to eight bytes and to determine the size of the account. In other places, the code relies on the size_of function to determine the type of accounts passed to the instruction. While we could not nd any security-related problem with the scheme today, this does mean that every accounts in-memory representation is inated by the amount of padding, which could become a problem with respect to the limitation of the stack or heap size. Furthermore, if any of the accounts are updated in such a way that the repr(C) layout size diers from the Anchor space reference , it could cause a problem. For example, if the SpotMarket struct is changed so that its in-memory representation is smaller than the required Anchor size, the initialize_spot_market would fail because the created account would be too small to hold the serialized representation of the data. #[account] #[derive(Default)] #[repr(C)] pub struct State { pub admin: Pubkey , pub whitelist_mint: Pubkey , pub discount_mint: Pubkey , pub signer: Pubkey , pub srm_vault: Pubkey , pub perp_fee_structure: FeeStructure , pub spot_fee_structure: FeeStructure , pub oracle_guard_rails: OracleGuardRails , pub number_of_authorities: u64 , pub number_of_sub_accounts: u64 , pub lp_cooldown_time: u64 , pub liquidation_margin_buffer_ratio: u32 , pub settlement_duration: u16 , pub number_of_markets: u16 , pub number_of_spot_markets: u16 , pub signer_nonce: u8 , pub min_perp_auction_duration: u8 , pub default_market_order_time_in_force: u8 , pub default_spot_auction_duration: u8 , pub exchange_status: ExchangeStatus , pub padding : [ u8 ; 17 ], } Figure 20.1: The State struct, with corresponding padding #[account( init, seeds = [b \"drift_state\" .as_ref()], space = std::mem::size_of::<State>() + 8 , bump, payer = admin )] pub state: Box <Account<'info, State>>, Figure 20.2: The creation of the State account, using the in-memory size if data.len() < std::mem::size_of::<UserStats>() + 8 { return Ok (( None , None )); } Figure 20.3: An example of the in-memory size used to determine the account type Exploit Scenario Alice, a Drift Protocol developer, unaware of the implicit requirements of the in-memory size, makes changes to a state accounts structure or adds a state structure account such that the in-memory size is smaller than the size needed for the serialized data. As a result, instructions in the contract that save data to the account will fail. Recommendations Short term, add an implementation to each state struct that returns the size to be used for the corresponding Solana account. This avoids the overhead of the padding and removes the dependency on assumption about the in-memory size. Long term, avoid using assumptions about in-memory representation of type within programs created in Rust. This ensures that changes to the representation do not aect the program's operation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Integer overow in Peggo's deploy-erc20-raw command ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The denom-decimals argument of the deploy-erc20-raw command (in the deployERC20RawCmd function) may experience an integer overow. The argument is rst parsed into a value of the int type by the strconv.Atoi function and then cast to a value of the uint8 type (gure 1.1). If the denom-decimals argument with which deploy-erc20-raw is invoked is a negative value or a value that is too large, the casting operation will cause an overow; however, the user will not receive an error, and the execution will proceed with the overow value. func deployERC20RawCmd() *cobra.Command { return &cobra.Command{ Use: \"deploy-erc20-raw [gravity-addr] [denom-base] [denom-name] [denom-symbol] [denom-decimals]\" , /* (...) */ , RunE: func (cmd *cobra.Command, args [] string ) error { denomDecimals, err := strconv.Atoi(args[ 4 ]) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } tx, err := gravityContract.DeployERC20(auth, denomBase, denomName, denomSymbol, uint8 (denomDecimals) ) Figure 1.1: peggo/cmd/peggo/bridge.go#L348-L353 We identied this issue by running CodeQL's IncorrectIntegerConversionQuery.ql query. Recommendations Short term, x the integer overow in Peggos deployERC20RawCmd function by using the strconv.ParseUint function to parse the denom-decimals argument. To do this, use the patch in gure 1.2. diff --git a/cmd/peggo/bridge.go b/cmd/peggo/bridge.go index 49aabc5..4b3bc6a 100644 --- a/cmd/peggo/bridge.go +++ b/cmd/peggo/bridge.go @@ -345,7 +345,7 @@ network starting.`, - + denomBase := args[ 1 ] denomName := args[ 2 ] denomSymbol := args[ 3 ] denomDecimals, err := strconv.Atoi(args[ 4 ]) denomDecimals, err := strconv.ParseUint(args[ 4 ], 10 , 8 ) if err != nil { return fmt.Errorf( \"invalid denom decimals: %w\" , err) } Figure 1.2: A patch for the integer overow issue in Peggo's deploy-erc20-raw command Long term, integrate CodeQL into the CI/CD pipeline to nd similar issues in the future.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Rounding of the standard deviation value may deprive voters of rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The ExchangeRateBallot.StandardDeviation function calculates the standard deviation of the exchange rates submitted by voters. To do this, it converts the variance into a oat, prints its square root to a string, and parses it into a Dec value (gure 2.1). This logic rounds down the standard deviation value, which is likely unexpected behavior; if the exchange rate is within the reward spread value, voters may not receive the rewards they are owed. The rounding operation is performed by the fmt.Sprintf(\"%f\", floatNum) function, which, as shown in Appendix C , may cut o decimal places from the square root value. // StandardDeviation returns the standard deviation by the power of the ExchangeRateVote. func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { // (...) variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { /* (...) */ } floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { /* (...) */ } return standardDeviation, nil } Figure 2.1: Inaccurate oat conversions ( umee/x/oracle/types/ballot.go#L89-L97 ) Exploit Scenario A voter reports a price that should be within the reward spread. However, because the standard deviation value is rounded, the price is not within the reward spread, and the voter does not receive a reward. Recommendations Short term, have the ExchangeRateBallot.StandardDeviation function use the Dec.ApproxSqrt method to calculate the standard deviation instead of parsing the variance into a oat, calculating the square root, and parsing the formatted oat back into a value of the Dec type. That way, users who vote for exchange rates close to the correct reward spread will receive the rewards they are owed. Figure 2.2 shows a patch for this issue. diff --git a/x/oracle/types/ballot.go b/x/oracle/types/ballot.go index 6b201c2..9f6b579 100644 --- a/x/oracle/types/ballot.go +++ b/x/oracle/types/ballot.go @@ -1,12 +1,8 @@ package types import ( - - - - - + ) \"fmt\" \"math\" \"sort\" \"strconv\" sdk \"github.com/cosmos/cosmos-sdk/types\" \"sort\" // VoteForTally is a convenience wrapper to reduce redundant lookup cost. @@ - 88 , 13 + 84 , 8 @@ func (pb ExchangeRateBallot) StandardDeviation() (sdk.Dec, error ) { - - - - + - - variance := sum.QuoInt64( int64 ( len (pb))) floatNum, err := strconv.ParseFloat(variance.String(), 64 ) if err != nil { return sdk.ZeroDec(), err } standardDeviation, err := variance.ApproxSqrt() floatNum = math.Sqrt(floatNum) standardDeviation, err := sdk.NewDecFromStr(fmt.Sprintf( \"%f\" , floatNum)) if err != nil { return sdk.ZeroDec(), err } diff --git a/x/oracle/types/ballot_test.go b/x/oracle/types/ballot_test.go index 0cd09d8..0dd1f1a 100644 --- a/x/oracle/types/ballot_test.go +++ b/x/oracle/types/ballot_test.go @@ - 177 , 21 + 177 , 21 @@ func TestPBStandardDeviation(t *testing.T) { - + - + }, { }, { [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 }, [] int64 { 1 , 1 , 100 , 1 }, [] bool { true , true , true , true }, sdk.NewDecWithPrec( 4999500036300 , OracleDecPrecision), sdk.MustNewDecFromStr( \"49995.000362536252310906\" ), // Adding fake validator doesn't change outcome [] float64 { 1.0 , 2.0 , 10.0 , 100000.0 , 10000000000 }, [] int64 { 1 , 1 , 100 , 1 , 10000 }, [] bool { true , true , true , true , false }, sdk.NewDecWithPrec( 447213595075100600 , OracleDecPrecision), sdk.MustNewDecFromStr( \"4472135950.751005519905537611\" ), // Tie votes [] float64 { 1.0 , 2.0 , 3.0 , 4.0 }, [] int64 { 1 , 100 , 100 , 1 }, - + [] bool { true , true , true , true }, sdk.NewDecWithPrec( 122474500 , OracleDecPrecision), sdk.MustNewDecFromStr( \"1.224744871391589049\" ), }, { // No votes Figure 2.2: A patch for this issue", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Vulnerabilities in exchange rate commitment scheme ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Umee oracle implements a commitment scheme in which users vote on new exchange rates by submitting \"pre-vote\" and \"vote\" messages. However, vulnerabilities in this scheme could allow an attacker to (1) predict the prices to which other voters have committed and (2) send two prices for an asset in a pre-vote message hash and then submit one of the prices in the vote message. (Note that predicting other prices would likely require the attacker to make some correct guesses about those prices.) The rst issue is that the random salt used in the scheme is too short. The salt is generated as two random bytes (gure 3.1) and is later hex-encoded and limited to four bytes (gure 3.2). As a result, an attacker could pre-compute the pre-vote commitment hash of every salt value (and thus the expected exchange rate), eectively violating the hiding property of the scheme. salt, err := GenerateSalt( 2 ) Figure 3.1: The salt-generation code ( umee/price-feeder/oracle/oracle.go#358 ) if len (msg.Salt) > 4 || len (msg.Salt) < 1 { return sdkerrors.Wrap(ErrInvalidSaltLength, \"salt length must be [1, 4]\" ) } Figure 3.2: The salt-validation logic ( umee/x/oracle/types/msgs.go#148150 ) The second issue is the lack of proper salt validation, which would guarantee sucient domain separation between a random salt and the exchange rate when the commitment hash is calculated. The domain separator string consists of a colon character, as shown in gure 3.3. However, there is no verication of whether the salt is a hex-encoded string or whether it contains the separator character; only the length of the salt is validated. This bug could allow an attacker to reveal an exchange rate other than the one the attacker had committed to, violating the binding property of the scheme. func GetAggregateVoteHash(salt string , exchangeRatesStr string , voter sdk.ValAddress) AggregateVoteHash { hash := tmhash.NewTruncated() sourceStr := fmt.Sprintf( \"%s:%s:%s\" , salt, exchangeRatesStr, voter.String() ) Figure 3.3: The generation of a commitment hash ( umee/x/oracle/types/hash.go#2325 ) The last vulnerability in the scheme is the insucient validation of exchange rate strings: the strings undergo unnecessary trimming, and the code checks only that len(denomAmountStr) is less than two (gure 3.4), rather than performing a stricter check to conrm that it is not equal to two. This could allow an attacker to exploit the second bug described in this nding. func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { tuplesStr = strings.TrimSpace(tuplesStr) if len (tuplesStr) == 0 { return nil , nil } tupleStrs := strings.Split(tuplesStr, \",\" ) // (...) for i, tupleStr := range tupleStrs { denomAmountStr := strings.Split(tupleStr, \":\" ) if len (denomAmountStr) < 2 { return nil , fmt.Errorf( \"invalid exchange rate %s\" , tupleStr) } } // (...) } Figure 3.4: The code that parses exchange rates ( umee/x/oracle/types/vote.go#7286 ) Exploit Scenario The maximum salt length of two is increased. During a subsequent pre-voting period, a malicious validator submits the following commitment hash: sha256(\"whatever:UMEE:123:UMEE:456,USDC:789:addr\") . (Note that  represents a normal whitespace character.) Then, during the voting period, the attacker waits for all other validators to reveal their exchange rates and salts and then chooses the UMEE price that he will reveal ( 123 or 456 ). In this way, the attacker can manipulate the exchange rate to his advantage. If the attacker chooses to reveal a price of 123 , the following will occur: 1. The salt will be set to whatever . 2. The attacker will submit an exchange rate string of UMEE:123:UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever : UMEE:123:UMEE:456,USDC:789 : addr) . 4. The exchange rate will then be parsed as 123/789 (UMEE/USDC). Note that  UMEE = 456 (with its leading whitespace character) will be ignored. This is because of the insucient validation of exchange rate strings (as described above) and the fact that only the rst and second items of denomAmountStr are used. (See the screenshot in Appendix D). If the attacker chooses to reveal a price of 456 , the following will occur: 1. The salt will be set to whatever:UMEE:123 . 2. The exchange rate string will be set to UMEE:456,USDC:789 . 3. The value will be hashed as sha256( whatever:UMEE:123 : UMEE:456,USDC:789 : addr) . 4. Because exchange rate strings undergo space trimming, the exchange rate will be parsed as 456/789 (UMEE/USDC). Recommendations Short term, take the following steps:    Increase the salt length to prevent brute-force attacks. To ensure a security level of X bits, use salts of 2*X random bits. For example, for a 128-bit security level, use salts of 256 bits (32 bytes). Ensure domain separation by implementing validation of a salts format and accepting only hex-encoded strings. Implement stricter validation of exchange rates by ensuring that every exchange rate substring contains exactly one colon character and checking whether all denominations are included in the list of accepted denominations; also avoid trimming whitespaces at the beginning of the parsing process. Long term, consider replacing the truncated SHA-256 hash function with a SHA-512/256 or HMAC-SHA256 function. This will increase the level of security from 80 bits to about 128, which will help prevent collision and length extension attacks.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. Validators can crash other nodes by triggering an integer overow ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "By submitting a large exchange rate value, a validator can trigger an integer overow that will cause a Go panic and a node crash. The Umee oracle code checks that each exchange rate submitted by a validator is a positive value with a bit size of less than or equal to 256 (gures 4.1 and 4.2). The StandardDeviation method iterates over all exchange rates and adds up their squares (gure 4.3) but does not check for an overow. A large exchange rate value will cause the StandardDeviation method to panic when performing multiplication or addition . func ParseExchangeRateTuples(tuplesStr string ) (ExchangeRateTuples, error ) { // (...) for i, tupleStr := range tupleStrs { // (...) decCoin, err := sdk.NewDecFromStr(denomAmountStr[ 1 ]) // (...) if !decCoin.IsPositive() { return nil , types.ErrInvalidOraclePrice } Figure 4.1: The check of whether the exchange rate values are positive ( umee/x/oracle/types/vote.go#L71-L96 ) func (msg MsgAggregateExchangeRateVote) ValidateBasic() error { // (...) exchangeRates, err := ParseExchangeRateTuples(msg.ExchangeRates) if err != nil { /* (...) - returns wrapped error */ } for _, exchangeRate := range exchangeRates { // check overflow bit length if exchangeRate.ExchangeRate.BigInt().BitLen() > 255 +sdk.DecimalPrecisionBits // (...) - returns error Figure 4.2: The check of the exchange rate values bit lengths ( umee/x/oracle/types/msgs.go#L136-L146 ) sum := sdk.ZeroDec() for _, v := range pb { deviation := v.ExchangeRate.Sub(median) sum = sum.Add(deviation.Mul(deviation)) } Figure 4.3: Part of the StandardDeviation method ( umee/x/oracle/types/ballot.go#8387 ) The StandardDeviation method is called by the Tally function, which is called in the EndBlocker function. This means that an attacker could trigger an overow remotely in another validator node. Exploit Scenario A malicious validator commits to and then sends a large UMEE exchange rate value. As a result, all validator nodes crash, and the Umee blockchain network stops working. Recommendations Short term, implement overow checks for all arithmetic operations involving exchange rates. Long term, use fuzzing to ensure that no other parts of the code are vulnerable to overows.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "5. The repayValue variable is not used after being modied ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Keeper.LiquidateBorrow function uses the local variable repayValue to calculate the repayment.Amount value. If repayValue is greater than or equal to maxRepayValue , it is changed to that value. However, the repayValue variable is not used again after being modied, which suggests that the modication could be a bug or a code quality issue. func (k Keeper) LiquidateBorrow( // (...) // repayment cannot exceed borrowed value * close factor maxRepayValue := borrowValue.Mul(closeFactor) repayValue, err := k.TokenValue(ctx, repayment) if err != nil { return sdk.ZeroInt(), sdk.ZeroInt(), err } if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue).Quo( repayValue ).TruncateInt() repayValue = maxRepayValue } // (...) Figure 5.1: umee/x/leverage/keeper/keeper.go#L446-L456 We identied this issue by running CodeQL's DeadStoreOfLocal.ql query. Recommendations Short term, review and x the repayValue variable in the Keeper.LiquidateBorrow function, which is not used after being modied, to prevent related issues in the future.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "6. Inconsistent error checks in GetSigners methods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The GetSigners methods in the x/oracle and x/leverage modules exhibit dierent error-handling behavior when parsing strings into validator or account addresses. The GetSigners methods in the x/oracle module always panic upon an error, while the methods in the x/leverage module explicitly ignore parsing errors. Figures 6.1 and 6.2 show examples of the GetSigners methods in those modules. We set the severity of this nding to informational because message addresses parsed in the x/leverage modules GetSigners methods are also validated in the ValidateBasic methods. As a result, the issue is not currently exploitable. // GetSigners implements sdk.Msg func (msg MsgDelegateFeedConsent) GetSigners() []sdk.AccAddress { operator, err := sdk.ValAddressFromBech32(msg.Operator) if err != nil { panic (err) } return []sdk.AccAddress{sdk.AccAddress(operator)} } Figure 6.1: umee/x/oracle/types/msgs.go#L174-L182 func (msg *MsgLendAsset) GetSigners() []sdk.AccAddress { lender, _ := sdk.AccAddressFromBech32(msg.GetLender()) return []sdk.AccAddress{lender} } Figure 6.2: umee/x/leverage/types/tx.go#L30-L33 Recommendations Short term, use a consistent error-handling process in the x/oracle and x/leverage modules GetSigners methods. The x/leverage module's GetSigners functions should handle errors in the same way that the x/oracle methods doby panicking.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Incorrect price assumption in the GetExchangeRateBase function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "If the denominator string passed to the GetExchangeRateBase function contains the substring USD (gure 7.1), the function returns 1 , presumably to indicate that the denominator is a stablecoin. If the system accepts an ERC20 token that is not a stablecoin but has a name containing USD, the system will report an incorrect exchange rate for the asset, which may enable token theft. Moreover, the price of an actual USD stablecoin may vary from USD 1. Therefore, if a stablecoin used as collateral for a loan loses its peg, the loan may not be liquidated correctly. // GetExchangeRateBase gets the consensus exchange rate of an asset // in the base denom (e.g. ATOM -> uatom) func (k Keeper) GetExchangeRateBase(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if strings.Contains(strings.ToUpper(denom), types.USDDenom) { return sdk.OneDec(), nil } // (...) Figure 7.1: umee/x/oracle/keeper/keeper.go#L89-L94 func (k Keeper) TokenPrice(ctx sdk.Context, denom string ) (sdk.Dec, error ) { if !k.IsAcceptedToken(ctx, denom) { return sdk.ZeroDec(), sdkerrors.Wrap(types.ErrInvalidAsset, denom) } price, err := k.oracleKeeper.GetExchangeRateBase(ctx, denom) // (...) return price, nil } Figure 7.2: umee/x/leverage/keeper/oracle.go#L12-L34 Exploit Scenario Umee adds the cUSDC ERC20 token as an accepted token. Upon its addition, its price is USD 0.02, not USD 1. However, because of the incorrect price assumption, the system sets its price to USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Exploit Scenario 2 The price of a stablecoin drops signicantly. However, the x/leverage module fails to detect the change and reports the price as USD 1. This enables an attacker to create an undercollateralized loan and to draw funds from the system. Recommendations Short term, remove the condition that causes the GetExchangeRateBase function to return a price of USD 1 for any asset whose name contains USD.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "8. Oracle price-feeder is vulnerable to manipulation by a single malicious price feed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The price-feeder component uses a volume-weighted average price (VWAP) formula to compute average prices from various third-party providers. The price it determines is then sent to the x/oracle module, which commits it on-chain. However, an asset price could easily be manipulated by only one compromised or malfunctioning third-party provider. Exploit Scenario Most validators are using the Binance API as one of their price providers. The API is compromised by an attacker and suddenly starts to report prices that are much higher than those reported by other providers. However, the price-feeder instances being used by the validators do not detect the discrepancies in the Binance API prices. As a result, the VWAP value computed by the price-feeder and committed on-chain is much higher than it should be. Moreover, because most validators have committed the wrong price, the average computed on-chain is also wrong. The attacker then draws funds from the system. Recommendations Short term, implement a price-feeder mechanism for detecting the submission of wildly incorrect prices by a third-party provider. Have the system temporarily disable the use of the malfunctioning provider(s) and issue an alert calling for an investigation. If it is not possible to automatically identify the malfunctioning provider(s), stop committing prices. (Note, though, that this may result in a loss of interest for validators.) Consider implementing a similar mechanism in the x/oracle module so that it can identify when the exchange rates committed by validators are too similar to one another or to old values. References  Synthetix Response to Oracle Incident", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "9. Oracle rewards may not be distributed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "If the x/oracle module lacks the coins to cover a reward payout, the rewards will not be distributed or registered for payment in the future. var periodRewards sdk.DecCoins for _, denom := range rewardDenoms { rewardPool := k.GetRewardPool(ctx, denom) // return if there's no rewards to give out if rewardPool.IsZero() { continue } periodRewards = periodRewards.Add(sdk.NewDecCoinFromDec( denom, sdk.NewDecFromInt(rewardPool.Amount).Mul(distributionRatio), )) } Figure 9.1: A loop in the code that calculates oracle rewards ( umee/x/oracle/keeper/reward.go#4356 ) Recommendations Short term, document the fact that oracle rewards will not be distributed when the x/oracle module does not have enough coins to cover the rewards.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "10. Risk of server-side request forgery attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The price-feeder sends HTTP requests to congured providers APIs. If any of the HTTP responses is a redirect response (e.g., one with HTTP response code 301), the module will automatically issue a new request to the address provided in the responses header. The new address may point to a local address, potentially one that provides access to restricted services. Exploit Scenario An attacker gains control over the Osmosis API. He changes the endpoint used by the price-feeder such that it responds with a redirect like that shown in gure 10.1, with the goal of removing a transaction from a Tendermint validators mempool. The price-feeder automatically issues a new request to the Tendermint REST API. Because the API does not require authentication and is running on the same machine as the price-feeder , the request is successful, and the target transaction is removed from the validator's mempool. HTTP/1.1 301 Moved Permanently Location: http://localhost:26657/remove_tx?txKey=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa Figure 10.1: The redirect response Recommendations Short term, use a function such as CheckRedirect to disable redirects, or at least redirects to local services, in all HTTP clients.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "11. Incorrect comparison in SetCollateralSetting method ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Umee users can send a SetCollateral message to disable the use of a certain asset as collateral. The messages are handled by the SetCollateralSetting method (gure 11.1), which should ensure that the borrow limit will not drop below the amount borrowed. However, the function uses an incorrect comparison, checking that the borrow limit will be greater than, not less than, that amount. // Return error if borrow limit would drop below borrowed value if newBorrowLimit.GT(borrowedValue) { return sdkerrors.Wrap(types.ErrBorrowLimitLow, newBorrowLimit.String()) } Figure 11.1: The incorrect comparison in the SetCollateralSetting method ( umee/x/leverage/keeper/keeper.go#343346 ) Exploit Scenario An attacker provides collateral to the Umee system and borrows some coins. Then the attacker disables the use of the collateral asset; because of the incorrect comparison in the SetCollateralSetting method, the disable operation succeeds, and the collateral is sent back to the attacker. Recommendations Short term, correct the comparison in the SetCollateralSetting method. Long term, implement tests to check whether basic functionality works as expected.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "12. Voters ability to overwrite their own pre-votes is not documented ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The x/oracle module allows voters to submit more than one pre-vote message during the same pre-voting period, overwriting their previous pre-vote messages (gure 12.1). This feature is not documented; while it does not constitute a direct security risk, it may be unintended behavior. Third parties may incorrectly assume that validators cannot change their pre-vote messages. Monitoring systems may detect only the rst pre-vote event for a validators pre-vote messages, while voters may trust the exchange rates and salts revealed by other voters to be nal. On the other hand, this feature may be an intentional one meant to allow voters to update the exchange rates they submit as they obtain more accurate pricing information. func (ms msgServer) AggregateExchangeRatePrevote( goCtx context.Context, msg *types.MsgAggregateExchangeRatePrevote, ) (*types.MsgAggregateExchangeRatePrevoteResponse, error ) { // (...) aggregatePrevote := types.NewAggregateExchangeRatePrevote(voteHash, valAddr, uint64 (ctx.BlockHeight())) // This call overwrites previous pre-vote if there was one ms.SetAggregateExchangeRatePrevote(ctx, valAddr, aggregatePrevote) ctx.EventManager().EmitEvents(sdk.Events{ // (...) - emit EventTypeAggregatePrevote and EventTypeMessage }) return &types.MsgAggregateExchangeRatePrevoteResponse{}, nil } Figure 12.1: umee/x/oracle/keeper/msg_server.go#L23-L66 Recommendations Short term, document the fact that a pre-vote message can be submitted and overwritten in the same voting period. Alternatively, disallow this behavior by having the AggregateExchangeRatePrevote function return an error if a validator attempts to submit an additional exchange rate pre-vote message. Long term, add tests to check for this behavior.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Lack of user-controlled limits for input amount in LiquidateBorrow ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The x/leverage modules LiquidateBorrow function computes the amount of funds that will be transferred from the module to the functions caller in a liquidation. The computation uses asset prices retrieved from an oracle. There is no guarantee that the amount returned by the module will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to LiquidateBorrow . Adding a lower limit to the amount sent by the module would enable the caller to explicitly state his or her assumptions about the liquidation and to ensure that the collateral payout is as protable as expected. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls the LiquidateBorrow function. Due to an oracle malfunction, the amount of collateral transferred from the module is much lower than the amount she would receive on another market. Recommendations Short term, introduce a minRewardAmount parameter and add a check verifying that the reward value is greater than or equal to the minRewardAmount value. Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a module and an upper limit for a transfer of the callers funds to a module.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "14. Lack of simulation and fuzzing of leverage module invariants ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Umee system lacks comprehensive Cosmos SDK simulations and invariants for its x/oracle and x/leverage modules. More thorough use of the simulation feature would facilitate fuzz testing of the entire blockchain and help ensure that the invariants hold. Additionally, the current simulation module may need to be modied for the following reasons:     It exits on the rst transaction error . To avoid an early exit, it could skip transactions that are expected to fail when they are generated; however, that could also cause it to skip logic that contains issues. The numKeys argument , which determines how many accounts it will use, can range from 2 to 2,500. Using too many accounts may hinder the detection of bugs that require multiple transactions to be executed by a few accounts. By default, it is congured to use a \"stake\" currency , which may not be used in the nal Umee system. Running it with a small number of accounts and a large block size for many blocks could quickly cause all validators to be unbonded. To avoid this issue, the simulation would need the ability to run for a longer time. attempted to use the simulation module by modifying the recent changes to the Umee codebase, which introduce simulations for the x/oracle and x/leverage modules (commit f22b2c7f8e ). We enabled the x/leverage module simulation and modied the Cosmos SDK codebase locally so that the framework would use fewer accounts and log errors via Fatalf logs instead of exiting. The framework helped us nd the issue described in TOB-UMEE-15 , but the setup and tests we implemented were not exhaustive. We sent the codebase changes we made to the Umee team via an internal chat. Recommendations Short term, identify, document, and test all invariants that are important for the systems security, and identify and document the arbitrage opportunities created by the system. Enable simulation of the x/oracle and x/leverage modules and ensure that the following assertions and invariants are checked during simulation runs: 1. In the UpdateExchangeRates function , the token supply value corresponds to the uToken supply value. Implement the following check: if uTokenSupply != 0 { assert(tokenSupply != 0) } 2. In the LiquidateBorrow function (after the line  if !repayment.Amount.IsPositive()  ) , the following comparisons evaluate to true: ExchangeUToken(reward) == EquivalentTokenValue(repayment, baseRewardDenom) TokenValue(ExchangeUToken(ctx, reward)) == TokenValue(repayment) borrowed.AmountOf(repayment.Denom) >= repayment.Amount collateral.AmountOf(rewardDenom) >= reward.Amount module's collateral amount >= reward.Amount repayment <= desiredRepayment 3. The x/leverage module is never signicantly undercollateralized at the end of a transaction. Implement a check, total collateral value * X >= total borrows value , in which X is close to 1. (It may make sense for the value of X to be greater than or equal to 1 to account for module reserves.) It may be acceptable for the module to be slightly undercollateralized, as it may mean that some liquidations have yet to be executed. 4. The amount of reserves remains above a certain minimum value, or new loans cannot be issued if the amount of reserves drops below a certain value. 5. The interest on a loan is less than or equal to the borrowing fee. (This invariant is related to the issue described in TOB-UMEE-23 .) 6. 7. 8. It is impossible to borrow funds without paying a fee. Currently, when four messages (lend, borrow, repay, and withdraw messages) are sent in one transaction, the EndBlocker method will not collect borrowing fees. Token/uToken exchange rates are always greater than or equal to 1 and are less than an expected maximum. To avoid rapid signicant price increases and decreases, ensure that the rates do not change more quickly than expected. The exchangeRate value cannot be changed by public (user-callable) methods like LendAsset and WithdrawAsset . Pay special attention to rounding errors and make sure that the module is the beneciary of all rounding operations. 9. It is impossible to liquidate more than the closeFactor in a single liquidation transaction for a defaulted loan; be mindful of the fact that a single transaction can include more than one message. Long term, e xtend the simulation module to cover all operations that may occur in a real Umee deployment, along with all potential error states, and run it many times before each release. Ensure the following:       All modules and operations are included in the simulation module. The simulation uses a small number of accounts (e.g., between 5 and 20) to increase the likelihood of an interesting state change. The simulation uses the currencies/tokens that will be used in the production network. Oracle price changes are properly simulated. (In addition to a mode in which prices are changed randomly, implement a mode in which prices are changed only slightly, a mode in which prices are highly volatile, and a mode in which prices decrease or increase continuously for a long time period.) The simulation continues running when a transaction triggers an error. All transaction code paths are executed. (Enable code coverage to see how often individual lines are executed.)", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "15. Attempts to overdraw collateral cause WithdrawAsset to panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The WithdrawAsset function panics when an account attempts to withdraw more collateral than the account holds. While panics triggered during transaction runs are recovered by the Cosmos SDK , they should be used only to handle unexpected events that should not occur in normal blockchain operations. The function should instead check the collateralToWithdraw value and return an error if it is too large. The panic occurs in the Dec.Sub method when the calculation it performs results in an overow (gure 15.1). func (k Keeper) WithdrawAsset( /* (...) */ ) error { // (...) if amountFromCollateral.IsPositive() { if k.GetCollateralSetting(ctx, lenderAddr, uToken.Denom) { // (...) // Calculate what borrow limit will be AFTER this withdrawal collateral := k.GetBorrowerCollateral(ctx, lenderAddr) collateralToWithdraw := sdk.NewCoins(sdk.NewCoin(uToken.Denom, amountFromCollateral)) newBorrowLimit, err := k.CalculateBorrowLimit(ctx, collateral.Sub(collateralToWithdraw) ) Figure 15.1: umee/x/leverage/keeper/keeper.go#L124-L159 To reproduce this issue, use the test shown in gure 15.2. Exploit Scenario A user of the Umee system who has enabled the collateral setting lends 1,000 UMEE tokens. The user later tries to withdraw 1,001 UMEE tokens. Due to the lack of validation of the collateralToWithdraw value, the transaction causes a panic. However, the panic is recovered, and the transaction nishes with a panic error . Because the system does not provide a proper error message, the user is confused about why the transaction failed. Recommendations Short term, when a user attempts to withdraw collateral, have the WithdrawAsset function check whether the collateralToWithdraw value is less than or equal to the collateral balance of the users account and return an error if it is not. This will prevent the function from panicking if the withdrawal amount is too large. Long term, integrate the test shown in gure 15.2 into the codebase and extend it with additional assertions to verify other program states. func (s *IntegrationTestSuite) TestWithdrawAsset_InsufficientCollateral() { app, ctx := s.app, s.ctx lenderAddr := sdk.AccAddress([] byte ( \"addr________________\" )) lenderAcc := app.AccountKeeper.NewAccountWithAddress(ctx, lenderAddr) app.AccountKeeper.SetAccount(ctx, lenderAcc) // mint and send coins s.Require().NoError(app.BankKeeper.MintCoins(ctx, minttypes.ModuleName, initCoins)) s.Require().NoError(app.BankKeeper.SendCoinsFromModuleToAccount(ctx, minttypes.ModuleName, lenderAddr, initCoins)) // mint additional coins for just the leverage module; this way it will have available reserve // to meet conditions in the withdrawal logic s.Require().NoError(app.BankKeeper.MintCoins(ctx, types.ModuleName, initCoins)) // set collateral setting for the account uTokenDenom := types.UTokenFromTokenDenom(umeeapp.BondDenom) err := s.app.LeverageKeeper.SetCollateralSetting(ctx, lenderAddr, uTokenDenom, true ) s.Require().NoError(err) // lend asset err = s.app.LeverageKeeper.LendAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 1000000000 )) // 1k umee s.Require().NoError(err) // verify collateral amount and total supply of minted uTokens collateral := s.app.LeverageKeeper.GetCollateralAmount(ctx, lenderAddr, uTokenDenom) expected := sdk.NewInt64Coin(uTokenDenom, 1000000000 ) // 1k u/umee s.Require().Equal(collateral, expected) supply := s.app.LeverageKeeper.TotalUTokenSupply(ctx, uTokenDenom) s.Require().Equal(expected, supply) // withdraw more collateral than having - this panics currently uToken := collateral.Add(sdk.NewInt64Coin(uTokenDenom, 1 )) err = s.app.LeverageKeeper.WithdrawAsset(ctx, lenderAddr, uToken) s.Require().EqualError(err, \"TODO/FIXME: set proper error string here after fixing panic error\" ) // TODO/FIXME: add asserts to verify all other program state } Figure 15.2: A test that can be used to reproduce this issue", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "16. Division by zero causes the LiquidateBorrow function to panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Two operations in the x/leverage modules LiquidateBorrow method may involve division by zero and lead to a panic. The rst operation is shown in gure 16.1. If both repayValue and maxRepayValue are zero, the GTE (greater-than-or-equal-to) comparison will succeed, and the Quo method will panic. The repayValue variable will be set to zero if liquidatorBalance is set to zero; maxRepayValue will be set to zero if either closeFactor or borrowValue is set to zero. if repayValue.GTE(maxRepayValue) { // repayment *= (maxRepayValue / repayValue) repayment.Amount = repayment.Amount.ToDec().Mul(maxRepayValue) .Quo(repayValue) .TruncateInt() repayValue = maxRepayValue } Figure 16.1: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#452456 ) The second operation is shown in gure 16.2. If both reward.Amount and collateral.AmountOf(rewardDenom) are set to zero, the GTE comparison will succeed, and the Quo method will panic. The collateral.AmountOf(rewardDenom) variable can easily be set to zero, as the user may not have any collateral in the denomination indicated by the variable; reward.Amount will be set to zero if liquidatorBalance is set to zero. // reward amount cannot exceed available collateral if reward.Amount.GTE(collateral.AmountOf(rewardDenom)) { // reduce repayment.Amount to the maximum value permitted by the available collateral reward repayment.Amount = repayment.Amount.Mul(collateral.AmountOf(rewardDenom)) .Quo(reward.Amount) // use all collateral of reward denom reward.Amount = collateral.AmountOf(rewardDenom) } Figure 16.2: A potential instance of division by zero ( umee/x/leverage/keeper/keeper.go#474480 ) Exploit Scenario A user tries to liquidate a loan. For reasons that are unclear to the user, the transaction fails with a panic. Because the error message is not specic, the user has diculty debugging the error. Recommendations Short term, replace the GTE comparison with a strict inequality GT (greater-than) comparison. Long term, carefully validate variables in the LiquidateBorrow method to ensure that every variable stays within the expected range during the entire computation . Write negative tests with edge-case values to ensure that the methods handle errors gracefully.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "17. Architecture-dependent code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In the Go programming language, the bit size of an int variable depends on the platform on which the code is executed. On a 32-bit platform, it will be 32 bits, and on a 64-bit platform, 64 bits. Validators running on dierent architectures will therefore interpret int types dierently, which may lead to transaction-parsing discrepancies and ultimately to a consensus failure or chain split. One use of the int type is shown in gure 17.1. Because casting the maxValidators variable to the int type should not cause it to exceed the maximum int value for a 32-bit platform, we set the severity of this nding to informational. for ; iterator.Valid() && i < int (maxValidators) ; iterator.Next() { Figure 17.1: An architecture-dependent loop condition in the EndBlocker method ( umee/x/oracle/abci.go#34 ) Exploit Scenario The maxValidators variable (a variable of the uint32 type) is set to its maximum value, 4,294,967,296. During the execution of the x/oracle modules EndBlocker method, some validators cast the variable to a negative number, while others cast it to a large positive integer. The chain then stops working because the validators cannot reach a consensus. Recommendations Short term, ensure that architecture-dependent types are not used in the codebase . Long term, test the system with parameters set to various edge-case values, including the maximum and minimum values of dierent integer types. Test the system on all common architectures (e.g., architectures with 32- and 64-bit CPUs), or develop documentation specifying the architecture(s) used in testing.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Weak cross-origin resource sharing settings ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In the price-feeder s cross-origin resource sharing (CORS) settings, most of the same-origin policy protections are disabled. This increases the severity of vulnerabilities like cross-site request forgery. v1Router.Methods( \"OPTIONS\" ).HandlerFunc( func (w http.ResponseWriter, r *http.Request) { w.Header().Set( \"Access-Control-Allow-Origin\" , r.Header.Get( \"Origin\" )) w.Header().Set( \"Access-Control-Allow-Methods\" , \"GET, PUT, POST, DELETE, OPTIONS\" ) w.Header().Set( \"Access-Control-Allow-Headers\" , \"Content-Type, Access-Control-Allow-Headers, Authorization, X-Requested-With\" ) w.Header().Set( \"Access-Control-Allow-Credentials\" , \"true\" ) w.WriteHeader(http.StatusOK) }) Figure 18.1: The current CORS conguration ( umee/price-feeder/router/v1/router.go#4652 ) We set the severity of this nding to informational because no sensitive endpoints are exposed by the price-feeder router. Exploit Scenario A new endpoint is added to the price-feeder API. It accepts PUT requests that can update the tools provider list. An attacker uses phishing to lure the price-feeder s operator to a malicious website. The website triggers an HTTP PUT request to the API, changing the provider list to a list in which all addresses are controlled by the attacker. The attacker then repeats the attack against most of the validators, manipulates on-chain prices, and drains the systems funds. Recommendations Short term, use strong default values in the CORS settings . Long term, ensure that APIs exposed by the price-feeder have proper protections against web vulnerabilities.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "19. price-feeder is at risk of rate limiting by public APIs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Price providers used by the price-feeder tool may enforce limits on the number of requests served to them. After reaching a limit, the tool should take certain actions to avoid a prolonged or even permanent ban. Moreover, using API keys or non-HTTP access channels would decrease the price-feeder s chance of being rate limited. Every API has its own rules, which should be reviewed and respected. The rules of three APIs are summarized below.    Binance has hard, machine-learning, and web application rewall limits . Users are required to stop sending requests if they receive a 429 HTTP response code . Kraken implements rate limiting based on call counters and recommends using the WebSockets API instead of the REST API. Huopi restricts the number of requests to 10 per second and recommends using an API key. Exploit Scenario A price-feeder exceeds the limits of the Binance API. It is rate limited and receives a 429 HTTP response code from the API. The tool does not notice the response code and continues to spam the API. As a result, it receives a permanent ban. The validator using the price-feeder then starts reporting imprecise exchange rates and gets slashed. Recommendations Short term, review the requirements and recommendations of all APIs supported by the system . Enforce their requirements in a user-friendly manner; for example, allow users to set and rotate API keys, delay HTTP requests so that the price-feeder will avoid rate limiting but still report accurate prices, and log informative error messages upon reaching rate limits. Long term, perform stress-testing to ensure that the implemented safety checks work properly and are robust.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "20. Lack of prioritization of oracle messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Oracle messages are not prioritized over other transactions for inclusion in a block. If the network is highly congested, the messages may not be included in a block. Although the Umee system could increase the fee charged for including an oracle message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario The Umee network is congested. Validators send their exchange rate votes, but the exchange rates are not included in a block. An attacker then exploits the situation by draining the network of its tokens. Recommendations Short term, use a custom CheckTx method to prioritize oracle messages . This will help prevent validators votes from being left out of a block and ignored by an oracle. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "21. Risk of token/uToken exchange rate manipulation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Umee specication states that the token/uToken exchange rate can be aected only by the accrual of interest (not by Lend , Withdraw , Borrow , Repay , or Liquidate transactions). However, this invariant can be broken:   When tokens are burned or minted through an Inter-Blockchain Communication (IBC) transfer, the ibc-go library accesses the x/bank modules keeper interface, which changes the total token supply (as shown in gure 21.2). This behavior is mentioned in a comment shown in gure 22.1. Sending tokens directly to the module through an x/bank message also aects the exchange rate. func (k Keeper) TotalUTokenSupply(ctx sdk.Context, uTokenDenom string ) sdk.Coin { if k.IsAcceptedUToken(ctx, uTokenDenom) { return k.bankKeeper.GetSupply(ctx, uTokenDenom) // TODO - Question: Does bank module still track balances sent (locked) via IBC? // If it doesn't then the balance returned here would decrease when the tokens // are sent off, which is not what we want. In that case, the keeper should keep // an sdk.Int total supply for each uToken type. } return sdk.NewCoin(uTokenDenom, sdk.ZeroInt()) } Figure 21.1: The method vulnerable to unexpected IBC transfers ( umee/x/leverage/keeper/keeper.go#6573 ) if err := k.bankKeeper.BurnCoins( ctx, types.ModuleName, sdk.NewCoins(token), Figure 21.2: The IBC library code that accesses the x/bank modules keeper interface ( ibc-go/modules/apps/transfer/keeper/relay.go#136137 ) Exploit Scenario An attacker with two Umee accounts lends tokens through the system and receives a commensurate number of uTokens. He temporarily sends the uTokens from one of the accounts to another chain (chain B), decreasing the total supply and increasing the token/uToken exchange rate. The attacker uses the second account to withdraw more tokens than he otherwise could and then sends uTokens back from chain B to the rst account. In this way, he drains funds from the module. Recommendations Short term, ensure that the TotalUTokenSupply method accounts for IBC transfers. Use the Cosmos SDKs blocklisting feature to disable direct transfers to the leverage and oracle modules. Consider setting DefaultSendEnabled to false and explicitly enabling certain tokens transfer capabilities. Long term, follow GitHub issues #10386 and #5931 , which concern functionalities that may enable module developers to make token accounting more reliable. Additionally, ensure that the system accounts for ination .", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "22. Collateral dust prevents the designation of defaulted loans as bad debt ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "An accounts debt is considered bad debt only if its collateral balance drops to zero. The debt is then repaid from the modules reserves. However, users may liquidate the majority of an accounts assets but leave a small amount of debt unpaid. In that case, the transaction fees may make liquidation of the remaining collateral unprotable. As a result, the bad debt will not be paid from the module's reserves and will linger in the system indenitely. Exploit Scenario A large loan taken out by a user becomes highly undercollateralized. An attacker liquidates most of the users collateral to repay the loan but leaves a very small amount of the collateral unliquidated. As a result, the loan is not considered bad debt and is not paid from the reserves. The rest of the tokens borrowed by the user remain out of circulation, preventing other users from withdrawing their funds. Recommendations Short term, establish a lower limit on the amount of collateral that must be liquidated in one transaction to prevent accounts from holding dust collateral. Long term, establish a lower limit on the number of tokens to be used in every system operation. That way, even if the systems economic incentives are lacking, the operations will not result in dust tokens.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "23. Users can borrow assets that they are actively using as collateral ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "When a user calls the BorrowAsset function to take out a loan, the function does not check whether the user is borrowing the same type of asset as the collateral he or she supplied. In other words, a user can borrow tokens from the collateral that the user supplied. The Umee system prohibits users from borrowing assets worth more than the collateral they have provided, so a user cannot directly exploit this issue to borrow more funds than the user should be able to borrow. However, a user can borrow the vast majority of his or her collateral to continue accumulating lending rewards while largely avoiding the risks of providing collateral. Exploit Scenario An attacker provides 10 ATOMs to the protocol as collateral and then immediately borrows 9 ATOMs. He continues to earn lending rewards on his collateral but retains the use of most of the collateral. The attacker, through ash loans, could also resupply the borrowed amount as collateral and then immediately take out another loan, repeating the process until the amount he had borrowed asymptotically approached the amount of liquidity he had provided. Recommendations Short term, determine whether borrowers ability to borrow their own collateral is an issue. (Note that Compounds front end disallows such operations, but its actual contracts do not.) If it is, have BorrowAsset check whether a user is attempting to borrow the same asset that he or she staked as collateral and block the operation if so. Alternatively, ensure that borrow fees are greater than prots from lending. Long term, assess whether the liquidity-mining incentives accomplish their intended purpose, and ensure that the lending incentives and borrowing costs work well together.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "24. Providing additional collateral may be detrimental to borrowers in default ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "When a user who is in default on a loan deposits additional collateral, the collateral will be immediately liquidable. This may be surprising to users and may aect their satisfaction with the system. Exploit Scenario A user funds a loan and plans to use the coins he deposited as the collateral on a new loan. However, the user does not realize that he defaulted on a previous loan. As a result, bots instantly liquidate the new collateral he provided. Recommendations Short term, if a user is in default on a loan, consider blocking the user from calling the LendAsset or SetCollateralSetting function with an amount of collateral insucient to collateralize the defaulted position . Alternatively, document the risks associated with calling these functions when a user has defaulted on a loan. Long term, ensure that users cannot incur unexpected nancial damage, or document the nancial risks that users face.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "25. Insecure storage of price-feeder keyring passwords ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts. 26. Insu\u0000cient validation of genesis parameters Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-UMEE-26 Target: Genesis parameters", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "27. Potential overows in Peggo's current block calculations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In a few code paths, Peggo calculates the number of a delayed block by subtracting a delay value from the latest block number. This subtraction will result in an overow and cause Peggo to operate incorrectly if it is run against a blockchain node whose latest block number is less than the delay value. We set the severity of this nding to informational because the issue is unlikely to occur in practice; moreover, it is easy to have Peggo wait to perform the calculation until the latest block number is one that will not cause an overow. An overow may occur in the following methods:  gravityOrchestrator.GetLastCheckedBlock (gure 27.1)  gravityOrchestrator.CheckForEvents  gravityOrchestrator.EthOracleMainLoop  gravityRelayer.FindLatestValset // add delay to ensure minimum confirmations are received and block is finalized currentBlock := latestHeader.Number.Uint64() - ethBlockConfirmationDelay Figure 27.1: peggo/orchestrator/oracle_resync.go#L35-L42 Recommendations Short term, have Peggo wait to calculate the current block number until the blockchain for which Peggo was congured reaches a block number that will not cause an overow.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "28. Peggo does not validate Ethereum address formats ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "In several code paths in the Peggo codebase, the go-ethereum HexToAddress function (gure 28.1) is used to parse Ethereum addresses. This function does not return an error when the format of the address passed to it is incorrect. The HexToAddress function is used in tests as well as in the following parts of the codebase:  peggo/cmd/peggo/bridge.go#L143 (in the peggo deploy-gravity command, to parse addresses fetched from gravityQueryClient )  peggo/cmd/peggo/bridge.go#L403 (in parsing of the peggo send-to-cosmos command's token-address argument)  peggo/cmd/peggo/orchestrator.go#L150 (in the peggo orchestrator [gravity-addr] command)  peggo/cmd/peggo/bridge.go#L536 and twice in #L545-L555  peggo/cmd/peggo/keys.go#L199 , #L274 , and #L299  peggo/orchestrator/ethereum/gravity/message_signatures.go#L36 , #L40 , #L102 , and #L117  p eggo/orchestrator/ethereum/gravity/submit_batch.go#L53 , #L72 , #L94 , #L136 , and #L144  peggo/orchestrator/ethereum/gravity/valset_update.go#L37 , #L55 , and #L87  peggo/orchestrator/main_loops.go#L307  peggo/orchestrator/relayer/batch_relaying.go#L81-L82 , #L237 , and #L250 We set the severity of this nding to undetermined because time constraints prevented us from verifying the impact of the issue. However, without additional validation of the addresses fetched from external sources, Peggo may operate on an incorrect Ethereum address. // HexToAddress returns Address with byte values of s. // If s is larger than len(h), s will be cropped from the left. func HexToAddress( s string ) Address { return BytesToAddress( FromHex(s) ) } // FromHex returns the bytes represented by the hexadecimal string s. // s may be prefixed with \"0x\". func FromHex(s string ) [] byte { if has0xPrefix(s) { s = s[ 2 :] } if len (s)% 2 == 1 { s = \"0\" + s } return Hex2Bytes(s) } // Hex2Bytes returns the bytes represented by the hexadecimal string str. func Hex2Bytes(str string ) [] byte { h, _ := hex.DecodeString(str) return h } Figure 28.1: The HexToAddress function, which calls the BytesToAddress , FromHex , and Hex2Bytes functions, ignores any errors that occur during hex-decoding. Recommendations Short term, review the code paths that use the HexToAddress function, and use a function like ValidateEthAddress to validate Ethereum address string formats before calls to HexToAddress . Long term, add tests to ensure that all code paths that use the HexToAddress function properly validate Ethereum address strings before parsing them.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "29. Peggo takes an Ethereum private key as a command-line argument ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Certain Peggo commands take an Ethereum private key ( --eth-pk ) as a command-line argument. If an attacker gained access to a user account on a system running Peggo, the attacker would also gain access to any Ethereum private key passed through the command line. The attacker could then use the key to steal funds from the Ethereum account. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 29.1: An example of a Peggo command line In Linux, all users can inspect other users commands and their arguments. A user can enable the proc lesystem's hidepid=2 gid=0 mount options to hide metadata about spawned processes from users who are not members of the specied group. However, in many Linux distributions, those options are not enabled by default. Exploit Scenario An attacker gains access to an unprivileged user account on a system running the Peggo orchestrator. The attacker then uses a tool such as pspy to inspect processes run on the system. When a user or script launches the Peggo orchestrator, the attacker steals the Ethereum private key passed to the orchestrator. Recommendations Short term, avoid using a command-line argument to pass an Ethereum private key to the Peggo program. Instead, fetch the private key from the keyring.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "30. Peggo allows the use of non-local unencrypted URL schemes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The peggo orchestrator command takes --tendermint-rpc and --cosmos-grpc ags specifying Tendermint and Cosmos remote procedure call (RPC) URLs. If an unencrypted non-local URL scheme (such as http://<some-external-ip>/) is passed to one of those ags, Peggo will not reject it or issue a warning to the user. As a result, an attacker connected to the same local network as the system running Peggo could launch a man-in-the-middle attack, intercepting and modifying the network trac of the device. $ peggo orchestrator {gravityAddress} \\ --eth-pk= $ETH_PK \\ --eth-rpc= $ETH_RPC \\ --relay-batches= true \\ --relay-valsets= true \\ --cosmos-chain-id=... \\ --cosmos-grpc= \"tcp://...\" \\ --tendermint-rpc= \"http://...\" \\ --cosmos-keyring=... \\ --cosmos-keyring-dir=... \\ --cosmos-from=... Figure 30.1: The problematic ags Exploit Scenario A user sets up Peggo with an external Tendermint RPC address and an unencrypted URL scheme (http://). An attacker on the same network performs a man-in-the-middle attack, modifying the values sent to the Peggo orchestrator to his advantage. Recommendations Short term, warn users that they risk a man-in-the-middle attack if they set the RPC endpoint addresses to external hosts that use unencrypted schemes such as http://.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "31. Lack of prioritization of Peggo orchestrator messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion. 32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure Severity: Undetermined Diculty: High Type: Conguration Finding ID: TOB-UMEE-32 Target: Peggo orchestrator", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "33. Peggo orchestrators IsBatchProtable function uses only one price oracle ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Peggo orchestrator relays batches of Ethereum transactions only when doing so will be protable (gure 33.1). To determine an operations protability, it uses the price of ETH in USD, which is fetched from a single sourcethe CoinGecko API. This creates a single point of failure, as a hacker with control of the API could eectively choose which batches Peggo would relay by manipulating the price. The IsBatchProfitable function (gure 33.2) fetches the ETH/USD price; the gravityRelayer.priceFeeder eld it uses is set earlier in the getOrchestratorCmd function (gure 33.3). func (s *gravityRelayer) RelayBatches( /* (...) */ ) error { // (...) for tokenContract, batches := range possibleBatches { // (...) // Now we iterate through batches per token type. for _, batch := range batches { // (...) // If the batch is not profitable, move on to the next one. if !s.IsBatchProfitable(ctx, batch.Batch, estimatedGasCost, gasPrice, s.profitMultiplier) { continue } // (...) Figure 33.1: peggo/orchestrator/relayer/batch_relaying.go#L173-L176 func (s *gravityRelayer) IsBatchProfitable( / * (...) */ ) bool { // (...) // First we get the cost of the transaction in USD usdEthPrice, err := s.priceFeeder.QueryETHUSDPrice() Figure 33.2: peggo/orchestrator/relayer/batch_relaying.go#L211-L223 func getOrchestratorCmd() *cobra.Command { cmd := &cobra.Command{ Use: \"orchestrator [gravity-addr]\" , Args: cobra.ExactArgs( 1 ), Short: \"Starts the orchestrator\" , RunE: func (cmd *cobra.Command, args [] string ) error { // (...) coingeckoAPI := konfig.String(flagCoinGeckoAPI) coingeckoFeed := coingecko.NewCoingeckoPriceFeed( /* (...) */ ) // (...) relayer := relayer.NewGravityRelayer( /* (...) */ , relayer.SetPriceFeeder(coingeckoFeed), ) Figure 33.3: peggo/cmd/peggo/orchestrator.go#L162-L188 Exploit Scenario All Peggo orchestrator instances depend on the CoinGecko API. An attacker hacks the CoinGecko API and falsies the ETH/USD prices provided to the Peggo relayers, causing them to relay unprotable batches. Recommendations Short term, address the Peggo orchestrators reliance on a single ETH/USD price feed. Consider using the price-feeder tool to fetch pricing information or reading prices from the Umee blockchain. Long term, implement protections against extreme ETH/USD price changes; if the ETH/USD price changes by too large a margin, have the system stop fetching prices and require an operator to investigate whether the issue was caused by malicious behavior. Additionally, implement tests to check the orchestrators handling of random and extreme changes in the prices reported by the price feed. References  Check Coingecko prices separately from BatchRequesterLoop (GitHub issue)", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "34. Rounding errors may cause the module to incur losses ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The amount that a user has borrowed is calculated using AdjustedBorrow data and an InterestScalar value. Because the system uses xed-precision decimal numbers that are truncated to integer values, there may be small rounding errors in the computation of those amounts. If an error occurs, it will benet the user, whose repayment will be slightly lower than the amount the user borrowed. Figure 34.1 shows a test case demonstrating this vulnerability. It should be added to the umee/x/leverage/keeper/keeper_test.go le. Appendix G discusses general rounding recommendations. // Test rounding error bug - users can repay less than have borrowed // It should pass func (s *IntegrationTestSuite) TestTruncationBug() { lenderAddr, _ := s.initBorrowScenario() app, ctx := s.app, s.ctx // set some interesting interest scalar _ = s.app.LeverageKeeper.SetInterestScalar(s.ctx, umeeapp.BondDenom, sdk.MustNewDecFromStr( \"2.9\" )) // save initial balances initialSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply.Amount.Int64(), int64 ( 10000000000 )) initialModuleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) // lender borrows 20 umee err := s.app.LeverageKeeper.BorrowAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 20000000 )) s.Require().NoError(err) // lender repays in a few transactions iters := int64 ( 99 ) payOneIter := int64 ( 2000 ) amountDelta := int64 ( 99 ) // borrowed expects to \"earn\" this amount for i := int64 ( 0 ); i < iters; i++ { repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, payOneIter)) s.Require().NoError(err) s.Require().Equal(sdk.NewInt(payOneIter), repaid) } // lender repays remaining debt - less than he borrowed // we send 90000000, because it will be truncated to the actually owned amount repaid, err := s.app.LeverageKeeper.RepayAsset(ctx, lenderAddr, sdk.NewInt64Coin(umeeapp.BondDenom, 90000000 )) s.Require().NoError(err) s.Require().Equal(repaid.Int64(), 20000000 -(iters*payOneIter)-amountDelta) // verify lender's new loan amount in the correct denom (zero) loanBalance := s.app.LeverageKeeper.GetBorrow(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(loanBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 0 )) // we expect total supply to not change finalSupply := s.app.BankKeeper.GetSupply(s.ctx, umeeapp.BondDenom) s.Require().Equal(initialSupply, finalSupply) // verify lender's new umee balance // should be 10 - 1k from initial + 20 from loan - 20 repaid = 9000 umee // it is more -> borrower benefits tokenBalance := app.BankKeeper.GetBalance(ctx, lenderAddr, umeeapp.BondDenom) s.Require().Equal(tokenBalance, sdk.NewInt64Coin(umeeapp.BondDenom, 9000000000 +amountDelta)) // in test, we didn't pay interest, so module balance should not have changed // but it did because of rounding moduleBalance := s.app.LeverageKeeper.ModuleBalance(s.ctx, umeeapp.BondDenom) s.Require().NotEqual(moduleBalance, initialModuleBalance) s.Require().Equal(moduleBalance.Int64(), int64 ( 1000000000 -amountDelta)) } Figure 34.1: A test case demonstrating the rounding bug Exploit Scenario An attacker identies a high-value coin. He takes out a loan and repays it in a single transaction and then repeats the process again and again. By using a single transaction for both operations, he evades the borrowing fee (i.e., the interest scalar is not increased). Because of rounding errors in the systems calculations, he turns a prot by repaying less than he borrowed each time. His prots exceed the transaction fees, and he continues his attack until he has completely drained the module of its funds. Exploit Scenario 2 The Umee system has numerous users. Each user executes many transactions, so the system must perform many calculations. Each calculation with a rounding error causes it to lose a small amount of tokens, but eventually, the small losses add up and leave the system without the essential funds. Recommendations Short term, always use the rounding direction that will benet the module rather than the user. Long term, to ensure that users pay the necessary fees, consider prohibiting them from borrowing and repaying a loan in the same block. Additionally, use fuzz testing to ensure that it is not possible for users to secure free tokens. References  How to Become a Millionaire, 0.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "35. Outdated and vulnerable dependencies ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Both Umee and Peggo rely on outdated and vulnerable dependencies. The table below lists the problematic packages used by Umee dependencies; the yellow rows indicate packages that were also detected in Peggo dependencies. We set the severity of this nding to undetermined because we could not conrm whether these vulnerabilities aect Umee or Peggo. However, they likely do not, since most of the CVEs are related to binaries or components that are not run in the Umee or Peggo code. Package Vulnerabilities golang/github.com/coreos/etc d@3.3.13 pkg:golang/github.com/dgrija lva/jwt-go@3.2.0 CVE-2020-15114 CVE-2020-15136 CVE-2020-15115 CVE-2020-26160 golang/github.com/microcosm- cc/bluemonday@1.0.4 #111 (CWE-79) golang/k8s.io/kubernetes@1.1 3.0 CVE-2020-8558, CVE-2019-11248, CVE-2019-11247, CVE-2019-11243, CVE-2021-25741, CVE-2019-9946, CVE-2020-8552, CVE-2019-11253, CVE-2020-8559, CVE-2021-25735, CVE-2019-11250, CVE-2019-11254, CVE-2019-11249, CVE-2019-11246, CVE-2019-1002100, CVE-2020-8555, CWE-601, CVE-2019-11251, CVE-2019-1002101, CVE-2020-8563, CVE-2020-8557, CVE-2019-11244 Recommendations Short term, update the outdated and vulnerable dependencies. Even if they do not currently aect Umee or Peggo, a change in the way they are used could introduce a bug. Long term, integrate a dependency-checking tool such as nancy into the CI/CD pipeline. Frequently update any direct dependencies, and ensure that any indirect dependencies in upstream libraries remain up to date. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "25. Insecure storage of price-feeder keyring passwords ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Users can store oracle keyring passwords in the price-feeder conguration le. However, the price-feeder stores these passwords in plaintext and does not provide a warning if the conguration le has overly broad permissions (like those shown in gure 25.1). Additionally, neither the price-feeder README nor the relevant documentation string instructs users to provide keyring passwords via standard input (gure 25.2), which is a safer approach. Moreover, neither source provides information on dierent keyring back ends, and the example price-feeder conguration uses the \"test\" back end . An attacker with access to the conguration le on a users system, or to a backup of the conguration le, could steal the users keyring information and hijack the price-feeder oracle instance. $ ls -la ./price-feeder/price-feeder.example.toml -rwx rwxrwx 1 dc dc 848 Feb 6 10:37 ./price-feeder/price-feeder.example.toml $ grep pass ./price-feeder/price-feeder.example.toml pass = \"exampleKeyringPassword\" $ ~/go/bin/price-feeder ./price-feeder/price-feeder.example.toml 10:42AM INF starting price-feeder oracle... 10:42AM ERR oracle tick failed error=\"key with addressA4F324A31DECC0172A83E57A3625AF4B89A91F1Fnot found: key not found\" module=oracle 10:42AM INF starting price-feeder server... listen_addr=0.0.0.0:7171 Figure 25.1: The price-feeder does not warn the user if the conguration le used to store the keyring password in plaintext has overly broad permissions. // CreateClientContext creates an SDK client Context instance used for transaction // generation, signing and broadcasting. func (oc OracleClient) CreateClientContext() (client.Context, error ) { var keyringInput io.Reader if len (oc.KeyringPass) > 0 { keyringInput = newPassReader(oc.KeyringPass) } else { keyringInput = os.Stdin } Figure 25.2: The price-feeder supports the use of standard input to provide keyring passwords. ( umee/price-feeder/oracle/client/client.go#L184-L192 ) Exploit Scenario A user sets up a price-feeder oracle and stores the keyring password in the price-feeder conguration le, which has been miscongured with overly broad permissions. An attacker gains access to another user account on the user's machine and is able to read the price-feeder oracle's keyring password. The attacker uses that password to access the keyring data and can then control the user's oracle account. Recommendations Short term, take the following steps:     Recommend that users provide keyring passwords via standard input. Check the permissions of the conguration le. If the permissions are too broad, provide an error warning the user of the issue, as openssh does when it nds that a private key le has overly broad permissions. Document the risks associated with storing a keyring password in the conguration le. Improve the price-feeder s keyring-related documentation. Include a link to the Cosmos SDK keyring documentation so that users can learn about dierent keyring back ends and the addition of keyring entries, among other concepts.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "26. Insu\u0000cient validation of genesis parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "A few system parameters must be set correctly for the system to function properly. The system checks the parameter input against minimum and maximum values (not always correctly) but does not check the correctness of the parameters dependencies. Exploit Scenario When preparing a protocol upgrade, the Umee team accidentally introduces an invalid value into the conguration le. As a result, the upgrade is deployed with an invalid or unexpected parameter. Recommendations Short term, implement proper validation of congurable values to ensure that the following expected invariants hold:  BaseBorrowRate <= KinkBorrowRate <= MaxBorrowRate  LiquidationIncentive <= some maximum  CompleteLiquidationThreshold > 0 (The third invariant is meant to prevent division by zero in the Interpolate method.)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "31. Lack of prioritization of Peggo orchestrator messages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "Peggo orchestrator messages, like oracle messages ( TOB-UMEE-20 ), are not prioritized over other transactions for inclusion in a block. As a result, if the network is highly congested, orchestrator transactions may not be included in the earliest possible block. Although the Umee system could increase the fee charged for including a Peggo orchestrator message in a block, that solution is suboptimal and may not work. Tactics for prioritizing important transactions include the following:    Using the custom CheckTx implementation introduced in Tendermint version 0.35 , which returns a priority argument Reimplementing part of the Tendermint engine , as Terra Money did Using Substrates dispatch classes , which allow developers to mark transactions as normal , operational , or mandatory Exploit Scenario A user sends tokens from Ethereum to Umee by calling Gravity Bridges sendToCosmos function. When validators notice the transaction in the Ethereum logs, they send MsgSendToCosmosClaim messages to Umee. However, 34% of the messages are front-run by an attacker, eectively stopping Umee from acknowledging the token transfer. Recommendations Short term, use a custom CheckTx method to prioritize Peggo orchestrator messages. Long term, ensure that operations that aect the whole system cannot be front-run or delayed by attackers or blocked by network congestion.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "32. Failure of a single broadcast Ethereum transaction causes a batch-wide failure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Umee.pdf", "body": "The Peggo orchestrator broadcasts Ethereum events as Cosmos messages and sends them in batches of 10 ( at least by default ). According to a code comment (gure 32.1), if the execution of a single message fails on the Umee side, all of the other messages in the batch will also be ignored. We set the severity of this nding to undetermined because it is unclear whether it is exploitable. // runTx processes a transaction within a given execution mode, encoded transaction // bytes, and the decoded transaction itself. All state transitions occur through // a cached Context depending on the mode provided. State only gets persisted // if all messages get executed successfully and the execution mode is DeliverTx. // Note, gas execution info is always returned. A reference to a Result is // returned if the tx does not run out of gas and if all the messages are valid // and execute successfully. An error is returned otherwise. func (app *BaseApp) runTx(mode runTxMode, txBytes [] byte , tx sdk.Tx) (gInfo sdk.GasInfo, result *sdk.Result, err error ) { Figure 32.1: cosmos-sdk/v0.45.1/baseapp/baseapp.go#L568-L575 Recommendations Short term, review the practice of ignoring an entire batch of Peggo-broadcast Ethereum events when the execution of one of them fails on the Umee side, and ensure that it does not create a denial-of-service risk. Alternatively, change the system such that it can identify any messages that will fail and exclude them from the batch. Long term, generate random messages corresponding to Ethereum events and use them in testing to check the systems handling of failed messages.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "1. Insecure download process for the yq tool ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Dockerle uses the Wget utility to download the yq tool but does not verify the le it has downloaded by its checksum or signature. Without verication, an archive that has been corrupted or modied by a malicious third party may not be detected. Figures 1.1 and 1.2 show cases in which a tool is downloaded without verication of its checksum. 6 RUN wget https://github.com/mikefarah/yq /releases/download/v4.17.2/yq_linux_386.tar.gz -O - | \\ 7 tar xz && mv yq_linux_386 /usr/bin/yq Figure 1.1: The Dockerle downloads and unarchives the yq tool. ( ci/image/Dockerfile#67 ) 41 wget https://github.com/bodymindarts/cepler /releases/download/v ${ cepler_version } /cepler-x 86_64-unknown-linux-musl- ${ cepler_version } .tar.gz \\ 42 && tar -zxvf cepler-x86_64-unknown-linux-musl- ${ cepler_version } .tar.gz \\ 43 && mv cepler-x86_64-unknown-linux-musl- ${ cepler_version } /cepler /usr/local/bin \\ 44 && chmod +x /usr/local/bin/cepler \\ 45 && rm -rf ./cepler-* Figure 1.2: The bastion-startup script downloads and unarchives the cepler tool. ( modules/inception/gcp/bastion-startup.tmpl#4145 ) Exploit Scenario An attacker gains access to the GitHub repository from which yq is downloaded. The attacker then modies the binary to create a reverse shell upon yq s startup. When a user runs the Dockerle, the attacker gains access to the users container. Recommendations Short term, have the Dockerle and other scripts in the solution verify each le they download by its checksum . Long term, implement checks to ensure the integrity of all third-party components used in the solution and periodically check that all components are downloaded from encrypted URLs.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Use of unencrypted HTTP scheme ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Galoy ipfetcher module uses the unencrypted HTTP scheme (gure 2.1). As a result, an attacker in the same network as the host invoking the code in gure 2.1 could intercept and modify both the request and ipfetcher s response to it, potentially accessing sensitive information. 8 9 const { data } = await axios.get( ` http ://proxycheck.io/v2/ ${ ip } ?key= ${ PROXY_CHECK_APIKEY } &vpn=1&asn=1` , 10 ) Figure 2.1: src/services/ipfetcher/index.ts#810 Exploit Scenario Eve gains access to Alices network and obtains Alices PROXY_CHECK_APIKEY by observing the unencrypted network trac. Recommendations Short term, change the URL scheme used in the ipfetcher service to HTTPS. Long term, use tools such as WebStorm code inspections to nd other uses of unencrypted URLs.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Lack of expiration and revocation mechanism for JWTs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Galoy system uses JSON web tokens (JWTs) for authentication. A user obtains a new JWT by calling the userLogin GraphQL mutation. Once a token has been signed, it is valid forever; the platform does not set an expiration time for tokens and cannot revoke them. 7 export const createToken = ({ 8 uid, 9 network, 10 }: { 11 uid: UserId 12 network: BtcNetwork 13 }): JwtToken => { 14 return jwt.sign({ uid, network }, JWT_SECRET, { // (...) 25 algorithm: \"HS256\" , 26 }) as JwtToken 27 } Figure 3.1: The creation of a JWT ( src/services/jwt.ts#727 ) Exploit Scenario An attacker obtains a users JWT and gains persistent access to the system. The attacker then engages in destructive behavior. The victim eventually notices the behavior but does not have a way to stop it. Recommendations Short term, consider setting an expiration time for JWTs, and implement a mechanism for revoking tokens. That way, if a JWT is leaked, an attacker will not gain persistent access to the system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Use of insecure function to generate phone codes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Galoy application generates a verication code by using the JavaScript function Math.random() , which is not a cryptographically secure pseudorandom number generator (CSPRNG) . const randomIntFromInterval = (min, max) => Math .floor( Math .random() * (max - min + 1 ) + min) 10 11 12 // (...) 82 83 84 85 86 87 const code = String ( randomIntFromInterval( 100000 , 999999 ) ) as PhoneCode const galoyInstanceName = getGaloyInstanceName() const body = ` ${ code } is your verification code for ${ galoyInstanceName } ` const result = await PhoneCodesRepository().persistNew({ 88 phone: phoneNumberValid , 89 code, 90 }) 91 92 93 94 95 96 } if (result instanceof Error ) return result const sendTextArguments = { body, to: phoneNumberValid , logger } return TwilioClient().sendText(sendTextArguments) Figure 4.1: src/app/users/request-phone-code.ts#1096 Exploit Scenario An attacker repeatedly generates verication codes and analyzes the values and the order of their generation. The attacker attempts to deduce the pseudorandom number generator's internal state. If successful, the attacker can then perform an oine calculation to predict future verication codes. Recommendations Short term, replace Math.random() with a CSPRNG. Long term, always use a CSPRNG to generate random values for cryptographic operations.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "5. Redundant basic authentication method ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Galoy application implements a basic authentication method (gure 5.1) that is redundant because the apiKey is not being used. Superuous authentication methods create new attack vectors and should be removed from the codebase. 1 2 3 import express from \"express\" const formatError = new Error ( \"Format is Authorization: Basic <base64(key:secret)>\" ) 4 5 export default async function ( 6 req: express.Request , 7 _res: express.Response , 8 next: express.NextFunction , 9 ) { 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 const authorization = req.headers[ \"authorization\" ] if (!authorization) return next() const parts = authorization.split( \" \" ) if (parts.length !== 2 ) return next() const scheme = parts[ 0 ] if (! /Basic/i .test(scheme)) return next() const credentials = Buffer. from (parts[ 1 ], \"base64\" ).toString().split( \":\" ) if (credentials.length !== 2 ) return next(formatError) const [apiKey, apiSecret] = credentials if (!apiKey || !apiSecret) return next(formatError) 25 req[ \"apiKey\" ] = apiKey 26 req[ \"apiSecret\" ] = apiSecret 27 next() 28 } Figure 5.1: The basic authentication method implementation ( src/servers/middlewares/api-key-auth.ts#128 ) Recommendations Short term, remove the apiKey -related code. Long term, review and clearly document the Galoy authentication methods.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "6. GraphQL queries may facilitate CSRF attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Galoy applications /graphql endpoint handles queries sent via GET requests. It is impossible to pass state-changing mutations or subscriptions in GET requests, and authorized queries need the Authorization: Bearer header. However, if a state-changing GraphQL operation were mislabeled as a query (typically a non-state-changing request), the endpoint would be vulnerable to cross-site request forgery (CSRF) attacks. Exploit Scenario An attacker creates a malicious website with JavaScript code that sends requests to the /graphql endpoint (gure 6.1). When a user visits the website, the JavaScript code is executed in the users browser, changing the servers state. <html> <body> <script> history.pushState('', '', '/') </script> <form action= \"http://192.168.236.135:4002/graphql\" > <input type= \"hidden\" name= \"query\" value= \"query&#32;&#123;&#10;&#9;btcPriceList&#40;range&#58;ONE&#95;MONTH&#41;&#32;&# 123;&#10;&#9;&#9;price&#32;&#123;&#10;&#9;&#9;&#9;offset&#10;&#9;&#9;&#125;&#10;&#9; &#9;timestamp&#10;&#9;&#125;&#10;&#125;\" /> <input type= \"submit\" value= \"Submit request\" /> </form> </body> </html> Figure 6.1: In this proof-of-concept CSRF attack, the malicious website sends a request (the btcPriceList query) when the victim clicks Submit request. Recommendations Short term, disallow the use of the GET method to send queries, or enhance the CSRF protections for GET requests. Long term, identify all state-changing endpoints and ensure that they are protected by an authentication or anti-CSRF mechanism. Then implement tests for those endpoints. References   Cross-Origin Resource Sharing, Mozilla documentation Cross-Site Request Forgery Prevention, OWASP Cheat Sheet Series", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "7. Potential ReDoS risk ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The caseInsensitiveRegex function takes an input: string parameter and uses it to create a new RegExp object (gure 7.1). Users cannot currently control the input parameter (the regular expression) or the string; however, if users gain that ability as the code is developed, it may enable them to cause a regular expression denial of service (ReDoS) . 13 14 export const caseInsensitiveRegex = (input: string ) => { return new RegExp ( `^ ${ input } $` , \"i\" ) 15 } Figure 7.1: src/services/mongoose/users.ts#1315 37 const findByUsername = async ( 38 username: Username , 39 ): Promise<Account | RepositoryError> => { 40 41 try { const result = await User.findOne( 42 { username: caseInsensitiveRegex (username) }, Figure 7.2: src/services/mongoose/accounts.ts#3742 Exploit Scenario An attacker registers an account with a specially crafted username (line 2, gure 7.3), which forms part of a regex. The attacker then nds a way to pass the malicious regex (line 1, gure 7.3) to the findByUsername function, causing a denial of service on a victims machine. 1 2 let test = caseInsensitiveRegex( \"(.*){1,32000}[bc]\" ) let s = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa!\" 3 s.match(test) Figure 7.3: A proof of concept for the ReDoS vulnerability Recommendations Short term, ensure that input passed to the caseInsensitiveRegex function is properly validated and sanitized.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "8. Use of MD5 to generate unique GeeTest identiers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Galoy application uses MD5 hashing to generate a unique identier during GeeTest service registration. MD5 is an insecure hash function and should never be used in a security-relevant context. 33 const register = async (): Promise<UnknownCaptchaError | GeetestRegister> => { 34 35 36 37 try { const gtLib = new GeetestLib(config.id, config.key) const digestmod = \"md5\" const params = { 38 digestmod, 39 client_type: \"native\" , 40 } 41 42 43 const bypasscache = await getBypassStatus() // not a cache let result if (bypasscache === \"success\" ) { 44 result = await gtLib.register(digestmod, params) Figure 8.1: src/services/geetest.ts#3344 Recommendations Short term, change the hash function used in the register function to a stronger algorithm that will not cause collisions, such as SHA-256. Long term, document all cryptographic algorithms used in the system, implement a policy governing their use, and create a plan for when and how to deprecate them.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Reliance on SMS-based OTPs for authentication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "Galoys authentication process is heavily reliant on the delivery of one-time passwords (OTPs) over SMS. This authentication method contravenes best practices and should not be used in applications that handle nancial transactions or other sensitive operations. SMS-based OTP leaves users vulnerable to multiple attacks and is considered an unsafe authentication method. Several of the most common and eective attack scenarios are described below.    Text messages received by a mobile device can be intercepted by rogue applications on the device. Many users blindly authorize untrusted third-party applications to access their mobile phones SMS databases; this means that a vulnerability in a third-party application could lead to the compromise and disclosure of the text messages on the device, including Galoys SMS OTP messages. Another common technique used to target mobile nance applications is the interception of notications on a device. Android operating systems, for instance, broadcast notications across applications by design; a rogue application could subscribe to those notications to access incoming text message notications. Attackers also target SMS-based two-factor authentication and OTP implementations through SIM swapping . In short, an attacker uses social engineering to gather information about the owner of a SIM card and then, impersonating its owner, requests a new SIM card from the telecom company. All calls and text messages will then be sent to the attacker, leaving the original owner of the number out of the loop. This approach has been used in many recent attacks against crypto wallet owners, leading to millions of dollars in losses. Recommendations Short term, avoid using SMS authentication as anything other than an optional way to validate an account holder's identity and prole information. Instead of SMS-based OTP, provide support for hardware-based two-factor authentication methods such as Yubikey tokens, or software-based time-based one-time password (TOTP) implementations such as Google Authenticator and Authy. References  What is a Sim Swap? Denition and Related FAQs, Yubico", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "10. Incorrect handling and implementation of SMS OTPs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "Users authenticate to the web panel by providing OTPs sent to them over SMS. We identied two issues in the OTP authentication implementation: 1. The generated OTPs are persistent because OTP expiration dates are calculated incorrectly. The Date.now() method returns the epoch time in milliseconds, whereas it is meant to return the time in seconds. 294 const PhoneCodeSchema = new Schema({ 295 created_at: { 296 297 type : Date , default: Date.now , 298 required: true , Figure 10.1: The default date value is expressed in milliseconds. ( src/services/mongoose/schema.ts#294298 ) 11 export const VALIDITY_TIME_CODE = ( 20 * 60 ) as Seconds Figure 10.2: The default validity period is expressed in seconds. ( src/config/index.ts#11 ) 49 50 51 const age = VALIDITY_TIME_CODE const validCode = await isCodeValid({ phone: phoneNumberValid , code, age }) if (validCode instanceof Error ) return validCode Figure 10.3: Validation of an OTPs age ( src/app/users/login.ts#4951 ) 18 }): Promise < true | RepositoryError> => { 19 20 21 const timestamp = Date .now() / 1000 - age try { const phoneCode = await PhoneCode.findOne({ 22 phone, 23 code, 24 created_at: { 25 $gte: timestamp , 26 }, Figure 10.4: The codebase validates the timestamp in seconds, while the default date is in milliseconds, as shown in gure 10.1. ( src/services/mongoose/phone-code.ts#1826 ) 2. The SMS OTPs are never discarded. When a new OTP is sent to a user, the old one remains valid regardless of its expiration time. A users existing OTP tokens also remain valid if the user manually logs out of a session, which should not be the case. Tests of the admin-panel and web-wallet code conrmed that all SMS OTPs generated for a given phone number remain valid in these cases. Exploit Scenario After executing a successful phishing attack against a user, an attacker is able to intercept an OTP sent to that user, gaining persistent access to the victim's account. The attacker will be able to use the code even when the victim logs out of the session or requests a new OTP. Recommendations Short term, limit the lifetime of OTPs to two minutes. Additionally, immediately invalidate an OTP, even an unexpired one, when any of the following events occur:      The user logs out of a session The user requests a new OTP The OTP is used successfully The OTP reaches its expiration time The users account is locked for any reason (e.g., too many login attempts) References  NIST best practices for implementing authentication tokens", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "11. Vulnerable and outdated Node packages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "We used the yarn audit and snyk tools to audit the project dependencies and components for known vulnerabilities and outdated versions, respectively. The project uses many outdated packages with known security vulnerabilities ranging from critical to low severity. A list of vulnerable and outdated packages is included in appendix C . Vulnerabilities in packages imported by an application are not necessarily exploitable. In most cases, an aected method in a vulnerable package needs to be used in the right context to be exploitable. We manually reviewed the packages with high- or critical-severity vulnerabilities and did not nd any vulnerabilities that could be exploited in the Galoy application. However, that could change as the code is further developed. Exploit Scenario An attacker ngerprints one of Galoys components, identies an out-of-date package with a known vulnerability, and uses it in an exploit against the component. Recommendations Short term, update the outdated and vulnerable dependencies. Long term, integrate static analysis tools that can detect outdated and vulnerable libraries (such as the yarn audit and snyk tools) into the build and / or test pipeline. This will improve the system's security posture and help prevent the exploitation of project dependencies.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "12. Outdated and internet-exposed Grafana instance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Grafana admin panel is exposed over the internet. A management interface should not be exposed over the internet unless it is protected by a secondary authentication or access control mechanism; these mechanisms (e.g., IP address restrictions and VPN solutions) can mitigate the immediate risk to an application if it experiences a vulnerability. Moreover, the Grafana version deployed at grafana.freecorn.galoy.io is outdated and vulnerable to known security issues. Figure 12.1: The outdated Grafana version (8.2.1) with known security issues The version banner on the login page (gure 12.1) identies the version as v8.2.1 ( 88622d7f09 ). This version has multiple moderate- and high-risk vulnerabilities. One of them, a path traversal vulnerability ( CVE-2021-43798 ), could enable an unauthenticated attacker to read the contents of arbitrary les on the server. However, we could not exploit this issue, and the Galoy team suggested that the code might have been patched through an upstream software deployment. Time constraints prevented us from reviewing all Grafana instances for potential vulnerabilities. We reviewed only the grafana.freecorn.galoy.io instance, but the recommendations in this nding apply to all deployed instances. Exploit Scenario An attacker identies the name of a valid plugin installed and active on the instance. By using a specially crafted URL, the attacker can read the contents of any le on the server (as long as the Grafana process has permission to access the le). This enables the attacker to read sensitive conguration les and to engage in remote command execution on the server. Recommendations Short term, avoid exposing any Grafana instance over the internet, and restrict access to each instances management interface. This will make the remote exploitation of any issues much more challenging. Long term, to avoid known security issues, review all deployed instances and ensure that they have been updated to the latest version. Additionally, review the Grafana log les for any indication of the attack described in CVE-2021-43798, which has been exploited in the wild. References  List of publicly known vulnerabilities aecting recent versions of Grafana", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "13. Incorrect processing of GET path parameter ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "If the value of the hidden path parameter in the GET request in gure 13.1 does not match the value in the appRoutesDef array, the request will cause an unhandled error (gure 13.2). The error occurs when the result of the serverRenderer function is undefined (line 21, gure 13.3), because the Invalid route path error is thrown in the call to the renderToStringWithData function (gure 13.4). GET / ?path=aaaa HTTP / 1.1 Host: localhost:3000 Figure 13.1: The HTTP request that triggers the error HTTP / 1.1 500 Internal Server Error // (...) ReferenceError: /Users/md/work/web-wallet/views/index.ejs:8 6| <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /> 7| >> 8| <title><%- pageData.title %></title> 9| 10| <link rel=\"stylesheet\" href=\"/themes/<%- GwwConfig.walletTheme -%>/colors.css\" /> 11| <link rel=\"stylesheet\" href=\"/bundles/<%- gVars['main'][0] -%>\" /> pageData is not defined at eval (\"web-wallet/views/index.ejs\":12:17) at index (web-wallet/node_modules/ejs/lib/ejs.js:692:17) at tryHandleCache (web-wallet/node_modules/ejs/lib/ejs.js:272:36) at View.exports.renderFile [as engine] (web-wallet/node_modules/ejs/lib/ejs.js:489:10) at View.render (web-wallet/node_modules/express/lib/view.js:135:8) at tryRender (web-wallet/node_modules/express/lib/application.js:640:10) at Function.render (web-wallet/node_modules/express/lib/application.js:592:3) at ServerResponse.render (web-wallet/node_modules/express/lib/response.js:1017:7) at web-wallet/src/server/ssr-router.ts:24:18 </ pre ></ body ></ html > Figure 13.2: The HTTP response that shows the unhandled error 21 22 23 24 const vars = await serverRenderer(req)({ // undefined path: checkedRoutePath , }) return res.render( \"index\" , vars) // call when the vars is undefined Figure 13.3: src/server/ssr-router.ts#2124 10 export const serverRenderer = 11 (req: Request ) => 12 async ({ 13 path, 14 flowData, 15 }: { 16 path: RoutePath | AuthRoutePath 17 flowData?: KratosFlowData 18 }) => { 19 try { // (...) 43 const initialMarkup = await renderToStringWithData(App) // (...) 79 }) 80 } catch (err) { 81 console.error(err) 82 } Figure 13.4: src/renderers/server.tsx#1082 Exploit Scenario An attacker nds a way to inject malicious code into the hidden path parameter. This results in an open redirect vulnerability, enabling the attacker to redirect a victim to a malicious website. Recommendations Short term, ensure that errors caused by an invalid path parameter value (one not included in the appRoutesDef whitelist) are handled correctly. A path parameter should not be processed if it is unused. Long term, use Burp Suite Professional with the Param Miner extension to scan the application for hidden parameters.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "14. Discrepancies in API and GUI access controls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "Although the Web Wallets graphical user interface (GUI) does not allow changes to a username (gure 14.1), they can be made through the GraphQL userUpdateUsername mutation (gure 14.2). Figure 14.1: The lock icon on the Settings page indicates that it is not possible to change a username. POST /graphql HTTP / 2 Host: api.freecorn.galoy.io Content-Length: 345 Content-Type: application/json Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOiI2MjI3ODYwMWJlOGViYWYxZWRmNDBhNDYiLCJ uZXR3b3JrIjoibWFpbm5ldCIsImlhdCI6MTY0Njc1NzU4NX0.ed2dk9gMQh5DJXCPpitj5wq78n0gFnmulRp 2KIXTVX0 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Origin: https://wallet.freecorn.galoy.io { \"operationName\" : \"userUpdateUsername\" , \"variables\" :{ \"input\" :{ \"username\" : \"aaaaaaaaaaaa aaa\" }}, \"query\" : \" mutation userUpdateUsername($input: UserUpdateUsernameInput!) {\\n userUpdateUsername(input: $input) {\\n errors {\\n message\\n __typename\\n }\\n user {\\n id\\n username\\n __typename\\n }\\n __typename\\n }\\n} \" } HTTP/ 2 200 OK // (...) { \"data\" :{ \"userUpdateUsername\" :{ \"errors\" :[], \"user\" :{ \"id\" : \"04f01fb4-6328-5982-a39a-eeb 027a2ceef\" , \"username\" : \"aaaaaaaaaaaaaaa\" , \"__typename\" : \"User\" }, \"__typename\" : \"UserUpdat eUsernamePayload\" }}} Figure 14.2: The HTTP request-response cycle that enables username changes Exploit Scenario An attacker nds a discrepancy in the access controls of the GUI and API and is then able to use a sensitive method that the attacker should not be able to access. Recommendations Short term, avoid relying on client-side access controls. If the business logic of a functionality needs to be blocked, the block should be enforced in the server-side code. Long term, create an access control matrix for specic roles in the application and implement unit tests to ensure that appropriate access controls are enforced server-side.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "15. Cloud SQL does not require TLS connections ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "Terraforms declarative conguration le for the Cloud SQL instance does not indicate that PostgreSQL should enforce the use of Transport Layer Security (TLS) connections. Similarly, the Galoy solution does not use the Cloud SQL Auth proxy, which provides strong encryption and authentication using identity and access management. Because the database is exposed only in a virtual private cloud (VPC) network, this nding is of low severity. Exploit Scenario An attacker manages to eavesdrop on trac in the VPC network. If one of the database clients is miscongured, the attacker will be able to observe the database trac in plaintext. Recommendations Short term, congure Cloud SQL to require the use of TLS, or use the Cloud SQL Auth proxy. Long term, integrate Terrascan or another automated analysis tool into the workow to detect areas of improvement in the solution. References   Congure SSL/TLS certicates , Cloud SQL documentation Connect from Google Kubernetes Engine , Cloud SQL documentation", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "16. Kubernetes node pools are not congured to auto-upgrade ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The Galoy application uses Google Kubernetes Engine (GKE) node pools in which the auto-upgrade functionality is disabled. The auto-upgrade functionality helps keep the nodes in a Kubernetes cluster up to date with the Kubernetes version running on the cluster control plane, which Google updates on the users behalf. Auto-upgrades also ensure that security updates are timely applied. Disabling this setting is not recommended by Google and could create a security risk if patching is not performed manually. 124 125 126 management { auto_repair = true auto_upgrade = false 127 } Figure 16.1: The auto-upgrade property is set to false . ( modules/platform/gcp/kube.tf#124127 ) Recommendations Short term, enable the auto-upgrade functionality to ensure that the nodes are kept up to date and that security patches are timely applied. Long term, remain up to date on the security features oered by Google Cloud. Integrate Terrascan or another automated tool into the development workow to detect areas of improvement in the solution. References  Auto-upgrading nodes , GKE documentation", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. Overly permissive rewall rules ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The VPC rewall conguration is overly permissive. This conguration, in conjunction with Google Clouds default VPC rules, allows most communication between pods (gure 17.2), the bastion host (gure 17.3), and the public internet (gure 17.1). 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 resource \"google_compute_firewall\" \"bastion_allow_all_inbound\" { project = local.project name = \"${local.name_prefix}-bastion-allow-ingress\" network = google_compute_network.vpc.self_link target_tags = [ local.tag ] direction = \"INGRESS\" source_ranges = [ \"0.0.0.0/0\" ] priority = \"1000\" allow { protocol = \"all\" } 107 } Figure 17.1: The bastion ingress rules allow incoming trac on all protocols and ports from all addresses. ( modules/inception/gcp/bastion.tf#92107 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 resource \"google_compute_firewall\" \"intra_egress\" { project = local.project name = \"${local.name_prefix}-intra-cluster-egress\" description = \"Allow pods to communicate with each other and the master\" network = data.google_compute_network.vpc.self_link priority = 1000 direction = \"EGRESS\" target_tags = [ local.cluster_name ] destination_ranges = [ local.master_ipv4_cidr_block , google_compute_subnetwork.cluster.ip_cidr_range , google_compute_subnetwork.cluster.secondary_ip_range[0].ip_cidr_range , ] # Allow all possible protocols allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } allow { protocol = \"sctp\" } allow { protocol = \"esp\" } allow { protocol = \"ah\" } 23 } Figure 17.2: Pods can initiate connections to other pods on all protocols and ports. ( modules/platform/gcp/firewall.tf#123 ) 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 resource \"google_compute_firewall\" \"dmz_nodes_ingress\" { name = \"${var.name_prefix}-bastion-nodes-ingress\" description = \"Allow ${var.name_prefix}-bastion to reach nodes\" project = local.project network = data.google_compute_network.vpc.self_link priority = 1000 direction = \"INGRESS\" target_tags = [ local.cluster_name ] source_ranges = [ data.google_compute_subnetwork.dmz.ip_cidr_range , ] # Allow all possible protocols allow { protocol = \"tcp\" } allow { protocol = \"udp\" } allow { protocol = \"icmp\" } allow { protocol = \"sctp\" } allow { protocol = \"esp\" } 63 allow { protocol = \"ah\" } 64 } Figure 17.3: The bastion host can initiate connections to pods on all protocols and ports. ( modules/platform/gcp/firewall.tf#4464 ) Exploit Scenario 1 An attacker gains access to a pod through a vulnerability in an application. He takes advantage of the unrestricted egress trac and miscongured pods to launch attacks against other services and pods in the network. Exploit Scenario 2 An attacker discovers a vulnerability on the Secure Shell server running on the bastion host. She exploits the vulnerability to gain network access to the Kubernetes cluster, which she can then use in additional attacks. Recommendations Short term, restrict both egress and ingress trac to necessary protocols and ports. Document the expected network interactions across the components and check them against the implemented rewall rules. Long term, use services such as the Identity-Aware Proxy to avoid exposing hosts directly to the internet, and enable VPC Flow Logs for network monitoring. Additionally, integrate automated analysis tools such as tfsec into the development workow to detect rewall issues early on. References  Using IAP for TCP forwarding, Identity-Aware Proxy documentation", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "18. Lack of uniform bucket-level access in Terraform state bucket ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "Uniform bucket-level access is not enabled in the bootstrap module bucket used to store the Terraform state. When enabled, this feature implements a uniform permission system, providing access at the bucket level rather than on a per-object basis. It also simplies the access controls / permissions of a bucket, making them easier to manage and reason about. 1 2 3 4 5 6 7 8 resource \"google_storage_bucket\" \"tf_state\" { name = \"${local.name_prefix}-tf-state\" project = local.project location = local.tf_state_bucket_location versioning { enabled = true } force_destroy = local.tf_state_bucket_force_destroy 9 } Figure 18.1: The bucket denition lacks a uniform_bucket_level_access eld set to true . ( modules/bootstrap/gcp/tf-state-bucket.tf#19 ) Exploit Scenario The permissions of some objects in a bucket are miscongured. An attacker takes advantage of that fact to access the Terraform state. Recommendations Short term, enable uniform bucket-level access in this bucket. Long term, integrate automated analysis tools such as tfsec into the development workow to identify any similar issues and areas of improvement.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "19. Insecure storage of passwords ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "Galoy passwords are stored in conguration les and environment variables or passed in as command-line arguments. There are two issues with this method of storage: (1) the default keys are low entropy (gure 19.1) and (2) the fact that there are default keys in the rst place suggests that users deploying components may not realize that they need to set passwords. 53 export BITCOINDRPCPASS=rpcpassword // (...) 68 export MONGODB_PASSWORD=password // (...) 79 export JWT_SECRET= \"jwt_secret\" Figure 19.1: An example conguration le with passwords ( .envrc#5379 ) Passing in sensitive values through environment variables (gure 19.2) increases the risk of a leak for several reasons:    Environment variables are often dumped to external services through crash-logging mechanisms. All processes started by a user can read environment variables from the /proc/$pid/environ le. Attackers often use this ability to dump sensitive values passed in through environment variables (though this requires nding an arbitrary le read vulnerability in the application). An application can also overwrite the contents of a special /proc/$pid/environ le. However, overwriting the le is not as simple as calling setenv(SECRET, \"******\") , because runtimes copy environment variables upon initialization and then operate on the copy. To clear environment variables from that special environ le, one must either overwrite the stack data in which they are located or make a low-level prctl system call with the PR_SET_MM_ENV_START and PR_SET_MM_ENV_END ags enabled to change the memory address of the content the le is rendered from. 12 const jwtSecret = process.env.JWT_SECRET Figure 19.2: src/config/process.ts#12 Certain initialization commands take a password as a command-line argument (gures 19.3 and 19.4). If an attacker gained access to a user account on a system running the script, the attacker would also gain access to any password passed as a command-line argument. 65 66 command : [ '/bin/sh' ] args : 67 - '-c' 68 - | 69 70 71 72 73 if [ ! -f /root/.lnd/data/chain/bitcoin/${NETWORK}/admin.macaroon ]; then while ! test -f /root/.lnd/tls.cert; do sleep 1; done apk update; apk add expect /home/alpine/walletInit.exp ${NETWORK} $LND_PASS fi Figure 19.3: charts/lnd/templates/statefulset.yaml#6573 55 set PASSWORD [lindex $argv 1]; Figure 19.4: charts/lnd/templates/wallet-init-configmap.yaml#55 In Linux, all users can inspect other users commands and their arguments. A user can enable the proc lesystem's hidepid=2 gid=0 mount options to hide metadata about spawned processes from users who are not members of the specied group. However, in many Linux distributions, those options are not enabled by default. Recommendations Short term, take the following actions:   Remove the default encryption keys and avoid using any one default key across installs. The user should be prompted to provide a key when deploying the Galoy application, or the application should generate a key using known-good cryptographically secure methods and provide it to the user for safekeeping. Avoid storing encryption keys in conguration les. Conguration les are often broadly readable or rendered as such accidentally. Long term, ensure that keys, passwords, and other sensitive data are never stored in plaintext in the lesystem, and avoid providing default values for that data. Also take the following steps:    Document the risks of providing sensitive values through environment variables. Encourage developers to pass sensitive values through standard input or to use a launcher that can fetch them from a service like HashiCorp Vault. Allow developers to pass in those values from a conguration le, but document the fact that the conguration le should not be saved in backups, and provide a warning if the le has overly broad permissions when the program is started.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "20. Third-party container images are not version pinned ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The continuous integration (CI) pipeline and Helm charts reference third-party components such as Docker registry images by named tags (or by no tag at all). Registry tags are not immutable; if an attacker compromised an image publishers account, the pipeline or Kubernetes cluster could be provided a malicious container image. 87 - name : build-chain-dl-image 88 89 90 91 92 93 94 95 96 97 98 serial : true plan : - { get : chain-dl-image-def , trigger : true } - task : build privileged : true config : platform : linux image_resource : type : registry-image source : repository : vito/oci-build-task Figure 20.1: A third-party image referenced without an explicit tag ( ci/pipeline.yml#8798 ) 270 resource_types : 271 - name : terraform 272 273 274 275 type : docker-image source : repository : ljfranklin/terraform-resource tag : latest Figure 20.2: An image referenced by the latest tag ( ci/pipeline.yml#270275 ) Exploit Scenario An attacker gains access to a Docker Hub account hosting an image used in the CI pipeline. The attacker then tags a malicious container image and pushes it to Docker Hub. The CI pipeline retrieves the tagged malicious image and uses it to execute tasks. Recommendations Short term, refer to Docker images by SHA-256 digests to prevent the use of an incorrect or modied image. Long term, integrate automated tools such as Checkov into the development workow to detect similar issues in the codebase.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "21. Compute instances do not leverage Shielded VM features ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The bastion host denition does not enable all of Google Clouds Shielded VM (virtual machine) features for Compute Engine VM instances. These features provide veriable integrity of VM instances and assurance that VM instances have not been compromised by boot- or kernel-level malware or rootkits. Three features provide this veriable integrity: Secure Boot, virtual trusted platform module (vTPM)-enabled Measured Boot, and integrity monitoring. Google also oers Shielded GKE nodes, which are built on top of Shielded VMs and provide strong veriable node identity and integrity to increase the security of GKE nodes. The node pool denition does enable this feature but disables Secure Boot checks on the node instances. 168 169 170 shielded_instance_config { enable_secure_boot = false enable_integrity_monitoring = true 171 } Figure 21.1: Secure Boot is disabled. ( modules/platform/gcp/kube.tf#168171 ) Exploit Scenario The bastion host is compromised, and persistent kernel-level malware is installed. Because the bastion host is still operational, the malware remains undetected for an extended period. Recommendations Short term, enable these security features to increase the security and trustworthiness of the infrastructure. Long term, integrate automated analysis tools such as tfsec into the development workow to detect other areas of improvement in the solution. References   What is Shielded VM? , Compute Engine documentation Using GKE Shielded Nodes, GKE documentation", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "22. Excessive container permissions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "Kubernetes containers launch processes under user and group IDs corresponding to users and groups on the host system. Container processes that are running as root usually have more permissions than their workload requires. If such a process were compromised, the permissions would enable the attacker to perform further attacks against the container or host. Kubernetes provides several ways to further limit these permissions, such as disabling the allowPrivilegeEscalation ag to ensure that a child process of a container cannot gain more privileges than its parent, dropping all Linux capabilities, and enforcing Seccomp and AppArmor proles. We found several instances of containers run as root, with allowPrivilegeEscalation enabled by omission (gure 22.1) or with low user IDs that overlap with host user IDs (gure 22.2). In some of the containers, Linux capabilities were not dropped (gure 22.2), and neither Seccomp nor AppArmor proles were enabled. 24 containers : 25 - name : auth-backend 26 27 28 29 30 image : \"{{ .Values.image.repository }}@{{ .Values.image.digest }}\" ports : - containerPort : 3000 env : - (...) Figure 22.1: Without a securityContext eld, commands will run as root and a container will allow privilege escalation by default. ( charts/galoy-auth/charts/auth-backend/templates/deployment.yaml#2430 ) 38 39 40 41 42 43 44 45 securityContext : # capabilities: # drop: # - ALL readOnlyRootFilesystem : true runAsNonRoot : true runAsUser : 1000 runAsGroup : 3000 Figure 22.2: User ID 1000 is typically used by the rst non-system user account. ( charts/bitcoind/values.yaml#3845 ) Exploit Scenario An attacker is able to trigger remote code execution in the Web Wallet application. The attacker then leverages the lax permissions to exploit CVE-2022-0185, a buer overow vulnerability in the Linux kernel that allows her to obtain root privileges and escape the Kubernetes pod. The attacker then gains the ability to execute code on the host system. Recommendations Short term, review and adjust the securityContext conguration of all charts used by the Galoy system. Run pods as non-root users with high user IDs that will not overlap with host user IDs. Drop all unnecessary capabilities, and enable security policy enforcement when possible. Long term, integrate automated tools such as Checkov into the CI pipeline to detect areas of improvement in the solution. Additionally, review the Docker recommendations outlined in appendix E . References   Kubernetes container escape using Linux Kernel exploit , CrowdStrike 10 Kubernetes Security Context settings you should understand, snyk", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "23. Unsigned and unversioned Grafana BigQuery Datasource plugin ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Galoy.pdf", "body": "The BigQuery Datasource plugin is installed as part of the Grafana conguration found in the Helm charts. The plugin, which is unsigned, is pulled directly from the master branch of the doitintl/bigquery-grafana GitHub repository, and signature checks for the plugin are disabled. Grafana advises against running unsigned plugins. 10 11 plugins : - https://github.com/doitintl/bigquery-grafana/archive/master.zip ;doit-bigquery-dataso urce 12 13 14 15 grafana.ini : plugins : allow_loading_unsigned_plugins : \"doitintl-bigquery-datasource\" Figure 23.1: The plugin is downloaded directly from the GitHub repository, and signature checks are disabled. ( charts/monitoring/values.yaml#1015 ) Exploit Scenario An attacker compromises the doitintl/bigquery-grafana repository and pushes malicious code to the master branch. When Grafana is set up, it downloads the plugin code from the master branch. Because unsigned plugins are allowed, Grafana directly loads the malicious plugin. Recommendations Short term, install the BigQuery Datasource plugin from a signed source such as the Grafana catalog, and disallow the loading of any unsigned plugins. Long term, review the vendor recommendations when conguring new software and avoid disabling security features such as signature checks. When referencing external code and software releases, do so by immutable hash digests instead of named tags or branches to prevent unintended modications. References  Plugin Signatures , Grafana Labs 24. Insu\u0000cient validation of JWTs used for GraphQL subscriptions Severity: Low Diculty: Low Type: Authentication Finding ID: TOB-GALOY-24 Target: galoy/src/servers/graphql-server.ts", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Reliance on third-party library for deployment ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Due to the use of the delegatecall proxy pattern, some NFTX contracts cannot be initialized with their own constructors; instead, they have initializer functions. These functions can be front-run, allowing an attacker to initialize contracts incorrectly. function __NFTXInventoryStaking_init(address _nftxVaultFactory) external virtual override initializer { __Ownable_init(); nftxVaultFactory = INFTXVaultFactory(_nftxVaultFactory); address xTokenImpl = address(new XTokenUpgradeable()); __UpgradeableBeacon__init(xTokenImpl); } Figure 1.1: The initializer function in NFTXInventoryStaking.sol:37-42 The following contracts have initializer functions that can be front-run:  NFTXInventoryStaking  NFTXVaultFactoryUpgradeable  NFTXEligibilityManager  NFTXLPStaking  NFTXSimpleFeeDistributor The NFTX team relies on hardhat-upgrades, a library that oers a series of safety checks for use with certain OpenZeppelin proxy reference implementations to aid in the proxy deployment process. It is important that the NFTX team become familiar with how the hardhat-upgrades library works internally and with the caveats it might have. For example, some proxy patterns like the beacon pattern are not yet supported by the library. Exploit Scenario Bob uses the library incorrectly when deploying a new contract: he calls upgradeTo() and then uses the fallback function to initialize the contract. Eve front-runs the call to the initialization function and initializes the contract with her own address, which results in an incorrect initialization and Eves control over the contract. Recommendations Short term, document the protocols use of the library and the proxy types it supports. Long term, use a factory pattern instead of the initializer functions to prevent front-running of the initializer functions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Missing validation of proxy admin indices ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Multiple functions of the ProxyController contract take an index as an input. The index determines which proxy (managed by the controller) is being targeted. However, the index is never validated, which means that the function will be executed even if the index is out of bounds with respect to the number of proxies managed by the contract (in this case, ve). function changeProxyAdmin(uint256 index, address newAdmin) public onlyOwner { } if (index == 0) { vaultFactoryProxy.changeAdmin(newAdmin); } else if (index == 1) { eligManagerProxy.changeAdmin(newAdmin); } else if (index == 2) { stakingProviderProxy.changeAdmin(newAdmin); } else if (index == 3) { stakingProxy.changeAdmin(newAdmin); } else if (index == 4) { feeDistribProxy.changeAdmin(newAdmin); } emit ProxyAdminChanged(index, newAdmin); Figure 2.1: The changeProxyAdmin function in ProxyController.sol:79-95 In the changeProxyAdmin function, a ProxyAdminChanged event is emitted even if the supplied index is out of bounds (gure 2.1). Other ProxyController functions return the zero address if the index is out of bounds. For example, getAdmin() should return the address of the targeted proxys admin. If getAdmin() returns the zero address, the caller cannot know whether she supplied the wrong index or whether the targeted proxy simply has no admin. function getAdmin(uint256 index) public view returns (address admin) { if (index == 0) { return vaultFactoryProxy.admin(); } else if (index == 1) { return eligManagerProxy.admin(); } else if (index == 2) { return stakingProviderProxy.admin(); } else if (index == 3) { return stakingProxy.admin(); } else if (index == 4) { return feeDistribProxy.admin(); } } Figure 2.2: The getAdmin function in ProxyController.sol:38-50 Exploit Scenario A contract relying on the ProxyController contract calls one of the view functions, like getAdmin(), with the wrong index. The function is executed normally and implicitly returns zero, leading to unexpected behavior. Recommendations Short term, document this behavior so that clients are aware of it and are able to include safeguards to prevent unanticipated behavior. Long term, consider adding an index check to the aected functions so that they revert if they receive an out-of-bounds index.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Random token withdrawals can be gamed ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "The algorithm used to randomly select a token for withdrawal from a vault is deterministic and predictable. function getRandomTokenIdFromVault() internal virtual returns (uint256) { uint256 randomIndex = uint256( keccak256( abi.encodePacked( blockhash(block.number - 1), randNonce, block.coinbase, block.difficulty, block.timestamp ) ) ) % holdings.length(); ++randNonce; return holdings.at(randomIndex); } Figure 3.1: The getRandomTokenIdFromVault function in NFTXVaultUpgradable.sol:531-545 All the elements used to calculate randomIndex are known to the caller (gure 3.1). Therefore, a contract calling this function can predict the resulting token before choosing to execute the withdrawal. This nding is of high diculty because NFTXs vault economics incentivizes users to deposit tokens of equal value. Moreover, the cost of deploying a custom exploit contract will likely outweigh the fee savings of choosing a token at random for withdrawal. Exploit Scenario Alice wishes to withdraw a specic token from a vault but wants to pay the lower random redemption fee rather than the higher target redemption fee. She deploys a contract that checks whether the randomly chosen token is her target and, if so, automatically executes the random withdrawal. Recommendations Short term, document the risks described in this nding so that clients are aware of them. Long term, consider removing all randomness from NFTX.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Duplicate receivers allowed by addReceiver() ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "The NFTXSimpleFeeDistributor contract is in charge of protocol fee distribution. To facilitate the fee distribution process, it allows the contract owner (the NFTX DAO) to manage a list of fee receivers. To add a new fee receiver to the contract, the owner calls the addReceiver() function. function addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) external override virtual onlyOwner { _addReceiver(_allocPoint, _receiver, _isContract); } Figure 4.1: The addReceiver() function in NFTXSimpleFeeDistributor This function in turn executes the internal logic that pushes a new receiver to the receiver list. function _addReceiver( uint256 _allocPoint, address _receiver, bool _isContract ) internal virtual { FeeReceiver memory _feeReceiver = FeeReceiver(_allocPoint, _receiver, _isContract); feeReceivers.push(_feeReceiver); allocTotal += _allocPoint; emit AddFeeReceiver(_receiver, _allocPoint); } Figure 4.2: The _addReceiver() function in NFTXSimpleFeeDistributor However, the function does not check whether the receiver is already in the list. Without this check, receivers can be accidentally added multiple times to the list, which would increase the amount of fees they receive. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a DAO proposal has to be created and a certain quorum has to be met for it to be executed. Exploit Scenario A proposal is created to add a new receiver to the fee distributor contract. The receiver address was already added, but the DAO members are not aware of this. The proposal passes, and the receiver is added. The receiver gains more fees than he is entitled to. Recommendations Short term, document this behavior so that the NFTX DAO is aware of it and performs the adequate checks before adding a new receiver. Long term, consider adding a duplicate check to the _addReceiver() function.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. OpenZeppelin vulnerability can break initialization ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "NFTX extensively uses OpenZeppelin v3.4.1. A bug was recently discovered in all OpenZeppelin versions prior to v4.4.1 that aects initializer functions invoked separately during contract creation: the bug causes the contract initialization modier to fail to prevent reentrancy to the initializers (see CVE-2021-46320). Currently, no external calls to untrusted code are made during contract initialization. However, if the NFTX team were to add a new feature that requires such calls to be made, it would have to add the necessary safeguards to prevent reentrancy. Exploit Scenario An NFTX contract initialization function makes a call to an external contract that calls back to the initializer with dierent arguments. The faulty OpenZeppelin initializer modier fails to prevent this reentrancy. Recommendations Short term, upgrade OpenZeppelin to v4.4.1 or newer. Long term, integrate a dependency checking tool like Dependabot into the NFTX CI process.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Potentially excessive gas fees imposed on users for protocol fee distribution ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. function _chargeAndDistributeFees(address user, uint256 amount) internal virtual { // Do not charge fees if the zap contract is calling // Added in v1.0.3. Changed to mapping in v1.0.5. INFTXVaultFactory _vaultFactory = vaultFactory; if (_vaultFactory.excludedFromFees(msg.sender)) { return; } // Mint fees directly to the distributor and distribute. if (amount > 0) { address feeDistributor = _vaultFactory.feeDistributor(); // Changed to a _transfer() in v1.0.3. _transfer(user, feeDistributor, amount); INFTXFeeDistributor(feeDistributor).distribute(vaultId); } } Figure 6.1: The _chargeAndDistributeFees() function in NFTXVaultUpgradeable.sol After the fee is sent to the NFXTSimpleFeeDistributor contract, the distribute() function is then called to distribute all accrued fees. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 6.2: The distribute() function in NFTXSimpleFeeDistributor.sol If the token balance of the contract is low enough (but not zero), the number of tokens distributed to each receiver (amountToSend) will be close to zero. Ultimately, this can disincentivize the use of the protocol, regardless of the number of tokens distributed. Users have to pay the gas fee for the fee distribution operation, the gas fees for the token operations (e.g., redeeming, minting, or swapping), and the protocol fees themselves. Exploit Scenario Alice redeems a token from a vault, pays the necessary protocol fee, sends it to the NFTXSimpleFeeDistributor contract, and calls the distribute() function. Because the balance of the distributor contract is very low (e.g., $0.50), Alice has to pay a substantial amount in gas to distribute a near-zero amount in fees between all fee receiver addresses. Recommendations Short term, add a requirement for a minimum balance that the NFTXSimpleFeeDistributor contract should have for the distribution operation to execute. Alternatively, implement a periodical distribution of fees (e.g., once a day or once every number of blocks). Long term, consider redesigning the fee distribution mechanism to prevent the distribution of small fees. Also consider whether protocol users should pay for said distribution. See appendix D for guidance on redesigning this mechanism.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "7. Risk of denial of service due to unbounded loop ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "When protocol fees are distributed, the system loops through the list of beneciaries (known internally as receivers) to send them the protocol fees they are entitled to. function distribute(uint256 vaultId) external override virtual nonReentrant { require(nftxVaultFactory != address(0)); address _vault = INFTXVaultFactory(nftxVaultFactory).vault(vaultId); uint256 tokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); if (distributionPaused || allocTotal == 0) { IERC20Upgradeable(_vault).safeTransfer(treasury, tokenBalance); return; } uint256 length = feeReceivers.length; uint256 leftover; for (uint256 i; i < length; ++i) { FeeReceiver memory _feeReceiver = feeReceivers[i]; uint256 amountToSend = leftover + ((tokenBalance * _feeReceiver.allocPoint) / allocTotal); uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); amountToSend = amountToSend > currentTokenBalance ? currentTokenBalance : amountToSend; bool complete = _sendForReceiver(_feeReceiver, vaultId, _vault, amountToSend); if (!complete) { uint256 remaining = IERC20Upgradeable(_vault).allowance(address(this), _feeReceiver.receiver); IERC20Upgradeable(_vault).safeApprove(_feeReceiver.receiver, 0); leftover = remaining; } else { leftover = 0; } } if (leftover != 0) { uint256 currentTokenBalance = IERC20Upgradeable(_vault).balanceOf(address(this)); IERC20Upgradeable(_vault).safeTransfer(treasury, currentTokenBalance); } } Figure 7.1: The distribute() function in NFTXSimpleFeeDistributor.sol Because this loop is unbounded and the number of receivers can grow, the amount of gas consumed is also unbounded. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 7.2: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol Additionally, if one of the receivers is a contract, code that signicantly increases the gas cost of the fee distribution will execute (gure 7.2). It is important to note that fees are usually distributed within the context of user transactions (redeeming, minting, etc.), so the total cost of the distribution operation depends on the logic outside of the distribute() function. Exploit Scenario The NFTX team adds a new feature that allows NFTX token holders who stake their tokens to register as receivers and gain a portion of protocol fees; because of that, the number of receivers grows dramatically. Due to the large number of receivers, the distribute() function cannot execute because the cost of executing it has reached the block gas limit. As a result, users are unable to mint, redeem, or swap tokens. Recommendations Short term, examine the execution cost of the function to determine the safe bounds of the loop and, if possible, consider splitting the distribution operation into multiple calls. Long term, consider redesigning the fee distribution mechanism to avoid unbounded loops and prevent denials of service. See appendix D for guidance on redesigning this mechanism.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "8. A malicious fee receiver can cause a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Whenever a user executes a minting, redeeming, or swapping operation on a vault, a fee is charged to the user and is sent to the NFXTSimpleFeeDistributor contract for distribution. The distribution function loops through all fee receivers and sends them the number of tokens they are entitled to (see gure 7.1). If the fee receiver is a contract, a special logic is executed; instead of receiving the corresponding number of tokens, the receiver pulls all the tokens from the NFXTSimpleFeeDistributor contract. function _sendForReceiver(FeeReceiver memory _receiver, uint256 _vaultId, address _vault, uint256 amountToSend) internal virtual returns (bool) { if (_receiver.isContract) { IERC20Upgradeable(_vault).safeIncreaseAllowance(_receiver.receiver, amountToSend); bytes memory payload = abi.encodeWithSelector(INFTXLPStaking.receiveRewards.selector, _vaultId, amountToSend); (bool success, ) = address(_receiver.receiver).call(payload); // If the allowance has not been spent, it means we can pass it forward to next. return success && IERC20Upgradeable(_vault).allowance(address(this), _receiver.receiver) == 0; } else { IERC20Upgradeable(_vault).safeTransfer(_receiver.receiver, amountToSend); return true; } } Figure 8.1: The _sendForReceiver() function in NFTXSimpleFeeDistributor.sol In this case, because the receiver contract executes arbitrary logic and receives all of the gas, the receiver contract can spend all of it; as a result, only 1/64 of the original gas forwarded to the receiver contract would remain to continue executing the distribute() function (see EIP-150), which may not be enough to complete the execution, leading to a denial of service. The issue is of high diculty because the addReceiver() function is owner-protected and, as indicated by the NFTX team, the owner is the NFTX DAO. Because the DAO itself was out of scope for this review, we do not know what the process to become a receiver looks like. We assume that a proposal is created and a certain quorum has to be met for it to be executed. Exploit Scenario Eve, a malicious receiver, sets up a smart contract that consumes all the gas forwarded to it when receiveRewards is called. As a result, the distribute() function runs out of gas, causing a denial of service on the vaults calling the function. Recommendations Short term, change the fee distribution mechanism so that only a token transfer is executed even if the receiver is a contract. Long term, consider redesigning the fee distribution mechanism to prevent malicious fee receivers from causing a denial of service on the protocol. See appendix D for guidance on redesigning this mechanism.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "9. Vault managers can grief users ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "The process of creating vaults in the NFTX protocol is trustless. This means that anyone can create a new vault and use any asset as the underlying vault NFT. The user calls the NFTXVaultFactoryUpgradeable contract to create a new vault. After deploying the new vault, the contract sets the user as the vault manager. Vault managers can disable certain vault features (gure 9.1) and change vault fees (gure 9.2). function setVaultFeatures( bool _enableMint, bool _enableRandomRedeem, bool _enableTargetRedeem, bool _enableRandomSwap, bool _enableTargetSwap ) public override virtual { onlyPrivileged(); enableMint = _enableMint; enableRandomRedeem = _enableRandomRedeem; enableTargetRedeem = _enableTargetRedeem; enableRandomSwap = _enableRandomSwap; enableTargetSwap = _enableTargetSwap; emit EnableMintUpdated(_enableMint); emit EnableRandomRedeemUpdated(_enableRandomRedeem); emit EnableTargetRedeemUpdated(_enableTargetRedeem); emit EnableRandomSwapUpdated(_enableRandomSwap); emit EnableTargetSwapUpdated(_enableTargetSwap); } Figure 9.1: The setVaultFeatures() function in NFTXVaultUpgradeable.sol function setFees( uint256 _mintFee, uint256 _randomRedeemFee, uint256 _targetRedeemFee, uint256 _randomSwapFee, uint256 _targetSwapFee ) public override virtual { onlyPrivileged(); vaultFactory.setVaultFees( vaultId, _mintFee, _randomRedeemFee, _targetRedeemFee, _randomSwapFee, _targetSwapFee ); } Figure 9.2: The setFees() function in NFTXVaultUpgradeable.sol The eects of these functions are instantaneous, which means users may not be able to react in time to these changes and exit the vaults. Additionally, disabling vault features with the setVaultFeatures() function can trap tokens in the contract. Ultimately, this risk is related to the trustless nature of vault creation, but the NFTX team can take certain measures to minimize the eects. One such measure, which is already in place, is vault verication, in which the vault manager calls the finalizeVault() function to pass her management rights to the zero address. This function then gives the veried status to the vault in the NFTX web application. Exploit Scenario Eve, a malicious manager, creates a new vault for a popular NFT collection. After it gains some user traction, she unilaterally changes the vault fees to the maximum (0.5 ether), which forces users to either pay the high fee or relinquish their tokens. Recommendations Short term, document the risks of interacting with vaults that have not been nalized (i.e., vaults that have managers). Long term, consider adding delays to manager-only functionality (e.g., a certain number of blocks) so that users have time to react and exit the vault.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "10. Lack of zero address check in functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/NFTX.pdf", "body": "Certain setter functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. This issue aects the following contracts and functions:  NFTXInventoryStaking.sol  __NFTXInventoryStaking_init()  NFTXSimpleFeeDistributor.sol  setInventoryStakingAddress()  addReceiver()  changeReceiverAddress()  RewardDistributionToken  __RewardDistributionToken_init() Exploit Scenario Alice deploys a new version of the NFTXInventoryStaking contract. When she initializes the proxy contract, she inputs the zero address as the address of the _nftxVaultFactory state variable, leading to an incorrect initialization. Recommendations Short term, add zero-value checks on all function arguments to ensure that users cannot accidentally set incorrect values, misconguring the system. Long term, use Slither, which will catch functions that do not have zero checks. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Out-of-bounds crash in extract_claims ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The strip_custom_section function does not suciently validate data and crashes when the range is not within the buer (gure 1.1). The function is used in the extract_claims function and is given an untrusted input. In the wasmCloud-otp , even though extract_claims is called as an Erlang NIF (Native Implemented Function) and potentially could bring down the VM upon crashing, the panic is handled gracefully by the Rustler library, resulting in an isolated crash of the Elixir process. if let Some ((id, range )) = payload.as_section() { wasm_encoder::RawSection { id, data: & buf [range] , } .append_to(& mut output); } Figure 1.1: wascap/src/wasm.rs#L161-L167 We found this issue by fuzzing the extract_claims function with cargo-fuzz (gure 2.1). #![no_main] use libfuzzer_sys::fuzz_target; use getrandom::register_custom_getrandom; // TODO: the program wont compile without this, why? fn custom_getrandom (buf: & mut [ u8 ]) -> Result <(), getrandom::Error> { return Ok (()); } register_custom_getrandom!(custom_getrandom); fuzz_target!(|data: & [ u8 ]| { let _ = wascap::wasm::extract_claims(data); }); Figure 1.2: A simple extract_claims fuzzing harness that passes the fuzzer-provided bytes straight to the function After xing the issue (gure 1.3), we fuzzed the function for an extended period of time; however, we found no additional issues. if let Some ((id, range)) = payload.as_section() { if range.end <= buf.len() { wasm_encoder::RawSection { id, data: & buf [range], } .append_to(& mut output); } else { return Err (errors::new(ErrorKind::InvalidCapability)); } } Figure 1.3: The x we applied to continue fuzzing extract_claims . The code requires a new error value because we reused one of the existing ones that likely does not match the semantics. Exploit Scenario An attacker deploys a new module with invalid claims. While decoding the claims, the extract_claims function panics and crashes the Elixir process. Recommendations Short term, x the strip_custom_section function by adding the range check, as shown in the gure 1.3. Add the extract_claims fuzzing harness to the wascap repository and run it for an extended period of time before each release of the library. Long term, add a fuzzing harness for each Rust function that processes user-provided data. References  Erlang - NIFs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. Stack overow while enumerating containers in blobstore-fs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The all_dirs function is vulnerable to a stack overow caused by unbounded recursion, triggered by either the presence of circular symlinks inside the root of the blobstore (as congured during startup), or the presence of excessively nested directory inside the same. Because this function is used by FsProvider::list_containers , this issue would result in a denial of service for all actors that use the method exposed by aected blobstores. let mut subdirs: Vec <PathBuf> = Vec ::new(); for dir in &dirs { let mut local_subdirs = all_dirs(prefix.join(dir.as_path()).as_path(), prefix); subdirs.append( &mut local_subdirs); } dirs.append( &mut subdirs); dirs Figure 2.1: capability-providers/blobstore-fs/src/fs_utils.rs#L24-L30 Exploit Scenario An attacker creates a circular symlink inside the storage directory. Alternatively, an attacker canunder the right circumstancescreate successively nested directories with a sucient depth to cause a stack overow. blobstore.create_container(ctx, &\"a\".to_string()). await ?; blobstore.create_container(ctx, &\"a/a\".to_string()). await ?; blobstore.create_container(ctx, &\"a/a/a\".to_string()). await ?; ... blobstore.create_container(ctx, &\"a/a/a/.../a/a/a\".to_string()). await ?; blobstore.list_containers(). await ?; Figure 2.2: Possible attack to a vulnerable blobstore In practice, this attack requires the underlying le system to allow exceptionally long lenames, and we have not been able to produce a working attack payload. However, this does not prove that no such le systems exist or will exist in the future. Recommendations Short term, limit the amount of allowable recursion depth to ensure that no stack overow attack is possible given realistic stack sizes, as shown in gure 2.3. pub fn all_dirs(root: &Path, prefix: &Path, depth: i32 ) -> Vec <PathBuf> { if depth > 1000 { return vec![]; } ... // Now recursively go in all directories and collect all sub-directories let mut subdirs: Vec <PathBuf> = Vec ::new(); for dir in &dirs { let mut local_subdirs = all_dirs( prefix.join(dir.as_path()).as_path(), prefix, depth + 1 ); subdirs.append( &mut local_subdirs); } dirs.append( &mut subdirs); dirs } Figure 2.3: Limiting the amount of allowable recursion depth Long term, consider limiting the reliance on the underlying le system to a minimum by disallowing nesting containers. For example, Base64-encode all container and object names before passing them down to the le system routines. References  OWASP Denial of Service Cheat Sheet (\"Input validation\" section)", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Denial of service in blobstore-s3 using malicious actor ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The stream_bytes function continues looping until it detects that all of the available bytes have been sent. It does this based on the output of the send_chunk function, which reports the amount of bytes that have been sent by the call. An attacker could send specially crafted responses that cause stream_bytes to continue looping, causing send_chunk to report that no errors were detected while also reporting that no bytes were sent. while bytes_sent < bytes_to_send { let chunk_offset = offset + bytes_sent; let chunk_len = (self.max_chunk_size() as u64).min(bytes_to_send - bytes_sent); bytes_sent += self .send_chunk ( ctx, Chunk { is_last: offset + chunk_len > end_range, bytes: bytes[bytes_sent as usize..(bytes_sent + chunk_len) as usize] .to_vec(), offset: chunk_offset as u64, container_id: bucket_id.to_string(), object_id: cobj.object_id.clone(), }, ) .await?; } Figure 3.1: capability-providers/blobstore-s3/src/lib.rs#L188-L204 Exploit Scenario An attacker can send a maliciously crafted request to get an object from a blobstore-s3 provider, then send successful responses without making actual progress in the transfer by reporting that empty-sized chunks were received. Recommendations Make send_chunk report a failure if a zero-sized response is received.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "4. Unexpected panic in validate_token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The validate_token function from the wascap library panics with an out-of-bounds error when input is given in an unexpected format. The function expects the input to be a valid JWT token with three segments separated by a dot (gure 4.1). This implicit assumption is satised in the code; however, the function is public and does not mention the assumption in its documentation. /// Validates a signed JWT. This will check the signature, expiration time, and not-valid-before time pub fn validate_token <T>(input: &str ) -> Result <TokenValidation> where T: Serialize + DeserializeOwned + WascapEntity, { } let segments: Vec <& str > = input.split( '.' ).collect(); let header_and_claims = format! ( \"{}.{}\" , segments[ 0 ] , segments[ 1 ] ); let sig = base64::decode_config( segments[ 2 ] , base64::URL_SAFE_NO_PAD)?; ... Figure 4.1: wascap/src/jwt.rs#L612-L641 Exploit Scenario A developer uses the validate_token function expecting it to fully validate the token string. The function receives an untrusted malicious input that forces the program to panic. Recommendations Short term, add input format validation before accessing the segments and a test case with malformed input. Long term, always validate all inputs to functions or document the input assumptions if validation is not in place for a specic reason.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Incorrect error message when starting actor from le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-09-wasmCloud-securityreview.pdf", "body": "The error message when starting an actor from a le contains a string interpolation bug that causes the message to not include the fileref content (gure 5.1). This causes the error message to contain the literal string ${fileref} instead. It is worth noting that the leref content will be included anyway as an attribute. Logger .error( \"Failed to read actor file from ${fileref} : #{ inspect(err) } \" , fileref : fileref ) Figure 5.1: host_core/lib/host_core/actors/actor_supervisor.ex#L301 Recommendations Short term, change the error message to correctly interpolate the fileref string. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Denial-of-service conditions caused by the use of more than 256 slices ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "The owner of a Proteus-based automated market maker (AMM) can update the system parameters to cause a denial of service (DoS) upon the execution of swaps, withdrawals, and deposits. The Proteus AMM engine design supports the creation of an arbitrary number of slices. Slices are used to segment an underlying bonding curve and provide variable liquidity across that curve. The owner of a Proteus contract can update the number of slices by calling the _updateSlices function at any point. When a user requests a swap, deposit, or withdrawal operation, the Proteus contract rst calls the _findSlice function (gure 1.1) to identify the slice in which it should perform the operation. The function iterates across the slices array and returns the index, i, of the slice that has the current ratio of token balances, m. function _findSlice(int128 m) internal view returns (uint8 i) { i = 0; while (i < slices.length) { if (m <= slices[i].mLeft && m > slices[i].mRight) return i; unchecked { ++i; } } // while loop terminates at i == slices.length // if we just return i here we'll get an index out of bounds. return i - 1; } Figure 1.1: The _findSlice() function in Proteus.sol#L1168-1179 However, the index, i, is dened as a uint8. If the owner sets the number of slices to at least 257 (by calling _updateSlices) and the current m is in the 257th slice, i will silently overow, and the while loop will continue until an out-of-gas (OOG) exception occurs. If a deposit, withdrawal, or swap requires the 257th slice to be accessed, the operation will fail because the _findSlice function will be unable to reach that slice. Exploit Scenario Eve creates a seemingly correct Proteus-based primitive (one with only two slices near the asymptotes of the bonding curve). Alice deposits assets worth USD 100,000 into a pool. Eve then makes a deposit of X and Y tokens that results in a token balance ratio, m, of 1. Immediately thereafter, Eve calls _updateSlices and sets the number of slices to 257, causing the 256th slice to have an m of 1.01. Because the current m resides in the 257th slice, the _findSlice function will be unable to nd that slice in any subsequent swap, deposit, or withdrawal operation. The system will enter a DoS condition in which all future transactions will fail. If Eve identies an arbitrage opportunity on another exchange, Eve will be able to call _updateSlices again, use the unlocked curve to buy the token of interest, and sell that token on the other exchange for a pure prot. Eectively, Eve will be able to steal user funds. Recommendations Short term, change the index, i, from the uint8 type to uint256; alternatively, create an upper limit for the number of slices that can be created and ensure that i will not overow when the _findSlice function searches through the slices array. Long term, consider adding a delay between a call to _updateSlices and the time at which the call takes eect on the bonding curve. This will allow users to withdraw from the system if they are unhappy with the new parameters. Additionally, consider making slices immutable after their construction; this will signicantly reduce the risk of undened behavior.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. LiquidityPoolProxy owners can steal user funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "The LiquidityPoolProxy contract implements the IOceanPrimitive interface and can integrate with the Ocean contract as a primitive. The proxy contract calls into an implementation contract to perform deposit, swap, and withdrawal operations (gure 2.1). function swapOutput(uint256 inputToken, uint256 inputAmount) public view override returns (uint256 outputAmount) { (uint256 xBalance, uint256 yBalance) = _getBalances(); outputAmount = implementation.swapOutput(xBalance, yBalance, inputToken == xToken ? 0 : 1, inputAmount); } Figure 2.1: The swapOutput() function in LiquidityPoolProxy.sol#L3947 However, the owner of a LiquidityPoolProxy contract can perform the privileged operation of changing the underlying implementation contract via a call to setImplementation (gure 2.2). The owner could thus replace the underlying implementation with a malicious contract to steal user funds. function setImplementation(address _implementation) external onlyOwner { } implementation = ILiquidityPoolImplementation(_implementation); Figure 2.2: The setImplementation() function in LiquidityPoolProxy.sol#L2833 This level of privilege creates a single point of failure in the system. It increases the likelihood that a contracts owner will be targeted by an attacker and incentivizes the owner to act maliciously. Exploit Scenario Alice deploys a LiquidityPoolProxy contract as an Ocean primitive. Eve gains access to Alices machine and upgrades the implementation to a malicious contract that she controls. Bob attempts to swap USD 1 million worth of shDAI for shUSDC by calling computeOutputAmount. Eves contract returns 0 for outputAmount. As a result, the malicious primitives balance of shDAI increases by USD 1 million, but Bob does not receive any tokens in exchange for his shDAI. Recommendations Short term, document the functions and implementations that LiquidityPoolProxy contract owners can change. Additionally, split the privileges provided to the owner role across multiple roles to ensure that no one address has excessive control over the system. Long term, develop user documentation on all risks associated with the system, including those associated with privileged users and the existence of a single point of failure.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Risk of sandwich attacks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "The Proteus liquidity pool implementation does not use a parameter to prevent slippage. Without such a parameter, there is no guarantee that users will receive any tokens in a swap. The LiquidityPool contracts computeOutputAmount function returns an outputAmount value indicating the number of tokens a user should receive in exchange for the inputAmount. Many AMM protocols enable users to specify the minimum number of tokens that they would like to receive in a swap. This minimum number of tokens (indicated by a slippage parameter) protects users from receiving fewer tokens than expected. As shown in gure 3.1, the computeOutputAmount function signature includes a 32-byte metadata eld that would allow a user to encode a slippage parameter. function computeOutputAmount( uint256 inputToken, uint256 outputToken, uint256 inputAmount, address userAddress, bytes32 metadata ) external override onlyOcean returns (uint256 outputAmount) { Figure 3.1: The signature of the computeOutputAmount() function in LiquidityPool.sol#L192198 However, this eld is not used in swaps (gure 3.2) and thus does not provide any protection against excessive slippage. By using a bot to sandwich a users trade, an attacker could increase the slippage incurred by the user and prot o of the spread at the users expense. function computeOutputAmount( uint256 inputToken, uint256 outputToken, uint256 inputAmount, address userAddress, bytes32 metadata ) external override onlyOcean returns (uint256 outputAmount) { ComputeType action = _determineComputeType(inputToken, outputToken); [...] } else if (action == ComputeType.Swap) { // Swap action + computeOutput context => swapOutput() outputAmount = swapOutput(inputToken, inputAmount); emit Swap( inputAmount, outputAmount, metadata, userAddress, (inputToken == xToken), true ); } [...] Figure 3.2: Part of the computeOutputAmount() function in LiquidityPool.sol#L192260 Exploit Scenario Alice wishes to swap her shUSDC for shwETH. Because the computeOutputAmount functions metadata eld is not used in swaps to prevent excessive slippage, the trade can be executed at any price. As a result, when Eve sandwiches the trade with a buy and sell order, Alice sells the tokens without purchasing any, eectively giving away tokens for free. Recommendations Short term, document the fact that protocols that choose to use the Proteus AMM engine should encode a slippage parameter in the metadata eld. The use of this parameter will reduce the likelihood of sandwich attacks against protocol users. Long term, ensure that all calls to computeOutputAmount and computeInputAmount use slippage parameters when necessary, and consider relying on an oracle to ensure that the amount of slippage that users can incur in trades is appropriately limited.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "4. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "Although dependency scans did not identify a direct threat to the project under review, npm and yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure that dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repository under review. The output below details these issues: CVE ID", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "5. Use of duplicate functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "The ProteusLogic and Proteus contracts both contain a function used to update the internal slices array. Although calls to these functions currently lead to identical outcomes, there is a risk that a future update could be applied to one function but not the other, which would be problematic. < function _updateSlices(int128[] memory slopes, int128[] memory rootPrices) internal { < require(slopes.length == rootPrices.length); < require(slopes.length > 1); --- > function _updateSlices(int128[] memory slopes, int128[] memory rootPrices) > internal > { > if (slopes.length != rootPrices.length) { > revert UnequalArrayLengths(); > } > if (slopes.length < 2) { > revert TooFewParameters(); > } Figure 5.1: The di between the ProteusLogic and Proteus contracts _updateSlices() functions Using duplicate functions in dierent contracts is not best practice. It increases the risk of a divergence between the contracts and could signicantly aect the system properties. Dening a function in one contract and having other contracts call that function is less risky. Exploit Scenario Alice, a developer of the Shell Protocol, is tasked with updating the ProteusLogic contract. The update requires a change to the Proteus._updateSlices function. However, Alice forgets to update the ProteusLogic._updateSlices function. Because of this omission, the functions updates to the internal slices array may produce dierent results. Recommendations Short term, select one of the two _updateSlices functions to retain in the codebase and to maintain going forward. Long term, consider consolidating the Proteus and ProteusLogic contracts into a single implementation, and avoid duplicating logic.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Certain identity curve congurations can lead to a loss of pool tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "A rounding error in an integer division operation could lead to a loss of pool tokens and the dilution of liquidity provider (LP) tokens. We reimplemented certain of Cowri Labss fuzz tests and used Echidna to test the system properties specied in the Automated Testing section. The original fuzz testing used a xed amount of 100 tokens for the initial xBalance and yBalance values; after we removed that limitation, Echidna was able to break some of the invariants. The Shell Protocol team should identify the largest possible percentage decrease in pool utility or utility per shell (UPS) to better quantify the impact of a broken invariant on system behavior. In some of the breaking cases, the ratio of token balances, m, was close to the X or Y asymptote of the identity curve. This means that an attacker might be able to disturb the balance of the pool (through ash minting or a large swap, for example) and then exploit the broken invariants. Exploit Scenario Alice withdraws USD 100 worth of token X from a Proteus-based liquidity pool by burning her LP tokens. She eventually decides to reenter the pool and to provide the same amount of liquidity. Even though the curves conguration is similar to the conguration at the time of her withdrawal, her deposit leads to only a USD 90 increase in the pools balance of token X; thus, Alice receives fewer LP tokens than she should in return, eectively losing money because of an arithmetic error. Recommendations Short term, investigate the root cause of the failing properties. Document and test the expected rounding direction (up or down) of each arithmetic operation, and ensure that the rounding direction used in each operation benets the pool. Long term, implement the fuzz testing recommendations outlined in appendix C.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Undetermined"]}, {"title": "7. Lack of events for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "Two critical operations do not trigger events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. The LiquidityPoolProxy contracts setImplementation function is called to set the implementation address of the liquidity pool and does not emit an event providing conrmation of that operation to the contracts caller (gure 7.1). function setImplementation(address _implementation) external onlyOwner { } implementation = ILiquidityPoolImplementation(_implementation); Figure 7.1: The setImplementation() function in LiquidityPoolProxy.sol#L2833 Calls to the updateSlices function in the Proteus contract do not trigger events either (gure 7.2). This is problematic because updates to the slices array have a signicant eect on the conguration of the identity curve (TOB-SHELL-1). function updateSlices(int128[] memory slopes, int128[] memory rootPrices) external onlyOwner { } _updateSlices(slopes, rootPrices); Figure 7.2: The updateSlices() function in Proteus.sol#L623628 Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior. Exploit Scenario Eve, an attacker, is able to take ownership of the LiquidityPoolProxy contract. She then sets a new implementation address. Alice, a Shell Protocol team member, is unaware of the change and does not raise a security incident. Recommendations Short term, add events for all critical operations that result in state changes. Events aid in contract monitoring and the detection of suspicious behavior. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "8. Ocean may accept unexpected airdrops ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/ShellProtocolv2.pdf", "body": "Unexpected transfers of tokens to the Ocean contract may break its internal accounting, essentially leading to the loss of the transferred asset. To mitigate this risk, Ocean attempts to reject airdrops. Per the ERC721 and ERC1155 standards, contracts must implement specic methods to accept or deny token transfers. To do this, the Ocean contract uses the onERC721Received and onERC1155Received callbacks and _ERC1155InteractionStatus and _ERC721InteractionStatus storage ags. These storage ags are enabled in ERC721 and ERC1155 wrapping operations to facilitate successful standard-compliant transfers. However, the _erc721Unwrap and _erc1155Unwrap functions also enable the _ERC721InteractionStatus and _ERC1155InteractionStatus ags, respectively. Enabling these ags allows for airdrops, since the Ocean contract, not the user, is the recipient of the tokens in unwrapping operations. function _erc721Unwrap( address tokenAddress, uint256 tokenId, address userAddress, uint256 oceanId ) private { _ERC721InteractionStatus = INTERACTION; IERC721(tokenAddress).safeTransferFrom( address(this), userAddress, tokenId ); _ERC721InteractionStatus = NOT_INTERACTION; emit Erc721Unwrap(tokenAddress, tokenId, userAddress, oceanId); } Figure 8.1: The _erc721Unwrap() function in Ocean.sol#L1020- Exploit Scenario Alice calls the _erc721Unwrap function. When the onERC721Received callback function in Alices contract is called, Alice mistakenly sends the ERC721 tokens back to the Ocean contract. As a result, her ERC721 is permanently locked in the contract and eectively burned. Recommendations Short term, disallow airdrops of standard-compliant tokens during unwrapping interactions and document the edge cases in which the Ocean contract will be unable to stop token airdrops. Long term, when the Ocean contract is expecting a specic airdrop, consider storing the originating address of the transfer and the token type alongside the relevant interaction ag.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Lack of domain separation allows proof forgery ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "Merkle trees are nested tree data structures in which the hash of each branch node depends upon the hashes of its children. The hash of each node is then assumed to uniquely represent the subtree of which that node is a root. However, that assumption may be false if a leaf node can have the same hash as a branch node. A general method for preventing leaf and branch nodes from colliding in this way is domain separation. That is, given a hash function , dene the hash of a leaf to be and the hash of a branch to be return the same result (perhaps because s return values all start with the byte 0 and s all start with the byte 1). Without domain separation, a malicious entity may be able to insert a leaf into the tree that can be later used as a branch in a Merkle path. , where and are encoding functions that can never  ((_)) ((_))     In zktrie, the hash for a node is dened by the NodeHash method, shown in gure 1.1. As shown in the highlighted portions, the hash of a branch node is HashElems(n.ChildL,n.ChildR), while the hash of a leaf node is HashElems(1,n.NodeKey,n.valueHash). // LeafHash computes the key of a leaf node given the hIndex and hValue of the // entry of the leaf. func LeafHash(k, v *zkt.Hash) (*zkt.Hash, error) { return zkt.HashElems(big.NewInt(1), k.BigInt(), v.BigInt()) } // NodeHash computes the hash digest of the node by hashing the content in a // specific way for each type of node. // Merkle tree for each node. func (n *Node) NodeHash() (*zkt.Hash, error) { This key is used as the hash of the if n.nodeHash == nil { // Cache the key to avoid repeated hash computations. // NOTE: We are not using the type to calculate the hash! switch n.Type { case NodeTypeParent: // H(ChildL || ChildR) var err error n.nodeHash, err = zkt.HashElems(n.ChildL.BigInt(), n.ChildR.BigInt()) if err != nil { return nil, err } case NodeTypeLeaf: n.ValuePreimage) var err error n.valueHash, err = zkt.PreHandlingElems(n.CompressedFlags, if err != nil { return nil, err } n.nodeHash, err = LeafHash(n.NodeKey, n.valueHash) if err != nil { return nil, err } case NodeTypeEmpty: // Zero n.nodeHash = &zkt.HashZero default: n.nodeHash = &zkt.HashZero } } return n.nodeHash, nil } Figure 1.1: NodeHash and LeafHash (zktrie/trie/zk_trie_node.go#118156) The function HashElems used here performs recursive hashing in a binary-tree fashion. For the purpose of this nding, the key property is that HashElems(1,k,v) == H(H(1,k),v) and HashElems(n.ChildL,n.ChildR) == H(n.ChildL,n.ChildR), where H is the global two-input, one-output hash function. Therefore, a branch node b and a leaf node l where b.ChildL == H(1,l.NodeKey) and b.ChildR == l.valueHash will have equal hash values. This allows proof forgery and, for example, a malicious entity to insert a key that can be proved to be both present and nonexistent in the tree, as illustrated by the proof-of-concept test in gure 1.2. func TestMerkleTree_ForgeProof(t *testing.T) { zkTrie := newTestingMerkle(t, 10) t.Run(\"Testing for malicious proofs\", func(t *testing.T) { // Find two distinct values k1,k2 such that the first step of // the path has the sibling on the LEFT (i.e., path[0] == // false) k1, k2 := (func() (zkt.Byte32, zkt.Byte32) { k1 := zkt.Byte32{1} k2 := zkt.Byte32{2} k1_hash, _ := k1.Hash() k2_hash, _ := k2.Hash() for !getPath(1, zkt.NewHashFromBigInt(k1_hash)[:])[0] { for i := len(k1); i > 0; i -= 1 { k1[i-1] += 1 if k1[i-1] != 0 { break } } k1_hash, _ = k1.Hash() } zkt.NewHashFromBigInt(k2_hash)[:])[0] { for k1 == k2 || !getPath(1, for i := len(k2); i > 0; i -= 1 { k2[i-1] += 1 if k2[i-1] != 0 { break } } k2_hash, _ = k2.Hash() } return k1, k2 })() k1_hash_int, _ := k1.Hash() k2_hash_int, _ := k2.Hash() k1_hash := zkt.NewHashFromBigInt(k1_hash_int) k2_hash := zkt.NewHashFromBigInt(k2_hash_int) // create a dummy value for k2, and use that to craft a // malicious value for k1 k2_value := (&[2]zkt.Byte32{{2}})[:] k1_value, _ := NewLeafNode(k2_hash, 1, k2_value).NodeHash() []zkt.Byte32{*zkt.NewByte32FromBytes(k1_value.Bytes())} k1_value_array := // insert k1 into the trie with the malicious value assert.Nil(t, zkTrie.TryUpdate(zkt.NewHashFromBigInt(k1_hash_int), 0, k1_value_array)) getNode := func(hash *zkt.Hash) (*Node, error) { return zkTrie.GetNode(hash) } // query an inclusion proof for k1 k1Proof, _, err := BuildZkTrieProof(zkTrie.rootHash, k1_hash_int, 10, getNode) assert.Nil(t, err) assert.True(t, k1Proof.Existence) // check that inclusion proof against our root hash k1_val_hash, _ := NewLeafNode(k1_hash, 0, k1_value_array).NodeHash() k1Proof_root, _ := k1Proof.Verify(k1_val_hash, k1_hash) assert.Equal(t, k1Proof_root, zkTrie.rootHash) // forge a non-existence proof fakeNonExistProof := *k1Proof fakeNonExistProof.Existence = false // The new non-existence proof needs one extra level, where // the sibling hash is H(1,k1_hash) fakeNonExistProof.depth += 1 zkt.SetBitBigEndian(fakeNonExistProof.notempties[:], fakeNonExistProof.depth-1) fakeSibHash, _ := zkt.HashElems(big.NewInt(1), k1_hash_int) fakeNonExistProof.Siblings = append(fakeNonExistProof.Siblings, fakeSibHash) // Construct the NodeAux details for the malicious leaf k2_value_hash, _ := zkt.PreHandlingElems(1, k2_value) k2_nodekey := zkt.NewHashFromBigInt(k2_hash_int) fakeNonExistProof.NodeAux = &NodeAux{Key: k2_nodekey, Value: k2_value_hash} // Check our non-existence proof against the root hash fakeNonExistProof_root, _ := fakeNonExistProof.Verify(k1_val_hash, assert.Equal(t, fakeNonExistProof_root, zkTrie.rootHash) // fakeNonExistProof and k1Proof prove opposite things. k1 // is both in and not-in the tree! assert.NotEqual(t, fakeNonExistProof.Existence, k1Proof.Existence) k1_hash) }) } Figure 1.2: A proof-of-concept test case for proof forgery Exploit Scenario Suppose Alice uses the zktrie to implement the Ethereum account table in a zkEVM-based bridge with trustless state updates. Bob submits a transaction that inserts specially crafted account data into some position in that tree. At a later time, Bob submits a transaction that depends on the result of an account table lookup. Bob generates two contradictory Merkle proofs and uses those proofs to create two zkEVM execution proofs that step to dierent nal states. By submitting one proof each to the opposite sides of the bridge, Bob causes state divergence and a loss of funds. Recommendations Short term, modify NodeHash to domain-separate leaves and branches, such as by changing the branch hash to zkt.HashElems(big.NewInt(2),n.ChildL.BigInt(), n.ChildR.BigInt()). Long term, fully document all data structure designs and requirements, and review all assumptions to ensure that they are well founded.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. Lack of proof validation causes denial of service on the verier ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The Merkle tree proof verier assumes several well-formedness properties about the received proof and node arguments. If at least one of these properties is violated, the verier will have a runtime error. The rst property that must hold is that the node associated with the Merkle proof must be a leaf node (i.e., must contain a non-nil NodeKey eld). If this is not the case, computing the rootFromProof for a nil NodeKey will cause a panic when computing the getPath function. Secondly, the Proof elds must be guaranteed to be consistent with the other elds. Assuming that the proof depth is correct will cause out-of-bounds accesses to both the NodeKey and the notempties eld. Finally, the Siblings array length should also be validated; for example, the VerifyProofZkTrie will panic due to an out-of-bounds access if the proof.Siblings eld is empty (highlighted in yellow in the rootFromProof function). // VerifyProof verifies the Merkle Proof for the entry and root. func VerifyProofZkTrie(rootHash *zkt.Hash, proof *Proof, node *Node) bool { nodeHash, err := node.NodeHash() if err != nil { return false } rootFromProof, err := proof.Verify(nodeHash, node.NodeKey) if err != nil { return false } return bytes.Equal(rootHash[:], rootFromProof[:]) } // Verify the proof and calculate the root, nodeHash can be nil when try to verify // a nonexistent proof func (proof *Proof) Verify(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { if proof.Existence { if nodeHash == nil { return nil, ErrKeyNotFound } return proof.rootFromProof(nodeHash, nodeKey) } else { if proof.NodeAux == nil { return proof.rootFromProof(&zkt.HashZero, nodeKey) } else { if bytes.Equal(nodeKey[:], proof.NodeAux.Key[:]) { return nil, fmt.Errorf(\"non-existence proof being checked against hIndex equal to nodeAux\") } midHash, err := LeafHash(proof.NodeAux.Key, proof.NodeAux.Value) if err != nil { return nil, err } return proof.rootFromProof(midHash, nodeKey) } } } func (proof *Proof) rootFromProof(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { var err error sibIdx := len(proof.Siblings) - 1 path := getPath(int(proof.depth), nodeKey[:]) var siblingHash *zkt.Hash for lvl := int(proof.depth) - 1; lvl >= 0; lvl-- { if zkt.TestBitBigEndian(proof.notempties[:], uint(lvl)) { siblingHash = proof.Siblings[sibIdx] sibIdx-- } else { siblingHash = &zkt.HashZero } if path[lvl] { nodeHash, err = NewParentNode(siblingHash, nodeHash).NodeHash() if err != nil { return nil, err } } else { nodeHash, err = NewParentNode(nodeHash, siblingHash).NodeHash() if err != nil { return nil, err } } } return nodeHash, nil } Figure 2.1: zktrie/trie/zk_trie_impl.go#595 Exploit Scenario An attacker crafts an invalid proof that causes the proof verier to crash, causing a denial of service in the system. Recommendations Short term, validate the proof structure before attempting to use its values. Add fuzz testing to the VerifyProofZkTrie function. Long term, add extensive tests and fuzz testing to functions interfacing with attacker-controlled values.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "3. Two incompatible ways to generate proofs ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "There are two incompatible ways to generate proofs. The rst implementation (gure 3.1) writes to a given callback, eectively returning []bytes. It does not have a companion verication function; it has only positive tests (zktrie/trie/zk_trie_test.go#L93-L125); and it is accessible from the C function TrieProve and the Rust function prove. The second implementation (gure 3.2) returns a pointer to a Proof struct. It has a companion verication function (zktrie/trie/zk_trie_impl.go#L595-L632); it has positive and negative tests (zktrie/trie/zk_trie_impl_test.go#L484-L537); and it is not accessible from C or Rust. // Prove is a simlified calling of ProveWithDeletion func (t *ZkTrie) Prove(key []byte, fromLevel uint, writeNode func(*Node) error) error { return t.ProveWithDeletion(key, fromLevel, writeNode, nil) } // ProveWithDeletion constructs a merkle proof for key. The result contains all encoded nodes // on the path to the value at key. The value itself is also included in the last // node and can be retrieved by verifying the proof. // // If the trie does not contain a value for key, the returned proof contains all // nodes of the longest existing prefix of the key (at least the root node), ending // with the node that proves the absence of the key. // // If the trie contain value for key, the onHit is called BEFORE writeNode being called, // both the hitted leaf node and its sibling node is provided as arguments so caller // would receive enough information for launch a deletion and calculate the new root // base on the proof data // Also notice the sibling can be nil if the trie has only one leaf func (t *ZkTrie) ProveWithDeletion(key []byte, fromLevel uint, writeNode func(*Node) error, onHit func(*Node, *Node)) error { [...] } Figure 3.1: The rst way to generate proofs (zktrie/trie/zk_trie.go#143164) // Proof defines the required elements for a MT proof of existence or // non-existence. type Proof struct { // existence indicates wether this is a proof of existence or // non-existence. Existence bool // depth indicates how deep in the tree the proof goes. depth uint // notempties is a bitmap of non-empty Siblings found in Siblings. notempties [zkt.HashByteLen - proofFlagsLen]byte // Siblings is a list of non-empty sibling node hashes. Siblings []*zkt.Hash // NodeAux contains the auxiliary information of the lowest common ancestor // node in a non-existence proof. NodeAux *NodeAux } // BuildZkTrieProof prove uniformed way to turn some data collections into Proof struct func BuildZkTrieProof(rootHash *zkt.Hash, k *big.Int, lvl int, getNode func(key *zkt.Hash) (*Node, error)) (*Proof, *Node, error) { [...] } Figure 3.2: The second way to generate proofs (zktrie/trie/zk_trie_impl.go#531551) Recommendations Short term, decide on one implementation and remove the other implementation. Long term, ensure full test coverage in the chosen implementation; ensure the implementation has both positive and negative testing; and add fuzz testing to the proof verication routine.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "4. BuildZkTrieProof does not populate NodeAux.Value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "A nonexistence proof for some key k in a Merkle tree is a Merkle path from the root of the tree to a subtree, which would contain k if it were present but which instead is either an empty subtree or a subtree with a single leaf k2 where k != k2. In the zktrie codebase, that second case is handled by the NodeAux eld in the Proof struct, as illustrated in gure 4.1. // Verify the proof and calculate the root, nodeHash can be nil when try to verify // a nonexistent proof func (proof *Proof) Verify(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { if proof.Existence { if nodeHash == nil { return nil, ErrKeyNotFound } return proof.rootFromProof(nodeHash, nodeKey) } else { if proof.NodeAux == nil { return proof.rootFromProof(&zkt.HashZero, nodeKey) } else { if bytes.Equal(nodeKey[:], proof.NodeAux.Key[:]) { return nil, fmt.Errorf(\"non-existence proof being checked against hIndex equal to nodeAux\") } midHash, err := LeafHash(proof.NodeAux.Key, proof.NodeAux.Value) if err != nil { return nil, err } return proof.rootFromProof(midHash, nodeKey) } } } Figure 4.1: The Proof.Verify method (zktrie/trie/zk_trie_impl.go#609632) When a non-inclusion proof is generated, the BuildZkTrieProof function looks up the other leaf node and uses its NodeKey and valueHash elds to populate the Key and Value elds of NodeAux, as shown in gure 4.2. However, the valueHash eld of this node may be nil, causing NodeAux.Value to be nil and causing proof verication to crash with a nil pointer dereference error, which can be triggered by the test case shown in gure 4.3. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.2: Populating NodeAux (zktrie/trie/zk_trie_impl.go#560574) func TestMerkleTree_GetNonIncProof(t *testing.T) { zkTrie := newTestingMerkle(t, 10) t.Run(\"Testing for non-inclusion proofs\", func(t *testing.T) { k := zkt.Byte32{1} k_value := (&[1]zkt.Byte32{{1}})[:] k_other := zkt.Byte32{2} k_hash_int, _ := k.Hash() k_other_hash_int, _ := k_other.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) k_other_hash := zkt.NewHashFromBigInt(k_other_hash_int) assert.Nil(t, zkTrie.TryUpdate(k_hash, 0, k_value)) getNode := func(hash *zkt.Hash) (*Node, error) { return zkTrie.GetNode(hash) } proof, _, err := BuildZkTrieProof(zkTrie.rootHash, k_other_hash_int, 10, getNode) assert.Nil(t, err) assert.False(t, proof.Existence) proof_root, _ := proof.Verify(nil, k_other_hash) assert.Equal(t, proof_root, zkTrie.rootHash) }) } Figure 4.3: A test case that will crash with a nil dereference of NodeAux.Value Adding a call to n.NodeHash() inside BuildZkTrieProof, as shown in gure 4.4, xes this problem. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.4: Adding the highlighted n.NodeHash() call xes this problem. (zktrie/trie/zk_trie_impl.go#560574) Exploit Scenario An adversary or ordinary user requests that the software generate and verify a non-inclusion proof, and the software crashes, leading to the loss of service. Recommendations Short term, x BuildZkTrieProof by adding a call to n.NodeHash(), as described above. Long term, ensure that all major code paths in important functions, such as proof generation and verication, are tested. The Go coverage analysis report generated by the command go test -cover -coverprofile c.out && go tool cover -html=c.out shows that this branch in Proof.Verify is not currently tested: Figure 4.5: The Go coverage analysis report 5. Leaf nodes with di\u0000erent values may have the same hash Severity: High Diculty: Medium Type: Cryptography Finding ID: TOB-ZKTRIE-5 Target: trie/zk_trie_node.go, types/util.go", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "6. Empty UpdatePreimage function body ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The UpdatePreimage function implementation for the Database receiver type is empty. Instead of an empty function body, the function should either panic with an unimplemented message or a message that is logged. This would prevent the function from being used without any warning. func (db *Database) UpdatePreimage([]byte, *big.Int) {} Figure 6.1: zktrie/trie/zk_trie_database.go#19 Recommendations Short term, add an unimplemented message to the function body, through either a panic or message logging.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "7. CanonicalValue is not canonical ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The CanonicalValue function does not uniquely generate a representation of Node structures, allowing dierent Nodes with the same CanonicalValue, and two nodes with the same NodeHash but dierent CanonicalValues. ValuePreimages in a Node can be either uncompressed or compressed (by hashing); the CompressedFlags value indicates which data is compressed. Only the rst 24 elds can be compressed, so CanonicalValue truncates CompressedFlags to the rst 24 bits. But NewLeafNode accepts any uint32 for the CompressedFlags eld of a Node. Figure 7.3 shows how this can be used to construct two dierent Node structs that have the same CanonicalValue. // CanonicalValue returns the byte form of a node required to be persisted, and strip unnecessary fields // from the encoding (current only KeyPreimage for Leaf node) to keep a minimum size for content being // stored in backend storage func (n *Node) CanonicalValue() []byte { switch n.Type { case NodeTypeParent: // {Type || ChildL || ChildR} bytes := []byte{byte(n.Type)} bytes = append(bytes, n.ChildL.Bytes()...) bytes = append(bytes, n.ChildR.Bytes()...) return bytes case NodeTypeLeaf: // {Type || Data...} bytes := []byte{byte(n.Type)} bytes = append(bytes, n.NodeKey.Bytes()...) tmp := make([]byte, 4) compressedFlag := (n.CompressedFlags << 8) + uint32(len(n.ValuePreimage)) binary.LittleEndian.PutUint32(tmp, compressedFlag) bytes = append(bytes, tmp...) for _, elm := range n.ValuePreimage { bytes = append(bytes, elm[:]...) } bytes = append(bytes, 0) return bytes case NodeTypeEmpty: // { Type } return []byte{byte(n.Type)} default: return []byte{} } } Figure 7.1: This gure shows the CanonicalValue computation. The highlighted code assumes that CompressedFlags is 24 bits. (zktrie/trie/zk_trie_node.go#187214) // NewLeafNode creates a new leaf node. func NewLeafNode(k *zkt.Hash, valueFlags uint32, valuePreimage []zkt.Byte32) *Node { return &Node{Type: NodeTypeLeaf, NodeKey: k, CompressedFlags: valueFlags, ValuePreimage: valuePreimage} } Figure 7.2: Node construction in NewLeafNode (zktrie/trie/zk_trie_node.go#5558) // CanonicalValue implicitly truncates CompressedFlags to 24 bits. This test should ideally fail. func TestZkTrie_CanonicalValue1(t *testing.T) { key, err := hex.DecodeString(\"0000000000000000000000000000000000000000000000000000000000000000\") assert.NoError(t, err) vPreimage := []zkt.Byte32{{0}} k := zkt.NewHashFromBytes(key) vFlag0 := uint32(0x00ffffff) vFlag1 := uint32(0xffffffff) lf0 := NewLeafNode(k, vFlag0, vPreimage) lf1 := NewLeafNode(k, vFlag1, vPreimage) // These two assertions should never simultaneously pass. assert.True(t, lf0.CompressedFlags != lf1.CompressedFlags) assert.True(t, reflect.DeepEqual(lf0.CanonicalValue(), lf1.CanonicalValue())) } Figure 7.3: A test showing that one can construct dierent nodes with the same CanonicalValue // PreHandlingElems turn persisted byte32 elements into field arrays for our hashElem // it also has the compressed byte32 func PreHandlingElems(flagArray uint32, elems []Byte32) (*Hash, error) { ret := make([]*big.Int, len(elems)) var err error for i, elem := range elems { if flagArray&(1<<i) != 0 { ret[i], err = elem.Hash() if err != nil { return nil, err } } else { ret[i] = new(big.Int).SetBytes(elem[:]) } } if len(ret) < 2 { return NewHashFromBigInt(ret[0]), nil } return HashElems(ret[0], ret[1], ret[2:]...) } Figure 7.4: The subroutine called in NodeHash that hashes uncompressed elements (zktrie/types/util.go#3862) Furthermore, CanonicalValue and NodeHash are inconsistent in their processing of uncompressed values. CanonicalValue uses them directly, while NodeHash hashes them. Figure 7.5 shows how this can be used to construct two Node structs that have the same NodeHash but dierent CanonicalValues. // CanonicalValue and NodeHash are not consistent func TestZkTrie_CanonicalValue2(t *testing.T) { t.Run(\"Testing for value collisions\", func(t *testing.T) { k := zkt.Byte32{1} k_hash_int, _ := k.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) {3}})[:] value1 := (&[2]zkt.Byte32{*zkt.NewByte32FromBytes(k_hash.Bytes()), value2 := (&[2]zkt.Byte32{{1}, {3}})[:] leaf1 := NewLeafNode(k_hash, 0, value1) leaf2 := NewLeafNode(k_hash, 1, value2) leaf1_node_hash, _ := leaf1.NodeHash() leaf2_node_hash, _ := leaf2.NodeHash() assert.Equal(t, leaf1_node_hash, leaf2_node_hash) leaf1_canonical := leaf1.CanonicalValue() leaf2_canonical := leaf2.CanonicalValue() assert.NotEqual(t, leaf1_canonical, leaf2_canonical) }) } Figure 7.5: A test showing that CanonicalValue and NodeHash are inconsistent Recommendations Short term, have CanonicalValue validate all assumptions, and make CanonicalValue and NodeHash consistent. Long term, document all assumptions and use Gos type system to enforce them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "8. ToSecureKey and ToSecureKeyBytes implicitly truncate the key ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "ToSecureKey and ToSecureKeyBytes accept a key of arbitrary length but implicitly truncate it to 32 bytes. ToSecureKey makes an underlying call to NewByte32FromBytesPaddingZero that truncates the key to its rst 32 bytes. The ToSecureKeyBytes function also truncates the key because it calls ToSecureKey. // ToSecureKey turn the byte key into the integer represent of \"secured\" key func ToSecureKey(key []byte) (*big.Int, error) { word := NewByte32FromBytesPaddingZero(key) return word.Hash() } Figure 8.1: ToSecureKey accepts a key of arbitrary length. (zktrie/types/util.go#9397) // create bytes32 with zeropadding to shorter bytes, or truncate it func NewByte32FromBytesPaddingZero(b []byte) *Byte32 { byte32 := new(Byte32) copy(byte32[:], b) return byte32 } Figure 8.2: But NewByte32FromBytesPaddingZero truncates the key to the rst 32 bytes. (zktrie/types/byte32.go#3540) // ToSecureKeyBytes turn the byte key into a 32-byte \"secured\" key, which represented a big-endian integer func ToSecureKeyBytes(key []byte) (*Byte32, error) { k, err := ToSecureKey(key) if err != nil { return nil, err } return NewByte32FromBytes(k.Bytes()), nil } Figure 8.3: ToSecureKeyBytes accepts a key of arbitrary length and calls ToSecureKey on it. (zktrie/types/util.go#99107) // zkt.ToSecureKey implicitly truncates keys to 32 bytes. This test should ideally fail. func TestZkTrie_ToSecureKeyTruncation(t *testing.T) { key1, err := hex.DecodeString(\"000000000000000000000000000000000000000000000000000000000000000011 \") assert.NoError(t, err) key2, err := hex.DecodeString(\"000000000000000000000000000000000000000000000000000000000000000022 \") assert.NoError(t, err) assert.NotEqual(t, key1, key2) skey1, err := zkt.ToSecureKey(key1) assert.NoError(t, err) // This should fail skey2, err := zkt.ToSecureKey(key2) assert.NoError(t, err) // This should fail assert.True(t, skey1.Cmp(skey2) == 0) // If above don't fail, this should fail } Figure 8.4: A test showing the truncation of keys longer than 32 bytes Recommendations Short term, x the ToSecureKey and ToSecureKeyBytes functions so that they do not truncate keys that are longer than 32 bytes, and instead hash all the bytes. If this behavior is not desired, ensure that the functions return an error if given a key longer than 32 bytes. Long term, add fuzz tests to public interfaces like TryGet, TryUpdate, and TryDelete.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "9. Unused key argument on the bridge_prove_write function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The bridge_prove_write function implementation does not use the key argument. void bridge_prove_write(proveWriteF f, unsigned char* key, unsigned char* val, int size, void* param){ f(val, size, param); } Figure 9.1: zktrie/c.go#1719 This function is always called with a nil value: err = tr.Prove(s_key.Bytes(), 0, func(n *trie.Node) error { dt := n.Value() C.bridge_prove_write( C.proveWriteF(callback), nil, //do not need to prove node key (*C.uchar)(&dt[0]), C.int(len(dt)), cb_param, ) return nil }) if err != nil { return C.CString(err.Error()) } tailingLine := trie.ProofMagicBytes() C.bridge_prove_write( C.proveWriteF(callback), nil, //do not need to prove node key (*C.uchar)(&tailingLine[0]), C.int(len(tailingLine)), cb_param, ) return nil } Figure 9.2: zktrie/lib.go#263292 Recommendations Short term, document the intended behavior and the role and requirements of each function. Decide whether to remove the unused argument or document why it is currently unused in the implementation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "10. The PreHandlingElems function panics with an empty elems array ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The PreHandlingElems function experiences a runtime error when the elems array is empty. There is an early return path for when the array has fewer than two elements that assumes there is at least one element. If this is not the case, there will be an out-of-bounds access that will cause a runtime error. func PreHandlingElems(flagArray uint32, elems []Byte32) (*Hash, error) { ret := make([]*big.Int, len(elems)) var err error for i, elem := range elems { if flagArray&(1<<i) != 0 { ret[i], err = elem.Hash() if err != nil { return nil, err } } else { ret[i] = new(big.Int).SetBytes(elem[:]) } } if len(ret) < 2 { return NewHashFromBigInt(ret[0]), nil Figure 10.1: When ret is empty, the array access will cause a runtime error. (zktrie/types/util.go#4057) Figure 10.2 shows tests that demonstrate this issue by directly calling PreHandlingElems with an empty array and by triggering the issue via the NodeHash function. func TestEmptyPreHandlingElems(t *testing.T) { flagArray := uint32(0) elems := make([]Byte32, 0) _, err := PreHandlingElems(flagArray, elems) assert.NoError(t, err) } func TestPrehandlingElems(t *testing.T) { k := zkt.Byte32{1} k_hash_int, _ := k.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) value1 := (&[0]zkt.Byte32{})[:] node, _ := NewLeafNode(k_hash, 0, value1).NodeHash() t.Log(node) } Figure 10.2: Tests that trigger the out-of-bounds access Note that the TrieUpdate exported function would also trigger the same issue if called with an empty vPreimage argument, but this is checked in the function. It is also possible to trigger this panic from the Rust API by calling ZkTrieNode::parse with the byte array obtained from the Value() function operated on a maliciously constructed leaf node. This is because ZkTrieNode::parse will eventually call NewNodeFromBytes and the NodeHash function on that node. The NewNodeFromBytes function also does not validate that the newly created node is well formed (TOB-ZKTRIE-14). #[test] fn test_zktrienode_parse() { let buff = hex::decode(\"011baa09b39b1016e6be4467f3d58c1e1859d5e883514ff707551a9355a5941e2200000 00000\").unwrap(); let _node = ZkTrieNode::parse(&buff); } Figure 10.3: A test that triggers the out-of-bounds access from the Rust API Both the Data() and String() functions also panic when operated on a Node receiver with an empty ValuePreimage array: // Data returns the wrapped data inside LeafNode and cast them into bytes // for other node type it just return nil func (n *Node) Data() []byte { switch n.Type { case NodeTypeLeaf: var data []byte hdata := (*reflect.SliceHeader)(unsafe.Pointer(&data)) //TODO: uintptr(reflect.ValueOf(n.ValuePreimage).UnsafePointer()) should be more elegant but only available until go 1.18 hdata.Data = uintptr(unsafe.Pointer(&n.ValuePreimage[0])) hdata.Len = 32 * len(n.ValuePreimage) hdata.Cap = hdata.Len return data default: return nil } } Figure 10.4: zktrie/trie/zk_trie_node.go#170185 // String outputs a string representation of a node (different for each type). func (n *Node) String() string { switch n.Type { case NodeTypeParent: // {Type || ChildL || ChildR} return fmt.Sprintf(\"Parent L:%s R:%s\", n.ChildL, n.ChildR) case NodeTypeLeaf: // {Type || Data...} return fmt.Sprintf(\"Leaf I:%v Items: %d, First:%v\", n.NodeKey, len(n.ValuePreimage), n.ValuePreimage[0]) case NodeTypeEmpty: // {} return \"Empty\" default: return \"Invalid Node\" } } Figure 10.5: zktrie/trie/zk_trie_node.go#230242 Exploit Scenario An attacker calls the public Rust ZkTrieNode::parse function with a maliciously chosen buer, causing the system to experience a runtime error. Recommendations Short term, document which properties need to hold for all data structures. Ensure that edge cases are documented in the type constructors, and add checks to validate that functions do not raise a runtime error. Long term, add fuzz testing to the public Rust and Go APIs.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "11. The hash_external function panics with integers larger than 32 bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The hash_external function will cause a runtime error due to an out-of-bounds access if the input integers are larger than 32 bytes. func hash_external(inp []*big.Int) (*big.Int, error) { if len(inp) != 2 { return big.NewInt(0), errors.New(\"invalid input size\") } a := zkt.ReverseByteOrder(inp[0].Bytes()) b := zkt.ReverseByteOrder(inp[1].Bytes()) a = append(a, zeros[0:(32-len(a))]...) b = append(b, zeros[0:(32-len(b))]...) Figure 11.1: zktrie/lib.go#3139 Exploit Scenario An attacker causes the system to call hash_external with integers larger than 32 bytes, causing the system to experience a runtime error. Recommendations Short term, document the function requirements that the integers need to be less than 32 bytes. If the function is reachable by an adversary, add checks to ensure that the runtime error is not reachable. Long term, carefully check all indexing operations done on adversary-controlled values with respect to out-of-bounds accessing.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Mishandling of cgo.Handles causes runtime errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The interaction between the Rust and Go codebases relies on the use of cgo.Handles. These handles are a way to encode Go pointers between Go and, in this case, Rust. Handles can be passed back to the Go runtime, which will be able to retrieve the original Go value. According to the documentation, it is safe to represent an error with the zero value, as this is an invalid handle. However, the implementation should take this into account when retrieving the Go values from the handle, as both the Value and Delete methods for Handles panic on invalid handles. The codebase contains multiple instances of this behavior. For example, the NewTrieNode function will return 0 if it nds an error: // parse raw bytes and create the trie node //export NewTrieNode func NewTrieNode(data *C.char, sz C.int) C.uintptr_t { bt := C.GoBytes(unsafe.Pointer(data), sz) n, err := trie.NewNodeFromBytes(bt) if err != nil { return 0 } // calculate key for caching if _, err := n.NodeHash(); err != nil { return 0 } return C.uintptr_t(cgo.NewHandle(n)) } Figure 12.1: zktrie/lib.go#7388 However, neither the Rust API nor the Go API takes these cases into consideration. Looking at the Rust API, the ZkTrieNode::parse function will simply save the result from NewTrieNode regardless of whether it is a valid or invalid Go handle. Then, calling any other function will cause a runtime error due to the use of an invalid handle. This issue is present in all functions implemented for ZkTrieNode: drop, node_hash, and value_hash. We now precisely describe how it fails in the drop function case. After constructing a malformed ZkTrieNode, the drop function will call FreeTrieNode on the invalid handle: impl Drop for ZkTrieNode { fn drop(&mut self) { unsafe { FreeTrieNode(self.trie_node) }; } } Figure 12.2: zktrie/src/lib.rs#127131 This will cause a panic given the direct use of the invalid handle on the Handle.Delete function: // free created trie node //export FreeTrieNode func FreeTrieNode(p C.uintptr_t) { freeObject(p) } func freeObject(p C.uintptr_t) { h := cgo.Handle(p) h.Delete() } Figure 12.3: zktrie/lib.go#114131 The following test triggers the described issue: #[test] fn invalid_handle_drop() { init_hash_scheme(hash_scheme); let _nd = ZkTrieNode::parse(&hex::decode(\"0001\").unwrap()); } // // panic: runtime/cgo: misuse of an invalid Handle running 1 test /opt/homebrew/Cellar/go/1.18.3/libexec/src/runtime/cgo/handle.go:137 // goroutine 17 [running, locked to thread]: // runtime/cgo.Handle.Delete(...) // // main.freeObject(0x14000060d01?) // // main.FreeTrieNode(...) // /zktrie/lib.go:130 +0x5c /zktrie/lib.go:116 Figure 12.4: A test case that triggers the nding in the drop case Exploit Scenario An attacker provides malformed data to ZkTrieNode::parse, causing it to contain an invalid Go handle. This subsequently causes the system to crash when one of the value_hash or node_hash functions is called or eventually when the node variable goes out of scope and the drop function is called. Recommendations Short term, ensure that invalid handles are not used with Delete or Value; for this, document the Go exported function requirements, and ensure that Rust checks for this before these functions are called. Long term, add tests that exercise all return paths for both the Go and Rust libraries.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "13. Unnecessary unsafe pointer manipulation in Node.Data() ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The Node.Data() function returns the underlying value of a leaf node as a byte slice (i.e., []byte). Since the ValuePreimage eld is a slice of zkt.Byte32s, returning a value of type []byte requires some form of conversion. The implementation, shown in gure 13.1, uses the reflect and unsafe packages to manually construct a byte slice that overlaps with ValuePreimage. case NodeTypeLeaf: var data []byte hdata := (*reflect.SliceHeader)(unsafe.Pointer(&data)) //TODO: uintptr(reflect.ValueOf(n.ValuePreimage).UnsafePointer()) should be more elegant but only available until go 1.18 hdata.Data = uintptr(unsafe.Pointer(&n.ValuePreimage[0])) hdata.Len = 32 * len(n.ValuePreimage) hdata.Cap = hdata.Len return data Figure 13.1: Unsafe casting from []zkt.Byte32 to []byte (trie/zk_trie_node.go#174181) Manual construction of slices and unsafe casting between pointer types are error-prone and potentially very dangerous. This particular case appears to be harmless, but it is unnecessary and can be replaced by allocating a byte buer and copying ValuePreimage into it. Recommendations Short term, replace this unsafe cast with code that allocated a byte buer and then copies ValuePreimage, as described above. Long term, evaluate all uses of unsafe pointer manipulation and replace them with a safe alternative where possible.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "14. NewNodeFromBytes does not fully validate its input ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The NewNodeFromBytes function parses a byte array into a value of type Node. It checks several requirements for the Node value and returns nil and an error value if those checks fail. However, it allows a zero-length value for ValuePreimage (which allows TOB-ZKTRIE-10 to be exploited) and ignores extra data at the end of leaf and empty nodes. As shown in gure 14.1, the exact length of the byte array is checked in the case of a branch, but is unchecked for empty nodes and only lower-bounded in the case of a leaf node. case NodeTypeParent: if len(b) != 2*zkt.HashByteLen { return nil, ErrNodeBytesBadSize } n.ChildL = zkt.NewHashFromBytes(b[:zkt.HashByteLen]) n.ChildR = zkt.NewHashFromBytes(b[zkt.HashByteLen : zkt.HashByteLen*2]) case NodeTypeLeaf: if len(b) < zkt.HashByteLen+4 { return nil, ErrNodeBytesBadSize } n.NodeKey = zkt.NewHashFromBytes(b[0:zkt.HashByteLen]) mark := binary.LittleEndian.Uint32(b[zkt.HashByteLen : zkt.HashByteLen+4]) preimageLen := int(mark & 255) n.CompressedFlags = mark >> 8 n.ValuePreimage = make([]zkt.Byte32, preimageLen) curPos := zkt.HashByteLen + 4 if len(b) < curPos+preimageLen*32+1 { return nil, ErrNodeBytesBadSize }  if preImageSize != 0 { if len(b) < curPos+preImageSize { return nil, ErrNodeBytesBadSize } n.KeyPreimage = new(zkt.Byte32) copy(n.KeyPreimage[:], b[curPos:curPos+preImageSize]) } case NodeTypeEmpty: break Figure 14.1: preimageLen and len(b) are not fully checked. (trie/zk_trie_node.go#78111) Recommendations Short term, add checks of the total byte array length and the preimageLen eld to NewNodeFromBytes. Long term, explicitly document the serialization format for nodes, and add tests for incorrect serialized nodes.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "15. init_hash_scheme is not thread-safe ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "zktrie provides a safe-Rust interface around its Go implementation. Safe Rust statically prevents various memory safety errors, including null pointer dereferences and data races. However, when unsafe Rust is wrapped in a safe interface, the unsafe code must provide any guarantees that safe Rust expects. For more information about writing unsafe Rust, consult The Rustonomicon. The init_hash_scheme function, shown in gure 15.1, calls InitHashScheme, which is a cgo wrapper for the Go function shown in gure 15.2. pub fn init_hash_scheme(f: HashScheme) { unsafe { InitHashScheme(f) } } Figure 15.1: src/lib.rs#6769 // notice the function must use C calling convention //export InitHashScheme func InitHashScheme(f unsafe.Pointer) { hash_f := C.hashF(f) C.init_hash_scheme(hash_f) zkt.InitHashScheme(hash_external) } Figure 15.2: lib.go#6571 InitHashScheme calls two other functions: rst, a C function called init_hash_scheme and second, a second Go function (this time, in the hash module) called InitHashScheme. This second Go function is synchronized with a sync.Once object, as shown in gure 15.3. func InitHashScheme(f func([]*big.Int) (*big.Int, error)) { setHashScheme.Do(func() { hashScheme = f }) } Figure 15.3: types/hash.go#29 However, the C function init_hash_scheme, shown in gure 15.4, performs a completely unsynchronized write to the global variable hash_scheme, which can lead to a data race. void init_hash_scheme(hashF f){ hash_scheme = f; } Figure 15.4: c.go#1315 However, the only potential data race comes from multi-threaded initialization, which contradicts the usage recommendation in the README, shown in gure 15.5. We must init the crate with a poseidon hash scheme before any actions:  zktrie_util::init_hash_scheme(hash_scheme); Figure 15.5: README.md#824 Recommendations Short term, add synchronization to C.init_hash_scheme, perhaps by using the same sync.Once object as hash.go. Long term, carefully review all interactions between C and Rust, paying special attention to anything mentioned in the How Safe and Unsafe Interact section of the Rustonomicon.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "16. Safe-Rust ZkMemoryDb interface is not thread-safe ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The Go function Database.Init, shown in gure 16.1, is not thread-safe. In particular, if it is called from multiple threads, a data race may occur when writing to the map. In normal usage, that is not a problem; any user of the Database.Init function is expected to run the function only during initialization, when synchronization is not required. // Init flush db with batches of k/v without locking func (db *Database) Init(k, v []byte) { db.db[string(k)] = v } Figure 16.1: trie/zk_trie_database.go#4043 However, this function is called by the safe Rust function ZkMemoryDb::add_node_bytes (gure 16.2) via the cgo function InitDbByNode (gure 16.3): pub fn add_node_bytes(&mut self, data: &[u8]) -> Result<(), ErrString> { let ret_ptr = unsafe { InitDbByNode(self.db, data.as_ptr(), data.len() as c_int) }; if ret_ptr.is_null() { Ok(()) } else { Err(ret_ptr.into()) } } Figure 16.2: src/lib.rs#171178 // flush db with encoded trie-node bytes //export InitDbByNode func InitDbByNode(pDb C.uintptr_t, data *C.uchar, sz C.int) *C.char { h := cgo.Handle(pDb) db := h.Value().(*trie.Database) bt := C.GoBytes(unsafe.Pointer(data), sz) n, err := trie.DecodeSMTProof(bt) if err != nil { return C.CString(err.Error()) } else if n == nil { //skip magic string return nil } hash, err := n.NodeHash() if err != nil { return C.CString(err.Error()) } db.Init(hash[:], n.CanonicalValue()) return nil } Figure 16.3: lib.go#147170 Safe Rust is required to never invoke undened behavior, such as data races. When wrapping unsafe Rust code, including FFI calls, care must be taken to ensure that safe Rust code cannot invoke undened behavior through that wrapper. (Refer to the How Safe and Unsafe Interact section of the Rustonomicon.) Although add_node_bytes takes &mut self, and thus cannot be called from more than one thread at once, a second reference to the database can be created in a way that Rusts borrow checker cannot track, by calling new_trie. Figures 16.4, 16.5, and 16.6 show the call trace by which a pointer to the Database is stored in the ZkTrieImpl. pub fn new_trie(&mut self, root: &Hash) -> Option<ZkTrie> { let ret = unsafe { NewZkTrie(root.as_ptr(), self.db) }; if ret.is_null() { None } else { Some(ZkTrie { trie: ret }) } } Figure 16.4: src/lib.rs#181189 func NewZkTrie(root_c *C.uchar, pDb C.uintptr_t) C.uintptr_t { h := cgo.Handle(pDb) db := h.Value().(*trie.Database) root := C.GoBytes(unsafe.Pointer(root_c), 32) zktrie, err := trie.NewZkTrie(*zkt.NewByte32FromBytes(root), db) if err != nil { return 0 } return C.uintptr_t(cgo.NewHandle(zktrie)) } Figure 16.5: lib.go#174185 func NewZkTrieImpl(storage ZktrieDatabase, maxLevels int) (*ZkTrieImpl, error) { return NewZkTrieImplWithRoot(storage, &zkt.HashZero, maxLevels) } // NewZkTrieImplWithRoot loads a new ZkTrieImpl. If in the storage already exists one // will open that one, if not, will create a new one. func NewZkTrieImplWithRoot(storage ZktrieDatabase, root *zkt.Hash, maxLevels int) (*ZkTrieImpl, error) { mt := ZkTrieImpl{db: storage, maxLevels: maxLevels, writable: true} mt.rootHash = root if *root != zkt.HashZero { _, err := mt.GetNode(mt.rootHash) if err != nil { return nil, err } } return &mt, nil } Figure 16.6: trie/zk_trie_impl.go#5672 Then, by calling add_node_bytes in one thread and ZkTrie::root() or some other method that calls Database.Get(), one can trigger a data race from safe Rust. Exploit Scenario A Rust-based library consumer uses threads to improve performance. Relying on Rusts type system, they assume that thread safety has been enforced and they run ZkMemoryDb::add_node_bytes in a multi-threaded scenario. A data race occurs and the system crashes. Recommendations Short term, add synchronization to Database.Init, such as by calling db.lock.Lock(). Long term, carefully review all interactions between C and Rust, paying special attention to guidance in the How Safe and Unsafe Interact section of the Rustonomicon.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "17. Some Node functions return the zero hash instead of errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The Node.NodeHash and Node.ValueHash methods each return the zero hash in cases in which an error return would be more appropriate. In the case of NodeHash, all invalid node types return the zero hash, the same hash as an empty node (shown in gure 17.1). case NodeTypeEmpty: // Zero n.nodeHash = &zkt.HashZero default: n.nodeHash = &zkt.HashZero } } return n.nodeHash, nil Figure 17.1: trie/zk_trie_node.go#149155 In the case of ValueHash, non-leaf nodes have a zero value hash, as shown in gure 17.2. func (n *Node) ValueHash() (*zkt.Hash, error) { if n.Type != NodeTypeLeaf { return &zkt.HashZero, nil } Figure 17.2: trie/zk_trie_node.go#160163 In both of these cases, returning an error is more appropriate and prevents potential confusion if client software assumes that the main return value is valid whenever the error returned is nil. Recommendations Short term, have the functions return an error in these cases instead of the zero hash. Long term, ensure that exceptional cases lead to non-nil error returns rather than default values. 18. get_account can read past the bu\u0000er Severity: High Diculty: Medium Type: Data Exposure Finding ID: TOB-ZKTRIE-18 Target: lib.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "4. BuildZkTrieProof does not populate NodeAux.Value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "A nonexistence proof for some key k in a Merkle tree is a Merkle path from the root of the tree to a subtree, which would contain k if it were present but which instead is either an empty subtree or a subtree with a single leaf k2 where k != k2. In the zktrie codebase, that second case is handled by the NodeAux eld in the Proof struct, as illustrated in gure 4.1. // Verify the proof and calculate the root, nodeHash can be nil when try to verify // a nonexistent proof func (proof *Proof) Verify(nodeHash, nodeKey *zkt.Hash) (*zkt.Hash, error) { if proof.Existence { if nodeHash == nil { return nil, ErrKeyNotFound } return proof.rootFromProof(nodeHash, nodeKey) } else { if proof.NodeAux == nil { return proof.rootFromProof(&zkt.HashZero, nodeKey) } else { if bytes.Equal(nodeKey[:], proof.NodeAux.Key[:]) { return nil, fmt.Errorf(\"non-existence proof being checked against hIndex equal to nodeAux\") } midHash, err := LeafHash(proof.NodeAux.Key, proof.NodeAux.Value) if err != nil { return nil, err } return proof.rootFromProof(midHash, nodeKey) } } } Figure 4.1: The Proof.Verify method (zktrie/trie/zk_trie_impl.go#609632) When a non-inclusion proof is generated, the BuildZkTrieProof function looks up the other leaf node and uses its NodeKey and valueHash elds to populate the Key and Value elds of NodeAux, as shown in gure 4.2. However, the valueHash eld of this node may be nil, causing NodeAux.Value to be nil and causing proof verication to crash with a nil pointer dereference error, which can be triggered by the test case shown in gure 4.3. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.2: Populating NodeAux (zktrie/trie/zk_trie_impl.go#560574) func TestMerkleTree_GetNonIncProof(t *testing.T) { zkTrie := newTestingMerkle(t, 10) t.Run(\"Testing for non-inclusion proofs\", func(t *testing.T) { k := zkt.Byte32{1} k_value := (&[1]zkt.Byte32{{1}})[:] k_other := zkt.Byte32{2} k_hash_int, _ := k.Hash() k_other_hash_int, _ := k_other.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) k_other_hash := zkt.NewHashFromBigInt(k_other_hash_int) assert.Nil(t, zkTrie.TryUpdate(k_hash, 0, k_value)) getNode := func(hash *zkt.Hash) (*Node, error) { return zkTrie.GetNode(hash) } proof, _, err := BuildZkTrieProof(zkTrie.rootHash, k_other_hash_int, 10, getNode) assert.Nil(t, err) assert.False(t, proof.Existence) proof_root, _ := proof.Verify(nil, k_other_hash) assert.Equal(t, proof_root, zkTrie.rootHash) }) } Figure 4.3: A test case that will crash with a nil dereference of NodeAux.Value Adding a call to n.NodeHash() inside BuildZkTrieProof, as shown in gure 4.4, xes this problem. n, err := getNode(nextHash) if err != nil { return nil, nil, err } switch n.Type { case NodeTypeEmpty: return p, n, nil case NodeTypeLeaf: if bytes.Equal(kHash[:], n.NodeKey[:]) { p.Existence = true return p, n, nil } // We found a leaf whose entry didn't match hIndex p.NodeAux = &NodeAux{Key: n.NodeKey, Value: n.valueHash} return p, n, nil Figure 4.4: Adding the highlighted n.NodeHash() call xes this problem. (zktrie/trie/zk_trie_impl.go#560574) Exploit Scenario An adversary or ordinary user requests that the software generate and verify a non-inclusion proof, and the software crashes, leading to the loss of service. Recommendations Short term, x BuildZkTrieProof by adding a call to n.NodeHash(), as described above. Long term, ensure that all major code paths in important functions, such as proof generation and verication, are tested. The Go coverage analysis report generated by the command go test -cover -coverprofile c.out && go tool cover -html=c.out shows that this branch in Proof.Verify is not currently tested: Figure 4.5: The Go coverage analysis report", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "5. Leaf nodes with di\u0000erent values may have the same hash ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The hash value of a leaf node is derived from the hash of its key and its value. A leaf nodes value comprises up to 256 32-byte elds, and that values hash is computed by passing those elds to the HashElems function. HashElems hashes these elds in a Merkle treestyle binary tree pattern, as shown in gure 5.1. func HashElems(fst, snd *big.Int, elems ...*big.Int) (*Hash, error) { l := len(elems) baseH, err := hashScheme([]*big.Int{fst, snd}) if err != nil { return nil, err } if l == 0 { return NewHashFromBigInt(baseH), nil } else if l == 1 { return HashElems(baseH, elems[0]) } tmp := make([]*big.Int, (l+1)/2) for i := range tmp { if (i+1)*2 > l { tmp[i] = elems[i*2] } else { h, err := hashScheme(elems[i*2 : (i+1)*2]) if err != nil { return nil, err } tmp[i] = h } } return HashElems(baseH, tmp[0], tmp[1:]...) } Figure 5.1: Binary-tree hashing in HashElems (zktrie/types/util.go#936) However, HashElems does not include the number of elements being hashed, so leaf nodes with dierent values may have the same hash, as illustrated in the proof-of-concept test case shown in gure 5.2. func TestMerkleTree_MultiValue(t *testing.T) { t.Run(\"Testing for value collisions\", func(t *testing.T) { k := zkt.Byte32{1} k_hash_int, _ := k.Hash() k_hash := zkt.NewHashFromBigInt(k_hash_int) value1 := (&[3]zkt.Byte32{{1}, {2}, {3}})[:] value1_hash, _ := NewLeafNode(k_hash, 0, value1).NodeHash() first2_hash, _ := zkt.PreHandlingElems(0, value1[:2]) value2 := (&[2]zkt.Byte32{*zkt.NewByte32FromBytes(first2_hash.Bytes()), value2_hash, _ := NewLeafNode(k_hash, 0, value2).NodeHash() assert.NotEqual(t, value1, value2) assert.NotEqual(t, value1_hash, value2_hash) {3}})[:] }) } Figure 5.2: A proof-of-concept test case for value collisions Exploit Scenario An adversary inserts a maliciously crafted value into the tree and then creates a proof for a dierent, colliding value. This violates the security requirements of a Merkle tree and may lead to incorrect behavior such as state divergence. Recommendations Short term, modify PrehandlingElems to prex the ValuePreimage array with its length before being passed to HashElems. Long term, document and review all uses of hash functions to ensure that they commit to their inputs.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "17. Some Node functions return the zero hash instead of errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The Node.NodeHash and Node.ValueHash methods each return the zero hash in cases in which an error return would be more appropriate. In the case of NodeHash, all invalid node types return the zero hash, the same hash as an empty node (shown in gure 17.1). case NodeTypeEmpty: // Zero n.nodeHash = &zkt.HashZero default: n.nodeHash = &zkt.HashZero } } return n.nodeHash, nil Figure 17.1: trie/zk_trie_node.go#149155 In the case of ValueHash, non-leaf nodes have a zero value hash, as shown in gure 17.2. func (n *Node) ValueHash() (*zkt.Hash, error) { if n.Type != NodeTypeLeaf { return &zkt.HashZero, nil } Figure 17.2: trie/zk_trie_node.go#160163 In both of these cases, returning an error is more appropriate and prevents potential confusion if client software assumes that the main return value is valid whenever the error returned is nil. Recommendations Short term, have the functions return an error in these cases instead of the zero hash. Long term, ensure that exceptional cases lead to non-nil error returns rather than default values.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "18. get_account can read past the bu\u0000er ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "The public get_account function assumes that the provided key corresponds to an account key. However, if the function is instead called with a storage key, it will cause an out-of-bounds read that could leak secret information. In the Rust implementation, leaf nodes can have two types of values: accounts and storage. Account values have a size of either 128 or 160 bytes depending on whether they include one or two code hashes. On the other hand, storage values always have a size of 32 bytes. The get_account function takes a key and returns the account associated with it. In practice, it computes the value pointer associated with the key and reads 128 or 160 bytes at that address. If the key contains a storage value rather than an account value, then get_account reads 96 or 128 bytes past the buer. This is shown in gure 18.4. // get account data from account trie pub fn get_account(&self, key: &[u8]) -> Option<AccountData> { self.get::<ACCOUNTSIZE>(key).map(|arr| unsafe { std::mem::transmute::<[u8; FIELDSIZE * ACCOUNTFIELDS], AccountData>(arr) }) } Figure 18.1: get_account calls get with type ACCOUNTSIZE and key. (zktrie/src/lib.rs#230235) // all errors are reduced to \"not found\" fn get<const T: usize>(&self, key: &[u8]) -> Option<[u8; T]> { let ret = unsafe { TrieGet(self.trie, key.as_ptr(), key.len() as c_int) }; if ret.is_null() { None } else { Some(must_get_const_bytes::<T>(ret)) } } Figure 18.2: get calls must_get_const_bytes with type ACCOUNTSIZE and the pointer returned by TrieGet. (zktrie/src/lib.rs#214223) fn must_get_const_bytes<const T: usize>(p: *const u8) -> [u8; T] { let bytes = unsafe { std::slice::from_raw_parts(p, T) }; let bytes = bytes .try_into() .expect(\"the buf has been set to specified bytes\"); unsafe { FreeBuffer(p.cast()) } bytes } Figure 18.3: must_get_const_bytes calls std::slice::from_raw_parts with type ACCOUNTSIZE and pointer p to read ACCOUNTSIZE bytes from pointer p. (zktrie/src/lib.rs#100107) #[test] fn get_account_overflow() { let storage_key = hex::decode(\"0000000000000000000000000000000000000000000000000000000000000000\") .unwrap(); let storage_data = [10u8; 32]; init_hash_scheme(hash_scheme); let mut db = ZkMemoryDb::new(); let root_hash = Hash::from([0u8; 32]); let mut trie = db.new_trie(&root_hash).unwrap(); trie.update_store(&storage_key, &storage_data).unwrap(); println!(\"{:?}\", trie.get_account(&storage_key).unwrap()); } // Sample output (picked from a sample of ten runs): // [[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [160, 113, 63, 0, 2, 0, 0, 0, 161, 67, 240, 40, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 158, 63, 0, 2, 0, 0, 0, 17, 72, 240, 40, 1, 0, 0, 0], [16, 180, 85, 254, 1, 0, 0, 0, 216, 179, 85, 254, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] Figure 18.4: This is a proof-of-concept demonstrating the buer over-read. When run with cargo test get_account_overflow -- --nocapture, it prints 128 bytes with the last 96 bits being over-read. Exploit Scenario Suppose the Rust program leaves secret data in memory. An attacker can interact with the zkTrie to read secret data out-of-bounds. Recommendations Short term, have get_account return an error when it is called on a key containing a storage value. Additionally, this logic should be moved to the Go implementation instead of residing in the Rust bindings. Long term, review all unsafe code, especially code related to pointer manipulation, to prevent similar issues.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "19. Unchecked usize to c_int casts allow hash collisions by length misinterpretation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-scroll-zktrie-securityreview.pdf", "body": "A set of unchecked integer casting operations can lead to hash collisions and runtime errors reached from the public Rust interface. The Rust library regularly needs to convert the input functions byte array length from the usize type to the c_int type. Depending on the architecture, these types might dier in size and signedness. This dierence allows an attacker to provide an array with a maliciously chosen length that will be cast to a dierent number. The attacker can choose to manipulate the array and cast the value to a smaller value than the actual array length, allowing the attacker to create two leaf nodes from dierent byte arrays that result in the same hash. The attacker is also able to cast the value to a negative number, causing a runtime error when the Go library calls the GoBytes function. The issue is caused by the explicit and unchecked cast using the as operator and occurs in the ZkTrieNode::parse, ZkMemoryDb::add_node_bytes, ZkTrie::get, ZkTrie::prove, ZkTrie::update, and ZkTrie::delete functions (all of which are public). Figure 19.1 shows ZkTrieNode::parse: impl ZkTrieNode { pub fn parse(data: &[u8]) -> Self { Self { trie_node: unsafe { NewTrieNode(data.as_ptr(), data.len() as c_int) }, } } Figure 19.1: zktrie/src/lib.rs#133138 To achieve a collision for nodes constructed from dierent byte arrays, rst observe that (c_int::MAX as usize) * 2 + 2 is 0 when cast to c_int. Thus, creating two nodes that have the same prex and are then padded with dierent bytes with that length will cause the Go library to interpret only the common prex of these nodes. The following test showcases this exploit. #[test] fn invalid_cast() { init_hash_scheme(hash_scheme); // common prefix let nd = &hex::decode(\"012098f5fb9e239eab3ceac3f27b81e481dc3124d55ffed523a839ee8446b648640101 00000000000000000000000000000000000000000000000000000000018282256f8b00\").unwrap(); // create node1 with prefix padded by zeroes let mut vec_nd = nd.to_vec(); let mut zero_padd_data = vec![0u8; (c_int::MAX as usize) * 2 + 2]; vec_nd.append(&mut zero_padd_data); let node1 = ZkTrieNode::parse(&vec_nd); // create node2 with prefix padded by ones let mut vec_nd = nd.to_vec(); let mut one_padd_data = vec![1u8; (c_int::MAX as usize) * 2 + 2]; vec_nd.append(&mut one_padd_data); let node2 = ZkTrieNode::parse(&vec_nd); // create node3 with just the prefix let node3 = ZkTrieNode::parse(&hex::decode(\"012098f5fb9e239eab3ceac3f27b81e481dc3124d55ffed523a8 39ee8446b64864010100000000000000000000000000000000000000000000000000000000018282256f 8b00\").unwrap()); // all hashes are equal assert_eq!(node1.node_hash(), node2.node_hash()); assert_eq!(node1.node_hash(), node3.node_hash()); } Figure 19.2: A test showing three dierent leaf nodes with colliding hashes This nding also allows an attacker to cause a runtime error by choosing the data array with the appropriate length that will cause the cast to result in a negative number. Figure 19.2 shows a test that triggers the runtime error for the parse function: #[test] fn invalid_cast() { init_hash_scheme(hash_scheme); let data = vec![0u8; c_int::MAX as usize + 1]; println!(\"{:?}\", data.len() as c_int); let _nd = ZkTrieNode::parse(&data); } // running 1 test // -2147483648 // panic: runtime error: gobytes: length out of range // goroutine 17 [running, locked to thread]: _cgo_gotypes.go:102 // main._Cfunc_GoBytes(...) // // main.NewTrieNode.func1(0x14000062de8?, 0x80000000) // /zktrie/lib.go:78 +0x50 // main.NewTrieNode(0x14000062e01?, 0x2680?) /zktrie/lib.go:78 +0x1c // Figure 19.3: A test that triggers the issue, whose output shows the reinterpreted length of the array Exploit Scenario An attacker provides two dierent byte arrays that will have the same node_hash, breaking the assumption that such nodes are hard to obtain. Recommendations Short term, have the code perform the cast in a checked manner by using the c_int::try_from method to allow validation if the conversion succeeds. Determine whether the Rust functions should allow arbitrary length inputs; document the length requirements and assumptions. Long term, regularly run Clippy in pedantic mode to nd and x all potentially dangerous casts. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "1. Publish-subscribe protocol users are vulnerable to a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The API3 system implements a publish-subscribe protocol through which a requester can receive a callback from an API when specied conditions are met. These conditions can be hard-coded when the Airnode is congured or stored on-chain. When they are stored on-chain, the user can call storeSubscription to establish other conditions for the callback (by specifying parameters and conditions arguments of type bytes ). The arguments are then used in abi.encodePacked , which could result in a subscriptionId collision. function storeSubscription( [...] bytes calldata parameters, bytes calldata conditions, [...] ) external override returns ( bytes32 subscriptionId) { [...] subscriptionId = keccak256 ( abi .encodePacked( chainId, airnode, templateId, parameters , conditions , relayer, sponsor, requester, fulfillFunctionId ) ); subscriptions[subscriptionId] = Subscription({ chainId: chainId, airnode: airnode, templateId: templateId, parameters: parameters, conditions: conditions, relayer: relayer, sponsor: sponsor, requester: requester, fulfillFunctionId: fulfillFunctionId }); Figure 1.1: StorageUtils.sol#L135-L158 The Solidity documentation includes the following warning: If you use keccak256(abi.encodePacked(a, b)) and both a and b are dynamic types, it is easy to craft collisions in the hash value by moving parts of a into b and vice-versa. More specically, abi.encodePacked(\"a\", \"bc\") == abi.encodePacked(\"ab\", \"c\"). If you use abi.encodePacked for signatures, authentication or data integrity, make sure to always use the same types and check that at most one of them is dynamic. Unless there is a compelling reason, abi.encode should be preferred. Figure 1.2: The Solidity documentation details the risk of a collision caused by the use of abi.encodePacked with more than one dynamic type. Exploit Scenario Alice calls storeSubscription to set the conditions for a callback from a specic API to her smart contract. Eve, the owner of a competitor protocol, calls storeSubscription with the same arguments as Alice but moves the last byte of the parameters argument to the beginning of the conditions argument. As a result, the Airnode will no longer report API results to Alices smart contract. Recommendations Short term, use abi.encode instead of abi.encodePacked . Long term, carefully review the Solidity documentation , particularly the Warning sections regarding the pitfalls of abi.encodePacked .", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The API3 contracts have enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the API3 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Decisions to opt out of a monetization scheme are irreversible ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The API3 protocol implements two on-chain monetization schemes. If an Airnode owner decides to opt out of a scheme, the Airnode will not receive additional token payments or deposits (depending on the scheme). Although the documentation states that Airnodes can opt back in to a scheme, the current implementation does not allow it. /// @notice If the Airnode is participating in the scheme implemented by /// the contract: /// Inactive: The Airnode is not participating, but can be made to /// participate by a mantainer /// Active: The Airnode is participating /// OptedOut: The Airnode actively opted out, and cannot be made to /// participate unless this is reverted by the Airnode mapping(address => AirnodeParticipationStatus) public override airnodeToParticipationStatus; Figure 3.1: RequesterAuthorizerWhitelisterWithToken.sol#L59-L68 /// @notice Sets Airnode participation status /// @param airnode Airnode address /// @param airnodeParticipationStatus Airnode participation status function setAirnodeParticipationStatus( address airnode, AirnodeParticipationStatus airnodeParticipationStatus ) external override onlyNonZeroAirnode(airnode) { if (msg.sender == airnode) { require( airnodeParticipationStatus == AirnodeParticipationStatus.OptedOut, \"Airnode can only opt out\" ); } else { [...] Figure 3.2: RequesterAuthorizerWhitelisterWithToken.sol#L229-L242 Exploit Scenario Bob, an Airnode owner, decides to temporarily opt out of a scheme, believing that he will be able to opt back in; however, he later learns that that is not possible and that his Airnode will be unable to accept any new requesters. Recommendations Short term, adjust the setAirnodeParticipationStatus function to allow Airnodes that have opted out of a scheme to opt back in. Long term, write extensive unit tests that cover all of the expected pre- and postconditions. Unit tests could have uncovered this issue.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Depositors can front-run request-blocking transactions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "A depositor can front-run a request-blocking transaction and withdraw his or her deposit. The RequesterAuthorizerWhitelisterWithTokenDeposit contract enables a user to indenitely whitelist a requester by depositing tokens on behalf of the requester. A manager or an address with the blocker role can call setRequesterBlockStatus or setRequesterBlockStatusForAirnode with the address of a requester to block that user from submitting requests; as a result, any user who deposited tokens to whitelist the requester will be blocked from withdrawing the deposit. However, because one can execute a withdrawal immediately, a depositor could monitor the transactions and call withdrawTokens to front-run a blocking transaction. Exploit Scenario Eve deposits tokens to whitelist a requester. Because the requester then uses the system maliciously, the manager blacklists the requester, believing that the deposited tokens will be seized. However, Eve front-runs the transaction and withdraws the tokens. Recommendations Short term, implement a two-step withdrawal process in which a depositor has to express his or her intention to withdraw a deposit and the funds are then unlocked after a waiting period. Long term, analyze all possible front-running risks in the system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. Incompatibility with non-standard ERC20 tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The RequesterAuthorizerWhitelisterWithTokenPayment and RequesterAuthorizerWhitelisterWithTokenDeposit contracts are meant to work with any ERC20 token. However, several high-prole ERC20 tokens do not correctly implement the ERC20 standard. These include USDT, BNB, and OMG, all of which have a large market cap. The ERC20 standard denes two transfer functions, among others:  transfer(address _to, uint256 _value) public returns (bool success)  transferFrom(address _from, address _to, uint256 _value) public returns (bool success) These high-prole ERC20 tokens do not return a boolean when at least one of the two functions is executed. As of Solidity 0.4.22, the size of return data from external calls is checked. As a result, any call to the transfer or transferFrom function of an ERC20 token with an incorrect return value will fail. Exploit Scenario Bob deploys the RequesterAuthorizerWhitelisterWithTokenPayment contract with USDT as the token. Alice wants to pay for a requester to be whitelisted and calls payTokens , but the transferFrom call fails. As a result, the contract is unusable. Recommendations Short term, consider using the OpenZeppelin SafeERC20 library or adding explicit support for ERC20 tokens with incorrect return values. Long term, adhere to the token integration best practices outlined in appendix C .", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "6. Compromise of a single oracle enables limited control of the dAPI value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "By compromising only one oracle, an attacker could gain control of the median price of a dAPI and set it to a value within a certain range. The dAPI value is the median of all values provided by the oracles. If the number of oracles is odd (i.e., the median is the value in the center of the ordered list of values), an attacker could skew the median, setting it to a value between the lowest and highest values submitted by the oracles. Exploit Scenario There are three available oracles: O 0 , with a price of 603; O 1 , with a price of 598; and O 2 , which has been compromised by Eve. Eve is able to set the median price to any value in the range [598 , 603] . Eve can then turn a prot by adjusting the rate when buying and selling assets. Recommendations Short term, be mindful of the fact that there is no simple x for this issue; regardless, we recommend implementing o-chain monitoring of the DapiServer contracts to detect any suspicious activity. Long term, assume that an attacker may be able to compromise some of the oracles. To mitigate a partial compromise, ensure that dAPI value computations are robust.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The execution of yarn audit identied dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the repositories under review. The output below details these issues. CVE ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "8. DapiServer beacon data is accessible to all users ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The lack of access controls on the conditionPspDapiUpdate function could allow an attacker to read private data on-chain. The dataPoints[] mapping contains private data that is supposed to be accessible on-chain only by whitelisted users. However, any user can call conditionPspDapiUpdate , which returns a boolean that depends on arithmetic over dataPoint : /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters /// @dev This method does not allow the caller to indirectly read a dAPI, /// which is why it does not require the sender to be a void signer with /// zero address. [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 8.1: dapis/DapiServer.sol:L468-L502 An attacker could abuse this function to deduce one bit of data per call (to determine, for example, whether a users account should be liquidated). An attacker could also automate the process of accessing one bit of data to extract a larger amount of information by using a mechanism such as a dichotomic search. An attacker could therefore infer the value of dataPoin t directly on-chain. Exploit Scenario Eve, who is not whitelisted, wants to read a beacon value to determine whether a certain users account should be liquidated. Using the code provided in appendix E , she is able to conrm that the beacon value is greater than or equal to a certain threshold. Recommendations Short term, implement access controls to limit who can call conditionPspDapiUpdate . Long term, document all read and write operations related to dataPoint , and highlight their access controls. Additionally, consider implementing an o-chain monitoring system to detect any suspicious activity.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Misleading function name ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/API3.pdf", "body": "The conditionPspDapiUpdate function always updates the dataPoints storage variable (by calling updateDapiWithBeacons ), even if the function returns false (i.e., the condition for updating the variable is not met). This contradicts the code comment and the behavior implied by the functions name. /// @notice Returns if the respective dAPI needs to be updated based on the /// condition parameters [...] function conditionPspDapiUpdate( bytes32 subscriptionId, // solhint-disable-line no-unused-vars bytes calldata data, bytes calldata conditionParameters ) external override returns (bool) { bytes32 dapiId = keccak256(data); int224 currentDapiValue = dataPoints[dapiId].value; require( dapiId == updateDapiWithBeacons(abi.decode(data, (bytes32[]))), \"Data length not correct\" ); return calculateUpdateInPercentage( currentDapiValue, dataPoints[dapiId].value ) >= decodeConditionParameters(conditionParameters); } Figure 9.1: dapis/DapiServer.sol#L468-L502 Recommendations Short term, revise the documentation to inform users that a call to conditionPspDapiUpdate will update the dAPI even if the function returns false . Alternatively, develop a function similar to updateDapiWithBeacons that returns the updated value without actually updating it. Long term, ensure that functions names reect the implementation.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft Finance contracts have enabled compiler optimizations. There have been several optimization bugs with security implications. Additionally, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild use them, so how well they are being tested and exercised is unknown. High-severity security issues due to optimization bugs have occurred in the past. For example, a high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG . Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity v0.5.6 . More recently, a bug due to the incorrect caching of Keccak-256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizations causes a security vulnerability in the Raft Finance contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "2. Issues with Chainlink oracles return data validation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "Chainlink oracles are used to compute the price of a collateral token throughout the protocol. When validating the oracle's return data, the returned price is compared to the price of the previous round. However, there are a few issues with the validation:    The increase of the currentRoundId value may not be statically increasing across rounds. The only requirement is that the roundID increases monotonically. The updatedAt value in the oracle response is never checked, so potentially stale data could be coming from the priceAggregator contract. The roundId and answeredInRound values in the oracle response are not checked for equality, which could indicate that the answer returned by the oracle is fresh. function _badChainlinkResponse (ChainlinkResponse memory response) internal view returns ( bool ) { return !response.success || response.roundId == 0 || response.timestamp == 0 || response.timestamp > block.timestamp || response.answer <= 0 ; } Figure 2.1: The Chainlink oracle response validation logic Exploit Scenario The Chainlink oracle attempts to compare the current returned price to the price in the previous roundID . However, because the roundID did not increase by one from the previous round to the current round, the request fails, and the price oracle returns a failure. A stale price is then used by the protocol. Recommendations Short term, have the code validate that the timestamp value is greater than 0 to ensure that the data is fresh. Also, have the code check that the roundID and answeredInRound values are equal to ensure that the returned answer is not stale. Lastly check that the timestamp value is not decreasing from round to round. Long term, carefully investigate oracle integrations for potential footguns in order to conform to correct API usage. References  The Historical-Price-Feed-Data Project", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "3. Incorrect constant for 1000-year periods ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft nance contracts rely on computing the exponential decay to determine the correct base rate for redemptions. In the MathUtils library, a period of 1000 years is chosen as the maximum time period for the decay exponent to prevent an overow. However, the _MINUTES_IN_1000_YEARS constant used is currently incorrect: /// @notice Number of minutes in 1000 years. uint256 internal constant _MINUTES_IN_1000_YEARS = 1000 * 356 days / 1 minutes; Figure 3.1: The declaration of the _MINUTES_IN_1000_YEARS constant Recommendations Short term, change the code to compute the _MINUTES_IN_1000_YEARS constant as 1000 * 365 days / 1 minutes . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the system. Integrate Echidna and smart contract fuzzing in the system to triangulate subtle arithmetic issues.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Inconsistent use of safeTransfer for collateralToken ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft contracts rely on ERC-20 tokens as collateral that must be deposited in order to mint R tokens. However, although the SafeERC20 library is used for collateral token transfers, there are a few places where the safeTransfer function is missing:  The transfer of collateralToken in the liquidate function in the PositionManager contract: if (!isRedistribution) { rToken.burn( msg.sender , entirePositionDebt); _totalDebt -= entirePositionDebt; emit TotalDebtChanged(_totalDebt); // Collateral is sent to protocol as a fee only in case of liquidation collateralToken.transfer(feeRecipient, collateralLiquidationFee); } collateralToken.transfer( msg.sender , collateralToSendToLiquidator); Figure 4.1: Unchecked transfers in PositionManager.liquidate  The transfer of stETH in the managePositionStETH function in the PositionManagerStETH contract: { if (isCollateralIncrease) { stETH.transferFrom( msg.sender , address ( this ), collateralChange); stETH.approve( address (wstETH), collateralChange); uint256 wstETHAmount = wstETH.wrap(collateralChange); _managePosition( ... ); } else { _managePosition( ... ); uint256 stETHAmount = wstETH.unwrap(collateralChange); stETH.transfer( msg.sender , stETHAmount); } } Figure 4.2: Unchecked transfers in PositionManagerStETH.managePositionStETH Exploit Scenario Governance approves an ERC-20 token that returns a Boolean on failure to be used as collateral. However, since the return values of this ERC-20 token are not checked, Alice, a liquidator, does not receive any collateral for performing a liquidation. Recommendations Short term, use the SafeERC20 librarys safeTransfer function for the collateralToken . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "5. Tokens may be trapped in an invalid position ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft nance contracts allow users to take out positions by depositing collateral and minting a corresponding amount of R tokens as debt. In order to exit a position, a user must pay back their debt, which allows them to receive their collateral back. To check that a position is closed, the _managePosition function contains a branch that validates that the position's debt is zero after adjustment. However, if the position's debt is zero but there is still some collateral present even after adjustment, then the position is considered invalid and cannot be closed. This could be problematic, especially if some dust is present in the position after the collateral is withdrawn. if (positionDebt == 0 ) { if (positionCollateral != 0 ) { revert InvalidPosition(); } // position was closed, remove it _closePosition(collateralToken, position, false ); } else { _checkValidPosition(collateralToken, positionDebt, positionCollateral); if (newPosition) { collateralTokenForPosition[position] = collateralToken; emit PositionCreated(position); } } Figure 5.1: A snippet from the _managePosition function showing that a position with no debt cannot be closed if any amount of collateral remains Exploit Scenario Alice, a borrower, wants to pay back her debt and receive her collateral in exchange. However, she accidentally leaves some collateral in her position despite paying back all her debt. As a result, her position cannot be closed. Recommendations Short term, if a position's debt is zero, have the _managePosition function refund any excess collateral and close the position. Long term, carefully investigate potential edge cases in the system and use smart contract fuzzing to determine if those edge cases can be realistically reached.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Price deviations between stETH and ETH may cause Tellor oracle to return an incorrect price ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft nance contracts rely on oracles to compute the price of the collateral tokens used throughout the codebase. If the Chainlink oracle is down, the Tellor oracle is used as a backup. However, the Tellor oracle does not use the stETH/USD price feed. Instead it uses the ETH/USD price feed to determine the price of stETH. This could be problematic if stETH depegs, which can occur during black swan events. function _getCurrentTellorResponse() internal view returns (TellorResponse memory tellorResponse) { uint256 count; uint256 time; uint256 value; try tellor.getNewValueCountbyRequestId(ETHUSD_TELLOR_REQ_ID) returns ( uint256 count_) { count = count_; } catch { return (tellorResponse); } Figure 6.1: The Tellor oracle fetching the price of ETH to determine the price of stETH Exploit Scenario Alice has a position in the system. A signicant black swan event causes the depeg of staked Ether. As a result, the Tellor oracle returns an incorrect price, which prevents Alice's position from being liquidated despite being eligible for liquidation. Recommendations Short term, carefully monitor the Tellor oracle, especially during any sort of market volatility. Long term, investigate the robustness of the oracles and document possible circumstances that could cause them to return incorrect prices.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Incorrect constant value for MAX_REDEMPTION_SPREAD ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "The Raft protocol allows a user to redeem their R tokens for underlying wstETH at any time. By doing so, the protocol ensures that it maintains overcollateralization. The redemption spread is part of the redemption rate, which changes based on the price of the R token to incentivize or disincentivize redemption. However, the documentation says that the maximum redemption spread should be 100% and that the protocol will initially set it to 100%. In the code, the MAX_REDEMPTION_SPREAD constant is set to 2%, and the redemptionSpread variable is set to 1% at construction. This is problematic because setting the rate to 100% is necessary to eectively disable redemptions at launch. uint256 public constant override MIN_REDEMPTION_SPREAD = MathUtils._100_PERCENT / 10_000 * 25 ; // 0.25% uint256 public constant override MAX_REDEMPTION_SPREAD = MathUtils._100_PERCENT / 100 * 2 ; // 2% Figure 7.1: Constants specifying the minimum and maximum redemption spread percentages constructor (ISplitLiquidationCollateral newSplitLiquidationCollateral) FeeCollector( msg.sender ) { rToken = new RToken( address ( this ), msg.sender ); raftDebtToken = new ERC20Indexable( address ( this ), string ( bytes .concat( \"Raft \" , bytes (IERC20Metadata( address (rToken)).name()), \" debt\" )), string ( bytes .concat( \"r\" , bytes (IERC20Metadata( address (rToken)).symbol()), \"-d\" )) ); setRedemptionSpread(MathUtils._100_PERCENT / 100 ); setSplitLiquidationCollateral(newSplitLiquidationCollateral); emit PositionManagerDeployed(rToken, raftDebtToken, msg.sender ); } Figure 7.2: The redemption spread being set to 1% instead of 100% in the PositionManager s constructor Exploit Scenario The protocol sets the redemption spread to 2%. Alice, a borrower, redeems her R tokens for some underlying wstETH, despite the developers intentions. As a result, the stablecoin experiences signicant volatility. Recommendations Short term, set the MAX_REDEMPTION_SPREAD value to 100% and set the redemptionSpread variable to MAX_REDEMPTION_SPREAD in the PositionManager contracts constructor. Long term, improve unit test coverage to identify incorrect behavior and edge cases in the protocol.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "8. Liquidation rewards are calculated incorrectly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-04-tempus-raft-securityreview.pdf", "body": "Whenever a position's collateralization ratio falls between 100% and 110%, the position becomes eligible for liquidation. A liquidator can pay o the position's total debt to restore solvency. In exchange, the liquidator receives a liquidation reward for removing bad debt, in addition to the amount of debt the liquidator has paid o. However, the calculation performed in the split function is incorrect and does not reward the liquidator with the matchingCollateral amount of tokens: function split( uint256 totalCollateral, uint256 totalDebt, uint256 price, bool isRedistribution ) external pure returns ( uint256 collateralToSendToProtocol, uint256 collateralToSentToLiquidator) { if (isRedistribution) { ... } else { uint256 matchingCollateral = totalDebt.divDown(price); uint256 excessCollateral = totalCollateral - matchingCollateral; uint256 liquidatorReward = excessCollateral.mulDown(_calculateLiquidatorRewardRate(totalDebt)); collateralToSendToProtocol = excessCollateral - liquidatorReward; collateralToSentToLiquidator = liquidatorReward; } } Figure 8.1: The calculations for how to split the collateral between the liquidator and the protocol, showing that the matchingCollateral is omitted from the liquidators reward Exploit Scenario Alice, a liquidator, attempts to liquidate an insolvent position. However, upon liquidation, she receives only the liquidationReward amount of tokens, without the matchingCollateral . As a result her liquidation is unprotable and she has lost funds. Recommendations Short term, have the code compute the collateralToSendToLiquidator variable as liquidationReward + matchingCollateral . Long term, improve unit test coverage to uncover edge cases and ensure intended behavior throughout the protocol. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "1. Consoles Field and Scalar divisions panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The division operation of the Field and Scalar types do not guard against a division by zero. This causes a runtime panic when values from these types are divided by zero. Figure 1.1 shows a test and the respective stack backtrace, where a None option is unconditionally unwrapped in snarkvm/fields/src/fp_256.rs : #[test] fn test_div () { let zero = Field::<CurrentEnvironment>::zero(); // Sample a new field. let num = Field::<CurrentEnvironment>::new(Uniform::rand(& mut test_rng())); // Divide by zero let neg = num.div(zero); } // running 1 test // thread 'arithmetic::tests::test_div' panicked at 'called `Option::unwrap()` on a `None` value', /snarkvm/fields/src/fp_256.rs:709:42 // stack backtrace: // 0: rust_begin_unwind // at /rustc/v/library/std/src/panicking.rs:584:5 // 1: core::panicking::panic_fmt // at /rustc/v/library/core/src/panicking.rs:142:14 // 2: core::panicking::panic // at /rustc/v/library/core/src/panicking.rs:48:5 // 3: core::option::Option<T>::unwrap // at /rustc/v/library/core/src/option.rs:755:21 // 4: <snarkvm_fields::fp_256::Fp256<P> as core::ops::arith::DivAssign<&snarkvm_fields::fp_256::Fp256<P>>>::div_assign // at snarkvm/fields/src/fp_256.rs:709:26 // 5: <snarkvm_fields::fp_256::Fp256<P> as core::ops::arith::Div>::div // at snarkvm/fields/src/macros.rs:524:17 // 6: snarkvm_console_types_field::arithmetic::<impl core::ops::arith::Div for snarkvm_console_types_field::Field<E>>::div // at ./src/arithmetic.rs:143: // 7: snarkvm_console_types_field::arithmetic::tests::test_div // at ./src/arithmetic.rs:305:23 Figure 1.1: Test triggering the division by zero The same issue is present in the Scalar type, which has no zero-check for other : impl <E: Environment > Div<Scalar<E>> for Scalar<E> { type Output = Scalar<E>; /// Returns the `quotient` of `self` and `other`. #[inline] fn div ( self , other: Scalar <E>) -> Self ::Output { Scalar::new( self .scalar / other.scalar) } } Figure 1.2: console/types/scalar/src/arithmetic.rs#L137-L146 Exploit Scenario An attacker sends a zero value which is used in a division, causing a runtime error and the program to halt. Recommendations Short term, add checks to validate that the divisor is non-zero on both the Field and Scalar divisions. Long term, add tests exercising all arithmetic operations with the zero element.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "2. from_xy_coordinates function lacks checks and can panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "Unlike Group::from_x_coordinate , the Group::from_xy_coordinates function does not enforce the resulting point to be on the elliptic curve or in the correct subgroup. Two dierent behaviors can occur depending on the underlying curve:  For a short Weierstrass curve (gure 2.1), the function will always succeed and not perform any membership checks on the point; this could lead to an invalid point being used in other curve operations, potentially leading to an invalid curve attack. /// Initializes a new affine group element from the given coordinates. fn from_coordinates (coordinates: Self ::Coordinates) -> Self { let (x, y, infinity) = coordinates; Self { x, y, infinity } } Figure 2.1: No curve membership checks present at curves/src/templates/short_weierstrass_jacobian/affine.rs#L103-L107  For a twisted Edwards curve (gure 2.2), the function will panic if the point is not on the curveunlike the from_x_coordinate function, which returns an Option . /// Initializes a new affine group element from the given coordinates. fn from_coordinates (coordinates: Self ::Coordinates) -> Self { let (x, y) = coordinates; let point = Self { x, y }; assert! (point.is_on_curve()); point } Figure 2.2: curves/src/templates/twisted_edwards_extended/affine.rs#L102-L108 Exploit Scenario An attacker is able to construct an invalid point for the short Weierstrass curve, potentially revealing secrets if this point is used in scalar multiplications with secret data. Recommendations Short term, make the output type similar to the from_x_coordinate function, returning an Option . Enforce curve membership on the short Weierstrass implementation and consider returning None instead of panicking when the point is not on the twisted Edwards curve.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "3. Blake2Xs implementation fails to provide the requested number of bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The Blake2Xs implementation returns an empty byte array when the requested number of bytes is between u16::MAX-30 and u16::MAX . The Blake2Xs is an extendible-output hash function (XOF): It receives a parameter called xof_digest_length that determines how many bytes the hash function should return. When computing the necessary number of rounds, there is an integer overow if xof_digest_length is between u16::MAX-30 and u16::MAX . This integer overow causes the number of rounds to be zero and the resulting hash to have zero bytes. fn evaluate (input: & [ u8 ], xof_digest_length : u16 , persona: & [ u8 ]) -> Vec < u8 > { assert! (xof_digest_length > 0 , \"Output digest must be of non-zero length\" ); assert! (persona.len() <= 8 , \"Personalization may be at most 8 characters\" ); // Start by computing the digest of the input bytes. let xof_digest_length_node_offset = (xof_digest_length as u64 ) << 32 ; let input_digest = blake2s_simd::Params::new() .hash_length( 32 ) .node_offset(xof_digest_length_node_offset) .personal(persona) .hash(input); let mut output = vec! []; let num_rounds = ( xof_digest_length + 31 ) / 32 ; for node_offset in 0 ..num_rounds { Figure 3.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The nding is informational because the hash function is used only on the hash_to_curve routine, and never with an attacker-controlled digest length parameter. The currently used value is the size of the generator, which is not expected to reach values similar to u16::MAX . Exploit Scenario The Blake2Xs hash function is used with the maximum number of bytes, u16::MAX , to compare password hashes. Due to the vulnerability, any password will match the correct one since the hash output is always the empty array, allowing an attacker to gain access. Recommendations Short term, upcast the xof_digest_length variable to a larger type before the sum. This will prevent the overow while enforcing the u16::MAX bound on the requested digest length. 4. Blake2Xs implementations node o\u0000set denition di\u0000ers from specication Severity: Informational Diculty: High Type: Cryptography Finding ID: TOB-ALEOA-4 Target: console/algorithms/src/blake2xs/mod.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Compiling cast instructions can lead to panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The output_types function of the cast instruction assumes that the number of record or interface elds equals the number of input types. // missing checks for (input_type, (_, entry_type)) in input_types.iter().skip( 2 ). zip_eq(record.entries() ) { Figure 5.1: Invocation of zip_eq on two iterators that dier in length ( cast.rs:401 ) Therefore, compiling a program with an unmatched cast instruction will cause a runtime panic. The program in gure 5.2 casts two registers into an interface type with only one eld: program aleotest.aleo; interface message: amount as u64; function test: input r0 as u64.private; cast r0 r0 into r1 as message; Figure 5.2: Program panics during compilation Figure 5.3 shows a program that will panic when compiling because it casts three registers into a record type with two elds: program aleotest.aleo; record token: owner as address.private; gates as u64.private; function test: input r0 as address.private; input r1 as u64.private; cast r0 r1 r1 into r2 as token.record; Figure 5.3: Program panics during compilation The following stack trace is printed in both cases: <itertools::zip_eq_impl::ZipEq<I,J> as core::iter::traits::iterator::Iterator>::next::h5c767bbe55881ac0 snarkvm_compiler::program::instruction::operation::cast::Cast<N>::output_types::h3d1 251fbb81d620f snarkvm_compiler::process::stack::helpers::insert::<impl snarkvm_compiler::process::stack::Stack<N>>::check_instruction::h6bf69c769d8e877b snarkvm_compiler::process::stack::Stack<N>::new::hb1c375f6e4331132 snarkvm_compiler::process::deploy::<impl snarkvm_compiler::process::Process<N>>::deploy::hd75a28b4fc14e19e snarkvm_fuzz::harness::fuzz_program::h131000d3e1900784 This bug was discovered through fuzzing with LibAFL . Figure 5.4: Stack trace Recommendations Short term, add a check to validate that the number of Cast arguments equals the number of record or interface elds. Long term, review all other uses of zip_eq and check the length of their iterators.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "6. Displaying an Identier can cause a panic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The Identifier of a program uses Fields internally. It is possible to construct an Identifier from an arbitrary bits array. However, the implementation of the Display trait of Identifier expects that this arbitrary data is valid UTF-8. Creating an identier from a bytes array already checks whether the bytes are valid UTF-8. The following formatting function tries to create a UTF-8 string regardless of the bits of the eld. fn fmt (& self , f: & mut Formatter) -> fmt :: Result { // Convert the identifier to bits. let bits_le = self . 0. to_bits_le(); // Convert the bits to bytes. let bytes = bits_le .chunks( 8 ) .map(|byte| u8 ::from_bits_le(byte).map_err(|_| fmt::Error)) .collect::< Result < Vec < u8 >, _>>()?; // Parse the bytes as a UTF-8 string. let string = String ::from_utf8(bytes).map_err(|_| fmt::Error)? ; ... } Figure 6.1: Relevant code ( parse.rs:76 ) As a result, constructing an Identifier with invalid UTF-8 bit array will cause a runtime error when the Identifier is displayed. The following test shows how to construct such an Identifier . #[test] fn test_invalid_identifier () { let invalid: & [ u8 ] = &[ 112 , 76 , 113 , 165 , 54 , 175 , 250 , 182 , 196 , 85 , 111 , 26 , 71 , 35 , 81 , 194 , 56 , 50 , 216 , 176 , 126 , 15 ]; let bits: Vec < bool > = invalid.iter().flat_map(|n| [n & ( 1 << 7 ) != 0 , n & ( 1 << 6 ) != 0 , n & ( 1 << 5 ) != 0 , n & ( 1 << 4 ) != 0 , n & ( 1 << 3 ) != 0 , n & ( 1 << 2 ) != 0 , n & ( 1 << 1 ) != 0 , n & ( 1 << 0 ) != 0 ]).collect(); let name = Identifier::from_bits_le(&bits).unwrap(); let network = Identifier::from_str( \"aleo\" ).unwrap(); let id = ProgramID::<CurrentNetwork>::from((name, network)); println!( \"{:?}\" , id.to_string()); } // a Display implementation returned an error unexpectedly: Error // thread 'program::tests::test_invalid_identifier' panicked at 'a Display implementation returned an error unexpectedly: Error', library/core/src/result.rs:1055:23 4: <T as alloc::string::ToString>::to_string at /rustc/dc80ca78b6ec2b6bba02560470347433bcd0bb3c/library/alloc/src/string.rs:2489:9 5: snarkvm_compiler::program::tests::test_invalid_identifier at ./src/program/mod.rs:650:26 Figure 6.2: Test causing a panic The testnet3_add_fuzz_tests branch has a workaround that prevents nding this issue. Using the arbitrary crate, it is likely that non-UTF-8 bit-strings end up in identiers. We suggest xing this bug instead of using the workaround. This bug was discovered through fuzzing with LibAFL. Recommendations Short term, we suggest using a placeholder like unprintable identifier instead of returning a formatting error. Alternatively, a check for UTF-8 could be added in the Identifier::from_bits_le .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "7. Build script causes compilation to rerun ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "Using the current working directory as a rerun condition causes unnecessary recompilations, as any change in cargos target directory will trigger a compilation. // Re-run upon any changes to the workspace. println!( \"cargo:rerun-if-changed=.\" ); Figure 7.1: Rerun condition in build.rs ( build.rs:57 ) The root build script also implements a check that all les include the proper license. However, the check is insucient to catch all cases where developers forget to include a license. Adding a new empty Rust le without modifying any other le will not make the check in the build.rs fail because the check is not re-executed. Recommendations Short term, remove the rerun condition and use the default Cargo behavior . By default cargo reruns the build.rs script if any Rust le in the source tree has changed. Long term, consider using a git commit hook to check for missing licenses at the top of les. An example of such a commit hook can be found here .", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "8. Invisible codepoints are supported ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The current parser allows any Unicode character in strings or comments, which can include invisible bidirectional override characters . Using such characters can lead to dierences between the code reviewed in a pull request and the compiled code. Figure 8.1 shows such a program: since r2 and r3 contain the hash of the same string, r4 is true , and r5 equals r1 , the output token has the amount eld set to the second input. However, the compiled program always returns a token with a zero amount . // Program comparing aleo with aleo string program aleotest.aleo; record token: owner as address.private; gates as u64.private; amount as u64.private; function mint: input r0 as address.private; input r1 as u64.private; hash.psd2 \"aleo\" into r2; \" into r3; hash.psd2 \"aleo // Same string again is.eq r2 r3 into r4; // r4 is true because r2 == r3 ternary r4 r1 0u64 into r5; // r5 is r1 because r4 is true cast r0 0u64 r5 into r6 as token.record; output r6 as token.record; Figure 8.1: Aleo program that evaluates unexpectedly By default, VSCode shows the Unicode characters (gure 8.2). Google Docs and GitHub display the source code as in gure 8.1. Figure 8.2: The actual source This nding is inspired by CVE-2021-42574 . Recommendations Short term, reject the following code points: U+202A, U+202B, U+202C, U+202D, U+202E, U+2066, U+2067, U+2068, U+2069. This list might not be exhaustive. Therefore, consider disabling all non-ASCII characters in the Aleo language. In the future, consider introducing escape sequences so users can still use bidirectional code points if there is a legitimate use case.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "9. Merkle tree constructor panics with large leaf array ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The Merkle tree constructor panics or returns a malformed Merkle tree when the provided leaves array has more than usize::MAX/2 elements. To build a Merkle tree, the constructor receives the necessary array of leaves. Being a binary tree, the nal total number of nodes is computed using the smallest power of two above the number of leaves given: pub fn new (leaf_hasher: & LH , path_hasher: & PH , leaves: & [LH::Leaf]) -> Result < Self > { // Ensure the Merkle tree depth is greater than 0. ensure!(DEPTH > 0 , \"Merkle tree depth must be greater than 0\" ); // Ensure the Merkle tree depth is less than or equal to 64. ensure!(DEPTH <= 64 u8 , \"Merkle tree depth must be less than or equal to 64\" ); // Compute the maximum number of leaves. let max_leaves = leaves.len().next_power_of_two() ; // Compute the number of nodes. let num_nodes = max_leaves - 1 ; Figure 9.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The next_power_of_two function will panic in debug mode, and return 0 in release mode if the number is larger than (1 << (N-1)) . For the usize type, on 64-bit machines, the function returns 0 for numbers above 2 63 . On 32-bit machines, the necessary number of leaves would be at least 1+2 31 . Exploit Scenario An attacker triggers a call to the Merkle tree constructor with 1+2 31 leaves, causing the 32-bit machine to abort due to a runtime error or to return a malformed Merkle tree. Recommendations Short term, use checked_next_power_of_two and check for success. Check all other uses of the next_power_of_two for similar issues.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Downcast possibly truncates value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "To validate the console's Ciphertext eld vector length against a u32 constant, the program downcasts the length from usize to u32 . This could cause a value truncation and a successful write when an error should occur. Then, the program downcasts the value to a u16 , not checking rst if this is safe without truncation. fn write_le <W: Write >(& self , mut writer: W ) -> IoResult <()> { // Ensure the number of field elements does not exceed the maximum allowed size. if self . 0. len() as u32 > N::MAX_DATA_SIZE_IN_FIELDS { return Err (error( \"Ciphertext is too large to encode in field elements.\" )); } // Write the number of ciphertext field elements. ( self . 0. len() as u16 ).write_le(& mut writer)?; // Write the ciphertext field elements. self . 0. write_le(& mut writer) } } Figure 10.1: console/program/src/data/ciphertext/bytes.rs#L36-L49 Figure 10.2 shows another instance where the value is downcasted to u16 without checking if this is safe: // Ensure the number of field elements does not exceed the maximum allowed size. match num_fields <= N::MAX_DATA_SIZE_IN_FIELDS as usize { // Return the number of field elements. true => Ok ( num_fields as u16 ), Figure 10.2: console/program/src/data/ciphertext/size_in_fields.rs#L21-L30 A similar downcast is present in the Plaintext size_in_fields function . Currently, this downcast causes no issue because the N::MAX_DATA_SIZE_IN_FIELDS constant is less than u16::MAX . However, if this constant were changed, truncating downcasts could occur. Recommendations Short term, upcast N::MAX_DATA_SIZE_IN_FIELDS in Ciphertext::write_le to usize instead of downcasting the vector length, and ensure that the downcasts to u16 are safe.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "11. Plaintext::from_bits_* functions assume array has elements ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The from_bits_le function assumes that the provided array is not empty, immediately indexing the rst and second positions without a length check: /// Initializes a new plaintext from a list of little-endian bits *without* trailing zeros. fn from_bits_le (bits_le: & [ bool ]) -> Result < Self > { let mut counter = 0 ; let variant = [bits_le[counter], bits_le[counter + 1 ]]; counter += 2 ; Figure 11.1: circuit/program/src/data/plaintext/from_bits.rs#L22-L28 A similar pattern is present on the from_bits_be function on both the Circuit and Console implementations of Plaintext . Instead, the function should rst check if the array is empty before accessing elements, or documentation should be added so that the function caller enforces this. Recommendations Short term, check if the array is empty before accessing elements, or add documentation so that the function caller enforces this.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "12. Arbitrarily deep recursion causes stack exhaustion ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The codebase has recursive functions that operate on arbitrarily deep structures. This causes a runtime error as the programs stack is exhausted with a very large number of recursive calls. The Plaintext parser allows an arbitrarily deep interface value such as {bar: {bar: {bar: {... bar: true}}} . Since the formatting function is recursive, a suciently deep interface will exhaust the stack on the fmt_internal recursion. We conrmed this nding with a 2880-level nested interface. Parsing the interface with Plaintext::from_str succeeds, but printing the result causes stack exhaustion: #[test] fn test_parse_interface3 () -> Result <()> { let plain = Plaintext::<CurrentNetwork>::from_str( /* too long to display */ )?; println! ( \"Found: {plain}\\n\" ); Ok (()) } // running 1 test // thread 'data::plaintext::parse::tests::test_deep_interface' has overflowed its stack // fatal runtime error: stack overflow // error: test failed, to rerun pass '-p snarkvm-console-program --lib' Figure 12.1: console/algorithms/src/blake2xs/mod.rs#L32-L47 The same issue is present on the record and record entry formatting routines. The Record::find function is also recursive, and a suciently large argument array could also lead to stack exhaustion. However, we did not conrm this with a test. Exploit Scenario An attacker provides a program with a 2880-level deep interface, which causes a runtime error if the result is printed. Recommendations Short term, add a maximum depth to the supported data structures. Alternatively, implement an iterative algorithm for creating the displayed structure.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "13. Inconsistent pair parsing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The codebase has several implementations to parse pairs from strings of the form key: value depending on the expected type of value . However, these parsers also handle whitespaces around the colon dierently. As an example, gure 13.1 shows a parser that allows whitespaces before the colon, while gure 13.2 shows one that does not: fn parse_pair <N: Network >(string: &str ) -> ParserResult <(Identifier<N>, Plaintext<N>)> { // Parse the whitespace and comments from the string. let (string, _) = Sanitizer::parse(string)?; // Parse the identifier from the string. let (string, identifier) = Identifier::parse(string)?; // Parse the whitespace from the string. let (string, _) = Sanitizer::parse_whitespaces(string)?; // Parse the \":\" from the string. let (string, _) = tag( \":\" )(string)?; // Parse the plaintext from the string. let (string, plaintext) = Plaintext::parse(string)?; Figure 13.1: console/program/src/data/plaintext/parse.rs#L23-L34 fn parse_pair <N: Network >(string: &str ) -> ParserResult <(Identifier<N>, Entry<N, Plaintext<N>>)> { // Parse the whitespace and comments from the string. let (string, _) = Sanitizer::parse(string)?; // Parse the identifier from the string. let (string, identifier) = Identifier::parse(string)?; // Parse the \":\" from the string. let (string, _) = tag( \":\" )(string)?; // Parse the entry from the string. let (string, entry) = Entry::parse(string)?; Figure 13.2: console/program/src/data/record/parse_plaintext.rs#L23-L33 We also found that whitespaces before the comma symbol are not allowed: let (string, owner) = alt(( map(pair( Address::parse, tag( \".public\" ) ), |(owner, _)| Owner::Public(owner)), map(pair( Address::parse, tag( \".private\" ) ), |(owner, _)| { Owner::Private(Plaintext::from(Literal::Address(owner))) }), ))(string)?; // Parse the \",\" from the string. let (string, _) = tag( \",\" )(string)?; Figure 13.3: console/program/src/data/record/parse_plaintext.rs#L52-L60 Recommendations Short term, handle whitespace around marker tags (such as colon, commas, and brackets) uniformly. Consider implementing a generic pair parser that receives the expected value type parser instead of reimplementing it for each type. 14. Signature veries with di\u0000erent messages Severity: Informational Diculty: Low Type: Cryptography Finding ID: TOB-ALEOA-14 Target: console/account/src/signature/verify.rs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "15. Unchecked output length during ToFields conversion ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "When converting dierent types to vectors of Field elements, the codebase has checks to validate that the resulting Field vector has fewer than MAX_DATA_SIZE_IN_FIELDS elements. However, the StringType::to_fields function is missing this validation: impl <E: Environment > ToFields for StringType<E> { type Field = Field<E>; /// Casts a string into a list of base fields. fn to_fields (& self ) -> Vec < Self ::Field> { // Convert the string bytes into bits, then chunk them into lists of size // `E::BaseField::size_in_data_bits()` and recover the base field element for each chunk. // (For advanced users: Chunk into CAPACITY bits and create a linear combination per chunk.) self .to_bits_le().chunks(E::BaseField::size_in_data_bits()).map(Field::from_bits_le) .collect() } } Figure 15.1: circuit/types/string/src/helpers/to_fields.rs#L20-L30 We also remark that other conversion functions, such as from_bits and to_bits , do not constraint the input or output length. Recommendations Short term, add checks to validate the Field vector length for the StringType::to_fields function. Determine if other output functions (e.g., to_bits ) should also enforce length constraints.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "16. Potential panic on ensure_console_and_circuit_registers_match ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The codebase implements the ensure_console_and_circuit_registers_match function, which validates that the values on the console and circuit registers match. The function uses zip_eq to iterate over the two register arrays, but does not check if these arrays have the same length, leading to a runtime error when they do not. pub fn ensure_console_and_circuit_registers_match (& self ) -> Result <()> { use circuit::Eject; for ((console_index, console_register), (circuit_index, circuit_register)) in self .console_registers.iter(). zip_eq (& self .circuit_registers) Figure 16.1: vm/compiler/src/process/registers/mod.rs This runtime error is currently not reachable since the ensure_console_and_circuit_registers_match function is called only in CallStack::Execute mode, and the number of stored registers match in this case: // Store the inputs. function.inputs().iter().map(|i| i.register()).zip_eq(request.inputs()).try_for_each(|(register, input)| { // If the circuit is in execute mode, then store the console input. if let CallStack::Execute(..) = registers.call_stack() { // Assign the console input to the register. registers.store( self , register, input.eject_value())?; } // Assign the circuit input to the register. registers.store_circuit( self , register, input.clone()) })?; Figure 16.2: vm/compiler/src/process/stack/execute.rs Recommendations Short term, add a check to validate that the number of circuit and console registers match on the ensure_console_and_circuit_registers_match function.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. Reserved keyword list is missing owner ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The compiler veries that identiers are not part of a list of reserved keywords. However, the list of keywords is missing the owner keyword. This contrasts with the other record eld, gates , which is a reserved keyword. // Record \"record\" , \"gates\" , // Program Figure 17.1: vm/compiler/src/program/mod.rs Recommendations Short term, add owner to the list of reserved keywords.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "18. Commit and hash instructions not matched against the opcode in check_instruction_opcode ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The check_instruction_opcode function validates that the opcode and instructions match for the Literal , Call , and Cast opcodes, but not for the Commit and Hash opcodes. Although there is partial code for this validation, it is commented out: Opcode::Commit(opcode) => { // Ensure the instruction belongs to the defined set. if ![ \"commit.bhp256\" , \"commit.bhp512\" , \"commit.bhp768\" , \"commit.bhp1024\" , \"commit.ped64\" , \"commit.ped128\" , ] .contains(&opcode) { bail!( \"Instruction '{instruction}' is not the opcode '{opcode}'.\" ); } // Ensure the instruction is the correct one. // match opcode { // \"commit.bhp256\" => ensure!( // matches!(instruction, Instruction::CommitBHP256(..)), // \"Instruction '{instruction}' is not the opcode '{opcode}'.\" // ), // } } Opcode::Hash(opcode) => { // Ensure the instruction belongs to the defined set. if ![ \"hash.bhp256\" , \"hash.bhp512\" , \"hash.bhp768\" , \"hash.bhp1024\" , \"hash.ped64\" , \"hash.ped128\" , \"hash.psd2\" , \"hash.psd4\" , \"hash.psd8\" , ] .contains(&opcode) { bail!( \"Instruction '{instruction}' is not the opcode '{opcode}'.\" ); } // Ensure the instruction is the correct one. // match opcode { // \"hash.bhp256\" => ensure!( // matches!(instruction, Instruction::HashBHP256(..)), // \"Instruction '{instruction}' is not the opcode '{opcode}'.\" // ), // } } Figure 18.1: vm/compiler/src/process/stack/helpers/insert.rs Recommendations Short term, add checks to validate that the opcode and instructions match for the Commit and Hash opcodes.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "19. Incorrect validation of the number of operands ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The implementation of Literals::fmt and Literals::write_le do not correctly validate the number of operands in the operation. Instead of enforcing the exact number of arguments, the implementations only ensure that the number of operands is less than or equal to the expected number of operands: /// Writes the operation to a buffer. fn write_le <W: Write >(& self , mut writer: W ) -> IoResult <()> { // Ensure the number of operands is within the bounds. if NUM_OPERANDS > N::MAX_OPERANDS { return Err (error( format! ( \"The number of operands must be <= {}\" , N::MAX_OPERANDS))); } // Ensure the number of operands is correct. if self .operands.len() > NUM_OPERANDS { return Err (error( format! ( \"The number of operands must be {}\" , NUM_OPERANDS))); } Figure 19.1: vm/compiler/src/program/instruction/operation/literals.rs#L294-L303 Recommendations Short term, replace the if statement guard with self.operands.len() != NUM_OPERANDS in both the Literals::fmt and Literals::write_le functions.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "20. Inconsistent and random compiler error message ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "When the compiler nds a type mismatch between arguments and expected parameters, it emits an error message containing a dierent integer each time the code is compiled. Figure 20.1 shows an Aleo program that, when compiled twice, shows two dierent error messages (shown in gure 20.2). The error message also states that u8 is invalid, but at the same time expecting u8 . program main.aleo; closure clo: input r0 as i8; input r1 as u8; pow r0 r1 into r2; output r2 as i8; function compute: input r0 as i8.private; input r1 as i8.public; call clo r0 r1 into r2; // r1 is i8 but the closure requires u8 output r2 as i8.private; Figure 20.1: Aleo program ~/Documents/aleo/foo (testnet3?) $ aleo build  Compiling 'main.aleo'...  Loaded universal setup (in 1537 ms)  'u8' is invalid : expected u8, found 124i8 ~/Documents/aleo/foo (testnet3?) $ aleo build  Compiling 'main.aleo'...  Loaded universal setup (in 1487 ms)  'u8' is invalid : expected u8, found -39i8 Figure 20.2: Two compilation results Figure 20.3 shows the check that validates that the types match and shows the error message containing the actual literal instead of literal.to_type() : Plaintext::Literal(literal, ..) => { // Ensure the literal type matches. match literal.to_type() == *literal_type { true => Ok (()), false => bail!( \"'{ plaintext_type }' is invalid: expected {literal_type}, found { literal }\" ), Figure 20.3: vm/compiler/src/process/stack/helpers/matches.rs#L204-L209 Recommendations Short term, clarify the error message by rephrasing it and presenting only the literal type instead of the full literal.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "21. Instruction add_* methods incorrectly compare maximum number of allowed instructions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "During function and closure parsing, the compiler collects input, regular, and output instructions into three dierent IndexSet s in the add_input , add_instruction , and add_output functions. All of these functions check that the current number of elements in their respective IndexSet does not exceed the maximum allowed number. However, the check is done before inserting the element in the set, allowing inserting in a set that is already at full capacity and creating a set with one element more than the maximum. Figure 21.1 shows the comparison between the current and the maximum number of allowed elements and the subsequent insertion, which is allowed even though the set could already be at full capacity. All add_input , add_instruction , and add_output functions for both the Function and Closure types present similar behavior. Note that although the number of input and output instructions is checked in other locations (e.g., on the add_closure or get_closure functions), the number of regular instructions is not checked there, allowing a function or a closure with 1 + N::MAX_INSTRUCTIONS . fn add_output (& mut self , output: Output <N>) -> Result <()> { // Ensure there are input statements and instructions in memory. ensure!(! self .inputs.is_empty(), \"Cannot add outputs before inputs have been added\" ); ensure!(! self .instructions.is_empty(), \"Cannot add outputs before instructions have been added\" ); // Ensure the maximum number of outputs has not been exceeded. ensure!( self .outputs.len() <= N::MAX_OUTPUTS , \"Cannot add more than {} outputs\" , N::MAX_OUTPUTS); // Insert the output statement. self .outputs.insert(output); Ok (()) } Figure 21.1: vm/compiler/src/program/function/mod.rs#L142-L153 Figure 21.1 shows another issue present only in the add_output functions (for both Function and Closure types): When an output instruction is inserted into the set, no check validates if this particular element is already in the set, replacing the previous element with the same key if present. This causes two output statements to be interpreted as a single one: program main.aleo; closure clo: input r0 as i8; input r1 as u8; pow r0 r1 into r2; output r2 as i8; output r2 as i8; function compute: input r0 as i8.private; input r1 as u8.public; call clo r0 r1 into r2 ; output r2 as i8.private; Figure 21.2: Test program Recommendations Short term, we recommend the following actions:    Modify the checks to validate the maximum number of allowed instructions to prevent the o-by-one error. Validate if outputs are already present in the Function and Closure sets before inserting an output. Add checks to validate the maximum number of instructions in the get_closure , get_function , add_closure , and add_function functions.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "22. Instances of unchecked zip_eq can cause runtime errors ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The zip_eq operator requires that both iterators being zipped have the same length, and panics if they do not. In addition to the cases presented in TOB-ALEOA-5 , we found one more instance where this should be checked: // Retrieve the interface and ensure it is defined in the program. let interface = stack.program().get_interface(&interface_name)?; // Initialize the interface members. let mut members = IndexMap::new(); for (member, (member_name, member_type)) in inputs.iter(). zip_eq (interface.members()) { Figure 22.1: compiler/src/program/instruction/operation/cast.rs#L92-L99 Additionally, we found uses of the zip operator that should be replaced by zip_eq , together with an associated check to validate the equal length of their iterators: /// Checks that the given operands matches the layout of the interface. The ordering of the operands matters. pub fn matches_interface (& self , stack: & Stack <N>, operands: & [Operand<N>], interface: & Interface <N>) -> Result <()> { // Ensure the operands is not empty. if operands.is_empty() { bail!( \"Casting to an interface requires at least one operand\" ) } // Ensure the operand types match the interface. for (operand, (_, member_type)) in operands.iter(). zip (interface.members()) { Figure 22.2: vm/compiler/src/process/register_types/matches.rs#L20-L27 for (operand, (_, entry_type)) in operands.iter().skip( 2 ). zip (record_type.entries()) { Figure 22.3: vm/compiler/src/process/register_types/matches.rs#L106-L107 Exploit Scenario An incorrectly typed program causes the compiler to panic due to a mismatch between the number of arguments in a cast and the number of elements in the casted type. Recommendations Short term, add checks to validate the equal length of the iterators being zipped and replace the uses of zip with zip_eq together with the associated length validations.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "23. Hash functions lack domain separation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The BHP hash function takes as input a collection of booleans, and hashes them. This hash is used to commit to a Record , hashing together the bits of program_id , the record_name , and the record itself. However, no domain separation or input length is added to the hash, allowing a hash collision if a types to_bits_le function returns variable-length arrays: impl <N: Network > Record<N, Plaintext<N>> { /// Returns the record commitment. pub fn to_commitment (& self , program_id: & ProgramID <N>, record_name: & Identifier <N>) -> Result <Field<N>> { // Construct the input as `(program_id || record_name || record)`. let mut input = program_id.to_bits_le(); input.extend(record_name.to_bits_le()); input.extend( self .to_bits_le()); // Compute the BHP hash of the program record. N::hash_bhp1024(&input) } } Figure 23.1: console/program/src/data/record/to_commitment.rs#L19-L29 A similar situation is present on the hash_children function, which is used to compute hashes of two nodes in a Merkle tree: impl <E: Environment , const NUM_WINDOWS: u8 , const WINDOW_SIZE: u8 > PathHash for BHP<E, NUM_WINDOWS, WINDOW_SIZE> { type Hash = Field<E>; /// Returns the hash of the given child nodes. fn hash_children (& self , left: & Self ::Hash, right: & Self ::Hash) -> Result < Self ::Hash> { // Prepend the nodes with a `true` bit. let mut input = vec! [ true ]; input.extend(left.to_bits_le()); input.extend(right.to_bits_le()); // Hash the input. Hash::hash( self , &input) } } Figure 23.2: circuit/collections/src/merkle_tree/helpers/path_hash.rs#L33-L47 If the implementations of the to_bits_le functions return variable length arrays, it would be easy to create two dierent inputs that would result in the same hash. Recommendations Short term, either enforce each types to_bits_le function to always be xed length or add the input length and domain separators to the elements to be hashed by the BHP hash function. This would prevent the hash collisions even if the to_bits_le functions were changed in the future.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "24. Deployment constructor does not enforce the network edition value ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The Deployment type includes the edition value, which should match the network edition value. However, this is not enforced in the deployment constructor as it is in the Execution constructor. impl <N: Network > Deployment<N> { /// Initializes a new deployment. pub fn new ( edition: u16 , program: Program <N>, verifying_keys: IndexMap <Identifier<N>, (VerifyingKey<N>, Certificate<N>)>, ) -> Result < Self > { Ok ( Self { edition, program, verifying_keys }) } } Figure 24.1: vm/compiler/src/process/deployment/mod.rs#L37-L44 Recommendations Short term, consider using the N::EDITION value in the Deployment constructor, similarly to the Execution constructor.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "25. Map insertion return value is ignored ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "Some insertions into hashmap data types ignore whether the insertion overwrote an element already present in the hash map. For example, when handling the proving and verifying key index maps, the Optional return value from the insert function is ignored: #[inline] pub fn insert_proving_key (& self , function_name: & Identifier <N>, proving_key: ProvingKey <N>) { self .proving_keys.write().insert(*function_name, proving_key); } /// Inserts the given verifying key for the given function name. #[inline] pub fn insert_verifying_key (& self , function_name: & Identifier <N>, verifying_key: VerifyingKey <N>) { self .verifying_keys.write().insert(*function_name, verifying_key); } Figure 25.1: vm/compiler/src/process/stack/mod.rs#L336-L346 Other examples of ignored insertion return values are present in the codebase and can be found using the regular expression  \\.insert.*\\); . Recommendations Short term, investigate if any of the unguarded map insertions should be checked.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "26. Potential truncation on reading and writing Programs, Deployments, and Executions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "When writing a Program to bytes, the number of import statements and identiers are casted to an u8 integer, leading to the truncation of elements if there are more than 256 identiers: // Write the number of program imports. ( self .imports.len() as u8 ).write_le(& mut writer)?; // Write the program imports. for import in self .imports.values() { import.write_le(& mut writer)?; } // Write the number of components. ( self .identifiers.len() as u8 ).write_le(& mut writer)?; Figure 26.1: vm/compiler/src/program/bytes.rs#L73-L81 During Program parsing, this limit of 256 identiers is never enforced. Similarly, the Execution and Deployment write_le functions assume that there are fewer than u16::MAX transitions and verifying keys, respectively. // Write the number of transitions. ( self .transitions.len() as u16 ).write_le(& mut writer)?; Figure 26.2: vm/compiler/src/process/execution/bytes.rs#L52-L53 // Write the number of entries in the bundle. ( self .verifying_keys.len() as u16 ).write_le(& mut writer)?; Figure 26.3: vm/compiler/src/process/deployment/bytes.rs#L62-L63 Recommendations Short term, determine a maximum number of allowed import statements and identiers and enforce this bound on Program parsing. Then, guarantee that the integer type used in the write_le function includes this bound. Perform the same analysis for the Execution and Deployment functions.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "27. StatePath::verify accepts invalid states ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The StatePath::verify function attempts to validate several properties in the transaction using the code shown in gure 28.1. However, this code does not actually check that all checks are true; it checks only that there are an even number of false booleans. Since there are six booleans in the operation, the function will return true if all are false. // Ensure the block path is valid. let check_block_hash = A::hash_bhp1024(&block_hash_preimage).is_equal(& self .block_hash); // Ensure the state root is correct. let check_state_root = A::verify_merkle_path_bhp(& self .block_path, & self .state_root, & self .block_hash.to_bits_le()); check_transition_path .is_equal(&check_transaction_path) .is_equal(&check_transactions_path) .is_equal(&check_header_path) .is_equal(&check_block_hash) .is_equal(&check_state_root) } Figure 27.1: vm/compiler/src/ledger/state_path/circuit/verify.rs#L57-L70 We marked the severity as informational since the function is still not being used. Exploit Scenario An attacker submits a StatePath where no checks hold, but the verify function still returns true. Recommendations Short term, ensure that all checks are true (e.g., by conjuncting all booleans and checking that the resulting boolean is true).", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "28. Potential panic in encryption/decryption circuit generation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The decrypt_with_randomizers and encrypt_with_randomizers functions do not check the length of the randomizers argument against the length of the underlying ciphertext and plaintext, respectively. This can cause a panic in the zip_eq call. Existing calls to the function seem safe, but since it is a public function, the lengths of its underlying values should be checked to prevent panics in future code. pub ( crate ) fn decrypt_with_randomizers (& self , randomizers: & [Field<A>]) -> Plaintext <A> { // Decrypt the ciphertext. Plaintext::from_fields( & self .iter() .zip_eq(randomizers) Figure 28.1: circuit/program/src/data/ciphertext/decrypt.rs#L31-L36 Recommendations Short term, add a check to ensure that the length of the underlying plaintext/ciphertext matches the length of the randomizer values.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "29. Variable timing of certain cryptographic functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-09-aleosystems-snarkvm-securityreview.pdf", "body": "The Pedersen commitment code computes the masking element [r]h by ltering out powers of h not indicated by the randomizer r and adding the remaining values. However, the timing of this function leaks information about the randomizer value. In particular, it can reveal the Hamming weight (or approximate Hamming weight) of the randomizer. If the randomizer r is a 256-bit value, but timing indicates that the randomizer has a Hamming weight of 100 (for instance), then the possible set of randomizers has only about 2 243 elements. This compromises the information-theoretic security of the hiding property of the Pedersen commitment. randomizer.to_bits_le().iter().zip_eq(&* self .random_base_window).filter(|(bit, _)| **bit).for_each( |(_, base)| { output += base; }, ); Figure 29.1: console/algorithms/src/pedersen/commit_uncompressed.rs#L27-L33 Recommendations Short term, consider switching to a constant-time algorithm for computing the masking value.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "1. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "Sherlock has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-jscauses a security vulnerability in the Sherlock contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. Certain functions lack zero address checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "Certain functions fail to validate incoming arguments, so callers can accidentally set important state variables to the zero address. For example, the AaveV2Strategy contracts constructor function does not validate the aaveLmReceiver, which is the address that receives Aave rewards on calls to AaveV2Strategy.claimRewards. 39 40 41 42 43 44 45 46 47 } constructor(IAToken _aWant, address _aaveLmReceiver) { aWant = _aWant; // This gets the underlying token associated with aUSDC (USDC) want = IERC20(_aWant.UNDERLYING_ASSET_ADDRESS()); // Gets the specific rewards controller for this token type aaveIncentivesController = _aWant.getIncentivesController(); aaveLmReceiver = _aaveLmReceiver; Figure 2.1: managers/AaveV2Strategy.sol:39-47 If the aaveLmReceiver variable is set to the address zero, the Aave contract will revert with INVALID_TO_ADDRESS. This prevents any Aave rewards from being claimed for the designated token. The following functions are missing zero address checks:  Manager.setSherlockCoreAddress  AaveV2Strategy.sweep  SherDistributionManager.sweep  SherlockProtocolManager.sweep  Sherlock.constructor Exploit Scenario Bob deploys AaveV2Strategy with aaveLmReceiver set to the zero address. All calls to claimRewards revert. Recommendations Short term, add zero address checks on all function arguments to ensure that users cannot accidentally set incorrect values. Long term, use Slither, which will catch functions that do not have zero address checks.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. updateYieldStrategy could leave funds in the old strategy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The updateYieldStrategy function sets a new yield strategy manager contract without calling yieldStrategy.withdrawAll() on the old strategy, potentially leaving funds in it. 257 // Sets a new yield strategy manager contract 258 /// @notice Update yield strategy 259 /// @param _yieldStrategy News address of the strategy 260 /// @dev try a yieldStrategyWithdrawAll() on old, ignore failure 261 function updateYieldStrategy(IStrategyManager _yieldStrategy) external override onlyOwner { 262 263 264 265 266 yieldStrategy = _yieldStrategy; 267 } if (address(_yieldStrategy) == address(0)) revert ZeroArgument(); if (yieldStrategy == _yieldStrategy) revert InvalidArgument(); emit YieldStrategyUpdated(yieldStrategy, _yieldStrategy); Figure 3.1: contracts/Sherlock.sol:257-267 Even though one could re-add the old strategy to recover the funds, this issue could cause stakers and the protocols insured by Sherlock to lose trust in the system. This issue has a signicant impact on the result of totalTokenBalanceStakers, which is used when calculating the shares in initialStake. totalTokenBalanceStakers uses the balance of the yield strategy. If the balance is missing the funds that should have been withdrawn from a previous strategy, the result will be incorrect. return 151 function totalTokenBalanceStakers() public view override returns (uint256) { 152 153 token.balanceOf(address(this)) + 154 155 sherlockProtocolManager.claimablePremiums(); 156 } yieldStrategy.balanceOf() + Figure 3.2: contracts/Sherlock.sol:151-156 uint256 _amount, uint256 _period, address _receiver 483 function initialStake( 484 485 486 487 ) external override whenNotPaused returns (uint256 _id, uint256 _sher) { ... 501 502 stakeShares_ = (_amount * totalStakeShares_) / (totalTokenBalanceStakers() - _amount); 503 // If this is the first stake ever, we just mint stake shares equal to the amount of USDC staked 504 else stakeShares_ = _amount; if (totalStakeShares_ != 0) Figure 3.3: contracts/Sherlock.sol:483-504 Exploit Scenario Bob, the owner of the Sherlock contract, calls updateYieldStrategy with a new strategy. Eve calls initialStake and receives more shares than she is due because totalTokenBalanceStakers returns a signicantly lower balance than it should. Bob notices the missing funds, calls updateYieldStrategy with the old strategy and then yieldStrategy.WithdrawAll to recover the funds, and switches back to the new strategy. Eves shares now have notably more value. Recommendations Short term, in updateYieldStrategy, add a call to yieldStrategy.withdrawAll() on the old strategy. Long term, when designing systems that store funds, use extensive unit testing and property-based testing to ensure that funds cannot become stuck.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Pausing and unpausing the system may not be possible when removing or replacing connected contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The Sherlock contract allows all of the connected contracts to be paused or unpaused at the same time. However, if the sherDistributionManager contract is removed, or if any of the connected contracts are replaced when the system is paused, it might not be possible to pause or unpause the system. function removeSherDistributionManager() external override onlyOwner { if (address(sherDistributionManager) == address(0)) revert InvalidConditions(); emit SherDistributionManagerUpdated( sherDistributionManager, ISherDistributionManager(address(0)) ); delete sherDistributionManager; 206 207 208 209 210 211 212 213 214 } Figure 4.1: contracts/Sherlock.sol:206-214 Of all the connected contracts, the only one that can be removed is the sherDistributionManager contract. On the other hand, all of the connected contracts can be replaced through an update function. function pause() external onlyOwner { _pause(); yieldStrategy.pause(); sherDistributionManager.pause(); sherlockProtocolManager.pause(); sherlockClaimManager.pause(); 302 303 304 305 306 307 308 } 309 310 311 /// @notice Unpause external functions in all contracts function unpause() external onlyOwner { 312 313 314 315 316 317 } _unpause(); yieldStrategy.unpause(); sherDistributionManager.unpause(); sherlockProtocolManager.unpause(); sherlockClaimManager.unpause(); Figure 4.2: contracts/Sherlock.sol:302-317 If the sherDistributionManager contract is removed, a call to Sherlock.pause will revert, as it is attempting to call the zero address. If sherDistributionManager is removed while the system is paused, then a call to Sherlock.unpause will revert for the same reason. If any of the contracts is replaced while the system is paused, the replaced contract will be in an unpaused state while the other contracts are still paused. As a result, a call to Sherlock.unpause will revert, as it is attempting to unpause an already unpaused contract. Exploit Scenario Bob, the owner of the Sherlock contract, pauses the system to replace the sherlockProtocolManager contract, which contains a bug. Bob deploys a new sherlockProtocolManager contract and calls updateSherlockProtocolManager to set the new address in the Sherlock contract. To unpause the system, Bob calls Sherlock.unpause, which reverts because sherlockProtocolManager is already unpaused. Recommendations Short term, add conditional checks to the Sherlock.pause and Sherlock.unpause functions to check that a contract is either paused or unpaused, as expected, before attempting to update its state. For sherDistributionManager, the check should verify that the contract to be paused or unpaused is not the zero address. Long term, for pieces of code that depend on the states of multiple contracts, implement unit tests that cover each possible combination of contract states.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. SHER reward calculation uses confusing six-decimal SHER reward rate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The reward calculation in calcReward uses a six-decimal SHER reward rate value. This might confuse readers and developers of the contracts because the SHER token has 18 decimals, and the calculated reward will also have 18 decimals. Also, this value does not allow the SHER reward rate to be set below 0.000001000000000000 SHER. function calcReward( 89 90 91 92 93 ) public view override returns (uint256 _sher) { uint256 _tvl, uint256 _amount, uint256 _period 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 [..] // If there are some max rewards available... if (maxRewardsAvailable != 0) { // And if the entire stake is still within the maxRewardsAvailable amount if (_amount <= maxRewardsAvailable) { // Then the entire stake amount should accrue max SHER rewards return (_amount * maxRewardsRate * _period) * DECIMALS; } else { // Otherwise, the stake takes all the maxRewardsAvailable left  // We add the maxRewardsAvailable amount to the TVL (now _tvl _tvl += maxRewardsAvailable; // We subtract the amount of the stake that received max rewards _amount -= maxRewardsAvailable; // We accrue the max rewards available at the max rewards  // This could be: $20M of maxRewardsAvailable which gets  // Calculation continues after this _sher += (maxRewardsAvailable * maxRewardsRate * _period) * DECIMALS; } } // If there are SHER rewards still available  if (slopeRewardsAvailable != 0) { _sher += (((zeroRewardsStartTVL - position) * _amount * maxRewardsRate * _period) / (zeroRewardsStartTVL - maxRewardsEndTVL)) * DECIMALS; 144 145 146 147 148 149 } } Figure 5.1: contracts/managers/SherDistributionManager.sol:89-149 In the reward calculation, the 6-decimal maxRewardsRate is rst multiplied by _amount and _period, resulting in a 12-decimal intermediate product. To output a nal 18-decimal product, this 12-decimal product is multiplied by DECIMALS to add 6 decimals. Although this leads to a correct result, it would be clearer to use an 18-decimal value for maxRewardsRate and to divide by DECIMALS at the end of the calculation. // using 6 decimal maxRewardsRate (10e6 * 1e6 * 10) * 1e6 = 100e18 = 100 SHER // using 18 decimal maxRewardsRate (10e6 * 1e18 * 10) / 1e6 = 100e18 = 100 SHER Figure 5.2: Comparison of a 6-decimal and an 18-decimal maxRewardsRate Exploit Scenario Bob, a developer of the Sherlock protocol, writes a new version of the SherDistributionManager contract that changes the reward calculation. He mistakenly assumes that the SHER maxRewardsRate has 18 decimals and updates the calculation incorrectly. As a result, the newly calculated reward is incorrect. Recommendations Short term, use an 18-decimal value for maxRewardsRate and divide by DECIMALS instead of multiplying. Long term, when implementing calculations that use the rate of a given token, strive to use a rate variable with the same number of decimals as the token. This will prevent any confusion with regard to decimals, which might lead to introducing precision bugs when updating the contracts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. A claim cannot be paid out or escalated if the protocol agent changes after the claim has been initialized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The escalate and payoutClaim functions can be called only by the protocol agent that started the claim. Therefore, if the protocol agent role is reassigned after a claim is started, the new protocol agent will be unable to call these functions and complete the claim. function escalate(uint256 _claimID, uint256 _amount) external override nonReentrant whenNotPaused 388 389 390 391 392 393 { 394 395 396 397 398 399 400 401 402 403 if (_amount < BOND) revert InvalidArgument(); // Gets the internal ID of the claim bytes32 claimIdentifier = publicToInternalID[_claimID]; if (claimIdentifier == bytes32(0)) revert InvalidArgument(); // Retrieves the claim struct Claim storage claim = claims_[claimIdentifier]; // Requires the caller to be the protocol agent if (msg.sender != claim.initiator) revert InvalidSender(); Figure 6.1: contracts/managers/SherlockClaimManager.sol:388-403 Due to this scheme, care should be taken when updating the protocol agent. That is, the protocol agent should not be reassigned if there is an existing claim. However, if the protocol agent is changed when there is an existing claim, the protocol agent role could be transferred back to the original protocol agent to complete the claim. Exploit Scenario Alice is the protocol agent and starts a claim. Alice transfers the protocol agent role to Bob. The claim is approved by SPCC and can be paid out. Bob calls payoutClaim, but the transaction reverts. Recommendations Short term, update the comment in the escalate and payoutClaim functions to state that the caller needs to be the protocol agent that started the claim, and clearly describe this requirement in the protocol agent documentation. Alternatively, update the check to verify that msg.sender is the current protocol agent rather than specically the protocol agent who initiated the claim. Long term, review and document the eects of the reassignment of privileged roles on the systems state transitions. Such a review will help uncover cases in which the reassignment of privileged roles causes issues and possibly a denial of service to (part of) the system.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "7. Missing input validation in setMinActiveBalance could cause a confusing event to be emitted ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The setMinActiveBalance functions input validation is incomplete: it should check that the minActiveBalance has not been set to its existing value, but this check is missing. Additionally, if the minActiveBalance is set to its existing value, the emitted MinBalance event will indicate that the old and new values are identical. This could confuse systems monitoring the contract that expect this event to be emitted only when the minActiveBalance changes. function setMinActiveBalance(uint256 _minActiveBalance) external override onlyOwner { // Can't set a value that is too high to be reasonable require(_minActiveBalance < MIN_BALANCE_SANITY_CEILING, 'INSANE'); emit MinBalance(minActiveBalance, _minActiveBalance); minActiveBalance = _minActiveBalance; 422 423 424 425 426 427 428 } Figure 7.1: contracts/managers/SherlockProtocolManager.sol:422-428 Exploit Scenario An o-chain monitoring system controlled by the Sherlock protocol is listening for events that indicate that a contract conguration value has changed. When such events are detected, the monitoring system sends an email to the admins of the Sherlock protocol. Alice, a contract owner, calls setMinActiveBalance with the existing minActiveBalance as input. The o-chain monitoring system detects the emitted event and noties the Sherlock protocol admins. The Sherlock protocol admins are confused since the value did not change. Recommendations Short term, add input validation that causes setMinActiveBalance to revert if the proposed minActiveBalance value equals the current value. Long term, document and test the expected behavior of all the systems events. Consider using a blockchain-monitoring system to track any suspicious behavior in the contracts.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. payoutClaims calling of external contracts in a loop could cause a denial of service ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "The payoutClaim function uses a loop to call the PreCorePayoutCallback function on a list of external contracts. If any of these calls reverts, the entire payoutClaim function reverts, and, hence, the transaction reverts. This may not be the desired behavior; if that is the case, a denial of service would prevent claims from being paid out. for (uint256 i; i < claimCallbacks.length; i++) { claimCallbacks[i].PreCorePayoutCallback(protocol, _claimID, amount); 499 500 501 } Figure 8.1: contracts/managers/SherlockClaimManager.sol:499-501 The owner of the SherlockClaimManager contract controls the list of contracts on which the PreCorePayoutCallback function is called. The owner can add or remove contracts from this list at any time. Therefore, if a contract is causing unexpected reverts, the owner can x the problem by (temporarily) removing that contract from the list. It might be expected that some of these calls revert and cause the entire transaction to revert. However, the external contracts that will be called and the expected behavior in the event of a revert are currently unknown. If a revert should not cause the entire transaction to revert, the current implementation does not fulll that requirement. To accommodate both casesa revert of an external call reverts the entire transaction or allows the transaction to continue a middle road can be taken. For each contract in the list, a boolean could indicate whether the transaction should revert or continue if the external call fails. If the boolean indicates that the transaction should continue, an emitted event would indicate the contract address and the input arguments of the callback that reverted. This would allow the system to continue functioning while admins investigate the cause of the revert and x the issue(s) if needed. Exploit Scenario Alice, the owner of the SherlockClaimManager contract, registers contract A in the list of contracts on which PreCorePayoutCallback is called. Contract A contains a bug that causes the callback to revert every time. Bob, a protocol agent, successfully les a claim and calls payoutClaim. The transaction reverts because the call to contract A reverts. Recommendations Short term, review the requirements of contracts that will be called by callback functions, and adjust the implementation to fulll those requirements. Long term, when designing a system reliant on external components that have not yet been determined, carefully consider whether to include those integrations during the development process or to wait until those components have been identied. This will prevent unforeseen problems due to incomplete or incorrect integrations with unknown contracts.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "9. pullReward could silently fail and cause stakers to lose all earned SHER rewards ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/Sherlockv2.pdf", "body": "If the SherDistributionManager.pullReward function reverts, the calling function (_stake) will not set the SHER rewards in the stakers position NFT. As a result, the staker will not receive the payout of SHER rewards after the stake period has passed. // Sets the timestamp at which this position can first be unstaked/restaked lockupEnd_[_id] = block.timestamp + _period; if (address(sherDistributionManager) == address(0)) return 0; // Does not allow restaking of 0 tokens if (_amount == 0) return 0; // Checks this amount of SHER tokens in this contract before we transfer new ones uint256 before = sher.balanceOf(address(this)); // pullReward() calcs then actually transfers the SHER tokens to this contract try sherDistributionManager.pullReward(_amount, _period, _id, _receiver) returns ( function _stake( uint256 _amount, uint256 _period, uint256 _id, address _receiver 354 355 356 357 358 359 ) internal returns (uint256 _sher) { 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 } catch (bytes memory reason) { _sher = amount; uint256 amount ) { } // If for whatever reason the sherDistributionManager call fails emit SherRewardsError(reason); return 0; // actualAmount should represent the amount of SHER tokens transferred to this contract for the current stake position 382 383 384 385 386 } uint256 actualAmount = sher.balanceOf(address(this)) - before; if (actualAmount != _sher) revert InvalidSherAmount(_sher, actualAmount); // Assigns the newly created SHER tokens to the current stake position sherRewards_[_id] = _sher; Figure 9.1: contracts/Sherlock.sol:354-386 When the pullReward call reverts, the SherRewardsError event is emitted. The staker could check this event and see that no SHER rewards were set. The staker could also call the sherRewards function and provide the positions NFT ID to check whether the SHER rewards were set. However, stakers should not be expected to make these checks after every (re)stake. There are two ways in which the pullReward function can fail. First, a bug in the arithmetic could cause an overow and revert the function. Second, if the SherDistributionManager contract does not hold enough SHER to be able to transfer the calculated amount, the pullReward function will fail. The SHER balance of the contract needs to be manually topped up. If a staker detects that no SHER was set for her (re)stake, she may want to cancel the stake. However, stakers are not able to cancel a stake until the stakes period has passed (currently, at least three months). Exploit Scenario Alice creates a new stake, but the SherDistributionManager contract does not hold enough SHER to transfer the rewards, and the transaction reverts. The execution continues and sets Alices stake allocation to zero. Recommendations Short term, have the system revert transactions if pullReward reverts. Long term, have the system revert transactions if part of the expected rewards are not allocated due to an internal revert. This will prevent situations in which certain users get rewards while others do not.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "1. Lack of validation of signed dealing against original dealing ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf", "body": "The EcdsaPreSignerImpl::validate_dealing_support method does not check that the content of signed dealings matches the original dealings. A malicious receiver could exploit this lack of validation by changing the requested height (that is, the support.content.requested_height eld) or internal data (the support.content.idk_dealing.internal_dealing_raw eld) before signing the ECDSA dealing. The resulting signature would not be agged as invalid by the validate_dealing_support method but would result in an invalid aggregated signature. After all nodes sign a dealing, the EcdsaTranscriptBuilderImpl::build_transcript method checks the signed dealings content hashes before attempting to aggregate all the dealing support signatures to produce the nal aggregated signature. The method logs a warning when the hashes do not agree, but does not otherwise act on signed dealings with dierent content. let mut content_hash = BTreeSet::new(); for share in &support_shares { content_hash.insert(ic_crypto::crypto_hash(&share.content)); } if content_hash.len() > 1 { warn!( self.log, \"Unexpected multi share content: support_shares = {}, content_hash = {}\", support_shares.len(), content_hash.len() ); self.metrics.payload_errors_inc(\"invalid_content_hash\"); } if let Some(multi_sig) = self.crypto_aggregate_dealing_support( transcript_state.transcript_params, &support_shares, ) { } transcript_state.add_completed_dealing(signed_dealing.content, multi_sig); Figure 1.1: ic/rs/consensus/src/ecdsa/pre_signer.rs:1015-1034 The dealing content is added to the set of completed dealings along with the aggregated signature. When the node attempts to create a new transcript from the dealing, the aggregated signature is checked by IDkgProtocol::create_transcript. If a malicious receiver changes the content of a dealing before signing it, the resulting invalid aggregated signature would be rejected by this method. In such a case, the EcdsaTranscriptBuilderImpl methods build_transcript and get_completed_transcript would return None for the corresponding transcript ID. That is, neither the transcript nor the corresponding quadruple would be completed. Additionally, since signing requests are deterministically matched against quadruples, including quadruples that are not yet available, this issue could allow a single node to block the service of individual signing requests. pub(crate) fn get_signing_requests<'a>( ecdsa_payload: &ecdsa::EcdsaPayload, sign_with_ecdsa_contexts: &'a BTreeMap<CallbackId, SignWithEcdsaContext>, ) -> BTreeMap<ecdsa::RequestId, &'a SignWithEcdsaContext> { let known_random_ids: BTreeSet<[u8; 32]> = ecdsa_payload .iter_request_ids() .map(|id| id.pseudo_random_id) .collect::<BTreeSet<_>>(); let mut unassigned_quadruple_ids = ecdsa_payload.unassigned_quadruple_ids().collect::<Vec<_>>(); // sort in reverse order (bigger to smaller). unassigned_quadruple_ids.sort_by(|a, b| b.cmp(a)); let mut new_requests = BTreeMap::new(); // The following iteration goes through contexts in the order // of their keys, which is the callback_id. Therefore we are // traversing the requests in the order they were created. for context in sign_with_ecdsa_contexts.values() { if known_random_ids.contains(context.pseudo_random_id.as_slice()) { continue; }; if let Some(quadruple_id) = unassigned_quadruple_ids.pop() { let request_id = ecdsa::RequestId { quadruple_id, pseudo_random_id: context.pseudo_random_id, }; new_requests.insert(request_id, context); } else { break; } } new_requests } Figure 1.2: ic/rs/consensus/src/ecdsa/payload_builder.rs:752-782 Exploit Scenario A malicious node wants to prevent the signing request SRi from completing. Assume that the corresponding quadruple, Qi, is not yet available. The node waits until it receives a dealing corresponding to quadruple Qi. It generates a support message for the dealing, but before signing the dealing, the malicious node changes the dealing.idk_dealing.internal_dealing_raw eld. The signature is valid for the updated dealing but not for the original dealing. The malicious dealing support is gossiped to the other nodes in the network. Since the signature on the dealing support is correct, all nodes move the dealing support to the validated pool. However, when the dealing support signatures are aggregated by the other nodes, the aggregated signature is rejected as invalid, and no new transcript is created for the dealing. This means that the quadruple Qi never completes. Since the matching of signing requests to quadruples is deterministic, SRi is matched with Qi every time a new ECDSA payload is created. Thus, SRi is never serviced. Recommendations Short term, add validation code in EcdsaPreSignerImpl::validate_dealing_support to verify that a signed dealings content hash is identical to the hash of the original dealing. Long term, consider whether the BLS multisignature aggregation APIs need to be better documented to ensure that API consumers verify that all individual signatures are over the same message.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "2. The ECDSA payload is not updated if a quadruple fails to complete ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf", "body": "If a transcript fails to complete (as described in TOB-DFTECDSA-1), the corresponding quadruple, Qi, will also fail to complete. This means that the quadruple ID for Qi will remain in the quadruples_in_creation set until the key is reshared and the set is purged. (Currently, the key is reshared if a node joins or leaves the subnet, which is an uncommon occurrence.) Moreover, if a transcript and the corresponding Qi fail to complete, so will the corresponding signing request, SRi, as it is matched deterministically with Qi. let ecdsa_payload = ecdsa::EcdsaPayload { signature_agreements: ecdsa_payload.signature_agreements.clone(), ongoing_signatures: ecdsa_payload.ongoing_signatures.clone(), available_quadruples: if is_new_key_transcript { BTreeMap::new() } else { ecdsa_payload.available_quadruples.clone() }, quadruples_in_creation: if is_new_key_transcript { BTreeMap::new() } else { ecdsa_payload.quadruples_in_creation.clone() }, uid_generator: ecdsa_payload.uid_generator.clone(), idkg_transcripts: BTreeMap::new(), ongoing_xnet_reshares: if is_new_key_transcript { // This will clear the current ongoing reshares, and // the execution requests will be restarted with the // new key and different transcript IDs. BTreeMap::new() } else { ecdsa_payload.ongoing_xnet_reshares.clone() }, xnet_reshare_agreements: ecdsa_payload.xnet_reshare_agreements.clone(), }; Figure 2.1: The quadruples_in_creation set will be purged only when the key is reshared. The canister will never be notied that the signing request failed and will be left waiting indenitely for the corresponding reply from the distributed signing service. Recommendations Short term, revise the code so that if a transcript (permanently) fails to complete, the quadruple ID and corresponding transcripts are dropped from the ECDSA payload. To ensure that a malicious node cannot inuence how signing requests are matched with quadruples, revise the code so that it noties the canister that the signing request failed.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "3. Malicious canisters can exhaust the number of available quadruples ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf", "body": "By requesting a large number of signatures, a canister (or set of canisters) could exhaust the number of available quadruples, preventing other signature requests from completing in a timely manner. The ECDSA payload builder defaults to creating one extra quadruple in create_data_payload if there is no ECDSA conguration for the subnet in the registry. let ecdsa_config = registry_client .get_ecdsa_config(subnet_id, summary_registry_version)? .unwrap_or(EcdsaConfig { quadruples_to_create_in_advance: 1, // default value ..EcdsaConfig::default() }); Figure 3.1: ic/rs/consensus/src/ecdsa/payload_builder.rs:400-405 Signing requests are serviced by the system in the order in which they are made (as determined by their CallbackID values). If a canister (or set of canisters) makes a large number of signing requests, the system would be overwhelmed and would take a long time to recover. This issue is partly mitigated by the fee that is charged for signing requests. However, we believe that the nancial ramications of this problem could outweigh the fees paid by attackers. For example, the type of denial-of-service attack described in this nding could be devastating for a DeFi application that is sensitive to small price uctuations in the Bitcoin market. Since the ECDSA threshold signature service is not yet deployed on the Internet Computer, it is unclear how the service will be used in practice, making the severity of this issue dicult to determine. Therefore, the severity of this issue is marked as undetermined. Exploit Scenario A malicious canister learns that another canister on the Internet Computer is about to request a time-sensitive signature on a message. The malicious canister immediately requests a large number of signatures from the signing service, exhausting the number of available quadruples and preventing the original signature from completing in a timely manner. Recommendations One possible mitigation is to increase the number of quadruples that the system creates in advance, making it more expensive for an attacker to carry out a denial-of-service attack on the ECDSA signing service. Another possibility is to run multiple signing services on multiple subnets of the Internet Computer. This would have the added benet of protecting the system from resource exhaustion related to cross-network bandwidth limitations. However, both of these solutions scale only linearly with the number of added quadruples/subnets. Another potential mitigation is to introduce a dynamic fee or stake based on the number of outstanding signing requests. In the case of a dynamic fee, the canister would pay a set number of tokens proportional to the number of outstanding signing requests whenever it requests a new signature from the service. In the case of a stake-based system, the canister would stake funds proportional to the number of outstanding requests but would recover those funds once the signing request completed. As any signing service that depends on consensus will have limited throughput compared to a centralized service, this issue is dicult to mitigate completely. However, it is important that canister developers are aware of the limits of the implementation. Therefore, regardless of the mitigations imposed, we recommend that the DFINITY team clearly document the limits of the current implementation.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "4. Aggregated signatures are dropped if their request IDs are not recognized ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/DFINITYThresholdECDSAandBtcCanisters.pdf", "body": "The update_signature_agreements function populates the set of completed signatures in the ECDSA payload. The function aggregates the completed signatures from the ECDSA pool by calling EcdsaSignatureBuilderImpl::get_completed_signatures. However, if a signatures associated signing request ID is not in the set of ongoing signatures, update_signature_agreements simply drops the signature. for (request_id, signature) in builder.get_completed_signatures( chain, ecdsa_pool.deref() ) { if payload.ongoing_signatures.remove(&request_id).is_none() { warn!( log, \"ECDSA signing request {:?} is not found in payload but we have a signature for it\", request_id ); } else { payload .signature_agreements .insert(request_id, ecdsa::CompletedSignature::Unreported(signature)); } } Figure 4.1: ic/rs/consensus/src/ecdsa/payload_builder.rs:817-830 Barring an implementation error, this should not happen under normal circumstances. Recommendations Short term, consider adding the signature to the set of completed signatures on the next ECDSA payload. This will ensure that all outstanding signing requests are completed. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: N/A"]}, {"title": "1. Canceling all transaction requests causes DoS on MMF system ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Any shareholder can cancel any transaction request, which can result in a denial of service (DoS) from the MMF system. The TransactionalModule contract uses transaction requests to store buy and sell orders from users. These requests are settled at the end of the day by the admins. Admins can create or cancel a request for any user. Users can create requests for themselves and cancel their own requests. The TransferAgentGateway contract is an entry point for all user and admin actions. It implements access control checks and forwards the calls to their respective modules. The cancelRequest function in the TransferAgentGateway contract checks that the caller is the owner or a shareholder. However, if the caller is not the owner, the caller is not matched against the account argument. This allows any shareholder to call the cancelRequest function in the TransactionalModule for any account and requestId . function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override { require ( msg.sender == owner() || IAuthorization( moduleRegistry.getModuleAddress(AUTHORIZATION_MODULE) ).isAccountAuthorized( msg.sender ), \"OPERATION_NOT_ALLOWED_FOR_CALLER\" ); ICancellableTransaction( moduleRegistry.getModuleAddress(TRANSACTIONAL_MODULE) ).cancelRequest(account, requestId, memo); } Figure 1.1: The cancelRequest function in the TransferAgentGateway contract As shown in gure 1.2, the if condition in the cancelRequest function in the TransactionalModule contract implements a check that does not allow shareholders to cancel transaction requests created by the admin. However, this check passes because the TransferAgentGateway contract is set up as the admin account in the authorization module. function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override onlyAdmin onlyShareholder(account) { require ( transactionDetailMap[requestId].txType > ITransactionStorage.TransactionType.INVALID, \"INVALID_TRANSACTION_ID\" ); if (!transactionDetailMap[requestId].selfService) { require ( IAuthorization(modules.getModuleAddress(AUTHORIZATION_MODULE)) .isAdminAccount( msg.sender ), \"CALLER_IS_NOT_AN_ADMIN\" ); } require ( pendingTransactionsMap[account].remove(requestId), \"INVALID_TRANSACTION_ID\" ); delete transactionDetailMap[requestId]; accountsWithTransactions.remove(account); emit TransactionCancelled(account, requestId, memo); } Figure 1.2: The cancelRequest function in the TransactionalModule contract Thus, a shareholder can cancel any transaction request created by anyone. Exploit Scenario Eve becomes an authorized shareholder and sets up a bot to listen to the TransactionSubmitted event on the TransactionalModule contract. The bot calls the cancelRequest function on the TransferAgentGateway contract for every event and cancels all the transaction requests before they are settled, thus executing a DoS attack on the MMF system. Recommendations Short term, add a check in the TransferAgentGateway contract to allow shareholders to cancel requests only for their own accounts. Long term, document access control rules in a publicly accessible location. These rules should encompass admin, non-admin, and common functions. Ensure the code adheres to that specication by extending unit test coverage for positive and negative expectations within the system. Add fuzz tests where access control rules are the invariants under test.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Lack of validation in the IntentValidationModule contract can lead to inconsistent state ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Lack of validation in the state-modifying functions of the IntentValidationModule contract can cause users to be locked out of the system. As shown in gure 2.1, the setDeviceKey function in IntentValidationModule allows adding a device ID and key to multiple accounts, which may result in the unauthorized use of a device ID. function setDeviceKey ( address account , uint256 deviceId , string memory key ) external override onlyAdmin { devicesMap[account].add(deviceId); deviceKeyMap[deviceId] = key; emit DeviceKeyAdded(account, deviceId); } Figure 2.1: The setDeviceKey functions in the IntentValidationModule contract Additionally, a lack of validation in the clearDeviceKey and clearAccountKeys functions can cause the key for a device ID to become zero, which may prevent users from authenticating their requests. function clearDeviceKey ( address account , uint256 deviceId ) external override onlyAdmin { _removeDeviceKey(account, deviceId); } function clearAccountKeys ( address account ) external override onlyAdmin { uint256 [] memory devices = devicesMap[account].values(); for ( uint i = 0 ; i < devices.length; ) { _removeDeviceKey(account, devices[i]); unchecked { i++; } } } Figure 2.2: Functions to clear device ID and key in the IntentValidationModule contract The account-todevice ID mapping and device IDto-key mapping are used to authenticate user actions in an o-chain component, which can malfunction in the presence of these inconsistent states and lead to the authentication of malicious user actions. Exploit Scenario An admin adds the DEV_A device and the KEY_K key to Bob. Then there are multiple scenarios to cause an inconsistent state, such as the following: Adding one device to multiple accounts: 1. An admin adds the DEV_A device and the KEY_K key to Alice by mistake. 2. Alice can use Bobs device to send unauthorized requests. Overwriting a key for a device ID: 1. An admin adds the DEV_A device and the KEY_L key to Alice, which overwrites the key for the DEV_A device from KEY_K to KEY_L . 2. Bob cannot authenticate his requests with his KEY_K key. Setting a key to zero for a device ID: 1. An admin adds the DEV_A device and the KEY_K key to Alice by mistake. 2. An admin removes the DEV_A device from Alices account. This sets the key for the DEV_A device to zero, which is still added to Bobs account. 3. Bob cannot authenticate his requests with his KEY_K key. Recommendations Short term, make the following changes:   Add a check in the setDeviceKey function to ensure that a device is not added to multiple accounts. Add a new function to update the key of an already added device with correct validation checks for the update. Long term, document valid system states and the state transitions allowed from each state. Ensure proper data validation checks are added in all state-modifying functions with unit and fuzzing tests.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "3. Pending transactions cannot be settled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "An account removed from the accountsWithTransactions state variable will have its pending transactions stuck in the system, resulting in an opportunity cost loss for the users. The accountsWithTransactions state variable in the TransactionalModule contract is used to keep track of accounts with pending transactions. It is used in the following functions:   The getAccountsWithTransactions function to return the list of accounts with pending transactions The hasTransactions function to check if an account has pending transactions. However, the cancelRequest function in the TransactionalModule contract removes the account from the accountsWithTransactions list for every cancellation. If an account has multiple pending transactions, canceling only one of the transaction requests will remove the account from the accountsWithTransactions list. function cancelRequest ( address account , bytes32 requestId , string calldata memo ) external override onlyAdmin onlyShareholder(account) { require ( transactionDetailMap[requestId].txType > ITransactionStorage.TransactionType.INVALID, \"INVALID_TRANSACTION_ID\" ); if (!transactionDetailMap[requestId].selfService) { require ( IAuthorization(modules.getModuleAddress(AUTHORIZATION_MODULE)) .isAdminAccount( msg.sender ), \"CALLER_IS_NOT_AN_ADMIN\" ); } require ( pendingTransactionsMap[account].remove(requestId), \"INVALID_TRANSACTION_ID\" ); delete transactionDetailMap[requestId]; accountsWithTransactions.remove(account); emit TransactionCancelled(account, requestId, memo); } Figure 3.1: The cancelRequest function in the TransactionalModule contract In gure 3.1, the account has pending transactions, but it is not present in the accountsWithTransactions list. The o-chain components and other functionality relying on the getAccountsWithTransactions and hasTransactions functions will see these accounts as not having any pending transactions. This may result in non-settlement of the pending transactions for these accounts, leading to a loss for the users. Exploit Scenario Alice, a shareholder, creates multiple transaction requests and cancels the last request. For the next settlement process, the o-chain component calls the getAccountsWithTransactions function to get the list of accounts with pending transactions and settles these accounts. After the settlement run, Alice checks her balance and is surprised that her transaction requests are not settled. She loses prots from upcoming market movements. Recommendations Short term, have the code use the unlistFromAccountsWithPendingTransactions function in the cancelRequest function to update the accountsWithTransactions list. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Deauthorized accounts can keep shares of the MMF ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "An unauthorized account can keep shares if the admin deauthorizes the shareholder without zeroing their balance. This can lead to legal issues because unauthorized users can keep shares of the MMF. The deauthorizeAccount function in the AuthorizationModule contract does not check that the balance of the provided account is zero before revoking the ROLE_FUND_AUTHORIZED role: function deauthorizeAccount ( address account ) external override onlyRole(ROLE_AUTHORIZATION_ADMIN) { require (account != address ( 0 ), \"INVALID_ADDRESS\" ); address txModule = modules.getModuleAddress( keccak256 ( \"MODULE_TRANSACTIONAL\" ) ); require (txModule != address ( 0 ), \"MODULE_REQUIRED_NOT_FOUND\" ); require ( hasRole(ROLE_FUND_AUTHORIZED, account), \"SHAREHOLDER_DOES_NOT_EXISTS\" ); require ( !ITransactionStorage(txModule).hasTransactions(account), \"PENDING_TRANSACTIONS_EXIST\" ); _revokeRole(ROLE_FUND_AUTHORIZED, account); emit AccountDeauthorized(account); } Figure 4.1: The deauthorizeAccount function in the AuthorizationModule contract If an admin account deauthorizes a shareholder account without making the balance zero, the unauthorized account will keep the shares of the MMF. The impact is limited, however, because the unauthorized account will not be able to liquidate the shares. The admin can also adjust the balance of the account to make it zero. However, if the admin forgets to adjust the balance or is unable to adjust the balance, it can lead to an unauthorized account holding shares of the MMF. Recommendations Short term, add a check in the deauthorizeAccount function to ensure that the balance of the provided account is zero. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions. Add fuzz tests where the rules enforced by those validation checks are the invariants under test.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "5. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The MMF has enabled optional compiler optimizations in Solidity. According to a November 2018 audit of the Solidity compiler , the optional optimizations may not be safe . optimizer: { enabled: true , runs: 200 , }, Figure 5.1: Hardhat optimizer enabled in hardhat.config.js Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild use them. Therefore, it is unclear how well they are being tested and exercised. Moreover, optimizations are actively being developed . High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018; the x for this bug was not reported in the Solidity changelog. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of Keccak-256 was reported. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the MMF contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "6. Project dependencies contain vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Although dependency scans did not identify a direct threat to the project codebase, npm audit found dependencies with known vulnerabilities. Due to the sensitivity of the deployment code and its environment, it is important to ensure dependencies are not malicious. Problems with dependencies in the JavaScript community could have a signicant eect on the MMF system. The output detailing the identied issues has been included below: Dependency Version ID", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "7. Unimplemented getVersion function returns default value of zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The getVersion function within the TransferAgentModule contract is not implemented; at present, it yields the default uint8 value of zero. function getVersion() external pure virtual override returns ( uint8 ) {} Figure 7.1: Unimplemented getVersion function in the TransferAgentModule contract The other module contracts establish a pattern where the getVersion function is supposed to return a value of one. function getVersion() external pure virtual override returns ( uint8 ) { return 1; } Figure 7.2: Implemented getVersion function in the TransactionalModule contract Exploit Scenario Alice calls the getVersion function on the TransferAgentModule contract. It returns zero, and all the other module contracts return one. Alice misunderstands the system and which contracts are on what version of their lifecycle. Recommendations Short term, implement the getVersion function in the TransferAgentModule contract so it matches the specication established in the other modules. Long term, use the Slither static analyzer to catch common issues such as this one. Implement slither-action into the projects CI pipeline.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "8. The MultiSigGenVerier threshold can be passed with a single signature ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "A single signature can be used multiple times to pass the threshold in the MultiSigGenVerifier contract, allowing a single signer to take full control of the system. The signedDataExecution function in the MultiSigGenVerifier contract veries provided signatures and accumulates the acquiredThreshold value in a loop as shown in gure 8.1: for ( uint256 i = 0 ; i < signaturesCount; i++) { (v, r, s) = _splitSignature(signatures, i); address signerRecovered = ecrecover( hash , v, r, s); if (signersSet.contains(signerRecovered)) { acquiredThreshold += signersMap[signerRecovered]; } } Figure 8.1: The signer recovery section of the signedDataExecution function in the MultiSigGenVerifier contract This code checks whether the recovered signer address is one of the previously added signers and adds the signers weight to acquiredThreshold . However, the code does not check that all the recorded signers are unique, which allows the submitter to pass the threshold with only a single signature to execute the signed transaction. The current function has an implicit zero-address check in the subsequent if statementto add new signers, they must not be address(0) . If this logic changes in the future, the impact of the ecrecover function returning address(0) (which happens when a signature is malformed) must be carefully reviewed. Exploit Scenario Eve, a signer, colludes with a submitter to settle their transactions at a favorable date and price. Eve signs the transaction and provides it to the submitter. The submitter uses this signature to call the signedDataExecution function by repeating the same signature multiple times in the signatures argument array to pass the threshold. Using this method, Eve can execute any admin transaction without consent from other admins. Recommendations Short term, have the code verify that the signatures provided to the signedDataExecution function are unique. One way of doing this is to sort the signatures in increasing order of the signer addresses and verify this order in the loop. An example of this order verication code is shown in gure 8.2: address lastSigner = address(0); for ( uint256 i = 0 ; i < signaturesCount; i++) { (v, r, s) = _splitSignature(signatures, i); address signerRecovered = ecrecover( hash , v, r, s); require (lastSigner < signerRecovered); lastSigner = signerRecovered; if (signersSet.contains(signerRecovered)) { acquiredThreshold += signersMap[signerRecovered]; } } Figure 8.2: An example code to verify uniqueness of the provided signatures Long term, expand unit test coverage to account for common edge cases, and carefully consider all possible values for any user-provided inputs. Implement fuzz testing to explore complex scenarios and nd dicult-to-detect bugs in functions with user-provided inputs.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "9. Shareholders can renounce their authorization role ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Shareholders can renounce their authorization role. As a result, system contracts that check for authorization and o-chain components may not work as expected because of an inconsistent system state. The AuthorizationModule contract extends the AccessControlUpgradeable contract from the OpenZeppelin library. The AccessControlUpgradeable contract has a public renounceRole function, which can be called by anyone to revoke a role on their own account. function renounceRole ( bytes32 role , address account ) public virtual override { require (account == _msgSender(), \"AccessControl: can only renounce roles for self\" ); _revokeRole(role, account); } Figure 9.1: The renounceRole function of the base contract from the OpenZeppelin library Any shareholder can use the renounceRole function to revoke the ROLE_FUND_AUTHORIZED role on their own account without authorization from the admin. This role is used in three functions in the AccessControlUpgradeable contract: 1. The isAccountAuthorized function to check if an account is authorized 2. The getAuthorizedAccountsCount to get the number of authorized accounts 3. The getAuthorizedAccountAt to get the authorized account at an index Other contracts and o-chain components relying on these functions may nd the system in an inconsistent state and may not be able to work as expected. Exploit Scenario Eve, an authorized shareholder, renounces her ROLE_FUND_AUTHORIZED role. The o-chain components fetch the number of authorized accounts, which is one less than the expected value. The o-chain component is now operating on an inaccurate contract state. Recommendations Short term, have the code override the renounceRole function in the AuthorizationModule contract. Make this overridden function an admin-only function. Long term, read all the library code to nd public functions exposed by the base contracts and override them to implement correct business logic and enforce proper access controls. Document any changes between the original OpenZeppelin implementation and the MMF implementation. Be sure to thoroughly test overridden functions and changes in unit tests and fuzz tests.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "10. Risk of multiple dividend payouts in a day ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The fund manager can lose the systems money by making multiple dividend payouts in a day when they should be paid out only once a day. The distributeDividends function in the MoneyMarketFund contract takes the date as an argument. This date value is not validated to be later than the date from an earlier execution of the distributeDividends function. function distributeDividends ( address [] calldata accounts, uint256 date , int256 rate , uint256 price ) { } external onlyAdmin onlyWithValidRate(rate) onlyValidPaginationSize(accounts.length, MAX_ACCOUNT_PAGE_SIZE) lastKnownPrice = price; for ( uint i = 0 ; i < accounts.length; ) { _processDividends(accounts[i], date, rate, price); unchecked { i++; } } Figure 10.1: The distributeDividends function in the MoneyMarketFund contract As a result, the admin can distribute dividends multiple times a day, which will result in the loss of funds from the company to the users. The admin can correct this mistake by using the adjustBalance function, but adjusting the balance for all the system users will be a dicult and costly process. The same issue also aects the following three functions: 1. The endOfDay function in the MoneyMarketFund contract 2. The distributeDividends function in the TransferAgentModule contract 3. The endOfDay function in the TransferAgentModule contract. Exploit Scenario The admin sends a transaction to distribute dividends. The transaction is not included in the blockchain because of congestion or gas estimation errors. Forgetting about the earlier transaction, the admin sends another transaction, and both transactions are executed to distribute dividends on the same day. Recommendations Short term, have the code store the last dividend distribution date and validate that the date argument in all the dividend distribution functions is later than the last stored dividend date. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added to all state-modifying functions.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "11. Shareholders can stop admin from deauthorizing them ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "Shareholders can prevent the admin from deauthorizing them by front-running the deauthorizeAccount function in the AuthorizationModule contract. The deauthorizeAccount function reverts if the provided account has one or more pending transactions. function deauthorizeAccount ( address account ) external override onlyRole(ROLE_AUTHORIZATION_ADMIN) { require (account != address ( 0 ), \"INVALID_ADDRESS\" ); address txModule = modules.getModuleAddress( keccak256 ( \"MODULE_TRANSACTIONAL\" ) ); require (txModule != address ( 0 ), \"MODULE_REQUIRED_NOT_FOUND\" ); require ( hasRole(ROLE_FUND_AUTHORIZED, account), \"SHAREHOLDER_DOES_NOT_EXISTS\" ); require ( !ITransactionStorage(txModule).hasTransactions(account), \"PENDING_TRANSACTIONS_EXIST\" ); _revokeRole(ROLE_FUND_AUTHORIZED, account); emit AccountDeauthorized(account); } Figure 11.1: The deauthorizeAccount function in the AuthorizationModule contract A shareholder can front-run a transaction executing the deauthorizeAccount function for their account by submitting a new transaction request to buy or sell shares. The deauthorizeAccount transaction will revert because of a pending transaction for the shareholder. Exploit Scenario Eve, a shareholder, sets up a bot to front-run all deauthorizeAccount transactions that add a new transaction request for her. As a result, all admin transactions to deauthorize Eve fail. Recommendations Short term, remove the check for the pending transactions of the provided account and consider one of the following: 1. Have the code cancel the pending transactions of the provided account in the deauthorizeAccount function. 2. Add a check in the _processSettlements function in the MoneyMarketFund contract to skip unauthorized accounts. Add the same check in the _processSettlements function in the TransferAgentModule contract. Long term, always analyze all contract functions that can be aected by attackers front-running calls to manipulate the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "12. Total number of submitters in MultiSigGenVerier contract can be more than allowed limit of MAX_SUBMITTERS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The total number of submitters in the MultiSigGenVerifier contract can be more than the allowed limit of MAX_SUBMITTERS . The addSubmitters function in the MultiSigGenVerifier contract does not check that the total number of submitters in the submittersSet is less than the value of the MAX_SUBMITTERS constant. function addSubmitters ( address [] calldata submitters) public onlyVerifier { require (submitters.length <= MAX_SUBMITTERS, \"INVALID_ARRAY_LENGTH\" ); for ( uint256 i = 0 ; i < submitters.length; i++) { submittersSet.add(submitters[i]); } } Figure 12.1: The addSubmitters function in the MultiSigGenVerifier contract This allows the admin to add more than the maximum number of allowed submitters to the MultiSigGenVerifier contract. Recommendations Short term, add a check to the addSubmitters function to verify that the length of the submittersSet is less than or equal to the MAX_SUBMITTERS constant. Long term, document the system state machine specication and follow it to ensure proper data validation checks are added in all state-modifying functions. To ensure MAX_SUBMITTERS is never exceeded, add fuzz testing where MAX_SUBMITTERS is the system invariant under test.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. Lack of contract existence check on target address ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The signedDataExecution function lacks validation to ensure that the target argument is a contract address and not an externally owned account (EOA). The absence of such a check could lead to potential security issues, particularly when executing low-level calls to an address not containing contract code. Low-level calls to an EOA return true for the success variable instead of reverting as they would with a contract address. This unexpected behavior could trigger inadvertent execution of subsequent code relying on the success variable to be accurate, potentially resulting in undesired outcomes. The onlySubmitter modier limits the potential impact of this vulnerability. function signedDataExecution( address target, bytes calldata payload, bytes calldata signatures ) external onlySubmitter { ... // Wallet logic if (acquiredThreshold >= _getRequiredThreshold(target)) { (bool success, bytes memory result) = target.call{value: 0}( payload ); emit TransactionExecuted(target, result); if (!success) { assembly { result := add(result, 0x04) } revert(abi.decode(result, (string))); } } else { revert(\"INSUFICIENT_THRESHOLD_ACQUIRED\"); } } Figure 13.1: The signedDataExecution function in the MultiSigGenVerifier contract Exploit Scenario Alice, an authorized submitter account, calls the signedDataExecution function, passing in an EOA address instead of the expected contract address. The low-level call to the target address returns successfully and does not revert. As a result, Alice thinks she has executed code but in fact has not. Recommendations Short term, integrate a contract existence check to ensure that code is present at the address passed in as the target argument. Long term, use the Slither static analyzer to catch issues such as this one. Consider integrating slither-action into the projects CI pipeline.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "14. Pending transactions can trigger a DoS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "An unbounded number of pending transactions can cause the _processSettlements function to run out of gas while trying to process them. There is no restriction on the length of pending transactions a user might have, and gas-intensive operations are performed in the for-loop of the _processSettlements function. If an account returns too many pending transactions, operations that call _processSettlements might revert with an out-of-gas error. function _processSettlements( address account, uint256 date, uint256 price ) internal whenTransactionsExist(account) { bytes32 [] memory pendingTxs = ITransactionStorage( moduleRegistry.getModuleAddress(TRANSACTIONAL_MODULE) ).getAccountTransactions(account); for ( uint256 i = 0; i < pendingTxs.length; ) { ... Figure 14.1: The pendingTxs loop in the _processSettlements function in the MoneyMarketFund contract The same issue aects the _processSettlements function in the TransferAgentModule contract. Exploit Scenario Eve submits multiple transactions to the requestSelfServiceCashPurchase function, and each creates a pending transaction record in the pendingTransactionsMap for Eves account. When settleTransactions is called with an array of accounts that includes Eve, the _processSettlements function tries to process all her pending transactions and runs out of gas in the attempt. Recommendations Short term, make the following changes to the transaction settlement ow: 1. Enhance the o-chain component of the system to identify accounts with too many pending transactions and exclude them from calls to _processSettlements ows. 2. Create another transaction settlement function that paginates over the list of pending transactions of a single account. Long term, implement thorough testing protocols for these loop structures, simulating various scenarios and edge cases that could potentially result in unbounded inputs. Ensure that all loop structures are robustly designed with safeguards in place, such as constraints and checks on input variables.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "15. Dividend distribution has an incorrect rounding direction for negative rates ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-05-franklintempleton-moneymarket-securityreview.pdf", "body": "The rounding direction of the dividend calculation in the _processDividends function benets the user when the dividend rate is negative, causing the fund to lose value it should retain. The division operation that computes dividend shares is rounding down in the _processDividends function of the MoneyMarketFund contract: function _processDividends ( address account , uint256 date , int256 rate , uint256 price ) internal whenHasHoldings(account) { uint256 dividendAmount = balanceOf(account) * uint256 (abs(rate)); uint256 dividendShares = dividendAmount / price; _payDividend(account, rate, dividendShares); // handle very unlikely scenario if occurs _handleNegativeYield(account, rate, dividendShares); _removeEmptyAccountFromHoldingsSet(account); emit DividendDistributed(account, date, rate, price, dividendShares); } Figure 15.1: The _processDividends function in the MoneyMarketFund contract As a result, for a negative dividend rate, the rounding benets the user by subtracting a lower number of shares from the user balance. In particular, if the rate is low and the price is high, the dividend can round down to zero. The same issue aects the _processDividends function in the TransferAgentModule contract. Exploit Scenario Eve buys a small number of shares from multiple accounts. The dividend rounds down and is equal to zero. As a result, Eve avoids the losses from the downside movement of the fund while enjoying prots from the upside. Recommendations Short term, have the _processDividends function round up the number of dividendShares for negative dividend rates. Long term, document the expected rounding direction for every arithmetic operation (see appendix G ) and follow it to ensure that rounding is always benecial to the fund. Use Echidna to nd issues arising from the wrong rounding direction.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. Use of fmt.Sprintf to build host:port string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue. 2. MongoDB scaler does not encode username and password in connection string Severity: Low Diculty: Low Type: Data Validation Finding ID: TOB-KEDA-2 Target: pkg/scalers/mongo_scaler.go", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "3. Prometheus metrics server does not support TLS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port 9022. When Prometheus makes a connection to the server, it is unencrypted, leaving both the request and response vulnerable to interception and tampering in transit. As KEDA does not support TLS for the server, the user has no way to ensure the condentiality and integrity of these metrics. Recommendations Short term, provide a ag to enable TLS for Prometheus metrics exposed by the Metrics Adapter. The usual way to enable TLS for an HTTP server is using the http.ListenAndServeTLS function.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. Return value is dereferenced before error check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "After certain calls to http.NewRequestWithContext , the *Request return value is dereferenced before the error return value is checked (see the highlighted lines in gures 4.1 and 4.2). checkTokenRequest, err := http.NewRequestWithContext(ctx, \"HEAD\" , tokenURL.String(), nil ) checkTokenRequest.Header.Set( \"X-Subject-Token\" , token) checkTokenRequest.Header.Set( \"X-Auth-Token\" , token) if err != nil { return false , err } Figure 4.1: pkg/scalers/openstack/keystone_authentication.go#L118-L124 req, err := http.NewRequestWithContext(ctx, \"GET\" , url, nil ) req.SetBasicAuth(s.metadata.username, s.metadata.password) req.Header.Set( \"Origin\" , s.metadata.corsHeader) if err != nil { return - 1 , err } Figure 4.2: pkg/scalers/artemis_scaler.go#L241-L248 If an error occurred in the call to NewRequestWithContext , this behavior could result in a panic due to a nil pointer dereference. Exploit Scenario One of the calls to http.NewRequestWithContext shown in gures 4.1 and 4.2 returns an error and a nil *Request pointer. The subsequent code dereferences the nil pointer, resulting in a panic, crash, and DoS condition for the aected KEDA scaler. Recommendations Short term, check the error return value before accessing the returned *Request (e.g., by calling methods on it). Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Undetermined"]}, {"title": "5. Unescaped components in PostgreSQL connection string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The PostgreSQL scaler creates a connection string by formatting the congured host, port, username, database name, SSL mode, and password with fmt.Sprintf : meta.connection = fmt.Sprintf( \"host=%s port=%s user=%s dbname=%s sslmode=%s password=%s\" , host, port, userName, dbName, sslmode, password, ) Figure 5.1: pkg/scalers/postgresql_scaler.go#L127-L135 However, none of the parameters included in the format string are escaped before the call to fmt.Sprintf . According to the PostgreSQL documentation ,  To write an empty value, or a value containing spaces, surround it with single quotes, for example keyword = 'a value' . Single quotes and backslashes within a value must be escaped with a backslash, i.e., \\' and \\\\ . As KEDA does not perform this escaping, the connection string could fail to parse if any of the conguration parameters (e.g., the password) contains symbols with special meaning in PostgreSQL connection strings. Furthermore, this issue may allow the injection of harmful or unintended parameters into the connection string using spaces and equal signs. Although the latter attack violates assumptions about the applications behavior, it is not a severe issue in KEDAs case because users can already pass full connection strings via the connectionFromEnv conguration parameter. Exploit Scenario A user congures the PostgreSQL scaler with a password containing a space. As the PostgreSQL scaler does not escape the password in the connection string, when the client connection is initialized, the string fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, escape the user-provided PostgreSQL parameters using the method described in the PostgreSQL documentation . Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter. 7. Insu\u0000cient check against nil Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-KEDA-7 Target: pkg/scalers/azure_eventhub_scaler.go#L253-L259", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "1. Use of fmt.Sprintf to build host:port string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "Several scalers use a construct like fmt.Sprintf(\"%s:%s\", host, port) to create a host:port address string from a user-supplied host and port. This approach is problematic when the host is a literal IPv6 address, which should be enclosed in square brackets when the address is part of a resource identier. An address created using simple string concatenation, such as with fmt.Sprintf , may fail to parse when given to Go standard library functions. The following source les incorrectly use fmt.Sprintf to create an address:  pkg/scalers/cassandra_scaler.go:115  pkg/scalers/mongo_scaler.go:191  pkg/scalers/mssql_scaler.go:220  pkg/scalers/mysql_scaler.go:149  pkg/scalers/predictkube_scaler.go:128  pkg/scalers/redis_scaler.go:296  pkg/scalers/redis_scaler.go:364 Recommendations Short term, use net.JoinHostPort instead of fmt.Sprintf to construct network addresses. The documentation for the net package states the following: JoinHostPort combines host and port into a network address of the form host:port . If host contains a colon, as found in literal IPv6 addresses, then JoinHostPort returns [host]:port . Long term, use Semgrep and the sprintf-host-port rule of semgrep-go to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "2. MongoDB scaler does not encode username and password in connection string ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The MongoDB scaler creates a connection string URI by concatenating the congured host, port, username, and password: addr := fmt.Sprintf( \"%s:%s\" , meta.host, meta.port) auth := fmt.Sprintf( \"%s:%s\" , meta.username, meta.password) connStr = \"mongodb://\" + auth + \"@\" + addr + \"/\" + meta.dbName Figure 2.1: pkg/scalers/mongo_scaler.go#L191-L193 Per MongoDB documentation, if either the username or password contains a character in the set :/?#[]@ , it must be percent-encoded . However, KEDA does not do this. As a result, the constructed connection string could fail to parse. Exploit Scenario A user congures the MongoDB scaler with a password containing an  @  character, and the MongoDB scaler does not encode the password in the connection string. As a result, when the client object is initialized, the URL fails to parse, an error is thrown, and the scaler does not function. Recommendations Short term, percent-encode the user-supplied username and password before constructing the connection string. Long term, use the custom Semgrep rule provided in Appendix C to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "3. Prometheus metrics server does not support TLS ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The KEDA Metrics Adapter exposes Prometheus metrics on an HTTP server listening on port 9022. Though Prometheus supports scraping metrics over TLS-enabled connections, KEDA does not oer TLS for this server. The function responsible for starting the HTTP server, prommetrics.NewServer , does so using the http.ListenAndServe function, which does not enable TLS. func (metricsServer PrometheusMetricServer) NewServer(address string , pattern string ) { http.HandleFunc( \"/healthz\" , func (w http.ResponseWriter, _ *http.Request) { w.WriteHeader(http.StatusOK) _, err := w.Write([] byte ( \"OK\" )) if err != nil { log.Fatalf( \"Unable to write to serve custom metrics: %v\" , err) } }) log.Printf( \"Starting metrics server at %v\" , address) http.Handle(pattern, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // initialize the total error metric _, errscaler := scalerErrorsTotal.GetMetricWith(prometheus.Labels{}) if errscaler != nil { log.Fatalf( \"Unable to initialize total error metrics as : %v\" , errscaler) } log.Fatal( http.ListenAndServe(address, nil ) ) } Figure 3.1: prommetrics.NewServer exposes Prometheus metrics without TLS ( pkg/prommetrics/adapter_prommetrics.go#L82-L99 ). Exploit Scenario A user sets up KEDA with Prometheus integration, enabling the scraping of metrics on port", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "6. Redis scalers set InsecureSkipVerify when TLS is enabled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "The Redis Lists scaler (of which most of the code is reused by the Redis Streams scaler) supports the enableTLS option to allow the connection to the Redis server to use Transport Layer Security (TLS). However, when creating the TLSConfig for the Redis client, the scaler assigns the InsecureSkipVerify eld to the value of enableTLS (Figure 6.1), which means that certicate and server name verication is always disabled when TLS is enabled. This allows trivial MitM attacks, rendering TLS ineective. if info.enableTLS { options.TLSConfig = &tls.Config{ InsecureSkipVerify: info.enableTLS, } } Figure 6.1: KEDA sets InsecureSkipVerify to the value of info.enableTLS , which is always true in the block above. This pattern occurs in three locations: pkg/scalers/redis_scaler.go#L472-L476 , pkg/scalers/redis_scaler.go#L496-L500 , and pkg/scalers/redis_scaler.go#L517-L521 . KEDA does not document this insecure behavior, and users likely expect that enableTLS is implemented securely to prevent MitM attacks. The only public mention of this behavior is a stale, closed issue concerning this problem on GitHub . Exploit Scenario A user deploys KEDA with the Redis Lists or Redis Streams scaler. To protect the condentiality and integrity of data in transit between KEDA and the Redis server, the user sets the enableTLS metadata eld to true . Unbeknownst to the user, KEDA has disabled TLS certicate verication, allowing attackers on the network to modify the data in transit. An adversary can then falsify metrics coming from Redis to maliciously inuence the scaling behavior of KEDA and the Kubernetes cluster (e.g., by causing a DoS). Recommendations Short term, add a warning to the public documentation that the enableTLS option, as currently implemented, is not secure. Short term, do not enable InsecureSkipVerify when the user species the enableTLS parameter.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "7. Insu\u0000cient check against nil ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "Within a function in the scaler for Azure event hubs, the object partitionInfo is dereferenced before correctly checking it against nil . Before the object is used, a check conrms that partitionInfo is not nil . However, this check is insucient because the function returns if the condition is met, and the function subsequently uses partitionInfo without additional checks against nil . As a result, a panic may occur when partitionInfo is later used in the same function. func (s *azureEventHubScaler) GetUnprocessedEventCountInPartition(ctx context.Context, partitionInfo *eventhub.HubPartitionRuntimeInformation) (newEventCount int64 , checkpoint azure.Checkpoint, err error ) { // if partitionInfo.LastEnqueuedOffset = -1, that means event hub partition is empty if partitionInfo != nil && partitionInfo.LastEnqueuedOffset == \"-1\" { return 0 , azure.Checkpoint{}, nil } checkpoint, err = azure.GetCheckpointFromBlobStorage(ctx, s.httpClient, s.metadata.eventHubInfo, partitionInfo.PartitionID ) Figure 7.1: partionInfo is dereferenced before a nil check pkg/scalers/azure_eventhub_scaler.go#L253-L259 Exploit Scenario While the Azure event hub performs its usual applications, an application error causes GetUnprocessedEventCountInPartition to be called with a nil partitionInfo parameter. This causes a panic and the scaler to crash and to stop monitoring events. Recommendations Short term, edit the code so that partitionInfo is checked against nil before dereferencing it. Long term, use CodeQL and its go/missing-error-check query to detect future instances of this issue.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "8. Prometheus metrics server does not support authentication ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-01-keda-securityreview.pdf", "body": "When scraping metrics, Prometheus supports multiple forms of authentication , including Basic authentication, Bearer authentication, and OAuth 2.0. KEDA exposes Prometheus metrics but does not oer the ability to protect its metrics server with any of the supported authentication types. Exploit Scenario A user deploys KEDA on a network. An adversary gains access to the network and is able to issue HTTP requests to KEDAs Prometheus metrics server. As KEDA does not support authentication for the server, the attacker can trivially view the exposed metrics. Recommendations Short term, implement one or more of the authentication types that Prometheus supports for scrape targets. A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "1. Project dependencies are not monitored for vulnerabilities ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The osquery project depends on a large number of dependencies to realize the functionality of the existing tables. They are included as Git submodules in the project. The build mechanism of each dependency has been rewritten to suit the specic needs of osquery (e.g., so that it has as few dynamically loaded libraries as possible), but there is no process in place to detect published vulnerabilities in the dependencies. As a result, osquery could continue to use code with known vulnerabilities in the dependency projects. Exploit Scenario An attacker, who has gained a foothold on a machine running osquery, leverages an existing vulnerability in a dependency to exploit osquery. He escalates his privileges to those of the osquery agent or carries out a denial-of-service attack to block the osquery agent from sending data. Recommendations Short term, regularly update the dependencies to their latest versions. Long term, establish a process within the osquery project to detect published vulnerabilities in its dependencies. 15 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "2. No separation of privileges when executing dependency code ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "In several places in the codebase, the osquery agent realizes the functionality of a table by invoking code in a dependency project. For example, the yara table is implemented by invoking code in libyara. However, there is no separation of privileges or sandboxing in place when the code in the dependency library is called, so the library code executes with the same privileges as the osquery agent. Considering the projects numerous dependencies, this issue increases the osquery agents attack surface and would exacerbate the eects of any vulnerabilities in the dependencies. Exploit Scenario An attacker nds a vulnerability in a dependency library that allows her to gain code execution, and she elevates her privileges to that of the osquery agent. Recommendations Short term, regularly update the dependencies to their latest versions. Long term, create a security barrier against the dependency library code to minimize the impact of vulnerabilities. 16 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "3. No limit on the amount of information that can be read from the Firefox add-ons table ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The implementation of the Firefox add-ons table has no limit on the amount of information that it can read from JSON les while enumerating the add-ons installed on a user prole. This is because to directly read and parse the Firefox prole JSON le, the parseJSON implementation in the osquery agent uses boost::property_tree, which does not have this limit. pt::ptree tree; if (!osquery::parseJSON(path + kFirefoxExtensionsFile, tree).ok()) { TLOG << \"Could not parse JSON from: \" << path + kFirefoxExtensionsFile; return; } Figure 3.1: The osquery::parseJSON function has no limit on the amount of data it can read. Exploit Scenario An attacker crafts a large, valid JSON le and stores it on the Firefox prole path as extensions.json (e.g., in ~/Library/Application Support/Firefox/Profiles/foo/extensions.json on a macOS system). When osquery executes a query using the firefox_addons table, the parseJSON function reads and parses the complete le, causing high resource consumption. Recommendations Short term, enforce a maximum le size within the Firefox table, similar to the limits on other tables in osquery. Long term, consider removing osquery::parseJSON and implementing a single, standard way to parse JSON les across osquery. The osquery project currently uses both boost::property_tree and RapidJSON libraries to parse JSON les, resulting in the use of dierent code paths to handle untrusted content. 17 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "4. The SIP status on macOS may be misreported ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "If System Integrity Protection (SIP) is disabled on a Mac running osquery, the SIP conguration table might not report the correct value in the enabled column for the config_flag: sip row. For this misreporting to happen, extra ags need to be present in the value returned by csr_get_active_config and absent in the osquery kRootlessConfigFlags list. This is the case for the ags CSR_ALLOW_ANY_RECOVERY_OS, CSR_ALLOW_UNAPPROVED_KEXTS, CSR_ALLOW_EXECUTABLE_POLICY_OVERRIDE, and CSR_ALLOW_UNAUTHENTICATED_ROOT in xnu-7195.141.2/bsd/sys/csr.h (compare gure 4.1 and gure 4.3). /* CSR configuration flags */ #define CSR_ALLOW_UNTRUSTED_KEXTS (1 << 0) #define CSR_ALLOW_UNRESTRICTED_FS (1 << 1) #define CSR_ALLOW_TASK_FOR_PID (1 << 2) #define CSR_ALLOW_KERNEL_DEBUGGER (1 << 3) #define CSR_ALLOW_APPLE_INTERNAL (1 << 4) #define CSR_ALLOW_DESTRUCTIVE_DTRACE (1 << 5) /* name deprecated */ #define CSR_ALLOW_UNRESTRICTED_DTRACE (1 << 5) #define CSR_ALLOW_UNRESTRICTED_NVRAM (1 << 6) #define CSR_ALLOW_DEVICE_CONFIGURATION (1 << 7) #define CSR_ALLOW_ANY_RECOVERY_OS (1 << 8) #define CSR_ALLOW_UNAPPROVED_KEXTS (1 << 9) #define CSR_ALLOW_EXECUTABLE_POLICY_OVERRIDE (1 << 10) #define CSR_ALLOW_UNAUTHENTICATED_ROOT (1 << 11) Figure 4.1: The CSR ags in xnu-7159.141.2 18 Atlassian: osquery Security Assessment QueryData results; csr_config_t config = 0; csr_get_active_config(&config); csr_config_t valid_allowed_flags = 0; for (const auto& kv : kRootlessConfigFlags) { valid_allowed_flags |= kv.second; } Row r; r[\"config_flag\"] = \"sip\"; if (config == 0) { // SIP is enabled (default) r[\"enabled\"] = INTEGER(1); r[\"enabled_nvram\"] = INTEGER(1); } else if ((config | valid_allowed_flags) == valid_allowed_flags) { // mark SIP as NOT enabled (i.e. disabled) if // any of the valid_allowed_flags is set r[\"enabled\"] = INTEGER(0); r[\"enabled_nvram\"] = INTEGER(0); } results.push_back(r); Figure 4.2: How the SIP state is computed in genSIPConfig // rootless configuration flags // https://opensource.apple.com/source/xnu/xnu-3248.20.55/bsd/sys/csr.h const std::map<std::string, uint32_t> kRootlessConfigFlags = { // CSR_ALLOW_UNTRUSTED_KEXTS {\"allow_untrusted_kexts\", (1 << 0)}, // CSR_ALLOW_UNRESTRICTED_FS {\"allow_unrestricted_fs\", (1 << 1)}, // CSR_ALLOW_TASK_FOR_PID {\"allow_task_for_pid\", (1 << 2)}, // CSR_ALLOW_KERNEL_DEBUGGER {\"allow_kernel_debugger\", (1 << 3)}, // CSR_ALLOW_APPLE_INTERNAL {\"allow_apple_internal\", (1 << 4)}, // CSR_ALLOW_UNRESTRICTED_DTRACE {\"allow_unrestricted_dtrace\", (1 << 5)}, // CSR_ALLOW_UNRESTRICTED_NVRAM {\"allow_unrestricted_nvram\", (1 << 6)}, // CSR_ALLOW_DEVICE_CONFIGURATION {\"allow_device_configuration\", (1 << 7)}, }; Figure 4.3: The ags currently supported by osquery 19 Atlassian: osquery Security Assessment Exploit Scenario An attacker, who has gained a foothold with root privileges, disables SIP on a device running macOS and sets the csr_config ags to 0x3e7. When building the response for the sip_config table, osquery misreports the state of SIP. Recommendations Short term, consider reporting SIP as disabled if any ag is present or if any of the known ags are present (e.g., if (config & valid_allowed_flags) != 0). Long term, add support for reporting the raw ag values to the table specication and code so that the upstream server can make the nal determination on the state of SIP, irrespective of the ags supported by the osquery daemon. Additionally, monitor for changes and add support for new ags as they are added on the macOS kernel. 20 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. The OpenReadableFile function can hang on reading a le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The OpenReadableFile function creates an instance of the PlatformFile class, which is used for reading and writing les. The constructor of PlatformFile uses the open syscall to obtain a handle to the le. The OpenReadableFile function by default opens the le using the O_NONBLOCK ag, but if PlatformFiles isSpecialFile method returns true, it opens the le without using O_NONBLOCK. On the POSIX platform, the isSpecialFile method returns true for les in which fstat returns a size of zero. If the le to be read is a FIFO, the open syscall in the second invocation of PlatformFiles constructor blocks the osquery thread until another thread opens the FIFO le to write to it. struct OpenReadableFile : private boost::noncopyable { public: explicit OpenReadableFile(const fs::path& path, bool blocking = false) : blocking_io(blocking) { int mode = PF_OPEN_EXISTING | PF_READ; if (!blocking) { mode |= PF_NONBLOCK; } // Open the file descriptor and allow caller to perform error checking. fd = std::make_unique<PlatformFile>(path, mode); if (!blocking && fd->isSpecialFile()) { // A special file cannot be read in non-blocking mode, reopen in blocking // mode mode &= ~PF_NONBLOCK; blocking_io = true; fd = std::make_unique<PlatformFile>(path, mode); } } public: std::unique_ptr<PlatformFile> fd{nullptr}; 21 Atlassian: osquery Security Assessment bool blocking_io; }; Figure 5.1: The OpenReadableFile function can block the osquery thread. Exploit Scenario An attacker creates a special le, such as a FIFO, in a path known to be read by the osquery agent. When the osquery agent attempts to open and read the le, it blocks the osquery thread indenitely, in eect making osquery unable to report the status to the server. Recommendations Short term, ensure that the le operations in filesystem.cpp do not block the osquery thread. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. References  The Single Unix Specication, Version 2 22 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Methods in POSIX PlatformFile class are susceptible to race conditions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The POSIX implementation of the methods in the PlatformFile class includes several methods that return the current properties of a le. However, the properties can change during the lifetime of the le descriptor, so the return values of these methods may not reect the actual properties. For example, the isSpecialFile method, which is used to determine the strategy for reading the le, calls the size method. However, the le size can change between the time of the call and the reading operation, in which case the wrong strategy for reading the le could be used. bool PlatformFile::isSpecialFile() const { return (size() == 0); } static uid_t getFileOwner(PlatformHandle handle) { struct stat file; if (::fstat(handle, &file) < 0) { return -1; } return file.st_uid; } Status PlatformFile::isOwnerRoot() const { if (!isValid()) { return Status(-1, \"Invalid handle_\"); } uid_t owner_id = getFileOwner(handle_); if (owner_id == (uid_t)-1) { return Status(-1, \"fstat error\"); } if (owner_id == 0) { return Status::success(); } return Status(1, \"Owner is not root\"); } Status PlatformFile::isOwnerCurrentUser() const { 23 Atlassian: osquery Security Assessment if (!isValid()) { return Status(-1, \"Invalid handle_\"); } uid_t owner_id = getFileOwner(handle_); if (owner_id == (uid_t)-1) { return Status(-1, \"fstat error\"); } if (owner_id == ::getuid()) { return Status::success(); } return Status(1, \"Owner is not current user\"); } Status PlatformFile::isExecutable() const { struct stat file_stat; if (::fstat(handle_, &file_stat) < 0) { return Status(-1, \"fstat error\"); } if ((file_stat.st_mode & S_IXUSR) == S_IXUSR) { return Status::success(); } return Status(1, \"Not executable\"); } Status PlatformFile::hasSafePermissions() const { struct stat file; if (::fstat(handle_, &file) < 0) { return Status(-1, \"fstat error\"); } // We allow user write for now, since our main threat is external // modification by other users if ((file.st_mode & S_IWOTH) == 0) { return Status::success(); } return Status(1, \"Writable\"); } Figure 6.1: The methods in PlatformFile could cause race issues. Exploit Scenario A new function is added to osquery that uses hasSafePermissions to determine whether to allow a potentially unsafe operation. An attacker creates a le that passes the hasSafePermissions check, then changes the permissions and alters the le contents before the le is further processed by the osquery agent. 24 Atlassian: osquery Security Assessment Recommendations Short term, refactor the operations of the relevant PlatformFile class methods to minimize the race window. For example, the only place hasSafePermissions is currently used is in the safePermissions function, in which it is preceded by a check that the le is owned by root or the current user, which eliminates the possibility of an adversary using the race condition; therefore, refactoring may not be necessary in this method. Add comments to these methods describing possible adverse eects. Long term, refactor the interface of PlatformFile to contain the potential race issues within the class. For example, move the safePermissions function into the PlatformFile class so that hasSafePermissions is not exposed outside of the class. 25 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "7. No limit on the amount of data that parsePlist can parse ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "To support several macOS-specic tables, osquery contains a function called osquery::parsePlist, which reads and parses property list (.plist) les by using the Apple Foundation framework class NSPropertyListSerialization. The parsePlist function is used by tables such as browser_plugins and xprotect_reports to read user-accessible les. The function does not have any limit on the amount of data that it will parse. id ns_path = [NSString stringWithUTF8String:path.string().c_str()]; id stream = [NSInputStream inputStreamWithFileAtPath:ns_path]; if (stream == nil) { return Status(1, \"Unable to read plist: \" + path.string()); } // Read file content into a data object, containing potential plist data. NSError* error = nil; [stream open]; id plist_data = [NSPropertyListSerialization propertyListWithStream:stream options:0 format:NULL error:&error]; Figure 7.1: The parsePlist implementation does not have a limit on the amount of data that it can deserialize. 26 Atlassian: osquery Security Assessment auto info_path = path + \"/Contents/Info.plist\"; // Ensure that what we're processing is actually a plug-in. if (!pathExists(info_path)) { return; } if (osquery::parsePlist(info_path, tree).ok()) { // Plugin did not include an Info.plist, or it was invalid for (const auto& it : kBrowserPluginKeys) { r[it.second] = tree.get(it.first, \"\"); // Convert bool-types to an integer. jsonBoolAsInt(r[it.second]); } } Figure 7.2: browser_plugins uses parsePlist on user-controlled les. Exploit Scenario An attacker crafts a large, valid .plist le and stores it in ~/Library/Internet Plug-Ins/foo/Contents/Info.plist on a macOS system running the osquery daemon. When osquery executes a query using the browser_plugins table, it reads and parses the complete le, causing high resource consumption. Recommendations Short term, modify the browser_plugins and xprotect_reports tables to enforce a maximum le size (e.g., by combining readFile and parsePlistContent). Long term, to prevent this issue in future tables, consider removing the parsePlist function or rewriting it as a helper function around a safer implementation. 27 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "8. The parsePlist function can hang on reading certain les ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The osquery codebase contains a function called osquery::parsePlist, which reads and parses .plist les. This function opens the target le directly by using the inputStreamWithFileAtPath method from NSInputStream, as shown in gure 7.1 in the previous nding, and passes the resulting input stream to NSPropertyListSerialization for direct consumption. However, parsePlist can hang on reading certain les. For example, if the le to be read is a FIFO, the function blocks the osquery thread until another program or thread opens the FIFO to write to it. Exploit Scenario An attacker creates a FIFO le on a macOS device in ~/Library/Internet Plug-Ins/foo/Contents/Info.plist or ~/Library/Logs/DiagnosticReports/XProtect-foo using the mkfifo command. The osquery agent attempts to open and read the le when building a response for queries on the browser_plugins and xprotect_reports tables, but parsePlist blocks the osquery thread indenitely, leaving osquery unable to respond to the query request. Recommendations Short term, implement a check in parsePlist to verify that the .plist le to be read is not a special le. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. Also consider replacing parsePlist in favor of the parsePlistContent function and standardizing all le reads on a single code path to prevent similar issues going forward. 28 Atlassian: osquery Security Assessment 9. The parseJSON function can hang on reading certain les on Linux and macOS Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-ATL-9 Target: osquery/filesystem/filesystem.cpp", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "10. No limit on the amount of data read or expanded from the Safari extensions table ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The safari_extensions table allows the agent to query installed Safari extensions on a certain user prole. Said extensions consist of extensible archive format (XAR) compressed archives with the .safariextz le extension, which are stored in the ~/Library/Safari/Extensions folder. The osquery program does not have a limit on the amount of data that can be processed when reading and inating the Safari extension contents; a large amount of data may cause a denial of service. 31 Atlassian: osquery Security Assessment xar_t xar = xar_open(path.c_str(), READ); if (xar == nullptr) { TLOG << \"Cannot open extension archive: \" << path; return; } xar_iter_t iter = xar_iter_new(); xar_file_t xfile = xar_file_first(xar, iter); size_t max_files = 500; for (size_t index = 0; index < max_files; ++index) { if (xfile == nullptr) { break; } char* xfile_path = xar_get_path(xfile); if (xfile_path == nullptr) { break; } // Clean up the allocated content ASAP. std::string entry_path(xfile_path); free(xfile_path); if (entry_path.find(\"Info.plist\") != std::string::npos) { if (xar_verify(xar, xfile) != XAR_STREAM_OK) { TLOG << \"Extension info extraction failed verification: \" << path; } size_t size = 0; char* buffer = nullptr; if (xar_extract_tobuffersz(xar, xfile, &buffer, &size) != 0 || size == 0) { break; } std::string content(buffer, size); free(buffer); pt::ptree tree; if (parsePlistContent(content, tree).ok()) { for (const auto& it : kSafariExtensionKeys) { r[it.second] = tree.get(it.first, \"\"); } } break; } 32 Atlassian: osquery Security Assessment xfile = xar_file_next(iter); } Figure 10.1: genSafariExtension extracts the full Info.plist to memory. Exploit Scenario An attacker crafts a valid extension containing a large Info.plist le and stores it in ~/Library/Safari/Extensions/foo.safariextz. When the osquery agent attempts to respond to a query on the safari_extensions table, it opens the archive and expands the full Info.plist le in memory, causing high resource consumption. Recommendations Short term, enforce a limit on the amount of information that can be extracted from an XAR archive. Long term, add guidelines to the development documentation on handling untrusted input data. For instance, advise developers to limit the amount of data that may be ingested, processed, or read from untrusted sources such as user-writable les. Enforce said guidelines by performing code reviews on new contributions. 33 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "11. Extended attributes table may read uninitialized or out-of-bounds memory ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The extended_attributes table calls the listxattr function twice: rst to query the extended attribute list size and then to actually retrieve the list of attribute names. Additionally, the return values from the function calls are not checked for errors. This leads to a race condition in the parseExtendedAttributeList osquery function, in which the content buer is left uninitialized if the target le is deleted in the time between the two listxattr calls. As a result, std::string will consume uninitialized and unbounded memory, potentially leading to out-of-bounds memory reads. std::vector<std::string> attributes; ssize_t value = listxattr(path.c_str(), nullptr, (size_t)0, 0); char* content = (char*)malloc(value); if (content == nullptr) { return attributes; } ssize_t ret = listxattr(path.c_str(), content, value, 0); if (ret == 0) { free(content); return attributes; } char* stable = content; do { attributes.push_back(std::string(content)); content += attributes.back().size() + 1; } while (content - stable < value); free(stable); return attributes; Figure 11.1: parseExtendedAttributeList calls listxattr twice. 34 Atlassian: osquery Security Assessment Exploit Scenario An attacker creates and runs a program to race osquery while it is fetching extended attributes from the le system. The attacker is successful and causes the osquery agent to crash with a segmentation fault. Recommendations Short term, rewrite the aected code to check the return values for errors. Replace listxattr with flistxattr, which operates on opened le descriptors, allowing it to continue to query extended attributes even if the le is removed (unlink-ed) from the le system. Long term, establish and enforce best practices for osquery contributions by leveraging automated tooling and code reviews to prevent similar issues from reoccurring. For example, use le descriptors instead of le paths when you need to perform more than one operation on a le to ensure that the le is not deleted or replaced mid-operation. Consider using static analysis tools such as CodeQL to look for other instances of similar issues in the code and to detect new instances of the problem on new contributions. 35 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "12. The readFile function can hang on reading a le ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The readFile function is used to provide a standardized way to read les. It uses the read_max variable to prevent the functions from reading excessive amounts of data. It also selects one of two modes, blocking or non-blocking, depending on the le properties. When using the blocking approach, it reads block_size-sized chunks of the le, with a minimum of 4,096 bytes, and returns the chunks to the caller. However, the call to read can block the osquery thread when reading certain les. if (handle.blocking_io) { // Reset block size to a sane minimum. block_size = (block_size < 4096) ? 4096 : block_size; ssize_t part_bytes = 0; bool overflow = false; do { std::string part(block_size, '\\0'); part_bytes = handle.fd->read(&part[0], block_size); if (part_bytes > 0) { total_bytes += static_cast<off_t>(part_bytes); if (total_bytes >= read_max) { return Status::failure(\"File exceeds read limits\"); } if (file_size > 0 && total_bytes > file_size) { overflow = true; part_bytes -= (total_bytes - file_size); } predicate(part, part_bytes); } } while (part_bytes > 0 && !overflow); } else { Figure 12.1: The blocking_io ow can stall. 36 Atlassian: osquery Security Assessment Exploit Scenario An attacker creates a symlink to /dev/tty in a path known to be read by the osquery agent. When the osquery agent attempts to read the le, it stalls. Recommendations Short term, ensure that the le operations in filesystem.cpp do not block the osquery thread. Long term, introduce a timeout on le operations so that a block does not stall the osquery thread. 37 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "13. The POSIX PlatformFile constructor may block the osquery thread ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The POSIX implementation of PlatformFiles constructor uses the open syscall to obtain a handle to the le. If the le to be opened is a FIFO, the call to open blocks the osquery thread unless the O_NONBLOCK ag is used. There are several places in the codebase in which the constructor is called without the PF_NONBLOCK ag set; all of these calls may stall on opening a FIFO. PlatformFile::PlatformFile(const fs::path& path, int mode, int perms) : fname_(path) { ... if ((mode & PF_NONBLOCK) == PF_NONBLOCK) { oflag |= O_NONBLOCK; is_nonblock_ = true; } if ((mode & PF_APPEND) == PF_APPEND) { oflag |= O_APPEND; } if (perms == -1 && may_create) { perms = 0666; } boost::system::error_code ec; if (check_existence && (!fs::exists(fname_, ec) || ec.value() != errc::success)) { handle_ = kInvalidHandle; } else { handle_ = ::open(fname_.c_str(), oflag, perms); } } Figure 13.1: The POSIX PlatformFile constructor 38 Atlassian: osquery Security Assessment ./filesystem/file_compression.cpp:26: PlatformFile inFile(in, PF_OPEN_EXISTING | PF_READ); ./filesystem/file_compression.cpp:32: PlatformFile outFile(out, PF_CREATE_ALWAYS | PF_WRITE); ./filesystem/file_compression.cpp:102: PlatformFile inFile(in, PF_OPEN_EXISTING | PF_READ); ./filesystem/file_compression.cpp:108: PlatformFile outFile(out, PF_CREATE_ALWAYS | PF_WRITE); ./filesystem/file_compression.cpp:177: PlatformFile pFile(f, PF_OPEN_EXISTING | PF_READ); ./filesystem/filesystem.cpp:242: PlatformFile fd(path, PF_OPEN_EXISTING | PF_WRITE); ./filesystem/filesystem.cpp:258: PlatformFile fd(path, PF_OPEN_EXISTING | PF_READ); ./filesystem/filesystem.cpp:531: PlatformFile fd(path, PF_OPEN_EXISTING | PF_READ); ./carver/carver.cpp:230: PlatformFile src(srcPath, PF_OPEN_EXISTING | PF_READ); Figure 13.2: Uses of PlatformFile without PF_NONBLOCK Exploit Scenario An attacker creates a FIFO le that is opened by one of the functions above, stalling the osquery agent. Recommendations Short term, investigate the uses of PlatformFile to identify possible blocks. Long term, use a static analysis tool such as CodeQL to scan the code for instances in which uses of the open syscall block the osquery thread. 39 Atlassian: osquery Security Assessment 14. No limit on the amount of data the Carver::blockwiseCopy method can write Severity: Medium Diculty: Low Type: Denial of Service Finding ID: TOB-ATL-14 Target: osquery/carver/carver.cpp", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "15. The carves table truncates large le sizes to 32 bits ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The enumerateCarves function uses rapidjson::Value::GetInt() to retrieve a size value from a JSON string. The GetInt return type is int, so it cannot represent sizes exceeding 32 bits; as a result, the size of larger les will be truncated. void enumerateCarves(QueryData& results, const std::string& new_guid) { std::vector<std::string> carves; scanDatabaseKeys(kCarves, carves, kCarverDBPrefix); for (const auto& carveGuid : carves) { std::string carve; auto s = getDatabaseValue(kCarves, carveGuid, carve); if (!s.ok()) { VLOG(1) << \"Failed to retrieve carve GUID\"; continue; } JSON tree; s = tree.fromString(carve); if (!s.ok() || !tree.doc().IsObject()) { VLOG(1) << \"Failed to parse carve entries: \" << s.getMessage(); return; } Row r; if (tree.doc().HasMember(\"time\")) { r[\"time\"] = INTEGER(tree.doc()[\"time\"].GetUint64()); } if (tree.doc().HasMember(\"size\")) { r[\"size\"] = INTEGER(tree.doc()[\"size\"].GetInt()); } stringToRow(\"sha256\", r, tree); 42 Atlassian: osquery Security Assessment stringToRow(\"carve_guid\", r, tree); stringToRow(\"request_id\", r, tree); stringToRow(\"status\", r, tree); stringToRow(\"path\", r, tree); // This table can be used to request a new carve. // If this is the case then return this single result. auto new_request = (!new_guid.empty() && new_guid == r[\"carve_guid\"]); r[\"carve\"] = INTEGER((new_request) ? 1 : 0); results.push_back(r); } } } // namespace Figure 15.1: The enumerateCarves function truncates les of large sizes. Exploit Scenario An attacker creates a le on disk of a size that overows 32 bits by only a small amount, such as 0x100001336. The carves tables reports the le size incorrectly as 0x1336 bytes. The attacker bypasses checks based on the reported le size. Recommendations Short term, use GetUint64 instead of GetInt to retrieve the le size. Long term, use static analysis tools such as CodeQL to look for other instances in which a type of size int is retrieved from JSON and stored in an INTEGER eld. 43 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "16. The time table may not null-terminate strings correctly ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The osquery time table uses strftime to format time information, such as the time zone name, into user-friendly strings. If the amount of information to be written does not t in the provided buer, strftime returns zero and leaves the buer contents in an undened state. QueryData genTime(QueryContext& context) { Row r; time_t osquery_time = getUnixTime(); struct tm gmt; gmtime_r(&osquery_time, &gmt); struct tm now = gmt; auto osquery_timestamp = toAsciiTime(&now); char local_timezone[5] = {0}; { } struct tm local; localtime_r(&osquery_time, &local); strftime(local_timezone, sizeof(local_timezone), \"%Z\", &local); char weekday[10] = {0}; strftime(weekday, sizeof(weekday), \"%A\", &now); char timezone[5] = {0}; strftime(timezone, sizeof(timezone), \"%Z\", &now); Figure 16.1: genTime uses strftime to get the time zone name and day of the week. The strings to be written vary depending on the locale conguration, so some strings may not t in the provided buer. The code does not check the return value of strftime and assumes that the string buer is always null-terminated, which may not always be the case. 44 Atlassian: osquery Security Assessment Exploit Scenario An attacker nds a way to change the time zone on a system in which %Z shows the full time zone name. When the osquery agent attempts to respond to a query on the time table, it triggers undened behavior. Recommendations Short term, add a check to verify the return value of each strftime call made by the table implementation. If the function returns zero, ensure that the system writes a valid string in the buer before it is used as part of the table response. Long term, perform code reviews on new contributions and consider using automated code analysis tools to prevent these kinds of issues from reoccurring. 45 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "17. The elf_info table can crash the osquery agent ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/osquery.pdf", "body": "The elf_info table uses the libelfin library to read properties of ELF les. The library maps the entire ELF binary into virtual memory and then uses memory accesses to read the data. Specically, the load method of the mmap_loader class returns a pointer to the data for a given oset and size. To ensure that the memory access stays within the bounds of the memory-mapped le, the library checks that the result of adding the oset and the size is less than the size of the le. However, this check does not account for the possibility of overows in the addition operation. For example, an oset of 0xffffffffffffffff and a size of 1 would overow to the value 0. This makes it possible to bypass the check and to create references to memory outside of the bounds. The elf_info table indirectly uses this function when loading section headers from an ELF binary. class mmap_loader : public loader { public: void *base; size_t lim; mmap_loader(int fd) { off_t end = lseek(fd, 0, SEEK_END); if (end == (off_t)-1) throw system_error(errno, system_category(), \"finding file length\"); lim = end; base = mmap(nullptr, lim, PROT_READ, MAP_SHARED, fd, 0); if (base == MAP_FAILED) throw system_error(errno, system_category(), \"mmap'ing file\"); close(fd); } ... 46 Atlassian: osquery Security Assessment const void *load(off_t offset, size_t size) { } }; if (offset + size > lim) throw range_error(\"offset exceeds file size\"); return (const char*)base + offset; Figure 17.1: The libenfin librarys limit check does not account for overows. Exploit Scenario An attacker knows of a writable path in which osquery scans ELF binaries. He creates a malformed ELF binary, causing the pointer returned by the vulnerable function to point to an arbitrary location. He uses this to make the osquery agent crash, leak information from the process memory, or circumvent address space layout randomization (ASLR). Recommendations Short term, work with the developers of the libelfbin project to account for overows in the check. Long term, implement the recommendations in TOB-ATL-2 to minimize the impact of similar issues. 47 Atlassian: osquery Security Assessment", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "1. Solidity compiler optimizations can be problematic ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Spool V2 has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed . Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past . A high-severity bug in the emscripten -generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6 . More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe . It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Exploit Scenario A latent or future bug in Solidity compiler optimizationsor in the Emscripten transpilation to solc-js causes a security vulnerability in the Spool V2 contracts. Recommendations Short term, measure the gas savings from optimizations and carefully weigh them against the possibility of an optimization-related bug. Long term, monitor the development and adoption of Solidity compiler optimizations to assess their maturity.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "2. Risk of SmartVaultFactory DoS due to lack of access controls on grantSmartVaultOwnership ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Anyone can set the owner of the next smart vault to be created, which will result in a DoS of the SmartVaultFactory contract. The grantSmartVaultOwnership function in the SpoolAccessControl contract allows anyone to set the owner of a smart vault. This function reverts if an owner is already set for the provided smart vault. function grantSmartVaultOwnership( address smartVault, address owner) external { if (smartVaultOwner[smartVault] != address (0)) { revert SmartVaultOwnerAlreadySet(smartVault); } smartVaultOwner[smartVault] = owner; } Figure 2.1: The grantSmartVaultOwnership function in SpoolAccessControl.sol The SmartVaultFactory contract implements two functions for deploying new smart vaults: the deploySmartVault function uses the create opcode, and the deploySmartVaultDeterministically function uses the create2 opcode. Both functions create a new smart vault and call the grantSmartVaultOwnership function to make the message sender the owner of the newly created smart vault. Any user can pre-compute the address of the new smart vault for a deploySmartVault transaction by using the address and nonce of the SmartVaultFactory contract; to compute the address of the new smart vault for a deploySmartVaultDeterministically transaction, the user could front-run the transaction to capture the salt provided by the user who submitted it. Exploit Scenario Eve pre-computes the address of the new smart vault that will be created by the deploySmartVault function in the SmartVaultFactory contract. She then calls the grantSmartVaultOwnership function with the pre-computed address and a nonzero address as arguments. Now, every call to the deploySmartContract function reverts, making the SmartVaultFactory contract unusable. Using a similar strategy, Eve blocks the deploySmartVaultDeterministically function by front-running the user transaction to set the owner of the smart vault address computed using the user-provided salt. Recommendations Short term, add the onlyRole(ROLE_SMART_VAULT_INTEGRATOR, msg.sender) modier to the grantSmartVaultOwnership function to restrict access to it. Long term, follow the principle of least privilege by restricting access to the functions that grant specic privileges to actors of the system.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "3. Lack of zero-value check on constructors and initializers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Several contracts constructors and initialization functions fail to validate incoming arguments. As a result, important state variables could be set to the zero address, which would result in the loss of assets. constructor ( ISpoolAccessControl accessControl_, IAssetGroupRegistry assetGroupRegistry_, IRiskManager riskManager_, IDepositManager depositManager_, IWithdrawalManager withdrawalManager_, IStrategyRegistry strategyRegistry_, IMasterWallet masterWallet_ , IUsdPriceFeedManager priceFeedManager_, address ghostStrategy ) SpoolAccessControllable(accessControl_) { _assetGroupRegistry = assetGroupRegistry_; _riskManager = riskManager_; _depositManager = depositManager_; _withdrawalManager = withdrawalManager_; _strategyRegistry = strategyRegistry_; _masterWallet = masterWallet_; _priceFeedManager = priceFeedManager_; _ghostStrategy = ghostStrategy; } Figure 3.1: The SmartVaultManager contracts constructor function in spool-v2-core/SmartVaultManager.sol#L111L130 These constructors include that of the SmartVaultManager contract, which sets the _masterWallet address (gure 3.1). SmartVaultManager contract is the entry point of the system and is used by users to deposit their tokens. User deposits are transferred to the _masterWallet address (gure 3.2). function _depositAssets (DepositBag calldata bag) internal returns ( uint256 ) { [...] for ( uint256 i ; i < deposits.length; ++i) { IERC20(tokens[i]).safeTransferFrom( msg.sender , address ( _masterWallet ), deposits[i]); } [...] } Figure 3.2: The _depositAssets function in spool-v2-core/SmartVaultManager.sol#L649L676 If _masterWallet is set to the zero address, the tokens will be transferred to the zero address and will be lost permanently. The constructors and initialization functions of the following contracts also fail to validate incoming arguments:  StrategyRegistry  DepositSwap  SmartVault  SmartVaultFactory  SpoolAccessControllable  DepositManager  RiskManager  SmartVaultManager  WithdrawalManager  RewardManager  RewardPool  Strategy Exploit Scenario Bob deploys the Spool system. During deployment, Bob accidentally sets the _masterWallet parameter of the SmartVaultManager contract to the zero address. Alice, excited about the new protocol, deposits 1 million WETH into it. Her deposited WETH tokens are transferred to the zero address, and Alice loses 1 million WETH. Recommendations Short term, add zero-value checks on all constructor arguments to ensure that the deployer cannot accidentally set incorrect values. Long term, use Slither , which will catch functions that do not have zero-value checks.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "4. Upgradeable contracts set state variables in the constructor ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The state variables set in the constructor of the RewardManager implementation contract are not visible in the proxy contract, making the RewardManager contract unusable. The same issue exists in the RewardPool and Strategy smart contracts. Upgradeable smart contracts using the delegatecall proxy pattern should implement an initializer function to set state variables in the proxy contract storage. The constructor function can be used to set immutable variables in the implementation contract because these variables do not consume storage slots and their values are inlined in the deployed code. The RewardManager contract is deployed as an upgradeable smart contract, but it sets the state variable _assetGroupRegistry in the constructor function. contract RewardManager is IRewardManager, RewardPool, ReentrancyGuard { ... /* ========== STATE VARIABLES ========== */ /// @notice Asset group registry IAssetGroupRegistry private _assetGroupRegistry; ... constructor ( ISpoolAccessControl spoolAccessControl, IAssetGroupRegistry assetGroupRegistry_, bool allowPoolRootUpdates ) RewardPool(spoolAccessControl, allowPoolRootUpdates) { _assetGroupRegistry = assetGroupRegistry_; } Figure 4.1: The constructor function in spool-v2-core/RewardManager.sol The value of the _assetGroupRegistry variable will not be visible in the proxy contract, and the admin will not be able to add reward tokens to smart vaults, making the RewardManager contract unusable. The following smart contracts are also aected by the same issue: 1. The ReentrancyGuard contract, which is non-upgradeable and is extended by RewardManager 2. The RewardPool contract, which sets the state variable allowUpdates in the constructor 3. The Strategy contract, which sets the state variable StrategyName in the constructor Exploit Scenario Bob creates a smart vault and wants to add a reward token to it. He calls the addToken function on the RewardManager contract, but the transaction unexpectedly reverts. Recommendations Short term, make the following changes: 1. Make _assetGroupRegistry an immutable variable in the RewardManager contract. 2. Extend the ReentrancyGuardUpgradeable contract in the RewardManager contract. 3. Make allowUpdates an immutable variable in the RewardPool contract. 4. Move the statement _strategyName = strategyName_; from the Strategy contracts constructor to the contracts __Strategy_init function. 5. Review all of the upgradeable contracts to ensure that they extend only upgradeable library contracts and that the inherited contracts have a __gap storage variable to prevent storage collision issues with future upgrades. Long term, review all of the upgradeable contracts to ensure that they use the initializer function instead of the constructor function to set state variables. Use slither-check-upgradeability to nd issues related to upgradeable smart contracts. 5. Insu\u0000cient validation of oracle price data Severity: Low Diculty: Medium Type: Data Validation Finding ID: TOB-SPL-5 Target: managers/UsdPriceFeedManager.sol", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Incorrect handling of fromVaultsOnly in removeStrategy ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The removeStrategy function allows Spool admins to remove a strategy from the smart vaults using it. Admins are also able to remove the strategy from the StrategyRegistry contract, but only if the value of fromVaultsOnly is false ; however, the implementation enforces the opposite, as shown in gure 6.1. function removeStrategy( address strategy, bool fromVaultsOnly) external { _checkRole(ROLE_SPOOL_ADMIN, msg.sender ); _checkRole(ROLE_STRATEGY, strategy); ... if ( fromVaultsOnly ) { _strategyRegistry.removeStrategy(strategy); } } Figure 6.1: The removeStrategy function in spool-v2-core/SmartVaultManager.sol#L298L317 Exploit Scenario Bob, a Spool admin, calls removeStrategy with fromVaultsOnly set to true , believing that this call will not remove the strategy from the StrategyRegistry contract. However, once the transaction is executed, he discovers that the strategy was indeed removed. Recommendations Short term, replace if (fromVaultsOnly) with if (!fromVaultsOnly) in the removeStrategy function to implement the expected behavior. Long term, improve the systems unit and integration tests to catch issues such as this one.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "7. Risk of LinearAllocationProvider and ExponentialAllocationProvider reverts due to division by zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The LinearAllocationProvider and ExponentialAllocationProvider contracts calculateAllocation function can revert due to a division-by-zero error: LinearAllocationProvider s function reverts when the sum of the strategies APY values is 0 , and ExponentialAllocationProvider s function reverts when a single strategy has an APY value of 0 . Figure 7.1 shows a snippet of the LinearAllocationProvider contracts calculateAllocation function; if the apySum variable, which is the sum of all the strategies APY values, is 0 , a division-by-zero error will occur. uint8 [] memory arrayRiskScores = data.riskScores; for ( uint8 i; i < data.apys.length; ++i) { apySum += (data.apys[i] > 0 ? uint256 (data.apys[i]) : 0); riskSum += arrayRiskScores[i]; } uint8 riskt = uint8 (data.riskTolerance + 10); // from 0 to 20 for ( uint8 i; i < data.apys.length; ++i) { uint256 apy = data.apys[i] > 0 ? uint256 (data.apys[i]) : 0; apy = (apy * FULL_PERCENT) / apySum ; Figure 7.1: Part of the calculateAllocation function in spool-v2-core/LinearAllocationProvider.sol#L39L49 Figure 7.2 shows that for the ExponentialAllocationProvider contracts calculateAllocation function, if the call to log_2 occurs with partApy set to 0 , the function will revert because of log_2 s require statement shown in gure 7.3. for ( uint8 i; i < data.apys.length; ++i) { uint256 uintApy = (data.apys[i] > 0 ? uint256 (data.apys[i]) : 0); int256 partRiskTolerance = fromUint( uint256 (riskArray[ uint8 (20 - riskt)])); partRiskTolerance = div(partRiskTolerance, _100); int256 partApy = fromUint(uintApy); partApy = div(partApy, _100); int256 apy = exp_2(mul(partRiskTolerance, log_2(partApy) )); Figure 7.2: Part of the calculateAllocation function in spool-v2-core/ExponentialAllocationProvider.sol#L323L331 function log_2( int256 x) internal pure returns ( int256 ) { unchecked { require (x > 0); Figure 7.3: Part of the log_2 function in spool-v2-core/ExponentialAllocationProvider.sol#L32L34 Exploit Scenario Bob deploys a smart vault with two strategies using the ExponentialAllocationProvider contract. At some point, one of the strategies has 0 APY, causing the transaction call to reallocate the assets to unexpectedly revert. Recommendations Short term, modify both versions of the calculateAllocation function so that they correctly handle cases in which a strategys APY is 0 . Long term, improve the systems unit and integration tests to ensure that the basic operations work as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "8. Strategy APYs are never updated ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The _updateDhwYieldAndApy function is never called. As a result, each strategys APY will constantly be set to 0 . function _updateDhwYieldAndApy( address strategy, uint256 dhwIndex, int256 yieldPercentage) internal { if (dhwIndex > 1) { unchecked { _stateAtDhw[address(strategy)][dhwIndex - 1].timestamp); int256 timeDelta = int256 (block.timestamp - if (timeDelta > 0) { timeDelta; int256 normalizedApy = yieldPercentage * SECONDS_IN_YEAR_INT / int256 weight = _getRunningAverageApyWeight(timeDelta); _apys[strategy] = (_apys[strategy] * (FULL_PERCENT_INT - weight) + normalizedApy * weight) / FULL_PERCENT_INT; } } } } Figure 8.1: The _updateDhwYieldAndApy function in spool-v2-core/StrategyManager.sol#L298L317 A strategys APY is one of the parameters used by an allocator provider to decide where to allocate the assets of a smart vault. If a strategys APY is 0 , the LinearAllocationProvider and ExponentialAllocationProvider contracts will both revert when calculateAllocation is called due to a division-by-zero error. // set allocation if (uint16a16.unwrap(allocations) == 0) { _riskManager.setRiskProvider(smartVaultAddress, specification.riskProvider); _riskManager.setRiskTolerance(smartVaultAddress, specification.riskTolerance); _riskManager.setAllocationProvider(smartVaultAddress, specification.allocationProvider); allocations = _riskManager.calculateAllocation(smartVaultAddress, specification.strategies); } Figure 8.2: Part of the _integrateSmartVault function, which is called when a vault is created, in spool-v2-core/SmartVaultFactory.sol#L313L3 20 When a vault is created, the code in gure 8.2 is executed. For vaults whose strategyAllocation variable is set to 0 , which means the value will be calculated by the smart contract, and whose allocationProvide r variable is set to the LinearAllocationProvider or ExponentialAllocationProvider contract, the creation transaction will revert due to a division-by-zero error. Transactions for creating vaults with a nonzero strategyAllocation and with the same allocationProvider values mentioned above will succeed; however, the fund reallocation operation will revert because the _updateDhwYieldAndApy function is never called, causing the strategies APYs to be set to 0 , in turn causing the same division-by-zero error. Refer to nding TOB-SPL-7 , which is related to this issue; even if that nding is xed, incorrect results would still occur because of the missing _updateDhwYieldAndApy calls. Exploit Scenario Bob tries to deploy a smart vault with strategyAllocation set to 0 and allocationProvide r set to LinearAllocationProvider . The transaction unexpectedly fails. Recommendations Short term, add calls to _updateDhwYieldAndApy where appropriate. Long term, improve the systems unit and integration tests to ensure that the basic operations work as expected.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "9. Incorrect bookkeeping of assets deposited into smart vaults ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Assets deposited by users into smart vaults are incorrectly tracked. As a result, assets deposited into a smart vaults strategies when the flushSmartVault function is invoked correspond to the last deposit instead of the sum of all deposits into the strategies. When depositing assets into a smart vault, users can decide whether to invoke the flushSmartVault function. A smart vault ush is a synchronization process that makes deposited funds available to be deployed into the strategies and makes withdrawn funds available to be withdrawn from the strategies. However, the internal bookkeeping of deposits keeps track of only the last deposit of the current ush cycle instead of the sum of all deposits (gure 9.1). function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns ( uint256 [] memory , uint256 ) { ... // transfer tokens from user to master wallet for ( uint256 i; i < bag2.tokens.length; ++i) { _vaultDeposits[bag.smartVault][bag2.flushIndex][i] = bag.assets[i]; } ... Figure 9.1: A snippet of the depositAssets function in spool-v2-core/DepositManager.sol#L379L439 The _vaultDeposits variable is then used to calculate the asset distribution in the flushSmartVault function. function flushSmartVault( address smartVault, uint256 flushIndex, address [] calldata strategies, uint16a16 allocation, address [] calldata tokens ) external returns (uint16a16) { _checkRole(ROLE_SMART_VAULT_MANAGER, msg.sender ); if (_vaultDeposits[smartVault][flushIndex][0] == 0) { return uint16a16.wrap(0); } // handle deposits uint256 [] memory exchangeRates = SpoolUtils.getExchangeRates(tokens, _priceFeedManager); _flushExchangeRates[smartVault][flushIndex].setValues(exchangeRates); uint256 [][] memory distribution = distributeDeposit ( DepositQueryBag1({ deposit: _vaultDeposits[smartVault][flushIndex].toArray(tokens.length) , exchangeRates: exchangeRates, allocation: allocation, strategyRatios: SpoolUtils.getStrategyRatiosAtLastDhw(strategies, _strategyRegistry) }) ); ... return _strategyRegistry.addDeposits(strategies, distribution) ; } Figure 9.2: A snippet of the flushSmartVault function in spool-v2-core/DepositManager.sol#L188L 226 Lastly, the _strategyRegistry.addDeposits function is called with the computed distribution, which adds the amounts to deploy in the next doHardWork function call in the _assetsDeposited variable (gure 9.3). function addDeposits( address [] calldata strategies_, uint256 [][] calldata amounts) { external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns (uint16a16) uint16a16 indexes; for ( uint256 i; i < strategies_.length; ++i) { address strategy = strategies_[i]; uint256 latestIndex = _currentIndexes[strategy]; indexes = indexes.set(i, latestIndex); for ( uint256 j = 0; j < amounts[i].length; j++) { _assetsDeposited[strategy][latestIndex][j] += amounts[i][j]; } } return indexes; } Figure 9.3: The addDeposits function in spool-v2-core/StrategyRegistry.sol#L343L361 The next time the doHardWork function is called, it will transfer the equivalent of the last deposits amount instead of the sum of all deposits from the master wallet to the assigned strategy (gure 9.4). function doHardWork(DoHardWorkParameterBag calldata dhwParams) external whenNotPaused { ... // Transfer deposited assets to the strategy. for ( uint256 k; k < assetGroup.length; ++k) { if (_assetsDeposited[strategy][dhwIndex][k] > 0) { _masterWallet.transfer( IERC20(assetGroup[k]), strategy, _assetsDeposited[strategy][dhwIndex][k] ); } } ... Figure 9.4: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L222L3 41 Exploit Scenario Bob deploys a smart vault. One hundred deposits are made before a smart vault ush is invoked, but only the last deposits assets are deployed to the underlying strategies, severely impacting the smart vaults performance. Recommendations Short term, modify the depositAssets function so that it correctly tracks all deposits within a ush cycle, rather than just the last deposit. Long term, improve the systems unit and integration tests: test a smart vault with a single strategy and multiple strategies to ensure that smart vaults behave correctly when funds are deposited and deployed to the underlying strategies.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "10. Risk of malformed calldata of calls to guard contracts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The GuardManager contract does not pad custom values while constructing the calldata for calls to guard contracts. The calldata could be malformed, causing the aected guard contract to give incorrect results or to always revert calls. Guards for vaults are customizable checks that are executed on every user action. The result of a guard contract either approves or disapproves user actions. The GuardManager contract handles the logic to call guard contracts and to check their results (gure 10.1). function runGuards( address smartVaultId , RequestContext calldata context) external view { [...] bytes memory encoded = _encodeFunctionCall(smartVaultId, guard , context); ( bool success , bytes memory data) = guard.contractAddress.staticcall(encoded) ; _checkResult (success, data, guard.operator, guard.expectedValue, i); } } Figure 10.1: The runGuards function in spool-v2-core/GuardManager.sol#L19L33 The arguments of the runGuards function include information related to the given user action and custom values dened at the time of guard denition. The GuardManager.setGuards function initializes the guards in the GuardManager contract. Using the guard denition, the GuardManager contract manually constructs the calldata with the selected values from the user action information and the custom values (gure 10.2). function _encodeFunctionCall ( address smartVaultId , GuardDefinition memory guard, RequestContext memory context) internal pure returns ( bytes memory ) { [...] result = bytes .concat(result, methodID ); for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode(paramsEndLoc)); paramsEndLoc += 32 + guard.methodParamValues[customValueIdx].length ; customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } [...] } customValueIdx = 0 ; for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode(guard.methodParamValues[customValueIdx].length / 32 )); result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { customValueIdx++; } [...] } return result; } Figure 10.2: The _encodeFunctionCall function in spool-v2-core/GuardManager.sol#L111L177 However, the contract concatenates the custom values without considering their lengths and required padding. If these custom values are not properly padded at the time of guard initialization, the call will receive malformed data. As a result, either of the following could happen: 1. Every call to the guard contract will always fail, and user action transactions will always revert. The smart vault using the guard will become unusable. 2. The guard contract will receive incorrect arguments and return incorrect results. Invalid user actions could be approved, and valid user actions could be rejected. Exploit Scenario Bob deploys a smart vault and creates a guard for it. The guard contract takes only one custom value as an argument. Bob created the guard denition in GuardManager without padding the custom value. Alice tries to deposit into the smart vault, and the guard contract is called for her action. The call to the guard contract fails, and the transaction reverts. The smart vault is unusable. Recommendations Short term, modify the associated code so that it veries that custom values are properly padded before guard denitions are initialized in GuardManager.setGuards . Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly. Additionally, improve the user documentation with necessary technical details to properly use the system.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "11. GuardManager does not account for all possible types when encoding guard arguments ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "While encoding arguments for guard contracts, the GuardManager contract assumes that all static types are encoded to 32 bytes. This assumption does not hold for xed-size static arrays and structs with only static type members. As a result, guard contracts could receive incorrect arguments, leading to unintended behavior. The GuardManager._encodeFunctionCall function manually encodes arguments to call guard contracts (gure 11.1). function _encodeFunctionCall ( address smartVaultId , GuardDefinition memory guard, RequestContext memory context) internal pure returns ( bytes memory ) { bytes4 methodID = bytes4 ( keccak256 (abi.encodePacked(guard.methodSignature))); uint256 paramsLength = guard.methodParamTypes.length ; bytes memory result = new bytes ( 0 ); result = bytes .concat(result, methodID); uint16 customValueIdx = 0 ; uint256 paramsEndLoc = paramsLength * 32 ; // Loop through parameters and // - store values for simple types // - store param value location for dynamic types for ( uint256 i ; i < paramsLength; ++i) { GuardParamType paramType = guard.methodParamTypes[i]; if (paramType == GuardParamType.DynamicCustomValue) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + guard.methodParamValues[customValueIdx].length; customValueIdx++; } else if (paramType == GuardParamType.CustomValue) { result = bytes .concat(result, guard.methodParamValues[customValueIdx]); customValueIdx++; } [...] } else if (paramType == GuardParamType.Assets) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + context.assets.length * 32 ; } else if (paramType == GuardParamType.Tokens) { result = bytes .concat(result, abi.encode( paramsEndLoc )); paramsEndLoc += 32 + context.tokens.length * 32 ; } else { revert InvalidGuardParamType( uint256 (paramType)); } } [...] return result; } Figure 11.1: The _encodeFunctionCall function in spool-v2-core/GuardManager.sol#L111L177 The function calculates the oset for dynamic type arguments assuming that every parameter, static or dynamic, takes exactly 32 bytes. However, xed-length static type arrays and structs with only static type members are considered static. All static type values are encoded in-place, and static arrays and static structs could take more than 32 bytes. As a result, the calculated oset for the start of dynamic type arguments could be wrong, which would cause incorrect values for these arguments to be set, resulting in unintended behavior. For example, the guard could approve invalid user actions and reject valid user actions or revert every call. Exploit Scenario Bob deploys a smart vault and creates a guard contract that takes the custom value of a xed-length static array type. The guard contract uses RequestContext assets. Bob correctly creates the guard denition in GuardManager , but the GuardManager._encodeFunctionCall function incorrectly encodes the arguments. The guard contract fails to decode the arguments and always reverts the execution. Recommendations Short term, modify the GuardManager._encodeFunctionCall function so that it considers the encoding length of the individual parameters and calculates the osets correctly. Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "12. Use of encoded values in guard contract comparisons could lead to opposite results ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The GuardManager contract compares the return value of a guard contract to an expected value. However, the contract uses encoded versions of these values in the comparison, which could lead to incorrect results for signed values with numerical comparison operators. The GuardManager contract calls the guard contract and validates the return value using the GuardManager._checkResult function (gure 12.1). function _checkResult ( bool success , bytes memory returnValue, bytes2 operator, bytes32 value , uint256 guardNum ) internal pure { if (!success) revert GuardError(); bool result = true ; if (operator == bytes2( \"==\" )) { result = abi.decode(returnValue, ( bytes32 )) == value; } else if (operator == bytes2( \"<=\" )) { result = abi.decode(returnValue, ( bytes32 )) <= value; } else if (operator == bytes2( \">=\" )) { result = abi.decode(returnValue, ( bytes32 )) >= value; } else if (operator == bytes2( \"<\" )) { result = abi.decode(returnValue, ( bytes32 )) < value; } else if (operator == bytes2( \">\" )) { result = abi.decode(returnValue, ( bytes32 )) > value; } else { result = abi.decode(returnValue, ( bool )); } if (!result) revert GuardFailed(guardNum); } Figure 12.1: The _checkResult function in spool-v2-core/GuardManager.sol#L80L105 When a smart vault creator denes a guard using the GuardManager.setGuards function, they dene a comparison operator and the expected value, which the GuardManager contract uses to compare with the return value of the guard contract. The comparison is performed on the rst 32 bytes of the ABI-encoded return value and the expected value, which will cause issues depending on the return value type. First, the numerical comparison operators ( < , > , <= , >= ) are not well dened for bytes32 ; therefore, the contract treats encoded values with padding as uint256 values before comparing them. This way of comparing values gives incorrect results for negative values of the int<M> type. The Solidity documentation includes the following description about the encoding of int<M> type values: int<M>: enc(X) is the big-endian twos complement encoding of X, padded on the higher-order (left) side with 0x bytes for negative X and with zero-bytes for non-negative X such that the length is 32 bytes. Figure 12.2: A description about the encoding of int<M> type values in the Solidity documentation Because negative values are padded with 0xff and positive values with 0x00 , the encoded negative values will be considered greater than the encoded positive values. As a result, the result of the comparison will be the opposite of the expected result. Second, only the rst 32 bytes of the return value are considered for comparison. This will lead to inaccurate results for return types that use more than 32 bytes to encode the value. Exploit Scenario Bob deploys a smart vault and intends to allow only users who own B NFTs to use it. B NFTs are implemented using ERC-1155. Bob uses the B contract as a guard with the comparison operator > and an expected value of 0 . Bob calls the function B.balanceOfBatch to fetch the NFT balance of the user. B.balanceOfBatch returns uint256[] . The rst 32 bytes of the return data contain the oset into the return data, which is always nonzero. The comparison passes for every user regardless of whether they own a B NFT. As a result, every user can use Bobs smart vault. Recommendations Short term, restrict the return value of a guard contract to a Boolean value. If that is not possible, document the limitations and risks surrounding the guard contracts. Additionally, consider manually checking new action guards with respect to these limitations. Long term, avoid implementing low-level manipulations. If such implementations are unavoidable, carefully review the Solidity documentation before implementing them to ensure that they are implemented correctly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "13. Lack of contract existence checks on low-level calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The GuardManager and Swapper contracts use low-level calls without contract existence checks. If the target address is incorrect or the contract at that address is destroyed, a low-level call will still return success. The Swapper.swap function uses the address().call(...) function to swap tokens (gure 13.1). function swap ( address [] calldata tokensIn, SwapInfo[] calldata swapInfo, address [] calldata tokensOut, address receiver ) external returns ( uint256 [] memory tokenAmounts) { // Perform the swaps. for ( uint256 i ; i < swapInfo.length; ++i) { if (!exchangeAllowlist[swapInfo[i].swapTarget]) { revert ExchangeNotAllowed(swapInfo[i].swapTarget); } _approveMax(IERC20(swapInfo[i].token), swapInfo[i].swapTarget); ( bool success , bytes memory data) = swapInfo[i].swapTarget.call(swapInfo[i].swapCallData); if (!success) revert (SpoolUtils.getRevertMsg(data)); } // Return unswapped tokens. for ( uint256 i ; i < tokensIn.length; ++i) { uint256 tokenInBalance = IERC20(tokensIn[i]).balanceOf( address ( this )); if (tokenInBalance > 0 ) { IERC20(tokensIn[i]).safeTransfer(receiver, tokenInBalance); } } Figure 13.1: The swap function in spool-v2-core/Swapper.sol#L29L45 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their rst return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 13.2: The Solidity documentation details the necessity of executing existence checks before performing low-level calls. Therefore, if the swapTarget address is incorrect or the target contract has been destroyed, the execution will not revert even if the swap is not successful. We rated this nding as only a low-severity issue because the Swapper contract transfers the unswapped tokens to the receiver if a swap is not successful. However, the CompoundV2Strategy contract uses the Swapper contract to exchange COMP tokens for underlying tokens (gure 13.3). function _compound( address [] calldata tokens, SwapInfo[] calldata swapInfo, uint256 [] calldata ) internal override returns ( int256 compoundedYieldPercentage) { if (swapInfo.length > 0) { address [] memory markets = new address [](1); markets[0] = address (cToken); comptroller.claimComp( address ( this ), markets); uint256 compBalance = comp.balanceOf(address(this)); if (compBalance > 0) { comp.safeTransfer(address(swapper), compBalance); address [] memory tokensIn = new address [](1); tokensIn[0] = address(comp); uint256 swappedAmount = swapper.swap(tokensIn, swapInfo, tokens, address(this))[0]; if ( swappedAmount > 0) { uint256 cTokenBalanceBefore = cToken.balanceOf( address ( this )); _depositToCompoundProtocol (IERC20(tokens[0]), swappedAmount); uint256 cTokenAmountCompounded = cToken.balanceOf( address ( this )) - cTokenBalanceBefore; _calculateYieldPercentage(cTokenBalanceBefore, cTokenAmountCompounded); compoundedYieldPercentage = } } } } Figure 13.3: The _compound function in spool-v2-core/CompoundV2Strategy.sol If the swap operation fails, the COMP will stay in CompoundV2Strategy . This will cause users to lose the yield they would have gotten from compounding. Because the swap operation fails silently, the do hard worker may not notice that yield is not compounding. As a result, users will receive less in prot than they otherwise would have. The GuardManager.runGuards function, which uses the address().staticcall() function, is also aected by this issue. However, the return value of the call is decoded, so the calls would not fail silently. Exploit Scenario The Spool team deploys CompoundV2Strategy with a market that gives COMP tokens to its users. While executing the doHardWork function for smart vaults using CompoundV2Strategy , the do hard worker sets the swapTarget address to an incorrect address. The swap operation to exchange COMP to the underlying token fails silently. The gained yield is not deposited into the market. The users receive less in prot. Recommendations Short term, implement a contract existence check before the low-level calls in GuardManager.runGuards and Swapper.swap . Long term, avoid implementing low-level calls. If such calls are unavoidable, carefully review the Solidity documentation , particularly the Warnings section, before implementing them to ensure that they are implemented correctly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "14. Incorrect use of exchangeRates in doHardWork ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The StrategyRegistry contracts doHardWork function fetches the exchangeRates for all of the tokens involved in the do hard work process, and then it iterates over the strategies and saves the exchangeRates values for the current strategys tokens in the assetGroupExchangeRates variable; however, when doHardWork is called for a strategy, the exchangeRates variable rather than the assetGroupExchangeRates variable is passed, resulting in the use of incorrect exchange rates. function doHardWork(DoHardWorkParameterBag calldata dhwParams) external whenNotPaused { ... // Get exchange rates for tokens and validate them against slippages. uint256 [] memory exchangeRates = SpoolUtils.getExchangeRates(dhwParams.tokens, _priceFeedManager); for ( uint256 i; i < dhwParams.tokens.length; ++i) { if ( exchangeRates[i] < dhwParams.exchangeRateSlippages[i][0] || exchangeRates[i] > dhwParams.exchangeRateSlippages[i][1] revert ExchangeRateOutOfSlippages(); ) { } } ... // Get exchange rates for this group of strategies. uint256 assetGroupId = IStrategy(dhwParams.strategies[i][0]).assetGroupId(); address [] memory assetGroup = IStrategy(dhwParams.strategies[i][0]).assets(); uint256 [] memory assetGroupExchangeRates = new uint256 [](assetGroup.length); for (uint256 j; j < assetGroup.length; ++j) { bool found = false ; for ( uint256 k; k < dhwParams.tokens.length; ++k) { if (assetGroup[j] == dhwParams.tokens[k]) { assetGroupExchangeRates[j] = exchangeRates[k]; found = true ; break ; } } ... // Do the hard work on the strategy. DhwInfo memory dhwInfo = IStrategy(strategy).doHardWork( StrategyDhwParameterBag({ swapInfo: dhwParams.swapInfo[i][j], compoundSwapInfo: dhwParams.compoundSwapInfo[i][j], slippages: dhwParams.strategySlippages[i][j], assetGroup: assetGroup, exchangeRates: exchangeRates , withdrawnShares: _sharesRedeemed[strategy][dhwIndex], masterWallet: address(_masterWallet), priceFeedManager: _priceFeedManager, baseYield: dhwParams.baseYields[i][j], platformFees: platformFeesMemory }) ); // Bookkeeping. _dhwAssetRatios[strategy] = IStrategy(strategy).assetRatio(); _exchangeRates[strategy][dhwIndex].setValues( exchangeRates ); ... Figure 14.1: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L222L 341 The exchangeRates values are used by a strategys doHardWork function to calculate how many assets in USD value are to be deposited and how many in USD value are currently deposited in the strategy. As a consequence of using exchangeRates rather than assetGroupExchangeRates , the contract will return incorrect values. Additionally, the _exchangeRates variable is returned by the strategyAtIndexBatch function, which is used when simulating deposits. Exploit Scenario Bob deploys a smart vault, and users start depositing into it. However, the rst time doHardWork is called, they notice that the deposited assets and the reported USD value deposited into the strategies are incorrect. They panic and start withdrawing all of the funds. Recommendations Short term, replace exchangeRates with assetGroupExchangeRates in the relevant areas of doHardWork and where it sets the _exchangeRates variable. Long term, improve the systems unit and integration tests to verify that the deposited value in a strategy is the expected amount. Additionally, when reviewing the code, look for local variables that are set but then never used; this is a warning sign that problems may arise.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "15. LinearAllocationProvider could return an incorrect result ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The LinearAllocationProvider contract returns an incorrect result when the given smart vault has a riskTolerance value of -8 due to an incorrect literal value in the riskArray variable. function calculateAllocation(AllocationCalculationInput calldata data) external pure returns ( uint256 [] memory ) { ... uint24 [21] memory riskArray = [ 100000, 95000, 900000 , ... ]; ... uint8 riskt = uint8 (data.riskTolerance + 10); // from 0 to 20 for ( uint8 i; i < data.apys.length; ++i) { ... results[i] = apy * riskArray[ uint8 (20 - riskt)] + risk * riskArray[ uint8 (riskt)] ; resSum += results[i]; } uint256 resSum2; for ( uint8 i; i < results.length; ++i) { results[i] = FULL_PERCENT * results[i] / resSum; resSum2 += results[i]; } results[0] += FULL_PERCENT - resSum2; return results; Figure 15.1: A snippet of the calculateAllocation function in spool-v2-core/LinearAllocationProvider.sol#L9L67 The riskArray s third element is incorrect; this aects the computed allocation for smart vaults that have a riskTolerance value of -8 because the riskt variable would be 2 , which is later used as index for the riskArray . The subexpression risk * riskArray[uint8(rikst)] is incorrect by a factor of 10. Exploit Scenario Bob deploys a smart vault with a riskTolerance value of -8 and an empty strategyAllocation value. The allocation between the strategies is computed on the spot using the LinearAllocationProvider contract, but the allocation is wrong. Recommendations Short term, replace 900000 with 90000 in the calculateAllocation function. Long term, improve the systems unit and integration tests to catch issues such as this. Document the use and meaning of constants such as the values in riskArray . This will make it more likely that the Spool team will nd these types of mistakes.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "16. Incorrect formula used for adding/subtracting two yields ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The doHardWork function adds two yields with dierent base values to compute the given strategys total yield, which results in the collection of fewer ecosystem fees and treasury fees. It is incorrect to add two yields that have dierent base values. The correct formula to compute the total yield from two consecutive yields Y1 and Y2 is Y1 + Y2 + (Y1*Y2) . The doHardWork function in the Strategy contract adds the protocol yield and the rewards yield to calculate the given strategys total yield. The protocol yield percentage is calculated with the base value of the strategys total assets at the start of the current do hard work cycle, while the rewards yield percentage is calculated with the base value of the total assets currently owned by the strategy. dhwInfo.yieldPercentage = _getYieldPercentage(dhwParams.baseYield); dhwInfo.yieldPercentage += _compound(dhwParams.assetGroup, dhwParams.compoundSwapInfo, dhwParams.slippages); Figure 16.1: A snippet of the doHardWork function in spool-v2-core/Strategy.sol#L95L96 Therefore, the total yield of the strategy is computed as less than its actual yield, and the use of this value to compute fees results in the collection of fewer fees for the platforms governance system. Same issue also aects the computation of the total yield of a strategy on every do hard work cycle: _stateAtDhw[strategy][dhwIndex] = StateAtDhwIndex({ sharesMinted: uint128 (dhwInfo.sharesMinted), totalStrategyValue: uint128 (dhwInfo.valueAtDhw), totalSSTs: uint128 (dhwInfo.totalSstsAtDhw), yield: int96 (dhwInfo.yieldPercentage) + _stateAtDhw[strategy][dhwIndex - 1].yield, // accumulate the yield from before timestamp: uint32 (block.timestamp) }); Figure 16.2: A snippet of the doHardWork function in spool-v2-core/StrategyRegistry.sol#L331L337 This value of the total yield of a strategy is used to calculate the management fees for a given smart vault, which results in fewer fees paid to the smart vault owner. Exploit Scenario The Spool team deploys the system. Alice deposits 1,000 tokens into a vault, which mints 1,000 strategy share tokens for the vault. On the next do hard work execution, the tokens earn 8% yield and 30 reward tokens from the protocol. The 30 reward tokens are then exchanged for 20 deposit tokens. At this point, the total tokens earned by the strategy are 100 and the total yield is 10%. However, the doHardWork function computes the total yield as 9.85%, which is incorrect, resulting in fewer fees collected for the platform. Recommendations Short term, use the correct formula to calculate a given strategys total yield in both the Strategy contract and the StrategyRegistry contract. Note that the syncDepositsSimulate function subtracts a strategys total yield at dierent do hard work indexes in DepositManager.sol#L322L326 to compute the dierence between the strategys yields between two do hard work cycles. After xing this issue, this functions computation will be incorrect. Long term, review the entire codebase to nd all of the mathematical formulas used. Document these formulas, their assumptions, and their derivations to avoid the use of incorrect formulas.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "17. Smart vaults with re-registered strategies will not be usable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The StrategyRegistry contract does not clear the state related to a strategy when removing it. As a result, if the removed strategy is registered again, the StrategyRegistry contract will still contain the strategys previous state, resulting in a temporary DoS of the smart vaults using it. The StrategyRegistry.registerStrategy function is used to register a strategy and to initialize the state related to it (gure 17.1). StrategyRegistry tracks the state of the strategies by using their address. function registerStrategy ( address strategy ) external { _checkRole(ROLE_SPOOL_ADMIN, msg.sender ); if (_accessControl.hasRole(ROLE_STRATEGY, strategy)) revert StrategyAlreadyRegistered({address_: strategy}); _accessControl.grantRole(ROLE_STRATEGY, strategy); _currentIndexes[strategy] = 1 ; _dhwAssetRatios[strategy] = IStrategy(strategy).assetRatio(); _stateAtDhw[ address (strategy)][ 0 ].timestamp = uint32 ( block.timestamp ); } Figure 17.1: The registerStrategy function in spool-v2-core/StrategyRegistry.sol The StrategyRegistry._removeStrategy function is used to remove a strategy by revoking its ROLE_STRATEGY role. function _removeStrategy ( address strategy ) private { if (!_accessControl.hasRole(ROLE_STRATEGY, strategy)) revert InvalidStrategy({address_: strategy}); _accessControl.revokeRole(ROLE_STRATEGY, strategy); } Figure 17.2: The _removeStrategy function in spool-v2-core/StrategyRegistry.sol While removing a strategy, StrategyRegistry contract does not remove the state related to that strategy. As a result, when that strategy is registered again, StrategyRegistry will contain values from the previous period. This could make the smart vaults using the strategy unusable or cause the unintended transfer of assets between other strategies and this strategy. Exploit Scenario Strategy S is registered. StrategyRegistry._currentIndex[S] is equal to 1 . Alice creates a smart vault X that uses strategy S. Bob deposits 1 million WETH into smart vault X. StrategyRegistry._assetsDeposited[S][1][WETH] is equal to 1 million WETH. The doHardWork function is called for strategy S. WETH is transferred from the master wallet to strategy S and is deposited into the protocol. A Spool system admin removes strategy S upon hearing that the protocol is being exploited. However, the admin realizes that the protocol is not being exploited and re-registers strategy S. StrategyRegistry._currentIndex[S] is set to 1 . StrategyRegistry._assetsDeposited[S][1][WETH] is not set to zero and is still equal to 1 million WETH. Alice creates a new vault with strategy S. When doHardWork is called for strategy S, StrategyRegistry tries to transfer 1 million WETH to the strategy. The master wallet does not have those assets, so doHardWork fails for strategy S. The smart vault becomes unusable. Recommendations Short term, modify the StrategyRegistry._removeStrategy function so that it clears states related to removed strategies if re-registering strategies is an intended use case. If this is not an intended use case, modify the StrategyRegistry.registerStrategy function so that it veries that newly registered strategies have not been previously registered. Long term, properly document all intended use cases of the system and implement comprehensive tests to ensure that the system behaves as expected.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "18. Incorrect handling of partially burned NFTs results in incorrect SVT balance calculation ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The SmartVault._afterTokenTransfer function removes the given NFT ID from the SmartVault._activeUserNFTIds array even if only a fraction of it is burned. As a result, the SmartVaultManager.getUserSVTBalance function, which uses SmartVault._activeUserNFTIds , will show less than the given users actual balance. SmartVault._afterTokenTransfer is executed after every token transfer (gure 18.1). function _afterTokenTransfer ( address , address from , address to , uint256 [] memory ids, uint256 [] memory , bytes memory ) internal override { // burn if (to == address ( 0 )) { uint256 count = _activeUserNFTCount[from]; for ( uint256 i ; i < ids.length; ++i) { for ( uint256 j = 0 ; j < count; j++) { if (_activeUserNFTIds[from][j] == ids[i]) { _activeUserNFTIds[from][j] = _activeUserNFTIds[from][count - 1 ]; count--; break ; } } } _activeUserNFTCount[from] = count; return ; } [...] } Figure 18.1: A snippet of the _afterTokenTransfer function in spool-v2-core/SmartVault.sol It removes the burned NFT from _activeUserNFTIds . However, it does not consider the amount of the NFT that was burned. As a result, NFTs that are not completely burned will not be considered active by the vault. SmartVaultManager.getUserSVTBalance uses SmartVault._activeUserNFTIds to calculate a given users SVT balance (gure 18.2). function getUserSVTBalance ( address smartVaultAddress , address userAddress ) external view returns ( uint256 ) { if (_accessControl.smartVaultOwner(smartVaultAddress) == userAddress) { (, uint256 ownerSVTs ,, uint256 fees ) = _simulateSync(smartVaultAddress); return ownerSVTs + fees; } uint256 currentBalance = ISmartVault(smartVaultAddress).balanceOf(userAddress); uint256 [] memory nftIds = ISmartVault(smartVaultAddress).activeUserNFTIds(userAddress); if (nftIds.length > 0 ) { currentBalance += _simulateNFTBurn(smartVaultAddress, userAddress, nftIds); } return currentBalance; } Figure 18.2: The getUserSVTBalance function in spool-v2-core/SmartVaultManager.sol Because partial NFTs are not present in SmartVault._activeUserNFTIds , the calculated balance will be less than the users actual balance. The front end using getUserSVTBalance will show incorrect balances to users. Exploit Scenario Alice deposits assets into a smart vault and receives a D-NFT. Alice's assets are deposited to the protocols after doHardWork is called. Alice claims SVTs by burning a fraction of her D-NFT. The smart vault removes the D-NFT from _activeUserNFTIds . Alice checks her SVT balance and panics when she sees less than what she expected. She withdraws all of her assets from the system. Recommendations Short term, add a check to the _afterTokenTransfer function so that it checks the balance of the NFT that is burned and removes the NFT from _activeUserNFTIds only when the NFT is burned completely. Long term, improve the systems unit and integration tests to extensively test view functions.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "19. Transfers of D-NFTs result in double counting of SVT balance ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The _activeUserNFTIds and _activeUserNFTCount variables are not updated for the sender account on the transfer of NFTs. As a result, SVTs for transferred NFTs will be counted twice, causing the system to show an incorrect SVT balance. The _afterTokenTransfer hook in the SmartVault contract is executed after every token transfer to update information about users active NFTs: function _afterTokenTransfer ( address , address from , address to , uint256 [] memory ids, uint256 [] memory , bytes memory ) internal override { // burn if (to == address ( 0 )) { ... return ; } // mint or transfer for ( uint256 i; i < ids.length; ++i) { _activeUserNFTIds[to][_activeUserNFTCount[to]] = ids[i]; _activeUserNFTCount[to]++; } } Figure 19.1: A snippet of the _afterTokenTransfer function in spool-v2-core/SmartVault.sol When a user transfers an NFT to another user, the function adds the NFT ID to the active NFT IDs of the receivers account but does not remove the ID from the active NFT IDs of the senders account. Additionally, the active NFT count is not updated for the senders account. The getUserSVTBalance function of the SmartVaultManager contract uses the SmartVault contracts _activeUserNFTIds array to calculate a given users SVT balance: function getUserSVTBalance ( address smartVaultAddress , address userAddress ) external view returns ( uint256 ) { if (_accessControl.smartVaultOwner(smartVaultAddress) == userAddress) { (, uint256 ownerSVTs ,, uint256 fees ) = _simulateSync(smartVaultAddress); return ownerSVTs + fees; } uint256 currentBalance = ISmartVault(smartVaultAddress).balanceOf(userAddress); uint256 [] memory nftIds = ISmartVault(smartVaultAddress).activeUserNFTIds(userAddress); if (nftIds.length > 0 ) { currentBalance += _simulateNFTBurn(smartVaultAddress, userAddress, nftIds); } return currentBalance; } Figure 19.2: The getUserSVTBalance function in spool-v2-core/SmartVaultManager.sol Because transferred NFT IDs are active for both senders and receivers, the SVTs corresponding to the NFT IDs will be counted for both users. This double counting will keep increasing the SVT balance for users with every transfer, causing an incorrect balance to be shown to users and third-party integrators. Exploit Scenario Alice deposits assets into a smart vault and receives a D-NFT. Alice's assets are deposited into the protocols after doHardWork is called. Alice transfers the D-NFT to herself. The SmartVault contract adds the D-NFT ID to _activeUserNFTIds for Alice again. Alice checks her SVT balance and sees double the balance she had before. Recommendations Short term, modify the _afterTokenTransfer function so that it removes NFT IDs from the active NFT IDs for the senders account when users transfer D-NFTs and W-NFTs. Long term, add unit test cases for all possible user interactions to catch issues such as this.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "20. Flawed loop for syncing ushes results in higher management fees ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The loop used to sync ush indexes in the SmartVaultManager contract computes an inated value of the oldTotalSVTs variable, which results in higher management fees paid to the smart vault owner. The _syncSmartVault function in the SmartVaultManager contract implements a loop to process every ush index from flushIndex.toSync to flushIndex.current : while (flushIndex.toSync < flushIndex.current) { ... DepositSyncResult memory syncResult = _depositManager.syncDeposits( smartVault, [flushIndex.toSync, bag.lastDhwSynced, bag.oldTotalSVTs], strategies_, [indexes, _getPreviousDhwIndexes(smartVault, flushIndex.toSync)], tokens, bag.fees ); bag.newSVTs += syncResult.mintedSVTs; bag.feeSVTs += syncResult.feeSVTs; bag.oldTotalSVTs += bag.newSVTs; bag.lastDhwSynced = syncResult.dhwTimestamp; emit SmartVaultSynced(smartVault, flushIndex.toSync); flushIndex.toSync++; } Figure 20.1: A snippet of the _syncSmartVault function in spool-v2-core/SmartVaultManager.sol This loop adds the value of mintedSVTs to the newSVTs variables and then computes the value of oldTotalSVTs by adding newSVTs to it in every iteration. Because mintedSVTs are added in every iteration, new minted SVTs are added for each ush index multiple times when the loop is iterated more than once. The value of oldTotalSVTs is then passed to the syncDeposit function of the DepositManager contract, which uses it to compute the management fee for the smart vault. The use of the inated value of oldTotalSVTs causes higher management fees to be paid to the smart vault owner. Exploit Scenario Alice deposits assets into a smart vault and ushes it. Before doHardWork is executed, Bob deposits assets into the same smart vault and ushes it. At this point, flushIndex.current has been increased twice for the smart vault. After the execution of doHardWork , the loop to sync the smart vault is iterated twice. As a result, a double management fee is paid to the smart vault owner, and Alice and Bob lose assets. Recommendations Short term, modify the loop so that syncResult.mintedSVTs is added to bag.oldTotalSVTs instead of bag.newSVTs . Long term, be careful when implementing accumulators in loops. Add test cases for multiple interactions to catch such issues.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "21. Incorrect ghost strategy check ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The emergencyWithdraw and redeemStrategyShares functions incorrectly check whether a strategy is a ghost strategy after checking that the strategy has a ROLE_STRATEGY role. function emergencyWithdraw( address [] calldata strategies, uint256 [][] calldata withdrawalSlippages, bool removeStrategies ) external onlyRole(ROLE_EMERGENCY_WITHDRAWAL_EXECUTOR, msg.sender ) { for ( uint256 i; i < strategies.length; ++i) { _checkRole(ROLE_STRATEGY, strategies[i]); if (strategies[i] == _ghostStrategy) { continue ; } [...] Figure 21.1: A snippet the emergencyWithdraw function spool-v2-core/StrategyRegistry.sol#L456L465 function redeemStrategyShares( address [] calldata strategies, uint256 [] calldata shares, uint256 [][] calldata withdrawalSlippages ) external { for ( uint256 i; i < strategies.length; ++i) { _checkRole(ROLE_STRATEGY, strategies[i]); if (strategies[i] == _ghostStrategy) { continue ; } [...] Figure 21.2: A snippet the emergencyWithdraw function spool-v2-core/StrategyRegistry.sol#L477L486 A ghost strategy will never have the ROLE_STRATEGY role, so both functions will always incorrectly revert if a ghost strategy is passed in the strategies array. Exploit Scenario Bob calls redeemStrategyShares with the ghost strategy in strategies and the transaction unexpectedly reverts. Recommendations Short term, modify the aected functions so that they verify whether the given strategy is a ghost strategy before checking the role with _checkRole . Long term, clearly document which roles a contract should have and implement the appropriate checks to verify them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}, {"title": "22. Reward conguration not initialized properly when reward is zero ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The RewardManager.addToken function, which adds a new reward token for the given smart vault, does not initialize all conguration variables when the initial reward is zero. As a result, all calls to the RewardManager.extendRewardEmission function will fail, and rewards cannot be added for that vault. RewardManager.addToken adds a new reward token for the given smart vault. The reward tokens for a smart vault are tracked using the RewardManager.rewardConfiguration function. The tokenAdded value of the conguration is used to check whether the token has already been added for the vault (gure 22.1). function addToken ( address smartVault , IERC20 token, uint32 rewardsDuration , uint256 reward ) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { RewardConfiguration storage config = rewardConfiguration [smartVault][token]; if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted( address (token)); if ( config.tokenAdded != 0 ) revert RewardTokenAlreadyAdded( address (token)); if (rewardsDuration == 0 ) revert InvalidRewardDuration(); if (rewardTokensCount[smartVault] > 5 ) revert RewardTokenCapReached(); rewardTokens[smartVault][rewardTokensCount[smartVault]] = token; rewardTokensCount[smartVault]++; config.rewardsDuration = rewardsDuration; if ( reward > 0 ) { _extendRewardEmission(smartVault, token, reward); } } Figure 22.1: The addToken function in spool-v2-core/RewardManager.sol#L81L101 However, RewardManager.addToken does not update config.tokenAdded , and the _extendRewardEmission function, which updates config.tokenAdded , is called only when the reward is greater than zero. RewardManager.extendRewardEmission is the only entry point to add rewards for a vault. It checks whether token has been previously added by verifying that tokenAdded is greater than zero (gure 22.2). function extendRewardEmission ( address smartVault , IERC20 token, uint256 reward , uint32 rewardsDuration ) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted( address (token)); if (rewardsDuration == 0 ) revert InvalidRewardDuration(); if ( rewardConfiguration[smartVault][token].tokenAdded == 0 ) { revert InvalidRewardToken( address (token)); } [...] } Figure 22.2: The extendRewardEmission function in spool-v2-core/RewardManager.sol#L106L119 Because tokenAdded is not initialized when the initial rewards are zero, the vault admin cannot add the rewards for the vault in that token. The impact of this issue is lower because the vault admin can use the RewardManager.removeReward function to remove the token and add it again with a nonzero initial reward. Note that the vault admin can only remove the token without blacklisting it because the config.periodFinish value is also not initialized when the initial reward is zero. Exploit Scenario Alice is the admin of a smart vault. She adds a reward token for her smart vault with the initial reward set to zero. Alice tries to add rewards using extendRewardEmission , and the transaction fails. She cannot add rewards for her smart vault. She has to remove the token and re-add it with a nonzero initial reward. Recommendations Short term, use a separate Boolean variable to track whether a token has been added for a smart vault, and have RewardManager.addToken initialize that variable. Long term, improve the systems unit tests to cover all execution paths.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "23. Missing function for removing reward tokens from the blacklist ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "A Spool admin can blacklist a reward token for a smart vault through the RewardManager contract, but they cannot remove it from the blacklist. As a result, a reward token cannot be used again once it is blacklisted. The RewardManager.forceRemoveReward function blacklists the given reward token by updating the RewardManager.tokenBlacklist array (gure 23.1). Blacklisted tokens cannot be used as rewards. function forceRemoveReward ( address smartVault , IERC20 token) external onlyRole(ROLE_SPOOL_ADMIN, msg.sender ) { tokenBlacklist[smartVault][token] = true ; _removeReward(smartVault, token); delete rewardConfiguration[smartVault][token]; } Figure 23.1: The forceRemoveReward function in spool-v2-core/RewardManager.sol#L160L165 However, RewardManager does not have a function to remove tokens from the blacklist. As a result, if the Spool admin accidentally blacklists a token, then the smart vault admin will never be able to use that token to send rewards. Exploit Scenario Alice is the admin of a smart vault. She adds WETH and token A as rewards. The value of token A declines rapidly, so a Spool admin decides to blacklist the token for Alices vault. The Spool admin accidentally supplies the WETH address in the call to forceRemoveReward . As a result, WETH is blacklisted, and Alice cannot send rewards in WETH. Recommendations Short term, add a function with the proper access controls to remove tokens from the blacklist. Long term, improve the systems unit tests to cover all execution paths.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "24. Risk of unclaimed shares due to loss of precision in reallocation operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The ReallocationLib.calculateReallocation function releases strategy shares and calculates their USD value. The USD value is later converted into strategy shares in the ReallocationLib.doReallocation function. Because the conversion operations always round down, the number of shares calculated in doReallocation will be less than the shares released in calculateReallocation . As a result, some shares released in calculateReallocation will be unclaimed, as ReallocationLib distributes only the shares computed in doReallocation . ReallocationLib.calculateAllocation calculates the USD value that needs to be withdrawn from each of the strategies used by smart vaults (gure 24.1). The smart vaults release the shares equivalent to the calculated USD value. /** * @dev Calculates reallocation needed per smart vault. [...] * @return Reallocation of the smart vault: * - first index is 0 or 1 * - 0: * - second index runs over smart vault's strategies * - value is USD value that needs to be withdrawn from the strategy [...] */ function calculateReallocation ( [...] ) private returns ( uint256 [][] memory ) { [...] } else if (targetValue < currentValue) { // This strategy needs withdrawal. [...] IStrategy(smartVaultStrategies[i]). releaseShares (smartVault, sharesToRedeem ); // Recalculate value to withdraw based on released shares. reallocation[ 0 ][i] = IStrategy(smartVaultStrategies[i]).totalUsdValue() * sharesToRedeem / IStrategy(smartVaultStrategies[i]).totalSupply(); } } return reallocation ; } Figure 24.1: The calculateReallocation function in spool-v2-core/ReallocationLib.sol#L161L207 The ReallocationLib.buildReallocationTable function calculates the reallocationTable value. The reallocationTable[i][j][0] value represents the USD amount that should move from strategy i to strategy j (gure 24.2). These USD amounts are calculated using the USD values of the released shares computed in ReallocationLib.calculateReallocation (represented by reallocation[0][i] in gure 24.1). /** [...] * @return Reallocation table: * - first index runs over all strategies i * - second index runs over all strategies j * - third index is 0, 1 or 2 * - 0: value represent USD value that should be withdrawn by strategy i and deposited into strategy j */ function buildReallocationTable ( [...] ) private pure returns ( uint256 [][][] memory ) { Figure 24.2: A snippet of buildReallocationTable function in spool-v2-core/ReallocationLib.sol#L209L228 ReallocationLib.doReallocation calculates the total USD amount that should be withdrawn from a strategy (gure 24.3). This total USD amount is exactly equal to the sum of the USD values needed to be withdrawn from the strategy for each of the smart vaults. The doReallocation function converts the total USD value to the equivalent number of strategy shares. The ReallocationLib library withdraws this exact number of shares from the strategy and distributes them to other strategies that require deposits of these shares. function doReallocation ( [...] uint256 [][][] memory reallocationTable ) private { // Distribute matched shares and withdraw unamatched ones. for ( uint256 i ; i < strategies.length; ++i) { [...] { uint256 [ 2 ] memory totals; // totals[0] -> total withdrawals for ( uint256 j ; j < strategies.length; ++j) { totals[ 0 ] += reallocationTable[i][j][ 0 ] ; [...] } // Calculate amount of shares to redeem and to distribute. uint256 sharesToDistribute = // first store here total amount of shares that should have been withdrawn IStrategy(strategies[i]).totalSupply() * totals[ 0 ] / IStrategy(strategies[i]).totalUsdValue(); [...] } [...] Figure 24.3: A snippet of the doReallocation function in spool-v2-core/ReallocationLib.sol#L285L350 Theoretically, the shares calculated for a strategy should be equal to the shares released by all of the smart vaults for that strategy. However, there is a loss of precision in both the calculateReallocation functions calculation of the USD value of released shares and the doReallocation functions conversion of the combined USD value to strategy shares. As a result, the number of shares released by all of the smart vaults will be less than the shares calculated in calculateReallocation . Because the ReallocationLib library only distributes these calculated shares, there will be some unclaimed strategy shares as dust. It is important to note that the rounding error could be greater than one in the context of multiple smart vaults. Additionally, the error could be even greater if the conversion results were rounded in the opposite direction: in that case, if the calculated shares were greater than the released shares, the reallocation would fail when burn and claim operations are executed. Recommendations Short term, modify the code so that it stores the number of shares released in calculateReallocation , and implement dustless calculations to build the reallocationTable value with the share amounts and the USD amounts. Have doReallocation use this reallocationTable value to calculate the value of sharesToDistribute . Long term, use Echidna to test system and mathematical invariants.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "25. Curve3CoinPoolAdapters _addLiquidity reverts due to incorrect amounts deposited ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The _addLiquidity function loops through the amounts array but uses an additional element to keep track of whether deposits need to be made in the Strategy.doHardWork function. As a result, _addLiquidity overwrites the number of tokens to send for the rst asset, causing far fewer tokens to be deposited than expected, thus causing the transaction to revert due to the slippage check. function _addLiquidity( uint256 [] memory amounts, uint256 slippage) internal { uint256 [N_COINS] memory curveAmounts; for ( uint256 i; i < amounts.length; ++i) { curveAmounts[assetMapping().get(i)] = amounts[i]; } ICurve3CoinPool(pool()).add_liquidity(curveAmounts, slippage); } Figure 25.1: The _addLiquidity function in spool-v2-core/CurveAdapter.sol#L12L20 The last element in the doHardWork functions assetsToDeposit array keeps track of the deposits to be made and is incremented by one on each iteration of assets in assetGroup if that asset has tokens to deposit. This variable is then passed to the _depositToProtocol function and then, for strategies that use the Curve3CoinPoolAdapter , is passed to _addLiquidity in the amounts parameter. When _addLiquidity iterates over the last element in the amounts array, the assetMapping().get(i) function will return 0 because i in assetMapping is uninitialized. This return value will overwrite the number of tokens to deposit for the rst asset with a strictly smaller amount. function doHardWork(StrategyDhwParameterBag calldata dhwParams) external returns (DhwInfo memory dhwInfo) { _checkRole(ROLE_STRATEGY_REGISTRY, msg.sender ); // assetsToDeposit[0..token.length-1]: amount of asset i to deposit // assetsToDeposit[token.length]: is there anything to deposit uint256 [] memory assetsToDeposit = new uint256 [](dhwParams.assetGroup.length + 1); unchecked { for ( uint256 i; i < dhwParams.assetGroup.length; ++i) { assetsToDeposit[i] = IERC20(dhwParams.assetGroup[i]).balanceOf(address(this)); if (assetsToDeposit[i] > 0) { ++assetsToDeposit[dhwParams.assetGroup.length]; } } } [...] // - deposit assets into the protocol _depositToProtocol(dhwParams.assetGroup, assetsToDeposit, dhwParams.slippages); Figure 25.2: A snippet of the doHardWork function in spool-v2-core/Strategy.sol#L71L75 Exploit Scenario The doHardWork function is called for a smart vault that uses the ConvexAlusdStrategy strategy; however, the subsequent call to _addLiquidity reverts due to the incorrect number of assets that it is trying to deposit. The smart vault is unusable. Recommendations Short term, have _addLiquidity loop the amounts array for N_COINS time instead of its length. Long term, refactor the Strategy.doHardWork function so that it does not use an additional element in the assetsToDeposit array to keep track of whether deposits need to be made. Instead, use a separate Boolean variable. The current pattern is too error-prone.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "26. Reallocation process reverts when a ghost strategy is present ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The reallocation process reverts in multiple places when a ghost strategy is present. As a result, it is impossible to reallocate a smart vault with a ghost strategy. The rst revert would occur in the mapStrategies function (gure 26.1). Users calling the reallocate function would not know to add the ghost strategy address in the strategies array, which holds the strategies that need to be reallocated. This function reverts if it does not nd a strategy in the array. Even if the ghost strategy address is in strategies , a revert would occur in the areas described below. function mapStrategies( address [] calldata smartVaults, address [] calldata strategies, mapping ( address => address []) storage _smartVaultStrategies ) private view returns ( uint256 [][] memory ) { [...] // Loop over smart vault's strategies. for ( uint256 j; j < smartVaultStrategiesLength; ++j) { address strategy = smartVaultStrategies[j]; bool found = false ; // Try to find the strategy in the provided list of strategies. for ( uint256 k; k < strategies.length; ++k) { if (strategies[k] == strategy) { // Match found. found = true ; strategyMatched[k] = true ; // Add entry to the strategy mapping. strategyMapping[i][j] = k; break ; } } if (!found) { list // If a smart vault's strategy was not found in the provided // of strategies, this means that the provided list is invalid. revert InvalidStrategies(); } } } Figure 26.1: A snippet of the mapStrategies function in spool-v2-core/ReallocationLib.sol#L86L144 During the reallocation process, the doReallocation function calls the beforeRedeemalCheck and beforeDepositCheck functions even on ghost strategies (gure 26.2); however, their implementation is to revert on ghost strategies with an IsGhostStrategy error (gure 26.3) . function doReallocation( address [] calldata strategies, ReallocationParameterBag calldata reallocationParams, uint256 [][][] memory reallocationTable ) private { if (totals[0] == 0) { reallocationParams.withdrawalSlippages[i]); IStrategy(strategies[i]).beforeRedeemalCheck(0, // There is nothing to withdraw from strategy i. continue ; } // Calculate amount of shares to redeem and to distribute. uint256 sharesToDistribute = // first store here total amount of shares that should have been withdrawn IStrategy(strategies[i]).totalUsdValue(); IStrategy(strategies[i]).totalSupply() * totals[0] / IStrategy(strategies[i]).beforeRedeemalCheck( sharesToDistribute, reallocationParams.withdrawalSlippages[i] ); [...] // Deposit assets into the underlying protocols. for ( uint256 i; i < strategies.length; ++i) { IStrategy(strategies[i]).beforeDepositCheck(toDeposit[i], reallocationParams.depositSlippages[i]); [...] Figure 26.2: A snippet of the doReallocation function in spool-v2-core/ReallocationLib.sol#L285L 469 contract GhostStrategy is IERC20Upgradeable, IStrategy { [...] function beforeDepositCheck( uint256 [] memory , uint256 [] calldata ) external pure { revert IsGhostStrategy(); } function beforeRedeemalCheck( uint256 , uint256 [] calldata ) external pure { revert IsGhostStrategy(); } Figure 26.3: The beforeDepositCheck and beforeRedeemalCheck functions in spool-v2-core/GhostStrategy.sol#L98L104 Exploit Scenario A strategy is removed from a smart vault. Bob, who has the ROLE_ALLOCATOR role, calls reallocate , but it reverts and the smart vault is impossible to reallocate. Recommendations Short term, modify the associated code so that ghost strategies are not passed to the reallocate function in the _smartVaultStrategies parameter. Long term, improve the systems unit and integration tests to test for smart vaults with ghost strategies. Such tests are currently missing.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "27. Broken test cases that hide security issues ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Multiple test cases do not check sucient conditions to verify the correctness of the code, which could result in the deployment of buggy code in production and the loss of funds. The test_extendRewardEmission_ok test does not check the new reward rate and duration to verify the eect of the call to the extendRewardEmission function on the RewardManager contract: function test_extendRewardEmission_ok() public { deal(address(rewardToken), vaultOwner, rewardAmount * 2, true); vm.startPrank(vaultOwner); rewardToken.approve(address(rewardManager), rewardAmount * 2); rewardManager.addToken(smartVault, rewardToken, rewardDuration, rewardAmount); rewardManager.extendRewardEmission(smartVault, rewardToken, 1 ether, rewardDuration); vm.stopPrank(); } Figure 27.1: An insucient test case for extendRewardEmission spool-v2-core/RewardManager.t.sol The test_removeReward_ok test does not check the new reward token count and the deletion of the reward conguration for the smart vault to verify the eect of the call to the removeReward function on the RewardManager contract: function test_removeReward_ok() public { deal(address(rewardToken), vaultOwner, rewardAmount, true); vm.startPrank(vaultOwner); rewardToken.approve(address(rewardManager), rewardAmount); rewardManager.addToken(smartVault, rewardToken, rewardDuration, rewardAmount); skip(rewardDuration + 1); rewardManager.removeReward(smartVault, rewardToken); vm.stopPrank(); } Figure 27.2: An insucient test case for removeReward spool-v2-core/RewardManager.t.sol There is no test case to check the access controls of the removeReward function. Similarly, the test_forceRemoveReward_ok test does not check the eects of the forced removal of a reward token. Findings TOB-SPL-28 and TOB-SPL-29 were not detected by tests because of these broken test cases. The test_removeStrategy_betweenFlushAndDHW test does not check the balance of the master wallet. The test_removeStrategy_betweenFlushAndDhwWithdrawals test removes the strategy before the do hard work execution of the deposit cycle instead of removing it before the do hard work execution of the withdrawal cycle, making this test case redundant. Finding TOB-SPL-33 would have been detected if this test had been correctly implemented. There may be other broken tests that we did not nd, as we could not cover all of the test cases. Exploit Scenario The Spool team deploys the protocol. After some time, the Spool team makes some changes in the code that introduces a bug that goes unnoticed due to the broken test cases. The team deploys the new changes with condence in their tests and ends up introducing a security issue in the production deployment of the protocol. Recommendations Short term, x the test cases described above. Long term, review all of the systems test cases and make sure that they verify the given state change correctly and suciently after an interaction with the protocol. Use Necessist to nd broken test cases and x them.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Undetermined"]}, {"title": "28. Reward emission can be extended for a removed reward token ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Smart vault owners can extend the reward emission for a removed token, which may cause tokens to be stuck in the RewardManager contract. The removeReward function in the RewardManager contract calls the _removeReward function, which does not remove the reward conguration: function _removeReward( address smartVault, IERC20 token) private { uint256 _rewardTokensCount = rewardTokensCount[smartVault]; for ( uint256 i; i < _rewardTokensCount; ++i) { if (rewardTokens[smartVault][i] == token) { rewardTokens[smartVault][i] = rewardTokens[smartVault][_rewardTokensCount - 1]; delete rewardTokens[smartVault][_rewardTokensCount- 1]; rewardTokensCount[smartVault]--; emit RewardRemoved(smartVault, token); break ; } } } Figure 28.1: The _removeReward function in spool-v2-core/RewardManger.sol The extendRewardEmission function checks whether the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is not zero to make sure that the token was already added to the smart vault: function extendRewardEmission( address smartVault, IERC20 token, uint256 reward, uint32 rewardsDuration) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted(address(token)); if (rewardsDuration == 0) revert InvalidRewardDuration(); if (rewardConfiguration[smartVault][token].tokenAdded == 0) { revert InvalidRewardToken(address(token)); } rewardConfiguration[smartVault][token].rewardsDuration = rewardsDuration; _extendRewardEmission(smartVault, token, reward); } Figure 28.2: The extendRewardEmission function in spool-v2-core/RewardManger.sol After removing a reward token from a smart vault, the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is left as nonzero, which allows the smart vault owner to extend the reward emission for the removed token. Exploit Scenario Alice adds a reward token A to her smart vault S. After a month, she removes token A from her smart vault. After some time, she forgets that she removed token A from her vault. She calls extendRewardEmission with 1,000 token A as the reward. The amount of token A is transferred from Alice to the RewardManager contract, but it is not distributed to the users because it is not present in the list of reward tokens added for smart vault S. The 1,000 tokens are stuck in the RewardManager contract. Recommendations Short term, modify the associated code so that it deletes the rewardConfiguration[smartVault][token] conguration when removing a reward token for a smart vault. Long term, add test cases to check for expected user interactions to catch bugs such as this.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "29. A reward token cannot be added once it is removed from a smart vault ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Smart vault owners cannot add reward tokens again after they have been removed once from the smart vault, making owners incapable of providing incentives to users. The removeReward function in the RewardManager contract calls the _removeReward function, which does not remove the reward conguration: function _removeReward( address smartVault, IERC20 token) private { uint256 _rewardTokensCount = rewardTokensCount[smartVault]; for ( uint256 i; i < _rewardTokensCount; ++i) { if (rewardTokens[smartVault][i] == token) { rewardTokens[smartVault][i] = rewardTokens[smartVault][_rewardTokensCount - 1]; delete rewardTokens[smartVault][_rewardTokensCount- 1]; rewardTokensCount[smartVault]--; emit RewardRemoved(smartVault, token); break ; } } } Figure 29.1: The _removeReward function in spool-v2-core/RewardManger.sol The addToken function checks whether the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is zero to make sure that the token was not already added to the smart vault: function addToken( address smartVault, IERC20 token, uint32 rewardsDuration, uint256 reward) external onlyAdminOrVaultAdmin(smartVault, msg.sender ) exceptUnderlying(smartVault, token) { RewardConfiguration storage config = rewardConfiguration[smartVault][token]; if (tokenBlacklist[smartVault][token]) revert RewardTokenBlacklisted(address(token)); if (config.tokenAdded != 0) revert RewardTokenAlreadyAdded(address(token)); if (rewardsDuration == 0) revert InvalidRewardDuration(); if (rewardTokensCount[smartVault] > 5) revert RewardTokenCapReached(); rewardTokens[smartVault][rewardTokensCount[smartVault]] = token; rewardTokensCount[smartVault]++; config.rewardsDuration = rewardsDuration; if (reward > 0) { _extendRewardEmission(smartVault, token, reward); } } Figure 29.2: The addToken function in spool-v2-core/RewardManger.sol After a reward token is removed from a smart vault, the value of tokenAdded in the rewardConfiguration[smartVault][token] conguration is left as nonzero, which prevents the smart vault owner from adding the token again for reward distribution as an incentive to the users of the smart vault. Exploit Scenario Alice adds a reward token A to her smart vault S. After a month, she removes token A from her smart vault. Noticing the success of her earlier reward incentive program, she wants to add reward token A to her smart vault again, but her transaction to add the reward token reverts, leaving her with no choice but to distribute another token. Recommendations Short term, modify the associated code so that it deletes the rewardConfiguration[smartVault][token] conguration when removing a reward token for a smart vault. Long term, add test cases to check for expected user interactions to catch bugs such as this.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "30. Missing whenNotPaused modier ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The documentation species which functionalities should not be working when the system is paused, including the claiming of rewards; however, the claim function does not have the whenNotPaused modier. As a result, users can claim their rewards even when the system is paused. If the system is paused: - users cant claim vault incentives - [...] Figure 30.1: A snippet of the provided Spool documentation function claim(ClaimRequest[] calldata data) public { Figure 30.2: The claim function header in spool-v2-core/RewardPool.sol#L47 Exploit Scenario Alice, who has the ROLE_PAUSER role in the system, pauses the protocol after she sees a possible vulnerability in the claim function. The Spool team believes there are no possible funds moving from the system; however, users can still claim their rewards. Recommendations Short term, add the whenNotPaused modier to the claim function. Long term, improve the systems unit and integration tests by adding a test to verify that the expected functionalities do not work when the system is in a paused state.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "31. Users who deposit and then withdraw before doHardWork lose their tokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Users who deposit and then withdraw assets before doHardWork is called will receive zero tokens from their withdrawal operations. When a user deposits assets, the depositAssets function mints an NFT with some metadata to the user who can later redeem it for the underlying SVT tokens. function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender) returns ( uint256 [] memory , uint256 ) { [...] // mint deposit NFT DepositMetadata memory metadata = DepositMetadata(bag.assets, block.timestamp , bag2.flushIndex); uint256 depositId = ISmartVault(bag.smartVault).mintDepositNFT(bag.receiver, metadata); [...] } Figure 31.1: A snippet of the depositAssets function in spool-v2-core/DepositManager.sol#L379L439 Users call the claimSmartVaultTokens function in the SmartVaultManager contract to claim SVT tokens. It is important to note that this function calls the _syncSmartVault function with false as the last argument, which means that it will not revert if the current ush index and the ush index to sync are the same. Then, claimSmartVaultTokens delegates the work to the corresponding function in the DepositManager contract. function claimSmartVaultTokens( address smartVault, uint256 [] calldata nftIds, uint256 [] calldata nftAmounts) public whenNotPaused returns ( uint256 ) { _onlyRegisteredSmartVault(smartVault); address [] memory tokens = _assetGroupRegistry.listAssetGroup(_smartVaultAssetGroups[smartVault]); _syncSmartVault(smartVault, _smartVaultStrategies[smartVault], tokens, false ); return _depositManager.claimSmartVaultTokens(smartVault, nftIds, nftAmounts, tokens, msg.sender ); } Figure 31.2: A snippet of the claimSmartVaultTokens function in spool-v2-core/SmartVaultManager.sol#L238L247 Later, the claimSmartVaultTokens function in DepositManager (gure 31.3) computes the SVT tokens that users will receive by calling the getClaimedVaultTokensPreview function and passing the bag.mintedSVTs value for the ush corresponding to the burned NFT. function claimSmartVaultTokens( address smartVault, uint256 [] calldata nftIds, uint256 [] calldata nftAmounts, address [] calldata tokens, address executor ) external returns ( uint256 ) { _checkRole(ROLE_SMART_VAULT_MANAGER, msg.sender ); [...] ClaimTokensLocalBag memory bag; ISmartVault vault = ISmartVault(smartVault); bag.metadata = vault.burnNFTs(executor, nftIds, nftAmounts); for ( uint256 i; i < nftIds.length; ++i) { if (nftIds[i] > MAXIMAL_DEPOSIT_ID) { revert InvalidDepositNftId(nftIds[i]); } // we can pass empty strategy array and empty DHW index array, // because vault should already be synced and mintedVaultShares values available bag.data = abi.decode(bag.metadata[i], (DepositMetadata)); bag.mintedSVTs = _flushShares[smartVault][bag.data.flushIndex].mintedVaultShares; claimedVaultTokens += getClaimedVaultTokensPreview(smartVault, bag.data, nftAmounts[i], bag.mintedSVTs , tokens); } Figure 31.3: A snippet of the claimSmartVaultTokens in spool-v2-core/DepositManager.sol#L135L184 Then, getClaimedVaultTokensPreview calculates the SVT tokens proportional to the amount deposited. function getClaimedVaultTokensPreview( address smartVaultAddress, DepositMetadata memory data, uint256 nftShares, uint256 mintedSVTs, address [] calldata tokens ) public view returns ( uint256 ) { [...] for ( uint256 i; i < data.assets.length; ++i) { depositedUsd += _priceFeedManager.assetToUsdCustomPrice(tokens[i], data.assets[i], exchangeRates[i]); totalDepositedUsd += totalDepositedAssets[i], exchangeRates[i]); _priceFeedManager.assetToUsdCustomPrice(tokens[i], } uint256 claimedVaultTokens = mintedSVTs * depositedUsd / totalDepositedUsd; return claimedVaultTokens * nftShares / NFT_MINTED_SHARES; } Figure 31.4: A snippet of the getClaimedVaultTokensPreview function in spool-v2-core/DepositManager.sol#L546L572 However, the value of _flushShares[smartVault][bag.data.flushIndex].mintedVaultShares , shown in gure 31.3, will always be 0 : the value is updated in the syncDeposit function, but because the current ush cycle is not nished yet, syncDeposit cannot be called through syncSmartVault . The same problem appears in the redeem , redeemFast , and claimWithdrawal functions. Exploit Scenario Bob deposits assets into a smart vault, but he notices that he deposited in the wrong smart vault. He calls redeem and claimWithdrawal , expecting to receive back his tokens, but he receives zero tokens. The tokens are locked in the smart contracts. Recommendations Short term, do not allow users to withdraw tokens when the corresponding ush has not yet happened. Long term, document and test the expected eects when calling functions in all of the possible orders, and add adequate constraints to avoid unexpected behavior.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "32. Lack of events emitted for state-changing functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Multiple critical operations do not emit events. As a result, it will be dicult to review the correct behavior of the contracts once they have been deployed. Events generated during contract execution aid in monitoring, baselining of behavior, and detection of suspicious activity. Without events, users and blockchain-monitoring systems cannot easily detect behavior that falls outside the baseline conditions. This may prevent malfunctioning contracts or attacks from being detected. The following operations should trigger events:  SpoolAccessControl.grantSmartVaultOwnership  ActionManager.setActions  SmartVaultManager.registerSmartVault  SmartVaultManager.removeStrategy  SmartVaultManager.syncSmartVault  SmartVaultManager.reallocate  StrategyRegistry.registerStrategy  StrategyRegistry.removeStrategy  StrategyRegistry.doHardWork  StrategyRegistry.setEcosystemFee  StrategyRegistry.setEcosystemFeeReceiver  StrategyRegistry.setTreasuryFee  StrategyRegistry.setTreasuryFeeReceiver  Strategy.doHardWork  RewardManager.addToken  RewardManager.extendRewardEmission Exploit Scenario The Spool system experiences a security incident, but the Spool team has trouble reconstructing the sequence of events causing the incident because of missing log information. Recommendations Short term, add events for all operations that may contribute to a higher level of monitoring and alerting. Long term, consider using a blockchain-monitoring system to track any suspicious behavior in the contracts. The system relies on several contracts to behave as expected. A monitoring mechanism for critical events would quickly detect any compromised system components.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "33. Removal of a strategy could result in loss of funds ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "A Spool admin can remove a strategy from the system, which will be replaced by a ghost strategy in all smart vaults that use it; however, if a strategy is removed when the system is in specic states, funds to be deposited or withdrawn in the next do hard work cycle will be lost. If the following sequence of events occurs, the asset deposited will be lost from the removed strategy: 1. A user deposits assets into a smart vault. 2. The ush function is called. The StrategyRegistry._assetsDeposited[strategy][xxx][yyy] storage variable now has assets to send to the given strategy in the next do hard work cycle. 3. The strategy is removed. 4. doHardWork is called, but the assets for the removed strategy are locked in the master wallet because the function can be called only for valid strategies. If the following sequence of events occurs, the assets withdrawn from a removed strategy will be lost: 1. doHardWork is called. 2. The strategy is removed before a smart vault sync is done. Exploit Scenario Multiple smart vaults use strategy A. Users deposited a total of $1 million, and $300,000 should go to strategy A. Strategy A is removed due to an issue in the third-party protocol. All of the $300,000 is locked in the master wallet. Recommendations Short term, modify the associated code to properly handle deposited and withdrawn funds when strategies are removed. Long term, improve the systems unit and integration tests: consider all of the possible transaction sequences in the systems state and test them to ensure their correct behavior.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "34. ExponentialAllocationProvider reverts on strategies without risk scores ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The ExponentialAllocationProvider.calculateAllocation function can revert due to division-by-zero error when a strategys risk score has not been set by the risk provider. The risk variable in calculateAllocation represents the risk score set by the risk provider for the given strategy, represented by the index i . Ghost strategies can be passed to the function. If a ghost strategys risk score has not been set (which is likely, as there would be no reason to set one), the function will revert with a division-by-zero error. function calculateAllocation(AllocationCalculationInput calldata data) external pure returns ( uint256 [] memory ) { if (data.apys.length != data.riskScores.length) { revert ApysOrRiskScoresLengthMismatch(data.apys.length, data.riskScores.length); } [...] for ( uint8 i; i < data.apys.length; ++i) { [...] int256 risk = fromUint(data.riskScores[i]); results[i] = uint256 ( div(apy, risk) ); resultSum += results[i]; } Figure 34.1: A snippet of the calculateAllocation function in spool-v2-core/ExponentialAllocationProvider.sol#L309L340 Exploit Scenario A strategy is removed from a smart vault that uses the ExponentialAllocationProvider contract. Bob, who has the ROLE_ALLOCATOR role, calls reallocate ; however, it reverts, and the smart vault is impossible to reallocate. Recommendations Short term, modify the calculateAllocation function so that it properly handles strategies with uninitialized risk scores. Long term, improve the unit and integration tests for the allocators. Refactor the codebase so that ghost strategies are not passed to the calculateAllocator function.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "35. Removing a strategy makes the smart vault unusable ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Removing a strategy from a smart vault causes every subsequent deposit transaction to revert, making the smart vault unusable. The deposit function of the SmartVaultManager contract calls the depositAssets function on the DepositManager contract. The depositAssets function calls the checkDepositRatio function, which takes an argument called strategyRatios : function depositAssets(DepositBag calldata bag, DepositExtras calldata bag2) external onlyRole(ROLE_SMART_VAULT_MANAGER, msg.sender ) returns ( uint256 [] memory , uint256 ) { ... // check if assets are in correct ratio checkDepositRatio( bag.assets, SpoolUtils.getExchangeRates(bag2.tokens, _priceFeedManager), bag2.allocations, SpoolUtils.getStrategyRatiosAtLastDhw(bag2.strategies, _strategyRegistry) ); ... return (_vaultDeposits[bag.smartVault][bag2.flushIndex].toArray(bag2.tokens.length), depositId); } Figure 35.1: The depositAssets function in spool-v2-core/DepositManager.sol The value of strategyRatios is fetched from the StrategyRegistry contract, which returns an empty array on ghost strategies. This empty array is then used in a for loop in the calculateFlushFactors function: function calculateFlushFactors( uint256 [] memory exchangeRates, uint16a16 allocation, uint256 [][] memory strategyRatios ) public pure returns ( uint256 [][] memory ) { uint256 [][] memory flushFactors = new uint256 [][](strategyRatios.length); // loop over strategies for ( uint256 i; i < strategyRatios.length; ++i) { flushFactors[i] = new uint256 [](exchangeRates.length); uint256 normalization = 0; // loop over assets for ( uint256 j = 0; j < exchangeRates.length; j++) { normalization += strategyRatios[i][j] * exchangeRates[j]; } // loop over assets for ( uint256 j = 0; j < exchangeRates.length; j++) { flushFactors[i][j] = allocation.get(i) * strategyRatios[i][j] * PRECISION_MULTIPLIER / normalization; } } return flushFactors; } Figure 35.2: The calculateFlushFactors function in spool-v2-core/DepositManager.sol The statement calculating the value of normalization tries to access an index of the empty array and reverts with the Index out of bounds error, causing the deposit function to revert for every transaction thereafter. Exploit Scenario A Spool admin removes a strategy from a smart vault. Because of the presence of a ghost strategy, users deposit transactions into the smart vault revert with the Index out of bounds error. Recommendations Short term, modify the calculateFlushFactors function so that it skips ghost strategies in the loop used to calculate the value of normalization . Long term, review the entire codebase, check the eects of removing strategies from smart vaults, and ensure that all of the functionality works for smart vaults with one or more ghost strategies.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "36. Issues with the management of access control roles in deployment script ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The deployment script does not properly manage or assign access control roles. As a result, the protocol will not work as expected, and the protocols contracts cannot be upgraded. The deployment script has multiple issues regarding the assignment or transfer of access control roles. It fails to grant certain roles and to revoke temporary roles on deployment:      Ownership of the ProxyAdmin contract is not transferred to an EOA, multisig wallet, or DAO after the system is deployed, making the smart contracts non-upgradeable. The DEFAULT_ADMIN_ROLE role is not transferred to an EOA, multisig wallet, or DAO after the system is deployed, leaving no way to manage roles after deployment. The ADMIN_ROLE_STRATEGY role is not assigned to the StrategyRegistry contract, which is required to grant the ROLE_STRATEGY role to a strategy contract. Because of this, new strategies cannot be registered. The ADMIN_ROLE_SMART_VAULT_ALLOW_REDEEM role is not assigned to the SmartVaultFactory contract, which is required to grant the ROLE_SMART_VAULT_ALLOW_REDEEM role to smartVault contracts. The ROLE_SMART_VAULT_MANAGER and ROLE_MASTER_WALLET_MANAGER roles are not assigned to the DepositManager and WithdrawalManager contracts, making them unable to move funds from the master wallet contract. We also found that the ROLE_SMART_VAULT_ADMIN role is not assigned to the smart vault owner when a new smart vault is created. This means that smart vault owners will not be able to manage their smart vaults. Exploit Scenario The Spool team deploys the smart contracts using the deployment script, but due to the issues described in this nding, the team is not able to perform the role management and upgrades when required. Recommendations Short term, modify the deployment script so that it does the following on deployment:     Transfers ownership of the proxyAdmin contract to an EOA, multisig wallet, or DAO Transfers the DEFAULT_ADMIN_ROLE role to an EOA, multisig wallet, or DAO Grants the required roles to the smart contracts Allow the SmartVaultFactory contract to grant the ROLE_SMART_VAULT_ADMIN role to owners of newly created smart vaults Long term, document all of the systems roles and interactions between components that require privileged roles. Make sure that all of the components are granted their required roles following the principle of least privilege to keep the protocol secure and functioning as expected.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "37. Risk of DoS due to unbounded loops ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "Guards and actions are run in unbounded loops. A smart vault creator can add too many guards and actions, potentially trapping the deposit and withdrawal functionality due to a lack of gas. The runGuards function calls all the congured guard contracts in a loop: function runGuards( address smartVaultId, RequestContext calldata context) external view { if (guardPointer[smartVaultId][context.requestType] == address (0)) { return ; } GuardDefinition[] memory guards = _readGuards(smartVaultId, context.requestType); for ( uint256 i; i < guards.length; ++i) { GuardDefinition memory guard = guards[i]; bytes memory encoded = _encodeFunctionCall(smartVaultId, guard, context); ( bool success, bytes memory data) = guard.contractAddress.staticcall(encoded); _checkResult(success, data, guard.operator, guard.expectedValue, i); } } Figure 37.1: The runGuards function in spool-v2-core/GuardManager.sol Multiple conditions can cause this loop to run out of gas:    The vault creator adds too many guards. One of the guard contracts consumes a high amount of gas. A guard starts consuming a high amount of gas after a specic block or at a specic state. If user transactions reach out-of-gas errors due to these conditions, smart vaults can become unusable, and funds can become stuck in the protocol. A similar issue aects the runActions function in the AuctionManager contract. Exploit Scenario Eve creates a smart vault with an upgradeable guard contract. Later, when users have made large deposits, Eve upgrades the guard contract to consume all of the available gas to trap user deposits in the smart vault for as long as she wants. Recommendations Short term, model all of the system's variable-length loops, including the ones used by runGuards and runActions , to ensure they cannot block contract execution within expected system parameters. Long term, carefully audit operations that consume a large amount of gas, especially those in loops.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "38. Unsafe casts throughout the codebase ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-03-spool-platformv2-securityreview.pdf", "body": "The codebase contains unsafe casts that could cause mathematical errors if they are reachable in certain states. Examples of possible unsafe casts are shown in gures 38.1 and 38.2. function flushSmartVault( address smartVault, uint256 flushIndex, address [] calldata strategies, uint16a16 allocation, address [] calldata tokens ) external returns (uint16a16) { [...] _flushShares[smartVault][flushIndex].flushSvtSupply = uint128(ISmartVault(smartVault).totalSupply()) ; return _strategyRegistry.addDeposits(strategies, distribution); } Figure 38.1: A possible unsafe cast in spool-v2-core/DepositManager.sol#L220 function syncDeposits( address smartVault, uint256 [3] calldata bag, // uint256 flushIndex, // uint256 lastDhwSyncedTimestamp, // uint256 oldTotalSVTs, address [] calldata strategies, uint16a16[2] calldata dhwIndexes, address [] calldata assetGroup, SmartVaultFees calldata fees ) external returns (DepositSyncResult memory ) { [...] if (syncResult.mintedSVTs > 0) { _flushShares[smartVault][bag[0]].mintedVaultShares = uint128 (syncResult.mintedSVTs) ; [...] } return syncResult; } Figure 38.2: A possible unsafe cast in spool-v2-core/DepositManager.sol#L243 Recommendations Short term, review the codebase to identify all of the casts that may be unsafe. Analyze whether these casts could be a problem in the current codebase and, if they are unsafe, make the necessary changes to make them safe. Long term, when implementing potentially unsafe casts, always include comments to explain why those casts are safe in the context of the codebase.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Undetermined"]}, {"title": "1. Lack of two-step process for contract ownership changes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The owner of a contract that inherits from the FraxlendPairCore contract can be changed through a call to the transferOwnership function. This function internally calls the _setOwner function, which immediately sets the contracts new owner. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. function transferOwnership ( address newOwner ) public virtual onlyOwner { require (newOwner != address ( 0 ), \"Ownable: new owner is the zero address\" ); _setOwner(newOwner); } function _setOwner ( address newOwner ) private { address oldOwner = _owner; _owner = newOwner; emit OwnershipTransferred(oldOwner, newOwner); } Figure 1.1: OpenZeppelins OwnableUpgradeable contract Exploit Scenario Alice, a Frax Finance administrator, invokes the transferOwnership function to change the address of an existing contracts owner but mistakenly submits the wrong address. As a result, ownership of the contract is permanently lost. Recommendations Short term, implement ownership transfer operations that are executed in a two-step process, in which the owner proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and prevent future mistakes.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "2. Missing checks of constructor/initialization parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "In the Fraxlend protocols constructor function, various settings are congured; however, two of the conguration parameters do not have checks to validate the values that they are set to. First, the _liquidationFee parameter does not have an upper limit check: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { [...] cleanLiquidationFee = _liquidationFee; dirtyLiquidationFee = (_liquidationFee * 90000 ) / LIQ_PRECISION; // 90 % of clean fee Figure 2.1: The constructor functions parameters in FraxlendPairCore.sol#L193-L194 Second, the Fraxlend system can work with one or two oracles; however, there is no check to ensure that at least one oracle is set: constructor ( bytes memory _configData, bytes memory _immutables, uint256 _maxLTV , uint256 _liquidationFee , uint256 _maturityDate , uint256 _penaltyRate , bool _isBorrowerWhitelistActive , bool _isLenderWhitelistActive ) { // [...] // Oracle Settings { IFraxlendWhitelist _fraxlendWhitelist = IFraxlendWhitelist(FRAXLEND_WHITELIST_ADDRESS); // Check that oracles are on the whitelist if (_oracleMultiply != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleMultiply)) { revert NotOnWhitelist(_oracleMultiply); } if (_oracleDivide != address ( 0 ) && !_fraxlendWhitelist.oracleContractWhitelist(_oracleDivide)) { revert NotOnWhitelist(_oracleDivide); } // Write oracleData to storage oracleMultiply = _oracleMultiply; oracleDivide = _oracleDivide; oracleNormalization = _oracleNormalization; Figure 2.2: The constructor functions body in FraxlendPairCore.sol#L201-L214 Exploit Scenario Bob deploys a custom pair with a miscongured _configData argument in which no oracle is set. As a consequence, the exchange rate is incorrect. Recommendations Short term, add an upper limit check for the _liquidationFee parameter, and add a check for the _configData parameter to ensure that at least one oracle is set. The checks can be added in either the FraxlendPairCore contract or the FraxlendPairDeployer contract. Long term, add appropriate requirements to values that users set to decrease the likelihood of user error.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "3. Incorrect application of penalty fee rate ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "A Fraxlend pair can have a maturity date, after which a penalty rate is applied to the interest to be paid by the borrowers. However, the penalty rate is also applied to the amount of time immediately before the maturity date. As shown in gure 3.1, the _addInterest function checks whether a pair is past maturity. If it is, the function sets the new rate (the _newRate parameter) to the penalty rate (the penaltyRate parameter) and then uses it to calculate the matured interest. The function should apply the penalty rate only to the time between the maturity date and the current time; however, it also applies the penalty rate to the time between the last interest accrual ( _deltaTime ) and the maturity date, which should be subject only to the normal interest rate. function _addInterest () // [...] uint256 _deltaTime = block.timestamp - _currentRateInfo.lastTimestamp; // [...] if (_isPastMaturity()) { _newRate = uint64 (penaltyRate); } else { // [...] // Effects: bookkeeping _currentRateInfo.ratePerSec = _newRate; _currentRateInfo.lastTimestamp = uint64 ( block.timestamp ); _currentRateInfo.lastBlock = uint64 ( block.number ); // Calculate interest accrued _interestEarned = (_deltaTime * _totalBorrow.amount * _currentRateInfo.ratePerSec) / 1e18; Figure 3.1: The _addInterest function in FraxlendPairCore.sol#L406-L494 Exploit Scenario A Fraxlend pairs maturity date is 100, the delta time (the last time interest accrued) is 90, and the current time is 105. Alice decides to repay her debt. The _addInterest function is executed, and the penalty rate is also applied to the interest accrual and the maturity date. As a result, Alice owes more in interest than she should. Recommendations Short term, modify the associated code so that if the _isPastMaturity branch is taken and the _currentRateInfo.lastTimestamp value is less than maturityDate value, the penalty interest rate is applied only for the amount of time after the maturity date. Long term, identify edge cases that could occur in the interest accrual process and implement unit tests and fuzz tests to validate them.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "4. Improper validation of Chainlink data ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The current validation of the values returned by Chainlinks latestRoundData function could result in the use of stale data. The latestRoundData function returns the following values: the answer , the roundId (which represents the current round), the answeredInRound value (which corresponds to the round in which the answer was computed), and the updatedAt value (which is the timestamp of when the round was updated). An updatedAt value of zero means that the round is not complete and should not be used. An answeredInRound value that is less than the roundId could indicate stale data. However, the _updateExchangeRate function does not check for these conditions. function _updateExchangeRate () internal returns ( uint256 _exchangeRate ) { // [...] uint256 _price = uint256 (1e36); if (oracleMultiply != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleMultiply).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleMultiply); } _price = _price * uint256 (_answer); } if (oracleDivide != address ( 0 )) { (, int256 _answer , , , ) = AggregatorV3Interface(oracleDivide).latestRoundData(); if (_answer <= 0 ) { revert OracleLTEZero(oracleDivide); } _price = _price / uint256 (_answer); } // [...] } Figure 4.1: The _updateExchangeRate function in FraxlendPairCore.sol#L513-L544 Exploit Scenario Chainlink is not updated correctly in the current round, and Eve, who should be liquidated with the real collateral asset price, is not liquidated because the price reported is outdated and is higher than it is in reality. Recommendations Short term, have _updateExchangeRate perform the following sanity check: require(updatedAt != 0 && answeredInRound == roundId) . This check will ensure that the round has nished and that the pricing data is from the current round. Long term, when integrating with third-party protocols, make sure to accurately read their documentation and implement the appropriate sanity checks.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "5. Risk of oracle outages ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "Under extreme market conditions, the Chainlink oracle may cease to work as expected, causing unexpected behavior in the Fraxlend protocol. Such oracle issues have occurred in the past. For example, during the LUNA market crash, the Venus protocol was exploited because Chainlink stopped providing up-to-date prices. The interruption occurred because the price of LUNA dropped below the minimum price ( minAnswer ) allowed by the LUNA/USD price feed on the BNB chain. As a result, all oracle updates reverted. Chainlinks automatic circuit breakers , which will pause price feeds during extreme market conditions, could pose similar problems. Note that these kinds of events cannot be tracked on-chain. If a price feed is paused, updatedAt will still be greater than zero, and answeredInRound will still be equal to roundId . Therefore, the Frax Finance team should implement an o-chain monitoring solution to detect any anomalous behavior exhibited by Chainlink oracles. Recommendations Short term, implement an o-chain monitoring solution that checks for the following conditions and issues alerts if they occur, as they may be indicative of abnormal market events:    An asset price that is approaching the minAnswer or maxAnswer value The suspension of a price feed by an automatic circuit breaker Any large deviations in the price of an asset", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "6. Unapproved lenders could receive fTokens ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "A Fraxlend custom pair can include a list of approved lenders; these are the only lenders who can deposit the underlying asset into the given pair and receive the corresponding fTokens. However, the system does not perform checks when users transfer fTokens; as a result, approved lenders could send fTokens to unapproved addresses. Although unapproved addresses can only redeem fTokens sent to themmeaning this issue is not security-criticalthe ability for approved lenders to send fTokens to unapproved addresses conicts with the currently documented behavior. function deposit ( uint256 _amount , address _receiver ) external nonReentrant isNotPastMaturity whenNotPaused approvedLender(_receiver) returns ( uint256 _sharesReceived ) {...} Figure 6.1: The deposit function in FraxlendPairCore.sol#L587-L594 Exploit Scenario Bob, an approved lender, deposits 100 asset tokens and receives 90 fTokens. He then sends the fTokens to an unapproved address, causing other users to worry about the state of the protocol. Recommendations Short term, override the _beforeTokenTransfer function by applying the approvedLender modier to it. Alternatively, document the ability for approved lenders to send fTokens to unapproved addresses. Long term, when applying access controls to token owners, make sure to evaluate all the possible ways in which a token can be transferred and document the expected behavior.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "7. FraxlendPairDeployer cannot deploy contracts of fewer than 13,000 bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The FraxlendPairDeployer contract, which is used to deploy new pairs, does not allow contracts that contain less than 13,000 bytes of code to be deployed. To deploy new pairs, users call the deploy or deployCustom function, which then internally calls _deployFirst . This function uses the create2 opcode to create a contract for the pair by concatenating the bytecode stored in contractAddress1 and contractAddress2 . The setCreationCode function, which uses solmates SSTORE2 library to store the bytecode for use by create2 , splits the bytecode into two separate contracts ( contractAddress1 and contractAddress2 ) if the _creationCode size is greater than 13,000. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 7.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 The rst problem is that if the _creationCode size is less than 13,000, BytesLib.slice will revert with the slice_outOfBounds error, as shown in gure 7.2. function slice ( bytes memory _bytes, uint256 _start , uint256 _length ) internal pure returns ( bytes memory ) { require (_length + 31 >= _length, \"slice_overflow\" ); require (_bytes.length >= _start + _length, \"slice_outOfBounds\" ); Figure 7.2: The BytesLib.slice function from the solidity-bytes-utils library Assuming that the rst problem does not exist, another problem arises from the use of SSTORE2.read in the _deployFirst function (gure 7.3). If the creation code was less than 13,000 bytes, contractAddress2 would be set to address(0) . This would cause the SSTORE2.read functions pointer.code.length - DATA_OFFSET computation, shown in gure 7.4, to underow, causing the SSTORE2.read operation to panic. function _deployFirst ( // [...] ) private returns ( address _pairAddress ) { { // [...] bytes memory _creationCode = BytesLib.concat( SSTORE2.read(contractAddress1), SSTORE2.read(contractAddress2) ); Figure 7.3: The _deployFirst function in FraxlendPairDeployer.sol#L212-L231 uint256 internal constant DATA_OFFSET = 1 ; function read ( address pointer ) internal view returns ( bytes memory ) { return readBytecode(pointer, DATA_OFFSET, pointer.code.length - DATA_OFFSET ); } Figure 7.4: The SSTORE2.read function from the solmate library Exploit Scenario Bob, the FraxlendPairDeployer contracts owner, wants to set the creation code to be a contract with fewer than 13,000 bytes. When he calls setCreationCode , it reverts. Recommendations Short term, make the following changes:   In setCreationCode , in the line that sets the _firstHalf variable, replace 13000 in the third argument of BytesLib.slice with min(13000, _creationCode.length) . In _deployFirst , add a check to ensure that the SSTORE2.read(contractAddress2) operation executes only if contractAddress2 is not address(0) . Alternatively, document the fact that it is not possible to deploy contracts with fewer than 13,000 bytes. Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "8. setCreationCode fails to overwrite _secondHalf slice if updated code size is less than 13,000 bytes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The setCreationCode function permits the owner of FraxlendPairDeployer to set the bytecode that will be used to create contracts for newly deployed pairs. If the _creationCode size is greater than 13,000 bytes, it will be split into two separate contracts ( contractAddress1 and contractAddress2 ). However (assuming that TOB-FXLEND-7 were xed), if a FraxlendPairDeployer owner were to change the creation code from one of greater than 13,000 bytes to one of fewer than 13,000 bytes, contractAddress2 would not be reset to address(0) ; therefore, contractAddress2 would still contain the second half of the previous creation code. function setCreationCode ( bytes calldata _creationCode) external onlyOwner { bytes memory _firstHalf = BytesLib.slice(_creationCode, 0 , 13000 ); contractAddress1 = SSTORE2.write(_firstHalf); if (_creationCode.length > 13000 ) { bytes memory _secondHalf = BytesLib.slice(_creationCode, 13000 , _creationCode.length - 13000 ); contractAddress2 = SSTORE2.write(_secondHalf); } } Figure 8.1: The setCreationCode function in FraxlendPairDeployer.sol#L173-L180 Exploit Scenario Bob, FraxlendPairDeployer s owner, changes the creation code from one of more than 13,000 bytes to one of less than 13,000 bytes. As a result, deploy and deployCustom deploy contracts with unexpected bytecode. Recommendations Short term, modify the setCreationCode function so that it sets contractAddress2 to address(0) at the beginning of the function . Long term, improve the projects unit tests and fuzz tests to check that the functions behave as expected and cannot unexpectedly revert.", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Medium"]}, {"title": "9. Missing checks in setter functions ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The setFee and setMinWaitPeriods functions do not have appropriate checks. First, the setFee function does not have an upper limit, which means that the Fraxferry owner can set enormous fees. Second, the setMinWaitPeriods function does not require the new value to be at least one hour. A minimum waiting time of less than one hour would invalidate important safety assumptions. For example, in the event of a reorganization on the source chain, the minimum one-hour waiting time ensures that only transactions after the reorganization are ferried (as described in the code comment in gure 9.1). ** - Reorgs on the source chain. Avoided, by only returning the transactions on the source chain that are at least one hour old. ** - Rollbacks of optimistic rollups. Avoided by running a node. ** - Operators do not have enough time to pause the chain after a fake proposal. Avoided by requiring a minimal amount of time between sending the proposal and executing it. // [...] function setFee ( uint _FEE ) external isOwner { FEE=_FEE; emit SetFee(_FEE); } function setMinWaitPeriods ( uint _MIN_WAIT_PERIOD_ADD , uint _MIN_WAIT_PERIOD_EXECUTE ) external isOwner { MIN_WAIT_PERIOD_ADD=_MIN_WAIT_PERIOD_ADD; MIN_WAIT_PERIOD_EXECUTE=_MIN_WAIT_PERIOD_EXECUTE; emit SetMinWaitPeriods(_MIN_WAIT_PERIOD_ADD, _MIN_WAIT_PERIOD_EXECUTE); } Figure 9.1: The setFee and setMinWaitPeriods functions in Fraxferry.sol#L226-L235 Exploit Scenario Bob, Fraxferry s owner, calls setMinWaitPeriods with a _MIN_WAIT_PERIOD_ADD value lower than 3,600 (one hour) , invalidating the waiting periods protection regarding chain reorganizations. Recommendations Short term, add an upper limit check to the setFee function; add a check to the setMinWaitPeriods function to ensure that _MIN_WAIT_PERIOD_ADD and _MIN_WAIT_PERIOD_EXECUTE are at least 3,600 (one hour). Long term, make sure that conguration variables can be set only to valid values.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "10. Risk of invalid batches due to unsafe cast in depart function ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The depart function performs an unsafe cast operation that could result in an invalid batch. Users who want to send tokens to a certain chain use the various embark* functions. These functions eventually call embarkWithRecipient , which adds the relevant transactions to the transactions array. function embarkWithRecipient ( uint amount , address recipient ) public notPaused { // [...] transactions.push(Transaction(recipient,amountAfterFee, uint32 ( block.timestamp ))); } Figure 10.1: The embarkWithRecipient function in Fraxferry.sol#L127-L135 At a certain point, the captain role calls depart with the start and end indices within transactions to specify the transactions inside of a batch. However, the depart function performs an unsafe cast operation when creating the new batch; because of this unsafe cast operation, an end value greater than 2 ** 64 would be cast to a value lower than the start value, breaking the invariant that end is greater than or equal to start . function depart ( uint start , uint end , bytes32 hash ) external notPaused isCaptain { require ((batches.length== 0 && start== 0 ) || (batches.length> 0 && start==batches[batches.length- 1 ].end+ 1 ), \"Wrong start\" ); require (end>=start, \"Wrong end\" ); batches.push(Batch( uint64 (start), uint64 (end), uint64 ( block.timestamp ), 0 , hash )); emit Depart(batches.length- 1 ,start,end, hash ); } Figure 10.2: The depart function in Fraxferry.sol#L155-L160 If the resulting incorrect batch is not disputed by the crew member roles, which would cause the system to enter a paused state, the rst ocer role will call disembark to actually execute the transactions on the target chain. However, the disembark functions third check, highlighted in gure 10.3, on the invalid transaction will fail, causing the transaction to revert and the system to stop working until the incorrect batch is removed with a call to removeBatches . function disembark (BatchData calldata batchData) external notPaused isFirstOfficer { Batch memory batch = batches[executeIndex++]; require (batch.status== 0 , \"Batch disputed\" ); require (batch.start==batchData.startTransactionNo, \"Wrong start\" ); require (batch.start+batchData.transactions.length- 1 ==batch.end, \"Wrong size\" ); require ( block.timestamp -batch.departureTime>=MIN_WAIT_PERIOD_EXECUTE, \"Too soon\" ); // [...] } Figure 10.3: The disembark function in Fraxferry.sol#L162-L178 Exploit Scenario Bob, Fraxferry s captain, calls depart with an end value greater than 2 ** 64 , which is cast to a value less than start . As a consequence, the system becomes unavailable either because the crew members called disputeBatch or because the disembark function reverts. Recommendations Short term, replace the unsafe cast operation in the depart function with a safe cast operation to ensure that the end >= start invariant holds. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "11. Transactions that were already executed can be canceled ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The Fraxferry contracts owner can call the jettison or jettisonGroup functions to cancel a transaction or a series of transactions, respectively. However, these functions incorrectly use the executeIndex variable to determine whether the given transaction has already been executed. As a result, it is possible to cancel an already executed transaction. The problem is that executeIndex tracks executed batches, not executed transactions. Because a batch can contain more than one transaction, the check in the _jettison function (gure 11.1) does not work correctly. function _jettison ( uint index , bool cancel ) internal { require (index>=executeIndex, \"Transaction already executed\" ); cancelled[index]=cancel; emit Cancelled(index,cancel); } function jettison ( uint index , bool cancel ) external isOwner { _jettison(index,cancel); } function jettisonGroup ( uint [] calldata indexes, bool cancel ) external isOwner { for ( uint i = 0 ;i<indexes.length;++i) { _jettison(indexes[i],cancel); } } Figure 11.1: The _jettison , jettison , and jettisonGroup functions in Fraxferry.sol#L208-L222 Note that canceling a transaction that has already been executed does not cancel its eects (i.e., the tokens were already sent to the receiver). Exploit Scenario Two batches of 10 transactions are executed; executeIndex is now 2 . Bob, Fraxferry s owner, calls jettison with an index value of 13 to cancel one of these transactions. The call to jettison should revert, but it is executed correctly. The emitted Cancelled event shows that a transaction that had already been executed was canceled, confusing the o-chain monitoring system. Recommendations Short term, use a dierent index in the jettison and jettisonGroup functions to track executed transactions. Long term, implement robust unit tests and fuzz tests to check that important invariants hold.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "12. Lack of contract existence check on low-level call ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The execute function includes a low-level call operation without a contract existence check; call operations return true even if the _to address is not a contract, so it is important to include contract existence checks alongside such operations. // Generic proxy function execute ( address _to , uint256 _value , bytes calldata _data) external isOwner returns ( bool , bytes memory ) { ( bool success , bytes memory result) = _to.call{value:_value}(_data); return (success, result); } Figure 12.1: The execute function in Fraxferry.sol#L274-L278 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 12.2: A snippet of the Solidity documentation detailing unexpected behavior related to call Exploit Scenario Bob, Fraxferry s owner, calls execute with _to set to an address that should be a contract; however, the contract was self-destructed. Even though the contract at this address no longer exists, the operation still succeeds. Recommendations Short term, implement a contract existence check before the call operation in the execute function. If the call operation is expected to send ETH to an externally owned address, ensure that the check is performed only if the _data.length is not zero. Long term, carefully review the Solidity documentation , especially the Warnings section.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "13. Events could be improved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-10-fraxfinance-fraxlend-fraxferry-securityreview.pdf", "body": "The events declared in the Fraxferry contract could be improved to be more useful to users and monitoring systems. Certain events could be more useful if they used the indexed keyword. For example, in the Embark event, the indexed keyword could be applied to the sender parameter. Additionally, SetCaptain , SetFirstOfficier , SetFee , and SetMinWaitPeriods could be more useful if they emitted the previous value in addition to the newly set one. event Embark ( address sender , uint index , uint amount , uint amountAfterFee , uint timestamp ); event Disembark ( uint start , uint end , bytes32 hash ); event Depart ( uint batchNo , uint start , uint end , bytes32 hash ); event RemoveBatch ( uint batchNo ); event DisputeBatch ( uint batchNo , bytes32 hash ); event Cancelled ( uint index , bool cancel ); event Pause ( bool paused ); event OwnerNominated ( address newOwner ); event OwnerChanged ( address previousOwner , address newOwner ); event SetCaptain ( address newCaptain ); event SetFirstOfficer ( address newFirstOfficer ); event SetCrewmember ( address crewmember , bool set ); event SetFee ( uint fee ); event SetMinWaitPeriods ( uint minWaitAdd , uint minWaitExecute ); Figure 13.1: Events declared in Fraxferry.sol#L83-L96 Recommendations Short term, add the indexed keyword to any events that could benet from it; modify events that report on setter operations so that they report the previous values in addition to the newly set values.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "1. Ability to drain a pool by reusing a ash_loan_end index ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "The lack of validation of the flash_loan_end transaction could enable an attacker to drain a pool of its funds by reusing the same repayment transaction for multiple loans. Flash loan operations are split into two transactions included in the same group: a flash_loan_begin transaction (shown in gure 1.1) and a flash_loan_end transaction (shown in gure 1.2). CODE REDACTED CODE REDACTED Figure 1.1: REDACTED Figure 1.2: REDACTED The flash_loan_begin method sends the assets to the user taking out the ash loan and checks that there is an associated flash_loan_end transaction later in the transaction group. The flash_loan_end method ensures that the associated repayment transaction ( send_asset_txn ) has the correct amount value. CODE REDACTED Figure 1.3: REDACTED The flash_loan_key value serves as a mutex and is used to prevent a user from taking out a new ash loan before the users previous ash loan is complete. The flash_loan_begin method checks that flash_loan_key is set to 0 and then sets it to 1; flash_loan_end checks that flash_loan_key is set to 1 and then sets it to 0. However, there is no validation of whether the flash_loan_end transaction at the index passed to flash_loan_begin is the one that resets the flash_loan_key mutex. An attacker could reuse the same repayment in flash_loan_end transaction in multiple calls to flash_loan_begin , as long as he created additional calls to flash_loan_end (with any amount value) to reset the mutex. Thus, an attacker could drain a pool by taking out ash loans without repaying them. Exploit Scenario Eve creates a group of six transactions: 1. flash_loan_begin(1000, 5, ..) 2. An asset transfer with an amount of 0 3. flash_loan_end() (which serves only to reset the flash_loan_begin mutex) 4. flash_loan_begin(1000, 5, ..) 5. An asset transfer with an amount of 1000 6. flash_loan_end() Transactions 1 and 4 credit Eve with 2,000 tokens (1,000 tokens per transaction). Transactions 2 and 3 serve only to reset the mutex. Transactions 5 and 6 repay one of the ash loans by transferring 1,000 tokens. Thus, Eve receives 1,000 tokens for free. (Note that for the sake of simplicity, this exploit scenario ignores the fees that would normally be paid.) Recommendations Short term, store the amount to be repaid in flash_loan_key , and ensure that the correct amount is repaid. Long term, create schemas highlighting the relationships between the transactions, and document the invariants related to those relationships.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "2. Lack of a two-step process for admin role transfers ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "The Folks Finance methods used to transfer the admin role from one address to another perform those transfers in a single step, immediately updating the admin address. Making such a critical change in a single step is error-prone and can lead to irrevocable mistakes. These methods include the update_admin method (gure 2.1), which is used to update the address of the pool_manager applications admin. If the update_admin method were called with an incorrect address, it would no longer be possible to execute administrative actions such as the addition of a pool. CODE REDACTED Figure 2.1: REDACTED The update_admin methods of the pool , loan , lp_token_oracle , and oracle_adapter applications also perform admin role transfers in a single step. Exploit Scenario Alice, the admin of the pool_manager application, calls the update_admin method with an incorrect address. As a result, she permanently loses access to the admin role, and new pools cannot be added to the pool_manager application. Recommendations Short term, implement a two-step process for admin role transfers. One way to do this would be splitting each update_admin method into two methods: a propose_admin method that saves the address of the proposed new admin to the global state and an accept_admin method that nalizes the transfer of the role (and must be called by the address of the new admin). Long term, identify and document all possible actions that can be taken by privileged accounts and their associated risks. This will facilitate reviews of the codebase and help prevent future mistakes. 3. Insu\u0000cient validation of application initialization arguments Severity: Low Diculty: High Type: Data Validation Finding ID: TOB-FOLKS-3 Target: pool_manager.py , pool.py , loan.py , lp_token_oracle.py , oracle_adapter.py", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "4. Ability to reuse swap indexes ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "The lack of validation of the swap_collateral_end transaction enables reuse of the same swap_collateral_end transaction for multiple swap operations. Swaps are split into two transactions included in the same group: a swap_collateral_begin transaction (shown in gure 4.1) and a swap_collateral_end transaction (shown in gure 4.2). CODE REDACTED CODE REDACTED Figure 4.1: REDACTED Figure 4.2: REDACTED The swap_collateral_begin method sends the assets to the user executing the swap and checks that there is an associated swap_collateral_end transaction later in the transaction group. The swap_collateral_end method ensures that the loan is overcollateralized: CODE REDACTED Figure 4.3: REDACTED The methods use a mutex to prevent a user from starting a new swap loan before the users previous one is complete. The swap_collateral_begin method uses loan_not_blocked_check to check that the loan is not blocked; swap_collateral_end uses loan_blocked_check to check that the loan is blocked. However, there is no validation of whether the swap_collateral_end transaction at the index passed to swap_collateral_begin is the one that resets the loan block mutex. An attacker could reuse the same swap_collateral_end transaction in multiple calls to swap_collateral_begin , as long as he created additional calls to swap_collateral_end to reset the mutex. We set the severity of this nding to informational because it does not pose a direct threat to the system: despite this issue, a user must execute a call to swap_collateral_end between two calls to swap_collateral_begin , and the related loan must still be overcollateralized. However, the fact that the swap_collateral_begin transaction is not correlated to the swap_collateral_end transaction could lead to additional issues if the code is refactored. (See TOB-FOLKS-1 for details on a similar issue.) Recommendations Short term, consider storing the IDs of blocked operations, and ensure that the swap_collateral_begin and swap_collateral_end transactions are properly correlated with each other. Long term, create schemas highlighting the relationships between the transactions, and document the invariants related to those relationships.", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "5. oracle_adapter could be forced to use outdated LP token information in price calculations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "Because of the insucient validation of encoded byte arrays of compound type arguments, an attacker could force the oracle_adapter application to use outdated information when calculating a liquidity provider (LP) token price. The computation of an LP token price involves two transactions: update_lp_tokens(asset_ids) and refresh_prices(lp_assets, ..) . The update_lp_tokens method updates the supply of the LP token, while refresh_prices computes the LP tokens price. The refresh_prices method calls check_lps_updated , which checks that update_lp_tokens has been called and that the update_lp_tokens.asset_ids array is equal to the update_lp_tokens.lp_asset_ids array. CODE REDACTED Figure 5.1: REDACTED Instead of directly comparing asset_ids to lp_asset_ids , check_lps_updated calls convert_uint64_abi_array_to_uint64_bytes_array to convert the arrays into byte arrays; convert_uint64_abi_array_to_uint64_bytes_array removes the rst two bytes of the array (which indicate the length of the array) and returns the remaining bytes: CODE REDACTED Figure 5.2: REDACTED PyTeal does not provide any guarantees about the structure of compound type arguments. The PyTeal documentation includes the following warning: Figure 5.3: pyteal.readthedocs.io/en/stable/abi.html#registering-methods When data of the uint64 type is converted into a byte array, the bytes length may not match the uint64 values length. If that data is passed to a function that takes a uint64[] parameter, it may be a byte longer than the function expects. Even if the data extracted as the bytes of asset_ids and lp_asset_ids is equivalent, the length of the original arrays might not be. Thus, update_lp_tokens could be called with an lp_asset_ids array that is shorter than the refresh_prices.lp_asset_ids array. In that case, the LP information would not be updated, and the price of the LP token would be based on outdated information. Exploit Scenario Bob holds a position that is eligible for liquidation. However, the AMM pool state changes, causing the price of the LP token that Bob is using as collateral to increase; thus, the loan is safe again, and Bob does not add more collateral. Eve notices that the oracle is still using old information on the LP token. Eve then creates a group of three transactions:  lp_token_oracle.update_lp_tokens(0x0000 + 0xdeadbeefdeadbeef)  oracle_adapter.refresh_prices(0x0001 + 0xdeadbeefdeadbeef, ..)  liquidate(...) The check_lps_updated method veries that lp_token_oracle and oracle_adapter are using the same bytes ( 0xdeadbeefdeadbeef ); however, update_lp_tokens interprets its parameter as an array with a length of zero ( 0x0000 ). As a result, the LP token information is not updated, and the old price is used, enabling Eve to liquidate Bobs position. Recommendations Short term, have the oracle_adapter use the two dynamic arrays ( updated_lp_assets and lp_asset_ids ) directly and extract individual elements to perform an element-wise comparison. Long term, avoid relying on internal structures used by the compiler. Review the PyTeal documentation and test edge cases more broadly.", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Incorrect rounding directions in the calculation of borrowed asset amounts ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "Multiple incorrect rounding directions are used in the computation of the amount borrowed in a loan. Thus, the result of the calculation may be too low, causing the system to underestimate the amount of assets borrowed in a loan. To determine whether a loan is overcollateralized, is_loan_over_collateralized iterates over an array of all collateral assets and sums the underlying values of those assets: CODE REDACTED Figure 6.1: REDACTED As part of this process, it calls get_stable_borrow_balance and get_var_borrow_balance , both of which call calc_borrow_balance to calculate the borrow balance of the loan at time t : CODE REDACTED Figure 6.2: REDACTED The operation performed by the calc_borrow_balance function is equivalent to that shown in gure 6.3: CODE REDACTED Figure 6.3: REDACTED The function adds 1 to the result of the equation to round it up. However, the 14 * 10      1 portion of the equation rounds down, which can cause the overall rounding error to be greater than 1. Similar issues are present in other functions involved in the computation, including the following:  calc_asset_loan_value , which rounds down the results of its two calls to mul_scale  calc_borrow_interest_index , which rounds down the result of the mul_scale call  exp_by_squaring , which also rounds down the result of the mul_scale call The cumulative loss of precision can cause the system to underestimate the amount of assets borrowed in a loan, preventing the loans liquidation. We set the severity of this issue to low because the loss of precision is limited. However, there may be other rounding issues present in the codebase. Exploit Scenario Eves loan has become undercollateralized. However, because the loan contract rounds down when calculating the amount borrowed in a loan, it does not identify Eves loan as undercollateralized, and the position cannot be liquidated. By contrast, if the contract performed precise accounting, Eves loan would be eligible for liquidation. Recommendations Short term, ensure all arithmetic operations in is_loan_over_collateralized use a conservative rounding directionthat is, ensure that the loss of precision causes the system to interpret a loan as less collateralized than it actually is. Additionally, document those operations. Long term, document the expected rounding direction of every arithmetic operation, and create rounding-specic functions (e.g., mul_scale_down and mul_scale_down_up ) to facilitate reviews of the arithmetic rounding.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "7. Risk of global state variable collision ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "The layout of the loan applications global state could cause a loans params variable to collide with a pool variable. A loan has two types of variables:  params , which is set by the owner and contains the loan parameters  pools , which contains pool information CODE REDACTED DOCUMENTATION REDACTED Figure 7.1: REDACTED Figure 7.2: REDACTED The params variable is stored at oset 112 (Bytes(p)) . The pools variable contains an array in which every slot contains 3 loans, and only slots 062 are assumed to be used for loans. When the pool s slots are used, there is no guarantee that the global state is being accessed through slots 062. This means that slot 112 can be used to store a loan. We set the diculty rating of this issue to high because exploitation of the issue would likely require exploitation of another bug. This is because if slot 112 were used for a loan, its underlying values would likely not be directly usable, particularly because of the following:   The rst element of params is an admin address. The rst element of a loan variable is the pool application ID.  If params collides with a pool variable, its admin address must collide with an application ID. Exploit Scenario Eve nds a lack of validation in the loan ow that allows her to trick the loan application into believing that there is a loan at slot 112. Eve uses the variable collision to change the systems parameters and update the oracle_adapter ID. As a result, the system stops working. Recommendations Short term, store the params variable at the oset (Bytes(params)) . Because loan indexes are uint8 values, using a key with a value greater than 255 will prevent a collision. Long term, create documentation on the management of the global state, and use unit and fuzz testing to check for potential collisions.", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Lack of documentation on strategies in case of system parameter update ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "Malicious users of the Folks Finance capital market protocol could try to game the system and earn more than honest users. When users deposit assets into the protocol, they receive interest-bearing assets known as f-assets. To withdraw their original assets, users must return those f-assets. The amount of f-assets provided to a user upon a deposit (as well as the amount of the original asset collected during a withdrawal) depends on the deposit interest index,  .   DOCUMENTATION REDACTED Figure 8.1: REDACTED The value of    slowly increases over time, and certain actions can increase or decrease its rate of increase. For example, when someone borrows assets, the value of will increase    at a faster rate. By contrast, a deposit of additional assets or the repayment of a loan will cause to increase at a slower rate.    Active lenders with knowledge of this behavior can prioritize strategies that will maximize their prots, giving them an advantage over passive lenders. It is possible that updates to the systems parameters could also aect the way that the value of changes; however, determining whether that is the case would require further    investigation. Exploit Scenario Eve learns that Bob is going to borrow assets worth USD 10 million. Eve provides liquidity just before the execution of Bobs transaction and withdraws it right after. In this way, she earns fees from the protocol without participating in the protocol. Recommendations Short term, document the expected behavior of lenders and borrowers, and consider implementing a deposit lockup period. Long term, model and document the strategies that users are expected to leverage. Additionally, evaluate the impact of system parameter updates on the protocol. References   https://uniswap.org/blog/jit-liquidity https://medium.com/@peter_4205/curve-vulnerability-report-a1d7630140ec", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "9. Incorrect decoding of method arguments results in the use of invalid values ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "Certain methods use the Btoi instruction to decode the arguments of the other transactions in their group, resulting in the use of incorrect values. Most of the protocol operations involve a group of multiple transactions. In some cases, one method uses or validates the other transactions in the group and must decode their arguments. One such method is loan.add_pool (gure 9.1), which uses an argument of a pool application transaction. CODE REDACTED Figure 9.1: REDACTED The add_pool method decodes the rst argument of the pool.add_loan_application method, which is an index argument of type uint8 . The pool application decodes this argument by using the get_byte operation to extract the rst byte of the argument. However, add_pool decodes the argument by using the Btoi instruction, which is not the equivalent of extracting the rst byte. Specically, Btoi(0x00..0X) would return X , and get_byte(0, 0x00..0X) would return 0 . This results in the use of dierent values in the two methods and causes the system to enter an invalid state. This issue also aects loan.swap_collateral_begin (gure 9.2), which decodes an argument of loan.swap_collateral_end for validation. CODE REDACTED Figure 9.2: REDACTED The swap_collateral_begin method also uses Btoi for decoding, while correctly decoding the argument would require extraction of the rst byte. However, the issue has a limited impact on the collateral swap operation: a value of 0x00..0X would cause swap_collateral_end to use the account at index 0 of the transactions accounts array, which cannot be a valid escrow account. Exploit Scenario Alice, the admin of the loan application, creates a group of two transactions: 1. pool.add_loan_application(0x0000000000000001, ...) 2 . loan.add_pool(...) The loan application decodes the add_loan_application methods index as 1, whereas the pool application uses 0 as the index. The discrepancy causes the system to enter an invalid state. Recommendations Short term, use the compiler-provided decode() method ( from the abi.{type} object) to decode application arguments. Long term, avoid relying on compiler internals. Review the PyTeal documentation and test edge cases more broadly.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "10. Lack of minimum / maximum bounds on user operation parameters ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2022-11-folksfinance-securityreview.pdf", "body": "The outcomes of several pool and loan operations are dependent on the system state. This means that users have no on-chain guarantees that their transactions will produce the outcomes they expect. Examples of this issue include the following:    The caller of pool.deposit may receive less f-assets than expected (e.g., zero f-assets in exchange for a small deposit). If the amount value passed to pool.withdraw is variable, the user may receive less assets than expected. An update to the retention rate would aect the outcomes of all loan operations that use the retention rate. Note that loan.borrow and loan.switch_borrow_type do ha ve a max_stable_rate parameter. Exploit Scenario Bob calls pool.deposit with a small amount of assets but does not receive any f-assets in return. Recommendations Short term, add minimum and maximum bounds on the parameters of all user operations. Long term, document the front-running risks associated with each operation, and ensure that there are proper mitigations in place for those risks.", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "1. Hash collisions in untyped signatures Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "To post or execute a swap, a user must provide an ECDSA signature on a message containing the encoded swap information. The Meson protocol supports both typed (EIP-712) and legacy untyped (EIP-191) messages. The format of a message is determined by a bit in the encoded swap information itself. The Meson protocol denes two message types, a request message containing only an encoded swap and a release message containing the hash of an encoded swap concatenated with the recipients address. Figure 1.1 shows the relevant signature-verication code. 213 ... 237 238 239 function _checkRequestSignature( if (nonTyped) { bytes32 digest = keccak256(abi.encodePacked( bytes28(0x19457468657265756d205369676e6564204d6573736167653a0a3332), // HEX of \"\\x19Ethereum Signed Message:\\n32\" 240 241 242 243 244 ... 266 ... 293 294 295 encodedSwap )); require(signer == ecrecover(digest, v, r, s), \"Invalid signature\"); return; } function _checkReleaseSignature( if (nonTyped) { digest = keccak256(abi.encodePacked( bytes28(0x19457468657265756d205369676e6564204d6573736167653a0a3332), // HEX of \"\\x19Ethereum Signed Message:\\n32\" 296 297 ... keccak256(abi.encodePacked(encodedSwap, recipient)) )); Figure 1.1: contracts/utils/MesonHelpers.sol 11 Meson Protocol Fix Review Note that the form of both the request and release messages in the gure is \"\\x19Ethereum Signed Message:\\n32\" + msg, where msg is a 32-byte string. If an attacker could nd a message that would be interpreted as valid in both contexts, the attacker could use the signature on that message to both request and release funds, facilitating a number of potential attacks. Specically, the attacker would need to identify swap1, swap2, and recipient values such that swap1 = keccak256(swap2, recipient). The attacker could do that by choosing a valid swap2 value and then iterating through recipient values until nding one for which keccak256(swap2, recipient) would be interpreted as a valid message. With the current restrictions on the swap amount, chain, and token elds, we estimate that this would take between 260 and 270 tries. Fix Analysis This issue has been resolved. Untyped release messages are now prexed by the string \"\\x19Ethereum Signed Message:\\n52\", while request messages are prexed by \"\\x19Ethereum Signed Message:\\n32\". However, if Meson ever introduces new message types with a length of 32 or 53 bytes, their encodings may collide with the encodings of the existing message types. 12 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "2. Typed signatures implement insecure nonstandard encodings Status: Unresolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "EIP-712 species standard encodings for the hashing and signing of typed structured data. The goal of typed structured signing standards is twofold: ensuring a unique injective encoding for structured data in order to prevent collisions (like that detailed in TOB-MES-1) and allowing wallets to display complex structured messages unambiguously in human-readable form. The images in gure 2.1 demonstrate the dierence between a complex untyped unstructured message (left) and its EIP-712 equivalent (right), both in MetaMask: Figure 2.1: A reproduction of images from the EIP-712 standard Meson currently uses a form of typed message encoding that does not conform to EIP-712. Specically, the encoding is not EIP-191 compliant and thus could theoretically collide with 13 Meson Protocol Fix Review the encoding of personal messages (Ethereum signed messages) or Recursive Length Prex (RLP)-encoded transactions. The digest format for swap requests is included in gure 2.2, in which REQUEST_TYPE_HASH corresponds to keccak256(\"bytes32 Sign to request a swap on Meson (Testnet)\"). bytes32 typehash = REQUEST_TYPE_HASH; bytes32 digest; 246 247 248 assembly { 249 250 251 252 253 } mstore(0, encodedSwap) mstore(32, keccak256(0, 32)) mstore(0, typehash) digest := keccak256(0, 64) Figure 2.2: contracts/utils/MesonHelpers.sol#246253 While the message types currently used in the protocol do not appear to have any dangerous interactions with each other, message types added to future versions of the protocol could theoretically introduce such issues. Fix Analysis This issue has not been resolved. Although the issue is not currently exploitable, we recommend that Meson exercise caution when adding new message types to prevent unexpected collisions between those message types and message types used by other protocols. 14 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "3. Missing validation in the _addSupportToken function Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "Insucient input validation in the _addSupportToken function makes it possible to register the same token as supported multiple times. This does not cause a problem, because if there are duplicate entries for a token in the token list, the last one added will be the one that is used. However, it does mean that multiple indexes could point to the same token, while the token would point to only one of those indexes. function _addSupportToken(address token, uint8 index) internal { require(index != 0, \"Cannot use 0 as token index\"); _indexOfToken[token] = index; _tokenList[index] = token; 47 48 49 50 51 } Figure 3.1: contracts/utils/MesonTokens.sol Fix Analysis This issue has been resolved. The _addSupportToken function now validates that the token has not previously been registered, that the associated list index has not previously been used, and that the tokens address is not zero. The Meson team has also added tests to validate this behavior. 15 Meson Protocol Fix Review 4. Insu\u0000cient event generation Status: Resolved Severity: Informational Diculty: Low Type: Auditing and Logging Finding ID: TOB-MES-4 Target: contracts/Pools/MesonPools.sol", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "5. Use of an uninitialized state variable in functions Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "The _mesonContract address is not set in the UCTUpgradeable contracts initialize function during the contracts initialization. As a result, the value of _mesonContract defaults to the zero address. The UCTUpgradeable.allowance and UCTUpgradeable.transferFrom functions perform checks that rely on the value of the _mesonContract state variable, which may lead to unexpected behavior. address private _mesonContract; function initialize(address minter) public initializer { __ERC20_init(\"USD Coupon Token (https://meson.fi)\", \"UCT\"); _owner = _msgSender(); _minter = minter; // _mesonContract = ; 18 19 20 21 22 23 24 25 } Figure 5.1: contracts/Token/UCTUpgradeable.sol:1825 54 function allowance(address owner, address spender) public view override returns (uint256) { 55 if (spender == _mesonContract) { Figure 5.2: contracts/Token/UCTUpgradeable.sol:5455 65 if (msgSender == _mesonContract && ERC20Upgradeable.allowance(sender, msgSender) < amount) { Figure 5.3: contracts/Token/UCTUpgradeable.sol:65 Fix Analysis This issue has been resolved. The _mesonContract address is now populated by the initialize function. 17 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. Risk of upgrade issues due to missing __gap variable Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "None of the Meson protocol contracts include a __gap variable. Without this variable, it is not possible to add any new variables to the inherited contracts without causing storage slot issues. Specically, if variables are added to an inherited contract, the storage slots of all subsequent variables in the contract will shift by the number of variables added. Such a shift would likely break the contract. All upgradeable OpenZeppelin contracts contain a __gap variable, as shown in gure 6.1. 89 90 /** * @dev This empty reserved space is put in place to allow future versions to add new 91 92 93 94 * variables without shifting down storage in the inheritance chain. * See https://docs.openzeppelin.com/contracts/4.x/upgradeable#storage_gaps */ uint256[49] private __gap; Figure 6.1: openzeppelin-contracts-upgradeable/OwnerUpgradeable.sol Fix Analysis This issue has been resolved. All stateful contracts inherited by UpgradableMeson now contain gap slots. Thus, new state variables can be added in future upgrades. 18 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Medium"]}, {"title": "7. Lack of a zero-value check on the initialize function Status: Partially Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "The UCTUpgradeable contracts initialize function fails to validate the address of the incoming minter argument. This means that the caller can accidentally set the minter variable to the zero address. function initialize(address minter) public initializer { __ERC20_init(\"USD Coupon Token (https://meson.fi)\", \"UCT\"); _owner = _msgSender(); _minter = minter; // _mesonContract = ; 20 21 22 23 24 25 } Figure 7.1: contracts/Token/UCTUpgradeable.sol:2025 If the minter address is set to the zero address, the admin must immediately redeploy the contract and set the address to the correct value; a failure to do so could result in unexpected behavior. Fix Analysis This issue has been partially resolved. The _mesonContract address, added as a parameter in the resolution of TOB-MES-5, is now checked against the zero value. However, the initialize function does not validate that the minter address is non-zero. 19 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "8. Solidity compiler optimizations can be problematic Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "The Meson protocol has enabled optional compiler optimizations in Solidity. There have been several optimization bugs with security implications. Moreover, optimizations are actively being developed. Solidity compiler optimizations are disabled by default, and it is unclear how many contracts in the wild actually use them. Therefore, it is unclear how well they are being tested and exercised. High-severity security issues due to optimization bugs have occurred in the past. A high-severity bug in the emscripten-generated solc-js compiler used by True and Remix persisted until late 2018. The x for this bug was not reported in the Solidity CHANGELOG. Another high-severity optimization bug resulting in incorrect bit shift results was patched in Solidity 0.5.6. More recently, another bug due to the incorrect caching of keccak256 was reported. A compiler audit of Solidity from November 2018 concluded that the optional optimizations may not be safe. It is likely that there are latent bugs related to optimization and that new bugs will be introduced due to future optimizations. Fix Analysis This issue has been resolved. The Solidity optimizer in the commit evaluated in this x review has been disabled. However, the Meson team indicated that this change causes the deployment gas amount to exceed the block gas limit. We recommend that Meson closely follow Solidity compiler releases and CHANGELOGs in order to quickly resolve any compiler optimization bugs. 20 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: Low"]}, {"title": "9. Service fees cannot be withdrawn Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "If the service fee charged for a swap is waived, the fee collected for the swap is stored at index zero of the _balanceOfPoolToken mapping. However, because the fee withdrawal function does not allow withdrawals from index zero of the mapping, the fee can never be withdrawn. Although this limitation may be purposeful, the code appears to indicate that it is a mistake. 198 if (!feeWaived) { // If the swap should pay service fee (charged by Meson protocol) 199 200 201 202 uint256 serviceFee = _serviceFee(encodedSwap); // Subtract service fee from the release amount releaseAmount -= serviceFee; // The collected service fee will be stored in `_balanceOfPoolToken` with `poolIndex = 0` 203 _balanceOfPoolToken[_poolTokenIndexForOutToken(encodedSwap, 0)] += serviceFee; Figure 9.1: contracts/Pools/MesonPools.sol:198203 70 71 72 73 74 function withdraw(uint256 amount, uint48 poolTokenIndex) external { require(amount > 0, \"Amount must be positive\"); uint40 poolIndex = _poolIndexFrom(poolTokenIndex); require(poolIndex != 0, \"Cannot use 0 as pool index\"); Figure 9.2: contracts/Pools/MesonPools.sol:7074 Moreover, even if the function allowed the withdrawal of tokens stored at poolIndex 0, a withdrawal would still not be possible. This is because the owner of poolIndex 0 is not set during initialization, and it is not possible to register a pool with index 0. 13 14 function initialize(address[] memory supportedTokens) public { require(!_initialized, \"Contract instance has already been initialized\"); 21 Meson Protocol Fix Review _initialized = true; _owner = _msgSender(); _premiumManager = _msgSender(); for (uint8 i = 0; i < supportedTokens.length; i++) { _addSupportToken(supportedTokens[i], i + 1); 15 16 17 18 19 20 21 22 } } Figure 9.3: contracts/UpgradableMeson.sol:1322 Fix Analysis This issue has been resolved. The source code now includes comments explaining that the service fee will not be withdrawable until the contract is updated. 22 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "10. Lack of contract existence check on transfer / transferFrom calls Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "The MesonHelpers contract uses the low-level call function to execute the transfer / transferFrom function of an ERC20 token. However, it does not rst perform a contract existence check. Thus, if there is no contract at the token address, the low-level call will still return success. This means that if a supported token is subsequently self-destructed (which is unlikely to happen), it will be possible for a posted swap involving that token to succeed without actually depositing any tokens. function _unsafeDepositToken( address token, address sender, uint256 amount, bool isUCT 53 54 55 56 57 58 ) internal { 59 60 61 62 require(token != address(0), \"Token not supported\"); require(amount > 0, \"Amount must be greater than zero\"); (bool success, bytes memory data) = token.call(abi.encodeWithSelector( bytes4(0x23b872dd), // bytes4(keccak256(bytes(\"transferFrom(address,address,uint256)\"))) 63 64 65 66 sender, address(this), amount // isUCT ? amount : amount * 1e12 // need to switch to this line if deploying to BNB Chain or Conflux 67 68 )); require(success && (data.length == 0 || abi.decode(data, (bool))), \"transferFrom failed\"); 69 } Figure 10.1: contracts/util/MesonHelpers.sol:5369 The Solidity documentation includes the following warning: The low-level functions call, delegatecall and staticcall return true as their first 23 Meson Protocol Fix Review return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed. Figure 10.2: A snippet of the Solidity documentation detailing unexpected behavior related to call Fix Analysis This issue has been resolved. The low-level call is now paired with OpenZeppelins Address.isContract function, which ensures that the contract at the target address is populated as expected. This makes the deposit mechanism robust against self-destructs. 24 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "11. USDT transfers to third-party contracts will fail Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "To allow a user to release funds to a smart contract, the Meson protocol increases the contracts allowance (via a call to increaseAllowance) and then calls the contract, as shown in gure 11.1. 66 IERC20Minimal(token).increaseAllowance(contractAddr, adjustedAmount); 67 ITransferWithBeneficiary(contractAddr).transferWithBeneficiary(token, adjustedAmount, beneficiary, data); Figure 11.1: contracts/utils/MesonHelpers.sol#6667 The increaseAllowance method, which is part of OpenZeppelins ERC20 library, was introduced to prevent race conditions when token allowances are changed via top-level calls. However, this method is not in the ERC20 specication, and not all tokens implement it. In particular, USDT does not implement the method on the Ethereum mainnet. Thus, any attempt to release USDT to a smart contract wallet during a swap will fail, trapping the users funds. Fix Analysis This issue has been resolved. The protocol now uses the standard ERC20 approve function to increase allowances. The team also made subtle changes to the allowance behavior: instead of incrementing an allowance when executing a transfer, the Meson contract now sets the allowance to the most recent transfer amount. If a third-party contract has an outstanding allowance from a previous swap release, it will forfeit those tokens upon the next transfer. Meson has conrmed that this is the intended behavior. 25 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: High", "Difficulty: Low"]}, {"title": "12. SDK function _randomHex returns low-quality randomness Status: Partially Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "The Meson protocol software development kit (SDK) uses the _randomHex function to generate random salts for new swaps. This function accepts a string length as input and produces a random hexadecimal string of that length. To do that, _randomHex uses the JavaScript Math.random function to generate a 32-bit integer and then encodes the integer as a zero-padded hexadecimal string. The result is eight random hexadecimal characters, padded with zeros to the desired length. However, the function is called with an argument of 16, so half of the characters in the salt it produces will be zero. } 95 96 97 98 99 100 101 102 103 104 } 105 106 107 108 109 110 111 112 113 } private _makeFullSalt(salt?: string): string { if (salt) { if (!isHexString(salt) || salt.length > 22) { throw new Error('The given salt is invalid') } return `${salt}${this._randomHex(22 - salt.length)}` return `0x0000${this._randomHex(16)}` private _randomHex(strLength: number) { if (strLength === 0) { return '' } const max = 2 ** Math.min((strLength * 4), 32) const rnd = BigNumber.from(Math.floor(Math.random() * max)) return hexZeroPad(rnd.toHexString(), strLength / 2).replace('0x', '') Figure 12.1: packages/sdk/src/Swap.ts#95113 Furthermore, the Math.random function is not suitable for uses in which the output of the random number generator should be unpredictable. While the protocols current use of the function does not pose a security risk, future implementers and library users may assume that the function produces the requested amount of high-quality entropy. 26 Meson Protocol Fix Review Fix Analysis This issue has been partially resolved. While the _randomHex function now uses cryptographic randomness to generate random hexadecimal characters, the function continues to silently output leading zeros when more than eight characters are requested or when an odd number of characters is requested. To prevent future misuse of this function, we recommend having it return a uniformly random string with the exact number of characters requested. 27 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "13. encodedSwap values are used as primary swap identier Status: Unresolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "The primary identier of swaps in the MesonSwap contract is the encodedSwap structure. This structure does not contain the address of a swaps initiator, which is recorded, along with the poolIndex of the bonded liquidity provider (LP), as the postingValue. If a malicious actor or maximal extractable value (MEV) bot were able to front-run a users transaction and post an identical encodedSwap, the original initiators transaction would fail, and the initiators swap would not be posted. 48 function postSwap(uint256 encodedSwap, bytes32 r, bytes32 s, uint8 v, uint200 postingValue) external forInitialChain(encodedSwap) 49 50 { 51 require(_postedSwaps[encodedSwap] == 0, \"Swap already exists\"); ... Figure 13.1: contracts/Swap/MesonSwap.sol#4852 Because the Meson protocol supports only 1-to-1 stablecoin swaps, transaction front-running is unlikely to be protable. However, a bad actor could dramatically aect a specic users ability to transact within the system. Fix Analysis This issue has not been resolved. Meson acknowledged that user transactions can be blocked from execution by malicious actors. However, blocking a swap transaction would require an adversary to post a corresponding swap, and to thus burn gas and have his or her funds temporarily locked; these disincentives limit the impact of this issue. 28 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "14. Unnecessary _releasing mutex increases gas costs Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "When executing a swap in the third-party dApp integration release mode, the Meson protocol makes a call to an untrusted user-specied smart contract. To prevent reentrancy attacks, a ag is set before and cleared after the untrusted contract call. 181 ... require(!_releasing, \"Another release is running\"); 219 220 _transferToContract(_tokenList[tokenIndex], recipient, initiator, amount, _releasing = true; tokenIndex == 255, _saltDataFrom(encodedSwap)); 221 _releasing = false; Figure 14.1: contracts/Pools/MesonPools.sol#181221 This ag is not strictly necessary, as by the time the contract reaches the untrusted call, it has already cleared the _lockSwaps entry corresponding to the release, preventing duplicate releases via reentrancy. uint80 lockedSwap = _lockedSwaps[swapId]; require(lockedSwap != 0, \"Swap does not exist\"); 191 192 ... 196 _checkReleaseSignature(encodedSwap, recipient, r, s, v, initiator); 197 ... 211 _release(encodedSwap, tokenIndex, initiator, recipient, releaseAmount); _lockedSwaps[swapId] = 0; Figure 14.2: contracts/Pools/MesonPools.sol#191197 Fix Analysis This issue has been resolved. The redundant _releasing ag has been removed. The call to the external contract is the last step in the transaction, which prevents reentrancy attacks. 29 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "3. Missing validation in the _addSupportToken function Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "Insucient input validation in the _addSupportToken function makes it possible to register the same token as supported multiple times. This does not cause a problem, because if there are duplicate entries for a token in the token list, the last one added will be the one that is used. However, it does mean that multiple indexes could point to the same token, while the token would point to only one of those indexes. function _addSupportToken(address token, uint8 index) internal { require(index != 0, \"Cannot use 0 as token index\"); _indexOfToken[token] = index; _tokenList[index] = token; 47 48 49 50 51 } Figure 3.1: contracts/utils/MesonTokens.sol Fix Analysis This issue has been resolved. The _addSupportToken function now validates that the token has not previously been registered, that the associated list index has not previously been used, and that the tokens address is not zero. The Meson team has also added tests to validate this behavior. 15 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: High"]}, {"title": "4. Insu\u0000cient event generation Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "Several critical operations in the MesonPools contract do not emit events. As a result, it will be dicult to review the correct behavior of the contract once it has been deployed. The following operations should trigger events:  MesonPools.depositAndRegister  MesonPools.deposit  MesonPools.withdraw  MesonPools.addAuthorizedAddr  MesonPools.removeAuthorizedAddr  MesonPools.unlock Without events, users and blockchain-monitoring systems cannot easily detect suspicious behavior and may therefore overlook attacks or malfunctioning contracts. Fix Analysis This issue has been resolved. All of the functions listed in this nding now emit events, enabling Meson and protocol users to easily track all contract operations. 16 Meson Protocol Fix Review", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "15. Misleading result returned by view function getPostedSwap Status: Resolved ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MesonProtocolFixReview.pdf", "body": "The value returned by the getPostedSwap function to indicate whether a swap has been executed can be misleading. Once a swap has been executed, the value of the swap is reset to either 0 or 1. However, the getPostedSwap function returns a result indicating that a swap has been executed only if the swaps value is 1. if (_expireTsFrom(encodedSwap) < block.timestamp + MIN_BOND_TIME_PERIOD) { // The swap cannot be posted again and therefore safe to remove it. // LPs who execute in this mode can save ~5000 gas. _postedSwaps[encodedSwap] = 0; 141 142 143 144 145 } else { 146 // The same swap information can be posted again, so set `_postedSwaps` value to 1 to prevent that. 147 148 } _postedSwaps[encodedSwap] = 1; Figure 15.1: contracts/Swap/MesonSwap.sol:140148 161 162 163 164 { 165 166 167 168 169 170 171 172 173 } /// @notice Read information for a posted swap function getPostedSwap(uint256 encodedSwap) external view returns (address initiator, address poolOwner, bool executed) uint200 postedSwap = _postedSwaps[encodedSwap]; initiator = _initiatorFromPosted(postedSwap); executed = postedSwap == 1; if (initiator == address(0)) { poolOwner = address(0); } else { } poolOwner = ownerOfPool[_poolIndexFromPosted(postedSwap)]; Figure 15.2: contracts/Swap/MesonSwap.sol:162173 30 Meson Protocol Fix Review Front-end services (or any other service interacting with this function) may be misled by the return value, reacting as though a swap has not been executed when it actually has. Fix Analysis This issue has been resolved. The getPostedSwap functions return value has been renamed to exist, which more accurately reects the meaning of the value. 31 Meson Protocol Fix Review A. Status Categories The following table describes the statuses used to indicate whether an issue has been suciently addressed. Fix Status Status", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Low"]}, {"title": "1. Attackers can prevent lenders from funding or renancing loans ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "For the MapleLoan contracts fundLoan method to fund a new loan, the balance of fundsAsset in the contract must be equal to the requested principal. // Amount funded and principal are as requested. amount_ = _principal = _principalRequested; // Cannot under/over fund loan, so that accounting works in context of PoolV1 require (_getUnaccountedAmount(_fundsAsset) == amount_, \"MLI:FL:WRONG_FUND_AMOUNT\" ); Figure 1.1: An excerpt of the fundLoan function ( contracts/MapleLoanInternals.sol#240244 ) An attacker could prevent a lender from funding a loan by making a small transfer of fundsAsset every time the lender tried to fund it (front-running the transaction). However, transaction fees would make the attack expensive. A similar issue exists in the Refinancer contract: If the terms of a loan were changed to increase the borrowed amount, an attacker could prevent a lender from accepting the new terms by making a small transfer of fundsAsset . The underlying call to increasePrincipal from within the acceptNewTerms function would then cause the transaction to revert. function increasePrincipal ( uint256 amount_ ) external override { require (_getUnaccountedAmount(_fundsAsset) == amount_, \"R:IP:WRONG_AMOUNT\" ); _principal += amount_; _principalRequested += amount_; _drawableFunds += amount_; emit PrincipalIncreased(amount_); 13 Maple Labs } Figure 1.2: The vulnerable method in the Refinancer contract ( contracts/Refinancer.sol#2330 ) Exploit Scenario A borrower tries to quickly increase the principal of a loan to take advantage of a short-term high-revenue opportunity. The borrower proposes new terms, and the lender tries to accept them. However, an attacker blocks the process and performs the protable operation himself. Recommendations Short term, allow the lender to withdraw funds in excess of the expected value (by calling getUnaccountedAmount(fundsAsset) ) before a loan is funded and between the proposal and acceptance of new terms. Alternatively, have fundLoan and increasePrincipal use greater-than-or-equal-to comparisons, rather than strict equality comparisons, to check whether enough tokens have been transferred to the contract; if there are excess tokens, use the same function to transfer them to the lender. Long term, avoid using exact comparisons for ether and token balances, as users can increase those balances by executing transfers, making the comparisons evaluate to false . 14 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: High"]}, {"title": "2. Reentrancies can lead to misordered events ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "Several functions in the codebase do not use the checks-eects-interactions pattern, lack reentrancy guards, or emit events after interactions. These functions interact with external and third-party contracts that can execute callbacks and call the functions again (reentering them). The event for a reentrant call will be emitted before the event for the rst call, meaning that o-chain event monitors will observe incorrectly ordered events. function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer(collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 2.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) We identied this issue in the following functions:  DebtLocker  setAuctioneer  _handleClaim  _handleClaimOfReposessed  acceptNewTerms  Liquidator  liquidatePortion  pullFunds  MapleLoan 15 Maple Labs  acceptNewTerms  closeLoan  fundLoan  makePayment  postCollateral  returnFunds  skim  upgrade Exploit Scenario Alice calls Liquidator.liquidatePortion (gure 2.1). Since fundsAsset is an ERC777 token (or another token that allows callbacks), a callback function that Alice has registered on ERC20Helper.transfer is called. Alice calls Liquidator.liquidatePortion again from within that callback function. The event for the second liquidation is emitted before the event for the rst liquidation. As a result, the events observed by o-chain event monitors are incorrectly ordered. Recommendations Short term, follow the checks-eects-interactions pattern and ensure that all functions emit events before interacting with other contracts that may allow reentrancies. Long term, integrate Slither into the CI pipeline. Slither can detect low-severity reentrancies like those mentioned in this nding as well as high-severity reentrancies. Use reentrancy guards on all functions that interact with other contracts. 16 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "3. Lack of two-step process for critical operations ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The MapleLoan contracts setBorrower and setLender functions transfer the privileged borrower and lender roles to new addresses. If, because of a bug or a mistake, one of those functions is called with an address inaccessible to the Maple Labs team, the transferred role will be permanently inaccessible. It may be possible to restore access to the lender role by upgrading the loan contract to a new implementation. However, only the borrower can upgrade a loan contract, so no such bailout option exists for a transfer of the borrower role to an inaccessible address. Using a two-step process for role transfers would prevent such issues. Exploit Scenario Alice, the borrower of a Maple loan, notices that her borrower address key might have been compromised. To be safe, she calls MapleLoan.setBorrower with a new address. Because of a bug in the script that she uses to set the new borrower, the new borrower is set to an address for which Alice does not have the private key. As a result, she is no longer able to access her loan contract. Recommendations Short term, perform role transfers through a two-step process in which the borrower or lender proposes a new address and the transfer is completed once the new address has executed a call to accept the role. Long term, investigate whether implementing additional two-step processes could prevent any other accidental lockouts. 17 Maple Labs", "labels": ["Trail of Bits", "Severity: Low", "Difficulty: Medium"]}, {"title": "4. IERC20Like.decimals returns non-standard uint256 ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "IERC20Like.decimal s declares uint256 as its return type, whereas the ERC20 standard species that it must return a uint8 . As a result, functions that use the IERC20Like interface interpret the values returned by decimals as uint256 values; this can cause values greater than 255 to enter the protocol, which could lead to undened behavior. If the return type were uint8 , only the last byte of the return value would be used. Exploit Scenario A non-standard token with a decimals function that returns values greater than 255 is integrated into the protocol. The code is not prepared to handle decimals values greater than 255. As a result of the large value, the arithmetic becomes unstable, enabling an attacker to drain funds from the protocol. Recommendations Short term, change the return type of IERC20.decimals to uint8 . Long term, ensure that all interactions with ERC20 tokens follow the standard. 18 Maple Labs", "labels": ["Trail of Bits", "Severity: Undetermined", "Difficulty: High"]}, {"title": "5. Transfers in Liquidator.liquidatePortion can fail silently ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "Calls to ERC20Helper.transfer in the codebase are wrapped in require statements, except for the rst such call in the liquidatePortion function of the Liquidator contract (gure 5.1). As such, a token transfer executed through this call can fail silently, meaning that liquidatePortion can take a user's funds without providing any collateral in return. This contravenes the expected behavior of the function and the behavior outlined in the docstring of ILiquidator.liquidatePortion (gure 5.2). function liquidatePortion ( uint256 swapAmount_ , bytes calldata data_) external override { ERC20Helper.transfer (collateralAsset, msg.sender , swapAmount_); msg.sender .call(data_); uint256 returnAmount = getExpectedAmount(swapAmount_); require (ERC20Helper.transferFrom(fundsAsset, msg.sender , destination, returnAmount), \"LIQ:LP:TRANSFER_FROM\" ); emit PortionLiquidated(swapAmount_, returnAmount); } Figure 5.1: The liquidatePortion function ( contracts/Liquidator.sol#4151 ) * @dev Flash loan function that : * @dev 1 . Transfers a specified amount of `collateralAsset` to ` msg.sender `. * @dev 2 . Performs an arbitrary call to ` msg.sender `, to trigger logic necessary to get `fundsAsset` (e.g., AMM swap). * @dev 3 . Perfroms a `transferFrom`, taking the corresponding amount of `fundsAsset` from the user. * @dev If the required amount of `fundsAsset` is not returned in step 3 , the entire transaction reverts. * @param swapAmount_ Amount of `collateralAsset` that is to be borrowed in the flashloan. * @param data_ 2 . ABI-encoded arguments to be used in the low-level call to perform step 19 Maple Labs */ Figure 5.2: Docstring of liquidatePortion ( contracts/interfaces/ILiquidator.sol#7683 ) Exploit Scenario A loan is liquidated, and its liquidator contract has a collateral balance of 300 ether. The current ether price is 4,200 USDC. Alice wants to prot o of the liquidation by taking out a ash loan of 300 ether. Having checked that the contract holds enough collateral to cover the transaction, she calls liquidatePortion(1260000, ) in the liquidator contract. At the same time, Bob decides to buy 10 ether from the liquidator contract. Bob calls Liquidator.liquidatePortion(42000) . Because his transaction is mined rst, the liquidator does not have enough collateral to complete the transfer of collateral to Alice. As a result, the liquidator receives a transfer of 1,260,000 USDC from Alice but does not provide any ether in return, leaving her with a $1,260,000 loss. Recommendations Short term, wrap ERC20Helper.transfer in a require statement to ensure that a failed transfer causes the entire transaction to revert. Long term, ensure that a failed transfer of tokens to or from a user always causes the entire transaction to revert. To do that, follow the recommendations outlined in TOB-MAPLE-006 and have the ERC20Helper.transfer and ERC20Helper.transferFrom functions revert on a failure. Ensure that all functions behave as expected , that their behavior remains predictable when transactions are reordered, and that the code does not contain any footguns or surprises. 20 Maple Labs", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Low"]}, {"title": "6. ERC20Helpers functions do not revert on a failure ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The ERC20Helper contracts transfer , transferFrom , and approve functions do not revert on a failure. This makes it necessary for the developer to always check their return values. A failure to perform these checks can result in the introduction of high-severity bugs that can lead to a loss of funds. There are no uses of ERC20Helper.transfer for which not reverting on a failure is the best option. Making this standard behavior the default would make the code more robust and therefore more secure by default, as it would take less additional eort to make it secure. In the rare edge cases in which a transfer is allowed to fail or a failure status should be captured in a boolean, a try / catch statement can be used. Exploit Scenario Bob, a developer, writes a new function. He calls ERC20Helper.transfer but forgets to wrap the call in a require statement. As a result, token transfers can fail silently and lead to a loss of funds if that failure behavior is not accounted for. Recommendations Short term, have ERC20Helper.transfer , ERC20Helper.transferFrom , and ERC20Helper.approve revert on a failure. Long term, have all functions revert on a failure instead of returning false . Aim to make code secure by default so that less additional work will be required to make it secure. Additionally, whenever possible, avoid using optimizations that are detrimental to security. 21 Maple Labs", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Low"]}, {"title": "7. Lack of contract existence checks before low-level calls ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The ERC20Helper contract lls a purpose similar to that of OpenZeppelin's SafeERC20 contract. However, while OpenZeppelin's SafeERC20 transfer and approve functions will revert when called on an address that is not a token contract address (i.e., one with zero-length bytecode), ERC20Helper s functions will appear to silently succeed without transferring or approving any tokens. If the address of an externally owned account (EOA) is used as a token address in the protocol, all transfers to it will appear to succeed without any tokens being transferred. This will result in undened behavior. Contract existence checks are usually performed via the EXTCODESIZE opcode. Since the EXTCODESIZE opcode would precede a CALL to a token address, adding EXTCODESIZE would make the CALL a warm access. As a result, adding the EXTCODESIZE check would increase the gas cost by only a little more than 100. Assuming a high gas price of 200 gwei and a current ether price of $4,200, that equates to an additional cost of 10 cents for each call to the functions of ERC20Helper , which is a low price to pay for increased security. The following functions lack contract existence checks:  ERC20Helper  call in _call  ProxyFactory  call in _initializeInstance  call in _upgradeInstance (line 66)  call in _upgradeInstance (line 72)  Proxied  delegatecall in _migrate  Proxy  delegatecall in _ fallback 22 Maple Labs  MapleLoanInternals  delegatecall in _acceptNewTerms Exploit Scenario A token contract is destroyed. However, since all transfers of the destroyed token will succeed, all Maple protocol users can transact as though they have an unlimited balance of that token. If contract existence checks were executed before those transfers, all transfers of the destroyed token would revert. Recommendations Short term, add a contract existence check before each of the low-level calls mentioned above. Long term, add contract existence checks before all low-level CALL s, DELEGATECALL s, and STATICCALL s. These checks are inexpensive and add an important layer of defense. 23 Maple Labs", "labels": ["Trail of Bits", "Severity: High", "Difficulty: High"]}, {"title": "8. Missing zero checks ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "A number of constructors and functions in the codebase do not revert if zero is passed in for a parameter that should not be set to zero. The following parameters are not checked for the zero value:  Liquidator contract  constructor()  owner_  collateralAsset_  fundsAsset_  auctioneer_  destination_  setAuctioneer()  auctioneer_  MapleLoan contract  setBorrower()  borrower_  setLender()  lender_  MapleProxyFactory contract  constructor()  mapleGlobals_ If zero is passed in for one of those parameters, it will render the contract unusable, leaving its funds locked (and therefore eectively lost) and necessitating an expensive redeployment. For example, if there were a bug in the front end, MapleLoan.setBorrower could be called with address(0) , rendering the contract unusable and locking its funds in it. 24 Maple Labs The gas cost of checking a parameter for the zero value is negligible. Since the parameter is usually already on the stack, a zero check consists of a DUP opcode (3 gas) and an ISZERO opcode (3 gas). Given a high gas price of 200 gwei and an ether price of $4,200, a zero check would cost half a cent. Exploit Scenario A new version of the front end is deployed. A borrower suspects that the address currently used for his or her loan might have been compromised. As a precautionary measure, the borrower decides to transfer ownership of the loan to a new address. However, the new version of the front end contains a bug: the value of an uninitialized variable is used to construct the transaction. As a result, the borrower loses access to the loan contract, and to the collateral, forever. If zero checks had been in place, the transaction would have reverted instead. Recommendations Short term, add zero checks for the parameters mentioned above and for all other parameters for which zero is not an acceptable value. Long term, comprehensively validate all parameters. Avoid relying solely on the validation performed by front-end code, scripts, or other contracts, as a bug in any of those components could prevent it from performing that validation. Additionally, integrate Slither into the CI pipeline to automatically detect functions that lack zero checks. 25 Maple Labs", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: Medium"]}, {"title": "9. Lack of user-controlled limits for input amount in Liquidator.liquidatePortion ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/MapleFinance.pdf", "body": "The liquidatePortion function of the Liquidator contract computes the amount of funds that will be transferred from the caller to the liquidator contract. The computation uses an asset price retrieved from an oracle. There is no guarantee that the amount paid by the caller will correspond to the current market price, as a transaction that updates the price feed could be mined before the call to liquidatePortion in the liquidator contract. EOAs that call the function cannot predict the return value of the oracle. If the caller is a contract, though, it can check the return value, with some eort. Adding an upper limit to the amount paid by the caller would enable the caller to explicitly state his or her assumptions about the execution of the contract and to avoid paying too much. It would also provide additional protection against the misreporting of oracle prices. Since such a scenario is unlikely, we set the diculty level of this nding to high. Using caller-controlled limits for the amount of a transfer is a best practice commonly employed by large DeFi protocols such as Uniswap. Exploit Scenario Alice calls liquidatePortion in the liquidator contract. Due to an oracle malfunction, the amount of her transfer to the liquidator contract is much higher than the amount she would pay for the collateral on another market. Recommendations Short term, introduce a maxReturnAmount parameter and add a require statement require(returnAmount <= maxReturnAmount) to enforce that parameter. 26 Maple Labs Long term, always allow the caller to control the amount of a transfer. This is especially important for transfer amounts that depend on factors that can change between transactions. Enable the caller to add a lower limit for a transfer from a contract and an upper limit for a transfer of the callers funds to a contract. 27 Maple Labs A. Vulnerability Categories The following tables describe the vulnerability categories, severity levels, and diculty levels used in this document. Vulnerability Categories Category", "labels": ["Trail of Bits", "Severity: Medium", "Difficulty: High"]}, {"title": "3. callApprove does not follow approval best practices ", "html_url": "https://github.com/trailofbits/publications/tree/master/reviews/2023-07-arcade-securityreview.pdf", "body": "The AssetVault.callApprove function has undocumented behaviors and lacks the increase/decrease approval functions, which might impede third-party integrations. A well-known race condition exists in the ERC-20 approval mechanism. The race condition is enabled if a user or smart contract calls approve a second time on a spender that has already been allowed. If the spender sees the transaction containing the call before it has been mined, they can call transferFrom to transfer the previous value and then still receive authorization to transfer the new value. To mitigate this, AssetVault uses the SafeERC20.safeApprove function, which will revert if the allowance is updated from nonzero to nonzero. However, this behavior is not documented, and it might break the protocols integration with third-party contracts or o-chain components. 282 283 284 285 286 287 288 289 290 291 292 293 294 295 37 38 39 40 41 42 function callApprove( address token, address spender, uint256 amount ) external override onlyAllowedCallers onlyWithdrawDisabled nonReentrant { if (!CallWhitelistApprovals(whitelist).isApproved(token, spender)) { revert AV_NonWhitelistedApproval(token, spender); } // Do approval IERC20(token).safeApprove(spender, amount); emit Approve(msg.sender, token, spender, amount); } Figure 3.1: The callApprove function in arcade-protocol/contracts/vault/AssetVault.sol /** * @dev Deprecated. This function has issues similar to the ones found in * {IERC20-approve}, and its usage is discouraged. * * Whenever possible, use {safeIncreaseAllowance} and * {safeDecreaseAllowance} instead. 26 Arcade.xyz V3 Security Assessment */ function safeApprove( IERC20 token, address spender, uint256 value ) internal { 43 44 45 46 47 48 49 50 51 52 53 54 55 56 spender, value)); 57 } // safeApprove should only be called when setting an initial allowance, // or when resetting it to zero. To increase and decrease it, use // 'safeIncreaseAllowance' and 'safeDecreaseAllowance' require( (value == 0) || (token.allowance(address(this), spender) == 0), \"SafeERC20: approve from non-zero to non-zero allowance\" ); _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, Figure 3.2: The safeApprove function in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol An alternative way to mitigate the ERC-20 race condition is to use the increaseAllowance and decreaseAllowance functions to safely update allowances. These functions are widely used by the ecosystem and allow users to update approvals with less ambiguity. uint256 newAllowance = token.allowance(address(this), spender) + value; _callOptionalReturn(token, abi.encodeWithSelector(token.approve.selector, } ) internal { function safeIncreaseAllowance( function safeDecreaseAllowance( IERC20 token, address spender, uint256 value 59 60 61 62 63 64 65 spender, newAllowance)); 66 67 68 69 70 71 72 73 74 75 zero\"); 76 77 abi.encodeWithSelector(token.approve.selector, spender, newAllowance)); 78 79 uint256 newAllowance = oldAllowance - value; _callOptionalReturn(token, IERC20 token, address spender, uint256 value ) internal { unchecked { } } uint256 oldAllowance = token.allowance(address(this), spender); require(oldAllowance >= value, \"SafeERC20: decreased allowance below Figure 3.3: The safeIncreaseAllowance and safeDecreaseAllowance functions in openzeppelin-contracts/contracts/token/ERC20/utils/SafeERC20.sol 27 Arcade.xyz V3 Security Assessment Exploit Scenario Alice, the owner of an asset vault, sets up an approval of 1,000 for her external contract by calling callApprove. She later decides to update the approval amount to 1,500 and again calls callApprove. This second call reverts, which she did not expect. Recommendations Short term, take one of the following actions:  Update the documentation to make it clear to users and other integrating smart contract developers that two transactions are needed to update allowances.  Add two new functions in the AssetVault contract: callIncreaseAllowance and callDecreaseAllowance, which internally call SafeERC20.safeIncreaseAllowance and SafeERC20.safeDecreaseAllowance, respectively. Long term, when using external libraries/contracts, always ensure that they are being used correctly and that edge cases are explained in the documentation. 28 Arcade.xyz V3 Security Assessment", "labels": ["Trail of Bits", "Severity: Informational", "Difficulty: Medium"]}]